{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness</b> metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "Depending on the <b>fairness</b> <b>metric</b> that is considered most useful for a given scenario, this comparison can help a recommender system developer to select the appropriate <b>algorithm</b> while achieving the best <b>fairness</b> in recommendations. On the other hand, these results reflect the complexity of considering <b>different</b> types of <b>fairness</b> definitions and mitigation strategies, and also underline that achieving a good result in one <b>fairness</b> <b>metric</b> (e.g., ItemItem approaches lead to lower Value ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "On Consequentialism and <b>Fairness</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861221/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861221", "snippet": "Meanwhile, other papers have emphasized that simply satisfying a particular definition of <b>fairness</b> is no guarantee of the broader outcomes <b>people</b> care about, such as justice (Hu and Chen, 2018b). Selbst et al. ( 2019 ) discuss five common \u201ctraps\u201d in thinking about sociotechnical systems, and Friedler et al. ( 2019 ) demonstrate how outcomes differs depending on preprocessing and the choice of <b>fairness</b> <b>metric</b>.", "dateLastCrawled": "2022-01-28T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "C <b>AI Fairness</b> - IBM", "url": "https://community.ibm.com/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=09903149-3c91-6208-feaf-95004d77a8ee&forceDialog=0", "isFamilyFriendly": true, "displayUrl": "https://community.ibm.com/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=...", "snippet": "Many definitions focus either on individual <b>fairness</b> (<b>treating</b> similar individuals similarly) or on group <b>fairness</b> (making the model\u2019s pre\u2010 dictions/outcomes equitable across <b>groups</b>). Individual <b>fairness</b> seeks to ensure that statistical measures of outcomes are <b>equal</b> for similar individuals. Group <b>fairness</b> partitions a population into <b>groups</b> defined by protected attributes and seeks to ensure that stat\u2010 istical measures of outcomes are <b>equal</b> across <b>groups</b>. Within those who focus on ...", "dateLastCrawled": "2022-01-21T07:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "While this <b>metric</b> might feel <b>like</b> it meets a reasonable criteria of avoiding <b>treating</b> <b>groups</b> differently in terms of classification errors, there are other sources of disparities we might care about as well. For instance, suppose there are 10,000 individuals in Group A and 30,000 in Group B. Suppose further that 100 individuals from each group are jail, with 10 Group A <b>people</b> wrongly convicted and 30 Group B <b>people</b> wrongly convicted. We\u2019ve balanced the number of false positives by group ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The (Im)possibility of <b>Fairness: Different Value Systems Require</b> ...", "url": "https://cacm.acm.org/magazines/2021/4/251365-the-impossibility-of-fairness/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2021/4/251365-the-impossibility-of-<b>fairness</b>", "snippet": "When one believes that demographic <b>groups</b> are, on the whole, fundamentally similar, group <b>fairness</b> mechanisms successfully guarantee the top-level goal of non-discrimination: similar <b>groups</b> receiving similar treatment. Alternatively, one can assume the observed data generally reflects the true underlying reality about differences between <b>people</b>. These worldviews are in conflict; a single <b>algorithm</b> cannot satisfy either definition of <b>fairness</b> under both worldviews. Thus, researchers and ...", "dateLastCrawled": "2022-02-03T00:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Machine learning and algorithmic <b>fairness</b> in public and population ...", "url": "https://www.nature.com/articles/s42256-021-00373-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00373-4", "snippet": "Even if successful in ensuring that the risk is fair using a group <b>metric</b> such as demographic parity (<b>equal</b> decision rates across <b>groups</b> regardless of outcome) 94 with respect to racial identity ...", "dateLastCrawled": "2022-01-30T16:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Algorithmic Fairness: Choices, Assumptions, and Definitions</b> | Annual ...", "url": "https://www.annualreviews.org/doi/10.1146/annurev-statistics-042720-125902", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/10.1146/annurev-statistics-042720-125902", "snippet": "A recent wave of research has attempted to define <b>fairness</b> quantitatively. In particular, this work has explored what <b>fairness</b> might mean in the context of decisions based on the predictions of statistical and machine learning models. The rapid growth of this new field has led to wildly inconsistent motivations, terminology, and notation, presenting a serious challenge for cataloging and comparing definitions. This article attempts to bring much-needed order. First, we explicate the various ...", "dateLastCrawled": "2022-01-21T17:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Perception of <b>fairness</b> in algorithmic decisions: Future developers ...", "url": "https://www.cell.com/patterns/fulltext/S2666-3899(21)00247-6", "isFamilyFriendly": true, "displayUrl": "https://www.cell.com/patterns/fulltext/S2666-3899(21)00247-6", "snippet": "In particular, <b>people</b> rate the <b>algorithm</b> as more fair when the decision is in their favor, irrespective of whether it appears to be biased toward certain social <b>groups</b>. In the same vein, Pierson showed that there are gender differences in perceptions of algorithmic <b>fairness</b>, while demographic differences contribute to the variability of opinions on <b>fairness</b>.", "dateLastCrawled": "2021-11-22T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Perception of <b>fairness</b> in algorithmic decisions: Future developers ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "snippet": "<b>Different</b> variables\u2014such as the education level and favorable outcome, as well as development procedures of the system\u2014have also been proven to affect the perception of algorithmic <b>fairness</b>. 27 In particular, <b>people</b> rate the <b>algorithm</b> as more fair when the decision is in their favor, irrespective of whether it appears to be biased toward certain social <b>groups</b>.", "dateLastCrawled": "2022-01-30T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Evolution and impact of bias in human and <b>machine learning</b> <b>algorithm</b> ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "snippet": "Common to both recommender systems and information filters is: (1) selection, of a subset of data about which <b>people</b> express their preference, by a process that is not random sampling, and (2) an iterative learning process in which <b>people</b>\u2019s responses to the selected subset are used to train the <b>algorithm</b> for subsequent iterations. The data used to train and optimize performance of these systems are based on human actions. Thus, data that are observed and omitted are not randomly selected ...", "dateLastCrawled": "2021-11-15T00:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness</b> metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "Depending on the <b>fairness</b> <b>metric</b> that is considered most useful for a given scenario, this comparison can help a recommender system developer to select the appropriate <b>algorithm</b> while achieving the best <b>fairness</b> in recommendations. On the other hand, these results reflect the complexity of considering <b>different</b> types of <b>fairness</b> definitions and mitigation strategies, and also underline that achieving a good result in one <b>fairness</b> <b>metric</b> (e.g., ItemItem approaches lead to lower Value ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) An Overview of <b>Fairness</b> in Clustering", "url": "https://www.researchgate.net/publication/354718623_An_Overview_of_Fairness_in_Clustering", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354718623_An_Overview_of_<b>Fairness</b>_in_Clustering", "snippet": "For in-processing (or in-clustering) based fair approaches, the <b>fairness</b> intervention happens as a result of changing the. original learning model, to make it output only fair solutions. This is ...", "dateLastCrawled": "2022-01-24T16:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "C <b>AI Fairness</b> - IBM", "url": "https://community.ibm.com/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=09903149-3c91-6208-feaf-95004d77a8ee&forceDialog=0", "isFamilyFriendly": true, "displayUrl": "https://community.ibm.com/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=...", "snippet": "Many definitions focus either on individual <b>fairness</b> (<b>treating</b> <b>similar</b> individuals similarly) or on group <b>fairness</b> (making the model\u2019s pre\u2010 dictions/outcomes equitable across <b>groups</b>). Individual <b>fairness</b> seeks to ensure that statistical measures of outcomes are <b>equal</b> for <b>similar</b> individuals. Group <b>fairness</b> partitions a population into <b>groups</b> defined by protected attributes and seeks to ensure that stat\u2010 istical measures of outcomes are <b>equal</b> across <b>groups</b>. Within those who focus on ...", "dateLastCrawled": "2022-01-21T07:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "3. Fair Data \u2013 Practical <b>Fairness</b> \u2013 Dev Tutorials", "url": "https://goois.net/3-fair-data-practical-fairness.html", "isFamilyFriendly": true, "displayUrl": "https://goois.net/3-fair-data-practical-<b>fairness</b>.html", "snippet": "These methodologies generally rely on choosing a <b>fairness</b> <b>metric</b>, generally one that is group-oriented rather than individual-oriented, and selecting a <b>fairness</b> <b>metric</b> to equalize between <b>groups</b>. Even in the case of a black-box model, such models can be retrofitted for <b>fairness</b> in post-processing by choosing <b>different</b> thresholds and scoring criteria, or <b>different</b> probabilities of label swapping or categorizations, according to <b>different</b> <b>groups</b>. The methodologies generally work by identifying ...", "dateLastCrawled": "2021-12-07T07:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machine learning and algorithmic <b>fairness</b> in public and population ...", "url": "https://www.nature.com/articles/s42256-021-00373-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00373-4", "snippet": "Even if successful in ensuring that the risk is fair using a group <b>metric</b> such as demographic parity (<b>equal</b> decision rates across <b>groups</b> regardless of outcome) 94 with respect to racial identity ...", "dateLastCrawled": "2022-01-30T16:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Measuring Fairness, Inequality, and Big Data</b>: Social Choice Since Arrow ...", "url": "https://www.annualreviews.org/doi/full/10.1146/annurev-polisci-022018-024704", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/full/10.1146/annurev-polisci-022018-024704", "snippet": "Often, theory suggests that the data can and should be broken into distinct <b>groups</b>, where members of the same group are more <b>similar</b> to each other than they are to members of other <b>groups</b>. A popular class of methods to uncover such groupings are known as clustering algorithms, which attempt to infer <b>groups</b> (i.e., clusters) from a given data set. Many such algorithms exist, but the justifications for choosing one over another have often either been ignored or based on ad hoc considerations ...", "dateLastCrawled": "2022-01-22T10:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "While avoiding disparities on this <b>metric</b> focuses on the reasonable goal of <b>treating</b> <b>different</b> <b>groups</b> similarly in terms of classification errors, we may also want to directly consider two subsets within each group: (1) the set of families not receiving the subsidy, and (2) the set of families who would benefit from receiving the subsidy. We take a closer look at each of these cases below. 11.3.5.3 False Omission Rate. The False Omission Rate (\\(FOR\\)) focuses specifically on <b>people</b> on whom ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Perception of <b>fairness</b> in algorithmic decisions: Future developers ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "snippet": "<b>Different</b> variables\u2014such as the education level and favorable outcome, as well as development procedures of the system\u2014have also been proven to affect the perception of algorithmic <b>fairness</b>. 27 In particular, <b>people</b> rate the <b>algorithm</b> as more fair when the decision is in their favor, irrespective of whether it appears to be biased toward certain social <b>groups</b>.", "dateLastCrawled": "2022-01-30T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Perception of <b>fairness</b> in algorithmic decisions: Future developers ...", "url": "https://www.cell.com/patterns/fulltext/S2666-3899(21)00247-6", "isFamilyFriendly": true, "displayUrl": "https://www.cell.com/patterns/fulltext/S2666-3899(21)00247-6", "snippet": "In particular, <b>people</b> rate the <b>algorithm</b> as more fair when the decision is in their favor, irrespective of whether it appears to be biased toward certain social <b>groups</b>. In the same vein, Pierson showed that there are gender differences in perceptions of algorithmic <b>fairness</b>, while demographic differences contribute to the variability of opinions on <b>fairness</b>.", "dateLastCrawled": "2021-11-22T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Chapter 3 Test Fairness</b> | 2018-19 Summative Technical Report", "url": "https://technicalreports.smarterbalanced.org/2018-19_summative-report/_book/test-fairness.html", "isFamilyFriendly": true, "displayUrl": "https://technicalreports.smarterbalanced.org/.../_book/test-<b>fairness</b>.html", "snippet": "<b>treating</b> all <b>groups</b> <b>of people</b> with appropriate respect in test materials. These rules help ensure that the test content is fair for test takers and acceptable to the many stakeholders and constituent <b>groups</b> within Smarter Balanced member organizations. The more typical view is that bias and sensitivity guidelines apply primarily to the review of test items. However, <b>fairness</b> must be considered in all phases of test development and use. 3.3 Bias and Sensitivity Guidelines. Smarter Balanced ...", "dateLastCrawled": "2022-02-01T07:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness</b> metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "Depending on the <b>fairness</b> <b>metric</b> that is considered most useful for a given scenario, this comparison <b>can</b> help a recommender system developer to select the appropriate <b>algorithm</b> while achieving the best <b>fairness</b> in recommendations. On the other hand, these results reflect the complexity of considering <b>different</b> types of <b>fairness</b> definitions and mitigation strategies, and also underline that achieving a good result in one <b>fairness</b> <b>metric</b> (e.g., ItemItem approaches lead to lower Value ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness</b> as <b>Equal</b> Concession: Critical Remarks on Fair AI | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11948-021-00348-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11948-021-00348-z", "snippet": "Aristotle then tries to specify what such a mean would amount to in <b>different</b> situations, each generating a <b>different</b> species of <b>fairness</b> (NE v.3\u20135). In the distribution of common assets (e.g., honor or wealth), <b>treating</b> like cases alike amounts to distributing them in proportion to desert grounded in character (geometrical proportion). In the correction of injuries, <b>fairness</b> simply returns the parties to their original, <b>equal</b> standing (numerical proportion). Finally, in the exchange of ...", "dateLastCrawled": "2022-01-31T20:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Algorithmic Fairness: Choices, Assumptions, and Definitions</b> | Annual ...", "url": "https://www.annualreviews.org/doi/10.1146/annurev-statistics-042720-125902", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/10.1146/annurev-statistics-042720-125902", "snippet": "This <b>fairness</b> definition <b>can</b> <b>be thought</b> of in terms of ... More generally, we could define a similarity <b>metric</b> between <b>people</b> that is aware of the sensitive variables, motivating the next flavor of <b>fairness</b> definitions (Dwork et al. 2012). 4.4. Impossibilities . Although each flavor of <b>fairness</b> definition presented in this section formalizes an intuitive notion of <b>fairness</b>, these definitions are not mathematically or morally compatible in general. In this section, we review several ...", "dateLastCrawled": "2022-01-21T17:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "On Consequentialism and <b>Fairness</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861221/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861221", "snippet": "In the sense that \u201cactions speak louder than words,\u201d we <b>can</b> treat <b>people</b>&#39;s behavior as revealing of their view of what is ... probabilities such that the proportion of instances that are correctly classified within each bin is the same for all <b>groups</b>. In other words, <b>equal</b> calibration tries to ensure that . \u2211 i \u2208 X \u2161 [a i = 0, p ^ i \u2208 [b, c)] \u00b7 y i \u2211 i \u2208 X \u2161 [a i = 0, p ^ i \u2208 [b, c)] = \u2211 j \u2208 X \u2161 [a j = 1, p ^ j \u2208 [b, c)] \u00b7 y j \u2211 j \u2208 X \u2161 [a j = 1, p ^ j \u2208 ...", "dateLastCrawled": "2022-01-28T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "While avoiding disparities on this <b>metric</b> focuses on the reasonable goal of <b>treating</b> <b>different</b> <b>groups</b> similarly in terms of classification errors, we may also want to directly consider two subsets within each group: (1) the set of families not receiving the subsidy, and (2) the set of families who would benefit from receiving the subsidy. We take a closer look at each of these cases below. 11.3.5.3 False Omission Rate. The False Omission Rate (\\(FOR\\)) focuses specifically on <b>people</b> on whom ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "3. Fair Data \u2013 Practical <b>Fairness</b> \u2013 Dev Tutorials", "url": "https://goois.net/3-fair-data-practical-fairness.html", "isFamilyFriendly": true, "displayUrl": "https://goois.net/3-fair-data-practical-<b>fairness</b>.html", "snippet": "These methodologies generally rely on choosing a <b>fairness</b> <b>metric</b>, generally one that is group-oriented rather than individual-oriented, and selecting a <b>fairness</b> <b>metric</b> to equalize between <b>groups</b>. Even in the case of a black-box model, such models <b>can</b> be retrofitted for <b>fairness</b> in post-processing by choosing <b>different</b> thresholds and scoring criteria, or <b>different</b> probabilities of label swapping or categorizations, according to <b>different</b> <b>groups</b>. The methodologies generally work by identifying ...", "dateLastCrawled": "2021-12-07T07:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The (Im)possibility of <b>Fairness: Different Value Systems Require</b> ...", "url": "https://cacm.acm.org/magazines/2021/4/251365-the-impossibility-of-fairness/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2021/4/251365-the-impossibility-of-<b>fairness</b>", "snippet": "When one believes that demographic <b>groups</b> are, on the whole, fundamentally similar, group <b>fairness</b> mechanisms successfully guarantee the top-level goal of non-discrimination: similar <b>groups</b> receiving similar treatment. Alternatively, one <b>can</b> assume the observed data generally reflects the true underlying reality about differences between <b>people</b>. These worldviews are in conflict; a single <b>algorithm</b> cannot satisfy either definition of <b>fairness</b> under both worldviews. Thus, researchers and ...", "dateLastCrawled": "2022-02-03T00:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Perception of <b>fairness</b> in algorithmic decisions: Future developers ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "snippet": "<b>Different</b> variables\u2014such as the education level and favorable outcome, as well as development procedures of the system\u2014have also been proven to affect the perception of algorithmic <b>fairness</b>. 27 In particular, <b>people</b> rate the <b>algorithm</b> as more fair when the decision is in their favor, irrespective of whether it appears to be biased toward certain social <b>groups</b>.", "dateLastCrawled": "2022-01-30T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Fairness-aware Machine Learning: Practical Challenges and</b> Lessons ...", "url": "https://www.slideshare.net/KrishnaramKenthapadi/fairnessaware-machine-learning-practical-challenges-and-lessons-learned-kdd-2019-tutorial", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/KrishnaramKenthapadi/<b>fairness</b>aware-machine-learning...", "snippet": "<b>Fairness</b> AI systems should treat everyone fairly and avoid affecting similarly situated <b>groups</b> <b>of people</b> in <b>different</b> ways Key considerations 1. Understand the scope, spirit, and potential uses of the AI system 2. Attract a diverse pool of talent 3. Put processes and tools in place to identify bias in datasets and machine learning algorithms 4. Leverage human review and domain expertise 5. Research and employ best practices, analytical techniques, and tools 41. Sources of Data Biases in ML ...", "dateLastCrawled": "2022-01-21T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Perception of <b>fairness</b> in algorithmic decisions: Future developers ...", "url": "https://www.cell.com/patterns/fulltext/S2666-3899(21)00247-6", "isFamilyFriendly": true, "displayUrl": "https://www.cell.com/patterns/fulltext/S2666-3899(21)00247-6", "snippet": "In particular, <b>people</b> rate the <b>algorithm</b> as more fair when the decision is in their favor, irrespective of whether it appears to be biased toward certain social <b>groups</b>. In the same vein, Pierson showed that there are gender differences in perceptions of algorithmic <b>fairness</b>, while demographic differences contribute to the variability of opinions on <b>fairness</b>.", "dateLastCrawled": "2021-11-22T17:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness</b> metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "Depending on the <b>fairness</b> <b>metric</b> that is considered most useful for a given scenario, this comparison <b>can</b> help a recommender system developer to select the appropriate <b>algorithm</b> while achieving the best <b>fairness</b> in recommendations. On the other hand, these results reflect the complexity of considering <b>different</b> types of <b>fairness</b> definitions and mitigation strategies, and also underline that achieving a good result in one <b>fairness</b> <b>metric</b> (e.g., ItemItem approaches lead to lower Value ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness definitions explained</b> - researchgate.net", "url": "https://www.researchgate.net/publication/326051396_Fairness_definitions_explained", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/326051396_<b>Fairness_definitions_explained</b>", "snippet": "<b>Fairness</b> metrics are used to statistically evaluate notions of <b>fairness</b> in classifier performance, where certain metrics <b>can</b> reflect <b>different</b> definitions of <b>fairness</b> [32]. In previous work by ...", "dateLastCrawled": "2021-11-04T21:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Evaluating <b>Fairness</b> Using Permutation Tests | DeepAI", "url": "https://deepai.org/publication/evaluating-fairness-using-permutation-tests", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/evaluating-<b>fairness</b>-using-permutation-tests", "snippet": "We propose a permutation testing methodology that performs a hypothesis test that a model is fair across two <b>groups</b> with respect to any given <b>metric</b>. There are increasingly many notions of <b>fairness</b> that <b>can</b> speak to <b>different</b> aspects of model <b>fairness</b>. Our aim is to provide a flexible framework that empowers practitioners to identify significant biases in any <b>metric</b> they wish to study. We provide a formal testing mechanism as well as extensive experiments to show how this method works in ...", "dateLastCrawled": "2022-01-21T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The (Im)possibility of <b>Fairness: Different Value Systems Require</b> ...", "url": "https://cacm.acm.org/magazines/2021/4/251365-the-impossibility-of-fairness/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2021/4/251365-the-impossibility-of-<b>fairness</b>", "snippet": "When one believes that demographic <b>groups</b> are, on the whole, fundamentally similar, group <b>fairness</b> mechanisms successfully guarantee the top-level goal of non-discrimination: similar <b>groups</b> receiving similar treatment. Alternatively, one <b>can</b> assume the observed data generally reflects the true underlying reality about differences between <b>people</b>. These worldviews are in conflict; a single <b>algorithm</b> cannot satisfy either definition of <b>fairness</b> under both worldviews. Thus, researchers and ...", "dateLastCrawled": "2022-02-03T00:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Algorithmic Fairness: Choices, Assumptions, and Definitions</b> | Annual ...", "url": "https://www.annualreviews.org/doi/full/10.1146/annurev-statistics-042720-125902", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/full/10.1146/annurev-statistics-042720-125902", "snippet": "More generally, we could define a similarity <b>metric</b> between <b>people</b> that is aware of the sensitive variables, motivating the next flavor of <b>fairness</b> definitions (Dwork et al. 2012). 4.4. Impossibilities . Although each flavor of <b>fairness</b> definition presented in this section formalizes an intuitive notion of <b>fairness</b>, these definitions are not mathematically or morally compatible in general. In this section, we review several impossibility results about definitions of <b>fairness</b>, providing ...", "dateLastCrawled": "2022-02-02T18:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Perception of <b>fairness</b> in algorithmic decisions: Future developers ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "snippet": "<b>Different</b> variables\u2014such as the education level and favorable outcome, as well as development procedures of the system\u2014have also been proven to affect the perception of algorithmic <b>fairness</b>. 27 In particular, <b>people</b> rate the <b>algorithm</b> as more fair when the decision is in their favor, irrespective of whether it appears to be biased toward certain social <b>groups</b>.", "dateLastCrawled": "2022-01-30T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "While avoiding disparities on this <b>metric</b> focuses on the reasonable goal of <b>treating</b> <b>different</b> <b>groups</b> similarly in terms of classification errors, we may also want to directly consider two subsets within each group: (1) the set of families not receiving the subsidy, and (2) the set of families who would benefit from receiving the subsidy. We take a closer look at each of these cases below. 11.3.5.3 False Omission Rate. The False Omission Rate (\\(FOR\\)) focuses specifically on <b>people</b> on whom ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Machine learning and algorithmic <b>fairness</b> in public and population ...", "url": "https://www.nature.com/articles/s42256-021-00373-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00373-4", "snippet": "Even if successful in ensuring that the risk is fair using a group <b>metric</b> such as demographic parity (<b>equal</b> decision rates across <b>groups</b> regardless of outcome) 94 with respect to racial identity ...", "dateLastCrawled": "2022-01-30T16:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Fairness-aware Machine Learning: Practical Challenges and</b> Lessons ...", "url": "https://www.slideshare.net/KrishnaramKenthapadi/fairnessaware-machine-learning-practical-challenges-and-lessons-learned-kdd-2019-tutorial", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/KrishnaramKenthapadi/<b>fairness</b>aware-machine-learning...", "snippet": "<b>Fairness</b> AI systems should treat everyone fairly and avoid affecting similarly situated <b>groups</b> <b>of people</b> in <b>different</b> ways Key considerations 1. Understand the scope, spirit, and potential uses of the AI system 2. Attract a diverse pool of talent 3. Put processes and tools in place to identify bias in datasets and machine learning algorithms 4. Leverage human review and domain expertise 5. Research and employ best practices, analytical techniques, and tools 41. Sources of Data Biases in ML ...", "dateLastCrawled": "2022-01-21T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "1. <b>Fairness</b>, Technology, and the Real World - <b>Practical Fairness</b> [Book]", "url": "https://www.oreilly.com/library/view/practical-fairness/9781492075721/ch01.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/<b>practical-fairness</b>/9781492075721/ch01.html", "snippet": "That\u2019s why we write ML algorithms not so we <b>can</b> treat <b>people</b> equally but rather so we <b>can</b> treat them equitably\u2014that is, according to their merit on a <b>metric</b> specific to a given task or purpose. For example, most <b>people</b> like that we earn <b>different</b> incomes for <b>different</b> kinds of jobs. Likewise, we don\u2019t even want all children to be treated equally. If a child has special needs, such as the need for a speech therapist or additional medical treatment, we\u2019d like to give that child extra ...", "dateLastCrawled": "2021-12-17T10:20:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness</b> Metrics - Data Science, <b>Machine</b> <b>Learning</b>, AI", "url": "https://vitalflux.com/fairness-metrics-ml-model-sensitivity-bias-detection/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/<b>fairness-metrics-ml-model-sensitivity</b>-bias-detection", "snippet": "There are many different ways in which <b>machine</b> <b>learning</b> (ML) models\u2019 <b>fairness</b> could be determined. Some of them are statistical parity, the relative significance of features, model sensitivity etc. In this post, you would learn about how model sensitivity could be used to determine model <b>fairness</b> or bias of model towards the privileged or unprivileged group.The following are some of the topics covered in this post:", "dateLastCrawled": "2022-01-20T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A New <b>Metric</b> for Quantifying <b>Machine</b> <b>Learning</b> <b>Fairness</b> in Healthcare ...", "url": "https://towardsdatascience.com/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare-closedloop-ai-fc07b9c83487", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-new-<b>metric</b>-for-quantifying-<b>machine</b>-<b>learning</b>-<b>fairness</b>...", "snippet": "The <b>analogy</b> would be the difference between a Pearson correlation or a residual sum of squared errors in regression. While both quantify the models performance, the former is significantly easier to understand and explain; unsurprisingly, it is the <b>metric</b> most individuals use to describe a regression model. In order to achieve this effect, many <b>fairness</b> metrics are presented as the quotient of a protected subgroup to a base subgroup[2]. As the goal of healthcare is to deliver interventions ...", "dateLastCrawled": "2022-01-17T14:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "The case for a single <b>fairness</b> <b>metric</b>. Having a single <b>metric</b> to quantify what is fair or not seems to be a goal for many AI scholars in the field, as well as many of industry practitioners. After all, while methods might vary, the foundation of <b>machine</b> <b>learning</b> hinges on maximising or minimising clear objectives 3. A business problem can then be translated to a mathematical / statistical / computational problem and different methods can be compared against each other based on how far the ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A New <b>Metric</b> for Quantifying <b>Machine</b> <b>Learning</b> <b>Fairness</b> in Healthcare", "url": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.closedloop.ai/post/a-new-<b>metric</b>-for-quantifying-<b>machine</b>-<b>learning</b>-<b>fairness</b>...", "snippet": "A New <b>Metric</b> for Quantifying <b>Machine</b> <b>Learning</b> <b>Fairness</b> in Healthcare. Joseph Gartner. March 2, 2020. Background . Several recent, high profile cases of unfair AI algorithms have highlighted the vital need to address bias early in the development of any AI system. For the most part, bias does not come into algorithms due to malicious intent by the individual creating the algorithm. Bias comes from a lack of diligence in ensuring that the AI system is fair for everyone. In order to combat bias ...", "dateLastCrawled": "2022-02-01T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "RStudio AI Blog: Starting to think about AI <b>Fairness</b>", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-<b>fairness</b>", "snippet": "Papers on <b>fairness</b> in <b>machine</b> <b>learning</b>, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about <b>fairness</b> as it might apply to an ML process at hand, common language \u2013 and common sense \u2013 will do just fine. If, after analyzing your use case, you judge that the more technical results are relevant to the process in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Classification - <b>Fairness</b> and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>measure and mismeasure of fairness: a critical review</b> of fair ...", "url": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-<b>fairness</b>", "snippet": "One premise of many models of <b>fairness</b> in <b>machine</b> <b>learning</b> is that you can measure (\u2018prove\u2019) <b>fairness</b> of a <b>machine</b> <b>learning</b> model from within the system \u2013 i.e. from properties of the model itself and perhaps the data it is trained on. Beyond the questions of whether any one model of <b>fairness</b> is better or worse than another, I\u2019m coming to the realisation that this doesn\u2019t hold. To show that a <b>machine</b> <b>learning</b> model is fair, you need information from", "dateLastCrawled": "2022-01-30T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fairness</b> through awareness", "url": "https://www.cs.toronto.edu/~toni/Papers/awareness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~toni/Papers/awareness.pdf", "snippet": "based <b>fairness</b>, we assume a distance <b>metric</b> that de nes the similarity between the individuals. This is the source of \\awareness&quot; in the title of this paper. We formalize this guiding principle as a Lipschitz condition on the classi er. In our approach a classi er is a randomized mapping from individuals to outcomes, or equivalently, a mapping from individuals to distributions over outcomes. The Lipschitz condition requires that any two individuals x;ythat are at distance d(x;y) 2[0;1] map ...", "dateLastCrawled": "2021-12-06T13:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An evaluation of scanpath-comparison and <b>machine</b>-<b>learning</b> ...", "url": "https://link.springer.com/article/10.3758/s13428-016-0788-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.3758/s13428-016-0788-z", "snippet": "In recent years, eyetracking has begun to be used to study the dynamics of <b>analogy</b> making. Numerous scanpath-comparison algorithms and <b>machine</b>-<b>learning</b> techniques are available that can be applied to the raw eyetracking data. We show how scanpath-comparison algorithms, combined with multidimensional scaling and a classification algorithm, can be used to resolve an outstanding question in <b>analogy</b> making\u2014namely, whether or not children\u2019s and adults\u2019 strategies in solving <b>analogy</b> problems ...", "dateLastCrawled": "2021-11-05T03:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Fairness</b> Through Awareness", "url": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "snippet": "<b>Fairness</b> Through Awareness Cynthia Dwork Moritz Hardty Toniann Pitassiz Omer Reingoldx Richard Zemel{November 29, 2011 Abstract We study <b>fairness</b> in classi\ufb01cation, where individuals are classi\ufb01ed, e.g., admitted to a uni-versity, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classi\ufb01er (the university). The main conceptual contribution of this paper is a framework for fair classi\ufb01cation ...", "dateLastCrawled": "2022-01-29T03:48:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "FairFed: Enabling Group Fairness in Federated <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/fairfed-enabling-group-fairness-in-federated-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fairfed-enabling-group-fairness-in-federated-<b>learning</b>", "snippet": "As <b>machine</b> <b>learning</b> becomes increasingly incorporated in crucial decision-making scenarios such as healthcare, recruitment, and loan assessment, there have been increasing concerns about the privacy and fairness of such systems. Federated <b>learning</b> has been viewed as a promising solution for collaboratively <b>learning</b> <b>machine</b> <b>learning</b> models among multiple parties while maintaining the privacy of their local data.", "dateLastCrawled": "2022-01-12T21:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\u7ffb\u8a33\u7d50\u679c: FairFed: Enabling Group Fairness in Federated <b>Learning</b>", "url": "https://fugumt.com/fugumt/paper/translated/2110.00857.pdf.html", "isFamilyFriendly": true, "displayUrl": "https://fugumt.com/fugumt/paper/translated/2110.00857.pdf.html", "snippet": "As <b>machine</b> <b>learning</b> becomes increasingly incorporated in crucial decision-making scenarios such as healthcare, recruitment, and loan assessment, there have been increasing concerns about the privacy and fairness of such systems. Federated <b>learning</b> has been viewed as a promising solution for collaboratively <b>learning</b> <b>machine</b> <b>learning</b> models among multiple parties while maintaining the privacy of their local data. However, federated <b>learning</b> also poses new challenges in mitigating the potential ...", "dateLastCrawled": "2021-10-06T05:45:00.0000000Z", "language": "ja", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(fairness metric)  is like +(measuring how equal an algorithm is treating different groups of people)", "+(fairness metric) is similar to +(measuring how equal an algorithm is treating different groups of people)", "+(fairness metric) can be thought of as +(measuring how equal an algorithm is treating different groups of people)", "+(fairness metric) can be compared to +(measuring how equal an algorithm is treating different groups of people)", "machine learning +(fairness metric AND analogy)", "machine learning +(\"fairness metric is like\")", "machine learning +(\"fairness metric is similar\")", "machine learning +(\"just as fairness metric\")", "machine learning +(\"fairness metric can be thought of as\")", "machine learning +(\"fairness metric can be compared to\")"]}