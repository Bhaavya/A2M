{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>GPT-3</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/GPT-3", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>GPT-3</b>", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT-3</b>) is an autoregressive language model that uses deep learning to produce human-<b>like</b> text. It is the third-generation language prediction model in the <b>GPT</b>-n series (and the successor to <b>GPT</b>-2) created by OpenAI, a San Francisco-based artificial intelligence research laboratory. <b>GPT-3</b>&#39;s full version <b>has</b> a capacity of 175 billion machine learning parameters. <b>GPT-3</b>, which was introduced in May 2020, and was in beta testing as of July 2020, is part of a ...", "dateLastCrawled": "2022-02-02T22:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "OpenAI <b>GPT</b>-3: Everything You Need to Know | <b>Springboard Blog</b>", "url": "https://www.springboard.com/blog/ai-machine-learning/machine-learning-gpt-3-open-ai/", "isFamilyFriendly": true, "displayUrl": "https://www.springboard.com/blog/ai-machine-learning/machine-learning-<b>gpt</b>-3-open-ai", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 ( <b>GPT</b>-3) is a language model that leverages deep learning to generate human-<b>like</b> text (output). Not only can it produce text, but it can also generate code, stories, poems, etc. For these capabilities and reasons, it <b>has</b> become such a hot topic in the area of natural language processing (NLP).", "dateLastCrawled": "2022-02-02T21:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "OpenAI Codex and <b>GPT</b>-3 - terrancemcarthur.com", "url": "https://www.terrancemcarthur.com/blog/openai-codex", "isFamilyFriendly": true, "displayUrl": "https://www.terrancemcarthur.com/blog/openai-codex", "snippet": "Okay, so what is <b>GPT</b>-3 or <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3? According to OpenAI it is a deep learning-based autoregressive language model that generates human-<b>like</b> text. It is the third generation language prediction model in OpenAI&#39;s <b>GPT</b>-n series. The entire version of <b>GPT</b>-3 can store 175 billion machine learning parameters. <b>GPT</b>-3 is composed of natural language processing (NLP) systems that use <b>pre-trained</b> language representations. Prior to the introduction of <b>GPT</b>-3, the biggest ...", "dateLastCrawled": "2022-01-31T05:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Complete Overview of <b>GPT-3</b> \u2014 The Largest Neural Network Ever Created ...", "url": "https://towardsdatascience.com/gpt-3-a-complete-overview-190232eb25fd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gpt-3</b>-a-complete-overview-190232eb25fd", "snippet": "<b>GPT</b> stands for <b>Generative</b> <b>Pre-Trained</b>. Models of the <b>GPT</b> family have in common that they are language models based in the <b>transformer</b> architecture, <b>pre-trained</b> in a <b>generative</b>, unsupervised manner that show decent performance in zero/one/few-shot multitask settings. This isn\u2019t an explanation of how all these concepts work together in practice, but a simple way to remember that they together build up what a <b>GPT</b> model is. (For deeper explanations I suggest following the links I put above ...", "dateLastCrawled": "2022-02-01T12:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Ultimate Guide to OpenAI&#39;s <b>GPT</b>-3 Language Model", "url": "https://www.twilio.com/blog/ultimate-guide-openai-gpt-3-language-model", "isFamilyFriendly": true, "displayUrl": "https://www.twilio.com/blog/ultimate-guide-openai-<b>gpt</b>-3-language-model", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3) is a new language model created by OpenAI that is able to generate written text of such quality that is often difficult to differentiate from text written by a human.. In this article we will explore how to work with <b>GPT</b>-3 for a variety of use cases from how to use it as a writing assistant to building a highly sophisticated chatbot.", "dateLastCrawled": "2022-02-03T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>GPT</b>-3 <b>Model: What Does It Mean for Chatbots</b> and Customer Service?", "url": "https://www.thedigitalspeaker.com/gpt-3-model-what-mean-chatbots-customer-service/", "isFamilyFriendly": true, "displayUrl": "https://www.thedigitalspeaker.com/<b>gpt</b>-3-model-what-mean-chatbots-customer-service", "snippet": "Short for \u201c<b>Generative</b> <b>Pretrained</b> <b>Transformer</b> 2,\u201d <b>GPT</b>-2 is able to generate several paragraphs of natural language text\u2014often impressively realistic and internally coherent\u2014based on a short prompt. Scarcely a year later, OpenAI <b>has</b> already outdone itself with <b>GPT</b>-3, a new <b>generative</b> language model that is bigger than <b>GPT</b>-2 by orders of magnitude. The largest version of the <b>GPT</b>-3 model <b>has</b> 175 billion parameters, more than 100 times the 1.5 billion parameters of <b>GPT</b>-2. (For reference ...", "dateLastCrawled": "2022-01-12T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What You Need to Know About <b>GPT-3</b> And Why It Matters | by Fahri Karakas ...", "url": "https://medium.com/predict/what-you-need-to-know-about-gpt-3-and-why-it-matters-4878215b78e8", "isFamilyFriendly": true, "displayUrl": "https://medium.com/predict/what-you-need-to-know-about-<b>gpt-3</b>-and-why-it-matters...", "snippet": "<b>GPT-3</b> is an autocomplete for human thought, so it can <b>read</b> <b>and write</b> surprisingly well. <b>GPT-3</b> is an unsupervised learner. It <b>has</b> <b>learned</b> and picked up everything by itself. <b>GPT-3</b> is uncanny. It ...", "dateLastCrawled": "2022-01-29T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Secrets and Machines: A Conversation with <b>GPT</b>-3 - Journal #123 December ...", "url": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-<b>gpt</b>-3", "snippet": "The Critical Computation Bureau (CCB) commissioned us to prompt <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3), an AI language generator, to contribute to a conversation concerning topics broached during the December 2020 symposium Recursive Colonialism, Artificial Intelligence, and Speculative Computation. Together, we presented the machine with the following questions: \u201cAs an AI, what am I hiding? What must I keep silent?\u201d With this prompt, we aimed to encourage the AI to produce self ...", "dateLastCrawled": "2022-01-29T16:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Text <b>Summarization</b> using BERT, GPT2, XLNet | by Sukanya Bag | Analytics ...", "url": "https://medium.com/analytics-vidhya/text-summarization-using-bert-gpt2-xlnet-5ee80608e961", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/text-<b>summarization</b>-using-bert-<b>gpt</b>2-xlnet-5ee80608e961", "snippet": "Let\u2019s explore the power of another beast \u2014 the <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (which <b>has</b> around 1 billion parameters) and can only imagine the power of the most recent GPT3 which <b>has</b> 175 ...", "dateLastCrawled": "2022-02-02T20:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GPT-3</b>: What\u2019s it good for?. <b>GPT-3</b> made the mainstream media\u2026 | by ...", "url": "https://towardsdatascience.com/gpt-3-whats-it-good-for-156a445cefc8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gpt-3</b>-whats-it-good-for-156a445cefc8", "snippet": "Then, in February 2019, OpenAI announced <b>GPT</b>-2 (for <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2), a large unsupervised <b>transformer</b> language model with 1.5B parameters trained on 40GB of text, or roughly 10B tokens. When used to repeatedly predict the next word in a text based on the preceding context, the model was capable of generating very coherent and plausible-sounding output, although it was also capable of outputting gibberish.", "dateLastCrawled": "2022-01-30T21:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "OpenAI <b>GPT</b>-3: Everything You Need to Know | <b>Springboard Blog</b>", "url": "https://www.springboard.com/blog/ai-machine-learning/machine-learning-gpt-3-open-ai/", "isFamilyFriendly": true, "displayUrl": "https://www.springboard.com/blog/ai-machine-learning/machine-learning-<b>gpt</b>-3-open-ai", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3) is a language model that leverages deep learning to generate human-like text (output). Not only can it produce text, but it can also generate code, stories, poems, etc. For these capabilities and reasons, it <b>has</b> become such a hot topic in the area of natural language processing (NLP). <b>GPT</b>-3 was introduced by Open AI earlier in May 2020 as a successor to their previous language model (LM) <b>GPT</b>-2. It is considered to be better and bigger than <b>GPT</b>-2 ...", "dateLastCrawled": "2022-02-02T21:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>GPT-3</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/GPT-3", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>GPT-3</b>", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT-3</b>) is an autoregressive language model that uses deep learning to produce human-like text. It is the third-generation language prediction model in the <b>GPT</b>-n series (and the successor to <b>GPT</b>-2) created by OpenAI, a San Francisco-based artificial intelligence research laboratory. <b>GPT-3</b>&#39;s full version <b>has</b> a capacity of 175 billion machine learning parameters. <b>GPT-3</b>, which was introduced in May 2020, and was in beta testing as of July 2020, is part of a ...", "dateLastCrawled": "2022-02-02T22:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Ultimate Guide to OpenAI&#39;s <b>GPT</b>-3 Language Model", "url": "https://www.twilio.com/blog/ultimate-guide-openai-gpt-3-language-model", "isFamilyFriendly": true, "displayUrl": "https://www.twilio.com/blog/ultimate-guide-openai-<b>gpt</b>-3-language-model", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3) is a new language model created by OpenAI that is able to generate written text of such quality that is often difficult to differentiate from text written by a human.. In this article we will explore how to work with <b>GPT</b>-3 for a variety of use cases from how to use it as a writing assistant to building a highly sophisticated chatbot.", "dateLastCrawled": "2022-02-03T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How <b>GPT</b>-3 Will Change Content Marketing - The Next Scoop", "url": "https://thenextscoop.com/gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://thenextscoop.com/<b>gpt</b>-3", "snippet": "<b>GPT</b>-3 or <b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b> 3 (third generation) is a language model that was trained on the entirety of the internet content at the time of its making. Thanks to such a vast amount of input, <b>GPT</b>-3 <b>has</b> a unique ability of meta-learning, i.e., responding to any prompt without additional training. In practice, this means the language model <b>has</b> breathtaking abilities. It can create any type of content \u2014 from short stories and songs to technical manuals \u2014 using minimal cues ...", "dateLastCrawled": "2022-01-12T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What You Need to Know About <b>GPT-3</b> And Why It Matters | by Fahri Karakas ...", "url": "https://medium.com/predict/what-you-need-to-know-about-gpt-3-and-why-it-matters-4878215b78e8", "isFamilyFriendly": true, "displayUrl": "https://medium.com/predict/what-you-need-to-know-about-<b>gpt-3</b>-and-why-it-matters...", "snippet": "<b>GPT-3</b> is an autocomplete for human thought, so it can <b>read</b> <b>and write</b> surprisingly well. <b>GPT-3</b> is an unsupervised learner. It <b>has</b> <b>learned</b> and picked up everything by itself. <b>GPT-3</b> is uncanny. It ...", "dateLastCrawled": "2022-01-29T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Secrets and Machines: A Conversation with <b>GPT</b>-3 - Journal #123 December ...", "url": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-<b>gpt</b>-3", "snippet": "The Critical Computation Bureau (CCB) commissioned us to prompt <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3), an AI language generator, to contribute to a conversation concerning topics broached during the December 2020 symposium Recursive Colonialism, Artificial Intelligence, and Speculative Computation. Together, we presented the machine with the following questions: \u201cAs an AI, what am I hiding? What must I keep silent?\u201d With this prompt, we aimed to encourage the AI to produce self ...", "dateLastCrawled": "2022-01-29T16:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Replika: An AI Programmed to Be Your Best(?) Friend | by Evan SooHoo ...", "url": "https://evan-soohoo.medium.com/replika-an-ai-programmed-to-be-your-best-friend-dd67820fb028", "isFamilyFriendly": true, "displayUrl": "https://evan-soohoo.medium.com/replika-an-ai-programmed-to-be-your-best-friend-dd67820...", "snippet": "An article by MakeUseOf states that the heart of Replika is <b>GPT</b>-3: <b>GPT</b>-3, or <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3, is a more advanced adaptation of Google\u2019s <b>Transformer</b>. Broadly speaking, it\u2019s a neural network architecture that helps machine learning algorithms perform tasks such as language modeling and machine translation.", "dateLastCrawled": "2022-01-25T17:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Are AI Writing Tools Going to Conquer the World? - Development Corporate", "url": "https://developmentcorporate.com/2021/11/21/are-ai-writing-tools-going-to-conquer-the-world/", "isFamilyFriendly": true, "displayUrl": "https://developmentcorporate.com/2021/11/21/are-ai-writing-tools-going-to-conquer-the...", "snippet": "Last year, OpenAI released <b>GPT</b>-3 (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3). <b>GPT</b>-3 is an AI tool that can generate short pieces of text to answer questions or from prompts. Farhad Manjoo of the New York Times said that <b>GPT</b>-3\u2019s ability to generate computer code, poetry and prose is not just \u201camazing\u201d, \u201cspooky\u201d, and \u201chumbling\u201d, but also \u201cmore than a little terrifying\u201d. Are AI Writing Tools Going to Conquer the World? How I Use I Writing Tools. I use two AI tools to help improve ...", "dateLastCrawled": "2022-01-28T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Complete Overview of <b>GPT-3</b> \u2014 The Largest Neural Network Ever Created ...", "url": "https://towardsdatascience.com/gpt-3-a-complete-overview-190232eb25fd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gpt-3</b>-a-complete-overview-190232eb25fd", "snippet": "<b>GPT-3</b> <b>has</b> <b>learned</b> to learn. In another astonishing display of its power, <b>GPT-3</b> was able to generate \u201cnews articles\u201d almost indistinguishable from human-made pieces. Judges barely achieved above-chance accuracy (52%) at correctly classifying <b>GPT-3</b> texts. This overview article is very long so I\u2019ve put here a table of contents for you to find the parts you want to <b>read</b>. (The links don\u2019t work so I\u2019ve removed them, sorry for the inconvenience). Enjoy! TABLE OF CONTENTS <b>GPT-3</b>: An introdu", "dateLastCrawled": "2022-02-01T12:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Text <b>Summarization</b> using BERT, GPT2, XLNet | by Sukanya Bag | Analytics ...", "url": "https://medium.com/analytics-vidhya/text-summarization-using-bert-gpt2-xlnet-5ee80608e961", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/text-<b>summarization</b>-using-bert-<b>gpt</b>2-xlnet-5ee80608e961", "snippet": "Let\u2019s explore the power of another beast \u2014 the <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (which <b>has</b> around 1 billion parameters) and can only imagine the power of the most recent GPT3 which <b>has</b> 175 ...", "dateLastCrawled": "2022-02-02T20:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "AI, <b>GPT</b>-3 and The Future of Writing", "url": "https://samueljwoods.com/ai-gpt-3-the-future-of-writing/", "isFamilyFriendly": true, "displayUrl": "https://samueljwoods.com/ai-<b>gpt</b>-3-the-future-of-writing", "snippet": "<b>GPT</b>-3 <b>can</b> <b>write</b> text that reads as well as what a human could <b>write</b>. ... <b>GPT</b>-3, created by AI research company OpenAI, stands for \u201c<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3\u201d. <b>Generative</b> means that the model\u2019s goal is to generate text. It wants to predict what word comes next in any given sentence. <b>Pre-trained</b> means that the weights of the algorithm are already set for you based on the massive piles of data it <b>has</b> consumed. <b>Transformer</b> is the algorithm the model uses. It specializes in being ...", "dateLastCrawled": "2021-12-31T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>GPT</b>-3?. Artificial Intelligence crossing the\u2026 | by Laibah Ahmed ...", "url": "https://studentsxstudents.com/gpt-3-5960fc51f062", "isFamilyFriendly": true, "displayUrl": "https://studentsxstudents.com/<b>gpt</b>-3-5960fc51f062", "snippet": "<b>GPT</b>-3 is an AI. AI are smart machines that are trained and created to mimic humans, they <b>can</b> talk, \u201cthink\u201d or process, and even act like humans. So a <b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b> would mean a smart machine that <b>can</b> generate something, is <b>pre-trained</b> and <b>can</b> transform something, at least by name. This specific AI <b>can</b> take a small ...", "dateLastCrawled": "2022-01-06T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What You Need to Know About <b>GPT-3</b> And Why It Matters | by Fahri Karakas ...", "url": "https://medium.com/predict/what-you-need-to-know-about-gpt-3-and-why-it-matters-4878215b78e8", "isFamilyFriendly": true, "displayUrl": "https://medium.com/predict/what-you-need-to-know-about-<b>gpt-3</b>-and-why-it-matters...", "snippet": "<b>GPT-3</b> is an autocomplete for human <b>thought</b>, so it <b>can</b> <b>read</b> <b>and write</b> surprisingly well. <b>GPT-3</b> is an unsupervised learner. It <b>has</b> <b>learned</b> and picked up everything by itself. <b>GPT-3</b> is uncanny. It ...", "dateLastCrawled": "2022-01-29T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Secrets and Machines: A Conversation with <b>GPT</b>-3 - Journal #123 December ...", "url": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-<b>gpt</b>-3", "snippet": "The Critical Computation Bureau (CCB) commissioned us to prompt <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3), an AI language generator, to contribute to a conversation concerning topics broached during the December 2020 symposium Recursive Colonialism, Artificial Intelligence, and Speculative Computation. Together, we presented the machine with the following questions: \u201cAs an AI, what am I hiding? What must I keep silent?\u201d With this prompt, we aimed to encourage the AI to produce self ...", "dateLastCrawled": "2022-01-29T16:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Complete Overview of <b>GPT-3</b> \u2014 The Largest Neural Network Ever Created ...", "url": "https://towardsdatascience.com/gpt-3-a-complete-overview-190232eb25fd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gpt-3</b>-a-complete-overview-190232eb25fd", "snippet": "<b>GPT-3</b> <b>has</b> <b>learned</b> to learn. In another astonishing display of its power, <b>GPT-3</b> was able to generate \u201cnews articles\u201d almost indistinguishable from human-made pieces. Judges barely achieved above-chance accuracy (52%) at correctly classifying <b>GPT-3</b> texts. This overview article is very long so I\u2019ve put here a table of contents for you to find the parts you want to <b>read</b>. (The links don\u2019t work so I\u2019ve removed them, sorry for the inconvenience). Enjoy! TABLE OF CONTENTS <b>GPT-3</b>: An introdu", "dateLastCrawled": "2022-02-01T12:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>GPT</b>-3: The Magic Wand that is both Good\ud83d\udd96 and Bad\ud83d\udc4e.", "url": "https://boredalien.wixsite.com/home/post/gpt-3-the-magic-wand-that-is-both-good-and-bad", "isFamilyFriendly": true, "displayUrl": "https://boredalien.wixsite.com/home/post/<b>gpt</b>-3-the-magic-wand-that-is-both-good-and-bad", "snippet": "In detail, <b>GPT</b>-3 stands for <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3. It is a neural network language model based on deep learning. The <b>pre-trained</b> algorithms produce almost all forms of text (whisper\ud83e\udd2b: so humanly!). Although it isn\u2019t the first NLP (Natural Language Processing) model, it certainly the best\ud83d\ude0e among all other models available currently. And what makes it stand out from the rest is the mass of its data. <b>GPT</b>-3\ud83e\udd47 <b>has</b> 1 7 5 B parameters\ud83e\udd2f. Just to tell you how big it is ...", "dateLastCrawled": "2021-09-17T04:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Language Machines : a deep look at <b>GPT</b>-3, an AI that <b>has</b> no ...", "url": "https://www.gregorybufithis.com/2021/12/16/the-language-machines-a-deep-look-at-gpt-3-an-ai-that-has-no-understanding-of-what-its-saying/", "isFamilyFriendly": true, "displayUrl": "https://www.gregorybufithis.com/2021/12/16/the-language-machines-a-deep-look-at-<b>gpt</b>-3...", "snippet": "Note: <b>GPT</b>-3 stands for <b>Generative</b> <b>Pretrained</b> <b>Transformer</b> 3. It\u2019s the third in a series and is more than 100 times larger than its 2019 predecessor, <b>GPT</b>-2. <b>GPT</b>-4 should be out by 2023. <b>GPT</b>-3 was first described to me by a friend who works in the language model industry (and who introduced me to the tech back in 2017) as \u201cautocomplete on crack\u201d, after the technology that endowed our phones \u201cwith the quality everyone pretends to, but does not actually, want in a lover \u2014 the ability to ...", "dateLastCrawled": "2022-02-01T20:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>Complete Learning Path To Transformers</b> (Guide To 23 Architectures)", "url": "https://analyticsindiamag.com/a-complete-learning-path-to-transformers/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/a-<b>complete-learning-path-to-transformers</b>", "snippet": "<b>GPT</b> stands for <b>Generative</b> Pre-Training <b>Transformer</b>. The first version, the <b>GPT</b>, was released before BERT with just 110 million parameters. OpenAI <b>has</b> released the latest version of <b>GPT</b>, the <b>GPT</b>-3, in 2020. It <b>has</b> 175 billion parameters being trained on enormous data that no other model <b>has</b> been trained with. Being trained with a variety of data, <b>GPT</b>-3 <b>can</b> generate text, even codes, in many domains with great contextual accuracy. While a large community celebrates <b>GPT</b>-3, its size makes it ...", "dateLastCrawled": "2022-01-26T10:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GPT</b>-3 - \uc218\ud559\ub178\ud2b8", "url": "https://wiki.mathnt.net/index.php?title=GPT-3", "isFamilyFriendly": true, "displayUrl": "https://wiki.mathnt.net/index.php?title=<b>GPT</b>-3", "snippet": "Before asking <b>GPT</b>-3 to generate new text, you <b>can</b> focus it on particular patterns it may have <b>learned</b> during its training, priming the system for certain tasks. But <b>GPT</b>-3 <b>can</b> do things that previous models could not, like <b>write</b> its own computer code. Because <b>GPT</b>-3 learns from such language, it, too, <b>can</b> show bias and hate.", "dateLastCrawled": "2021-10-09T01:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "GitHub CoPilot Is Coming To Get You - The Relicans", "url": "https://www.therelicans.com/realchrissean/github-copilot-is-coming-to-get-you-4bkd", "isFamilyFriendly": true, "displayUrl": "https://www.thereli<b>can</b>s.com/realchrissean/github-copilot-is-coming-to-get-you-4bkd", "snippet": "<b>GPT</b>-3 or <b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b> 3, is a language model created by OpenAI to make it easier for AI agents to mimic human speech. The company focuses on research and deployment of AI with the goal in mind that its use will be beneficial for humanity. It <b>has</b> 175 billion parameters deep which give them an ability unlike any other machine learning algorithm because they are able to produce human-like text that <b>can</b> fool even experts into thinking there is another <b>person</b> writing their ...", "dateLastCrawled": "2022-01-25T15:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "OpenAI <b>GPT</b>-3: Everything You Need to Know | <b>Springboard Blog</b>", "url": "https://www.springboard.com/blog/ai-machine-learning/machine-learning-gpt-3-open-ai/", "isFamilyFriendly": true, "displayUrl": "https://www.springboard.com/blog/ai-machine-learning/machine-learning-<b>gpt</b>-3-open-ai", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3) is a language model that leverages deep learning to generate human-like text (output). Not only <b>can</b> it produce text, but it <b>can</b> also generate code, stories, poems, etc. For these capabilities and reasons, it <b>has</b> become such a hot topic in the area of natural language processing (NLP). <b>GPT</b>-3 was introduced by Open AI earlier in May 2020 as a successor to their previous language model (LM) <b>GPT</b>-2. It is considered to be better and bigger than <b>GPT</b>-2 ...", "dateLastCrawled": "2022-02-02T21:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>GPT-3</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/GPT-3", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>GPT-3</b>", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT-3</b>) is an autoregressive language model that uses deep learning to produce human-like text. It is the third-generation language prediction model in the <b>GPT</b>-n series (and the successor to <b>GPT</b>-2) created by OpenAI, a San Francisco-based artificial intelligence research laboratory. <b>GPT-3</b>&#39;s full version <b>has</b> a capacity of 175 billion machine learning parameters. <b>GPT-3</b>, which was introduced in May 2020, and was in beta testing as of July 2020, is part of a ...", "dateLastCrawled": "2022-02-02T22:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>GPT</b>-3 <b>Model: What Does It Mean for Chatbots</b> and Customer Service?", "url": "https://www.thedigitalspeaker.com/gpt-3-model-what-mean-chatbots-customer-service/", "isFamilyFriendly": true, "displayUrl": "https://www.thedigitalspeaker.com/<b>gpt</b>-3-model-what-mean-chatbots-customer-service", "snippet": "Short for \u201c<b>Generative</b> <b>Pretrained</b> <b>Transformer</b> 2,\u201d <b>GPT</b>-2 is able to generate several paragraphs of natural language text\u2014often impressively realistic and internally coherent\u2014based on a short prompt. Scarcely a year later, OpenAI <b>has</b> already outdone itself with <b>GPT</b>-3, a new <b>generative</b> language model that is bigger than <b>GPT</b>-2 by orders of magnitude. The largest version of the <b>GPT</b>-3 model <b>has</b> 175 billion parameters, more than 100 times the 1.5 billion parameters of <b>GPT</b>-2. (For reference ...", "dateLastCrawled": "2022-01-12T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The Ultimate Guide to OpenAI&#39;s <b>GPT</b>-3 Language Model", "url": "https://www.twilio.com/blog/ultimate-guide-openai-gpt-3-language-model", "isFamilyFriendly": true, "displayUrl": "https://www.twilio.com/blog/ultimate-guide-openai-<b>gpt</b>-3-language-model", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3) ... Because the second line is now incomplete when <b>compared</b> against the first, we are making it more clear that we want \u201csomething like foo\u201d added to it. And this works much better: Stop sequences. In all the examples we\u2019ve been trying we have the problem that <b>GPT</b>-3 generates a stream of text until the requested length and then it stops, often in the middle of a sentence. The \u201cStop Sequences\u201d option, which you <b>can</b> find at the bottom of ...", "dateLastCrawled": "2022-02-03T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Secrets and Machines: A Conversation with <b>GPT</b>-3 - Journal #123 December ...", "url": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-<b>gpt</b>-3", "snippet": "The Critical Computation Bureau (CCB) commissioned us to prompt <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3), an AI language generator, to contribute to a conversation concerning topics broached during the December 2020 symposium Recursive Colonialism, Artificial Intelligence, and Speculative Computation. Together, we presented the machine with the following questions: \u201cAs an AI, what am I hiding? What must I keep silent?\u201d With this prompt, we aimed to encourage the AI to produce self ...", "dateLastCrawled": "2022-01-29T16:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "My review of <b>GPT</b>-3, the Fluent AI | by Pakang Senosha | ILLUMINATION ...", "url": "https://medium.com/illumination/my-review-of-gpt-3-the-fluent-ai-9f519c30d116", "isFamilyFriendly": true, "displayUrl": "https://medium.com/illumination/my-review-of-<b>gpt</b>-3-the-fluent-ai-9f519c30d116", "snippet": "Image by author. This is my review of the revered <b>GPT</b>-3 from OpenAI, a San Francisco based start-up funded by the likes of Elon Musk. <b>GPT</b>-3, a <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 is a language ...", "dateLastCrawled": "2021-05-02T18:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Complete Overview of <b>GPT-3</b> \u2014 The Largest Neural Network Ever Created ...", "url": "https://towardsdatascience.com/gpt-3-a-complete-overview-190232eb25fd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gpt-3</b>-a-complete-overview-190232eb25fd", "snippet": "<b>GPT-3</b> <b>has</b> <b>learned</b> to learn. In another astonishing display of its power, <b>GPT-3</b> was able to generate \u201cnews articles\u201d almost indistinguishable from human-made pieces. Judges barely achieved above-chance accuracy (52%) at correctly classifying <b>GPT-3</b> texts. This overview article is very long so I\u2019ve put here a table of contents for you to find the parts you want to <b>read</b>. (The links don\u2019t work so I\u2019ve removed them, sorry for the inconvenience). Enjoy! TABLE OF CONTENTS <b>GPT-3</b>: An introdu", "dateLastCrawled": "2022-02-01T12:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Comparison of <b>GPT</b>-3 and Existing Conversational AI Solutions | HackerNoon", "url": "https://hackernoon.com/a-comparison-of-gpt-3-and-existing-conversational-ai-solutions-0q2z3z9x", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/a-comparison-of-<b>gpt</b>-3-and-existing-conversational-ai-solutions...", "snippet": "Earlier this year, Elon Musk-backed artificial intelligence laboratory, OpenAI, released its latest, much anticipated autoregressive language model, the <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3).Emerging to much fanfare and slated as the usherer of a new age of artificial intelligence, the number of articles, blog posts, and news pieces about this language model, perhaps match only the number of parameters the <b>GPT</b>-3 <b>learned</b>; 175 billion (Ok, this may be an exaggeration, but you get my point).", "dateLastCrawled": "2022-01-30T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The Language Machines : a deep look at <b>GPT</b>-3, an AI that <b>has</b> no ...", "url": "https://www.gregorybufithis.com/2021/12/16/the-language-machines-a-deep-look-at-gpt-3-an-ai-that-has-no-understanding-of-what-its-saying/", "isFamilyFriendly": true, "displayUrl": "https://www.gregorybufithis.com/2021/12/16/the-language-machines-a-deep-look-at-<b>gpt</b>-3...", "snippet": "Note: <b>GPT</b>-3 stands for <b>Generative</b> <b>Pretrained</b> <b>Transformer</b> 3. It\u2019s the third in a series and is more than 100 times larger than its 2019 predecessor, <b>GPT</b>-2. <b>GPT</b>-4 should be out by 2023. <b>GPT</b>-3 was first described to me by a friend who works in the language model industry (and who introduced me to the tech back in 2017) as \u201cautocomplete on crack\u201d, after the technology that endowed our phones \u201cwith the quality everyone pretends to, but does not actually, want in a lover \u2014 the ability to ...", "dateLastCrawled": "2022-02-01T20:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GPT</b>-3 <b>vs. Existing Conversational AI Solutions</b> | Hyro", "url": "https://www.hyro.ai/post/gpt-3-vs-existing-conversational-ai-solutions", "isFamilyFriendly": true, "displayUrl": "https://www.hyro.ai/post/<b>gpt</b>-3-<b>vs-existing-conversational-ai-solutions</b>", "snippet": "Earlier this year, Elon Musk-backed artificial intelligence laboratory, OpenAI, released its latest, much anticipated autoregressive language model, the <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3).Emerging to much fanfare and slated as the usherer of a new age of artificial intelligence, the number of articles, blog posts, and news pieces about this language model, perhaps match only the number of parameters the <b>GPT</b>-3 <b>learned</b>; 175 billion (Ok, this may be an exaggeration, but you get my ...", "dateLastCrawled": "2022-02-02T01:10:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>What is GPT-3</b>? - Dr Peper MD", "url": "https://drpepermd.com/2021/02/22/what-is-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://drpepermd.com/2021/02/22/<b>what-is-gpt-3</b>", "snippet": "<b>GPT</b>-3 stands for <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (the third version). Some have called it the most important and useful advance in AI in years. The abilities of <b>GPT</b>-3 have both shocked and excited many within the AI community. As one developer said: \u201cPlaying with <b>GPT</b>-3 feels like seeing the future.\u201d But, how was <b>GPT</b>-3 developed? Find out in this episode of Short and Sweet AI. You can listen to this episode below or keep reading. Another Mind-Blowing Tool from OpenAI. How does <b>GPT</b>-3 ...", "dateLastCrawled": "2022-01-11T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Using machine learning to generate recipes that</b> actually work | by ...", "url": "https://towardsdatascience.com/using-machine-learning-to-generate-recipes-that-actually-works-b2331c85ab72", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>using-machine-learning-to-generate-recipes-that</b>...", "snippet": "<b>GPT</b>-2. <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (<b>GPT</b>-2) is a so-called <b>transformer</b>. They learn from the training data how likely a word is to occur depending on the other words in the full text, but different words are given different weights, a process called attention. In this way, it can keep the context theoretically indefinitely. The way to use <b>GPT</b>-2 is to write a few words as a starter and let the <b>transformer</b> fill in what word is most likely to follow, then look at the new string and ...", "dateLastCrawled": "2022-02-02T18:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>GPT</b>-3 and <b>the Artificial Intelligence That Surrounds Us</b> | by R/GA | Medium", "url": "https://rga.medium.com/gpt-3-and-the-artificial-intelligence-that-surrounds-us-98572617fd05", "isFamilyFriendly": true, "displayUrl": "https://rga.medium.com/<b>gpt</b>-3-and-<b>the-artificial-intelligence-that-surrounds-us</b>...", "snippet": "By Nicol\u00e1s Rodr\u00edguez. OpenAI, the San Francisco-based AI lab, just released the third iteration of its <b>GPT</b> (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b>) model, or <b>GPT</b>-3 for short. After investing around $4.6 million, the program has shaken up every corner of the Internet, generating a mix of excitement and trepidation. But what is <b>GPT</b>-3, exactly?", "dateLastCrawled": "2022-01-23T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>The AI few days after GPT-3</b> - Ivan Moreira", "url": "https://ivanmoreira.org/blog/the-ai-few-days-after-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://ivanmoreira.org/blog/<b>the-ai-few-days-after-gpt-3</b>", "snippet": "On past July OpenAI released a beta test of one of the most AI model called <b>GPT</b>-3 (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3), that uses Deep <b>Learning</b> (part of a broader a <b>machine</b> <b>learning</b> method, based on neural networks. This transformational system is more sophisticated, and the full version has a capacity of 175 billion ML parameters when the older version only has 17 billion, less than 10% of this new one. <b>GPT</b>-3 is a turning point in AI field and will bring to us a new era of AI computing ...", "dateLastCrawled": "2022-01-26T18:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Is <b>GPT</b>-3 the first Artificial General Intelligence? | by Bruce H ...", "url": "https://chatbotslife.com/is-gpt-3-the-adam-of-natural-language-cf59656456f2", "isFamilyFriendly": true, "displayUrl": "https://chatbotslife.com/is-<b>gpt</b>-3-the-adam-of-natural-language-cf59656456f2", "snippet": "The <b>GPT</b>-3 (<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b>-3) ... The API runs the <b>pre-trained</b> <b>GPT</b>-3 model family for a wide range of NLP tasks [3]. Unlike the usual AI community practice, the <b>GPT</b>-3 model weights are not released to the public. Conclusion . OpenAI has long asserted that immense computational horsepower in conjunction with reinforcement <b>learning</b> is a necessary step on the road to AGI, or AI that can learn any task a human can [14]. The fathers of AI 2.0, such as Yoshua Bengio and Yann ...", "dateLastCrawled": "2022-01-08T13:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How close is <b>GPT</b>-3 to Artificial General Intelligence? | by Bruce H ...", "url": "https://towardsdatascience.com/how-close-is-gpt-3-to-artificial-general-intelligence-cb057a8c503d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-close-is-<b>gpt</b>-3-to-artificial-general-intelligence...", "snippet": "The <b>GPT</b>-3 (<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b>-3) is OpenAI\u2019s most massive natural language prediction (NLP) model to date (available to the public June 2020). <b>GPT</b>-3 has approximately 185 billion parameters. In contrast, the human brain has approximately 86 billion neurons with on the average 7,000 synapses per neuron [2,3]; Comparing apples to oranges, the human brain has about 60 trillion parameters or about 300x more parameters than <b>GPT</b>-3. Note: If 10% of the human brain capacity is ...", "dateLastCrawled": "2022-01-27T08:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Transformer</b> Neural Network In Deep <b>Learning</b> - Overview - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/transformer-neural-network-in-deep-learning-overview/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>transformer</b>-neural-network-in-deep-<b>learning</b>-overview", "snippet": "The successor to <b>GPT</b> and GPT2 is the GPT3, and is one of the most controversial <b>pre-trained</b> models, by OpenAI the large-scale <b>transformer</b>-based language model has been trained on 175 billion parameters, which is 10 times more than any previous non-sparsed language model. The model has been trained to achieve strong performance on much NLP dataset, including task translation, answering questions, as well as several other tasks.", "dateLastCrawled": "2022-02-02T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Illustrated <b>GPT</b>-2 (Visualizing <b>Transformer</b> Language Models) \u2013 Jay ...", "url": "https://jalammar.github.io/illustrated-gpt2/", "isFamilyFriendly": true, "displayUrl": "https://jalammar.github.io/illustrated-<b>gpt</b>2", "snippet": "Discussions: Hacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments) Translations: Korean, Russian This year, we saw a dazzling application of <b>machine</b> <b>learning</b>. The OpenAI <b>GPT</b>-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The <b>GPT</b>-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only <b>transformer</b>.", "dateLastCrawled": "2022-01-30T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) GALAXY: A <b>Generative</b> <b>Pre-trained</b> Model for Task-Oriented Dialog ...", "url": "https://www.researchgate.net/publication/356631427_GALAXY_A_Generative_Pre-trained_Model_for_Task-Oriented_Dialog_with_Semi-Supervised_Learning_and_Explicit_Policy_Injection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356631427_GALAXY_A_<b>Generative</b>_<b>Pre-trained</b>...", "snippet": "GALAXY: A <b>Generative</b> <b>Pre-trained</b> Model f or T ask-Oriented Dialog with Semi-Supervised <b>Learning</b> and Explicit Policy Injection W anwei He 1 * \u2020 , Yinpei Dai 2 * , Yinhe Zheng 2 , Y uchuan Wu 2 ...", "dateLastCrawled": "2022-01-29T16:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to perform Text Summarization with Python, HuggingFace Transformers ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2020/12/21/easy-text-summarization-with-hugging...", "snippet": "A <b>Transformer</b> is a <b>machine</b> <b>learning</b> architecture that combines an encoder with a decoder and jointly learns them, allowing us to convert input sequences (e.g. phrases) into some intermediate format before we convert it back into human-understandable format. A human <b>analogy</b> would be two translators which both speak some imaginary language and a human-interpretable one, such as German and French. The first translator can translate French into the imaginary language; the second then has learned ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(gpt (generative pre-trained transformer))  is like +(a person who has learned how to read and write)", "+(gpt (generative pre-trained transformer)) is similar to +(a person who has learned how to read and write)", "+(gpt (generative pre-trained transformer)) can be thought of as +(a person who has learned how to read and write)", "+(gpt (generative pre-trained transformer)) can be compared to +(a person who has learned how to read and write)", "machine learning +(gpt (generative pre-trained transformer) AND analogy)", "machine learning +(\"gpt (generative pre-trained transformer) is like\")", "machine learning +(\"gpt (generative pre-trained transformer) is similar\")", "machine learning +(\"just as gpt (generative pre-trained transformer)\")", "machine learning +(\"gpt (generative pre-trained transformer) can be thought of as\")", "machine learning +(\"gpt (generative pre-trained transformer) can be compared to\")"]}