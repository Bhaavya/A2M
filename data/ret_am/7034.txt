{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "LCS graph <b>kernel</b> based on <b>Wasserstein</b> distance in longest common ...", "url": "https://www.sciencedirect.com/science/article/pii/S0165168421003182", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0165168421003182", "snippet": "To show improvement of OT framework (<b>Wasserstein</b> distance) compared to R-convolution framework, we implement our FLCS <b>kernel</b> based on the R-convolution framework, which we call FLCS-R Graph <b>Kernel</b>. The FLCS-R Graph <b>Kernel</b> on two graphs G 1 , G 2 is computed as k FLCS \u2212 R ( G 1 , G 2 ) = \u2211 x 1 \u2208 X 1 , x 2 \u2208 X 2 F sim ( x 1 , x 2 ) , where X 1 , X 2 is respective path sequence sets of G 1 , G 2 , and F sim is the path sequence similarity in Eq.", "dateLastCrawled": "2021-12-14T00:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning with a Wasserstein Loss</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1506.05439/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1506.05439", "snippet": "In this paper, we develop a <b>loss</b> function for multi-label learning that measures the <b>Wasserstein</b> distance between a prediction and the target label, with respect to a chosen metric on the output space. The <b>Wasserstein</b> distance is defined as the cost of the optimal transport plan for moving the mass in the predicted measure to match that in the target, and has been applied to a wide range of problems, including barycenter estimation [3], label propagation [4], and clustering [5].To our ...", "dateLastCrawled": "2021-12-21T13:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sinkhorn-AutoDiff: Tractable Wasserstein Learning of Generative</b> Models", "url": "https://www.researchgate.net/publication/317300051_Sinkhorn-AutoDiff_Tractable_Wasserstein_Learning_of_Generative_Models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/317300051_Sinkhorn-AutoDiff_Tractable...", "snippet": "The <b>Wasserstein</b> <b>loss</b> can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the ...", "dateLastCrawled": "2021-12-17T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How to <b>Train a Progressive Growing GAN in Keras for Synthesizing Faces</b>", "url": "https://machinelearningmastery.com/how-to-train-a-progressive-growing-gan-in-keras-for-synthesizing-faces/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-<b>train-a-progressive-growing-gan-in-keras</b>-for...", "snippet": "In this case, we will use <b>Wasserstein</b> <b>loss</b> (or WGAN <b>loss</b>) and the Adam version of stochastic gradient descent configured as is specified in the paper. The authors of the paper recommend exploring using both WGAN-GP <b>loss</b> and least squares <b>loss</b> and found that the former performed slightly better. Nevertheless, we will use <b>Wasserstein</b> <b>loss</b> as it greatly simplifies the implementation.", "dateLastCrawled": "2022-02-02T01:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A growing collection of machine learning models, algorithms, and tools ...", "url": "https://pythonawesome.com/a-growing-collection-of-machine-learning-models-algorithms-and-tools-written-exclusively-in-numpy/", "isFamilyFriendly": true, "displayUrl": "https://pythonawesome.com/a-growing-collection-of-machine-learning-models-algorithms...", "snippet": "<b>Wasserstein</b> <b>loss</b> with gradient penalty; Noise contrastive estimation <b>loss</b>; Activations ReLU; Tanh; Affine; Sigmoid; Leaky ReLU; ELU; SELU ; Exponential; Hard Sigmoid; Softplus; Models Bernoulli variational autoencoder; <b>Wasserstein</b> GAN with gradient penalty; word2vec encoder with skip-gram and CBOW architectures; Utilities col2im (MATLAB port) im2col (MATLAB port) conv1D; conv2D; deconv2D; minibatch; Tree-based models. Decision trees (CART) [Bagging] Random forests [Boosting] Gradient-boosted ...", "dateLastCrawled": "2022-01-24T13:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Improving Maximum Likelihood Estimation of Temporal Point Process via ...", "url": "https://www.ijcai.org/Proceedings/2018/0409.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/2018/0409.pdf", "snippet": "<b>Wasserstein</b> distance <b>loss</b>. The hope is that the ad-versarial <b>loss</b> can add sharpness to the smooth effect inherently caused by the MSE <b>loss</b>. The method is generic and compatible with different differentiable parametric forms of the intensity function. Empir-ical results via a variant of the Hawkes processes demonstrate its effectiveness of our method. 1 Introduction and Related Work A major line of research has been devoted to modeling event sequences, especially exploring the continuous ...", "dateLastCrawled": "2022-01-30T08:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Designing Transformers with <b>Kernel</b> Methods", "url": "https://gregoiremialon.github.io/talk/mila/mila.pdf", "isFamilyFriendly": true, "displayUrl": "https://gregoiremialon.github.io/talk/mila/mila.pdf", "snippet": "Transformers, self-attention, and <b>kernel</b> <b>smoothing</b> Self-attention as a <b>kernel</b> <b>smoothing</b>. We can rewrite self-attention: Attention(Q;K;V) i = Xn j=1 exp Q iK &gt; pj d out P n j0=1 exp Q iK&gt; pj0 d out V j 2Rd out = Xn j=1 k(Q i;K j) P n j0=1 k(Q i;K j) V j 2Rd out; with k a non-negative <b>kernel</b> function, which can be seen as a <b>kernel</b> <b>smoothing</b>. <b>Kernel</b> construction. Di erent choices for k suggest di erent transformers architectures [Tsai et al., 2019]. G. Mialon (Inria Paris) Designing ...", "dateLastCrawled": "2021-09-10T15:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Adversarial Divergences are Good Task Losses for Generative Modeling", "url": "https://gauthiergidel.github.io/papers/1708.02511.pdf", "isFamilyFriendly": true, "displayUrl": "https://gauthiergidel.github.io/papers/1708.02511.pdf", "snippet": "while the adversarial <b>Wasserstein</b> optimized in WGANs corresponds to (6) where fis a neural network. See (Liu et al.,2017) for interpretations and a review and interpreta-tion of other divergences <b>like</b> the <b>Wasserstein</b> with entropic <b>smoothing</b> (Aude et al.,2016), energy-based distances (Li et al.,2017) which can be seen as adversarial MMD, and", "dateLastCrawled": "2021-08-30T19:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] <b>Wasserstein</b> GAN math : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/mcadxa/d_wasserstein_gan_math/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/mcadxa/d_<b>wasserstein</b>_gan_math", "snippet": "Label <b>smoothing</b> = use 0.1 and 0.9 instead of 0 and 1 for label targets (can smoothen training) Don\u2019t dichotomise your data, if your output is continuous then its better to train the network to predict continuous values rather than turning it into a classification problem", "dateLastCrawled": "2021-03-28T09:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GANimation \u2014 Facial Animation</b>. Ganimation was an interesting paper ...", "url": "https://medium.com/analytics-vidhya/ganimation-facial-animation-109439fa93ec", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>ganimation-facial-animation</b>-109439fa93ec", "snippet": "2. Attention <b>Loss</b> \u2014 This is a mix of two types of regularization. The first one is a Total Variation Regularization which is more of a pixel to pixel <b>smoothing</b>. And the second is an L2 ...", "dateLastCrawled": "2021-07-17T14:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "LCS graph <b>kernel</b> based on <b>Wasserstein</b> distance in longest common ...", "url": "https://www.sciencedirect.com/science/article/pii/S0165168421003182", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0165168421003182", "snippet": "To show improvement of OT framework (<b>Wasserstein</b> distance) compared to R-convolution framework, we implement our FLCS <b>kernel</b> based on the R-convolution framework, which we call FLCS-R Graph <b>Kernel</b>. The FLCS-R Graph <b>Kernel</b> on two graphs G 1 , G 2 is computed as k FLCS \u2212 R ( G 1 , G 2 ) = \u2211 x 1 \u2208 X 1 , x 2 \u2208 X 2 F sim ( x 1 , x 2 ) , where X 1 , X 2 is respective path sequence sets of G 1 , G 2 , and F sim is the path sequence similarity in Eq.", "dateLastCrawled": "2021-12-14T00:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning with a Wasserstein Loss</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1506.05439/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1506.05439", "snippet": "Figure 2: The <b>Wasserstein</b> <b>loss</b> encourages predictions that are <b>similar</b> to ground truth, robustly to incorrect labeling of <b>similar</b> classes (see Appendix E.1). Shown is Euclidean distance between prediction and ground truth vs. (left) number of classes, averaged over different noise levels and (right) noise level, averaged over number of classes. Baseline is the multiclass logistic <b>loss</b>.", "dateLastCrawled": "2021-12-21T13:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "[PDF] <b>Wasserstein</b> <b>Loss</b> for Image Synthesis and Restoration | Semantic ...", "url": "https://www.semanticscholar.org/paper/Wasserstein-Loss-for-Image-Synthesis-and-Tartavel-Peyr%C3%A9/a67aa6da4ecb80d979df84a318cea3b5d4bec1c7", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/<b>Wasserstein</b>-<b>Loss</b>-for-Image-Synthesis-and-Tartavel...", "snippet": "The empirical distributions of linear or nonlinear image descriptors are imposed to be close to some input distributions by minimizing a <b>Wasserstein</b> <b>loss</b>, i.e., the optimal transport distance between the distributions. We advocate the use of a <b>Wasserstein</b> distance because it is robust when using discrete distributions without the need to resort <b>to kernel</b> estimators. We showcase the use of different descriptors to tackle various image processing applications. These descriptors include linear ...", "dateLastCrawled": "2022-02-03T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "LEARNING TO GENERATE <b>WASSERSTEIN</b> BARYCEN", "url": "https://openreview.net/pdf?id=2ioNazs6lvw", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=2ioNazs6lvw", "snippet": "of machine learning as a <b>loss</b> for training neural networks (Arjovsky et al., 2017), as a manifold for dictionary learning (Schmitz et al., 2018), clustering (Mi et al., 2018) and metric learning ap-plications (Heitz et al., 2019), as a way to sample an embedding (Liutkus et al., 2019) and transfer learning (Courty et al., 2014), and many other applications (see Sec. 2.3). However, despite recent progress in computational optimal transport, in many cases these applications have remained ...", "dateLastCrawled": "2021-12-09T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to <b>Train a Progressive Growing GAN in Keras for Synthesizing Faces</b>", "url": "https://machinelearningmastery.com/how-to-train-a-progressive-growing-gan-in-keras-for-synthesizing-faces/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-<b>train-a-progressive-growing-gan-in-keras</b>-for...", "snippet": "Nevertheless, we will use <b>Wasserstein</b> <b>loss</b> as it greatly simplifies the implementation. First, we must define the <b>loss</b> function as the average predicted value multiplied by the target value. The target value will be 1 for real images and -1 for fake images. This means that weight updates will seek to increase the divide between real and fake images. 1. 2. 3 # calculate <b>wasserstein</b> <b>loss</b>. def <b>wasserstein</b>_<b>loss</b> (y_true, y_pred): return backend. mean (y_true * y_pred) The functions for defining ...", "dateLastCrawled": "2022-02-02T01:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lp-WGAN: Using Lp-norm normalization to stabilize <b>Wasserstein</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0950705118304003", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705118304003", "snippet": "<b>Wasserstein</b> <b>loss</b> (in log scale) with different p and \u03b1. For each p, \u03b1 is set to be 0.01, 0.1, 1, 10 and 100. \u03b1 values could also be determined by calculating inception scores.", "dateLastCrawled": "2021-12-19T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Adversarial Divergences are Good Task Losses for Generative Modeling", "url": "https://gauthiergidel.github.io/papers/1708.02511.pdf", "isFamilyFriendly": true, "displayUrl": "https://gauthiergidel.github.io/papers/1708.02511.pdf", "snippet": "while the adversarial <b>Wasserstein</b> optimized in WGANs corresponds to (6) where fis a neural network. See (Liu et al.,2017) for interpretations and a review and interpreta-tion of other divergences like the <b>Wasserstein</b> with entropic <b>smoothing</b> (Aude et al.,2016), energy-based distances (Li et al.,2017) which can be seen as adversarial MMD, and", "dateLastCrawled": "2021-08-30T19:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Designing Transformers with <b>Kernel</b> Methods", "url": "https://gregoiremialon.github.io/talk/mila/mila.pdf", "isFamilyFriendly": true, "displayUrl": "https://gregoiremialon.github.io/talk/mila/mila.pdf", "snippet": "Transformers, self-attention, and <b>kernel</b> <b>smoothing</b> Self-attention as a <b>kernel</b> <b>smoothing</b>. We can rewrite self-attention: Attention(Q;K;V) i = Xn j=1 exp Q iK &gt; pj d out P n j0=1 exp Q iK&gt; pj0 d out V j 2Rd out = Xn j=1 k(Q i;K j) P n j0=1 k(Q i;K j) V j 2Rd out; with k a non-negative <b>kernel</b> function, which can be seen as a <b>kernel</b> <b>smoothing</b>. <b>Kernel</b> construction. Di erent choices for k suggest di erent transformers architectures [Tsai et al., 2019]. G. Mialon (Inria Paris) Designing ...", "dateLastCrawled": "2021-09-10T15:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Semi-Supervised Pairing via Basis-Sharing <b>Wasserstein</b> Matching Auto-Encoder", "url": "http://bayesiandeeplearning.org/2018/papers/123.pdf", "isFamilyFriendly": true, "displayUrl": "bayesiandeeplearning.org/2018/papers/123.pdf", "snippet": "A <b>similar</b> idea that uses a different method has been shown to work on some language tasks [3]; our work explores a new approach based on distribution matching. Through preliminary experiments, we show that the proposed algorithm can successfully incorporate the unlabeled data for improving the classi\ufb01cation accuracy on MNIST and CIFAR10 datasets. 2 Proposed Method 2.1 <b>Wasserstein</b> Training of Cross-Domain Auto-encoders We denote unlabeled training set as D UL ={{x i}n i=1;{y j} m j=1}, on ...", "dateLastCrawled": "2021-10-16T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] <b>Wasserstein</b> GAN math : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/mcadxa/d_wasserstein_gan_math/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/mcadxa/d_<b>wasserstein</b>_gan_math", "snippet": "Always use transfer learning if you can by finding a model pre-trained for a <b>similar</b> task and then fine-tune that model for your particular task. e.g. see huggingface for help with this in NLP. Gradual unfreezing and discriminative learning rates work well when fine-tuning a transfer learned model. Gradual unfreezing = freeze earlier layers and train the later layers only, then gradually unfreeze the earlier layers one by one. Discriminative learning rates = having different learning rates ...", "dateLastCrawled": "2021-03-28T09:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Hierarchical Gaussian Processes with <b>Wasserstein</b>-2 Kernels", "url": "https://www.researchgate.net/publication/344945084_Hierarchical_Gaussian_Processes_with_Wasserstein-2_Kernels", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344945084_Hierarchical_Gaussian_Processes...", "snippet": "<b>can</b> <b>be thought</b> as a point close to. 0.0 and . V in \u2264 \u03c3 2, by assumption that the variances of the inducing points have been reduced compared to their prior. W e assume that the inducing point ...", "dateLastCrawled": "2021-11-11T19:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Wasserstein Active Contours</b>", "url": "https://www.researchgate.net/publication/261387174_Wasserstein_Active_Contours", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261387174_<b>Wasserstein_Active_Contours</b>", "snippet": "The listwise ranking <b>loss</b> is formulated as the minimum cost (the <b>Wasserstein</b> distance) of transporting (or reshaping) the pile of predicted relevance mass so that it matches the pile of ground ...", "dateLastCrawled": "2021-11-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "FlowPool: Pooling Graph Representations with <b>Wasserstein</b> Gradient Flows", "url": "https://deepai.org/publication/flowpool-pooling-graph-representations-with-wasserstein-gradient-flows", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/flowpool-pooling-graph-representations-with-<b>wasserstein</b>...", "snippet": "12/18/21 - In several machine learning tasks for graph structured data, the graphs under consideration may be composed of a varying number of...", "dateLastCrawled": "2022-01-27T22:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Debiased Sinkhorn barycenters", "url": "http://proceedings.mlr.press/v119/janati20a/janati20a-supp.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/janati20a/janati20a-supp.pdf", "snippet": "F <b>can</b> <b>be thought</b> as a weighted average of distri-butions. While the ( k) k may have a \ufb01xed support or known \ufb01nite supports when working in machine learning applications, the support of F may or may not be known. When the latter is unknown a priori, free support methods are needed to jointly minimize the objective with respect to both the support and the mass of the distribution (Cu-turi &amp; Doucet,2014). Otherwise, \ufb01xed support methods, which only optimize weights on known supports, are ...", "dateLastCrawled": "2021-10-18T04:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Debiased Sinkhorn barycenters", "url": "http://proceedings.mlr.press/v119/janati20a/janati20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/janati20a/janati20a.pdf", "snippet": "F <b>can</b> <b>be thought</b> as a weighted average of distri-butions. While the (\u21b5 k) may have a \ufb01xed support or known \ufb01nite supports when working in machine learning applications, the support of \u21b5 F may or may not be known. When the latter is unknown a priori, free support methods are needed to jointly minimize the objective with respect to both the support and the mass of the distribution (Cu-turi &amp; Doucet, 2014). Otherwise, \ufb01xed support methods, which only optimize weights on known supports ...", "dateLastCrawled": "2022-01-21T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Volume 15 Issue 1 | Electronic Journal of Statistics", "url": "https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-15/issue-1", "isFamilyFriendly": true, "displayUrl": "https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-15/issue-1", "snippet": "Multivariate goodness-of-fit tests based on <b>Wasserstein</b> distance. Marc Hallin, Gilles Mordant, Johan Segers. Electron. J. Statist. 15 (1), 1328-1371, (2021) DOI: 10.1214/21-EJS1816. KEYWORDS: copula, elliptical distribution, Goodness-of-fit, group families, multivariate normality, Optimal transport, semi-discrete problem, skew-t distribution, <b>Wasserstein</b> distance. Read Abstract + = = = DOWNLOAD PAPER. SAVE TO MY LIBRARY + Using the accelerated failure time model to analyze current status ...", "dateLastCrawled": "2022-02-01T14:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[D] <b>Wasserstein</b> GAN math : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/mcadxa/d_wasserstein_gan_math/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/mcadxa/d_<b>wasserstein</b>_gan_math", "snippet": "Clustering your features <b>can</b> help you identify which ones are the most redundant and then removing the <b>can</b> help performance. Label <b>smoothing</b> = use 0.1 and 0.9 instead of 0 and 1 for label targets (<b>can</b> smoothen training) Don\u2019t dichotomise your data, if your output is continuous then its better to train the network to predict continuous values rather than turning it into a classification problem. Progressive resizing = train model on smaller resolution images first, then increase resolution ...", "dateLastCrawled": "2021-03-28T09:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Introduction to <b>Reinforcement Learning</b> (DDPG and TD3) for News ...", "url": "https://towardsdatascience.com/reinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning</b>-ddpg-and-td3-for-news...", "snippet": "News Recommendation <b>can</b> <b>be thought</b> of as a game that we are trying to win. We act based on the state, and the state is what we know about the user: ratings and movies watched combined. The action is produced based on the state and describes a point in space. Markov Property. Everything from now on strictly obeys the Markov Property. Quoting Wikipedia: \u2018 A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on ...", "dateLastCrawled": "2022-02-02T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Develop a GAN for Generating MNIST Handwritten Digits", "url": "https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for...", "snippet": "The discriminator model has two convolutional layers with 64 filters each, a small <b>kernel</b> size of 3, and larger than normal stride of 2. The model has no pooling layers and a single node in the output layer with the sigmoid activation function to predict whether the input sample is real or fake. The model is trained to minimize the binary cross entropy <b>loss</b> function, appropriate for binary classification. We will use some best practices in defining the discriminator model, such as the use of ...", "dateLastCrawled": "2022-02-02T19:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How to Implement GAN Hacks</b> in Keras to Train Stable Models", "url": "https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks", "snippet": "There have been some suggestions that only positive-class label <b>smoothing</b> is required and to values less than 1.0. Nevertheless, you <b>can</b> also smooth negative class labels. The example below demonstrates generating 1,000 labels for the negative class (class=0) and <b>smoothing</b> the label values uniformly into the range [0.0, 0.3] as recommended.", "dateLastCrawled": "2022-01-31T17:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "LCS graph <b>kernel</b> based on <b>Wasserstein</b> distance in longest common ...", "url": "https://www.sciencedirect.com/science/article/pii/S0165168421003182", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0165168421003182", "snippet": "To show improvement of OT framework (<b>Wasserstein</b> distance) <b>compared</b> to R-convolution framework, we implement our FLCS <b>kernel</b> based on the R-convolution framework, which we call FLCS-R Graph <b>Kernel</b>. The FLCS-R Graph <b>Kernel</b> on two graphs G 1 , G 2 is computed as k FLCS \u2212 R ( G 1 , G 2 ) = \u2211 x 1 \u2208 X 1 , x 2 \u2208 X 2 F sim ( x 1 , x 2 ) , where X 1 , X 2 is respective path sequence sets of G 1 , G 2 , and F sim is the path sequence similarity in Eq.", "dateLastCrawled": "2021-12-14T00:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Improving Image-Based Plant Disease Classification With Generative ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7746658/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7746658", "snippet": "The first is that the WGAN-GP uses the <b>Wasserstein</b> <b>loss</b> function with gradient penalty. <b>Compared</b> with the Jensen\u2013Shannon (JS) and Kullback\u2013Leibler (KL) divergence used in the DCGAN, <b>Wasserstein</b> distance <b>can</b> measure the distance between the distribution of real images and fake images, which <b>can</b> help improve the convergence of the network. The second is that in the WGAN-GP, the real and fake images are labeled as 1 and -1, while in the DCGAN, they are labeled as 1 and 0. This encourages ...", "dateLastCrawled": "2021-11-29T14:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Wasserstein</b> Distance-Based Auto-Encoder Tracking | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11063-021-10507-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11063-021-10507-9", "snippet": "Most of the existing visual object trackers are based on deep convolutional feature maps, but there have fewer works about finding new features for tracking. This paper proposes a novel tracking framework based on a full convolutional auto-encoder appearance model, which is trained by using <b>Wasserstein</b> distance and maximum mean discrepancy . <b>Compared</b> with previous works, the proposed framework has better performance in three aspects, including appearance model, update scheme, and state ...", "dateLastCrawled": "2022-01-30T11:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Predictive density estimation under the <b>Wasserstein</b> <b>loss</b> | Request PDF", "url": "https://www.researchgate.net/publication/341437107_Predictive_density_estimation_under_the_Wasserstein_loss", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/341437107_Predictive_density_estimation_under...", "snippet": "Abstract. We investigate predictive density estimation under the L2 <b>Wasserstein</b> <b>loss</b> for location families and location-scale families. We show that plug-in densities form a complete class and ...", "dateLastCrawled": "2021-12-12T01:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Generalized <b>Wasserstein</b> Dice <b>Loss</b>, Test-time Augmentation, and ...", "url": "https://deepai.org/publication/generalized-wasserstein-dice-loss-test-time-augmentation-and-transformers-for-the-brats-2021-challenge", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/generalized-<b>wasserstein</b>-dice-<b>loss</b>-test-time...", "snippet": "From Table 2, we see that 3D U-Net trained with generalized <b>Wasserstein</b> Dice <b>loss</b> performs consistently better than the one with Dice <b>loss</b> (baseline model). TransUNet does not offer any improvement over the baseline. Rather the performance deteriorates slightly. We hypothesize that over-parameterization <b>can</b> be an issue in this case. The optimizer SGDP and ASAM perform similar to the baseline SGD. From Table", "dateLastCrawled": "2022-01-19T20:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Low Dose CT Image Denoising Using a Generative Adversarial Network with ...", "url": "https://deepai.org/publication/low-dose-ct-image-denoising-using-a-generative-adversarial-network-with-wasserstein-distance-and-perceptual-loss", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/low-dose-ct-image-denoising-using-a-generative...", "snippet": "The <b>Wasserstein</b> distance is a key concept of the optimal transform theory, and promises to improve the performance of the GAN. The perceptual <b>loss</b> compares the perceptual features of a denoised output against those of the ground truth in an established feature space, while the GAN helps migrate the data noise distribution from strong to weak ...", "dateLastCrawled": "2022-02-02T22:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Ground Metric</b> Learning on Graphs | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10851-020-00996-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10851-020-00996-z", "snippet": "The entropic regularization scheme has helped to tackle inverse problems that involve OT, since it converts the original <b>Wasserstein</b> distance into a fast, smooth, differentiable, and more robust <b>loss</b>. Although differentiating the <b>Wasserstein</b> <b>loss</b> has been extensively covered, differentiating quantities that build upon it, such as smooth <b>Wasserstein</b> barycenters, is less common. A few examples are <b>Wasserstein</b> barycentric coordinates", "dateLastCrawled": "2022-01-01T01:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "GRADIENT DESCENT <b>CAN</b> LEARN LESS <b>OVER PARAMETERIZED TWO LAYER NEURAL</b> ...", "url": "https://openreview.net/pdf?id=BJg641BKPH", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=BJg641BKPH", "snippet": "by utilizing the <b>Wasserstein</b> gradient \ufb02ow perspective (Nitanda &amp; Suzuki, 2017) on the gradient descent. For the scaling factor 1=m ( &lt;1), Du et al. (2019) essentially demonstrated that the <b>kernel</b> <b>smoothing</b> of functional gradients by the neural tangent <b>kernel</b> (Jacot et al., 2018; Chizat &amp; Bach, 2018b) has comparable performance with the functional gradient as m!1by making a positivity assumption on the Gram-matrix of this <b>kernel</b>, resulting in the global convergence property. In addition ...", "dateLastCrawled": "2022-01-26T10:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Adversarial Divergences are Good Task Losses for Generative Modeling", "url": "https://gauthiergidel.github.io/papers/1708.02511.pdf", "isFamilyFriendly": true, "displayUrl": "https://gauthiergidel.github.io/papers/1708.02511.pdf", "snippet": "this structured <b>loss</b>. In both cases, models <b>can</b> be objec-tively <b>compared</b> and evaluated with respect to the task <b>loss</b> (i.e., generalization error). On the other hand, we will show that it is not as obvious in generative modeling to de\ufb01ne a task <b>loss</b> that correlates well with the \ufb01nal task of generat-ing realistic samples. Traditionally in statistics, distribution learning is formu-lated as density estimation where the task <b>loss</b> is the ex-pected negative-log-likelihood. Although log ...", "dateLastCrawled": "2021-08-30T19:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] <b>Wasserstein</b> GAN math : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/mcadxa/d_wasserstein_gan_math/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/mcadxa/d_<b>wasserstein</b>_gan_math", "snippet": "Clustering your features <b>can</b> help you identify which ones are the most redundant and then removing the <b>can</b> help performance. Label <b>smoothing</b> = use 0.1 and 0.9 instead of 0 and 1 for label targets (<b>can</b> smoothen training) Don\u2019t dichotomise your data, if your output is continuous then its better to train the network to predict continuous values rather than turning it into a classification problem. Progressive resizing = train model on smaller resolution images first, then increase resolution ...", "dateLastCrawled": "2021-03-28T09:33:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to stabilize GAN training. Understand <b>Wasserstein</b> distance and ...", "url": "https://towardsdatascience.com/wasserstein-distance-gan-began-and-progressively-growing-gan-7e099f38da96", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>wasserstein</b>-distance-gan-began-and-progressively...", "snippet": "<b>Wasserstein</b> <b>loss</b> leads to a higher quality of the gradients to train G. ... Finally, one intuitive way to understand this paper is to make an <b>analogy</b> with the gradients on the history of in-layer activation functions. Specifically, the gradients of sigmoid and tanh activations that disappeared in favor of ReLUs, because of the improved gradients in the whole range of values. BEGAN (Boundary Equilibrium Generative Adversarial Networks 2017) We often see that the discriminator progresses too ...", "dateLastCrawled": "2022-01-25T18:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning Wasserstein Embeddings</b> | DeepAI", "url": "https://deepai.org/publication/learning-wasserstein-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-wasserstein-embeddings</b>", "snippet": "The <b>Wasserstein</b> distance received a lot of attention recently in the community of <b>machine</b> <b>learning</b>, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy ...", "dateLastCrawled": "2022-01-05T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Learning</b> <b>Wasserstein</b> Embeddings - ResearchGate", "url": "https://www.researchgate.net/publication/320564581_Learning_Wasserstein_Embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320564581_<b>Learning</b>_<b>Wasserstein</b>_Embeddings", "snippet": "Designed through an <b>analogy</b> with ... Fast dictionary <b>learning</b> with a smoothed <b>wasserstein</b> <b>loss</b>. In AISTA TS, pages 630\u2013638, 2016. [32] F. Santambrogio. Introduction to optimal transport theory ...", "dateLastCrawled": "2021-12-13T04:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "deep <b>learning</b> - How can both generator and discriminator losses ...", "url": "https://datascience.stackexchange.com/questions/32699/how-can-both-generator-and-discriminator-losses-decrease", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/32699", "snippet": "In the widely used <b>analogy</b>: ... despite the WGAN having a different <b>loss</b> function, namely the <b>Wasserstein</b> distance, one should still not expect that the discriminator and generator simultaneously monotonically increase -- generally one of them &quot;wins&quot; the round and receives a lower portion of the <b>loss</b>. $\\endgroup$ \u2013 PSub. Mar 13 &#39;21 at 6:07 $\\begingroup$ @PSub You are completely misunderstanding the question. It&#39;s not a question about the small scale changes of the <b>loss</b> values. OP is asking ...", "dateLastCrawled": "2022-01-28T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Manifold-Valued Image Generation with <b>Wasserstein</b> Generative ...", "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4277/4155", "isFamilyFriendly": true, "displayUrl": "https://ojs.aaai.org/index.php/AAAI/article/download/4277/4155", "snippet": "fundamental <b>machine</b> <b>learning</b> problems. However, few mod-ern generative models, including <b>Wasserstein</b> Generative Ad-versarial Nets (WGANs), are studied on manifold-valued im- ages that are frequently encountered in real-world applica-tions. To \ufb01ll the gap, this paper \ufb01rst formulates the problem of generating manifold-valued images and exploits three typical instances: hue-saturation-value (HSV) color image genera-tion, chromaticity-brightness (CB) color image generation, and diffusion ...", "dateLastCrawled": "2022-01-29T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Advanced <b>Machine</b> <b>Learning</b> - Computer Vision", "url": "https://www.vision.rwth-aachen.de/media/course/SS/2019/advanced-machine-learning/ml2_19-part17-gans-6on1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.vision.rwth-aachen.de/media/course/SS/2019/advanced-<b>machine</b>-<b>learning</b>/ml2...", "snippet": "<b>Analogy</b>: police investigator \u2022Both generator and discriminator are deep networks We can train them with backprop. Image sources: www.bundesbank.de, weclipart.com, Kevin McGuiness 15 Advanced <b>Machine</b> <b>Learning</b> Part 17 \u2013Generative Adversarial Networks Training the Discriminator \u2022Procedure Fix generator weights Train discriminator to distinguish between real and generated images Image credit: Kevin McGuiness 16 Visual Computing Institute | Prof. Dr . Bastian Leibe Advanced <b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2021-10-25T15:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[D] Is the <b>Wasserstein</b> distance really what we optimize in WGAN ...", "url": "https://www.reddit.com/r/MachineLearning/comments/ew2lzs/d_is_the_wasserstein_distance_really_what_we/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/ew2lzs/d_is_the_<b>wasserstein</b>_distance...", "snippet": "The &quot;genuine&quot; <b>Wasserstein</b> <b>loss</b> relies on optimal transport, a generalization of sorting to high-dimensional feature spaces. In a nutshell: OT relies on the matrix of distances between samples to define a &quot;least action&quot; matching between any two distributions. Now, unfortunately, in spaces of images, the L2 distance is (essentially) meaningless: natural images should not be compared with each other pixel-wise. As a consequence, the baseline <b>Wasserstein</b> distance between two batches of images is ...", "dateLastCrawled": "2021-09-30T11:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>Tour of Generative Adversarial Network Models</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/tour-of-generative-adversarial-network-models/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>tour-of-generative-adversarial-network-models</b>", "snippet": "By <b>analogy</b> with auto-encoders, we propose Context Encoders \u2013 a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. \u2014 Context Encoders: Feature <b>Learning</b> by Inpainting, 2016. Example of the Context Encoders Encoder-Decoder Model Architecture. Taken from: Context Encoders: Feature <b>Learning</b> by Inpainting. The model is trained with a joint-<b>loss</b> that combines both the adversarial <b>loss</b> of generator and discriminator models ...", "dateLastCrawled": "2022-02-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "SpringerLink - <b>Machine Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-020-05924-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05924-1", "snippet": "for a given set \\({\\mathcal {F}}\\) of distributions, twice differentiable and convex <b>loss</b> \\(\\ell\\), and prediction \\(f_\\theta (x)\\).The set \\({\\mathcal {F}}\\) is the set of distributions on which one would like the estimator to achieve a guaranteed performance bound.. Causal inference can be seen to be a specific instance of distributional robustness, where we take \\({\\mathcal {F}}\\) to be the class of all distributions generated under do-interventions on the predictors X (Meinshausen 2018 ...", "dateLastCrawled": "2022-01-15T22:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Gentle Introduction to Pix2Pix Generative</b> Adversarial Network", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-pix2pix-generative</b>...", "snippet": "Image-to-image translation is the controlled conversion of a given source image to a target image. An example might be the conversion of black and white photographs to color photographs. Image-to-image translation is a challenging problem and often requires specialized models and <b>loss</b> functions for a given translation task or dataset. The Pix2Pix GAN is a general approach for image-to-image translation. It is based", "dateLastCrawled": "2022-02-02T13:26:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(wasserstein loss)  is like +(kernel smoothing)", "+(wasserstein loss) is similar to +(kernel smoothing)", "+(wasserstein loss) can be thought of as +(kernel smoothing)", "+(wasserstein loss) can be compared to +(kernel smoothing)", "machine learning +(wasserstein loss AND analogy)", "machine learning +(\"wasserstein loss is like\")", "machine learning +(\"wasserstein loss is similar\")", "machine learning +(\"just as wasserstein loss\")", "machine learning +(\"wasserstein loss can be thought of as\")", "machine learning +(\"wasserstein loss can be compared to\")"]}