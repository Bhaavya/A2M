{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reflections \u2013 Matt&#39;s Homepage", "url": "https://mattfife.com/?cat=11", "isFamilyFriendly": true, "displayUrl": "https://mattfife.com/?cat=11", "snippet": "<b>GPT</b>-3 (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3) is an autoregressive language model that uses deep learning to produce human-<b>like</b> text. It is the third-generation language prediction model in the <b>GPT</b>-n series (and the successor to <b>GPT</b>-2) created by OpenAI. <b>GPT</b>-3\u2019s full version has a capacity of 175 billion machine learning parameters. People have created unbelievably accurate question-based search engines, ghost write articles, chatbots that can fool almost anyone, code generation based on ...", "dateLastCrawled": "2022-01-06T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How Algorithms Create and Prevent Fake News: Exploring the Impacts of ...", "url": "https://ebin.pub/download/how-algorithms-create-and-prevent-fake-news-exploring-the-impacts-of-social-media-deepfakes-gpt-3-and-more-1484271548-9781484271544.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/download/how-algorithms-create-and-prevent-fake-news-exploring-the...", "snippet": "OpenAI has created a variety of AI products, but the one that has grabbed the most headlines is its text generation software <b>GPT</b>, an acronym for the technical name <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> that need not concern us. <b>GPT</b> refers to a sequence of products: the original <b>GPT</b> came out in 2018 to limited fanfare; then a year later, <b>GPT</b>-2 was released10 and reached a whole new level of capability; and just one year after that, the current state10", "dateLastCrawled": "2022-01-13T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "PEGASUS: Pre-training with <b>Extracted Gap-sentences for Abstractive</b> ...", "url": "https://deepai.org/publication/pegasus-pre-training-with-extracted-gap-sentences-for-abstractive-summarization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/pegasus-pre-training-with-extracted-gap-sentences-for...", "snippet": "khandelwal2019sample <b>pre-trained</b> a <b>Transformer</b> language model on Wikipedia, and fine-tuned using 3000 examples, achieving 13.1 ROUGE-2. 3 Pre-training Objectives We propose a new pre-training objective, GSG, in this work, but for comparison, we also evaluate BERT\u2019s masked-language model objective, in isolation and in conjunction with GSG.", "dateLastCrawled": "2022-01-25T12:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Here Be Monsters</b> - Podcast | Global Player", "url": "https://www.globalplayer.com/podcasts/GwQy6/", "isFamilyFriendly": true, "displayUrl": "https://www.globalplayer.com/podcasts/GwQy6", "snippet": "OpenAI\u2019s <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (<b>GPT</b>-2) was turned into the Python library GPT2 Simple by Max Woolf, who also created a tutorial demonstrating how to train the model for free using Google Colab. Jeff used this tutorial to train Theodora on a corpus of about 900 Ted Talk transcripts for 5,000 training steps. Jeff then downloaded the model locally and used JupyterLab (Python) to generate new text. That text was then sent to Google Cloud\u2019s Text-To-Speech (TTS) service where it ...", "dateLastCrawled": "2021-11-29T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "ai | <b>ceoln</b>", "url": "https://ceoln.wordpress.com/tag/ai/", "isFamilyFriendly": true, "displayUrl": "https://<b>ceoln</b>.wordpress.com/tag/ai", "snippet": "The result of invoking <b>GPT</b>-3 will be appended to your story, and then <b>you</b> can go <b>back</b> to editing it or whatever. <b>You</b> can post (share, publish) stories, unless <b>you</b>\u2019ve started it as a Private Story, in which case <b>you</b> can\u2019t. It looks <b>like</b> there is a Naughty Story filter that is applied to stories that aren\u2019t Private. There doesn\u2019t seem to ...", "dateLastCrawled": "2021-12-29T22:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "ICML 2020 <b>Friday</b> 07/17", "url": "https://icml.cc/virtual/2020/day/7/17", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2020/day/7/17", "snippet": "The <b>Transformer</b> architecture has achieved considerable success recently; the key component of the <b>Transformer</b> is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with attention layers can be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence. As <b>Transformer</b> models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization ...", "dateLastCrawled": "2021-11-24T08:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "eWEEK - feedctl", "url": "https://zeroindex.org/feedctl/index.php?channelid=452", "isFamilyFriendly": true, "displayUrl": "https://zeroindex.org/feedctl/index.php?channelid=452", "snippet": "For example, if <b>you</b> were a regional retailer located in Grand Rapids, the competition to hire <b>someone</b> with technical skills was relatively lower than in a large tech hub. That has changed, though. One of the unanticipated results of COVID-19 has been the growth of remote work; suddenly, one could be employed by a top cloud-native company and happily reside in Grand Rapids. While some large technology firms <b>like</b> Google have put forward policies designed to induce/urge employees <b>back</b> into ...", "dateLastCrawled": "2022-01-15T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Search Results - Neural Network - papasearch.net", "url": "http://papasearch.net/Neural_Network/NeuralNetwork17.html", "isFamilyFriendly": true, "displayUrl": "papasearch.net/Neural_Network/NeuralNetwork17.html", "snippet": "It&#39;s a full layout of the ResNet-50 training graph, a neural network with ~3 million nodes, and ~10 million edges, using Gephi for the graph layout, to output a 25000x25000 pixel image.(If <b>you</b> <b>like</b> this sort of Stuff then please support me on Patreon.And take a look at Explain the Cloud <b>Like</b> I&#39;m 10, my new book for complete ...", "dateLastCrawled": "2022-01-19T19:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "RSS Rabbit", "url": "https://rssrabbit.com/?files=,Food,USA%20politics,Entertainment,Law", "isFamilyFriendly": true, "displayUrl": "https://rssrabbit.com/?files=,Food,USA politics,Entertainment,Law", "snippet": "They\u2019ve moved into producing streaming shows <b>like</b> The Boys and Invincible, ... the third generation of its massive <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> language model, which can write everything from computer code to poetry. A year later, with much less fanfare, Tsinghua University\u2019s Beijing Academy of Artificial Intelligence released an even larger model, Wu Dao 2.0, with 10 times as many parameters\u2014the neural network values that encode information. While <b>GPT</b>-3 boasts 175 billion ...", "dateLastCrawled": "2021-12-29T12:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>The Oldie magazine - February issue (396</b>) by oldieproduction - Issuu", "url": "https://issuu.com/oldieproduction/docs/to_february_2021_20210106", "isFamilyFriendly": true, "displayUrl": "https://issuu.com/oldieproduction/docs/to_february_2021_20210106", "snippet": "FREE SAMPLE COPY If <b>you</b> have a friend who would <b>like</b> a free sample of The Oldie, tell them to call 0800 8565867. GET THE OLDIE APP Go to App Store or Google Play Store. Search for Oldie Magazine ...", "dateLastCrawled": "2022-02-02T22:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How Algorithms Create and Prevent Fake News: Exploring the Impacts of ...", "url": "https://ebin.pub/download/how-algorithms-create-and-prevent-fake-news-exploring-the-impacts-of-social-media-deepfakes-gpt-3-and-more-1484271548-9781484271544.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/download/how-algorithms-create-and-prevent-fake-news-exploring-the...", "snippet": "OpenAI has created a variety of AI products, but the one that has grabbed the most headlines is its text generation software <b>GPT</b>, an acronym for the technical name <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> that need not concern us. <b>GPT</b> refers to a sequence of products: the original <b>GPT</b> came out in 2018 to limited fanfare; then a year later, <b>GPT</b>-2 was released10 and reached a whole new level of capability; and just one year after that, the current state10", "dateLastCrawled": "2022-01-13T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "PEGASUS: Pre-training with <b>Extracted Gap-sentences for Abstractive</b> ...", "url": "https://deepai.org/publication/pegasus-pre-training-with-extracted-gap-sentences-for-abstractive-summarization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/pegasus-pre-training-with-extracted-gap-sentences-for...", "snippet": "Most <b>similar</b> to our approach are <b>Transformer</b> encoder-decoder models <b>pre-trained</b> on some masked input pre-training objective. Mass (song2019mass) proposed masked sequence-to-sequence generation that reconstructs a sentence fragment given the remaining part of the sentence. A single sentence fragment was randomly selected. UniLM (unilm) proposed jointly training on three types of language modeling tasks: unidirectional (left-to-right and right-to-left), bidirectional (word-level mask, with ...", "dateLastCrawled": "2022-01-25T12:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Here Be Monsters</b> - Podcast | Global Player", "url": "https://www.globalplayer.com/podcasts/GwQy6/", "isFamilyFriendly": true, "displayUrl": "https://www.globalplayer.com/podcasts/GwQy6", "snippet": "OpenAI\u2019s <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (<b>GPT</b>-2) was turned into the Python library GPT2 Simple by Max Woolf, who also created a tutorial demonstrating how to train the model for free using Google Colab. Jeff used this tutorial to train Theodora on a corpus of about 900 Ted Talk transcripts for 5,000 training steps. Jeff then downloaded the model locally and used JupyterLab (Python) to generate new text. That text was then sent to Google Cloud\u2019s Text-To-Speech (TTS) service where it ...", "dateLastCrawled": "2021-11-29T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ai | <b>ceoln</b>", "url": "https://ceoln.wordpress.com/tag/ai/", "isFamilyFriendly": true, "displayUrl": "https://<b>ceoln</b>.wordpress.com/tag/ai", "snippet": "<b>Back</b> on the <b>GPT</b>-3 theme again, ... <b>You</b> can create a Prompt, and optionally share it for other users to see, and given a Prompt (your own or one <b>someone</b> has shared) <b>you</b> can write a story based on it, in what appears to be an input box of their own devising. At any time, <b>you</b> can push the \u201cWrite for me\u201d button, and it will invoke <b>GPT</b>-3; with exactly what inputs isn\u2019t clear, but one speculates that it\u2019s the Prompt and the most recent part of the Story and anything that one has entered ...", "dateLastCrawled": "2021-12-29T22:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "ICML 2020 <b>Friday</b> 07/17", "url": "https://icml.cc/virtual/2020/day/7/17", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2020/day/7/17", "snippet": "The <b>Transformer</b> architecture has achieved considerable success recently; the key component of the <b>Transformer</b> is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with attention layers can be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence. As <b>Transformer</b> models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization ...", "dateLastCrawled": "2021-11-24T08:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "NEW SAVANNA: August 2021", "url": "https://new-savanna.blogspot.com/2021/08/", "isFamilyFriendly": true, "displayUrl": "https://new-savanna.blogspot.com/2021/08", "snippet": "Large-scale <b>pre-trained</b> transformers such as <b>GPT</b>-2, <b>GPT</b>-3, BART, and others have helped with some of the \u201cfancy babbling\u201d issues by allowing for larger context windows, but the problem is not completely resolved. As language models themselves they cannot address the problem of forward-looking to ensure they are building toward something in the future, except by accident.", "dateLastCrawled": "2022-01-07T10:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "eWEEK - feedctl", "url": "https://zeroindex.org/feedctl/index.php?channelid=452", "isFamilyFriendly": true, "displayUrl": "https://zeroindex.org/feedctl/index.php?channelid=452", "snippet": "<b>Back</b> in the day, end-to-end monitoring used to mean observability applied to everything from the device Operating System (OS) and application User Interface (UI) to the compute, storage, analytics functions and <b>back</b> again to the user. The new end-to-end monitoring is all of that, plus an ability to deliver monitoring across the entire breadth of infrastructure that all of the above runs on.", "dateLastCrawled": "2022-01-15T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Search Results - Neural Network - papasearch.net", "url": "http://papasearch.net/Neural_Network/NeuralNetwork17.html", "isFamilyFriendly": true, "displayUrl": "papasearch.net/Neural_Network/NeuralNetwork17.html", "snippet": "Dec 08, 2016 \u00b7 That would practically allow us to skip the phase of training as the neural network would be already <b>pre-trained</b> to recognize a wide set of potential issues. This is basically the way a human mind operates: <b>you</b> use past experiences to gain insight on new situations, even if they are very different.", "dateLastCrawled": "2022-01-19T19:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Here Be Monsters - TopPodcast</b>.com", "url": "https://toppodcast.com/podcast_feeds/here-be-monsters/", "isFamilyFriendly": true, "displayUrl": "https://toppodcast.com/podcast_feeds/<b>here-be-monsters</b>", "snippet": "An independent podcast about fear, beauty and the unknown. Since 2012. Hosted by Jeff Emtman and others.", "dateLastCrawled": "2021-12-10T01:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>The Oldie magazine - February issue (396</b>) by oldieproduction - Issuu", "url": "https://issuu.com/oldieproduction/docs/to_february_2021_20210106", "isFamilyFriendly": true, "displayUrl": "https://issuu.com/oldieproduction/docs/to_february_2021_20210106", "snippet": "Fork up 2 eggs with \u00bc pint (150ml) double cream, season, add a handful of chopped ham (or grated cheese) and pour over the leeks. Prepare 12oz (350g) shortcrust with half butter, half lard, and ...", "dateLastCrawled": "2022-02-02T22:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reflections \u2013 Matt&#39;s Homepage", "url": "https://mattfife.com/?cat=11", "isFamilyFriendly": true, "displayUrl": "https://mattfife.com/?cat=11", "snippet": "<b>GPT</b>-3 (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3) is an autoregressive language model that uses deep learning to produce human-like text. It is the third-generation language prediction model in the <b>GPT</b>-n series (and the successor to <b>GPT</b>-2) created by OpenAI. <b>GPT</b>-3\u2019s full version has a capacity of 175 billion machine learning parameters. People have created unbelievably accurate question-based search engines, ghost write articles, chatbots that <b>can</b> fool almost anyone, code generation based on ...", "dateLastCrawled": "2022-01-06T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How Algorithms Create and Prevent Fake News: Exploring the Impacts of ...", "url": "https://ebin.pub/download/how-algorithms-create-and-prevent-fake-news-exploring-the-impacts-of-social-media-deepfakes-gpt-3-and-more-1484271548-9781484271544.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/download/how-algorithms-create-and-prevent-fake-news-exploring-the...", "snippet": "OpenAI has created a variety of AI products, but the one that has grabbed the most headlines is its text generation software <b>GPT</b>, an acronym for the technical name <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> that need not concern us. <b>GPT</b> refers to a sequence of products: the original <b>GPT</b> came out in 2018 to limited fanfare; then a year later, <b>GPT</b>-2 was released10 and reached a whole new level of capability; and just one year after that, the current state10", "dateLastCrawled": "2022-01-13T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "PEGASUS: Pre-training with Extracted Gap-sentences for - arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1912.08777/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1912.08777", "snippet": "khandelwal2019sample <b>pre-trained</b> a <b>Transformer</b> language model on Wikipedia, and fine-tuned using 3000 examples, achieving 13.1 ROUGE-2. 3 Pre-training Objectives We propose a new pre-training objective, GSG, in this work, but for comparison, we also evaluate BERT\u2019s masked-language model objective, in isolation and in conjunction with GSG.", "dateLastCrawled": "2022-02-02T08:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Here Be Monsters</b> - Podcast | Global Player", "url": "https://www.globalplayer.com/podcasts/GwQy6/", "isFamilyFriendly": true, "displayUrl": "https://www.globalplayer.com/podcasts/GwQy6", "snippet": "OpenAI\u2019s <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (<b>GPT</b>-2) was turned into the Python library GPT2 Simple by Max Woolf, who also created a tutorial demonstrating how to train the model for free using Google Colab. Jeff used this tutorial to train Theodora on a corpus of about 900 Ted Talk transcripts for 5,000 training steps. Jeff then downloaded the model locally and used JupyterLab (Python) to generate new text. That text was then sent to Google Cloud\u2019s Text-To-Speech (TTS) service where it ...", "dateLastCrawled": "2021-11-29T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "arXiv:1912.08777v1 [cs.CL] 18 Dec 2019", "url": "https://arxiv.org/pdf/1912.08777v1", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1912.08777v1", "snippet": "(<b>generative</b>) tasks and do not evaluate on NLU clas-si\ufb01cation tasks. There has been some work on the low-resource, summarization setting using the CNN/DailyMail dataset.Radford et al.(2018b) showed that a large <b>Transformer</b> language model <b>pre-trained</b> on Web text could generate summaries if prompted with \u201dTL;DR\u201d, achieving a ROUGE-2 of 8.27 on CNN/DailyMail.Khandelwal et al.(2019) <b>pre-trained</b> a <b>Transformer</b> language model on Wikipedia, and \ufb01ne-tuned using 3000 examples, achieving 13.1 ...", "dateLastCrawled": "2021-10-26T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "NEW SAVANNA: August 2021", "url": "https://new-savanna.blogspot.com/2021/08/", "isFamilyFriendly": true, "displayUrl": "https://new-savanna.blogspot.com/2021/08", "snippet": "Large-scale <b>pre-trained</b> transformers such as <b>GPT</b>-2, <b>GPT</b>-3, BART, and others have helped with some of the \u201cfancy babbling\u201d issues by allowing for larger context windows, but the problem is not completely resolved. As language models themselves they cannot address the problem of forward-looking to ensure they are building toward something in the future, except by accident.", "dateLastCrawled": "2022-01-07T10:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "ai | <b>ceoln</b>", "url": "https://ceoln.wordpress.com/tag/ai/", "isFamilyFriendly": true, "displayUrl": "https://<b>ceoln</b>.wordpress.com/tag/ai", "snippet": "<b>Back</b> on the <b>GPT</b>-3 theme again, there\u2019s another <b>GPT</b>-3 client at shortlyread.com. It doesn\u2019t use any special training as far as I\u2019m aware (like AI Dungeon has choose your story dot com), and I don\u2019t know if it has any hidden inputs. It has very (very!) little documentation about what it actually does, but at the moment its top-level entities are \u201cPrompts\u201d (which are rendered as story titles, basically) and \u201cStories\u201d (which are stories). <b>You</b> <b>can</b> create a Prompt, and optionally ...", "dateLastCrawled": "2021-12-29T22:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reinforcement Learning: Industrial Applications of Intelligent Agents ...", "url": "https://dokumen.pub/reinforcement-learning-industrial-applications-of-intelligent-agents-1098114833-9781098114831.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/reinforcement-learning-industrial-applications-of-intelligent...", "snippet": "Eventually, <b>you</b> will obtain a reward and <b>you</b> <b>can</b> go <b>back</b> and look at all the states visited along the way. <b>You</b> then add the reward to a cumulative sum and keep a counter to calculate the average return observed after visiting all of those states. Whenever <b>you</b> perform an experiment, it is prudent to imagine some theoretical expectations, or priors\u2014defining a hypothesis is the heart of the scientific method.", "dateLastCrawled": "2022-02-02T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "eWEEK - feedctl", "url": "https://zeroindex.org/feedctl/index.php?channelid=452", "isFamilyFriendly": true, "displayUrl": "https://zeroindex.org/feedctl/index.php?channelid=452", "snippet": "The page will come alive at that time with the real-time discussion. <b>You</b> <b>can</b> join in or simply watch the discussion as it is created. Special Guests, Digital Transformation Trends. The list of experts for this month\u2019s Tweetchat currently includes the following \u2013 please check <b>back</b> for additional expert guests: Llanor Alleyne, Managing Editor, IT Business Edge Chris Ehrlich, Managing Editor, Datamation; James Maguire, Editor-in-Chief, eWeek [moderator] Chat room real-time link: Go to the ...", "dateLastCrawled": "2022-01-15T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Here Be Monsters - TopPodcast</b>.com", "url": "https://toppodcast.com/podcast_feeds/here-be-monsters/", "isFamilyFriendly": true, "displayUrl": "https://toppodcast.com/podcast_feeds/<b>here-be-monsters</b>", "snippet": "An independent podcast about fear, beauty and the unknown. Since 2012. Hosted by Jeff Emtman and others.", "dateLastCrawled": "2021-12-10T01:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How Algorithms Create and Prevent Fake News: Exploring the Impacts of ...", "url": "https://ebin.pub/download/how-algorithms-create-and-prevent-fake-news-exploring-the-impacts-of-social-media-deepfakes-gpt-3-and-more-1484271548-9781484271544.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/download/how-algorithms-create-and-prevent-fake-news-exploring-the...", "snippet": "OpenAI has created a variety of AI products, but the one that has grabbed the most headlines is its text generation software <b>GPT</b>, an acronym for the technical name <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> that need not concern us. <b>GPT</b> refers to a sequence of products: the original <b>GPT</b> came out in 2018 to limited fanfare; then a year later, <b>GPT</b>-2 was released10 and reached a whole new level of capability; and just one year after that, the current state10", "dateLastCrawled": "2022-01-13T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Here Be Monsters</b> - Podcast | Global Player", "url": "https://www.globalplayer.com/podcasts/GwQy6/", "isFamilyFriendly": true, "displayUrl": "https://www.globalplayer.com/podcasts/GwQy6", "snippet": "OpenAI\u2019s <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (<b>GPT</b>-2) was turned into the Python library GPT2 Simple by Max Woolf, who also created a tutorial demonstrating how to train the model for free using Google Colab. Jeff used this tutorial to train Theodora on a corpus of about 900 Ted Talk transcripts for 5,000 training steps. Jeff then downloaded the model locally and used JupyterLab (Python) to generate new text. That text was then sent to Google Cloud\u2019s Text-To-Speech (TTS) service where it ...", "dateLastCrawled": "2021-11-29T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "PEGASUS: Pre-training with Extracted Gap-sentences for - arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1912.08777/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1912.08777", "snippet": "khandelwal2019sample <b>pre-trained</b> a <b>Transformer</b> language model on Wikipedia, and fine-tuned using 3000 examples, ... We <b>compared</b> six variants of GSG (Lead, Random, Ind-Orig, Ind-Uniq, Seq-Orig, Seq-Uniq) while choosing 30% sentences as gap sentences. As shown in Figure 3(a), Ind-Orig achieved the best performance followed by Seq-Uniq. Ind-Orig and Seq-Uniq were consistently better (or similar) than Random and Lead across the four downstream datasets. Lead had decent performance on the two ...", "dateLastCrawled": "2022-02-02T08:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ICML 2020 <b>Friday</b> 07/17", "url": "https://icml.cc/virtual/2020/day/7/17", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2020/day/7/17", "snippet": "The <b>Transformer</b> architecture has achieved considerable success recently; the key component of the <b>Transformer</b> is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with attention layers <b>can</b> be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence. As <b>Transformer</b> models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization ...", "dateLastCrawled": "2021-11-24T08:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "eWEEK - feedctl", "url": "https://zeroindex.org/feedctl/index.php?channelid=452", "isFamilyFriendly": true, "displayUrl": "https://zeroindex.org/feedctl/index.php?channelid=452", "snippet": "The page will come alive at that time with the real-time discussion. <b>You</b> <b>can</b> join in or simply watch the discussion as it is created. Special Guests, Digital Transformation Trends. The list of experts for this month\u2019s Tweetchat currently includes the following \u2013 please check <b>back</b> for additional expert guests: Llanor Alleyne, Managing Editor, IT Business Edge Chris Ehrlich, Managing Editor, Datamation; James Maguire, Editor-in-Chief, eWeek [moderator] Chat room real-time link: Go to the ...", "dateLastCrawled": "2022-01-15T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Here Be Monsters - TopPodcast</b>.com", "url": "https://toppodcast.com/podcast_feeds/here-be-monsters/", "isFamilyFriendly": true, "displayUrl": "https://toppodcast.com/podcast_feeds/<b>here-be-monsters</b>", "snippet": "An independent podcast about fear, beauty and the unknown. Since 2012. Hosted by Jeff Emtman and others.", "dateLastCrawled": "2021-12-10T01:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Search Results - Neural Network - papasearch.net", "url": "http://papasearch.net/Neural_Network/NeuralNetwork17.html", "isFamilyFriendly": true, "displayUrl": "papasearch.net/Neural_Network/NeuralNetwork17.html", "snippet": "Dec 08, 2016 \u00b7 That would practically allow us to skip the phase of training as the neural network would be already <b>pre-trained</b> to recognize a wide set of potential issues. This is basically the way a human mind operates: <b>you</b> use past experiences to gain insight on new situations, even if they are very different.", "dateLastCrawled": "2022-01-19T19:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>The Oldie magazine - February issue (396</b>) by oldieproduction - Issuu", "url": "https://issuu.com/oldieproduction/docs/to_february_2021_20210106", "isFamilyFriendly": true, "displayUrl": "https://issuu.com/oldieproduction/docs/to_february_2021_20210106", "snippet": "It buys a little time in a parking meter, as a sixpence used to do. It <b>can</b> be used in a cut-price shop for sets of soaps. A reel of cotton, if <b>you</b> <b>can</b> find one, costs more than a pound. A one ...", "dateLastCrawled": "2022-02-02T22:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "RSS Rabbit", "url": "https://rssrabbit.com/?files=,Food,xkcd,Europe", "isFamilyFriendly": true, "displayUrl": "https://rssrabbit.com/?files=,Food,xkcd,Europe", "snippet": "Of course, <b>you</b> <b>can</b>\u2019t very well call a robot a \u201cRetriever\u201d if it doesn\u2019t solve the problem of bringing things <b>back</b> to <b>you</b> without relying on a person on the other end to place those things onto the robot. I\u2019ve lost count of the number of beer- (or whatever-) fetching robots that attempt to use manipulators to open a refrigerator and find and grasp something inside, which is a superhard problem that is years away from being solved in a practical in-home way. Labrador has quite ...", "dateLastCrawled": "2022-01-17T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "RSS Rabbit", "url": "https://rssrabbit.com/?files=,Music,Food,Climate,Europe,Entertainment", "isFamilyFriendly": true, "displayUrl": "https://rssrabbit.com/?files=,Music,Food,Climate,Europe,Entertainment", "snippet": "<b>You</b> <b>can</b> make transistors as small as <b>you</b> want, but if <b>you</b> <b>can</b>&#39;t connect them up to each other, there&#39;s no point. So Arm and the Belgian research institute imec spent a few years finding room for those connections. The best scheme they found was to take the interconnects that carry power to logic circuits (as opposed to data) and bury them under the surface of the silicon, linking them to a power delivery network built on the backside of the chip. This research trend suddenly became", "dateLastCrawled": "2021-12-29T00:34:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>What is GPT-3</b>? - Dr Peper MD", "url": "https://drpepermd.com/2021/02/22/what-is-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://drpepermd.com/2021/02/22/<b>what-is-gpt-3</b>", "snippet": "<b>GPT</b>-3 stands for <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (the third version). Some have called it the most important and useful advance in AI in years. The abilities of <b>GPT</b>-3 have both shocked and excited many within the AI community. As one developer said: \u201cPlaying with <b>GPT</b>-3 feels like seeing the future.\u201d But, how was <b>GPT</b>-3 developed? Find out in this episode of Short and Sweet AI. You can listen to this episode below or keep reading. Another Mind-Blowing Tool from OpenAI. How does <b>GPT</b>-3 ...", "dateLastCrawled": "2022-01-11T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Using machine learning to generate recipes that</b> actually work | by ...", "url": "https://towardsdatascience.com/using-machine-learning-to-generate-recipes-that-actually-works-b2331c85ab72", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>using-machine-learning-to-generate-recipes-that</b>...", "snippet": "<b>GPT</b>-2. <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (<b>GPT</b>-2) is a so-called <b>transformer</b>. They learn from the training data how likely a word is to occur depending on the other words in the full text, but different words are given different weights, a process called attention. In this way, it can keep the context theoretically indefinitely. The way to use <b>GPT</b>-2 is to write a few words as a starter and let the <b>transformer</b> fill in what word is most likely to follow, then look at the new string and ...", "dateLastCrawled": "2022-02-02T18:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>GPT</b>-3 and <b>the Artificial Intelligence That Surrounds Us</b> | by R/GA | Medium", "url": "https://rga.medium.com/gpt-3-and-the-artificial-intelligence-that-surrounds-us-98572617fd05", "isFamilyFriendly": true, "displayUrl": "https://rga.medium.com/<b>gpt</b>-3-and-<b>the-artificial-intelligence-that-surrounds-us</b>...", "snippet": "By Nicol\u00e1s Rodr\u00edguez. OpenAI, the San Francisco-based AI lab, just released the third iteration of its <b>GPT</b> (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b>) model, or <b>GPT</b>-3 for short. After investing around $4.6 million, the program has shaken up every corner of the Internet, generating a mix of excitement and trepidation. But what is <b>GPT</b>-3, exactly?", "dateLastCrawled": "2022-01-23T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>The AI few days after GPT-3</b> - Ivan Moreira", "url": "https://ivanmoreira.org/blog/the-ai-few-days-after-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://ivanmoreira.org/blog/<b>the-ai-few-days-after-gpt-3</b>", "snippet": "On past July OpenAI released a beta test of one of the most AI model called <b>GPT</b>-3 (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3), that uses Deep <b>Learning</b> (part of a broader a <b>machine</b> <b>learning</b> method, based on neural networks. This transformational system is more sophisticated, and the full version has a capacity of 175 billion ML parameters when the older version only has 17 billion, less than 10% of this new one. <b>GPT</b>-3 is a turning point in AI field and will bring to us a new era of AI computing ...", "dateLastCrawled": "2022-01-26T18:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Is <b>GPT</b>-3 the first Artificial General Intelligence? | by Bruce H ...", "url": "https://chatbotslife.com/is-gpt-3-the-adam-of-natural-language-cf59656456f2", "isFamilyFriendly": true, "displayUrl": "https://chatbotslife.com/is-<b>gpt</b>-3-the-adam-of-natural-language-cf59656456f2", "snippet": "The <b>GPT</b>-3 (<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b>-3) ... The API runs the <b>pre-trained</b> <b>GPT</b>-3 model family for a wide range of NLP tasks [3]. Unlike the usual AI community practice, the <b>GPT</b>-3 model weights are not released to the public. Conclusion . OpenAI has long asserted that immense computational horsepower in conjunction with reinforcement <b>learning</b> is a necessary step on the road to AGI, or AI that can learn any task a human can [14]. The fathers of AI 2.0, such as Yoshua Bengio and Yann ...", "dateLastCrawled": "2022-01-08T13:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How close is <b>GPT</b>-3 to Artificial General Intelligence? | by Bruce H ...", "url": "https://towardsdatascience.com/how-close-is-gpt-3-to-artificial-general-intelligence-cb057a8c503d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-close-is-<b>gpt</b>-3-to-artificial-general-intelligence...", "snippet": "The <b>GPT</b>-3 (<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b>-3) is OpenAI\u2019s most massive natural language prediction (NLP) model to date (available to the public June 2020). <b>GPT</b>-3 has approximately 185 billion parameters. In contrast, the human brain has approximately 86 billion neurons with on the average 7,000 synapses per neuron [2,3]; Comparing apples to oranges, the human brain has about 60 trillion parameters or about 300x more parameters than <b>GPT</b>-3. Note: If 10% of the human brain capacity is ...", "dateLastCrawled": "2022-01-27T08:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Transformer</b> Neural Network In Deep <b>Learning</b> - Overview - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/transformer-neural-network-in-deep-learning-overview/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>transformer</b>-neural-network-in-deep-<b>learning</b>-overview", "snippet": "The successor to <b>GPT</b> and GPT2 is the GPT3, and is one of the most controversial <b>pre-trained</b> models, by OpenAI the large-scale <b>transformer</b>-based language model has been trained on 175 billion parameters, which is 10 times more than any previous non-sparsed language model. The model has been trained to achieve strong performance on much NLP dataset, including task translation, answering questions, as well as several other tasks.", "dateLastCrawled": "2022-02-02T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Illustrated <b>GPT</b>-2 (Visualizing <b>Transformer</b> Language Models) \u2013 Jay ...", "url": "https://jalammar.github.io/illustrated-gpt2/", "isFamilyFriendly": true, "displayUrl": "https://jalammar.github.io/illustrated-<b>gpt</b>2", "snippet": "Discussions: Hacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments) Translations: Korean, Russian This year, we saw a dazzling application of <b>machine</b> <b>learning</b>. The OpenAI <b>GPT</b>-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The <b>GPT</b>-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only <b>transformer</b>.", "dateLastCrawled": "2022-01-30T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) GALAXY: A <b>Generative</b> <b>Pre-trained</b> Model for Task-Oriented Dialog ...", "url": "https://www.researchgate.net/publication/356631427_GALAXY_A_Generative_Pre-trained_Model_for_Task-Oriented_Dialog_with_Semi-Supervised_Learning_and_Explicit_Policy_Injection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356631427_GALAXY_A_<b>Generative</b>_<b>Pre-trained</b>...", "snippet": "GALAXY: A <b>Generative</b> <b>Pre-trained</b> Model f or T ask-Oriented Dialog with Semi-Supervised <b>Learning</b> and Explicit Policy Injection W anwei He 1 * \u2020 , Yinpei Dai 2 * , Yinhe Zheng 2 , Y uchuan Wu 2 ...", "dateLastCrawled": "2022-01-29T16:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to perform Text Summarization with Python, HuggingFace Transformers ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2020/12/21/easy-text-summarization-with-hugging...", "snippet": "A <b>Transformer</b> is a <b>machine</b> <b>learning</b> architecture that combines an encoder with a decoder and jointly learns them, allowing us to convert input sequences (e.g. phrases) into some intermediate format before we convert it back into human-understandable format. A human <b>analogy</b> would be two translators which both speak some imaginary language and a human-interpretable one, such as German and French. The first translator can translate French into the imaginary language; the second then has learned ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(gpt (generative pre-trained transformer))  is like +(someone who hits balls back at you)", "+(gpt (generative pre-trained transformer)) is similar to +(someone who hits balls back at you)", "+(gpt (generative pre-trained transformer)) can be thought of as +(someone who hits balls back at you)", "+(gpt (generative pre-trained transformer)) can be compared to +(someone who hits balls back at you)", "machine learning +(gpt (generative pre-trained transformer) AND analogy)", "machine learning +(\"gpt (generative pre-trained transformer) is like\")", "machine learning +(\"gpt (generative pre-trained transformer) is similar\")", "machine learning +(\"just as gpt (generative pre-trained transformer)\")", "machine learning +(\"gpt (generative pre-trained transformer) can be thought of as\")", "machine learning +(\"gpt (generative pre-trained transformer) can be compared to\")"]}