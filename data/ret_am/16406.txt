{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "The most popular <b>sequence-to-sequence</b> <b>task</b> is translation: usually, from <b>one</b> natural <b>language</b> <b>to another</b>. In the last couple of years, commercial systems became surprisingly good at machine translation - check out, for example, Google Translate, Yandex Translate, DeepL Translator, Bing Microsoft Translator.Today we will learn about the core part of these systems.", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "Often, <b>translating</b> from <b>one</b> <b>language</b> <b>to another</b> requires learning copious complex features (<b>one</b>-to-many, many-to-<b>one</b>, many-to-many mappings, lexical dependencies, word alignment [2], etc). This is drastically different to image classification (i.e. Fixed size input \u2192 Class/Label) or a sentiment analysis problem (i.e. Arbitrary length input \u2192 Class/Label).", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Natural <b>Language</b> Processing with Deep Learning CS224N/Ling284", "url": "https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture07-nmt.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture07-nmt.pdf", "snippet": "Machine Translation (MT) is the <b>task</b> of <b>translating</b> a sentence x from <b>one</b> <b>language</b> (the source <b>language</b>) to a sentence y in <b>another</b> <b>language</b> (the target <b>language</b>). x: L&#39;hommeestn\u00e9 libre, et partoutil estdans les fers y: Man is born free, but everywhere he is in chains \u2013Rousseau. The early history of MT: 1950s \u2022Machine translation research began in the early 1950s on machines less powerful than high school calculators (before term \u201cA.I.\u201d coined!) \u2022Concurrent with foundational work ...", "dateLastCrawled": "2022-02-01T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Neural Machine Translation Using <b>Sequence to Sequence</b> Model | by Aditya ...", "url": "https://medium.com/geekculture/neural-machine-translation-using-sequence-to-sequence-model-164a5905bcd7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/neural-machine-translation-using-<b>sequence-to-sequence</b>...", "snippet": "M achine translation is <b>one</b> of earliest and challenging <b>task</b> of computers due to fluidity of human <b>language</b>. It is simply an automatic translation of text from <b>one</b> <b>language</b> <b>to another</b>.", "dateLastCrawled": "2022-02-03T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Natural <b>Language</b> Processing on Hinglish \u2013 DataDev", "url": "https://itsdatalytical.wordpress.com/2021/04/13/natural-language-processing-on-hinglish/", "isFamilyFriendly": true, "displayUrl": "https://itsdatalytical.wordpress.com/2021/04/13/natural-<b>language</b>-processing-on-hinglish", "snippet": "<b>Sequence-to-sequence</b> learning (Seq2Seq) is about training models to convert sequences from <b>one</b> domain (e.g. sentences in English) to sequences in <b>another</b> domain (e.g. the same sentences translated to French). Seq2Seq is a method of encoder-decoder based machine translation and <b>language</b> processing that maps an input of sequence to an output of sequence with a tag and attention value. The idea is to use 2 RNNs that will work together with a special token and try to predict the next state ...", "dateLastCrawled": "2022-01-12T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sequence-to-Sequence</b> Models Can Directly Translate Foreign Speech", "url": "https://www.isca-speech.org/archive/pdfs/interspeech_2017/weiss17_interspeech.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.isca-speech.org/archive/pdfs/interspeech_2017/weiss17_interspeech.pdf", "snippet": "chitecture that directly translates speech in <b>one</b> <b>language</b> into text in <b>another</b>. The model does not explicitly transcribe the speech into text in the source <b>language</b>, nor does it require supervision from the ground truth source <b>language</b> transcription during train-ing. We apply a slightly modi\ufb01ed <b>sequence-to-sequence</b> with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex <b>task</b>, illustrating the power of ...", "dateLastCrawled": "2021-10-24T07:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Natural Language Processing with</b> Deep Learning CS224N/Ling284", "url": "http://clgiles.ist.psu.edu/IST597/materials/slides/lect9/nmt.pptx", "isFamilyFriendly": true, "displayUrl": "clgiles.ist.psu.edu/IST597/materials/slides/lect9/nmt.pptx", "snippet": "is the <b>task</b> of <b>translating</b> a sentence . x . from <b>one</b> <b>language</b> (the source <b>language</b>) to a sentence . y . in <b>another</b> <b>language</b> (the targetlanguage). x: L&#39;homme . est n\u00e9 . libre, et partout . il . est dans les. fers. y: Man is . born free, but everywhere he . is in. chains \u2013 Rousseau. 4", "dateLastCrawled": "2022-02-03T10:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Hi, robot: Why robotics and <b>language</b> need each other", "url": "https://knowablemagazine.org/article/technology/2020/teaching-robots-to-talk", "isFamilyFriendly": true, "displayUrl": "https://knowablemagazine.org/article/technology/2020/teaching-robots-to-talk", "snippet": "The paper that introduced the R2R <b>task</b> also presented a simple machine-learning algorithm to tackle it. Implementing a \u201c<b>sequence-to-sequence</b>\u201d architecture, this system takes in a sequence of words and outputs a sequence of action commands, rather <b>like</b> <b>translating</b> from <b>one</b> <b>language</b> <b>to another</b>. In between is a neural network, an arrangement of simple computing elements roughly mimicking the brain\u2019s wiring. The network has sub-networks specialized for handling <b>language</b> (the instructions ...", "dateLastCrawled": "2022-02-03T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Udacity&amp;#39;s Natural <b>Language</b> Processing Nanodegree", "url": "https://www.linkedin.com/pulse/udacitys-natural-language-processing-nanodegree-marius-vadeika", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/udacitys-natural-<b>language</b>-processing-nanodegree-marius...", "snippet": "<b>Translating</b> from <b>one</b> <b>language</b> <b>to another</b> is really not a word to word, but rather a <b>sequence to sequence</b> problem (when you translate a sentence from english to french, the sequence of words and ...", "dateLastCrawled": "2021-05-10T08:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>CHAPTER</b> 11", "url": "https://attapol.github.io/compling/slides/slp11.pdf", "isFamilyFriendly": true, "displayUrl": "https://attapol.github.io/compling/slides/slp11.pdf", "snippet": "MT late from <b>one</b> <b>language</b> <b>to another</b>. Of course translation, in its full generality, such as the translation of literature, or poetry, is a dif\ufb01cult, fascinating, and intensely human endeavor, as rich as any other area of human creativity. Machine translation in its present form therefore focuses on a number of very practical tasks. Perhaps the most common current use of machine translation is information for information access. We might want to translate some instructions on the web ...", "dateLastCrawled": "2022-01-21T05:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "Often, <b>translating</b> from <b>one</b> <b>language</b> <b>to another</b> requires learning copious complex features (<b>one</b>-to-many, many-to-<b>one</b>, many-to-many mappings, lexical dependencies, word alignment [2], etc). This is drastically different to image classification (i.e. Fixed size input \u2192 Class/Label) or a sentiment analysis problem (i.e. Arbitrary length input \u2192 Class/Label).", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "The most popular <b>sequence-to-sequence</b> <b>task</b> is translation: usually, from <b>one</b> natural <b>language</b> <b>to another</b>. In the last couple of years, commercial systems became surprisingly good at machine translation - check out, for example, Google Translate, Yandex Translate, DeepL Translator, Bing Microsoft Translator.Today we will learn about the core part of these systems.", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequence-to-Sequence</b> Models Can Directly Translate Foreign Speech ...", "url": "https://www.arxiv-vanity.com/papers/1703.08581/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.08581", "snippet": "We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in <b>one</b> <b>language</b> into text in <b>another</b>. The model does not explicitly transcribe the speech into text in the source <b>language</b>, nor does it require supervision from the ground truth source <b>language</b> transcription during training. We apply a slightly modified <b>sequence-to-sequence</b> with attention architecture that has previously been used for speech recognition and show that it can be repurposed ...", "dateLastCrawled": "2021-10-17T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sequence-to-Sequence</b> Models Can Directly Translate Foreign Speech", "url": "https://www.isca-speech.org/archive/pdfs/interspeech_2017/weiss17_interspeech.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.isca-speech.org/archive/pdfs/interspeech_2017/weiss17_interspeech.pdf", "snippet": "chitecture that directly translates speech in <b>one</b> <b>language</b> into text in <b>another</b>. The model does not explicitly transcribe the speech into text in the source <b>language</b>, nor does it require supervision from the ground truth source <b>language</b> transcription during train-ing. We apply a slightly modi\ufb01ed <b>sequence-to-sequence</b> with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex <b>task</b>, illustrating the power of ...", "dateLastCrawled": "2021-10-24T07:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Why do Transformers yield Superior <b>Sequence to Sequence</b>(Seq2Seq)Results ...", "url": "https://medium.com/saarthi-ai/transformers-attention-based-seq2seq-machine-translation-a28940aaa4fe", "isFamilyFriendly": true, "displayUrl": "https://medium.com/saarthi-ai/<b>transformer</b>s-attention-based-seq2seq-machine-translation...", "snippet": "The network now attends over the preceding decoder states and does a <b>similar</b> <b>task</b> as decoder hidden state which we see in trivial Machine Translation architectures.", "dateLastCrawled": "2022-01-28T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Neural Machine Translation Using <b>Sequence to Sequence</b> Model | by Aditya ...", "url": "https://medium.com/geekculture/neural-machine-translation-using-sequence-to-sequence-model-164a5905bcd7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/neural-machine-translation-using-<b>sequence-to-sequence</b>...", "snippet": "M achine translation is <b>one</b> of earliest and challenging <b>task</b> of computers due to fluidity of human <b>language</b>. It is simply an automatic translation of text from <b>one</b> <b>language</b> <b>to another</b>.", "dateLastCrawled": "2022-02-03T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Multi-<b>task</b> <b>Sequence to Sequence Learning</b> | DeepAI", "url": "https://deepai.org/publication/multi-task-sequence-to-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/multi-<b>task</b>-<b>sequence-to-sequence-learning</b>", "snippet": "Despite the popularity of multi-<b>task</b> learning and <b>sequence to sequence learning</b>, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only <b>one</b> recent publication by Dong et al. which applies a seq2seq models for machine translation, where the goal is to translate from <b>one</b> <b>language</b> to multiple languages. In this work, we propose three MTL approaches that complement <b>one</b> <b>another</b>: (a) the <b>one</b>-to-many approach \u2013 for tasks that can have an ...", "dateLastCrawled": "2022-02-02T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "NMT based <b>Similar</b> <b>Language</b> Translation for Hindi - Marathi", "url": "https://aclanthology.org/2020.wmt-1.48.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.wmt-1.48.pdf", "snippet": "2020 <b>task</b>, <b>similar</b> <b>language</b> translation. We experimented with attention based recurrent neural network architecture (seq2seq) for this <b>task</b>. We explored the use of different lin- guistic features like POS and Morph along with back translation for Hindi-Marathi and Marathi-Hindi machine translation. 1 Introduction Machine Translation (MT) is the \ufb01eld of Natural <b>Language</b> Processing which aims to translate a text from <b>one</b> natural <b>language</b> (i.e Hindi) <b>to another</b> (i.e Marathi). The meaning of ...", "dateLastCrawled": "2022-01-28T19:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Translation in NLP: Examples, Flow</b> &amp; Models | upGrad blog", "url": "https://www.upgrad.com/blog/machine-translation-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/machine-translation-in-nlp", "snippet": "In other words, machine translation functions by employing an application that helps translate text from <b>one</b> input <b>language</b> <b>to another</b>. There are four different types of machine translation in NLP: statistical machine translation, rule-based machine translation, hybrid machine translation, and neural machine translation. The main advantage of machine translation is its delivery of an effective combination of both speed and cost-effectiveness.", "dateLastCrawled": "2022-02-02T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Language</b> <b>Translation</b> with RNNs. Build a recurrent neural network (RNN ...", "url": "https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>language</b>-<b>translation</b>-with-rnns-d84d43b40571", "snippet": "The ability to communicate with <b>one</b> <b>another</b> is a fundamental part of being human. There are nearly 7,000 different languages worldwide. As our world becomes increasingly connected, <b>language</b> <b>translation</b> provides a critical cultural and economic bridge between people from different countries and ethnic groups. Some of the more obvious use-cases include: business: international trade, investment, contracts, finance; commerce: travel, purchase of foreign goods and services, customer support ...", "dateLastCrawled": "2022-02-02T17:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "The most popular <b>sequence-to-sequence</b> <b>task</b> is translation: usually, from <b>one</b> natural <b>language</b> <b>to another</b>. In the last couple of years, commercial systems became surprisingly good at machine translation - check out, for example, Google Translate, Yandex Translate, DeepL Translator, Bing Microsoft Translator.Today we will learn about the core part of these systems.", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "Often, <b>translating</b> from <b>one</b> <b>language</b> <b>to another</b> requires learning copious complex features (<b>one</b>-to-many, many-to-<b>one</b>, many-to-many mappings, lexical dependencies, word alignment [2], etc). This is drastically different to image classification (i.e. Fixed size input \u2192 Class/Label) or a sentiment analysis problem (i.e. Arbitrary length input \u2192 Class/Label).", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Direct speech-to-speech translation with a <b>sequence-to-sequence</b> model ...", "url": "https://deepai.org/publication/direct-speech-to-speech-translation-with-a-sequence-to-sequence-model", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/direct-speech-to-speech-translation-with-a-sequence-to...", "snippet": "We address the <b>task</b> of speech-to-speech translation (S2ST): <b>translating</b> speech in <b>one</b> <b>language</b> into speech in <b>another</b>. This application is highly beneficial for breaking down communication barriers between people who do not share a common <b>language</b>. Specifically, we investigate whether it is possible to train model to accomplish this <b>task</b> directly, without relying on an intermediate text representation. This is in contrast to conventional S2ST systems which are often broken down into three ...", "dateLastCrawled": "2021-12-17T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Direct speech-to-speech translation with a <b>sequence-to-sequence</b> model ...", "url": "https://www.arxiv-vanity.com/papers/1904.06037/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1904.06037", "snippet": "We present an attention-based <b>sequence-to-sequence</b> neural network which <b>can</b> directly translate speech from <b>one</b> <b>language</b> into speech in <b>another</b> <b>language</b>, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in <b>another</b> <b>language</b>, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source ...", "dateLastCrawled": "2022-01-11T15:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Direct Speech-to-Speech Translation with a <b>Sequence-to-Sequence</b> Model", "url": "https://www.isca-speech.org/archive/pdfs/interspeech_2019/jia19_interspeech.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.isca-speech.org/archive/pdfs/interspeech_2019/jia19_interspeech.pdf", "snippet": "Direct speech-to-speech translation with a <b>sequence-to-sequence</b> model Ye Jia *, Ron J. Weiss , Fadi Biadsy, Wolfgang Macherey, Melvin Johnson , Zhifeng Chen, Yonghui Wu Google fjiaye,ronwg@google.com Abstract We present an attention-based <b>sequence-to-sequence</b> neural net-work which <b>can</b> directly translate speech from <b>one</b> <b>language</b> into speech in <b>another</b> <b>language</b>, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into ...", "dateLastCrawled": "2021-12-22T16:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Differences between Autoregressive, Autoencoding and Sequence</b>-to ...", "url": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive-autoencoding-and-sequence-to-sequence-models-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive...", "snippet": "Last Updated on 2 January 2021. Transformers have changed the application of Machine Learning in Natural <b>Language</b> Processing. They have replaced LSTMs as state-of-the-art (SOTA) approaches in the wide variety of <b>language</b> and text related tasks that <b>can</b> be resolved by Machine Learning.. However, as we have seen before when paradigms shift towards different approaches, <b>one</b> breakthrough spawns a large amount of research and hence a large amount of small improvements.", "dateLastCrawled": "2022-02-02T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Sequence to Sequence Learning with Neural Networks</b>", "url": "https://wandb.ai/authors/seq2seq/reports/Sequence-to-Sequence-Learning-with-Neural-Networks--Vmlldzo0Mzg0MTI", "isFamilyFriendly": true, "displayUrl": "https://wandb.ai/authors/seq2seq/reports/<b>Sequence-to-Sequence-Learning-with</b>-Neural...", "snippet": "Introduction. In the age of attention and transformers, I <b>thought</b> writing a simple report on <b>sequence to sequence</b> modelling thinking it would be a good starting point for a lot of people. In this article I will try declassifying the paper <b>Sequence to Sequence Learning with Neural Networks</b> by Ilya Sutskever et. al. Here in this paper, the authors have presented an end to end learning system that helps in <b>translating</b> <b>one</b> <b>language</b> into <b>another</b>.", "dateLastCrawled": "2022-01-13T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Transformers BART Model Explained for Text Summarization", "url": "https://www.projectpro.io/article/transformers-bart-model-explained/553", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/transformers-bart-model-explained/553", "snippet": "This type of model is relevant for machine translation (<b>translating</b> text from <b>one</b> <b>language</b> <b>to another</b>), question-answering (producing answers for a given question on a specific corpus), text summarization (giving a summary of or paraphrasing a long text document), or sequence classification (categorizing input text sentences or tokens). <b>Another</b> <b>task</b> is sentence entailment which, given two or more sentences, evaluates whether the sentences are logical extensions or are logically related to a ...", "dateLastCrawled": "2022-02-03T05:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Bouquet of <b>Sequence to Sequence</b> Architectures for Implementing ...", "url": "https://medium.com/analytics-vidhya/a-bouquet-of-sequence-to-sequence-architectures-for-implementing-machine-translation-5d13b286df5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-bouquet-of-<b>sequence-to-sequence</b>-architectures...", "snippet": "Machine Translation, as the name suggests, is a machine learning model that would help us convert text from <b>one</b> <b>language</b> <b>to another</b>. In this article, we will look at <b>translating</b> English sentences ...", "dateLastCrawled": "2021-08-20T07:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "translation from <b>one</b> <b>language</b> <b>to another</b> is called", "url": "https://gruporogel.com/21jm0/translation-from-one-language-to-another-is-called", "isFamilyFriendly": true, "displayUrl": "https://gruporogel.com/21jm0/translation-from-<b>one</b>-<b>language</b>-<b>to-another</b>-is-called", "snippet": "Translation is about <b>translating</b> ideas expressed in writing from <b>one</b> <b>language</b> into <b>another</b>, and interpretation is an oral <b>task</b>, which entails that the interpreter is present on the location. <b>One</b> of my favorite examples of this is the Japanese term arigata-meiwaku. in my project, there is a content section. 4. In geometry, a translation is the shifting of a figure from <b>one</b> place <b>to another</b> without rotating, reflecting or changing its size. NLP combines the power of linguistics and computer ...", "dateLastCrawled": "2022-01-15T11:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "Often, <b>translating</b> from <b>one</b> <b>language</b> <b>to another</b> requires learning copious complex features (<b>one</b>-to-many, many-to-<b>one</b>, many-to-many mappings, lexical dependencies, word alignment [2], etc). This is drastically different to image classification (i.e. Fixed size input \u2192 Class/Label) or a sentiment analysis problem (i.e. Arbitrary length input \u2192 Class/Label).", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Chapter 7: Text Translation Using <b>Sequence-to-Sequence</b> Neural Networks ...", "url": "https://w3sdev.com/chapter-7-text-translation-using-sequence-to-sequence-neural-networks-hands-on-natural-language-processing-with-pytorch-1-x.html", "isFamilyFriendly": true, "displayUrl": "https://w3sdev.com/chapter-7-text-translation-using-<b>sequence-to-sequence</b>-neural...", "snippet": "Figure 7.3 \u2013 <b>Sequence-to-sequence</b> modeling for translation. To train a <b>sequence-to-sequence</b> model that captures the context of the input sentence and translates this into an output sentence, we will essentially train two smaller models that allow us to do this:. An encoder model, which captures the context of our sentence and outputs it as a single context vector; A decoder, which takes the context vector representation of our original sentence and translates this into a different <b>language</b> ...", "dateLastCrawled": "2021-11-15T06:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why do Transformers yield Superior <b>Sequence to Sequence</b>(Seq2Seq)Results ...", "url": "https://medium.com/saarthi-ai/transformers-attention-based-seq2seq-machine-translation-a28940aaa4fe", "isFamilyFriendly": true, "displayUrl": "https://medium.com/saarthi-ai/<b>transformer</b>s-attention-based-seq2seq-machine-translation...", "snippet": "Attention models, just like humans, try to translate a part of the sentence at a time which makes them much more efficient when <b>compared</b> to any RNN based seq2seq model. Attention idea is <b>one</b> of ...", "dateLastCrawled": "2022-01-28T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machine Translation. This story includes pre-neural machine\u2026 | by ...", "url": "https://medium.com/analytics-vidhya/translation-seq2seq-attention-4f8b23f33965", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/translation-seq2seq-attention-4f8b23f33965", "snippet": "Machine translation (MT) is the <b>task</b> of <b>translating</b> a sentence x from <b>one</b> <b>language</b> (the source <b>language</b>) to a sentence y in <b>another</b> <b>language</b> (the target <b>language</b>). Machine translation research ...", "dateLastCrawled": "2022-01-30T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Natural Language Processing with</b> Deep Learning CS224N/Ling284", "url": "http://clgiles.ist.psu.edu/IST597/materials/slides/lect9/nmt.pptx", "isFamilyFriendly": true, "displayUrl": "clgiles.ist.psu.edu/IST597/materials/slides/lect9/nmt.pptx", "snippet": "is the <b>task</b> of <b>translating</b> a sentence . x . from <b>one</b> <b>language</b> (the source <b>language</b>) to a sentence . y . in <b>another</b> <b>language</b> (the targetlanguage). x: L&#39;homme . est n\u00e9 . libre, et partout . il . est dans les. fers. y: Man is . born free, but everywhere he . is in. chains \u2013 Rousseau. 4", "dateLastCrawled": "2022-02-03T10:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Week8-Lecture16-Seq2Seq.pdf - MSBD 6000H Natural <b>Language</b> Processing ...", "url": "https://www.coursehero.com/file/92586936/Week8-Lecture16-Seq2Seqpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/92586936/Week8-Lecture16-Seq2Seqpdf", "snippet": "Machine Translation \u2022 Machine Translation (MT) is the <b>task</b> of <b>translating</b> a sentence x from <b>one</b> <b>language</b> (the source <b>language</b>) to a sentence y in <b>another</b> <b>language</b> (the target <b>language</b>). \u2022 1950s: Early Machine Translation \u2013 Mostly Russian \u2192 English (motivated by the Cold War!) \u2013 Systems were mostly rule-based, using a bilingual dictionary to map Russian words to their English counterparts 6 Source: https:// youtu.be/K-HfpsHPmvw. 1990s-2010s: Statistical Machine Translation \u2022 Core ...", "dateLastCrawled": "2022-02-01T12:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "&quot;Attention&quot; for Neural Machine Translation (NMT) without pain", "url": "https://www.linkedin.com/pulse/attention-neural-machine-translation-nmt-without-pain-sobh-phd", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/attention-neural-machine-translation-nmt-without-pain...", "snippet": "Machine Translation (MT) is the <b>task</b> of <b>translating</b> a sentence x from <b>one</b> <b>language</b> (the source <b>language</b>) to a sentence y in <b>another</b> <b>language</b> (the target <b>language</b>). Machine Translation research ...", "dateLastCrawled": "2021-11-23T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Translation</b>. A literature review on Statistical MT\u2026 | by Chao ...", "url": "https://towardsdatascience.com/machine-translation-b0f0dbcef47c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-translation</b>-b0f0dbcef47c", "snippet": "<b>Machine Translation</b> is the <b>task</b> of <b>translating</b> a sentence from <b>one</b> <b>language</b> (the source <b>language</b>) to a sentence in <b>another</b> <b>language</b> (the target <b>language</b>). It is the sub-field of computational linguistics that aims to utilize computing devices to automatically translate text from <b>one</b> <b>language</b> <b>to another</b>. <b>Machine Translation</b> research began in the early 1950s (Cold War period). During that time, there is a need to translate Russian documents into English. Since there are not many Russian ...", "dateLastCrawled": "2022-01-31T10:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Real Time Direct Speech-to-Speech Translation", "url": "https://www.irjet.net/archives/V9/i1/IRJET-V9I1104.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.irjet.net/archives/V9/i1/IRJET-V9I1104.pdf", "snippet": "aforementioned characteristics while <b>translating</b> from <b>one</b> <b>language</b> <b>to another</b>. Furthermore, the cascaded three-stage machine translation systems have the potential to exacerbate the errors that occur with each successive phase. Speech-To-Speech systems, on the other hand, have a benefit over cascaded TT systems since they use a single step method that requires less computer power and has better inference latency. 1.3 Objective The main objectives of the system are: To create a login system ...", "dateLastCrawled": "2022-02-03T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "translation from <b>one</b> <b>language</b> <b>to another</b> is called", "url": "https://gruporogel.com/21jm0/translation-from-one-language-to-another-is-called", "isFamilyFriendly": true, "displayUrl": "https://gruporogel.com/21jm0/translation-from-<b>one</b>-<b>language</b>-<b>to-another</b>-is-called", "snippet": "Translation is about <b>translating</b> ideas expressed in writing from <b>one</b> <b>language</b> into <b>another</b>, and interpretation is an oral <b>task</b>, which entails that the interpreter is present on the location. <b>One</b> of my favorite examples of this is the Japanese term arigata-meiwaku. in my project, there is a content section. 4. In geometry, a translation is the shifting of a figure from <b>one</b> place <b>to another</b> without rotating, reflecting or changing its size. NLP combines the power of linguistics and computer ...", "dateLastCrawled": "2022-01-15T11:11:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Week 3: <b>Sequence to sequence</b> architectures. <b>Sequence to sequence</b> models Language translation for example; Image captioning, caption an image; Picking the most likely model <b>Machine</b> Transation Model Split into a model encoding the sentence; and then a language model. Calculate the probability of an English sentence conditioned on a French sentence.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Week 1 Quiz - Introduction to deep <b>learning</b>. What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI is powering personal devices in our homes and offices, similar to electricity. Through the \u201csmart grid\u201d, AI is delivering a new wave of electricity. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Note: Andrew ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(translating one language to another)", "+(sequence-to-sequence task) is similar to +(translating one language to another)", "+(sequence-to-sequence task) can be thought of as +(translating one language to another)", "+(sequence-to-sequence task) can be compared to +(translating one language to another)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}