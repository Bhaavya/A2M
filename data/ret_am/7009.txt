{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Gradient</b>-Based Optimizers in Deep Learning - Analytics Vidhya", "url": "https://www.analyticsvidhya.com/blog/2021/06/complete-guide-to-gradient-based-optimizers/", "isFamilyFriendly": true, "displayUrl": "https://www.analyticsvidhya.com/blog/2021/06/complete-guide-to-<b>gradient</b>-based-optimizers", "snippet": "Batch <b>Gradient</b> <b>Descent</b> or Vanilla <b>Gradient</b> <b>Descent</b> or <b>Gradient</b> <b>Descent</b> (GD) <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) Mini batch <b>Gradient</b> <b>Descent</b> (MB-GD) 4. Challenges with all Types of <b>Gradient</b>-based Optimizers . Role of an Optimizer. As discussed in the introduction part of the article, Optimizers update the parameters of neural networks such as weights and learning rate to minimize the loss function. Here, the loss function acts as a guide to the terrain telling optimizer if it is moving in the ...", "dateLastCrawled": "2022-01-30T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Gradient</b> <b>Descent</b>: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-<b>descent</b>", "snippet": "Instead of climbing <b>up</b> <b>a hill</b>, think of <b>gradient</b> <b>descent</b> as <b>hiking</b> down to the bottom of a valley. This is a better analogy because it is a minimization algorithm that minimizes a given function. The equation below describes what <b>gradient</b> <b>descent</b> does: b is the next position of our climber, while a represents his current position. The minus sign refers to the minimization part of <b>gradient</b> <b>descent</b>. The gamma in the middle is a waiting factor and the <b>gradient</b> term ( \u0394f(a) ) is simply the ...", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "CS231n Convolutional Neural Networks for Visual Recognition", "url": "https://cs231n.github.io/optimization-1/", "isFamilyFriendly": true, "displayUrl": "https://cs231n.github.io/optimization-1", "snippet": "This process is called <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) (or also sometimes on-line <b>gradient</b> <b>descent</b>). This is relatively less common to see because in practice due to vectorized code optimizations it can be computationally much more efficient to evaluate the <b>gradient</b> for 100 examples, than the <b>gradient</b> for one example 100 times. Even though <b>SGD</b> technically refers to using a single example at a time to evaluate the <b>gradient</b>, you will hear people use the term <b>SGD</b> even when referring to mini ...", "dateLastCrawled": "2022-01-29T18:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "\u201cIs it BAD?\u201d When the <b>Gradient</b> <b>Descent</b> algorithm meets high dimensional ...", "url": "https://siyuan1202.medium.com/is-it-bad-when-the-gradient-descent-algorithm-meets-high-dimensional-data-b4fefc63a000", "isFamilyFriendly": true, "displayUrl": "https://siyuan1202.medium.com/is-it-bad-when-the-<b>gradient</b>-<b>descent</b>-algorithm-meets-high...", "snippet": "The analogy roughly depicts how the <b>gradient</b> <b>descent</b> algorithm came <b>up</b>. Since when solving real-world problems, people always want the \u201cthings to be better\u201d. Namely, we might strive to either pu rsue more profits or fewer errors. Those purposes, similar to finding the bottom lake, can all be considered as the process of optimization. In the machine learning field, the focus is moving towards a loss function, which helps evaluate how well specific algorithm models the training data ...", "dateLastCrawled": "2022-01-27T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient</b> <b>descent</b> - Hashnode", "url": "https://hashnode.com/post/gradient-descent-ckw0ewd5f00xo0as10hy2971i", "isFamilyFriendly": true, "displayUrl": "https://hashnode.com/post/<b>gradient</b>-<b>descent</b>-ckw0ewd5f00xo0as10hy2971i", "snippet": "<b>Gradient</b> <b>Descent</b> is an optimization algorithm for finding a local minimum of a differentiable function. <b>Gradient</b> <b>descent</b> is simply used in machine learning to find the values of a function\u2019s parameters (coefficients) that minimize a cost function as far as possible. Imagine a blindfolded man who wants to climb to the top of <b>a hill</b> with the fewest steps along the way as possible. He might start climbing the <b>hill</b> by taking really big steps in the steepest direction, which he can do as long ...", "dateLastCrawled": "2022-01-21T03:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Complete Glossary of Keras Optimizers and When to Use Them (With Code)", "url": "https://analyticsarora.com/complete-glossary-of-keras-optimizers-and-when-to-use-them-with-code/", "isFamilyFriendly": true, "displayUrl": "https://analyticsarora.com/complete-glossary-of-keras-optimizers-and-when-to-use-them...", "snippet": "While a typical <b>gradient</b> <b>descent</b> algorithm would have us calculate the <b>gradient</b> with respect to each of the features available in the dataset, the \u201c<b>stochastic</b>\u201d nature of <b>SGD</b> helps us deal with this in a very efficient manner. Now you can imagine how computationally expensive it is to calculate the <b>gradient</b> with respect to each feature, given that Neural Networks have hundreds or even thousands of features at hand. This introduces a huge overhead, practically making <b>gradient</b> <b>descent</b> ...", "dateLastCrawled": "2022-01-28T22:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "ISHAN MUKHOPADHYAY_432920010002.pdf - AIM TO PERFORM THE EXPERIMENT OF ...", "url": "https://www.coursehero.com/file/125951735/ISHAN-MUKHOPADHYAY-432920010002pdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/125951735/ISHAN-MUKHOPADHYAY-432920010002pdf", "snippet": "The equation below describes what <b>gradient</b> <b>descent</b> does: b is the next position of our climber, while a represents his current position. The minus sign refers to the minimization part of <b>gradient</b> <b>descent</b>. The gamma in the middle is a waiting factor and the <b>gradient</b> term ( \u0394f(a) ) is simply the direction of the steepest <b>d escent</b>. So this formula basically tells us the next position we need to go, which is the direction of the steepest <b>descent</b>. Let&#39;s look at another example to really drive ...", "dateLastCrawled": "2022-01-17T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Optimization: Learning to Minimize Cost</b> | Training Deep Networks | <b>InformIT</b>", "url": "https://www.informit.com/articles/article.aspx?p=2990401&seqNum=2", "isFamilyFriendly": true, "displayUrl": "https://<b>www.informit.com</b>/articles/article.aspx?p=2990401&amp;seqNum=2", "snippet": "FIGURE 8.6 An outline of the overall process for training a neural network with <b>stochastic</b> <b>gradient</b> <b>descent</b>. The entire dataset is shuffled and split into batches. Each batch is forward propagated through the network; the output \u0177 is compared to the ground truth y and the cost C is calculated; backpropagation calculates the gradients; and the model parameters w and b are updated. The next batch (indicated by a dashed line) is forward propagated, and so on until all of the batches have moved ...", "dateLastCrawled": "2022-01-29T04:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Towards a science of <b>human stories: using sentiment analysis</b> ... - DeepAI", "url": "https://deepai.org/publication/towards-a-science-of-human-stories-using-sentiment-analysis-and-emotional-arcs-to-understand-the-building-blocks-of-complex-social-systems", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/towards-a-science-of-human-stories-using-sentiment...", "snippet": "applies machine learning over a bag-of-words analysis to predict action and sex scenes using Naive Bayes (NB) and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>). Training data is crowd-sourced from two ratings of 500 word chunks on the survey platform Mechanical Turk (MT), and Cherny develops novel visualizations of the relationships between topics in chapters.", "dateLastCrawled": "2021-11-27T12:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "multivariable calculus - Why is <b>gradient</b> the direction of <b>steepest</b> ...", "url": "https://math.stackexchange.com/questions/223252/why-is-gradient-the-direction-of-steepest-ascent", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/223252", "snippet": "$\\begingroup$ I <b>like</b> this anwer a lot, and my intuition also was, that the <b>gradient</b> points in the direction of greatest change. But that is not the same as ascent. E.g. in the <b>gradient</b> <b>descent</b> algorithm one always uses the negative <b>gradient</b>, suggesting ascent but not <b>descent</b>.", "dateLastCrawled": "2022-02-03T06:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "\u201cIs it BAD?\u201d When the <b>Gradient</b> <b>Descent</b> algorithm meets high dimensional ...", "url": "https://siyuan1202.medium.com/is-it-bad-when-the-gradient-descent-algorithm-meets-high-dimensional-data-b4fefc63a000", "isFamilyFriendly": true, "displayUrl": "https://siyuan1202.medium.com/is-it-bad-when-the-<b>gradient</b>-<b>descent</b>-algorithm-meets-high...", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b>. <b>SGD</b> is standing the opposite of the Batch method, for it takes only one example data point for consideration. We usually fit the point into a neural network, and then calculate the <b>gradient</b>. The new <b>gradient</b> will be used for tuning the weights parameters. The process will produce again and again until we approximate ...", "dateLastCrawled": "2022-01-27T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Complete Glossary of Keras Optimizers and When to Use Them (With Code)", "url": "https://analyticsarora.com/complete-glossary-of-keras-optimizers-and-when-to-use-them-with-code/", "isFamilyFriendly": true, "displayUrl": "https://analyticsarora.com/complete-glossary-of-keras-optimizers-and-when-to-use-them...", "snippet": "While a typical <b>gradient</b> <b>descent</b> algorithm would have us calculate the <b>gradient</b> with respect to each of the features available in the dataset, the \u201c<b>stochastic</b>\u201d nature of <b>SGD</b> helps us deal with this in a very efficient manner. Now you can imagine how computationally expensive it is to calculate the <b>gradient</b> with respect to each feature, given that Neural Networks have hundreds or even thousands of features at hand. This introduces a huge overhead, practically making <b>gradient</b> <b>descent</b> ...", "dateLastCrawled": "2022-01-28T22:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Optimization: Learning to Minimize Cost</b> | Training Deep Networks | <b>InformIT</b>", "url": "https://www.informit.com/articles/article.aspx?p=2990401&seqNum=2", "isFamilyFriendly": true, "displayUrl": "https://<b>www.informit.com</b>/articles/article.aspx?p=2990401&amp;seqNum=2", "snippet": "FIGURE 8.6 An outline of the overall process for training a neural network with <b>stochastic</b> <b>gradient</b> <b>descent</b>. The entire dataset is shuffled and split into batches. Each batch is forward propagated through the network; the output \u0177 is compared to the ground truth y and the cost C is calculated; backpropagation calculates the gradients; and the model parameters w and b are updated. The next batch (indicated by a dashed line) is forward propagated, and so on until all of the batches have moved ...", "dateLastCrawled": "2022-01-29T04:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The Equation of Knowledge: From Bayes&#39; Rule to a Unified Philosophy of ...", "url": "https://dokumen.pub/the-equation-of-knowledge-from-bayes-rule-to-a-unified-philosophy-of-science-1nbsped-0367428156-9780367428150.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen.pub</b>/the-equation-of-knowledge-from-bayes-rule-to-a-unified-philosophy...", "snippet": "17.6 <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>sgd</b>) 318 17.7 pseudo-random numbers 319 17.8 importance sampling 320 17.9 importance sampling for lda 321 17.10 the ising model* 323 17.11 the boltzmann machine 324 17.12 mcmc and google pagerank 326 17.13 metropolis-hasting sampling 327 17.14 gibbs sampling 328 17.15 mcmc and cognitive biases 330 17.16 constrastive divergence 332 chapter 18 the unreasonable effectiveness of abstraction 335 18.1 deep learning works! 335 18.2 feature learning 337 18.3 word ...", "dateLastCrawled": "2022-01-30T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "2018-2019 Extension Course Archive | Harvard Extension School", "url": "https://extension.harvard.edu/2018-2019-extension-course-archive/", "isFamilyFriendly": true, "displayUrl": "https://extension.harvard.edu/2018-2019-extension-course-archive", "snippet": "Topics include <b>stochastic</b> optimization such as <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) and simulated annealing, Bayesian data analysis, Markov chain Monte Carlo (MCMC), and variational analysis. This course is broadly about learning models from data. To do this, we typically want to solve an optimization problem. Some problems might have many optima, and we will want to explore them all. It is not enough to find an optimum. Bayesian statistics gives us a simple and principled way to find the ...", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A new taxonomy of global <b>optimization</b> algorithms | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11047-020-09820-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11047-020-09820-4", "snippet": "For example, the <b>stochastic</b> <b>hill</b> climber utilizes random variation with a small step size compared to the range of the complete search interval. <b>Gradient</b>-based methods directly compute or approximate the gradients of the objective function to find the best direction and strength for the variation. Algorithms such as Nelder-Mead create new candidates by computing a search direction using simplexes.", "dateLastCrawled": "2022-01-26T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Classification</b> | Jupyter notebooks \u2013 a Swiss Army Knife for Quants", "url": "https://ipythonquant.wordpress.com/category/classification/", "isFamilyFriendly": true, "displayUrl": "https://ipythonquant.wordpress.com/category/<b>classification</b>", "snippet": "<b>Gradient</b> <b>descent</b> method. As mentioned before the <b>gradient</b> is very useful if we need to minimize a loss function. It\u2019s like <b>hiking</b> down <b>a hill</b>, we walk step by step into the direction of the steepest <b>descent</b> and finally we reach the valley. The <b>gradient</b> provides us the information in which direct we need to walk. So if we want to minimize the ...", "dateLastCrawled": "2022-01-05T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) On the choice of metric in <b>gradient</b>-based theories of brain function", "url": "https://www.researchgate.net/publication/331618987_On_the_choice_of_metric_in_gradient-based_theories_of_brain_function", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/331618987_On_the_choice_of_metric_in_<b>gradient</b>...", "snippet": "The main message of this text. (A) A cost function and a metric together determine a unique flow field and update rule, given by <b>gradient</b> <b>descent</b> on the cost function in that metric.", "dateLastCrawled": "2021-09-15T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Towards a science of <b>human stories: using sentiment analysis</b> ... - DeepAI", "url": "https://deepai.org/publication/towards-a-science-of-human-stories-using-sentiment-analysis-and-emotional-arcs-to-understand-the-building-blocks-of-complex-social-systems", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/towards-a-science-of-human-stories-using-sentiment...", "snippet": "applies machine learning over a bag-of-words analysis to predict action and sex scenes using Naive Bayes (NB) and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>). Training data is crowd-sourced from two ratings of 500 word chunks on the survey platform Mechanical Turk (MT), and Cherny develops novel visualizations of the relationships between topics in chapters.", "dateLastCrawled": "2021-11-27T12:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning</b> | <b>Jupyter notebooks \u2013 a Swiss Army Knife</b> for Quants", "url": "https://ipythonquant.wordpress.com/category/machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://ipythonquant.wordpress.com/category/<b>machine-learning</b>", "snippet": "<b>Gradient</b> <b>descent</b> method. As mentioned before the <b>gradient</b> is very useful if we need to minimize a loss function. It\u2019s like <b>hiking</b> down <b>a hill</b>, we walk step by step into the direction of the steepest <b>descent</b> and finally we reach the valley. The <b>gradient</b> provides us the information in which direct we need to walk. So if we want to minimize the ...", "dateLastCrawled": "2021-12-30T06:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Complete Glossary of Keras Optimizers and When to Use Them (With Code)", "url": "https://analyticsarora.com/complete-glossary-of-keras-optimizers-and-when-to-use-them-with-code/", "isFamilyFriendly": true, "displayUrl": "https://analyticsarora.com/complete-glossary-of-keras-optimizers-and-when-to-use-them...", "snippet": "While a typical <b>gradient</b> <b>descent</b> algorithm would have us calculate the <b>gradient</b> with respect to each of the features available in the dataset, the \u201c<b>stochastic</b>\u201d nature of <b>SGD</b> helps us deal with this in a very efficient manner. Now you <b>can</b> imagine how computationally expensive it is to calculate the <b>gradient</b> with respect to each feature, given that Neural Networks have hundreds or even thousands of features at hand. This introduces a huge overhead, practically making <b>gradient</b> <b>descent</b> ...", "dateLastCrawled": "2022-01-28T22:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Optimization: Learning to Minimize Cost</b> | Training Deep Networks | <b>InformIT</b>", "url": "https://www.informit.com/articles/article.aspx?p=2990401&seqNum=2", "isFamilyFriendly": true, "displayUrl": "https://<b>www.informit.com</b>/articles/article.aspx?p=2990401&amp;seqNum=2", "snippet": "FIGURE 8.6 An outline of the overall process for training a neural network with <b>stochastic</b> <b>gradient</b> <b>descent</b>. The entire dataset is shuffled and split into batches. Each batch is forward propagated through the network; the output \u0177 is compared to the ground truth y and the cost C is calculated; backpropagation calculates the gradients; and the model parameters w and b are updated. The next batch (indicated by a dashed line) is forward propagated, and so on until all of the batches have moved ...", "dateLastCrawled": "2022-01-29T04:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "2018-2019 Extension Course Archive | Harvard Extension School", "url": "https://extension.harvard.edu/2018-2019-extension-course-archive/", "isFamilyFriendly": true, "displayUrl": "https://extension.harvard.edu/2018-2019-extension-course-archive", "snippet": "Topics include <b>stochastic</b> optimization such as <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) and simulated annealing, Bayesian data analysis, Markov chain Monte Carlo (MCMC), and variational analysis. This course is broadly about learning models from data. To do this, we typically want to solve an optimization problem. Some problems might have many optima, and we will want to explore them all. It is not enough to find an optimum. Bayesian statistics gives us a simple and principled way to find the ...", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "multivariable calculus - Why is <b>gradient</b> the direction of <b>steepest</b> ...", "url": "https://math.stackexchange.com/questions/223252/why-is-gradient-the-direction-of-steepest-ascent", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/223252", "snippet": "This means that the <b>gradient</b> will always point in the direction of the <b>steepest</b> <b>descent</b> (nb: which is of course not a proof but a hand-waving indication of its behaviour to give some intuition only!) For a little bit of background and the code for creating the animation see here: Why <b>Gradient</b> <b>Descent</b> Works (and How To Animate 3D-Functions in R) .", "dateLastCrawled": "2022-02-03T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning</b> | <b>Jupyter notebooks \u2013 a Swiss Army Knife</b> for Quants", "url": "https://ipythonquant.wordpress.com/category/machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://ipythonquant.wordpress.com/category/<b>machine-learning</b>", "snippet": "<b>Gradient</b> <b>descent</b> method. As mentioned before the <b>gradient</b> is very useful if we need to minimize a loss function. It\u2019s like <b>hiking</b> down <b>a hill</b>, we walk step by step into the direction of the steepest <b>descent</b> and finally we reach the valley. The <b>gradient</b> provides us the information in which direct we need to walk. So if we want to minimize the ...", "dateLastCrawled": "2021-12-30T06:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Deep Learning Illustrated</b> | PDF | Deep Learning | Artificial ... - Scribd", "url": "https://www.scribd.com/document/487250260/Deep-Learning-Illustrated", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/487250260", "snippet": "117 8.4 The learning rate (\u03b7 ) of <b>gradient</b> <b>descent</b> expressed as the size of a trilobite. 118 8.5 An individual round of training with <b>stochastic</b> <b>gradient</b> <b>descent</b>. . . . . . 121 8.6 An outline of the overall process for training a neural network with <b>stochastic</b>", "dateLastCrawled": "2021-12-29T23:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Equation of Knowledge: From Bayes&#39; Rule to a Unified Philosophy of ...", "url": "https://dokumen.pub/the-equation-of-knowledge-from-bayes-rule-to-a-unified-philosophy-of-science-1nbsped-0367428156-9780367428150.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen.pub</b>/the-equation-of-knowledge-from-bayes-rule-to-a-unified-philosophy...", "snippet": "17.6 <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>sgd</b>) 318 17.7 pseudo-random numbers 319 17.8 importance sampling 320 17.9 importance sampling for lda 321 17.10 the ising model* 323 17.11 the boltzmann machine 324 17.12 mcmc and google pagerank 326 17.13 metropolis-hasting sampling 327 17.14 gibbs sampling 328 17.15 mcmc and cognitive biases 330 17.16 constrastive divergence 332 chapter 18 the unreasonable effectiveness of abstraction 335 18.1 deep learning works! 335 18.2 feature learning 337 18.3 word ...", "dateLastCrawled": "2022-01-30T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Classification</b> | Jupyter notebooks \u2013 a Swiss Army Knife for Quants", "url": "https://ipythonquant.wordpress.com/category/classification/", "isFamilyFriendly": true, "displayUrl": "https://ipythonquant.wordpress.com/category/<b>classification</b>", "snippet": "<b>Gradient</b> <b>descent</b> method. As mentioned before the <b>gradient</b> is very useful if we need to minimize a loss function. It\u2019s like <b>hiking</b> down <b>a hill</b>, we walk step by step into the direction of the steepest <b>descent</b> and finally we reach the valley. The <b>gradient</b> provides us the information in which direct we need to walk. So if we want to minimize the ...", "dateLastCrawled": "2022-01-05T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Northwestern University", "url": "https://downey-n1.cs.northwestern.edu/openNerStemTypes.html", "isFamilyFriendly": true, "displayUrl": "https://downey-n1.cs.northwestern.edu/openNerStemTypes.html", "snippet": "Entity Type Type Frequency Type-Entity Freq; java: languages : 18713: 2091: google: engines : 2418: 980: microsoft: applications : 36521: 162: color: features : 22075 ...", "dateLastCrawled": "2022-01-17T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Open Thread 153.25</b> | Slate Star Codex", "url": "https://slatestarcodex.com/2020/05/06/open-thread-153-25/", "isFamilyFriendly": true, "displayUrl": "https://slatestarcodex.com/2020/05/06/<b>open-thread-153-25</b>", "snippet": "In my ideal world, there\u2019s nothing to stop you from setting <b>up</b> a worker\u2019s co-operative, or a joint-stock corporation with or without limited liability, or any other form of organisation you <b>can</b> define in a contract; as long as you <b>can</b> get enough other people to voluntarily subscribe the necessary capital / sign on to provide their labour / permit the use of their land beyond what you\u2019re able to supply yourself (for, if you could supply it all yourself, you wouldn\u2019t need the ...", "dateLastCrawled": "2022-01-21T11:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS231n Convolutional Neural Networks for Visual Recognition", "url": "https://cs231n.github.io/optimization-1/", "isFamilyFriendly": true, "displayUrl": "https://cs231n.github.io/optimization-1", "snippet": "This process is called <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) (or also sometimes on-line <b>gradient</b> <b>descent</b>). This is relatively less common to see because in practice due to vectorized code optimizations it <b>can</b> be computationally much more efficient to evaluate the <b>gradient</b> for 100 examples, than the <b>gradient</b> for one example 100 times. Even though <b>SGD</b> technically refers to using a single example at a time to evaluate the <b>gradient</b>, you will hear people use the term <b>SGD</b> even when referring to mini ...", "dateLastCrawled": "2022-01-29T18:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Gradient</b>-Based Optimizers in Deep Learning - Analytics Vidhya", "url": "https://www.analyticsvidhya.com/blog/2021/06/complete-guide-to-gradient-based-optimizers/", "isFamilyFriendly": true, "displayUrl": "https://www.analyticsvidhya.com/blog/2021/06/complete-guide-to-<b>gradient</b>-based-optimizers", "snippet": "Batch <b>Gradient</b> <b>Descent</b> or Vanilla <b>Gradient</b> <b>Descent</b> or <b>Gradient</b> <b>Descent</b> (GD) <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) Mini batch <b>Gradient</b> <b>Descent</b> (MB-GD) 4. Challenges with all Types of <b>Gradient</b>-based Optimizers . Role of an Optimizer. As discussed in the introduction part of the article, Optimizers update the parameters of neural networks such as weights and learning rate to minimize the loss function. Here, the loss function acts as a guide to the terrain telling optimizer if it is moving in the ...", "dateLastCrawled": "2022-01-30T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "\u201cIs it BAD?\u201d When the <b>Gradient</b> <b>Descent</b> algorithm meets high dimensional ...", "url": "https://siyuan1202.medium.com/is-it-bad-when-the-gradient-descent-algorithm-meets-high-dimensional-data-b4fefc63a000", "isFamilyFriendly": true, "displayUrl": "https://siyuan1202.medium.com/is-it-bad-when-the-<b>gradient</b>-<b>descent</b>-algorithm-meets-high...", "snippet": "Imagine we are <b>hiking</b> in beautiful mountains. Someday, we are dedicating to find a bottom lake ... (a local minimum) from halfway <b>up</b> a small <b>hill</b> (a starting-point). But our sight is blocked by the heavy fog inside the forests of the mountain. The only helpful tool is a walking stick, which <b>can</b> tell us what direction is the best for us to go from our current position (<b>gradient</b>). And since we are very careful about time consumption and worry about circling around, we take the downhill path ...", "dateLastCrawled": "2022-01-27T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "2018-2019 Extension Course Archive | Harvard Extension School", "url": "https://extension.harvard.edu/2018-2019-extension-course-archive/", "isFamilyFriendly": true, "displayUrl": "https://extension.harvard.edu/2018-2019-extension-course-archive", "snippet": "Topics include <b>stochastic</b> optimization such as <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) and simulated annealing, Bayesian data analysis, Markov chain Monte Carlo (MCMC), and variational analysis. This course is broadly about learning models from data. To do this, we typically want to solve an optimization problem. Some problems might have many optima, and we will want to explore them all. It is not enough to find an optimum. Bayesian statistics gives us a simple and principled way to find the ...", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[cv231n] Lecture 3 | Loss Functions and <b>Optimization</b>_gdtop\u7684\u4e2a\u4eba\u7b14\u8bb0-CSDN\u535a\u5ba2", "url": "https://blog.csdn.net/weixin_37993251/article/details/87944804", "isFamilyFriendly": true, "displayUrl": "https://blog.csdn.net/weixin_37993251/article/details/87944804", "snippet": "This process is called <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) (or also sometimes on-line <b>gradient</b> <b>descent</b>). This is relatively less common to see because in practice due to vectorized code optimizations it <b>can</b> be computationally much more efficient to evaluate the <b>gradient</b> for 100 examples, than the <b>gradient</b> for one example 100 times. Even though <b>SGD</b> technically refers to using a single example at a time to evaluate the <b>gradient</b>, you will hear people use the term <b>SGD</b> even when referring to mini ...", "dateLastCrawled": "2022-01-17T09:27:00.0000000Z", "language": "zh_chs", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Optimization: Learning to Minimize Cost</b> | Training Deep Networks | <b>InformIT</b>", "url": "https://www.informit.com/articles/article.aspx?p=2990401&seqNum=2", "isFamilyFriendly": true, "displayUrl": "https://<b>www.informit.com</b>/articles/article.aspx?p=2990401&amp;seqNum=2", "snippet": "FIGURE 8.6 An outline of the overall process for training a neural network with <b>stochastic</b> <b>gradient</b> <b>descent</b>. The entire dataset is shuffled and split into batches. Each batch is forward propagated through the network; the output \u0177 is <b>compared</b> to the ground truth y and the cost C is calculated; backpropagation calculates the gradients; and the model parameters w and b are updated. The next batch (indicated by a dashed line) is forward propagated, and so on until all of the batches have moved ...", "dateLastCrawled": "2022-01-29T04:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Towards a science of <b>human stories: using sentiment analysis</b> ... - DeepAI", "url": "https://deepai.org/publication/towards-a-science-of-human-stories-using-sentiment-analysis-and-emotional-arcs-to-understand-the-building-blocks-of-complex-social-systems", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/towards-a-science-of-human-stories-using-sentiment...", "snippet": "applies machine learning over a bag-of-words analysis to predict action and sex scenes using Naive Bayes (NB) and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>). Training data is crowd-sourced from two ratings of 500 word chunks on the survey platform Mechanical Turk (MT), and Cherny develops novel visualizations of the relationships between topics in chapters.", "dateLastCrawled": "2021-11-27T12:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) A new <b>taxonomy of global optimization algorithms</b>", "url": "https://www.researchgate.net/publication/346540233_A_new_taxonomy_of_global_optimization_algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/346540233_A_new_taxonomy_of_global...", "snippet": "<b>gradient</b> <b>descent</b> (<b>SGD</b>) algorithms (Ruder 2016). These algorithms have, by design, fast convergence to a local optimum situated in a region of attraction and com-", "dateLastCrawled": "2022-01-23T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) On the choice of metric in <b>gradient</b>-based theories of brain function", "url": "https://www.researchgate.net/publication/331618987_On_the_choice_of_metric_in_gradient-based_theories_of_brain_function", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/331618987_On_the_choice_of_metric_in_<b>gradient</b>...", "snippet": "The main message of this text. (A) A cost function and a metric together determine a unique flow field and update rule, given by <b>gradient</b> <b>descent</b> on the cost function in that metric.", "dateLastCrawled": "2021-09-15T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A new taxonomy of global <b>optimization</b> algorithms | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11047-020-09820-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11047-020-09820-4", "snippet": "For example, the <b>stochastic</b> <b>hill</b> climber utilizes random variation with a small step size <b>compared</b> to the range of the complete search interval. <b>Gradient</b>-based methods directly compute or approximate the gradients of the objective function to find the best direction and strength for the variation. Algorithms such as Nelder-Mead create new candidates by computing a search direction using simplexes.", "dateLastCrawled": "2022-01-26T17:05:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! | by Aishwarya V ...", "url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>stochastic-gradient-descent</b>-clearly-explained-53d239905d31", "snippet": "<b>Stochastic gradient descent</b> is a very popular and common algorithm used in various <b>Machine</b> <b>Learning</b> algorithms, most importantly forms the basis of Neural Networks. In this article, I have tried my\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! Aishwarya V Srinivasan. Sep 7, 2019 \u00b7 4 min read. <b>Stochastic gradient descent</b> is a ...", "dateLastCrawled": "2022-02-02T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Stochastic gradient descent</b> - The <b>Learning</b> <b>Machine</b>", "url": "https://the-learning-machine.com/article/optimization/stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://the-<b>learning</b>-<b>machine</b>.com/article/optimization/<b>stochastic-gradient-descent</b>", "snippet": "<b>Stochastic gradient descent</b> (<b>SGD</b>) is an approach for unconstrained optimization.<b>SGD</b> is the workhorse of optimization for <b>machine</b> <b>learning</b> approaches. It is used as a faster alternative for training support vector machines and is the preferred optimization routine for deep <b>learning</b> approaches.. In this article, we will motivate the formulation for <b>stochastic gradient descent</b> and provide interactive demos over multiple univariate and multivariate functions to show it in action.", "dateLastCrawled": "2022-01-26T05:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> in Theory and Practice", "url": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) is the most widely used optimization method in the <b>machine</b> <b>learning</b> community. Researchers in both academia and industry have put considerable e ort to optimize <b>SGD</b>\u2019s runtime performance and to develop a theoretical framework for its empirical success. For example, recent advancements in deep neural networks have been largely achieved because, surprisingly, <b>SGD</b> has been found adequate to train them. Here we present three works highlighting desirable ...", "dateLastCrawled": "2022-01-30T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> <b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b>", "url": "http://www.bel.utcluj.ro/dce/didactic/eai/04_GradientDescent_ML.pdf", "isFamilyFriendly": true, "displayUrl": "www.bel.utcluj.ro/dce/didactic/eai/04_<b>GradientDescent</b>_ML.pdf", "snippet": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> Elements of Artificial Intelligence G. Oltean BGD vs. <b>SGD</b> The summation part is important, especially with the concept of batch <b>gradient</b> <b>descent</b> (BGD) vs. <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>). In Batch <b>Gradient</b> <b>Descent</b>, all the training data is taken into consideration to take a single step (one training epoch ...", "dateLastCrawled": "2022-02-03T10:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Adam, <b>Momentum and Stochastic Gradient Descent</b> - <b>Machine</b> <b>Learning</b> From ...", "url": "https://mlfromscratch.com/optimizers-explained/", "isFamilyFriendly": true, "displayUrl": "https://mlfromscratch.com/optimizers-explained", "snippet": "The basic difference between batch <b>gradient</b> <b>descent</b> (BGD) and <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>), is that we only calculate the cost of one example for each step in <b>SGD</b>, but in BGD, we have to calculate the cost for all training examples in the dataset. Trivially, this speeds up neural networks greatly. Exactly this is the motivation behind <b>SGD</b>. The equation for <b>SGD</b> is used to update parameters in a neural network \u2013 we use the equation to update parameters in a backwards pass, using ...", "dateLastCrawled": "2022-02-02T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Scikit Learn - Stochastic Gradient Descent</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_stochastic_gradient_descent</b>.htm", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) is a simple yet efficient optimization algorithm used to find the values of parameters/coefficients of functions that minimize a cost function. In other words, it is used for discriminative <b>learning</b> of linear classifiers under convex loss functions such as SVM and Logistic regression. It has been successfully applied to large-scale datasets because the update to the coefficients is performed for each training instance, rather than at the end of instances.", "dateLastCrawled": "2022-02-03T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Gradient</b> <b>Descent</b>: One of <b>Machine</b> <b>Learning</b>\u2019s Most Popular Algorithms ...", "url": "https://urmiparekh.medium.com/gradient-descent-one-of-machine-learnings-most-popular-algorithms-c31963d1e67f", "isFamilyFriendly": true, "displayUrl": "https://urmiparekh.medium.com/<b>gradient</b>-<b>descent</b>-one-of-<b>machine</b>-<b>learning</b>s-most-popular...", "snippet": "<b>Gradient</b> <b>descent</b> is an optimization algorithm which is used to train a <b>machine</b> <b>learning</b> model. It is an optimization algorithm to find a local minimum of a differential function. It is used to find the values of a function\u2019s coefficients that minimize a cost function as much as possible. Source: Here. It i s a first-order iterative ...", "dateLastCrawled": "2022-01-17T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Batch, Mini Batch &amp; <b>Stochastic</b> <b>Gradient Descent</b> | by Sushant Patrikar ...", "url": "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/batch-mini-batch-<b>stochastic</b>-<b>gradient-descent</b>-7a62ecba642a", "snippet": "In Batch <b>Gradient Descent</b> we were considering all the examples for every step of <b>Gradient Descent</b>. But what if our dataset is very huge. Deep <b>learning</b> models crave for data. The more the data the more chances of a model to be good. Suppose our dataset has 5 million examples, then just to take one step the model will have to calculate the gradients of all the 5 million examples. This does not seem an efficient way. To tackle this problem we have <b>Stochastic</b> <b>Gradient Descent</b>. In <b>Stochastic</b> ...", "dateLastCrawled": "2022-02-02T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Gradient Descent With Momentum from Scratch</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>gradient-descent-with-momentum-from-scratch</b>", "snippet": "<b>Gradient</b> <b>descent</b> is an optimization algorithm that follows the negative <b>gradient</b> of an objective function in order to locate the minimum of the function. A problem with <b>gradient</b> <b>descent</b> is that it can bounce around the search space on optimization problems that have large amounts of curvature or noisy gradients, and it can get stuck in flat spots in the search", "dateLastCrawled": "2022-01-26T05:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent in Logistic Regression [Explained for Beginners</b> ...", "url": "https://www.upgrad.com/blog/gradient-descent-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/<b>gradient-descent-in-logistic-regression</b>", "snippet": "It\u2019s massive, and hence there was a need for a slightly modified <b>Gradient</b> <b>Descent</b> Algorithm, namely \u2013 <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Algorithm (<b>SGD</b>). The only difference <b>SGD</b> has with Normal <b>Gradient</b> <b>Descent</b> is that, in <b>SGD</b>, we don\u2019t deal with the entire training instance at a single time. In <b>SGD</b>, we compute the <b>gradient</b> of the cost function for just a single random example at each iteration. Now, doing so brings down the time taken for computations by a huge margin especially for large ...", "dateLastCrawled": "2022-01-28T16:49:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Gossip <b>Learning</b> as a Decentralized Alternative to Federated <b>Learning</b>", "url": "http://publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "isFamilyFriendly": true, "displayUrl": "publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "snippet": "Federated <b>learning</b> is adistributed <b>machine</b> <b>learning</b> approach for computing models over data collected by edge devices. Most impor-tantly, the data itself is not collected centrally, but a master-worker ar-chitecture is applied where a master node performs aggregation and the edge devices are the workers, not unlike the parameter server approach. Gossip <b>learning</b> also assumes that the data remains at the edge devices, but it requires no aggregation server or any central component. In this ...", "dateLastCrawled": "2022-01-27T14:06:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(stochastic gradient descent (sgd))  is like +(hiking up a hill)", "+(stochastic gradient descent (sgd)) is similar to +(hiking up a hill)", "+(stochastic gradient descent (sgd)) can be thought of as +(hiking up a hill)", "+(stochastic gradient descent (sgd)) can be compared to +(hiking up a hill)", "machine learning +(stochastic gradient descent (sgd) AND analogy)", "machine learning +(\"stochastic gradient descent (sgd) is like\")", "machine learning +(\"stochastic gradient descent (sgd) is similar\")", "machine learning +(\"just as stochastic gradient descent (sgd)\")", "machine learning +(\"stochastic gradient descent (sgd) can be thought of as\")", "machine learning +(\"stochastic gradient descent (sgd) can be compared to\")"]}