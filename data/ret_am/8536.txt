{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>SparseX</b>: A <b>Library</b> for High-Performance <b>Sparse</b> Matrix-<b>Vector</b> ...", "url": "https://dl.acm.org/doi/10.1145/3134442", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3134442", "snippet": "The <b>library</b>&#39;s kernels are based on the application of CSX for <b>sparse</b> matrices and are used to prepare a high-performance <b>sparse</b> matrix-<b>vector</b> multiplication code (written in the C/C++ language), which can be used in different high-level <b>sparse</b> solvers for systems of linear algebraic equations via iterative methods. The authors of the paper are trying to a) provide simple and clear semantics; b) serve users with different levels of expertise; c) facilitate the integration of their kernels in ...", "dateLastCrawled": "2022-01-04T04:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Sparse</b> Matrices for Machine Learning", "url": "https://machinelearningmastery.com/sparse-matrices-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>sparse</b>-matrices-for-machine-learning", "snippet": "In both cases, the matrix contained is <b>sparse</b> <b>with many</b> <b>more</b> zero values than data values. The problem with representing these <b>sparse</b> matrices as dense matrices is that memory is required and must be allocated for each 32-bit or even 64-bit zero value in the matrix. This is clearly a waste of memory resources as those zero values do not contain any information. Time Complexity. Assuming a very large <b>sparse</b> matrix can be fit into memory, we will want to perform operations on this matrix ...", "dateLastCrawled": "2022-02-02T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Data Representation in NLP. What is Vectorization ? | by Shivangi ...", "url": "https://shiivangii.medium.com/data-representation-in-nlp-7bb6a771599a", "isFamilyFriendly": true, "displayUrl": "https://shiivangii.medium.com/data-representation-in-nlp-7bb6a771599a", "snippet": "In <b>Sparse</b> <b>Vector</b> representations (Latent Semantic Analysis) ... They represent <b>more</b> advanced <b>vector</b> representations of term, and can determine the meaning of a word by looking at its company (its context). For example, in a big text corpus, there are two sentences: Sentence 1:\u201cBMW is a German car manufacturer\u201d Sentence 2: \u201cBMW is a German automobile manufacturer\u201d. Word2vec Architecture. There are two architectures used by word2vec. Continuous Bag of words (CBOW) skip gram; Why these ...", "dateLastCrawled": "2022-02-03T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) A <b>Sparse</b> Matrix <b>Library</b> in C++ for High Performance Architectures", "url": "https://www.researchgate.net/publication/2824976_A_Sparse_Matrix_Library_in_C_for_High_Performance_Architectures", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2824976_A_<b>Sparse</b>_Matrix_<b>Library</b>_in_C_for_High...", "snippet": "<b>Sparse</b> matrix-<b>vector</b> multiplication (SpMV) is a challenging computational kernel in linear algebra applications, <b>like</b> data mining, image processing, and machine learning. The performance of this ...", "dateLastCrawled": "2022-01-09T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Plotting <b>Library</b> Catalog Subjects | Data Science Portfolio", "url": "https://sourestdeeds.github.io/blog/library-catalog-subject/", "isFamilyFriendly": true, "displayUrl": "https://sourestdeeds.github.io/blog/<b>library</b>-catalog-subject", "snippet": "For word vectors, one problem is that words <b>like</b> \u201cthe\u201d occur with all subjects equally (or for <b>library</b> catalog subject vectors, maybe \u201cLarge print <b>books</b>\u201d co-occur <b>with many</b> other subjects). These subjects don\u2019t provide <b>more</b> information about the interesting subjects. So instead of using the counts directly, I\u2019ll use PPMI, which is higher if the words co-occur <b>more</b> often than chance. It is a function of the co-occurrence counts and global counts.", "dateLastCrawled": "2022-01-29T17:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sparse Matrix Multiplication in Python 3</b> - Eric J. Ma&#39;s Personal Site", "url": "https://ericmjl.github.io/blog/2016/7/24/sparse-matrix-multiplication-in-python-3/", "isFamilyFriendly": true, "displayUrl": "https://ericmjl.github.io/blog/2016/7/24/<b>sparse-matrix-multiplication-in-python-3</b>", "snippet": "<b>Sparse</b> matrix multiplication shows up in <b>many</b> places, and in Python, it&#39;s often handy to use a <b>sparse</b> matrix representation for memory purposes. One thing nice about the newest version of Python 3 is the @ operator, which takes two matrices and multiplies them. While numpy has had the np.dot(mat1, mat2) function for a while, I think mat1 @ mat2 can be a <b>more</b> expressive way of expressing the matrix multiplication operation. One hidden benefit of the @ operator: it handles scipy.<b>sparse</b> ...", "dateLastCrawled": "2022-01-31T07:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Why are sparse vectors so bad in</b> Bag-Of-Words? : learnmachinelearning", "url": "https://www.reddit.com/r/learnmachinelearning/comments/bf7mw6/why_are_sparse_vectors_so_bad_in_bagofwords/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/learnmachinelearning/comments/bf7mw6/why_are_<b>sparse</b>_<b>vectors</b>...", "snippet": "I read on a few articles that having <b>sparse</b> vectors <b>like</b> these are very troubling but I don&#39;t see why? <b>More</b> pressingly I honestly don&#39;t see how I can reduce this issue. Any thoughts? 1 comment. share. save. hide. report. 100% Upvoted. This thread is archived. New comments cannot be posted and votes cannot be cast . Sort by. best. level 1. 0 points \u00b7 1 year ago. <b>sparse</b> means you will need a lot of space to store meaningless information (zeros). there are data structures designed to store ...", "dateLastCrawled": "2020-12-25T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Introduction to Sparse Matrices in</b> R - Python and R Tips", "url": "https://cmdlinetips.com/2019/05/introduction-to-sparse-matrices-in-r/", "isFamilyFriendly": true, "displayUrl": "https://cmdlinetips.com/2019/05/<b>introduction-to-sparse-matrices-in</b>-r", "snippet": "Let us create a matrix with <b>sparse</b> data from scratch. We will first create data, a <b>vector</b> with million random numbers from normal distribution with zero mean and unit variance. data &lt;- rnorm(1e6) The above data <b>vector</b> is not <b>sparse</b> and contains data in all elements. Let us randomly select the indices and make them to contain zeroes. data &lt;- rnorm(1e6) zero_index &lt;- sample(1e6)[1:9e5] data[zero_index] &lt;- 0 Now we have created a <b>vector</b> of million elements, but 90% of the elements are zeros ...", "dateLastCrawled": "2022-01-28T12:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>vector</b> - Representation of Large <b>Graph</b> with 100 million nodes in C++ ...", "url": "https://stackoverflow.com/questions/40557400/representation-of-large-graph-with-100-million-nodes-in-c", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/40557400", "snippet": "So it is kind of <b>sparse</b> <b>graph</b>. I also have to store the weightage of each edge. I am currently using two vectors for that <b>like</b> following. // V could be 100 million <b>vector</b>&lt;int&gt; *AdjList = new <b>vector</b>&lt;int&gt; [V]; <b>vector</b>&lt;int&gt; *Weight = new <b>vector</b>&lt;int&gt; [V]; Using <b>vector</b> of <b>vector</b> does not seem to be space efficient. It takes <b>more</b> than 400gb of storage.", "dateLastCrawled": "2022-01-18T20:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "4. Text Vectorization and Transformation Pipelines - Applied Text ...", "url": "https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/<b>library</b>/view/applied-text-analysis/9781491963036/ch04.html", "snippet": "<b>Many</b> model families suffer from \u201cthe curse of dimensionality\u201d; as the feature space increases in dimensions, the data becomes <b>more</b> <b>sparse</b> and less informative to the underlying decision space. Text normalization reduces the number of dimensions, decreasing sparsity. Besides the simple filtering of tokens (removing punctuation and stopwords), there are two primary methods for text normalization:", "dateLastCrawled": "2022-02-01T20:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Data Representation in NLP. What is Vectorization ? | by Shivangi ...", "url": "https://shiivangii.medium.com/data-representation-in-nlp-7bb6a771599a", "isFamilyFriendly": true, "displayUrl": "https://shiivangii.medium.com/data-representation-in-nlp-7bb6a771599a", "snippet": "In <b>Sparse</b> <b>Vector</b> representations (Latent Semantic Analysis) ... They represent <b>more</b> advanced <b>vector</b> representations of term, and can determine the meaning of a word by looking at its company (its context). For example, in a big text corpus, there are two sentences: Sentence 1:\u201cBMW is a German car manufacturer\u201d Sentence 2: \u201cBMW is a German automobile manufacturer\u201d. Word2vec Architecture. There are two architectures used by word2vec. Continuous Bag of words (CBOW) skip gram; Why these ...", "dateLastCrawled": "2022-02-03T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sparse Matrix-Vector Multiplication</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/sparse-matrix-vector-multiplication", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>sparse-matrix-vector-multiplication</b>", "snippet": "<b>Sparse matrix-vector multiplication</b> (SpMV) is a fundamental computational kernel used in scientific and engineering applications. The nonzero elements of <b>sparse</b> matrices are represented in different formats, and a single <b>sparse</b> matrix representation is not suitable for all <b>sparse</b> matrices with different sparsity patterns. Extensive studies have been done on improving the performance of <b>sparse</b> matrices processing on different platforms. Graphics processing units (GPUs) are very well suited ...", "dateLastCrawled": "2022-01-28T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Plotting <b>Library</b> Catalog Subjects | Data Science Portfolio", "url": "https://sourestdeeds.github.io/blog/library-catalog-subject/", "isFamilyFriendly": true, "displayUrl": "https://sourestdeeds.github.io/blog/<b>library</b>-catalog-subject", "snippet": "For word vectors, one problem is that words like \u201cthe\u201d occur with all subjects equally (or for <b>library</b> catalog subject vectors, maybe \u201cLarge print <b>books</b>\u201d co-occur <b>with many</b> other subjects). These subjects don\u2019t provide <b>more</b> information about the interesting subjects. So instead of using the counts directly, I\u2019ll use PPMI, which is higher if the words co-occur <b>more</b> often than chance. It is a function of the co-occurrence counts and global counts.", "dateLastCrawled": "2022-01-29T17:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "c++ - What are <b>the differences between the various boost</b> ublas <b>sparse</b> ...", "url": "https://stackoverflow.com/questions/3399574/what-are-the-differences-between-the-various-boost-ublas-sparse-vectors", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/3399574", "snippet": "But the documentation is <b>sparse</b> (ha ha) on information about compressed_<b>vector</b> and coordinate_<b>vector</b>. Is anyone able to clarify? I&#39;m trying to figure out the algorithmic complexity of adding items to the various vectors, and also of dot products between two such vectors. A very helpful answer offered that compressed_<b>vector</b> is very <b>similar</b> to compressed_matrix. But it seems that, for example, compressed row storage is only for storing matrices -- not just vectors. I see that unbounded_array ...", "dateLastCrawled": "2022-01-26T05:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Performance Prediction Based on Statistics of <b>Sparse</b> Matrix-<b>Vector</b> ...", "url": "https://www.scirp.org/journal/PaperInformation.aspx?PaperID=75739", "isFamilyFriendly": true, "displayUrl": "https://www.scirp.org/journal/PaperInformation.aspx?PaperID=75739", "snippet": "1. Introduction. <b>Sparse</b> matrix-<b>vector</b> multiplication (SpMV) is an essential operation in solving linear systems and eigenvalue problems. For <b>many</b> iterative methods, the fraction of the execution time of SpMV may be <b>more</b> than 80% in the total time, so the study of its performance has attracted a lot of attention.", "dateLastCrawled": "2022-01-05T13:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "1. Vectors, Matrices, and Arrays - Machine Learning with Python ...", "url": "https://www.oreilly.com/library/view/machine-learning-with/9781491989371/ch01.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/<b>library</b>/view/machine-learning-with/9781491989371/ch01.html", "snippet": "# Load <b>library</b> import numpy as np # Create a <b>vector</b> as a row <b>vector</b>_row = np.array([1, 2, 3]) # Create a <b>vector</b> as a column <b>vector</b> _column = np.array([[1], [2], [3]]) Discussion. NumPy\u2019s main data structure is the multidimensional array. To create a <b>vector</b>, we simply create a one-dimensional array. Just like vectors, these arrays can be represented horizontally (i.e., rows) or vertically (i.e., columns). See Also. Vectors, Math Is Fun. Euclidean <b>vector</b>, Wikipedia. 1.2 Creating a Matrix ...", "dateLastCrawled": "2022-02-03T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "4. Text Vectorization and Transformation Pipelines - Applied Text ...", "url": "https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/<b>library</b>/view/applied-text-analysis/9781491963036/ch04.html", "snippet": "Moreover, the paragraph <b>vector</b> takes into consideration the ordering of words within a narrow context, <b>similar</b> to an n-gram model. The combined result is much <b>more</b> effective than a bag-of-words or bag-of- n -grams model because it generalizes better and has a lower dimensionality but still is of a fixed length so it can be used in common machine learning algorithms.", "dateLastCrawled": "2022-02-01T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Vector Representation of Words</b> by Siddhant7", "url": "https://siddhant7.github.io/Vector-Representation-of-Words/", "isFamilyFriendly": true, "displayUrl": "https://siddhant7.github.io/<b>Vector-Representation-of-Words</b>", "snippet": "<b>Vector Representation of Words</b> Siddhant&#39;s Blog. Of lately, word embeddings have been exceptionally successful in <b>many</b> NLP tasks. This blog is my attempt towards explaining the very popular word2vec model by Tomas Mikolov and why the hype around it is true. This model is used for learning word embeddings, which is nothing but <b>vector</b> representations of words in low-dimensional <b>vector</b> space.", "dateLastCrawled": "2022-02-02T04:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "6 most <b>commonly used Java Machine learning libraries</b> | Packt Hub", "url": "https://hub.packtpub.com/most-commonly-used-java-machine-learning-libraries/", "isFamilyFriendly": true, "displayUrl": "https://hub.packtpub.com/most-<b>commonly-used-java-machine-learning-libraries</b>", "snippet": "Java-ML is also a general-purpose machine learning <b>library</b>. Compared to Weka, it offers <b>more</b> consistent interfaces and implementations of recent algorithms that are not present in other packages, such as an extensive set of state-of-the-art similarity measures and feature-selection techniques, for example, dynamic time warping, random forest attribute evaluation, and so on. Java-ML is also available under the GNU GPL license. Java-ML supports any type of file as long as it contains one data ...", "dateLastCrawled": "2022-02-03T08:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Text Mining in R</b> | <b>Jan Kirenz</b>", "url": "https://www.kirenz.com/post/2019-09-16-r-text-mining/", "isFamilyFriendly": true, "displayUrl": "https://www.kirenz.com/post/2019-09-16-r-text-mining", "snippet": "We can access the full texts of various <b>books</b> from \u201cProject Gutenberg\u201d via the gutenbergr package. We can look up certain authors or titles with a regular expression using the stringr package. All functions in stringr start with str_and take a <b>vector</b> of strings as the first argument. To learn <b>more</b> about stringr, visit the stringr documentation.", "dateLastCrawled": "2022-02-02T11:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Sparse</b> Matrices for Machine Learning", "url": "https://machinelearningmastery.com/sparse-matrices-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>sparse</b>-matrices-for-machine-learning", "snippet": "In both cases, the matrix contained is <b>sparse</b> <b>with many</b> <b>more</b> zero values than data values. The problem with representing these <b>sparse</b> matrices as dense matrices is that memory is required and must be allocated for each 32-bit or even 64-bit zero value in the matrix. This is clearly a waste of memory resources as those zero values do not contain any information. Time Complexity. Assuming a very large <b>sparse</b> matrix <b>can</b> be fit into memory, we will want to perform operations on this matrix ...", "dateLastCrawled": "2022-02-02T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "PELLR: A Permutated ELLPACK-R Format for SpMV on GPUs", "url": "https://www.scirp.org/journal/paperinformation.aspx?paperid=99538", "isFamilyFriendly": true, "displayUrl": "https://www.scirp.org/journal/paperinformation.aspx?paperid=99538", "snippet": "The <b>sparse</b> matrix <b>vector</b> multiplication (SpMV) is inevitable in almost all kinds of scientific computation, such as iterative methods for solving linear systems and eigenvalue problems. With the emergence and development of Graphics Processing Units (GPUs), high efficient formats for SpMV should be constructed. The performance of SpMV is mainly determinted by the storage format for <b>sparse</b> matrix. Based on the idea of JAD format, this paper improved the ELLPACK-R format, reduced the waiting ...", "dateLastCrawled": "2022-02-01T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Iterative Reweighted Noninteger Norm Regularizing SVM for Gene ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3748415/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3748415", "snippet": "The weight <b>vector</b> will be a <b>sparse</b> <b>vector</b> <b>with many</b> zeros. In this situation, the optimization problem in and ... This <b>can</b> <b>be thought</b> of as a learning function = sign\u2061(w T x i + b): X \u2192 Y which maps each instance x i \u2208 R n to an estimated value y ^ i. In this paper, for simplicity and brevity, only two classification problems will be shown. The data set is assumed to be linearly separable. Then, the problem of hard-margin, support <b>vector</b> machine using p norm regularization <b>can</b> be ...", "dateLastCrawled": "2021-12-27T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "linear algebra - How to define <b>sparseness</b> of a <b>vector</b>? - Mathematics ...", "url": "https://math.stackexchange.com/questions/117860/how-to-define-sparseness-of-a-vector", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/117860", "snippet": "Here, <b>Sparseness</b>(X) = 0 whenever the <b>vector</b> is dense (all components are equal and non-zero) and <b>Sparseness</b>(X) = 1 whenever the <b>vector</b> is <b>sparse</b> (only one component is non zero). This post only explains the when 0 and 1 achieved by the above mentioned measure. Is there any other function defining the <b>sparseness</b> of the <b>vector</b>.", "dateLastCrawled": "2022-01-25T19:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sparse Matrix Computation</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/sparse-matrix-computation", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>sparse-matrix-computation</b>", "snippet": "<b>Sparse</b> matrices are important in <b>many</b> real-world applications that involve modeling complex phenomenon. Furthermore, <b>sparse matrix computation</b> is a simple example of data-dependent performance behavior of <b>many</b> large real-world applications. Due to the large amount of zero elements, compaction techniques are used to reduce the amount of storage, memory accesses, and computation performed on these zero elements. Unlike most other kernels presented in this book so far, the SpMV kernels are ...", "dateLastCrawled": "2022-01-24T19:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Mahout</b> - SlideShare", "url": "https://www.slideshare.net/EdurekaIN/mahout-36625358", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/EdurekaIN/<b>mahout</b>-36625358", "snippet": "Vectors implementation in <b>Mahout</b> Dense <b>Vector</b> Sequential Access <b>Sparse</b> <b>Vector</b> Random Access <b>Sparse</b> <b>Vector</b> Vectors implementation in <b>Mahout</b> It <b>can</b> <b>be thought</b> of as an array of doubles, whose size is the number of features in the data. Because all the entries in the array are preallocated regardless of whether the value is 0 or not, we call it dense. It is implemented as a HashMap between an integer and a double, where only nonzero valued features are allocated. Hence, they\u2019re called as ...", "dateLastCrawled": "2022-02-03T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Sparse autoencoder</b> - SlideShare", "url": "https://www.slideshare.net/DevashishPatel/sparse-autoencoder-95661509", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/DevashishPatel/<b>sparse-autoencoder</b>-95661509", "snippet": "write Equations (2-5) <b>more</b> compactly as: z(2) = W(1) x + b(1) a(2) = f(z(2) ) z(3) = W(2) a(2) + b(2) hW,b(x) = a(3) = f(z(3) ) <b>More</b> generally, recalling that we also use a(1) = x to also denote the values from the input layer, then given layer l\u2019s activations a(l) , we <b>can</b> compute layer l + 1\u2019s activations a(l+1) as: z(l+1) = W(l) a(l) + b(l) (6) a(l+1) = f(z(l+1) ) (7) By organizing our parameters in matrices and using matrix-<b>vector</b> operations, we <b>can</b> take advantage of fast linear ...", "dateLastCrawled": "2022-01-08T07:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Linear Algebra: What are <b>some optimization techniques for sparse matrix</b> ...", "url": "https://www.quora.com/Linear-Algebra-What-are-some-optimization-techniques-for-sparse-matrix-multiplication", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Linear-Algebra-What-are-<b>some-optimization-techniques-for-sparse</b>...", "snippet": "Answer (1 of 2): I&#39;ll concentrate on the matrix-<b>vector</b> multiplication, since that is much <b>more</b> common in the <b>sparse</b> case than matrix-matrix. The matrix-<b>vector</b> multiplication touches every matrix element once, and input/output <b>vector</b> elements multiple times. In the <b>sparse</b> case, the pattern with w...", "dateLastCrawled": "2022-01-14T18:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Word2Vec (<b>Skip-Gram</b> model) Explained | by n0obcoder | DataDrivenInvestor", "url": "https://medium.datadriveninvestor.com/word2vec-skip-gram-model-explained-383fa6ddc4ae", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/word2vec-<b>skip-gram</b>-model-explained-383fa6ddc4ae", "snippet": "The word embeddings <b>can</b> <b>be thought</b> of as a child\u2019s understanding of the words. Initially, the word embeddings are randomly initialized and they don\u2019t make any sense, just like the baby has no understanding of different words. It\u2019s only after the model has started getting trained, the word vectors/embeddings start to capture the meaning of the words, just like the baby hears and learns different words.", "dateLastCrawled": "2022-01-29T01:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "optimization - How <b>can</b> we solve the normal equations with limited ...", "url": "https://scicomp.stackexchange.com/questions/36941/how-can-we-solve-the-normal-equations-with-limited-memory", "isFamilyFriendly": true, "displayUrl": "https://scicomp.stackexchange.com/questions/36941/how-<b>can</b>-we-solve-the-normal...", "snippet": "Unlike Solving <b>sparse</b> least squares system with limited memor... Stack Exchange Network. Stack Exchange network consists of 178 Q&amp;A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange. Loading\u2026 0 +0; Tour Start here for a quick overview of the site Help Center Detailed answers to any questions you might have Meta Discuss the workings and policies of this site About Us ...", "dateLastCrawled": "2022-01-25T19:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse Matrix-Vector Multiplication</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/sparse-matrix-vector-multiplication", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>sparse-matrix-vector-multiplication</b>", "snippet": "<b>Sparse matrix-vector multiplication</b> (SpMV) is a fundamental computational kernel used in scientific and engineering applications. The nonzero elements of <b>sparse</b> matrices are represented in different formats, and a single <b>sparse</b> matrix representation is not suitable for all <b>sparse</b> matrices with different sparsity patterns. Extensive studies have been done on improving the performance of <b>sparse</b> matrices processing on different platforms. Graphics processing units (GPUs) are very well suited ...", "dateLastCrawled": "2022-01-28T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Performance evaluation of <b>sparse</b> matrix-<b>vector</b> product (SpMV ...", "url": "https://ieeexplore.ieee.org/abstract/document/7060964", "isFamilyFriendly": true, "displayUrl": "https://ieeexplore.ieee.org/abstract/document/7060964", "snippet": "In this work we present a comparative evaluation of <b>sparse</b> matrix <b>vector</b> product (SpMV) on different platforms. We use Cusp <b>library</b> on CUDA architecture GPUs and MKL Intel <b>library</b> as reference on CPUs. Experimental results have been conducted using a set of matrices from matrix market repository 1, comparing performance between GPU-based Cusp 2 and CPU-based MKL 3 libraries. The results showed a global speedup, obtained with GPU, ranging from 1.1 \u00d7 to 4.6 \u00d7 <b>compared</b> to CPU implementations ...", "dateLastCrawled": "2020-01-29T10:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) A <b>library</b> for parallel <b>sparse</b> matrix-<b>vector</b> multiplies", "url": "https://www.researchgate.net/publication/228544585_A_library_for_parallel_sparse_matrix-vector_multiplies", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228544585_A_<b>library</b>_for_parallel_<b>sparse</b>...", "snippet": "<b>Sparse</b> matrix\u2010<b>vector</b> multiplication is an extensively used kernel in <b>many</b> applications. Recently, a novel, purely combinatorial branch\u2010and\u2010bound\u2013based approach has been proposed for <b>sparse</b> ...", "dateLastCrawled": "2022-01-18T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "XAMG: A <b>library</b> for solving linear systems with multiple right-hand ...", "url": "https://www.sciencedirect.com/science/article/pii/S2352711021000406", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2352711021000406", "snippet": "Basic linear algebra operations like <b>vector</b> updates, dot products, and <b>sparse</b> matrix\u2013<b>vector</b> multiplications (SpMV) are characterized by flop per byte ratio of only about 0.1. This means that the corresponding operations are memory bound , , and its performance mostly depends on the amount of memory traffic and memory bandwidth. The solution of SLAEs with multiple RHSs allows to load a matrix for SpMV operations with m right-hand sides only once, <b>compared</b> to m loads when performing m SLAE ...", "dateLastCrawled": "2022-01-11T14:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "PELLR: A Permutated ELLPACK-R Format for SpMV on GPUs", "url": "https://www.scirp.org/journal/paperinformation.aspx?paperid=99538", "isFamilyFriendly": true, "displayUrl": "https://www.scirp.org/journal/paperinformation.aspx?paperid=99538", "snippet": "The <b>sparse</b> matrix <b>vector</b> multiplication (SpMV) is inevitable in almost all kinds of scientific computation, such as iterative methods for solving linear systems and eigenvalue problems. With the emergence and development of Graphics Processing Units (GPUs), high efficient formats for SpMV should be constructed. The performance of SpMV is mainly determinted by the storage format for <b>sparse</b> matrix. Based on the idea of JAD format, this paper improved the ELLPACK-R format, reduced the waiting ...", "dateLastCrawled": "2022-02-01T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Performance Prediction Based on Statistics of <b>Sparse</b> Matrix-<b>Vector</b> ...", "url": "https://www.scirp.org/journal/PaperInformation.aspx?PaperID=75739", "isFamilyFriendly": true, "displayUrl": "https://www.scirp.org/journal/PaperInformation.aspx?PaperID=75739", "snippet": "<b>Sparse</b> matrix-<b>vector</b> multiplication (SpMV) is an essential operation in solving linear systems and eigenvalue problems. For <b>many</b> iterative methods, the fraction of the execution time of SpMV may be <b>more</b> than 80% in the total time, so the study of its performance has attracted a lot of attention. Right now, the GPU has been from a graphics accelerator to a computing device with a broad spectrum of purposes, due to the characteristics of the multi-thread, high memory bandwidth. It <b>can</b> solve ...", "dateLastCrawled": "2022-01-05T13:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Gentle Introduction to <b>Sparse</b> Matrices for Machine Learning", "url": "https://machinelearningmastery.com/sparse-matrices-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>sparse</b>-matrices-for-machine-learning", "snippet": "In both cases, the matrix contained is <b>sparse</b> <b>with many</b> <b>more</b> zero values than data values. The problem with representing these <b>sparse</b> matrices as dense matrices is that memory is required and must be allocated for each 32-bit or even 64-bit zero value in the matrix. This is clearly a waste of memory resources as those zero values do not contain any information. Time Complexity. Assuming a very large <b>sparse</b> matrix <b>can</b> be fit into memory, we will want to perform operations on this matrix ...", "dateLastCrawled": "2022-02-02T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>SPARSE STORAGE RECOMMENDATION SYSTEM FOR SPARSE MATRIX</b> <b>VECTOR</b> MULTIPL\u2026", "url": "https://www.slideshare.net/iaeme/sparse-storage-recommendation-system-for-sparse-matrix-vector-multiplication-on-gpu", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/iaeme/<b>sparse-storage-recommendation-system-for-sparse</b>...", "snippet": "<b>Sparse</b> Matrix <b>Vector</b> Multiplication (SpMV) Ax=b is a well-known kernel in science, engineering, and web world. Harnessing large computing capabilities of GPU device, <b>many</b> <b>sparse</b> storage formats have been proposed to optimize performance of SpMV on GPU. Compressed <b>Sparse</b> Row (CSR), ELLPACK (ELL), Hybrid (HYB), and Aligned COO <b>sparse</b> storage ...", "dateLastCrawled": "2021-12-29T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "matlab - Are <b>sparse</b> matrices typically stored in column major order or ...", "url": "https://stackoverflow.com/questions/37309771/are-sparse-matrices-typically-stored-in-column-major-order-or-row-major-order", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/37309771", "snippet": "Anyhow, a short overview on <b>sparse</b> storage formats <b>can</b> be found here but <b>many</b> <b>more</b> storage formats were/are being proposed in <b>sparse</b> matrix research: There are also block storage formats, which attempt to leverage locally dense substructures of the matrix. See for example &quot;Fast <b>Sparse</b> Matrix-<b>Vector</b> Multiplication by Exploiting Variable Block ...", "dateLastCrawled": "2022-01-18T19:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "6 most <b>commonly used Java Machine learning libraries</b> | Packt Hub", "url": "https://hub.packtpub.com/most-commonly-used-java-machine-learning-libraries/", "isFamilyFriendly": true, "displayUrl": "https://hub.packtpub.com/most-<b>commonly-used-java-machine-learning-libraries</b>", "snippet": "There are over 70 Java-based open source machine learning projects listed on the MLOSS.org website and probably <b>many</b> <b>more</b> unlisted projects live at university servers, GitHub, or Bitbucket. In this article, we will review the major machine learning libraries and platforms in Java, the kind of problems they <b>can</b> solve, the algorithms they support, and the kind of data they <b>can</b> work with.. This article is an excerpt taken from Machine learning in Java, written by Bostjan Kaluza and published by ...", "dateLastCrawled": "2022-02-03T08:02:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to Vectors for <b>Machine</b> <b>Learning</b>", "url": "https://machinelearningmastery.com/gentle-introduction-vectors-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>vectors</b>-<b>machine</b>-<b>learning</b>", "snippet": "It is common to introduce vectors using a geometric <b>analogy</b>, where a <b>vector</b> represents a point or coordinate in an n-dimensional space, where n is the number of dimensions, such as 2. The <b>vector</b> can also be thought of as a line from the origin of the <b>vector</b> space with a direction and a magnitude. These analogies are good as a starting point, but should not be held too tightly as we often consider very high dimensional vectors in <b>machine</b> <b>learning</b>. I find the <b>vector</b>-as-coordinate the most ...", "dateLastCrawled": "2022-02-01T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Word Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C16-1332.pdf", "snippet": "Word Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man + Woman = Queen Aleksandr Drozd y, Anna Gladkova z, Satoshi Matsuoka y yTokyo Institute of Technology, Meguro-ku, Tokyo 152-8550, Japan alex@smg.is.titech.ac.jp, matsu@is.titech.ac.jp z The University of Tokyo, Meguro-ku, Tokyo 153-8902 Japan gladkova@phiz.c.u-tokyo.ac.jp Abstract Solving word analogies became one of the most popular benchmarks for word embeddings on the assumption that linear relations between word pairs ...", "dateLastCrawled": "2022-01-20T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Introduction to Matrices and Matrix Arithmetic for <b>Machine Learning</b>", "url": "https://machinelearningmastery.com/introduction-matrices-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/introduction-matrices-<b>machine-learning</b>", "snippet": "The geometric <b>analogy</b> used to help understand vectors and some of their operations does not hold with matrices. Further, a <b>vector</b> itself may be considered a matrix with one column and multiple rows. Often the dimensions of the matrix are denoted as m and n for the number of rows and the number of columns. Now that we know what a matrix is, let\u2019s look at defining one in Python. Defining a Matrix. We can represent a matrix in Python using a two-dimensional NumPy array. A NumPy array can be ...", "dateLastCrawled": "2022-02-02T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Word</b> <b>Embeddings</b> for NLP. Understanding <b>word</b> <b>embeddings</b> and their\u2026 | by ...", "url": "https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>word</b>-<b>embeddings</b>-for-nlp-5b72991e01d4", "snippet": "The size of the <b>vector</b> is equal to the number of elements in the vocabulary. If most of the elements are zero then the bag of words will be a <b>sparse</b> matrix. In deep <b>learning</b>, we would have <b>sparse</b> matrix as we will be working with huge amount of training data. <b>Sparse</b> representations are harder to model both for computational reasons as well as ...", "dateLastCrawled": "2022-02-02T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning :: Cosine Similarity for Vector</b> Space Models (Part III ...", "url": "https://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/", "isFamilyFriendly": true, "displayUrl": "https://blog.christianperone.com/2013/09/<b>machine</b>-<b>learning</b>-", "snippet": "<b>Machine Learning :: Cosine Similarity for Vector</b> Space Models (Part III) 12/09/2013 19/01/2020 Christian S. Perone <b>Machine</b> <b>Learning</b> , Programming , Python * It has been a long time since I wrote the TF-IDF tutorial ( Part I and Part II ) and as I promissed, here is the continuation of the tutorial.", "dateLastCrawled": "2022-01-29T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Word Embedding: Syntactics or Semantics</b> \u00b7 Shengbin&#39;s Studio", "url": "https://wushbin.github.io/2017/10/09/Word-Embedding-Syntactics-or-Semantics/", "isFamilyFriendly": true, "displayUrl": "https://wushbin.github.io/2017/10/09/<b>Word-Embedding-Syntactics-or-Semantics</b>", "snippet": "From all the result of the two method, we know that the dense <b>vector</b> method get a better result than the <b>sparse</b> PPMI method in <b>analogy</b> analysis and similar word search. In addition, the computational efficiency of the dense <b>vector</b> is also better than the PPMI. Short vectors may be easier to use as features in <b>machine</b> <b>learning</b>. Dense vectors may generalize better than storing explicit counts. In addition, dense vectors may perform better in capturing synonymy than <b>sparse</b> vectors.", "dateLastCrawled": "2022-01-09T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Art of <b>Vector</b> <b>Representation</b> of Words | by ASHISH RANA | Towards Data ...", "url": "https://towardsdatascience.com/art-of-vector-representation-of-words-5e85c59fee5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/art-of-<b>vector</b>-<b>representation</b>-of-words-5e85c59fee5", "snippet": "Words are represented by dense vectors where a <b>vector</b> represents the projection of the word into a continuous <b>vector</b> space. It is an improvement over more the traditional bag-of-word model encoding schemes where large <b>sparse</b> vectors were used to represent each word. Those representations were <b>sparse</b> because the vocabularies were vast and a given word or document would be represented by a large <b>vector</b> comprised mostly of zero values.", "dateLastCrawled": "2022-01-30T21:17:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sparse vector)  is like +(library with many more books)", "+(sparse vector) is similar to +(library with many more books)", "+(sparse vector) can be thought of as +(library with many more books)", "+(sparse vector) can be compared to +(library with many more books)", "machine learning +(sparse vector AND analogy)", "machine learning +(\"sparse vector is like\")", "machine learning +(\"sparse vector is similar\")", "machine learning +(\"just as sparse vector\")", "machine learning +(\"sparse vector can be thought of as\")", "machine learning +(\"sparse vector can be compared to\")"]}