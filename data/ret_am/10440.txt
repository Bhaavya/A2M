{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Learning from positive</b> and unlabeled <b>data</b>: a survey | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "snippet": "The goal of one-<b>class</b> classification is to learn a <b>model</b> that identifies <b>examples</b> from a certain <b>class</b>: the <b>positive</b> <b>class</b>, when only <b>examples</b> of that <b>class</b> are available (Khan and Madden 2014). It can be seen as <b>training</b> a binary classifier where the negative <b>class</b> consists <b>of all</b> other possible classes. This is in contrast to PU learning, where the domain of interest is defined by the unlabeled <b>data</b>. Also, the unlabeled <b>data</b> enables finding low-density areas which are likely to be ...", "dateLastCrawled": "2022-02-02T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Decision Tree</b> Tutorials &amp; Notes | Machine Learning | HackerEarth", "url": "https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml-decision-tree/tutorial/", "isFamilyFriendly": true, "displayUrl": "https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml...", "snippet": "Hypothesis <b>Class</b>: <b>Set</b> <b>of all</b> the possible functions; Sample: A <b>set</b> of inputs paired with a label, which is the correct output (also known as the <b>Training</b> <b>Set</b>) Candidate Concept: A concept which we think is the target concept; Testing <b>Set</b>: Similar to the <b>training</b> <b>set</b> and is used to test the candidate concept and determine its performance; Introduction. A <b>decision tree</b> is a tree-<b>like</b> graph with nodes representing the place where we pick an attribute and ask a question; edges represent the ...", "dateLastCrawled": "2022-02-02T20:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Training Data</b> and Test <b>Data</b> - <b>Tutorialspoint</b>", "url": "https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_training_test_data.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/.../machine_learning_with_python_<b>training</b>_test_<b>data</b>.htm", "snippet": "If the test <b>set</b> does contain <b>examples</b> from the <b>training</b> <b>set</b>, it will be difficult to assess whether the algorithm has learned to generalize from the <b>training</b> <b>set</b> or has simply memorized it. A program that generalizes well will be able to effectively perform a task with new <b>data</b>. In contrast, a program that memorizes the <b>training data</b> by learning an overly complex <b>model</b> could predict the values of the response variable for the <b>training</b> <b>set</b> accurately, but will fail to predict the value of the ...", "dateLastCrawled": "2022-01-31T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Correct ratio of <b>positive</b> to negative <b>training</b> <b>examples</b> for <b>training</b> a ...", "url": "https://stackoverflow.com/questions/17905205/correct-ratio-of-positive-to-negative-training-examples-for-training-a-random-fo", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/17905205", "snippet": "I realized that the related question Positives/negatives proportion in train <b>set</b> suggested that a 1-to-1 ratio of <b>positive</b> to negative <b>training</b> <b>examples</b> is favorable for the Rocchio algorithm. However, this question differs from the related question in that it concerns a random forest <b>model</b> and also in the following two ways. 1) I have plenty of <b>training</b> <b>data</b> to work with, and the main bottleneck on using more <b>training</b> <b>examples</b> is <b>training</b> iteration time. That is, I&#39;d prefer not to take more ...", "dateLastCrawled": "2022-01-22T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Calculate Precision, Recall, and F-Measure for Imbalanced ...", "url": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/precision-rec<b>all</b>-and-f-measure-for-", "snippet": "Recall quantifies the number of <b>positive</b> <b>class</b> predictions made out <b>of all</b> <b>positive</b> <b>examples</b> in the dataset. F-Measure provides a single score that balances both the concerns of precision and recall in one number. Kick-start your project with my new book Imbalanced Classification with Python, including step-by-step tutorials and the Python source code files for <b>all</b> <b>examples</b>. Let\u2019s get started. Update Jan/2020: Improved language about the objective of precision and recall. Fixed typos about ...", "dateLastCrawled": "2022-02-03T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning 99+ Most Important MCQ</b> - JOB SAARNEE", "url": "https://www.jobsaarnee.com/2020/09/machine-learning-100-most-important-mcq.html", "isFamilyFriendly": true, "displayUrl": "https://www.jobsaarnee.com/2020/09/machine-learning-100-most-important-mcq.html", "snippet": "Use a separate <b>set</b> of <b>examples</b>, distinct from the <b>training</b> <b>examples</b>, to evaluate the utility of post-pruning nodes from the tree. Use <b>all</b> the available <b>data</b> for <b>training</b>; Use an explicit measure of the complexity for encoding the <b>training</b> <b>examples</b> and the decision tree, halting growth of the tree when this encoding size is minimized. <b>All</b> of the ...", "dateLastCrawled": "2022-02-02T11:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Classification</b> In Machine Learning | by Amit Upadhyay | Analytics ...", "url": "https://medium.com/analytics-vidhya/classification-in-machine-learning-ed30753d9461", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>classification</b>-in-machine-learning-ed30753d9461", "snippet": "<b>Classification</b> Algorithm <b>examples</b> 3-&gt; <b>Classification</b> terminologies. Terminology we use in the <b>Classification</b> are: \u00b7 Classifier \u2014 It is an algorithm, which maps input <b>data</b> to a <b>class</b>, Example ...", "dateLastCrawled": "2022-02-01T00:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>ROC Curve</b>, a Complete Introduction | by Reza Bagheri | Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/roc-curve-a-complete-introduction-2f2da2e0434c", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>roc-curve</b>-a-complete-introduction-2f2da2e0434c", "snippet": "This <b>model</b> fits a sigmoid function to the <b>data</b> <b>set</b> to predict the probability of a <b>positive</b> label for each feature input. Scikit-learn\u2019s ... we have an ideal classifier that can predict <b>all</b> the labels of the <b>training</b> <b>data</b> <b>set</b> <b>correctly</b> for a threshold of 0.5. The ideal classifier always passes through this point (TPR=1, FPR=0), and this <b>ROC curve</b> is a characteristic <b>curve</b> for such a classifier. As mentioned before, the logistic regression <b>model</b> always uses a threshold of 0.5 to predict the ...", "dateLastCrawled": "2022-01-30T19:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Chapter 8</b> Flashcards | Quizlet", "url": "https://quizlet.com/570460057/chapter-8-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/570460057/<b>chapter-8</b>-flash-cards", "snippet": "When a <b>Class</b> 1 observation is <b>correctly</b> <b>classified</b> <b>by the model</b>, what would it be called? (according to the textbook) True <b>positive</b> (TP) True negative (TN) False negative (FN) False <b>positive</b> (FP) True <b>positive</b> (TP) A common practice in <b>data</b> partitioning is to partition some percent of the <b>data</b> into the <b>training</b> <b>data</b> <b>set</b> and some percent of the <b>data</b> into the validation <b>data</b> <b>set</b>. Which of the answer below is consistent with the percentage of <b>data</b> in <b>training</b> <b>data</b> <b>set</b> and percentage of <b>data</b> in ...", "dateLastCrawled": "2021-11-12T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Precision vs Recall</b>", "url": "https://www.brainstobytes.com/precision-vs-recall/", "isFamilyFriendly": true, "displayUrl": "https://www.brainstobytes.com/<b>precision-vs-recall</b>", "snippet": "True Positives are <b>all</b> the classes that were <b>correctly</b> <b>classified</b> as <b>positive</b>, now we need to divide it by the total number of actual members of the <b>positive</b> <b>class</b>. If you look at the previous images, you can see that the total of number members of the <b>positive</b> <b>class</b> is given by the sum of the true positives and false negatives.", "dateLastCrawled": "2022-02-02T16:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Decision Tree</b> Tutorials &amp; Notes | Machine Learning | HackerEarth", "url": "https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml-decision-tree/tutorial/", "isFamilyFriendly": true, "displayUrl": "https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml...", "snippet": "Hypothesis <b>Class</b>: <b>Set</b> <b>of all</b> the possible functions; Sample: A <b>set</b> of inputs paired with a label, which is the correct output (also known as the <b>Training</b> <b>Set</b>) Candidate Concept: A concept which we think is the target concept; Testing <b>Set</b>: <b>Similar</b> to the <b>training</b> <b>set</b> and is used to test the candidate concept and determine its performance; Introduction. A <b>decision tree</b> is a tree-like graph with nodes representing the place where we pick an attribute and ask a question; edges represent the ...", "dateLastCrawled": "2022-02-02T20:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning from positive</b> and unlabeled <b>data</b>: a survey | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "snippet": "The goal of one-<b>class</b> classification is to learn a <b>model</b> that identifies <b>examples</b> from a certain <b>class</b>: the <b>positive</b> <b>class</b>, when only <b>examples</b> of that <b>class</b> are available (Khan and Madden 2014). It can be seen as <b>training</b> a binary classifier where the negative <b>class</b> consists <b>of all</b> other possible classes. This is in contrast to PU learning, where the domain of interest is defined by the unlabeled <b>data</b>. Also, the unlabeled <b>data</b> enables finding low-density areas which are likely to be ...", "dateLastCrawled": "2022-02-02T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Out-of-<b>Distribution</b> <b>Detection</b> in Deep Neural Networks | by Neeraj ...", "url": "https://medium.com/analytics-vidhya/out-of-distribution-detection-in-deep-neural-networks-450da9ed7044", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/out-of-<b>distribution</b>-<b>detection</b>-in-deep-neural...", "snippet": "<b>All</b> points above the diagonal line (Random Classifier \u2014 red line) correspond to the situation where the proportion of <b>correctly</b> <b>classified</b> points belonging to the <b>Positive</b> <b>class</b> is greater than ...", "dateLastCrawled": "2022-01-29T23:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A simple guide to building a <b>confusion matrix</b>", "url": "https://blogs.oracle.com/ai-and-datascience/post/a-simple-guide-to-building-a-confusion-matrix", "isFamilyFriendly": true, "displayUrl": "https://blogs.oracle.com/ai-and-<b>data</b>science/post/a-simple-guide-to-building-a...", "snippet": "True <b>Positive</b> (TP) is an outcome where the <b>model</b> <b>correctly</b> predicts the <b>positive</b> <b>class</b>. True Negative (TN) is an outcome where the <b>model</b> <b>correctly</b> predicts the negative <b>class</b>. False <b>Positive</b> (FP) is an outcome where the <b>model</b> incorrectly predicts the <b>positive</b> <b>class</b>. False Negative (FN) is an outcome where the <b>model</b> incorrectly predicts the negative <b>class</b>. <b>Data</b> <b>set</b>: To understand the <b>confusion matrix</b> using a case study, I will use the UCI Machine Learning <b>data</b> <b>set</b> for breast cancer. Here are ...", "dateLastCrawled": "2022-02-03T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Calculate Precision, Recall, and F-Measure for Imbalanced ...", "url": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/precision-rec<b>all</b>-and-f-measure-for-", "snippet": "Recall quantifies the number of <b>positive</b> <b>class</b> predictions made out <b>of all</b> <b>positive</b> <b>examples</b> in the dataset. F-Measure provides a single score that balances both the concerns of precision and recall in one number. Kick-start your project with my new book Imbalanced Classification with Python, including step-by-step tutorials and the Python source code files for <b>all</b> <b>examples</b>. Let\u2019s get started. Update Jan/2020: Improved language about the objective of precision and recall. Fixed typos about ...", "dateLastCrawled": "2022-02-03T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evaluating a <b>Classification Model</b> | Machine Learning, Deep Learning ...", "url": "https://www.ritchieng.com/machine-learning-evaluate-classification-model/", "isFamilyFriendly": true, "displayUrl": "https://www.ritchieng.com/machine-learning-evaluate-<b>classification-model</b>", "snippet": "<b>Training</b> and testing on the same <b>data</b>. Rewards overly complex models that &quot;overfit&quot; the <b>training</b> <b>data</b> and won&#39;t necessarily generalize; Train/test split . Split the dataset into two pieces, so that the <b>model</b> can be trained and tested on different <b>data</b>; Better estimate of out-of-sample performance, but still a &quot;high variance&quot; estimate; Useful due to its speed, simplicity, and flexibility; K-fold cross-validation. Systematically create &quot;K&quot; train/test splits and average the results together ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "8 <b>Tactics to Combat Imbalanced Classes</b> in Your Machine Learning Dataset", "url": "https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/tactics", "snippet": "Coming To Grips With Imbalanced <b>Data</b>. I get emails about <b>class</b> imbalance <b>all</b> the time, for example: I have a binary classification problem and one <b>class</b> is present with 60:1 ratio in my <b>training</b> <b>set</b>. I used the logistic regression and the result seems to just ignores one <b>class</b>. And this: I am working on a classification <b>model</b>. In my dataset I ...", "dateLastCrawled": "2022-02-02T06:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Six Popular <b>Classification Evaluation Metrics</b> In Machine ... - Dataaspirant", "url": "https://dataaspirant.com/six-popular-classification-evaluation-metrics-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>data</b>aspirant.com/six-popular-<b>classification-evaluation-metrics</b>-in-machine-learning", "snippet": "True <b>Positive</b> (TP):- The number of <b>positive</b> samples <b>correctly</b> predicted by the trained <b>model</b> as <b>positive</b> <b>class</b>.This means if the actual target value is 1 <b>model</b> also predicted as 1. True Negative (TN):- The number of negative samples <b>correctly</b> predicted as negative <b>class</b> by the trained <b>model</b>.This means if the actual target value is 0, the <b>model</b> also predicted as 0.", "dateLastCrawled": "2022-02-02T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "10-701/15-781 Machine Learning - Midterm Exam, Fall 2010", "url": "https://cs.cmu.edu/~aarti/Class/10701/exams/midterm2010f_sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.cmu.edu/~aarti/<b>Class</b>/10701/exams/midterm2010f_sol.pdf", "snippet": "<b>training</b> <b>data</b>. 4. As the number of <b>data</b> points grows to in nity, the MAP estimate approaches the MLE estimate for <b>all</b> possible priors. In other words, given enough <b>data</b>, the choice of prior is irrelevant. False: A simple counterexample is the prior which assigns probability 1 to a single choice of parameter . 5. Cross validation can be used to ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Chapter 8</b> Flashcards | Quizlet", "url": "https://quizlet.com/570460057/chapter-8-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/570460057/<b>chapter-8</b>-flash-cards", "snippet": "When a <b>Class</b> 1 observation is <b>correctly</b> <b>classified</b> <b>by the model</b>, what would it be called? (according to the textbook) True <b>positive</b> (TP) True negative (TN) False negative (FN) False <b>positive</b> (FP) True <b>positive</b> (TP) A common practice in <b>data</b> partitioning is to partition some percent of the <b>data</b> into the <b>training</b> <b>data</b> <b>set</b> and some percent of the <b>data</b> into the validation <b>data</b> <b>set</b>. Which of the answer below is consistent with the percentage of <b>data</b> in <b>training</b> <b>data</b> <b>set</b> and percentage of <b>data</b> in ...", "dateLastCrawled": "2021-11-12T04:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "20 Popular Machine Learning <b>Metrics</b>. Part 1 ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/20-popular-machine-learning-<b>metrics</b>-part-1...", "snippet": "Figure 1. A sample confusion matrix. Out of 100 cat images the <b>model</b> has predicted 90 of them <b>correctly</b> and has mis-<b>classified</b> 10 of them. If we refer to the \u201ccat\u201d <b>class</b> as <b>positive</b> and the non-cat <b>class</b> as negative <b>class</b>, then 90 samples predicted as cat are considered as as true-<b>positive</b>, and the 10 samples predicted as non-cat are false negative.; Out of 1000 non-cat images, the <b>model</b> has <b>classified</b> 940 of them <b>correctly</b>, and mis-<b>classified</b> 60 of them.The 940 <b>correctly</b> <b>classified</b> ...", "dateLastCrawled": "2022-01-29T14:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Create confusion matrix chart for classification problem - MATLAB ...", "url": "https://www.mathworks.com/help/stats/confusionchart.html", "isFamilyFriendly": true, "displayUrl": "https://www.mathworks.com/help/stats/<b>confusionchart</b>.html", "snippet": "The percentages of <b>correctly</b> <b>classified</b> observations <b>can</b> <b>be thought</b> of as <b>class</b>-wise precisions (or <b>positive</b> predictive values). &#39;total-normalized&#39; Display the number of <b>correctly</b> and incorrectly <b>classified</b> observations for each predicted <b>class</b> as percentages of the total number of observations.", "dateLastCrawled": "2022-01-29T10:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "8 <b>Tactics to Combat Imbalanced Classes</b> in Your Machine Learning Dataset", "url": "https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/tactics", "snippet": "if I train my <b>model</b> over <b>all</b> the <b>positive</b> instances and an equal number of negative instances, there is a lot of unused <b>data</b>. Is there a reasonable way that I <b>can</b> perform several iterations, each with different negative instances, and combine the results into one <b>model</b>? Thanks in advance. Reply. Jon D. August 19, 2017 at 10:44 am # The situation you describe is exactly what oversampling is. Just use fewer iterations that what you would with undersampling. Reply. Vijay lokhande January 5 ...", "dateLastCrawled": "2022-02-02T06:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sentiment analysis</b>: Machine Learning Approach. | by Safdar Mirza | Medium", "url": "https://medium.com/@safdar.mirza94/sentiment-analysis-machine-learning-approach-2adb57a1af91", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@safdar.mirza94/<b>sentiment-analysis</b>-machine-learning-approach-2adb57...", "snippet": "They build a <b>model</b> by <b>training</b> multinomial Naive Bayes classifier in WEKA with n-gram and sentiwordnet as features. At last, they acquired an F-score of 0.504 for Bengali-English and 0.562 for ...", "dateLastCrawled": "2022-02-03T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: <b>Data</b> ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "Put another way it is the number of <b>positive</b> predictions divided by the number of <b>positive</b> <b>class</b> values in the test <b>data</b>. It is also called Sensitivity or the True <b>Positive</b> Rate. Recall <b>can</b> <b>be thought</b> of as a measure of a classifiers completeness. A low recall indicates many False Negatives. F1 Score (or F-score): A weighted average of precision and recall. I would also advise you to take a look at the following: Kappa (or Cohen\u2019s kappa): Classification accuracy normalized by the imbalance ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What metrics should be used for evaluating a <b>model</b> on an <b>imbalanced</b> ...", "url": "https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/what-metrics-should-we-use-on-<b>imbalanced</b>-<b>data</b>-<b>set</b>...", "snippet": "Example #3 \u2014 Majority of <b>positive</b> samples and not <b>all</b> <b>positive</b> samples are detected \u2014 Both metrics are the same in this case. Now, 9 samples are <b>positive</b> and 1 is negative. The <b>model</b> predicts 7 of the samples as <b>positive</b> (<b>all</b> are <b>positive</b>) and 3 as negative. The basic metrics are: TP = 7, FP = 0, TN = 1, FN = 2. The advanced metrics are:", "dateLastCrawled": "2022-01-29T20:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "10-701 Midterm Exam Solutions, Spring 2007", "url": "https://www.cs.cmu.edu/~aarti/Class/10701/exams/midterm2007s-solution.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~aarti/<b>Class</b>/10701/exams/midterm2007s-solution.pdf", "snippet": "[2 points] true/false The maximum likelihood <b>model</b> parameters (\u03b1) <b>can</b> be learned using linear regression for the <b>model</b>: yi = log(x \u03b11 1 e \u03b12) + \u01eb i where \u01ebi \u223cN(0,\u03c32) iid noise. \u22c6 SOLUTION: This is true. yi = log(x\u03b11 1 e \u03b12) + \u01eb i = \u03b11 logx1 + \u03b12 + \u01ebi, which is linear in \u03b11 and \u03b12. Also, assuming x1 &gt; 0. 6. [2 points] true/false In AdaBoost weights of the misclassi\ufb01ed <b>examples</b> go up by the same multiplicative factor. \u22c6 SOLUTION: True, follows from the update equation. 7 ...", "dateLastCrawled": "2022-02-01T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "machine learning - Calculating <b>F-Score</b>, which is the &quot;<b>positive</b>&quot; <b>class</b> ...", "url": "https://stats.stackexchange.com/questions/191645/calculating-f-score-which-is-the-positive-class-the-majority-or-minority-cla", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/191645", "snippet": "$\\begingroup$ I never gave this much <b>thought</b> before. In reference to your &quot;explicit question&quot; above, in the case when one is looking for someone with cancer, I would think a 1 in the <b>training</b> <b>set</b> would mean they have cancer. Meaning you are trying to increase the TP rate in your <b>model</b>. As mentioned below, F1-<b>score</b> may not be the best metric to ...", "dateLastCrawled": "2022-01-24T21:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Meaning of <b>correctly classified</b> instances weka - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/12252254/meaning-of-correctly-classified-instances-weka", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/12252254", "snippet": "Based on your <b>training</b> <b>set</b>, 69.92% of your instances are <b>classified</b> as <b>positive</b>. If the labels for the test <b>set</b>, that is the correct answers, indicate that they are <b>all</b> <b>positive</b>, then that makes 69.92% correct. If the test <b>set</b> (and thus the classification) is the same, but you switch the correct answers, then of course, the percentage correct ...", "dateLastCrawled": "2022-01-24T20:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Exam 2 Class Questions</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/460364182/exam-2-class-questions-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/460364182/<b>exam-2-class-questions</b>-flash-cards", "snippet": "A supervised method and <b>model</b> constructed using a <b>training</b> <b>data</b> <b>set</b> would be. A. Testing B. Logistic Regression C. Machine Learning D. Classification. Classification _____ is the probability that the transaction contains the antecedent and the consequent. Support. Which of the following is NOT true about Machine Learning? A. Machine Learning is a form of supervised learning. B. Machine Learning requires <b>training</b>, testing, and evaluation. C. Machine Learning is a subset of Deep Learning D ...", "dateLastCrawled": "2021-12-02T12:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Learning from positive</b> and unlabeled <b>data</b>: a survey | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "snippet": "The goal of one-<b>class</b> classification is to learn a <b>model</b> that identifies <b>examples</b> from a certain <b>class</b>: the <b>positive</b> <b>class</b>, when only <b>examples</b> of that <b>class</b> are available (Khan and Madden 2014). It <b>can</b> be seen as <b>training</b> a binary classifier where the negative <b>class</b> consists <b>of all</b> other possible classes. This is in contrast to PU learning, where the domain of interest is defined by the unlabeled <b>data</b>. Also, the unlabeled <b>data</b> enables finding low-density areas which are likely to be ...", "dateLastCrawled": "2022-02-02T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Data</b> MiningJHan Chapter8 Classification", "url": "http://cis.csuohio.edu/~sschung/CIS660/Data%20MiningJHan_Chapter8_Classification.pdf", "isFamilyFriendly": true, "displayUrl": "cis.csuohio.edu/~sschung/CIS660/<b>Data</b> MiningJHan_Chapter8_<b>Class</b>ification.pdf", "snippet": "The <b>class</b> labels of <b>training</b> <b>data</b> is unknown Given a <b>set</b> of measurements, observations, etc. with the aim of establishing the existence of classes or clusters in the <b>data</b>. 4 Classification predicts categorical <b>class</b> labels (discrete or nominal) classifies <b>data</b> (constructs a <b>model</b>) based on the <b>training</b> <b>set</b> and the values (<b>class</b> labels) in a classifying attribute and uses it in classifying new <b>data</b> Numeric Prediction models continuous-valued functions, i.e., predicts unknown or missing values ...", "dateLastCrawled": "2022-01-29T21:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "One-<b>Class</b> Classification Algorithms for Imbalanced Datasets", "url": "https://machinelearningmastery.com/one-class-classification-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/one-<b>class</b>-<b>class</b>ification-algorithms", "snippet": "It may also be appropriate where the number of <b>positive</b> cases in the <b>training</b> <b>set</b> is so few that they are not worth including in the <b>model</b>, such as a few tens of <b>examples</b> or fewer. Or for problems where no <b>examples</b> of <b>positive</b> cases <b>can</b> be collected prior to <b>training</b> a <b>model</b>. To be clear, this adaptation of one-<b>class</b> classification algorithms for imbalanced classification is unusual but <b>can</b> be effective on some problems. The downside of this approach is that any <b>examples</b> of outliers ...", "dateLastCrawled": "2022-02-02T17:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Confusion Matrix</b> and <b>Class</b> Statistics | by ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/confusion-matrix-and-class-statistics-68b79f4f510b", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>confusion-matrix</b>-and-<b>class</b>-statistics-68b79f4f510b", "snippet": "We divide the number of true positives by the number <b>of all</b> <b>positive</b> events in the dataset: the <b>positive</b> <b>class</b> events predicted <b>correctly</b> (TP) and the <b>positive</b> <b>class</b> events predicted incorrectly (FN). The <b>model</b> in this example reaches the sensitivity value of 0.882. This means that about 88 % of the spam emails in the dataset were <b>correctly</b> predicted as spam.", "dateLastCrawled": "2022-01-31T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evaluating a <b>Classification Model</b> | Machine Learning, Deep Learning ...", "url": "https://www.ritchieng.com/machine-learning-evaluate-classification-model/", "isFamilyFriendly": true, "displayUrl": "https://www.ritchieng.com/machine-learning-evaluate-<b>classification-model</b>", "snippet": "<b>Training</b> and testing on the same <b>data</b>. Rewards overly complex models that &quot;overfit&quot; the <b>training</b> <b>data</b> and won&#39;t necessarily generalize; Train/test split . Split the dataset into two pieces, so that the <b>model</b> <b>can</b> be trained and tested on different <b>data</b>; Better estimate of out-of-sample performance, but still a &quot;high variance&quot; estimate; Useful due to its speed, simplicity, and flexibility; K-fold cross-validation. Systematically create &quot;K&quot; train/test splits and average the results together ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning 99+ Most Important MCQ</b> - JOB SAARNEE", "url": "https://www.jobsaarnee.com/2020/09/machine-learning-100-most-important-mcq.html", "isFamilyFriendly": true, "displayUrl": "https://www.jobsaarnee.com/2020/09/machine-learning-100-most-important-mcq.html", "snippet": "Use a separate <b>set</b> of <b>examples</b>, distinct from the <b>training</b> <b>examples</b>, to evaluate the utility of post-pruning nodes from the tree. Use <b>all</b> the available <b>data</b> for <b>training</b>; Use an explicit measure of the complexity for encoding the <b>training</b> <b>examples</b> and the decision tree, halting growth of the tree when this encoding size is minimized. <b>All</b> of the ...", "dateLastCrawled": "2022-02-02T11:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Training Data</b> and Test <b>Data</b> - <b>Tutorialspoint</b>", "url": "https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_training_test_data.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/.../machine_learning_with_python_<b>training</b>_test_<b>data</b>.htm", "snippet": "If the test <b>set</b> does contain <b>examples</b> from the <b>training</b> <b>set</b>, it will be difficult to assess whether the algorithm has learned to generalize from the <b>training</b> <b>set</b> or has simply memorized it. A program that generalizes well will be able to effectively perform a task with new <b>data</b>. In contrast, a program that memorizes the <b>training data</b> by learning an overly complex <b>model</b> could predict the values of the response variable for the <b>training</b> <b>set</b> accurately, but will fail to predict the value of the ...", "dateLastCrawled": "2022-01-31T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Calculate Precision, Recall, and F-Measure for Imbalanced ...", "url": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/precision-rec<b>all</b>-and-f-measure-for-", "snippet": "Precision evaluates the fraction of correct <b>classified</b> instances among the ones <b>classified</b> as <b>positive</b> \u2026 \u2014 Page 52, Learning from Imbalanced <b>Data</b> Sets, 2018. Precision for Binary Classification. In an imbalanced classification problem with two classes, precision is calculated as the number of true positives divided by the total number of true positives and false positives. Precision = TruePositives / (TruePositives + FalsePositives) The result is a value between 0.0 for no precision and ...", "dateLastCrawled": "2022-02-03T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Precision vs Recall</b>", "url": "https://www.brainstobytes.com/precision-vs-recall/", "isFamilyFriendly": true, "displayUrl": "https://www.brainstobytes.com/<b>precision-vs-recall</b>", "snippet": "True Positives are <b>all</b> the classes that were <b>correctly</b> <b>classified</b> as <b>positive</b>, now we need to divide it by the total number of actual members of the <b>positive</b> <b>class</b>. If you look at the previous images, you <b>can</b> see that the total of number members of the <b>positive</b> <b>class</b> is given by the sum of the true positives and false negatives.", "dateLastCrawled": "2022-02-02T16:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>4 Reasons Your Machine Learning Model is Wrong</b> (and How to Fix It ...", "url": "https://www.kdnuggets.com/2016/12/4-reasons-machine-learning-model-wrong.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2016/12/4-reasons-machine-learning-<b>model</b>-wrong.html", "snippet": "Similarly, increasing the number of <b>training</b> <b>examples</b> <b>can</b> help in cases of high variance, helping the machine learning algorithm build a more generalizable <b>model</b>. For balancing cases of Low Precision and Low Recall, you <b>can</b> alter the probability threshold at which you classify the <b>positive</b> vs. negative <b>class</b> (see figure above).", "dateLastCrawled": "2022-02-02T06:32:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "In a <b>machine</b> <b>learning</b> context, the \u201cthings\u201d are data entities (instances), and the causes are predictions. In (binary) classification, for example, the above pattern might be used to explain the learner\u2019s prediction for a query instance: A belongs to the <b>positive</b> <b>class</b>, and B is similar to A, hence B is likely to be <b>positive</b>, too.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning from positive</b> and unlabeled data: a survey - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "snippet": "<b>Learning from positive</b> and unlabeled data or PU <b>learning</b> is the setting where a learner only has access to <b>positive</b> examples and unlabeled data. The assumption is that the unlabeled data can contain both <b>positive</b> and negative examples. This setting has attracted increasing interest within the <b>machine</b> <b>learning</b> literature as this type of data naturally arises in applications such as medical diagnosis and knowledge base completion. This article provides a survey of the current state of the art ...", "dateLastCrawled": "2022-02-02T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "CS 540 Lecture Notes: <b>Machine Learning</b>", "url": "http://pages.cs.wisc.edu/~dyer/cs540/notes/learning.html", "isFamilyFriendly": true, "displayUrl": "pages.cs.wisc.edu/~dyer/cs540/notes/<b>learning</b>.html", "snippet": "<b>Analogy</b> Determine correspondence between two different representations Discovery Unsupervised, specific goal not given Genetic Algorithms; Reinforcement Only feedback (<b>positive</b> or negative reward) given at end of a sequence of steps. Requires assigning reward to steps by solving the credit assignment problem--which steps should receive credit or blame for a final result? The Inductive <b>Learning</b> Problem. Extrapolate from a given set of examples so that we can make accurate predictions about ...", "dateLastCrawled": "2021-11-12T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Word Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C16-1332.pdf", "snippet": "Word Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man + Woman = Queen Aleksandr Drozd y, Anna Gladkova z, Satoshi Matsuoka y yTokyo Institute of Technology, Meguro-ku, Tokyo 152-8550, Japan alex@smg.is.titech.ac.jp, matsu@is.titech.ac.jp z The University of Tokyo, Meguro-ku, Tokyo 153-8902 Japan gladkova@phiz.c.u-tokyo.ac.jp Abstract Solving word analogies became one of the most popular benchmarks for word embeddings on the assumption that linear relations between word pairs ...", "dateLastCrawled": "2022-01-20T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> \u2014 Multiclass <b>Classification</b> with Imbalanced Dataset ...", "url": "https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine</b>-<b>learning</b>-multi<b>class</b>-<b>classification</b>-with...", "snippet": "The skewed distribution makes many conventional <b>machine</b> <b>learning</b> algorithms less effective, especially in predicting minority <b>class</b> examples. In order to do so, let us first understand the problem at hand and then discuss the ways to overcome those. Multiclass <b>Classification</b>: A <b>classification</b> task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multi-<b>class</b> <b>classification</b> makes the assumption that each sample is assigned to one and ...", "dateLastCrawled": "2022-02-02T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>machine</b> <b>learning</b> - What if I train a classifier with only <b>positive</b> ...", "url": "https://stats.stackexchange.com/questions/237538/what-if-i-train-a-classifier-with-only-positive-example", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/237538/what-if-i-train-a-<b>class</b>ifier-with...", "snippet": "Since I am interested to find outliers (anything other than <b>positive</b> <b>class</b>), will this model work? What is the necessity of using even small amount of negative <b>class</b> or unlabelled examples? Using <b>analogy</b> for explanation is much appreciated. <b>machine</b>-<b>learning</b> classification one-<b>class</b>. Share. Cite. Improve this question. Follow edited Jan 22 &#39;18 at 21:56. N. F. asked Sep 29 &#39;16 at 11:05. N. F. N. F. 282 1 1 gold badge 5 5 silver badges 14 14 bronze badges $\\endgroup$ 1 $\\begingroup$ You are ...", "dateLastCrawled": "2022-01-20T10:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>10 Machine Learning Terms Explained in Simple</b> English - AYLIEN News API", "url": "https://aylien.com/blog/10-machine-learning-terms-explained-in-simple-english", "isFamilyFriendly": true, "displayUrl": "https://aylien.com/blog/<b>10-machine-learning-terms-explained-in-simple</b>-english", "snippet": "<b>Machine</b> <b>learning</b> enables computers to act and make data-driven decisions rather than being explicitly programmed to carry out a certain task. <b>Machine</b> <b>Learning</b> programs are also designed to learn and improve over time when exposed to new data. <b>Machine</b> <b>learning</b> has been at the center of many technological advancements in recent years such as self-driving cars, computer vision and speech recognition systems. Supervised <b>Learning</b>. Where a program is \u201ctrained\u201d on a pre-defined dataset. Based ...", "dateLastCrawled": "2022-01-28T01:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Support Vector <b>Machine</b> (SVM) Algorithm. | by Nadeem | MLearning.ai | Medium", "url": "https://medium.com/mlearning-ai/support-vector-machine-svm-algorithm-a5acaa48fe3a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/support-vector-<b>machine</b>-svm-algorithm-a5acaa48fe3a", "snippet": "X +b \u2265 \u00b11 (where the value of Yi is \u00b11, +1 for <b>positive</b> <b>class</b> and -1 for negative <b>class</b>). W^T is the vector perpendicular to the hyperplane referred to as the weight vector. The above equation ...", "dateLastCrawled": "2022-01-26T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the best example for <b>false negative, false positive, true</b> ...", "url": "https://www.quora.com/What-is-the-best-example-for-false-negative-false-positive-true-negative-and-true-positive-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-best-example-for-<b>false-negative-false-positive-true</b>...", "snippet": "Answer (1 of 6): There was a funny picture I\u2019d come across a while ago [1]: Extending this example, a man whose test results say \u201cNot pregnant\u201d is True Negative, and a pregnant woman whose test results say \u201cPregnant\u201d is True <b>Positive</b>. A good way to understand it is this: * First, define what...", "dateLastCrawled": "2022-01-22T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Teaching and Learning with Analogies</b>", "url": "https://www.researchgate.net/publication/226440705_Teaching_and_Learning_with_Analogies", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/226440705_<b>Teaching_and_Learning_with_Analogies</b>", "snippet": "<b>Teaching and learning with analogies</b> friend or foe (Harrison &amp; Treagust) reviews. research studies to explore the advant ages and disadvantages of <b>analogy</b> and. provides a set of principles ...", "dateLastCrawled": "2022-02-03T05:22:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(positive class)  is like +(set of all training data examples that are correctly classified by the model)", "+(positive class) is similar to +(set of all training data examples that are correctly classified by the model)", "+(positive class) can be thought of as +(set of all training data examples that are correctly classified by the model)", "+(positive class) can be compared to +(set of all training data examples that are correctly classified by the model)", "machine learning +(positive class AND analogy)", "machine learning +(\"positive class is like\")", "machine learning +(\"positive class is similar\")", "machine learning +(\"just as positive class\")", "machine learning +(\"positive class can be thought of as\")", "machine learning +(\"positive class can be compared to\")"]}