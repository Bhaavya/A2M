{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "cs229.stanford.edu", "url": "http://cs229.stanford.edu/proj2021spr/report2/81994495.pdf", "isFamilyFriendly": true, "displayUrl": "cs229.stanford.edu/proj2021spr/report2/81994495.pdf", "snippet": "Three <b>gradient</b> <b>descent</b> optimization algorithms including traditional <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>), RMSprop, and Adam, are used to tune network weights iteratively. SGI) is adapted as it revealed higher performance. The best performance of model is achieved when dropout rate was 0.2. The optimal number of fully-connected layers is determined to be 6 layers after spanning a range of 0 to 10 layers. For the number of neurons in each layer, 300 neurons are selected after testing values from ...", "dateLastCrawled": "2022-01-15T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Decentralized learning works: An empirical comparison of gossip ...", "url": "https://www.sciencedirect.com/science/article/pii/S0743731520303890", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0743731520303890", "snippet": "We train linear models using <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) based on the logistic regression loss function. Our experimental methodology involves several scenarios, including smartphone churn traces collected by the application Stunner . We also vary the communication pattern (continuous or bursty) and the network size. In addition, we also evaluate different assumptions about the label distribution, that is, whether a given worker has a biased or unbiased subset of the training samples ...", "dateLastCrawled": "2022-01-23T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Recognition of Human Activities Using Continuous Autoencoders with ...", "url": "https://europepmc.org/article/MED/26861319", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/26861319", "snippet": "In order to shorten the training time, we propose a new fast <b>stochastic</b> <b>gradient</b> <b>descent</b> (FSGD) algorithm to update the gradients of CAE. The reconstruction of a swiss-roll dataset experiment demonstrates that the CAE can fit continuous data better than the basic autoencoder, and the training time can be reduced by an FSGD algorithm. In the experiment of human activities&#39; recognition, time and frequency domain feature extract (TFFE) method is raised to extract features from the original ...", "dateLastCrawled": "2021-07-27T17:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Sensors | Free Full-Text | Recognition of Human Activities Using ...", "url": "https://www.mdpi.com/1424-8220/16/2/189/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/16/2/189/htm", "snippet": "In order to shorten the training time, we propose a new fast <b>stochastic</b> <b>gradient</b> <b>descent</b> (FSGD) algorithm to update the gradients of CAE. The reconstruction of a swiss-roll dataset experiment demonstrates that the CAE can fit continuous data better than the basic autoencoder, and the training time can be reduced by an FSGD algorithm. In the experiment of human activities\u2019 recognition, time and frequency domain feature extract (TFFE) method is raised to extract features from the original ...", "dateLastCrawled": "2021-10-20T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Feature learning for <b>Human Activity Recognition</b> using Convolutional ...", "url": "https://link.springer.com/article/10.1007/s42486-020-00026-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s42486-020-00026-2", "snippet": "Issues <b>like</b> label noise may affect the comparison with HCFs leading to an incorrect evaluation. Consequently, two ... Evaluation of different configurations was made using Adam (Kingma and Ba 2015) and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) optimizers, with different number of layers, kernel sizes, and number of filters. For final training of the CNN feature extractor, the <b>SGD</b> optimizer was used. Compared to the Adam optimizer, <b>SGD</b> provides, in some cases, better generalization on unseen data ...", "dateLastCrawled": "2022-02-03T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Human <b>Activity Recognition - Using Deep Learning Model - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/human-activity-recognition-using-deep-learning-model/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/human-<b>activity-recognition-using-deep-learning</b>-model", "snippet": "Human <b>Activity Recognition \u2013 Using Deep Learning</b> Model. Human activity recognition using smartphone sensors <b>like</b> accelerometer is one of the hectic topics of research. HAR is one of the time series classification problem. In this project various machine learning and deep learning models have been worked out to get the best final result.", "dateLastCrawled": "2022-02-03T01:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "1. Introduction", "url": "http://cs230.stanford.edu/projects_spring_2021/reports/11.pdf", "isFamilyFriendly": true, "displayUrl": "cs230.stanford.edu/projects_spring_2021/reports/11.pdf", "snippet": "of identifying 6 types of human daily activities including <b>walking</b>, jogging, going upstairs, going <b>downstairs</b>, biking and standing, by using accessible signals collected from 9 subjects\u2019 smartphones platform. To eliminate the effect of smartphone\u2019s orientation, reduce calculation cost and speed up activity identification, the experiment investigates whether it is possible to successfully recognize human activity through the acceleration data in only one direction (gravity direction in ...", "dateLastCrawled": "2021-09-08T05:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Chapter 9 Multi-user Validation | Behavior Analysis with Machine ...", "url": "https://enriquegit.github.io/behavior-free/multiuser.html", "isFamilyFriendly": true, "displayUrl": "https://enriquegit.github.io/behavior-free/multiuser.html", "snippet": "Chapter 9 Multi-user Validation | Behavior Analysis with Machine Learning Using R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based ...", "dateLastCrawled": "2022-01-30T15:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Human activity recognition</b> with smartphone sensors ... - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0957417416302056", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0957417416302056", "snippet": "Convnet achieved almost perfect classification for moving activities (99.66%), especially very similar activity classes <b>like</b> <b>walking</b> upstairs and <b>walking</b> <b>downstairs</b>, which were previously perceived to be very difficult in classification (Bao and Intille, 2004, Kwapisz et al., 2010, Wu et al., 2012). Upon close look, the few confusion cases on moving activities were from subject 13, indicating that this particular subject has a very different style of <b>walking</b> compared to the rest of the 29 ...", "dateLastCrawled": "2022-01-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Homogeneous Data Normalization and Deep Learning: A Case Study in Human ...", "url": "https://www.readkong.com/page/homogeneous-data-normalization-and-deep-learning-a-case-2130902", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/homogeneous-data-normalization-and-deep-learning-a-case...", "snippet": "Predicted Class <b>Walking</b> <b>Walking</b> Standing <b>Walking</b> Upstairs <b>Downstairs</b> <b>walking</b> upstairs 117 16 1 0 <b>walking</b> <b>downstairs</b> 89 52 0 3 Actual Class standing 3 10 15 0 <b>walking</b> 1 9 7 12 Besides, the classification results of the data acquired from the gyroscope sensor after the application of the Z-Score normalizer were analyzed, verifying that the DNN method implemented reported an accuracy of 56.81%, a precision of 63.63%, a recall value of 53.08%, and an F1-Score of 57.88%. 3.5. Non-Normalized Data ...", "dateLastCrawled": "2021-11-27T15:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Decentralized learning works: An empirical comparison of gossip ...", "url": "https://www.sciencedirect.com/science/article/pii/S0743731520303890", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0743731520303890", "snippet": "We train linear models using <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) based on the logistic regression loss function. Our experimental methodology involves several scenarios, including smartphone churn traces collected by the application Stunner . We also vary the communication pattern (continuous or bursty) and the network size. In addition, we also evaluate different assumptions about the label distribution, that is, whether a given worker has a biased or unbiased subset of the training samples ...", "dateLastCrawled": "2022-01-23T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "MixNN: Protection of Federated Learning Against Inference Attacks by ...", "url": "https://deepai.org/publication/mixnn-protection-of-federated-learning-against-inference-attacks-by-mixing-neural-network-layers", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/mixnn-protection-of-federated-learning-against...", "snippet": "In this work, we consider <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) to do this optimization. <b>SGD</b> is an iterative approach where the optimizer receives a batch of training data and updates the model parameters \u03b8 at each iteration according to both the direction of the <b>gradient</b> of the objective function and a learning rate \u03b7. which scales the update. Once the <b>gradient</b> is close to zero, the model has converged to a local minimum and the training is finished. The model is evaluated through its ...", "dateLastCrawled": "2021-12-27T15:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "IRDS: Datasets for Mini-Projects - inf.ed.ac.uk", "url": "https://www.inf.ed.ac.uk/teaching/courses/irds/miniproject-datasets.html", "isFamilyFriendly": true, "displayUrl": "https://www.inf.ed.ac.uk/teaching/courses/irds/miniproject-datasets.html", "snippet": "Semi-<b>stochastic</b> <b>gradient</b> <b>descent</b> (S2GD): This method in each iterations does the work <b>similar</b> to <b>SGD</b>, but the method has a built in variance reduction strategy which makes it much more efficient (<b>similar</b> to SDCA).", "dateLastCrawled": "2022-01-31T19:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Recognition of Human Activities Using Continuous Autoencoders with ...", "url": "https://europepmc.org/article/MED/26861319", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/26861319", "snippet": "In order to shorten the training time, we propose a new fast <b>stochastic</b> <b>gradient</b> <b>descent</b> (FSGD) algorithm to update the gradients of CAE. The reconstruction of a swiss-roll dataset experiment demonstrates that the CAE can fit continuous data better than the basic autoencoder, and the training time can be reduced by an FSGD algorithm. In the experiment of human activities&#39; recognition, time and frequency domain feature extract (TFFE) method is raised to extract features from the original ...", "dateLastCrawled": "2021-07-27T17:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Feature learning for <b>Human Activity Recognition</b> using Convolutional ...", "url": "https://link.springer.com/article/10.1007/s42486-020-00026-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s42486-020-00026-2", "snippet": "Evaluation of different configurations was made using Adam (Kingma and Ba 2015) and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) optimizers, with different number of layers, kernel sizes, and number of filters. For final training of the CNN feature extractor, the <b>SGD</b> optimizer was used. Compared to the Adam optimizer, <b>SGD</b> provides, in some cases, better generalization on unseen data (Keskar and Socher 2017), and that is also for the case of IMU data for HAR (Cruciani et al. 2019a). The training stops ...", "dateLastCrawled": "2022-02-03T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Sensors | Free Full-Text | Recognition of Human Activities Using ...", "url": "https://www.mdpi.com/1424-8220/16/2/189/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/16/2/189/htm", "snippet": "In order to shorten the training time, we propose a new fast <b>stochastic</b> <b>gradient</b> <b>descent</b> (FSGD) algorithm to update the gradients of CAE. The reconstruction of a swiss-roll dataset experiment demonstrates that the CAE can fit continuous data better than the basic autoencoder, and the training time can be reduced by an FSGD algorithm. In the experiment of human activities\u2019 recognition, time and frequency domain feature extract (TFFE) method is raised to extract features from the original ...", "dateLastCrawled": "2021-10-20T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Human <b>Activity Recognition - Using Deep Learning Model - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/human-activity-recognition-using-deep-learning-model/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/human-<b>activity-recognition-using-deep-learning</b>-model", "snippet": "Human <b>Activity Recognition \u2013 Using Deep Learning</b> Model. Human activity recognition using smartphone sensors like accelerometer is one of the hectic topics of research. HAR is one of the time series classification problem. In this project various machine learning and deep learning models have been worked out to get the best final result.", "dateLastCrawled": "2022-02-03T01:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Chapter 9 Multi-user Validation | Behavior Analysis with Machine ...", "url": "https://enriquegit.github.io/behavior-free/multiuser.html", "isFamilyFriendly": true, "displayUrl": "https://enriquegit.github.io/behavior-free/multiuser.html", "snippet": "Chapter 9 Multi-user Validation | Behavior Analysis with Machine Learning Using R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based ...", "dateLastCrawled": "2022-01-30T15:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Homogeneous Data Normalization and Deep Learning: A Case Study in Human ...", "url": "https://www.readkong.com/page/homogeneous-data-normalization-and-deep-learning-a-case-2130902", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/homogeneous-data-normalization-and-deep-learning-a-case...", "snippet": "Predicted Class <b>Walking</b> <b>Walking</b> Standing <b>Walking</b> Upstairs <b>Downstairs</b> <b>walking</b> upstairs 117 16 1 0 <b>walking</b> <b>downstairs</b> 89 52 0 3 Actual Class standing 3 10 15 0 <b>walking</b> 1 9 7 12 Besides, the classification results of the data acquired from the gyroscope sensor after the application of the Z-Score normalizer were analyzed, verifying that the DNN method implemented reported an accuracy of 56.81%, a precision of 63.63%, a recall value of 53.08%, and an F1-Score of 57.88%. 3.5. Non-Normalized Data ...", "dateLastCrawled": "2021-11-27T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Human activity recognition</b> with smartphone sensors ... - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0957417416302056", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0957417416302056", "snippet": "Convnet achieved almost perfect classification for moving activities (99.66%), especially very <b>similar</b> activity classes like <b>walking</b> upstairs and <b>walking</b> <b>downstairs</b>, which were previously perceived to be very difficult in classification (Bao and Intille, 2004, Kwapisz et al., 2010, Wu et al., 2012). Upon close look, the few confusion cases on moving activities were from subject 13, indicating that this particular subject has a very different style of <b>walking</b> compared to the rest of the 29 ...", "dateLastCrawled": "2022-01-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Decentralized learning works: An empirical comparison of gossip ...", "url": "https://www.sciencedirect.com/science/article/pii/S0743731520303890", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0743731520303890", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) ... so its effect <b>can</b> <b>be thought</b> of correcting the learning rate, especially when | H | is small. 5. Experimental setup 5.1. Datasets. We used three datasets from the UCI machine learning repository to test the performance of our algorithms. The first is the Spambase (SPAM E-mail Database) dataset containing a collection of emails. Here, the task is to decide whether an email is spam or not. The emails are represented by high level features, mostly word or ...", "dateLastCrawled": "2022-01-23T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Entropy | Free Full-Text | Why Do Big Data and Machine Learning Entail ...", "url": "https://www.mdpi.com/1099-4300/23/3/297/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/23/3/297/htm", "snippet": "The <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) has been widely used in deep learning with great success because of the computational efficiency [80,81]. The <b>gradient</b> noise (GN) in the <b>SGD</b> algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. The machine-learning tasks are usually considered as solving the following optimization problem:", "dateLastCrawled": "2022-02-02T19:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Why Do Big Data and <b>Machine Learning Entail the Fractional Dynamics</b>?", "url": "https://www.researchgate.net/publication/349681844_Why_Do_Big_Data_and_Machine_Learning_Entail_the_Fractional_Dynamics", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349681844_Why_Do_Big_Data_and_Machine...", "snippet": "The <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) randomly selects times during the training process. Therefore, the cost function bounces up and down, decreasing on average, which is good for escape from ...", "dateLastCrawled": "2022-02-02T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "QUOTIENT: Two-<b>Party Secure Neural Network Training and Prediction</b> | DeepAI", "url": "https://deepai.org/publication/quotient-two-party-secure-neural-network-training-and-prediction", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/quotient-two-<b>party-secure-neural-network-training-and</b>...", "snippet": "They propose techniques based on secret-sharing to implement a <b>stochastic</b> <b>gradient</b> <b>descent</b> procedure for training linear/logistic regressors and DNNs in two-party computation. While the presented techniques are practical and general, there are three notable downsides: 1. They require an \u201coffline\u201d phase, that while being data-independent, takes up most of the time (more than . 80 hours for a 3-layer DNN on the MNIST dataset in the 2-Party Computation (2PC) setting); 2. Their techniques ...", "dateLastCrawled": "2021-12-25T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Human <b>Activity Recognition using Machine Learning Classification</b> ...", "url": "https://www.researchgate.net/publication/339566682_Human_Activity_Recognition_using_Machine_Learning_Classification_Techniques", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339566682_Human_Activity_Recognition_using...", "snippet": "The purpose of HAR is to detect, recognize, and classify human activities. HAR is one of the important technologies to monitor dynamism of human [1]. Various benefits of HAR in the health sector ...", "dateLastCrawled": "2022-01-14T21:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Data Science Question and Answer</b> | PDF | Machine Learning | Statistical ...", "url": "https://www.scribd.com/document/354655052/Data-Science-Question-and-Answer", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/354655052/<b>Data-Science-Question-and-Answer</b>", "snippet": "Tags: machine-learning (Prev Q) (Next Q), neuralnetwork (Next Q), algorithms (Next Q) Im currently working on implementing <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) for neural nets using backpropagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.", "dateLastCrawled": "2022-01-27T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Applied Sciences | Free Full-Text | Exploring Geometric Feature Hyper ...", "url": "https://www.mdpi.com/2076-3417/10/6/1994/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2076-3417/10/6/1994/htm", "snippet": "The data captured six activities: <b>walking</b>, <b>walking</b>_upstairs, <b>walking</b>_<b>downstairs</b>, sitting, standing, and laying. The hypothesis of this experiment is that the labels <b>walking</b>, <b>walking</b>_upstairs, <b>walking</b>_<b>downstairs</b> are identified by an abstract concept (say) mobile and the other three labels sitting, standing, and laying by abstract concept (say) immobile. In this experiment also classification operation <b>can</b> be used to prove the hypothesis.", "dateLastCrawled": "2021-12-17T13:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Titlebot/davidjayharris.txt at master \u00b7 <b>davharris/Titlebot</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/davharris/Titlebot/blob/master/data/davidjayharris.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/davharris/Titlebot/blob/master/data/davidjayharris.txt", "snippet": "This is exciting! MCMC on a budget. Like <b>stochastic</b> <b>gradient</b> <b>descent</b>, but for MCMC. via @StatMLPapers @ZachWeiner @brianwolven Just noticed a Typo. Should have been 1.8e-5 m^3. Yikes! @Mattcademia @Mchl_Wolfe @ucfagls <b>can</b> you point to an example where curated data was deemed copyrightable in the US? @michaelhoffman @hylopsar @dgmacarthur @algaebarnacle @jtleek good frequency properties too. Controlling false discovery rate is useful 2/2: @michaelhoffman @hylopsar @dgmacarthur @algaebarnacle ...", "dateLastCrawled": "2021-09-07T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Data Science: Questions and Answers</b> | George A Duckett | download", "url": "https://b-ok.africa/book/2701277/91ea70", "isFamilyFriendly": true, "displayUrl": "https://b-ok.africa/book/2701277/91ea70", "snippet": "You <b>can</b> use this book to look up commonly asked questions, browse questions on a particular topic, compare answers to common topics, check out the original source and much more. This book has been designed to be very easy to use, with many internal references set up that makes browsing in many different ways possible. Topics covered include: Machine Learning, Bigdata, Data Mining, Classification, Neuralnetwork, Statistics, Python, Clustering, R, Text Mining, NLP, Dataset, Efficiency ...", "dateLastCrawled": "2021-12-26T06:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Artificial Intelligence By Example.pdf [5wgl5d599xo7]", "url": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "isFamilyFriendly": true, "displayUrl": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "snippet": "The concept of <b>gradient</b> <b>descent</b> is minimizing loss or errors between the present result and a goal to attain. First, a cost function is needed. There are four predicates (0-0, 1-1, 1-0, 0-1) to train correctly. We simply need to find out how many are correctly trained at each epoch. The cost function will measure the difference between the training goal (4) and the result of this epoch or training iteration (result). When 0 convergence is reached, it means the training has succeeded. SFTVMU", "dateLastCrawled": "2022-01-27T23:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Smartphone-based Recognition of Human Activities using Shallow Machine ...", "url": "https://thesai.org/Downloads/Volume12No4/Paper_10-Smartphone_based_Recognition_of_Human_Activities.pdf", "isFamilyFriendly": true, "displayUrl": "https://thesai.org/Downloads/Volume12No4/Paper_10-Smartphone_based_Recognition_of...", "snippet": "The <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) decreases the computational strain to accelerate trade iterations at issue.a lower rate. <b>SGD</b> leads to J48 performance enhancement. Furthermore, a human activity recognition dataset based on smartphone sensors are used to validate the proposed solution. The findings showed that the proposed model was superior. Keywords\u2014Data preprocessing; data mining; classification; genetic programming; Na\u00efve Bayes; decision tree . I. INTRODUCTION. The aim of human ...", "dateLastCrawled": "2022-01-23T08:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "cs229.stanford.edu", "url": "http://cs229.stanford.edu/proj2021spr/report2/81994495.pdf", "isFamilyFriendly": true, "displayUrl": "cs229.stanford.edu/proj2021spr/report2/81994495.pdf", "snippet": "Three <b>gradient</b> <b>descent</b> optimization algorithms including traditional <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>), RMSprop, and Adam, are used to tune network weights iteratively. SGI) is adapted as it revealed higher performance. The best performance of model is achieved when dropout rate was 0.2. The optimal number of fully-connected layers is determined to be 6 layers after spanning a range of 0 to 10 layers. For the number of neurons in each layer, 300 neurons are selected after testing values from ...", "dateLastCrawled": "2022-01-15T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Feature learning for <b>Human Activity Recognition</b> using Convolutional ...", "url": "https://link.springer.com/article/10.1007/s42486-020-00026-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s42486-020-00026-2", "snippet": "Evaluation of different configurations was made using Adam (Kingma and Ba 2015) and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) optimizers, with different number of layers, kernel sizes, and number of filters. For final training of the CNN feature extractor, the <b>SGD</b> optimizer was used. <b>Compared</b> to the Adam optimizer, <b>SGD</b> provides, in some cases, better generalization on unseen data (Keskar and Socher", "dateLastCrawled": "2022-02-03T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Highly efficient <b>federated learning</b> with strong privacy ... - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0167404820301620", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167404820301620", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) (Zinkevich et al., 2010) and its variants are especially suitable for solving highly non-convex problems. The training process <b>can</b> be divided into feed-forward and back-propagation stages. In the feed-forward stage, the neural network successively computes the output of each layer, and calculates the error", "dateLastCrawled": "2021-12-25T23:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Decentralized learning works: An empirical comparison of gossip ...", "url": "https://www.sciencedirect.com/science/article/pii/S0743731520303890", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0743731520303890", "snippet": "We train linear models using <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) based on the logistic regression loss function. Our experimental methodology involves several scenarios, including smartphone churn traces collected by the application Stunner . We also vary the communication pattern (continuous or bursty) and the network size. In addition, we also evaluate different assumptions about the label distribution, that is, whether a given worker has a biased or unbiased subset of the training samples ...", "dateLastCrawled": "2022-01-23T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep <b>Convolutional Neural Networks for Human</b> Activity Recognition with ...", "url": "http://sclab.yonsei.ac.kr/publications/Papers/IC/ICONIP2015_CAR.pdf", "isFamilyFriendly": true, "displayUrl": "sclab.yonsei.ac.kr/publications/Papers/IC/ICONIP2015_CAR.pdf", "snippet": "<b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) on minibatches of sensor train data examples. Fig. 2. Convolutional neural network architecture and training procedure Deep <b>Convolutional Neural Networks for Human</b> Activity Recognition 49 . Backpropagation to adjust weights is done by computing the <b>gradient</b> of the convolu\u2010 tional weights: (4) where is the error/cost function, is the nonlinear mapping function equal to, and deltas are equal to . The forward and back propaga\u2010 tion procedure is repeated ...", "dateLastCrawled": "2022-01-30T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Lightweight Transformer in Federated Setting for Human Activity ...", "url": "https://deepai.org/publication/lightweight-transformer-in-federated-setting-for-human-activity-recognition", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/lightweight-transformer-in-federated-setting-for-human...", "snippet": "FL has many types of aggregation: Federated averaging, and federated <b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>). The Fl approach introduce in [mcmahan2016federated] showed that the communication cost using Federated averaging <b>can</b> be reduced by a factor of 0 to 100 <b>compared</b> to federated <b>SGD</b> and it has been used in many applications [yang2019federated].", "dateLastCrawled": "2022-01-18T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Data Science Question and Answer</b> | PDF | Machine Learning | Statistical ...", "url": "https://www.scribd.com/document/354655052/Data-Science-Question-and-Answer", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/354655052/<b>Data-Science-Question-and-Answer</b>", "snippet": "Tags: machine-learning (Prev Q) (Next Q), neuralnetwork (Next Q), algorithms (Next Q) Im currently working on implementing <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) for neural nets using backpropagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.", "dateLastCrawled": "2022-01-27T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Homogeneous Data Normalization and Deep Learning: A Case Study in Human ...", "url": "https://www.readkong.com/page/homogeneous-data-normalization-and-deep-learning-a-case-2130902", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/homogeneous-data-normalization-and-deep-learning-a-case...", "snippet": "Predicted Class <b>Walking</b> <b>Walking</b> Standing <b>Walking</b> Upstairs <b>Downstairs</b> <b>walking</b> upstairs 117 16 1 0 <b>walking</b> <b>downstairs</b> 89 52 0 3 Actual Class standing 3 10 15 0 <b>walking</b> 1 9 7 12 Besides, the classification results of the data acquired from the gyroscope sensor after the application of the Z-Score normalizer were analyzed, verifying that the DNN method implemented reported an accuracy of 56.81%, a precision of 63.63%, a recall value of 53.08%, and an F1-Score of 57.88%. 3.5. Non-Normalized Data ...", "dateLastCrawled": "2021-11-27T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "1. Introduction", "url": "http://cs230.stanford.edu/projects_spring_2021/reports/11.pdf", "isFamilyFriendly": true, "displayUrl": "cs230.stanford.edu/projects_spring_2021/reports/11.pdf", "snippet": "<b>Compared</b> with other systems, smartphone-based sensor system has significant advantages in terms of computing power, portability, ubiquity and cost [1][2]. This system has been widely used in many fields, such as health care, smart home, game entertainment, Robot R&amp;D, surveillance, sport and fitness [3][4][5][6]. However, previous work mainly focuses on several specific on-body positions, such as hand, trouser pocket, jacket pocket, hand, armband [7][8][9][10][11]. In fact, people also carry ...", "dateLastCrawled": "2021-09-08T05:00:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! | by Aishwarya V ...", "url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>stochastic-gradient-descent</b>-clearly-explained-53d239905d31", "snippet": "<b>Stochastic gradient descent</b> is a very popular and common algorithm used in various <b>Machine</b> <b>Learning</b> algorithms, most importantly forms the basis of Neural Networks. In this article, I have tried my\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! Aishwarya V Srinivasan. Sep 7, 2019 \u00b7 4 min read. <b>Stochastic gradient descent</b> is a ...", "dateLastCrawled": "2022-02-02T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Stochastic gradient descent</b> - The <b>Learning</b> <b>Machine</b>", "url": "https://the-learning-machine.com/article/optimization/stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://the-<b>learning</b>-<b>machine</b>.com/article/optimization/<b>stochastic-gradient-descent</b>", "snippet": "<b>Stochastic gradient descent</b> (<b>SGD</b>) is an approach for unconstrained optimization.<b>SGD</b> is the workhorse of optimization for <b>machine</b> <b>learning</b> approaches. It is used as a faster alternative for training support vector machines and is the preferred optimization routine for deep <b>learning</b> approaches.. In this article, we will motivate the formulation for <b>stochastic gradient descent</b> and provide interactive demos over multiple univariate and multivariate functions to show it in action.", "dateLastCrawled": "2022-01-26T05:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> in Theory and Practice", "url": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) is the most widely used optimization method in the <b>machine</b> <b>learning</b> community. Researchers in both academia and industry have put considerable e ort to optimize <b>SGD</b>\u2019s runtime performance and to develop a theoretical framework for its empirical success. For example, recent advancements in deep neural networks have been largely achieved because, surprisingly, <b>SGD</b> has been found adequate to train them. Here we present three works highlighting desirable ...", "dateLastCrawled": "2022-01-30T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> <b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b>", "url": "http://www.bel.utcluj.ro/dce/didactic/eai/04_GradientDescent_ML.pdf", "isFamilyFriendly": true, "displayUrl": "www.bel.utcluj.ro/dce/didactic/eai/04_<b>GradientDescent</b>_ML.pdf", "snippet": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> Elements of Artificial Intelligence G. Oltean BGD vs. <b>SGD</b> The summation part is important, especially with the concept of batch <b>gradient</b> <b>descent</b> (BGD) vs. <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>). In Batch <b>Gradient</b> <b>Descent</b>, all the training data is taken into consideration to take a single step (one training epoch ...", "dateLastCrawled": "2022-02-03T10:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Adam, <b>Momentum and Stochastic Gradient Descent</b> - <b>Machine</b> <b>Learning</b> From ...", "url": "https://mlfromscratch.com/optimizers-explained/", "isFamilyFriendly": true, "displayUrl": "https://mlfromscratch.com/optimizers-explained", "snippet": "The basic difference between batch <b>gradient</b> <b>descent</b> (BGD) and <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>), is that we only calculate the cost of one example for each step in <b>SGD</b>, but in BGD, we have to calculate the cost for all training examples in the dataset. Trivially, this speeds up neural networks greatly. Exactly this is the motivation behind <b>SGD</b>. The equation for <b>SGD</b> is used to update parameters in a neural network \u2013 we use the equation to update parameters in a backwards pass, using ...", "dateLastCrawled": "2022-02-02T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Scikit Learn - Stochastic Gradient Descent</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_stochastic_gradient_descent</b>.htm", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) is a simple yet efficient optimization algorithm used to find the values of parameters/coefficients of functions that minimize a cost function. In other words, it is used for discriminative <b>learning</b> of linear classifiers under convex loss functions such as SVM and Logistic regression. It has been successfully applied to large-scale datasets because the update to the coefficients is performed for each training instance, rather than at the end of instances.", "dateLastCrawled": "2022-02-03T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Gradient</b> <b>Descent</b>: One of <b>Machine</b> <b>Learning</b>\u2019s Most Popular Algorithms ...", "url": "https://urmiparekh.medium.com/gradient-descent-one-of-machine-learnings-most-popular-algorithms-c31963d1e67f", "isFamilyFriendly": true, "displayUrl": "https://urmiparekh.medium.com/<b>gradient</b>-<b>descent</b>-one-of-<b>machine</b>-<b>learning</b>s-most-popular...", "snippet": "<b>Gradient</b> <b>descent</b> is an optimization algorithm which is used to train a <b>machine</b> <b>learning</b> model. It is an optimization algorithm to find a local minimum of a differential function. It is used to find the values of a function\u2019s coefficients that minimize a cost function as much as possible. Source: Here. It i s a first-order iterative ...", "dateLastCrawled": "2022-01-17T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Batch, Mini Batch &amp; <b>Stochastic</b> <b>Gradient Descent</b> | by Sushant Patrikar ...", "url": "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/batch-mini-batch-<b>stochastic</b>-<b>gradient-descent</b>-7a62ecba642a", "snippet": "In Batch <b>Gradient Descent</b> we were considering all the examples for every step of <b>Gradient Descent</b>. But what if our dataset is very huge. Deep <b>learning</b> models crave for data. The more the data the more chances of a model to be good. Suppose our dataset has 5 million examples, then just to take one step the model will have to calculate the gradients of all the 5 million examples. This does not seem an efficient way. To tackle this problem we have <b>Stochastic</b> <b>Gradient Descent</b>. In <b>Stochastic</b> ...", "dateLastCrawled": "2022-02-02T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Gradient Descent With Momentum from Scratch</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>gradient-descent-with-momentum-from-scratch</b>", "snippet": "<b>Gradient</b> <b>descent</b> is an optimization algorithm that follows the negative <b>gradient</b> of an objective function in order to locate the minimum of the function. A problem with <b>gradient</b> <b>descent</b> is that it can bounce around the search space on optimization problems that have large amounts of curvature or noisy gradients, and it can get stuck in flat spots in the search", "dateLastCrawled": "2022-01-26T05:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent in Logistic Regression [Explained for Beginners</b> ...", "url": "https://www.upgrad.com/blog/gradient-descent-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/<b>gradient-descent-in-logistic-regression</b>", "snippet": "It\u2019s massive, and hence there was a need for a slightly modified <b>Gradient</b> <b>Descent</b> Algorithm, namely \u2013 <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Algorithm (<b>SGD</b>). The only difference <b>SGD</b> has with Normal <b>Gradient</b> <b>Descent</b> is that, in <b>SGD</b>, we don\u2019t deal with the entire training instance at a single time. In <b>SGD</b>, we compute the <b>gradient</b> of the cost function for just a single random example at each iteration. Now, doing so brings down the time taken for computations by a huge margin especially for large ...", "dateLastCrawled": "2022-01-28T16:49:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Gossip <b>Learning</b> as a Decentralized Alternative to Federated <b>Learning</b>", "url": "http://publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "isFamilyFriendly": true, "displayUrl": "publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "snippet": "Federated <b>learning</b> is adistributed <b>machine</b> <b>learning</b> approach for computing models over data collected by edge devices. Most impor-tantly, the data itself is not collected centrally, but a master-worker ar-chitecture is applied where a master node performs aggregation and the edge devices are the workers, not unlike the parameter server approach. Gossip <b>learning</b> also assumes that the data remains at the edge devices, but it requires no aggregation server or any central component. In this ...", "dateLastCrawled": "2022-01-27T14:06:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(stochastic gradient descent (sgd))  is like +(walking downstairs)", "+(stochastic gradient descent (sgd)) is similar to +(walking downstairs)", "+(stochastic gradient descent (sgd)) can be thought of as +(walking downstairs)", "+(stochastic gradient descent (sgd)) can be compared to +(walking downstairs)", "machine learning +(stochastic gradient descent (sgd) AND analogy)", "machine learning +(\"stochastic gradient descent (sgd) is like\")", "machine learning +(\"stochastic gradient descent (sgd) is similar\")", "machine learning +(\"just as stochastic gradient descent (sgd)\")", "machine learning +(\"stochastic gradient descent (sgd) can be thought of as\")", "machine learning +(\"stochastic gradient descent (sgd) can be compared to\")"]}