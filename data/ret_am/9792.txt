{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Disparate</b> Impact in <b>Machine</b> <b>Learning</b> \u00bb Dome | Blog Archive | Boston ...", "url": "https://sites.bu.edu/dome/2020/06/08/disparate-impact-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://sites.bu.edu/dome/2020/06/08/<b>disparate</b>-impact-in-<b>machine</b>-<b>learning</b>", "snippet": "Thus, instead of relying on either <b>disparate</b> impact or <b>disparate</b> <b>treatment</b> theory, perhaps legal analysis of discrimination in <b>machine</b> <b>learning</b> should be entirely outcomes-driven. If, in fact, an <b>algorithm</b> wrongly predicts the likelihood of an event occurring, and that <b>algorithm</b> is less accurate for protected class members than unprotected class members, the <b>algorithm</b> should be considered prima facie discriminatory. Such a solution is viable for examining recidivism, interest rates and loan ...", "dateLastCrawled": "2021-12-09T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Disparate</b> <b>Treatment</b> \u2014 Involves classifying someone in an impermissible way. It involves the intent to discriminate, evidenced by explicit reference to group membership. <b>Disparate</b> Impact \u2014 Looks at the consequences of classification/decision making on certain groups. No intent is required and it is facially neutral. <b>Disparate</b> impact is often referred to as unintentional <b>discrimination</b>, whereas <b>disparate</b> <b>treatment</b> is intentional. Practices with a disproportionate impact on a particular ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Two-stage <b>Algorithm</b> for <b>Fairness-aware</b> <b>Machine</b> <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/two-stage-algorithm-for-fairness-aware-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/two-stage-<b>algorithm</b>-for-<b>fairness-aware</b>-<b>machine</b>-<b>learning</b>", "snippet": "In other words, a <b>machine</b> <b>learning</b> <b>algorithm</b> that utilizes sensitive attributes is subject to biases in the existing data. This could be viewed as an algorithmic version of <b>disparate</b> <b>treatment</b> [], where decisions are made on the basis of these sensitive attributes.However, removing sensitive attributes from the dataset is not sufficient solution as it has a <b>disparate</b> impact.<b>Disparate</b> impact is a notion that was born in the 1970s.", "dateLastCrawled": "2021-12-10T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Does mitigating ML&#39;s <b>disparate impact require disparate treatment</b>?", "url": "https://arxiv.org/abs/1711.07076v1", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1711.07076v1", "snippet": "The natural way to reduce <b>disparate</b> impact would be to apply <b>disparate</b> <b>treatment</b> in favor of the disadvantaged group, i.e. to apply affirmative action. However, owing to the practice&#39;s contested legal status, several papers have proposed trying to eliminate both forms of unfairness simultaneously, introducing a family of algorithms that we denote <b>disparate</b> <b>learning</b> processes (DLPs). These processes incorporate the protected characteristic as an input to the <b>learning</b> <b>algorithm</b> (e.g.~via a ...", "dateLastCrawled": "2021-12-18T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Disparate</b> Impact Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-impact-analysis", "snippet": "<b>Disparate</b> Impact Analysis (DIA) Sensitivity Analysis(SA) As a matter of speaking, the above two features provide a solution to a common problem in ML: the multiplicity of good models. It is well understood that for the same set of input features and prediction targets, complex <b>machine</b> <b>learning</b> algorithms can produce multiple accurate models with very similar, but not the same, internal architectures: the multiplicity of good models [1]. This alone is an obstacle to interpretation, but when ...", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Differentially Private Fair <b>Learning</b> - Proceedings of <b>Machine</b> <b>Learning</b> ...", "url": "http://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "snippet": "can be viewed as a form of \u201c<b>disparate</b> <b>treatment</b>\u201d. Our second <b>algorithm</b> is a differentially private version of the oracle-ef\ufb01cient in-processing ap-proach of (Agarwal et al.,2018) which is more complex but need not have access to protected group membership at test time. We identify new tradeoffs between fairness, accuracy, and privacy that emerge only when requiring all three proper-ties, and show that these tradeoffs can be milder if group membership may be used at test time. We ...", "dateLastCrawled": "2022-01-22T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fairness in Machine Learning is Tricky</b> | by John Dickerson | Arthur AI ...", "url": "https://medium.com/arthur-ai/fairness-in-machine-learning-is-tricky-1ea47111b847", "isFamilyFriendly": true, "displayUrl": "https://medium.com/arthur-ai/<b>fairness-in-machine-learning-is-tricky</b>-1ea47111b847", "snippet": "Then, one goal that a <b>fairness in machine learning</b> practitioner might have is to mathematically certify that an <b>algorithm</b> does not suffer from <b>disparate</b> <b>treatment</b> or <b>disparate</b> impact, perhaps ...", "dateLastCrawled": "2021-04-29T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Disparate</b> impact in a <b>machine</b> <b>learning</b> model originates from bias in either the data or the algorithms. A popular example is the prejudicially biased data used for recidivism prediction. Due to <b>disparate</b> socioeconomic factors and systemic racism in the United States, blacks have historically been (and continue to be) incarcerated at higher rates than whites . Not coincidentally, blacks are also exonerated due to wrongful accusation at a considerably higher rate than whites . A recidivism ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Machine</b> <b>Learning</b> system beyond the code: the case of the COMPAS <b>algorithm</b>", "url": "https://www.crystal-violet.com/uploads/2/8/5/7/28577047/alzate_compas_algorithm.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.crystal-violet.com/uploads/2/8/5/7/28577047/alzate_compas_<b>algorithm</b>.pdf", "snippet": "A <b>Machine</b> <b>Learning</b> system beyond the code: the case of the COMPAS <b>algorithm</b> . Author: Catalina Alzate . School of Arts, Technology and Emerging Communication . University of Texas at Dallas . The COMPAS <b>algorithm</b> developed by the company Northpointe is an online tool used for assessing a person\u2019s risk of recidivism in several courts in the U.S. This tool was analyzed by an independent news agency and labeled as biased against black people. Since this 2016 expose, journalists have ...", "dateLastCrawled": "2022-01-21T02:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "AI Fairness \u2014 Explanation of <b>Disparate Impact</b> Remover | by Stacey ...", "url": "https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/ai-fairness-explanation-of-<b>disparate-impact</b>-remover-ce0...", "snippet": "The <b>algorithm</b> requires the user to specify a repair_level, this indicates how much you wish for the distributions of the groups to overlap. Let\u2019s explore the impact of two different repair levels, 1.0 and 0.8. Repair value = 1.0. This diagram shows the repaired values for Feature for the unprivileged group Blue and privileged group Orange after using DisparateImpactRemover with a repair level of 1.0. You are no longer able to select a point and infer which group it belongs to. This would ...", "dateLastCrawled": "2022-01-29T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning and Discrimination</b> - GitHub Pages", "url": "https://harvard-ml-courses.github.io/cs181-web-2020/files/lecture12.pdf", "isFamilyFriendly": true, "displayUrl": "https://harvard-ml-courses.github.io/cs181-web-2020/files/lecture12.pdf", "snippet": "<b>Machine Learning and Discrimination</b> Diana Acosta-Navas PhD candidate, Harvard Philosophy Department Adjunct Lecturer in Ethics and Public Policy, Harvard Kennedy School . For Today\u2026 \u2022Discrimination/ wrongful discrimination \u2022Case Study: PredPol \u2022<b>Disparate</b> <b>treatment</b> vs. <b>Disparate</b> impact \u2022How predictive policing could wrongfully discriminate \u2022What contextual considerations are important to determine whether an <b>algorithm</b> wrongfully discriminates? For Today\u2026 \u2022Content warning ...", "dateLastCrawled": "2021-09-15T23:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>AI Fairness \u2014 Explanation of Disparate Impact Remover</b> - Adolfo Eliaz\u00e0t ...", "url": "https://adolfoeliazat.com/2021/05/06/ai-fairness-explanation-of-disparate-impact-remover/", "isFamilyFriendly": true, "displayUrl": "https://adolfoeliazat.com/2021/05/06/<b>ai-fairness-explanation-of-disparate-impact-remover</b>", "snippet": "<b>Disparate</b> Impact Remover preserves rank-ordering within groups; if an individual has the highest score for group Blue, it will still have the highest score among Blues after repair. Building <b>Machine</b> <b>Learning</b> Models. Once <b>Disparate</b> Impact Remover has been implemented, a <b>machine</b> <b>learning</b> model can be built using the repaired data. The <b>Disparate</b> ...", "dateLastCrawled": "2022-01-22T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> in Drug Discovery: A Review", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8356896/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8356896", "snippet": "<b>Machine</b> <b>learning</b> techniques improve the decision-making in pharmaceutical data across various applications like QSAR analysis, hit discoveries, de novo drug architectures to retrieve accurate outcomes. Target validation, prognostic biomarkers, digital pathology are considered under problem statements in this review. ML challenges must be applicable for the main cause of inadequacy in interpretability outcomes that may restrict the applications in drug discovery. In clinical trials, absolute ...", "dateLastCrawled": "2022-01-27T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Disparate</b> impact in a <b>machine</b> <b>learning</b> model originates from bias in either the data or the algorithms. A popular example is the prejudicially biased data used for recidivism prediction. Due to <b>disparate</b> socioeconomic factors and systemic racism in the United States, blacks have historically been (and continue to be) incarcerated at higher rates than whites . Not coincidentally, blacks are also exonerated due to wrongful accusation at a considerably higher rate than whites . A recidivism ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>Racist Algorithm</b>? - University of Michigan", "url": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1657&context=mlr", "isFamilyFriendly": true, "displayUrl": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1657&amp;context=mlr", "snippet": "he calls the \u201csweet mystery of <b>machine</b> <b>learning</b>.\u201d Frank Pasquale, Bittersweet Mysteries of <b>Ma-chine</b> <b>Learning</b> (A Provocation) ... redesign the <b>algorithm</b> or to distrust its results. The distinction <b>is similar</b> to the evidentiary difference between demonstrating <b>disparate</b> <b>treatment</b> and demonstrating <b>disparate</b> impact. 10. My central claim is this: if we believe that the real-world facts, on which algorithms are trained and operate, are deeply suffused with invidious discrimination, then our ...", "dateLastCrawled": "2022-01-30T00:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2,", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Equal Protection Under the <b>Algorithm</b>: A Legal-Inspired Framework for ...", "url": "https://www.fatml.org/media/documents/equal_protection_under_the_algorithm.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.fatml.org/media/documents/equal_protection_under_the_<b>algorithm</b>.pdf", "snippet": "<b>machine</b> <b>learning</b>. There are many types of applications where this work may be relevant, and it is not possible to create a single <b>algorithm</b> that is applicable to all of them. Rather, we provide a high-level description of how one can de\ufb01ne an appropriate <b>algorithm</b>. 4.1. Terminology A <b>treatment</b> is a targeting action, such as showing an ad. A", "dateLastCrawled": "2022-02-02T07:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "6.S897 <b>Machine</b> <b>Learning</b> in Healthcare, Lecture 23: Fairness", "url": "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-notes/MIT6_S897S19_lec23.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/electrical-engineering-and-computer-science/6-s897-<b>machine</b>...", "snippet": "<b>algorithm</b>, starting in October 2019. A county o\ufb03cial will then take that grade and use it to recommend whether the accused should be released or remain in jail. \u2022 \u201c\u2026 the <b>machine</b> <b>learning</b> systems used to calculate these risk scores throughout the criminal justice system, have been shown to hold severe", "dateLastCrawled": "2022-01-28T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Fall 2020 Journal | Algorithms and Child Welfare: The <b>Disparate</b> Impact ...", "url": "https://bppj.berkeley.edu/2021/02/02/algorithms-and-child-welfare-the-disparate-impact-of-family-surveillance-in-risk-assessment-technologies/", "isFamilyFriendly": true, "displayUrl": "https://bppj.berkeley.edu/2021/02/02/<b>algorithms</b>-and-child-welfare-the-<b>disparate</b>-impact...", "snippet": "We believe that if <b>machine</b> <b>learning</b> is to continue to be used in social services, the history of the data must be considered [34]. Through our literature review, we did not find evidence of regulation over the child welfare data used in <b>machine</b> <b>learning</b> technologies. At the time of writing, Pennsylvania\u2019s statutes on Child Protective Services did not include any guidance on the use of <b>machine</b> <b>learning</b> or artificial intelligence. Searches for the words \u201cautomated\u201d and \u201c<b>algorithm</b> ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A primer on AI <b>fairness</b>. What it is and the tradeoffs to be made | by ...", "url": "https://towardsdatascience.com/artificial-intelligence-fairness-and-tradeoffs-ce11ac284b63", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/artificial-intelligence-<b>fairness</b>-and-tradeoffs-ce11ac284b63", "snippet": "Discrimination Law: <b>Disparate</b> <b>Treatment</b> (formal vs intentional) vs <b>Disparate</b> Impact (20% rule, legal rule of thumb). <b>Disparate</b> <b>treatment</b> <b>can</b> <b>be thought</b> of as procedural <b>fairness</b>. The underlying philosophy is equality of opportunity. <b>Disparate</b> impact is distributive justice. There is tension between these two goals.", "dateLastCrawled": "2022-02-02T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Disparate</b> <b>Treatment</b> \u2014 Involves classifying someone in an impermissible way. It involves the intent to discriminate, evidenced by explicit reference to group membership. <b>Disparate</b> Impact \u2014 Looks at the consequences of classification/decision making on certain groups. No intent is required and it is facially neutral. <b>Disparate</b> impact is often referred to as unintentional <b>discrimination</b>, whereas <b>disparate</b> <b>treatment</b> is intentional. Practices with a disproportionate impact on a particular ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> (CS 181) - 2018 Spring | Embedded EthiCS", "url": "https://embeddedethics.seas.harvard.edu/classes/cs-181-2018-spring", "isFamilyFriendly": true, "displayUrl": "https://embeddedethics.seas.harvard.edu/classes/cs-181-2018-spring", "snippet": "<b>machine</b> <b>learning</b> CS Module Overview: In this module, we probe the ways that <b>machine</b> <b>learning</b> models <b>can</b> be discriminatory and examine different methods for preventing discriminatory outcomes. We begin by introducing two concepts of discrimination: <b>disparate</b> <b>treatment</b> and <b>disparate</b> impact. We then use those concepts to argue that there are at ...", "dateLastCrawled": "2022-01-06T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Advancing Greater Fairness and Explainability for AI and <b>Machine</b> ...", "url": "https://www.capitalone.com/tech/machine-learning/advancing-greater-fairness-and-explainability-for-ai-and-machine-learning-across-the-banking-industry/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.capitalone.com</b>/tech/<b>machine</b>-<b>learning</b>/advancing-greater-fairness-and...", "snippet": "There is <b>disparate</b> <b>treatment</b>, treating people differently based on their protected attribute, and also <b>disparate</b> impact, in which the outcome of a policy could be evidence of discrimination. Banks want to be fair in both senses, with respect to the inputs to a decision as well as the outcomes of a decision. However, recent work by <b>machine</b> <b>learning</b> researchers shows that there are challenges when satisfying both notions of fairness in", "dateLastCrawled": "2022-01-05T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The promise of <b>machine learning in predicting treatment outcomes</b> in ...", "url": "https://onlinelibrary.wiley.com/doi/10.1002/wps.20882", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1002/wps.20882", "snippet": "Predicting <b>treatment</b> response is just one relatively narrow use case where <b>machine</b> <b>learning</b> <b>can</b> add value and improve mental health care. Prediction <b>can</b> help with so many more clinical decisions and clinical processes. We could predict barriers that prevent an individual from engaging in care initially, or non-adherence or dropout from care after initiation. We could streamline patients to the appropriate level of care, such as self-guided programs vs. outpatient care, or intensive ...", "dateLastCrawled": "2022-01-22T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Review of Challenges and Opportunities in <b>Machine</b> <b>Learning</b> for Health", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7233077/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7233077", "snippet": "Existing reviews of <b>machine</b> <b>learning</b> in the medical space have focused narrowly on biomedical applications 5, deep <b>learning</b> tasks well suited for healthcare 6, the need for transparency 7, and use of big data in precision medicine 8. Here, we emphasize the broad opportunities present in <b>machine</b> <b>learning</b> for healthcare and the careful considerations that must be made. We focus on the electronic health record (EHR), which documents the process of healthcare delivery and operational needs such ...", "dateLastCrawled": "2022-01-25T00:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Bias and Fairness in <b>Machine</b> <b>Learning</b>, Part 2: building a baseline ...", "url": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-2-building-a-baseline-model-and-features/", "isFamilyFriendly": true, "displayUrl": "https://freecontent.manning.com/bias-and-fairness-in-<b>machine</b>-<b>learning</b>-part-2-building...", "snippet": "This question goes hand in hand with our model\u2019s <b>disparate</b> <b>treatment</b>. Dalex has a very handy plot that <b>can</b> be used with tree-based models and linear models to help visualize the features our model is <b>learning</b> the most from. exp_tree.model_parts().plot() Figure 4. Feature importance of our bias-unaware model as reported by dalex. This visualization is taking feature importances directly from the Random Forest\u2019s feature importance attribute and is showing that priors_count and age are our ...", "dateLastCrawled": "2022-02-03T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "TOP: A Compiler-Based Framework for Optimizing <b>Machine</b> <b>Learning</b> ...", "url": "https://sites.cs.ucsb.edu/~yufeiding/publication/SysML18.pdf", "isFamilyFriendly": true, "displayUrl": "https://sites.cs.ucsb.edu/~yufeiding/publication/SysML18.pdf", "snippet": "<b>treatment</b> to various ML algorithms, and TOP, a compiler-based optimizer for e ectively applying TI to optimize <b>ma-chine</b> <b>learning</b> algorithms. Experiments show that TOP is able to automatically produce optimized algorithms that ei-ther matches or outperforms manually designed algorithms, giving up to 237x speedups and 2.5X on average1. 1. INTRODUCTION Vector dot products and point-to-point distance calcula-tions are essential to many important algorithms across var-ious domains. Vector dot ...", "dateLastCrawled": "2021-09-18T13:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fall 2020 Journal | Algorithms and Child Welfare: The <b>Disparate</b> Impact ...", "url": "https://bppj.berkeley.edu/2021/02/02/algorithms-and-child-welfare-the-disparate-impact-of-family-surveillance-in-risk-assessment-technologies/", "isFamilyFriendly": true, "displayUrl": "https://bppj.berkeley.edu/2021/02/02/<b>algorithms</b>-and-child-welfare-the-<b>disparate</b>-impact...", "snippet": "We believe that if <b>machine</b> <b>learning</b> is to continue to be used in social services, the history of the data must be considered [34]. Through our literature review, we did not find evidence of regulation over the child welfare data used in <b>machine</b> <b>learning</b> technologies. At the time of writing, Pennsylvania\u2019s statutes on Child Protective Services did not include any guidance on the use of <b>machine</b> <b>learning</b> or artificial intelligence. Searches for the words \u201cautomated\u201d and \u201c<b>algorithm</b> ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The ethics of <b>algorithms</b>: key problems and solutions", "url": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "snippet": "Actions (1) and (2) may be performed by (semi-)autonomous <b>algorithms</b>\u2014such as <b>machine</b> <b>learning</b> (ML) <b>algorithms</b>\u2014and this complicates, (3) the attribution of responsibility for the effects of actions that an <b>algorithm</b> may trigger. Here, ML is of particular interest, as a field which includes deep <b>learning</b> architectures. Computer systems deploying ML <b>algorithms</b> may be described as \u201cautonomous\u201d or \u201csemi-autonomous\u201d, to the extent that their outputs are induced from data and thus, non ...", "dateLastCrawled": "2022-01-30T20:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Disparate</b> <b>Treatment</b> \u2014 Involves classifying someone in an impermissible way. It involves the intent to discriminate, evidenced by explicit reference to group membership. <b>Disparate</b> Impact \u2014 Looks at the consequences of classification/decision making on certain groups. No intent is required and it is facially neutral. <b>Disparate</b> impact is often referred to as unintentional <b>discrimination</b>, whereas <b>disparate</b> <b>treatment</b> is intentional. Practices with a disproportionate impact on a particular ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b>", "url": "https://www.ftc.gov/system/files/documents/public_events/1567421/fuaserisinghsrinivasan_updated2.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ftc.gov</b>/system/files/documents/public_events/1567421/fuaserisinghsriniva...", "snippet": "<b>Compared</b> to the current law, which requires <b>treatment</b> parity, these \u201cfair\u201d algorithms, which require impact parity, limit the bene\ufb01ts of a more accurate <b>algorithm</b> for a \ufb01rm. As a result, pro\ufb01t maximizing \ufb01rms could under-invest in <b>learning</b>, i.e., improving the accuracy of their <b>machine</b> <b>learning</b> algorithms. We show that the investment in <b>learning</b> decreases when misclassi\ufb01cation is costly, which is exactly the case when greater accuracy is otherwise desired. Our paper highlights ...", "dateLastCrawled": "2022-02-03T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding and Reducing Bias in <b>Machine Learning</b> | by Jaspreet ...", "url": "https://towardsdatascience.com/understanding-and-reducing-bias-in-machine-learning-6565e23900ac", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-and-reducing-bias-in-<b>machine-learning</b>...", "snippet": "Even if we remove <b>disparate</b> <b>treatment</b> by removing the sensitive feature, discrimination <b>can</b> still happen through other correlated features such as zip code. Measuring and correcting <b>disparate</b> impact makes sure that this is corrected. This requirement should be used when the training dataset is biased. While being considered a controversial measure by many, notably by critics who hold that some scenarios cannot be freed from disproportionate outcomes.", "dateLastCrawled": "2022-01-28T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "\u201cUn\u201dFair <b>Machine</b> <b>Learning</b> Algorithms | Management Science", "url": "https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4065", "isFamilyFriendly": true, "displayUrl": "https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4065", "snippet": "However, in many cases, ensuring equal <b>treatment</b> leads to <b>disparate</b> impact particularly when there are differences among groups based on demographic classes. In response, several \u201cfair\u201d <b>machine</b> <b>learning</b> (ML) algorithms that require impact parity (e.g., equal opportunity) at the cost of equal <b>treatment</b> have recently been proposed to adjust for the societal inequalities. Advocates of fair ML propose changing the law to allow the use of protected class-specific decision rules. We show that ...", "dateLastCrawled": "2022-02-01T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> in Drug Discovery: A Review", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8356896/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8356896", "snippet": "In <b>machine</b> <b>learning</b>, mapping ability features <b>can</b> yield great accomplishment to extract physical, geometric, and chemical features (Khamis et al. ) to retrieve scores. Based on scores, data-driven black box models which are considered to predict interactions in binding affinities and furthermore avoiding few concepts in docking like physical function are very hard to study (Ain et al. 2015 ).", "dateLastCrawled": "2022-01-27T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How <b>machine-learning recommendations influence clinician treatment</b> ...", "url": "https://www.nature.com/articles/s41398-021-01224-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41398-021-01224-x", "snippet": "Decision support systems embodying <b>machine</b> <b>learning</b> models offer the promise of an improved standard of care for major depressive disorder, but little is known about how clinicians\u2019 <b>treatment</b> ...", "dateLastCrawled": "2022-01-28T19:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Algorithmic bias detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/<b>algorithm</b>ic-bias-detection-and-mitigation-best-", "snippet": "For example, the demonstration of <b>disparate</b> <b>treatment</b> does not describe the ways in which an <b>algorithm</b> <b>can</b> learn to treat similarly situated groups differently, as will be discussed later in the ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The promise of <b>machine learning in predicting treatment outcomes</b> in ...", "url": "https://onlinelibrary.wiley.com/doi/10.1002/wps.20882", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1002/wps.20882", "snippet": "Predicting <b>treatment</b> response is just one relatively narrow use case where <b>machine</b> <b>learning</b> <b>can</b> add value and improve mental health care. Prediction <b>can</b> help with so many more clinical decisions and clinical processes. We could predict barriers that prevent an individual from engaging in care initially, or non-adherence or dropout from care after initiation. We could streamline patients to the appropriate level of care, such as self-guided programs vs. outpatient care, or intensive ...", "dateLastCrawled": "2022-01-22T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b> by Runshan Fu, Manmohan Aseri ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "snippet": "<b>Machine</b> <b>Learning</b> algorithms are becoming widely deployed in real world decision-making. Ensuring fairness in algorithmic decision-making is a crucial policy issue. Current legislation ensures fairness by barring <b>algorithm</b> designers from using demographic information in their decision-making. As a result, the algorithms need to ensure equal <b>treatment</b> to be legally compliant. However, in many cases, ensuring equal <b>treatment</b> leads to <b>disparate</b> impact particularly when there are differences ...", "dateLastCrawled": "2022-01-29T16:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Machine</b> <b>learning</b> models <b>can</b> also be a source of <b>disparate</b> impact in their implementation, through unconscious human biases that affect the fair interpretation or use of the model&#39;s results. This reference does not cover measurement of fairness at implementation. However, if you are interested in fair implementation, we recommend looking at Google&#39;s Fairness Indicators. Harms. In evaluating the potential impact of an ML model, it <b>can</b> be helpful to first clarify what specific harm(s) <b>can</b> be ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>learning</b> and applications in microbiology", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8498514/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8498514", "snippet": "<b>Machine</b> <b>learning</b> has two main <b>learning</b> modes: supervised (also known as predictive) to make future predictions from training data, and unsupervised (descriptive), which is exploratory in nature without training data, defined target or output (Mitchell 1997). Training data are the initial information used to teach supervised ML algorithms in the process of developing a model, from which the model creates and refines its rules required for prediction. Typically, training data comprises a set ...", "dateLastCrawled": "2021-12-06T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Realistically Integrating <b>Machine</b> <b>Learning</b> Into Clinical Pra ...", "url": "https://journals.lww.com/anesthesia-analgesia/fulltext/2020/05000/realistically_integrating_machine_learning_into.4.aspx", "isFamilyFriendly": true, "displayUrl": "https://<b>journals.lww.com</b>/.../05000/realistically_integrating_<b>machine</b>_<b>learning</b>_into.4.aspx", "snippet": "<b>Machine</b>-<b>learning</b> models have been created to predict an increasing number of clinical outcomes, such as diagnoses and mortality, with applications including C. difficile infection in the inpatient hospital setting, 2 identifying molecular markers for cancer treatments, 3 and postoperative surgical outcomes. 4 Examples of <b>machine</b> <b>learning</b> include a cardiologist using an automated interpretation of an ECG and a radiologist using an automated detection of a lung nodule in a chest x-ray. In both ...", "dateLastCrawled": "2021-11-22T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> Solution Manual Tom M Mitchell", "url": "https://aiai.icaboston.org/d/download/F2T4O6/machine-learning-solution-manual-tom-m-mitchell_pdf", "isFamilyFriendly": true, "displayUrl": "https://aiai.icaboston.org/d/download/F2T4O6/<b>machine</b>-<b>learning</b>-solution-manual-tom-m...", "snippet": "offers expanded <b>treatment</b> of off-policy <b>learning</b> and policy-gradient methods. Part III has new chapters on reinforcement <b>learning</b>&#39;s relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson&#39;s wagering strategy. The final chapter discusses the future societal impacts of reinforcement <b>learning</b>. <b>Machine</b> <b>Learning</b>-Tom M. Mitchell 2012-12-06 One of the currently most active research areas within ...", "dateLastCrawled": "2022-01-28T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Trustworthy <b>Machine</b> <b>Learning</b> - Kush R. Varshney - Chapter 2: <b>Machine</b> ...", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-02.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthy<b>machinelearning</b>.com/trustworthy<b>machinelearning</b>-02.htm", "snippet": "<b>Machine</b> <b>learning</b> can also be applied in use cases that are new processes for an organization and no exact historical data exists. Here, proxy data must be identified. For example, a health system may wish to start offering home nursing care to indisposed individuals proactively, but may not have data directly applicable for understanding this decision. Data from previous interactions of patients with the health system may be used as a proxy. In other cases, it may be that new data must be ...", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Solution Manual Alpaydin Introduction To <b>Machine</b> <b>Learning</b>", "url": "https://se.rangle.io/solution+manual+alpaydin+introduction+to+machine+learning+pdf", "isFamilyFriendly": true, "displayUrl": "https://se.rangle.io/solution+manual+alpaydin+introduction+to+<b>machine</b>+<b>learning</b>+pdf", "snippet": "With in-depth Python and MATLAB/OCTAVE-based computational exercises and a complete <b>treatment</b> of cutting edge numerical optimization techniques, this is an essential resource for students and an ideal reference for researchers and practitioners working in <b>machine</b> <b>learning</b>, computer science, electrical engineering, signal processing, and numerical optimization. Handbook of Research on Innovations in Database Technologies and Applications One of the currently most active research areas within ...", "dateLastCrawled": "2022-01-11T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Protein function in precision medicine: deep understanding with <b>machine</b> ...", "url": "https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.12307", "isFamilyFriendly": true, "displayUrl": "https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.12307", "snippet": "<b>disparate</b> parts may ultimately lead to the same observable effect. In this <b>analogy</b>, we might argue that medicine has so far been often investing into mitigat- ing the inconvenience with lemons and much less into improving and augmenting the protocols for \ufb01nding the individual causes of problems. In his recent State-of-the-Union address, the US Pres-ident Barack Obama announced the Precision Medicine Initiative, making this challenge a national and interna-tional priority. Precision ...", "dateLastCrawled": "2021-12-12T19:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Latent bias and the implementation of <b>artificial intelligence</b> in ...", "url": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "snippet": "<b>Artificial intelligence</b> (AI) in general, and <b>machine</b> <b>learning</b> in particular, by all accounts, appear poised to revolutionize medicine. 1\u20133 With a wide spectrum of potential uses across translational research (from bench to bedside to health policy), clinical medicine (including diagnosis, <b>treatment</b>, prediction, and healthcare resource allocation), and public health, every area of medicine will be affected.", "dateLastCrawled": "2022-01-28T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Recurrence in biological and artificial neural <b>networks</b> | by Matthew ...", "url": "https://towardsdatascience.com/recurrence-in-biological-and-artificial-neural-networks-e8a6d5639781", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/recurrence-in-biological-and-artificial-neural-<b>networks</b>...", "snippet": "Recurrence is an overloaded term in the context of neural <b>networks</b>, with <b>disparate</b> colloquial meanings in the <b>machine</b> <b>learning</b> and the neuroscience communities. The difference is narrowing, however, as the artificial neural <b>networks</b> (ANNs) used for practical applications are increasingly sophisticated and more like biological neural <b>networks</b> (BNNs) in some ways (yet still vastly different on the whole).In this post we\u2019ll discuss the historic di f ferences in the use of term recurrence ...", "dateLastCrawled": "2022-01-14T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "47 <b>Analogy</b> Examples To Make You As Sharp As A Tack (and then some)", "url": "https://www.greetingcardpoet.com/good-analogy-examples-and-definition/", "isFamilyFriendly": true, "displayUrl": "https://www.greetingcardpoet.com/<b>good-analogy-examples-and-definition</b>", "snippet": "The essence of this literary device is to set up a comparison that highlights similarities between two seemingly <b>disparate</b> items. It is by examining how the two items are alike in some way that leads to a clear understanding. Differences between Similes, Metaphors, and Analogies . While similes, metaphors, and analogies are similar in that they all compare two different things, similes and metaphors are figures of speech. In contrast, an <b>analogy</b> is more akin to a logical argument. A writer ...", "dateLastCrawled": "2022-02-02T03:57:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A primer on AI <b>fairness</b>. What it is and the tradeoffs to be made | by ...", "url": "https://towardsdatascience.com/artificial-intelligence-fairness-and-tradeoffs-ce11ac284b63", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/artificial-intelligence-<b>fairness</b>-and-tradeoffs-ce11ac284b63", "snippet": "A <b>machine</b> <b>learning</b> algorithms value is being able to increase the number of true positives and true negatives, which each have a value attached. Each false positive and false negative is costly. The value assigned to each depends on each context. A false negative is more costly in medical situations while a false positive is costlier in death penalty decisions. Expected value is profits that businesses can expect from using the algorithm. The more accurate the model, the higher the profits.", "dateLastCrawled": "2022-02-02T02:42:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(disparate treatment)  is like +(machine learning algorithm)", "+(disparate treatment) is similar to +(machine learning algorithm)", "+(disparate treatment) can be thought of as +(machine learning algorithm)", "+(disparate treatment) can be compared to +(machine learning algorithm)", "machine learning +(disparate treatment AND analogy)", "machine learning +(\"disparate treatment is like\")", "machine learning +(\"disparate treatment is similar\")", "machine learning +(\"just as disparate treatment\")", "machine learning +(\"disparate treatment can be thought of as\")", "machine learning +(\"disparate treatment can be compared to\")"]}