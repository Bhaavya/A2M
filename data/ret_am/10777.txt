{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Three lines of machine learning (series 5)-a linear model you don&#39;t ...", "url": "https://www.programmersought.com/article/22083674392/", "isFamilyFriendly": true, "displayUrl": "https://www.programmersought.com/article/22083674392", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) The <b>stochastic</b> <b>gradient</b> <b>descent</b> method is to minimize the loss function of each sample. Although the loss function obtained by each iteration is not toward the global optimal direction, the direction of the large whole is toward the global optimal solution. The final result is often in Near the global optimal solution. Random means that I use an example in the sample to approximate all of my samples to adjust theta, which does not calculate the direction of ...", "dateLastCrawled": "2022-01-27T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Victoria&#39;s ML Implementation Notes - Persagen", "url": "http://persagen.com/files/ml-implementation_notes.html", "isFamilyFriendly": true, "displayUrl": "persagen.com/files/ml-implementation_notes.html", "snippet": "Large noise will dominate the noise-free <b>gradient</b> and allow <b>stochastic</b> <b>gradient</b> <b>descent</b> to be more exploratory. By adding noise only to the problematic parts of the activation function we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find ...", "dateLastCrawled": "2022-02-02T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep <b>learning: adaptive computation and machine learning 0262035618</b> ...", "url": "https://dokumen.pub/deep-learning-adaptive-computation-and-machine-learning-0262035618-9780262035613.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/deep-<b>learning-adaptive-computation-and-machine-learning-0262035618</b>...", "snippet": "The training algorithm used to adapt the weights of the ADALINE was a special case of an algorithm called <b>stochastic</b> <b>gradient</b> <b>descent</b>. Slightly modified versions of the <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm remain the dominant training algorithms for deep learning models today. Models based on the f (x, w) used by the perceptron and ADALINE are called linear models. These models remain some of the most widely used machine learning models, though in many cases they are trained in different ...", "dateLastCrawled": "2022-01-24T19:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>On-line expectation\u2013maximization algorithm for latent data</b> models ...", "url": "https://www.researchgate.net/publication/261536894_On-line_expectation-maximization_algorithm_for_latent_data_models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261536894_On-line_expectation-maximization...", "snippet": "Later, an alternative approach was proposed in [6] as the Online EM algorithm, which shares some similarities with <b>stochastic</b> <b>gradient</b> <b>descent</b> [4] even though Online EM is not a first-order method ...", "dateLastCrawled": "2021-11-17T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Funding Friday: Wines Of The French Alps</b> - AVC", "url": "https://avc.com/2017/04/funding-friday-wines-of-the-french-alps/", "isFamilyFriendly": true, "displayUrl": "https://avc.com/2017/04/<b>funding-friday-wines-of-the-french-alps</b>", "snippet": "Right \u2014 it unwinds as <b>stochastic</b> <b>gradient</b> <b>descent</b>. I can guess what the heck that is right away just from the name \u2014 it\u2019s one of the first things every student of unconstrained optimization thinks of! Unless the way <b>downhill</b> is awfully easy to find, <b>SGD</b> stands to be slower than molasses in January. Gee, we\u2019re not even talking dimensionality reduction and then BFGS (Broyden, Fletcher, Goldfarb, Shanno) quasi-Newton, conjugate gradients, etc.! Lesson: Scikit is still talking work in ...", "dateLastCrawled": "2021-12-27T16:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep Learning for Coders with fastai and PyTorch [First edition ...", "url": "https://dokumen.pub/qdownload/deep-learning-for-coders-with-fastai-and-pytorch-first-edition-9781492045496-1492045497.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/qdownload/deep-learning-for-coders-with-fastai-and-pytorch-first...", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Calculating Gradients Stepping with a Learning Rate An End-to-End <b>SGD</b> Example Summarizing <b>Gradient</b> <b>Descent</b> The MNIST Loss Function Sigmoid <b>SGD</b> and Mini-Batches Putting It All Together Creating an Optimizer Adding a Nonlinearity Going Deeper Jargon Recap Questionnaire Further Research Chapter 5. Image Classification From Dogs and Cats to Pet Breeds Presizing Checking and Debugging a DataBlock Cross-Entropy Loss Viewing Activations and Labels Softmax Log Likelihood ...", "dateLastCrawled": "2022-01-29T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Gluon Tutorials: Deep Learning - The Straight Dope - VSIP.INFO", "url": "https://vsip.info/gluon-tutorials-deep-learning-the-straight-dope-pdf-free.html", "isFamilyFriendly": true, "displayUrl": "https://vsip.info/gluon-tutorials-deep-learning-the-straight-dope-pdf-free.html", "snippet": "Just <b>like</b> layers, and whole networks, a loss in gluon is just a Block. In [44]: square_loss = gluon.loss.L2Loss() 3.9.9 Optimizer Instead of writing <b>stochastic</b> <b>gradient</b> <b>descent</b> from scratch every time, we can instantiate a gluon. Trainer, passing it a dictionary of parameters. Note that the <b>SGD</b> optimizer in gluon also has a few bells and ...", "dateLastCrawled": "2022-01-28T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Hands On Machine Learning with Scikit Le</b> - studylibfr.com", "url": "https://studylibfr.com/doc/10078323/hands-on-machine-learning-with-scikit-le", "isFamilyFriendly": true, "displayUrl": "https://studylibfr.com/doc/10078323/<b>hands-on-machine-learning-with-scikit-le</b>", "snippet": "Perhaps you would <b>like</b> to give your homemade robot a brain of its own? Make it recognize faces? Or learn to walk around? Or maybe your company has tons of data (user logs, financial data, production data, machine sensor data, hotline stats, HR reports, etc.), and more than likely you could unearth some hidden gems if you just knew where to look; for example: Segment customers and find the best marketing strategy for each group Recommend products for each client based on what similar clients ...", "dateLastCrawled": "2021-12-21T13:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Three lines of machine learning (series 5)-a linear model you don&#39;t ...", "url": "https://www.programmersought.com/article/22083674392/", "isFamilyFriendly": true, "displayUrl": "https://www.programmersought.com/article/22083674392", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) The <b>stochastic</b> <b>gradient</b> <b>descent</b> method is to minimize the loss function of each sample. Although the loss function obtained by each iteration is not toward the global optimal direction, the direction of the large whole is toward the global optimal solution. The final result is often in Near the global optimal solution. Random means that I use an example in the sample to approximate all of my samples to adjust theta, which does not calculate the direction of ...", "dateLastCrawled": "2022-01-27T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Victoria&#39;s ML Implementation Notes - Persagen", "url": "http://persagen.com/files/ml-implementation_notes.html", "isFamilyFriendly": true, "displayUrl": "persagen.com/files/ml-implementation_notes.html", "snippet": "Large noise will dominate the noise-free <b>gradient</b> and allow <b>stochastic</b> <b>gradient</b> <b>descent</b> to be more exploratory. By adding noise only to the problematic parts of the activation function we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find ...", "dateLastCrawled": "2022-02-02T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep <b>learning: adaptive computation and machine</b> learning ... - DOKUMEN.PUB", "url": "https://dokumen.pub/deep-learning-adaptive-computation-and-machine-learning-0262035618-9780262035613.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/deep-<b>learning-adaptive-computation-and-machine-learning-0262035618</b>...", "snippet": "The training algorithm used to adapt the weights of the ADALINE was a special case of an algorithm called <b>stochastic</b> <b>gradient</b> <b>descent</b>. Slightly modified versions of the <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm remain the dominant training algorithms for deep learning models today. Models based on the f (x, w) used by the perceptron and ADALINE are called linear models. These models remain some of the most widely used machine learning models, though in many cases they are trained in different ...", "dateLastCrawled": "2022-01-24T19:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>On-line expectation\u2013maximization algorithm for latent data</b> models ...", "url": "https://www.researchgate.net/publication/261536894_On-line_expectation-maximization_algorithm_for_latent_data_models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261536894_On-line_expectation-maximization...", "snippet": "Later, an alternative approach was proposed in [6] as the Online EM algorithm, which shares some similarities with <b>stochastic</b> <b>gradient</b> <b>descent</b> [4] even though Online EM is not a first-order method ...", "dateLastCrawled": "2021-11-17T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep Learning for Coders with fastai and PyTorch [First edition ...", "url": "https://dokumen.pub/qdownload/deep-learning-for-coders-with-fastai-and-pytorch-first-edition-9781492045496-1492045497.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/qdownload/deep-learning-for-coders-with-fastai-and-pytorch-first...", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Calculating Gradients Stepping with a Learning Rate An End-to-End <b>SGD</b> Example Summarizing <b>Gradient</b> <b>Descent</b> The MNIST Loss Function Sigmoid <b>SGD</b> and Mini-Batches Putting It All Together Creating an Optimizer Adding a Nonlinearity Going Deeper Jargon Recap Questionnaire Further Research Chapter 5. Image Classification From Dogs and Cats to Pet Breeds Presizing Checking and Debugging a DataBlock Cross-Entropy Loss Viewing Activations and Labels Softmax Log Likelihood ...", "dateLastCrawled": "2022-01-29T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Gluon Tutorials: Deep Learning - The Straight Dope - VSIP.INFO", "url": "https://vsip.info/gluon-tutorials-deep-learning-the-straight-dope-pdf-free.html", "isFamilyFriendly": true, "displayUrl": "https://vsip.info/gluon-tutorials-deep-learning-the-straight-dope-pdf-free.html", "snippet": "3.9.9 Optimizer Instead of writing <b>stochastic</b> <b>gradient</b> <b>descent</b> from scratch every time, we can instantiate a gluon. Trainer, passing it a dictionary of parameters. Note that the <b>SGD</b> optimizer in gluon also has a few bells and whistles that you can turn on at will, including momentum and clipping (both are switched off by default). These modifications can help to converge faster and we\u2019ll discuss them later when we go over a variety of optimization algorithms in detail. In [45]: trainer ...", "dateLastCrawled": "2022-01-28T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Artificial Intelligence By Example.pdf [5wgl5d599xo7]", "url": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "isFamilyFriendly": true, "displayUrl": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "snippet": "The concept of <b>gradient</b> <b>descent</b> is minimizing loss or errors between the present result and a goal to attain. First, a cost function is needed. There are four predicates (0-0, 1-1, 1-0, 0-1) to train correctly. We simply need to find out how many are correctly trained at each epoch. The cost function will measure the difference between the training goal (4) and the result of this epoch or training iteration (result). When 0 convergence is reached, it means the training has succeeded. SFTVMU", "dateLastCrawled": "2022-01-27T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Hands On Machine Learning with Scikit Le</b> - studylibfr.com", "url": "https://studylibfr.com/doc/10078323/hands-on-machine-learning-with-scikit-le", "isFamilyFriendly": true, "displayUrl": "https://studylibfr.com/doc/10078323/<b>hands-on-machine-learning-with-scikit-le</b>", "snippet": "Or maybe your company has tons of data (user logs, financial data, production data, machine sensor data, hotline stats, HR reports, etc.), and more than likely you could unearth some hidden gems if you just knew where to look; for example: Segment customers and find the best marketing strategy for each group Recommend products for each client based on what <b>similar</b> clients bought Detect which transactions are likely to be fraudulent Predict next year\u2019s revenue And more Whatever the reason ...", "dateLastCrawled": "2021-12-21T13:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Victoria&#39;s ML Implementation Notes - Persagen", "url": "http://persagen.com/files/ml-implementation_notes.html", "isFamilyFriendly": true, "displayUrl": "persagen.com/files/ml-implementation_notes.html", "snippet": "Large noise will dominate the noise-free <b>gradient</b> and allow <b>stochastic</b> <b>gradient</b> <b>descent</b> to be more exploratory. By adding noise only to the problematic parts of the activation function we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find ...", "dateLastCrawled": "2022-02-02T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep Learning for Computer Vision with Python: ImageNet Bundle ...", "url": "https://ebin.pub/deep-learning-for-computer-vision-with-python-imagenet-bundle-1722487860-9781722487867-d-7948098.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/deep-learning-for-computer-vision-with-python-imagenet-bundle...", "snippet": "In fact, this automatic training procedure formed the basis of <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) which is still used to train very deep neural networks today. During this time period, Perceptron-based techniques were all the rage in the neural network community. However, a 1969 publication by Minsky and Papert [14] effectively stagnated neural network research for nearly a decade. Their work demonstrated that a Perceptron with a linear activation function (regardless of depth) was merely a ...", "dateLastCrawled": "2022-01-20T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep Learning for Coders with fastai and PyTorch [First edition ...", "url": "https://dokumen.pub/qdownload/deep-learning-for-coders-with-fastai-and-pytorch-first-edition-9781492045496-1492045497.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/qdownload/deep-learning-for-coders-with-fastai-and-pytorch-first...", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Calculating Gradients Stepping with a Learning Rate An End-to-End <b>SGD</b> Example Summarizing <b>Gradient</b> <b>Descent</b> The MNIST Loss Function Sigmoid <b>SGD</b> and Mini-Batches Putting It All Together Creating an Optimizer Adding a Nonlinearity Going Deeper Jargon Recap Questionnaire Further Research Chapter 5. Image Classification From Dogs and Cats to Pet Breeds Presizing Checking and Debugging a DataBlock Cross-Entropy Loss Viewing Activations and Labels Softmax Log Likelihood ...", "dateLastCrawled": "2022-01-29T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Gluon Tutorials: Deep Learning - The Straight Dope - VSIP.INFO", "url": "https://vsip.info/gluon-tutorials-deep-learning-the-straight-dope-pdf-free.html", "isFamilyFriendly": true, "displayUrl": "https://vsip.info/gluon-tutorials-deep-learning-the-straight-dope-pdf-free.html", "snippet": "3.9.9 Optimizer Instead of writing <b>stochastic</b> <b>gradient</b> <b>descent</b> from scratch every time, we <b>can</b> instantiate a gluon. Trainer, passing it a dictionary of parameters. Note that the <b>SGD</b> optimizer in gluon also has a few bells and whistles that you <b>can</b> turn on at will, including momentum and clipping (both are switched off by default). These modifications <b>can</b> help to converge faster and we\u2019ll discuss them later when we go over a variety of optimization algorithms in detail. In [45]: trainer ...", "dateLastCrawled": "2022-01-28T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep <b>learning: adaptive computation and machine</b> learning ... - DOKUMEN.PUB", "url": "https://dokumen.pub/deep-learning-adaptive-computation-and-machine-learning-0262035618-9780262035613.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/deep-<b>learning-adaptive-computation-and-machine-learning-0262035618</b>...", "snippet": "Slightly modified versions of the <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm remain the dominant training algorithms for deep learning models today. Models based on the f (x, w) used by the perceptron and ADALINE are called linear models. These models remain some of the most widely used machine learning models, though in many cases they are trained in different ways than the original models were trained. Linear models have many limitations. Most famously, they cannot learn the XOR function, where ...", "dateLastCrawled": "2022-01-24T19:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Hands-<b>On Machine Learning with Scikit-Learn &amp; TensorFlow</b> | Hanwen ...", "url": "https://www.academia.edu/39335333/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39335333/Hands_<b>On_Machine_Learning_with_Scikit_Learn</b>_and...", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2021-12-25T13:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Artificial Intelligence By Example.pdf [5wgl5d599xo7]", "url": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "isFamilyFriendly": true, "displayUrl": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "snippet": "<b>Gradient</b> <b>descent</b> measures the value of the <b>descent</b> to find the direction of the slope: up, down, or 0. Then, once you have that slope and the steepness of it, you <b>can</b> optimize the weights. A derivative is a way to know whether you are going up or down a slope. In this case, I hijacked the concept and used it to set the learning rate with a one-line function. Why not? It helped to solve <b>gradient</b> <b>descent</b> optimization in one line: JG DPOWFSHFODF X C X", "dateLastCrawled": "2022-01-27T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Hands-On Machine Learning", "url": "https://studylib.net/doc/25672177/hands-on-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://studylib.net/doc/25672177/hands-on-machine-learning", "snippet": "Free essays, homework help, flashcards, research papers, book reports, term papers, history, science, politics", "dateLastCrawled": "2022-02-02T13:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Three lines of machine learning (series 5)-a linear model you don&#39;t ...", "url": "https://www.programmersought.com/article/22083674392/", "isFamilyFriendly": true, "displayUrl": "https://www.programmersought.com/article/22083674392", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) The <b>stochastic</b> <b>gradient</b> <b>descent</b> method is to minimize the loss function of each sample. Although the loss function obtained by each iteration is not toward the global optimal direction, the direction of the large whole is toward the global optimal solution. The final result is often in Near the global optimal solution. Random means that I use an example in the sample to approximate all of my samples to adjust theta, which does not calculate the direction of ...", "dateLastCrawled": "2022-01-27T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Victoria&#39;s ML Implementation Notes - Persagen", "url": "http://persagen.com/files/ml-implementation_notes.html", "isFamilyFriendly": true, "displayUrl": "persagen.com/files/ml-implementation_notes.html", "snippet": "Large noise will dominate the noise-free <b>gradient</b> and allow <b>stochastic</b> <b>gradient</b> <b>descent</b> to be more exploratory. By adding noise only to the problematic parts of the activation function we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find ...", "dateLastCrawled": "2022-02-02T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Gluon Tutorials: Deep Learning - The Straight Dope - VSIP.INFO", "url": "https://vsip.info/gluon-tutorials-deep-learning-the-straight-dope-pdf-free.html", "isFamilyFriendly": true, "displayUrl": "https://vsip.info/gluon-tutorials-deep-learning-the-straight-dope-pdf-free.html", "snippet": "3.9.9 Optimizer Instead of writing <b>stochastic</b> <b>gradient</b> <b>descent</b> from scratch every time, we <b>can</b> instantiate a gluon. Trainer, passing it a dictionary of parameters. Note that the <b>SGD</b> optimizer in gluon also has a few bells and whistles that you <b>can</b> turn on at will, including momentum and clipping (both are switched off by default). These modifications <b>can</b> help to converge faster and we\u2019ll discuss them later when we go over a variety of optimization algorithms in detail. In [45]: trainer ...", "dateLastCrawled": "2022-01-28T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Deep Learning for Computer Vision with Python: ImageNet Bundle ...", "url": "https://ebin.pub/deep-learning-for-computer-vision-with-python-imagenet-bundle-1722487860-9781722487867-d-7948098.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/deep-learning-for-computer-vision-with-python-imagenet-bundle...", "snippet": "In fact, this automatic training procedure formed the basis of <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) which is still used to train very deep neural networks today. During this time period, Perceptron-based techniques were all the rage in the neural network community. However, a 1969 publication by Minsky and Papert [14] effectively stagnated neural network research for nearly a decade. Their work demonstrated that a Perceptron with a linear activation function (regardless of depth) was merely a ...", "dateLastCrawled": "2022-01-20T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep Learning for Coders with fastai and PyTorch [First edition ...", "url": "https://dokumen.pub/qdownload/deep-learning-for-coders-with-fastai-and-pytorch-first-edition-9781492045496-1492045497.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/qdownload/deep-learning-for-coders-with-fastai-and-pytorch-first...", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Calculating Gradients Stepping with a Learning Rate An End-to-End <b>SGD</b> Example Summarizing <b>Gradient</b> <b>Descent</b> The MNIST Loss Function Sigmoid <b>SGD</b> and Mini-Batches Putting It All Together Creating an Optimizer Adding a Nonlinearity Going Deeper Jargon Recap Questionnaire Further Research Chapter 5. Image Classification From Dogs and Cats to Pet Breeds Presizing Checking and Debugging a DataBlock Cross-Entropy Loss Viewing Activations and Labels Softmax Log Likelihood ...", "dateLastCrawled": "2022-01-29T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>On-line expectation\u2013maximization algorithm for latent data</b> models ...", "url": "https://www.researchgate.net/publication/261536894_On-line_expectation-maximization_algorithm_for_latent_data_models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261536894_On-line_expectation-maximization...", "snippet": "Later, an alternative approach was proposed in [6] as the Online EM algorithm, which shares some similarities with <b>stochastic</b> <b>gradient</b> <b>descent</b> [4] even though Online EM is not a first-order method ...", "dateLastCrawled": "2021-11-17T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Artificial Intelligence By Example.pdf [5wgl5d599xo7]", "url": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "isFamilyFriendly": true, "displayUrl": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "snippet": "<b>Gradient</b> <b>descent</b> measures the value of the <b>descent</b> to find the direction of the slope: up, down, or 0. Then, once you have that slope and the steepness of it, you <b>can</b> optimize the weights. A derivative is a way to know whether you are going up or down a slope. In this case, I hijacked the concept and used it to set the learning rate with a one-line function. Why not? It helped to solve <b>gradient</b> <b>descent</b> optimization in one line: JG DPOWFSHFODF X C X", "dateLastCrawled": "2022-01-27T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) Hands-<b>On Machine Learning with Scikit-Learn &amp; TensorFlow</b> ...", "url": "https://www.academia.edu/43796316/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43796316/Hands_<b>On_Machine_Learning_with_Scikit_Learn</b>_and...", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-30T17:37:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! | by Aishwarya V ...", "url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>stochastic-gradient-descent</b>-clearly-explained-53d239905d31", "snippet": "<b>Stochastic gradient descent</b> is a very popular and common algorithm used in various <b>Machine</b> <b>Learning</b> algorithms, most importantly forms the basis of Neural Networks. In this article, I have tried my\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! Aishwarya V Srinivasan. Sep 7, 2019 \u00b7 4 min read. <b>Stochastic gradient descent</b> is a ...", "dateLastCrawled": "2022-02-02T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Stochastic gradient descent</b> - The <b>Learning</b> <b>Machine</b>", "url": "https://the-learning-machine.com/article/optimization/stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://the-<b>learning</b>-<b>machine</b>.com/article/optimization/<b>stochastic-gradient-descent</b>", "snippet": "<b>Stochastic gradient descent</b> (<b>SGD</b>) is an approach for unconstrained optimization.<b>SGD</b> is the workhorse of optimization for <b>machine</b> <b>learning</b> approaches. It is used as a faster alternative for training support vector machines and is the preferred optimization routine for deep <b>learning</b> approaches.. In this article, we will motivate the formulation for <b>stochastic gradient descent</b> and provide interactive demos over multiple univariate and multivariate functions to show it in action.", "dateLastCrawled": "2022-01-26T05:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> in Theory and Practice", "url": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) is the most widely used optimization method in the <b>machine</b> <b>learning</b> community. Researchers in both academia and industry have put considerable e ort to optimize <b>SGD</b>\u2019s runtime performance and to develop a theoretical framework for its empirical success. For example, recent advancements in deep neural networks have been largely achieved because, surprisingly, <b>SGD</b> has been found adequate to train them. Here we present three works highlighting desirable ...", "dateLastCrawled": "2022-01-30T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Scikit Learn - Stochastic Gradient Descent</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_stochastic_gradient_descent</b>.htm", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) is a simple yet efficient optimization algorithm used to find the values of parameters/coefficients of functions that minimize a cost function. In other words, it is used for discriminative <b>learning</b> of linear classifiers under convex loss functions such as SVM and Logistic regression. It has been successfully applied to large-scale datasets because the update to the coefficients is performed for each training instance, rather than at the end of instances.", "dateLastCrawled": "2022-02-03T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Adam, <b>Momentum and Stochastic Gradient Descent</b> - <b>Machine</b> <b>Learning</b> From ...", "url": "https://mlfromscratch.com/optimizers-explained/", "isFamilyFriendly": true, "displayUrl": "https://mlfromscratch.com/optimizers-explained", "snippet": "The basic difference between batch <b>gradient</b> <b>descent</b> (BGD) and <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>), is that we only calculate the cost of one example for each step in <b>SGD</b>, but in BGD, we have to calculate the cost for all training examples in the dataset. Trivially, this speeds up neural networks greatly. Exactly this is the motivation behind <b>SGD</b>. The equation for <b>SGD</b> is used to update parameters in a neural network \u2013 we use the equation to update parameters in a backwards pass, using ...", "dateLastCrawled": "2022-02-02T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> <b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b>", "url": "http://www.bel.utcluj.ro/dce/didactic/eai/04_GradientDescent_ML.pdf", "isFamilyFriendly": true, "displayUrl": "www.bel.utcluj.ro/dce/didactic/eai/04_<b>GradientDescent</b>_ML.pdf", "snippet": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> Elements of Artificial Intelligence G. Oltean BGD vs. <b>SGD</b> The summation part is important, especially with the concept of batch <b>gradient</b> <b>descent</b> (BGD) vs. <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>). In Batch <b>Gradient</b> <b>Descent</b>, all the training data is taken into consideration to take a single step (one training epoch ...", "dateLastCrawled": "2022-02-03T10:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Batch, Mini Batch &amp; <b>Stochastic</b> <b>Gradient Descent</b> | by Sushant Patrikar ...", "url": "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/batch-mini-batch-<b>stochastic</b>-<b>gradient-descent</b>-7a62ecba642a", "snippet": "In Batch <b>Gradient Descent</b> we were considering all the examples for every step of <b>Gradient Descent</b>. But what if our dataset is very huge. Deep <b>learning</b> models crave for data. The more the data the more chances of a model to be good. Suppose our dataset has 5 million examples, then just to take one step the model will have to calculate the gradients of all the 5 million examples. This does not seem an efficient way. To tackle this problem we have <b>Stochastic</b> <b>Gradient Descent</b>. In <b>Stochastic</b> ...", "dateLastCrawled": "2022-02-02T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Gradient</b> <b>Descent</b>: One of <b>Machine</b> <b>Learning</b>\u2019s Most Popular Algorithms ...", "url": "https://urmiparekh.medium.com/gradient-descent-one-of-machine-learnings-most-popular-algorithms-c31963d1e67f", "isFamilyFriendly": true, "displayUrl": "https://urmiparekh.medium.com/<b>gradient</b>-<b>descent</b>-one-of-<b>machine</b>-<b>learning</b>s-most-popular...", "snippet": "<b>Gradient</b> <b>descent</b> is an optimization algorithm which is used to train a <b>machine</b> <b>learning</b> model. It is an optimization algorithm to find a local minimum of a differential function. It is used to find the values of a function\u2019s coefficients that minimize a cost function as much as possible. Source: Here. It i s a first-order iterative ...", "dateLastCrawled": "2022-01-17T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Gradient Descent in Logistic Regression [Explained for Beginners</b> ...", "url": "https://www.upgrad.com/blog/gradient-descent-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/<b>gradient-descent-in-logistic-regression</b>", "snippet": "It\u2019s massive, and hence there was a need for a slightly modified <b>Gradient</b> <b>Descent</b> Algorithm, namely \u2013 <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Algorithm (<b>SGD</b>). The only difference <b>SGD</b> has with Normal <b>Gradient</b> <b>Descent</b> is that, in <b>SGD</b>, we don\u2019t deal with the entire training instance at a single time. In <b>SGD</b>, we compute the <b>gradient</b> of the cost function for just a single random example at each iteration. Now, doing so brings down the time taken for computations by a huge margin especially for large ...", "dateLastCrawled": "2022-01-28T16:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent With Momentum from Scratch</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>gradient-descent-with-momentum-from-scratch</b>", "snippet": "<b>Gradient</b> <b>descent</b> is an optimization algorithm that follows the negative <b>gradient</b> of an objective function in order to locate the minimum of the function. A problem with <b>gradient</b> <b>descent</b> is that it can bounce around the search space on optimization problems that have large amounts of curvature or noisy gradients, and it can get stuck in flat spots in the search", "dateLastCrawled": "2022-01-26T05:41:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Gossip <b>Learning</b> as a Decentralized Alternative to Federated <b>Learning</b>", "url": "http://publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "isFamilyFriendly": true, "displayUrl": "publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "snippet": "Federated <b>learning</b> is adistributed <b>machine</b> <b>learning</b> approach for computing models over data collected by edge devices. Most impor-tantly, the data itself is not collected centrally, but a master-worker ar-chitecture is applied where a master node performs aggregation and the edge devices are the workers, not unlike the parameter server approach. Gossip <b>learning</b> also assumes that the data remains at the edge devices, but it requires no aggregation server or any central component. In this ...", "dateLastCrawled": "2022-01-27T14:06:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(stochastic gradient descent (sgd))  is like +(person slowly walking downhill)", "+(stochastic gradient descent (sgd)) is similar to +(person slowly walking downhill)", "+(stochastic gradient descent (sgd)) can be thought of as +(person slowly walking downhill)", "+(stochastic gradient descent (sgd)) can be compared to +(person slowly walking downhill)", "machine learning +(stochastic gradient descent (sgd) AND analogy)", "machine learning +(\"stochastic gradient descent (sgd) is like\")", "machine learning +(\"stochastic gradient descent (sgd) is similar\")", "machine learning +(\"just as stochastic gradient descent (sgd)\")", "machine learning +(\"stochastic gradient descent (sgd) can be thought of as\")", "machine learning +(\"stochastic gradient descent (sgd) can be compared to\")"]}