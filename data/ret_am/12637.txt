{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Matrix Factorization</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/matrix-factorization", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>matrix-factorization</b>", "snippet": "ICA can also be considered as a low rank <b>matrix factorization</b>, if a <b>smaller</b> <b>number</b>, compared to the l observed random variables, of independent components is retained (e.g., selecting the m &lt; l least Gaussian ones). An alternative to the previously discussed low rank <b>matrix factorization</b> schemes was suggested in [144,145], which guarantees the nonnegativity of the elements of the resulting <b>matrix</b> factors. Such a constraint is enforced in certain applications because negative elements ...", "dateLastCrawled": "2022-01-30T01:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "High Dimensionality Reduction by <b>Matrix</b> <b>Factorization</b> for Systems ...", "url": "https://www.biorxiv.org/content/10.1101/2021.05.30.446301v2.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.biorxiv.org/content/10.1101/2021.05.30.446301v2.full.pdf", "snippet": "<b>Matrix</b> <b>Factorization</b>, Systems Pharmacology Key Points: <b>Matrix</b> <b>Factorization</b> (MF) is a useful framework for high dimensionality reduction in systems pharmacology. Novel feature selection methods using the incorporation of the mathematical conception of a basis for features into MF increases the performance of feature selection process.", "dateLastCrawled": "2021-11-19T12:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>FACTORIZATION</b> of MATRICES", "url": "https://web.ma.utexas.edu/users/gilbert/M340L/LA07MatrixDecompositions.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.ma.utexas.edu/users/gilbert/M340L/LA07<b>Matrix</b>Decompositions.pdf", "snippet": "<b>FACTORIZATION</b> of MATRICES Let&#39;s begin by looking at various decompositions of matrices, see also <b>Matrix</b> <b>Factorization</b>. In CS, these decompositions are used to implement efficient <b>matrix</b> algorithms. Indeed, as Lay says in his book: &quot;In the language of computer science, the expression of as a product amounts to a pre-processing of the data in , organizing that data into two or more parts whose structures are more useful in some way, perhaps more accessible for computation.&quot; Nonetheless, the ...", "dateLastCrawled": "2022-02-02T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Using <b>Non-negative matrix factorization</b> to classify companies. | by ...", "url": "https://towardsdatascience.com/using-nmf-to-classify-companies-a77e176f276f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-nmf-to-classify-companies-a77e176f276f", "snippet": "Hence the natural need to reduce the size of the problem by <b>reducing</b> the <b>number</b> of criteria used. Easier said than done. It\u2019s even more difficult to achieve in practice if we add an interpretability constraint. In this article, we will see how the non-negative <b>factorization</b> algorithm (NMF) allows to get closer to these two objectives. Let\u2019s now formalize the problem a bit. <b>Non-negative matrix factorization</b>. Suppose that the available data are represented by an X <b>matrix</b> of type (n,f), i.e ...", "dateLastCrawled": "2022-01-29T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "LOW-RANK <b>MATRIX</b> <b>FACTORIZATION FOR DEEP NEURAL NETWORK</b> TRAINING WITH ...", "url": "https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester1201314/Sainath2013_lrdnn.pdf", "isFamilyFriendly": true, "displayUrl": "https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester1201314/Sainath2013_lrdnn.pdf", "snippet": "rank. If the <b>matrix</b> is low-rank, we can use <b>factorization</b> to represent this <b>matrix</b> by two <b>smaller</b> matrices, thereby signicantly <b>reducing</b> the <b>number</b> of parameters in the network before training. Another benet of low-rank <b>factorization</b> for non-convex objective functions, such as those used in DNN training, is that it constrains the space", "dateLastCrawled": "2022-01-30T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "feature selection - Interpretation of <b>matrix</b> <b>factorization</b> results ...", "url": "https://stats.stackexchange.com/questions/158736/interpretation-of-matrix-factorization-results", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/.../158736/interpretation-of-<b>matrix</b>-<b>factorization</b>-results", "snippet": "<b>Matrix</b> <b>factorization</b> is widely used for its scalability, and its ability to handle sparse datasets, precisely by <b>reducing</b> the feature-space <b>to a smaller</b>, lower-dimensional latent feature-space. But one of its major drawbacks is the lack of interpretability, because the <b>factorization</b> does not preserve the features that were input, nor is there an easy transformation that helps interpret the latent features. But it also means that it is possibly able to find correlations between features that ...", "dateLastCrawled": "2022-01-08T04:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Non-Negative <b>Matrix</b> <b>Factorization</b> for Learning Alignment-Specific ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0028898", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0028898", "snippet": "Our method uses non-negative <b>matrix</b> <b>factorization</b> (NNMF) to learn a set of basis matrices from a general dataset containing <b>a large</b> <b>number</b> of alignments of different proteins, thus capturing the dimensions of important variation. It then learns a set of weights that are specific to the organism or gene of interest and for which only a <b>smaller</b> dataset is available. Thus the alignment-specific model is obtained as a weighted sum of the basis matrices. Having been constrained to vary along only ...", "dateLastCrawled": "2019-07-30T13:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Sparse <b>Matrix</b> <b>Factorization</b>: Applications to Latent Semantic Indexing", "url": "https://trec.nist.gov/pubs/trec18/papers/ursinus.LEGAL.pdf", "isFamilyFriendly": true, "displayUrl": "https://trec.nist.gov/pubs/trec18/papers/ursinus.LEGAL.pdf", "snippet": "called the term-document <b>matrix</b>. Entry ai,j of <b>matrix</b> A indicates how important term i is to document j, where 1 \u2264 i \u2264 t and 1 \u2264 j \u2264 d. The entries in A can be binary numbers (1 if the term appears in the document and 0 otherwise), raw term frequencies (the <b>number</b> of times the term appears in the document), or weighted term frequencies ...", "dateLastCrawled": "2021-11-06T18:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What is matrix factorization? - Quora</b>", "url": "https://www.quora.com/What-is-matrix-factorization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-matrix-factorization</b>", "snippet": "Answer (1 of 3): Given a <b>matrix</b> A, you want to find matrices P and Q such that A \\approx PQ The obvious question is, why would you want to do that? Here are some applications: 1. Storage: Suppose A is <b>a large</b> <b>matrix</b>, say 1000 \\times 1000. Storing this <b>matrix</b> will require storing 1 million valu...", "dateLastCrawled": "2022-01-19T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Strategies and Algorithms for Clustering <b>Large</b> Datasets: A Review", "url": "https://upcommons.upc.edu/bitstream/handle/2117/23415/R13-11.pdf", "isFamilyFriendly": true, "displayUrl": "https://upcommons.upc.edu/bitstream/handle/2117/23415/R13-11.pdf", "snippet": "Being K the <b>number</b> of clusters, with P K i=1 P(w i) = 1. This model is usually \ufb01t using the Expectation-Maximization algorithm (EM), assigning for each example a probability to each clus-ter. The main limitation of all these methods is to assume that the <b>number</b> of partition is known.", "dateLastCrawled": "2022-02-02T15:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Matrix Factorization</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/matrix-factorization", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>matrix-factorization</b>", "snippet": "ICA can also be considered as a low rank <b>matrix factorization</b>, if a <b>smaller</b> <b>number</b>, compared to the l observed random variables, of independent components is retained (e.g., selecting the m &lt; l least Gaussian ones). An alternative to the previously discussed low rank <b>matrix factorization</b> schemes was suggested in [144,145], which guarantees the nonnegativity of the elements of the resulting <b>matrix</b> factors. Such a constraint is enforced in certain applications because negative elements ...", "dateLastCrawled": "2022-01-30T01:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Comparing the <b>Effect of Matrix Factorization Techniques in Reducing the</b> ...", "url": "https://www.academia.edu/5320966/Comparing_the_Effect_of_Matrix_Factorization_Techniques_in_Reducing_the_Time_Complexity_for_Traversing_the_Big_Data_of_Recommendation_Systems", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/5320966/Comparing_the_Effect_of_<b>Matrix</b>_<b>Factorization</b>...", "snippet": "<b>MATRIX</b> <b>FACTORIZATION</b> MODELS information and the <b>number</b> of visitors to Web sites in recent years poses some key challenges for recommendation We have used five <b>factorization</b> models for our research systems. Data is growing very fast. Today, by analyzing <b>large</b> which are: data sets, one can spot business trends, detect environmental 1) Singular Value Decomposition changes, predict forthcoming social agendas and combat 2) LU Decomposition crime. This so-called &quot;Big Data\u201d analytics is ...", "dateLastCrawled": "2022-01-09T23:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Approximate Logic Synthesis Using Boolean Matrix Factorization</b>", "url": "https://cs.brown.edu/research/pubs/theses/masters/2020/ma.jingxiao.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.brown.edu/research/pubs/theses/masters/2020/ma.jingxiao.pdf", "snippet": "<b>Matrix</b> <b>factorization</b> (or decomposition) is a class of algorithms that propose to factor an input <b>matrix</b> n m A into two matrices: a n f <b>matrix</b>, B, and a f m <b>matrix</b>, C, such that A \u02c7BC. In many applications the <b>factorization</b> degree, f, is required to be <b>smaller</b> than m in approximations in the multiplication results. Note that one", "dateLastCrawled": "2022-01-23T00:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reducing</b> Negative Impact of Noise in Boolean <b>Matrix</b> <b>Factorization</b> with ...", "url": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_29", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_29", "snippet": "Boolean <b>matrix</b> <b>factorization</b> (BMF) is a well-established data analytical method whose goal is to decompose a single <b>large</b> <b>matrix</b> into two, preferably <b>smaller</b>, matrices, carrying the same or <b>similar</b> information as the original <b>matrix</b>. In essence, it can be used to reduce data dimensionality and to provide fundamental insight into data. Existing algorithms are often negatively affected by the presence of noise in the data, which is a common case for real-world datasets. We present an initial ...", "dateLastCrawled": "2021-12-29T21:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is the state-of-the-art method for <b>matrix</b> <b>factorization</b> for ...", "url": "https://www.quora.com/What-is-the-state-of-the-art-method-for-matrix-factorization-for-collaborative-filtering", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-state-of-the-art-method-for-<b>matrix</b>-<b>factorization</b>-for...", "snippet": "Answer: MF algorithms try to decompose an original <b>matrix</b> of user/item preferences into two <b>smaller</b> matrices called latent factors : U from users&#39; characteristics and I from item characteristics. Then these factors are used for recommender system: There are some problems that arise when using th...", "dateLastCrawled": "2022-01-15T23:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Non-Negative <b>Matrix</b> <b>Factorization</b> for Learning Alignment-Specific ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0028898", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0028898", "snippet": "Our method uses non-negative <b>matrix</b> <b>factorization</b> (NNMF) to learn a set of basis matrices from a general dataset containing <b>a large</b> <b>number</b> of alignments of different proteins, thus capturing the dimensions of important variation. It then learns a set of weights that are specific to the organism or gene of interest and for which only a <b>smaller</b> dataset is available. Thus the alignment-specific model is obtained as a weighted sum of the basis matrices. Having been constrained to vary along only ...", "dateLastCrawled": "2019-07-30T13:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Matrix</b> <b>Factorization Algorithms for the Identification</b> of Muscle ...", "url": "https://journals.physiology.org/doi/full/10.1152/jn.00222.2005", "isFamilyFriendly": true, "displayUrl": "https://journals.physiology.org/doi/full/10.1152/jn.00222.2005", "snippet": "Several recent studies have used <b>matrix</b> <b>factorization</b> algorithms to assess the hypothesis that behaviors might be produced through the combination of a small <b>number</b> of muscle synergies. Although generally agreeing in their basic conclusions, these studies have used a range of different algorithms, making their interpretation and integration difficult. We therefore compared the performance of these different algorithms on both simulated and experimental data sets. We focused on the ability of ...", "dateLastCrawled": "2022-01-29T23:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>What is matrix factorization? - Quora</b>", "url": "https://www.quora.com/What-is-matrix-factorization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-matrix-factorization</b>", "snippet": "Answer (1 of 3): Given a <b>matrix</b> A, you want to find matrices P and Q such that A \\approx PQ The obvious question is, why would you want to do that? Here are some applications: 1. Storage: Suppose A is <b>a large</b> <b>matrix</b>, say 1000 \\times 1000. Storing this <b>matrix</b> will require storing 1 million valu...", "dateLastCrawled": "2022-01-19T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The magic behind <b>Recommendation</b> Systems | by Anthony Figueroa | Towards ...", "url": "https://towardsdatascience.com/the-magic-behind-recommendation-systems-c3fc44927b3c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-magic-behind-<b>recommendation</b>-systems-c3fc44927b3c", "snippet": "<b>Matrix</b> <b>factorization</b> can be seen as breaking down <b>a large</b> <b>matrix</b> into a product of <b>smaller</b> ones. This <b>is similar</b> to the <b>factorization</b> of integers, where 12 can be written as 6 x 2 or 4 x 3. In the case of matrices, a <b>matrix</b> A with dimensions m x n can be reduced to a product of two matrices X and Y with dimensions m x p and p x n respectively.", "dateLastCrawled": "2022-01-19T18:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Build a Recommendation Engine With <b>Collaborative Filtering</b> \u2013 Real Python", "url": "https://realpython.com/build-recommendation-engine-collaborative-filtering/", "isFamilyFriendly": true, "displayUrl": "https://realpython.com/build-recommendation-engine-<b>collaborative-filtering</b>", "snippet": "<b>Matrix</b> <b>factorization</b> can be seen as breaking down <b>a large</b> <b>matrix</b> into a product of <b>smaller</b> ones. This <b>is similar</b> to the <b>factorization</b> of integers, where 12 can be written as 6 x 2 or 4 x 3. In the case of matrices, a <b>matrix</b> A with dimensions m x n can be reduced to a product of two matrices X and Y with dimensions m x p and p x n respectively. Note: In <b>matrix</b> multiplication, a <b>matrix</b> X can be multiplied by Y only if the <b>number</b> of columns in X is equal to the <b>number</b> of rows in Y. Therefore ...", "dateLastCrawled": "2022-02-02T15:03:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Matrix factorization</b> of <b>large</b> scale data using multistage <b>matrix</b> ...", "url": "https://link.springer.com/article/10.1007/s10489-020-01957-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10489-020-01957-0", "snippet": "A <b>number</b> of approaches <b>can</b> be used to optimize the objective function ... further experimentation is possible wherein extending to multiple stages <b>can</b> <b>be thought</b> of. Incremental <b>matrix factorization</b> as discussed in Section 4 is one of the applications of MsMF. The incremental MF approach <b>can</b> be enhanced to include additional contextual information, of each group, for better insights and convergence as detailed in . <b>Matrix factorization</b> of <b>large</b> data sets with parallelism is another ...", "dateLastCrawled": "2022-02-01T00:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "feature selection - Interpretation of <b>matrix</b> <b>factorization</b> results ...", "url": "https://stats.stackexchange.com/questions/158736/interpretation-of-matrix-factorization-results", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/.../158736/interpretation-of-<b>matrix</b>-<b>factorization</b>-results", "snippet": "<b>Matrix</b> <b>factorization</b> is widely used for its scalability, and its ability to handle sparse datasets, precisely by <b>reducing</b> the feature-space <b>to a smaller</b>, lower-dimensional latent feature-space. But one of its major drawbacks is the lack of interpretability, because the <b>factorization</b> does not preserve the features that were input, nor is there an easy transformation that helps interpret the latent features.", "dateLastCrawled": "2022-01-08T04:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Stochastic <b>Matrix</b> <b>Factorization</b> | DeepAI", "url": "https://deepai.org/publication/stochastic-matrix-factorization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/stochastic-<b>matrix</b>-<b>factorization</b>", "snippet": "Additionally, we <b>can</b> speed up the retrieval problem by <b>reducing</b> the <b>number</b> of points to be checked in each picture from 81 to 10. The trade off is that the <b>matrix</b> <b>factorization</b> is an approximation of the original data. The <b>smaller</b> <b>number</b> of base images, the poorer the approximation but the <b>smaller</b> the data and the quicker the retrieval. The ...", "dateLastCrawled": "2022-01-05T07:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Exploration Of Dimensionality Reduction Techniques- Part</b> I | by Shubham ...", "url": "https://medium.com/subex-ai-labs/exploration-of-dimensionality-reduction-techniques-part-i-1f8f6fd0e280", "isFamilyFriendly": true, "displayUrl": "https://medium.com/subex-ai-labs/<b>exploration-of-dimensionality-reduction-techniques</b>...", "snippet": "The singular value decomposition of a <b>matrix</b> A is the <b>factorization</b> of A into the product of three matrices A= UDV^T where the columns of U and V are orthonormal and the <b>matrix</b> D is diagonal with ...", "dateLastCrawled": "2021-01-19T23:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Matrix</b> <b>Factorization Algorithms for the Identification</b> of Muscle ...", "url": "https://www.researchgate.net/publication/7377612_Matrix_Factorization_Algorithms_for_the_Identification_of_Muscle_Synergies_Evaluation_on_Simulated_and_Experimental_Data_Sets", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/7377612_<b>Matrix</b>_<b>Factorization</b>_Algorithms_for...", "snippet": "Several recent studies have used <b>matrix</b> <b>factorization</b> algorithms to assess the hypothesis that behaviors might be produced through the combination of a small <b>number</b> of muscle synergies.", "dateLastCrawled": "2022-01-18T09:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>What is matrix factorization? - Quora</b>", "url": "https://www.quora.com/What-is-matrix-factorization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-matrix-factorization</b>", "snippet": "Answer (1 of 3): Given a <b>matrix</b> A, you want to find matrices P and Q such that A \\approx PQ The obvious question is, why would you want to do that? Here are some applications: 1. Storage: Suppose A is <b>a large</b> <b>matrix</b>, say 1000 \\times 1000. Storing this <b>matrix</b> will require storing 1 million valu...", "dateLastCrawled": "2022-01-19T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Simultaneous Non-Negative Matrix Factorization for Multiple</b> <b>Large</b> ...", "url": "https://www.researchgate.net/publication/234000191_Simultaneous_Non-Negative_Matrix_Factorization_for_Multiple_Large_Scale_Gene_Expression_Datasets_in_Toxicology", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/234000191", "snippet": "Non-negative <b>matrix</b> <b>factorization</b> is a useful tool for <b>reducing</b> the dimension of <b>large</b> datasets. This work considers <b>simultaneous non-negative matrix factorization</b> of multiple sources of data.", "dateLastCrawled": "2021-12-20T06:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Do I have to train <b>my matrix factorization based recommendation system</b> ...", "url": "https://www.quora.com/Do-I-have-to-train-my-matrix-factorization-based-recommendation-system-each-time-a-new-user-is-added", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Do-I-have-to-train-<b>my-matrix-factorization-based-recommendation</b>...", "snippet": "Answer (1 of 2): No, it is possible to estimate the new users\u2019 latent factors . A <b>matrix</b> <b>factorization</b> system decomposes the user-item <b>matrix</b> into a user-factor <b>matrix</b> and a factor-item <b>matrix</b>. Normally a recommendation is just the dot product of the users\u2019 row and the items\u2019 column, possibly wi...", "dateLastCrawled": "2022-01-21T00:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What&#39;s New in <b>Compressive Sensing and Matrix Factorization</b> This Month", "url": "https://nuit-blanche.blogspot.com/2012/10/whats-new-in-compressive-sensing-and.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2012/10/whats-new-in-compressive-sensing-and.html", "snippet": "Compressive sensing theory has shown that sparse signals <b>can</b> be recovered exactly from only a small <b>number</b> of measurements when the forward sensing <b>matrix</b> is sufficiently incoherent. In this Letter, we present a method of preconditioning the FDOT forward <b>matrix</b> to reduce its coherence. The reconstruction results using real data obtained from a phantom experiment show visual and quantitative improvements due to preconditioning in conjunction with convex relaxation and greedy-type sparse ...", "dateLastCrawled": "2022-01-17T10:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "This Week in Compressive Sensing and Advanced <b>Matrix</b> <b>Factorization</b>", "url": "https://nuit-blanche.blogspot.com/2012/06/this-week-in-compressive-sensing-and.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2012/06/this-week-in-compressive-sensing-and.html", "snippet": "Compressed sensing electronics <b>can</b> be used to multiplex <b>a large</b> <b>number</b> of individual readout sensors to significantly reduce the <b>number</b> of readout channels. Methods: Using brute force optimization method, a two level sensing <b>matrix</b> based on a 2-weight constant weight code C1[128:32] followed by a 3 weight constant weight code C2[32:16] was designed. These codes consists of discrete resistor elements either connected or not connected to intermediate or output signals. A PET block detector PCB ...", "dateLastCrawled": "2022-01-31T02:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "NMF-mGPU: non-negative <b>matrix</b> <b>factorization</b> on multi-GPU systems", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4339678/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4339678", "snippet": "The Non-negative <b>Matrix</b> <b>Factorization</b> (NMF) [4,5] technique has also been established as a very effective method to discover biological patterns. NMF decomposes <b>a large</b> input dataset into a small set of highly interpretable and additive parts. This property has centered the attention of scientists for a wide range of applications in Bioinformatics, such as gene-expression analysis 6,7], scientific literature mining , neuroscience , and several -omics techniques [10,11]. A good review of some ...", "dateLastCrawled": "2022-01-06T21:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Matrix Factorization</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/matrix-factorization", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>matrix-factorization</b>", "snippet": "ICA <b>can</b> also be considered as a low rank <b>matrix factorization</b>, if a <b>smaller</b> <b>number</b>, <b>compared</b> to the l observed random variables, of independent components is retained (e.g., selecting the m &lt; l least Gaussian ones). An alternative to the previously discussed low rank <b>matrix factorization</b> schemes was suggested in [144,145], which guarantees the nonnegativity of the elements of the resulting <b>matrix</b> factors. Such a constraint is enforced in certain applications because negative elements ...", "dateLastCrawled": "2022-01-30T01:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Simultaneous Non-Negative <b>Matrix</b> <b>Factorization</b> for Multiple <b>Large</b> Scale ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0048238", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0048238", "snippet": "Non-negative <b>matrix</b> <b>factorization</b> is a useful tool for <b>reducing</b> the dimension of <b>large</b> datasets. This work considers simultaneous non-negative <b>matrix</b> <b>factorization</b> of multiple sources of data. In particular, we perform the first study that involves more than two datasets. We discuss the algorithmic issues required to convert the approach into a practical computational tool and apply the technique to new gene expression data quantifying the molecular changes in four tissue types due to ...", "dateLastCrawled": "2020-01-12T05:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "LOW-RANK <b>MATRIX FACTORIZATION FOR DEEP NEURAL</b> NETWORK TRAINING WITH ...", "url": "http://www.vikas.sindhwani.org/lowRank.pdf", "isFamilyFriendly": true, "displayUrl": "www.vikas.sindhwani.org/lowRank.pdf", "snippet": "If the <b>matrix</b> is low-rank, we <b>can</b> use <b>factorization</b> to represent this <b>matrix</b> by two <b>smaller</b> matrices, thereby signi\ufb01cantly <b>reducing</b> the <b>number</b> of parameters in the network before training. Another bene\ufb01t of low-rank <b>factorization</b> for non-convex objective functions, such as those used in DNN training, is that it constrains the space of search directions that <b>can</b> be explored to maximize the objective function. This helps to make the optimization more ef\ufb01cient and reduce the <b>number</b> of ...", "dateLastCrawled": "2022-02-01T22:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Generalized <b>Matrix</b> <b>Factorization</b> Techniques for Approximate Logic Synthesis", "url": "http://scale.engin.brown.edu/pdfs/date19.pdf", "isFamilyFriendly": true, "displayUrl": "scale.engin.brown.edu/pdfs/date19.pdf", "snippet": "Binary <b>matrix</b> <b>factorization</b> <b>can</b> use different algebra when optimizing the factorized matrices to better approximate the original <b>matrix</b>. In the case of Boolean <b>matrix</b> <b>factorization</b> (BMF) proposed in our previous work [1], the algebra imple-ments a semi-ring algebra, where the addition is carried out using logical OR, i.e., 1+1=1. One potential shortcoming of using OR-based Boolean arithmetic is that ORing two bases from B with a \u20181\u2019 in the same location will lead to a \u20181\u2019, and this ...", "dateLastCrawled": "2021-09-20T22:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Sparse multi-view <b>matrix</b> <b>factorization</b>: a multivariate approach to ...", "url": "https://academic.oup.com/bioinformatics/article/31/19/3163/211636", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/bioinformatics/article/31/19/3163/211636", "snippet": "Except for house-keeping genes, the expressions of <b>a large</b> <b>number</b> of genes vary from tissue to tissue, and some may only be expressed in a particular tissue or a certain cell type (Xia etal., 2007). The regulation of tissue-specific expression is a complex process in which a gene\u2019s enhancer plays a key role regulating gene expressions via DNA methylation ( Ong and Corces, 2011 ).", "dateLastCrawled": "2022-01-17T14:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Block-<b>matrix</b>: optimal <b>fill-in reduction</b> for LU <b>factorization</b> ...", "url": "https://scicomp.stackexchange.com/questions/34006/block-matrix-optimal-fill-in-reduction-for-lu-factorization", "isFamilyFriendly": true, "displayUrl": "https://scicomp.stackexchange.com/questions/34006/block-<b>matrix</b>-optimal-fill-in...", "snippet": "This is done for a very good reason: for <b>a large</b> sparse <b>matrix</b>, finding an optimal reordering is very expensive, unneeded, and is unlikely to give substantial benefits <b>compared</b> to the common relatively fast techniques. However, I wonder if that <b>can</b> be slightly different for the <b>factorization</b> of a block-<b>matrix</b>.", "dateLastCrawled": "2022-01-19T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Stochastic <b>Matrix</b> <b>Factorization</b> | DeepAI", "url": "https://deepai.org/publication/stochastic-matrix-factorization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/stochastic-<b>matrix</b>-<b>factorization</b>", "snippet": "Additionally, we <b>can</b> speed up the retrieval problem by <b>reducing</b> the <b>number</b> of points to be checked in each picture from 81 to 10. The trade off is that the <b>matrix</b> <b>factorization</b> is an approximation of the original data. The <b>smaller</b> <b>number</b> of base images, the poorer the approximation but the <b>smaller</b> the data and the quicker the retrieval. The ...", "dateLastCrawled": "2022-01-05T07:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Why are Matrix Factorization models learnt by</b> SVD-like methods prone to ...", "url": "https://www.quora.com/Why-are-Matrix-Factorization-models-learnt-by-SVD-like-methods-prone-to-overfitting", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Why-are-Matrix-Factorization-models-learnt-by</b>-SVD-like-methods...", "snippet": "Answer (1 of 3): <b>Matrix</b> <b>factorization</b> models are prone to overfitting for the same reasons other machine learning models: the complexity of the model (i.e. <b>number</b> of parameters) is usually <b>large</b>, especially when <b>compared</b> to the <b>number</b> of points in the training set. In a standard MF formulation, ...", "dateLastCrawled": "2022-01-15T23:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What is matrix factorization? - Quora</b>", "url": "https://www.quora.com/What-is-matrix-factorization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-matrix-factorization</b>", "snippet": "Answer (1 of 3): Given a <b>matrix</b> A, you want to find matrices P and Q such that A \\approx PQ The obvious question is, why would you want to do that? Here are some applications: 1. Storage: Suppose A is <b>a large</b> <b>matrix</b>, say 1000 \\times 1000. Storing this <b>matrix</b> will require storing 1 million valu...", "dateLastCrawled": "2022-01-19T19:30:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Matrix</b> <b>Factorization</b> for <b>Machine</b> <b>Learning</b>", "url": "https://machinelearningmastery.com/introduction-to-matrix-decompositions-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/introduction-to-<b>matrix</b>-decompositions-for-<b>machine</b>...", "snippet": "A common <b>analogy</b> for <b>matrix</b> decomposition is the factoring of numbers, such as the factoring of 10 into 2 x 5. For this reason, <b>matrix</b> decomposition is also called <b>matrix</b> <b>factorization</b>. Like factoring real values, there are many ways to decompose a <b>matrix</b>, hence there are a range of different <b>matrix</b> decomposition techniques. Two simple and widely used <b>matrix</b> decomposition methods are the LU <b>matrix</b> decomposition and the QR <b>matrix</b> decomposition. Next, we will take a closer look at each of ...", "dateLastCrawled": "2022-02-03T04:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "16.3. <b>Matrix</b> <b>Factorization</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://www.d2l.ai/chapter_recommender-systems/mf.html", "isFamilyFriendly": true, "displayUrl": "https://www.d2l.ai/chapter_recommender-systems/mf.html", "snippet": "<b>Matrix</b> <b>Factorization</b> [Koren et al., 2009] is a well-established algorithm in the recommender systems literature. The first version of <b>matrix</b> <b>factorization</b> model is proposed by Simon Funk in a famous blog post in which he described the idea of factorizing the interaction <b>matrix</b>. It then became widely known due to the Netflix contest which was held in 2006.", "dateLastCrawled": "2022-01-31T10:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to Matrices and <b>Matrix</b> Arithmetic for <b>Machine Learning</b>", "url": "https://machinelearningmastery.com/introduction-matrices-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/introduction-matrices-<b>machine-learning</b>", "snippet": "A likely first place you may encounter a <b>matrix</b> in <b>machine learning</b> is in model training data comprised of many rows and columns and often represented using the capital letter \u201cX\u201d. The geometric <b>analogy</b> used to help understand vectors and some of their operations does not hold with matrices. Further, a vector itself may be considered a <b>matrix</b> with one column and multiple rows. Often the dimensions of the <b>matrix</b> are denoted as m and n for the number of rows and the number of columns. Now ...", "dateLastCrawled": "2022-02-02T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "6 Math Foundations to Start <b>Learning</b> <b>Machine Learning</b> | by Cornellius ...", "url": "https://towardsdatascience.com/6-math-foundation-to-start-learning-machine-learning-1afef04f42bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/6-math-foundation-to-start-<b>learning</b>-<b>machine-learning</b>-1...", "snippet": "<b>Matrix</b> Decomposition aims to simplify more complex <b>matrix</b> operations on the decomposed <b>matrix</b> rather than on its original <b>matrix</b>. A common <b>analogy</b> for <b>matrix</b> decomposition is like factoring numbers, such as factoring 8 into 2 x 4. This is why <b>matrix</b> decomposition is synonymical to <b>matrix</b> <b>factorization</b>. There are many ways to decompose a <b>matrix</b> ...", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "16.9. <b>Factorization Machines</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_recommender-systems/fm.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_recommender-systems/fm.html", "snippet": "<b>Factorization machines</b> (FM) [Rendle, 2010], proposed by Steffen Rendle in 2010, is a supervised algorithm that can be used for classification, regression, and ranking tasks. It quickly took notice and became a popular and impactful method for making predictions and recommendations. Particularly, it is a generalization of the linear regression model and the <b>matrix</b> <b>factorization</b> model. Moreover, it is reminiscent of support vector machines with a polynomial kernel. The strengths of ...", "dateLastCrawled": "2022-01-30T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning</b> Word Vectors with <b>Linear Constraints: A Matrix Factorization</b> ...", "url": "https://www.ijcai.org/Proceedings/2018/0582.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/2018/0582.pdf", "snippet": "A <b>Matrix</b> <b>Factorization</b> Approach Wenye Li1;2, Jiawei Zhang1, Jianjun Zhou2 andLaizhong Cui3 1 The Chinese University of Hong Kong, Shenzhen, China 2 Shenzhen Research Institute of Big Data, Shenzhen, China 3 Shenzhen University, Shenzhen, China wyli@cuhk.edu.cn, 216019001@link.cuhk.edu.cn, benz@sribd.cn, cuilz@szu.edu.cn Abstract <b>Learning</b> vector space representation of words, or word embedding, has attracted much recent research attention. With the objective of better capturing the semantic ...", "dateLastCrawled": "2021-11-19T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Matrix Factorization</b> Intuition for Movie Recommender System | by Himang ...", "url": "https://medium.com/skyshidigital/matrix-factorization-intuition-for-movie-recommender-system-f25804836327", "isFamilyFriendly": true, "displayUrl": "https://medium.com/skyshidigital/<b>matrix-factorization</b>-intuition-for-movie-recommender...", "snippet": "The classic problem in any supervised <b>machine</b> <b>learning</b> is overfitting which is a condition where the model manage to accurately predict for the data that we use in training process but is not able ...", "dateLastCrawled": "2021-12-12T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Objective Functions: A Simple Example with <b>Matrix</b> Factorisation", "url": "https://mlatcl.github.io/mlai/slides/02-matrix-factorization.slides.html", "isFamilyFriendly": true, "displayUrl": "https://mlatcl.github.io/mlai/slides/02-<b>matrix</b>-<b>factorization</b>.slides.html", "snippet": "Objective Functions: A Simple Example with <b>Matrix</b> Factorisation. Neil D. Lawrence. Objective Function. Last week we motivated the importance of probability. This week we motivate the idea of the \u2018objective function\u2019. Introduction to Classification Classification. Wake word classification (Global Pulse Project). Breakthrough in 2012 with ImageNet result of Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. We are given a data set containing \u2018inputs\u2019, \\(\\mathbf{X}\\) and \u2018targets ...", "dateLastCrawled": "2022-02-02T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Deep Non-Negative <b>Matrix</b> <b>Factorization</b> Neural Network", "url": "https://www1.cmc.edu/pages/faculty/BHunter/papers/deep-negative-matrix.pdf", "isFamilyFriendly": true, "displayUrl": "https://www1.cmc.edu/pages/faculty/BHunter/papers/deep-negative-<b>matrix</b>.pdf", "snippet": "A Deep Non-Negative <b>Matrix</b> <b>Factorization</b> Neural Network Jennifer Flenner Blake Hunter 1 Abstract Recently, deep neural network algorithms have emerged as one of the most successful <b>machine</b> <b>learning</b> strategies, obtaining state of the art results for speech recognition, computer vision, and classi cation of large data sets. Their success is due to advancement in computing power, availability of massive amounts of data and the development of new computational techniques. Some of the drawbacks ...", "dateLastCrawled": "2022-02-03T04:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>Learning</b> Classifier: Basics and Evaluation \u2014 <b>James Le</b>", "url": "https://jameskle.com/writes/ml-basics-and-evaluation", "isFamilyFriendly": true, "displayUrl": "https://jameskle.com/writes/ml-basics-and-evaluation", "snippet": "<b>Matrix</b> transpose is when we flip a <b>matrix</b>\u2019s columns and rows, so row 1 is now column 1, and so on. Given a <b>matrix</b> A, its inverse A^(-1) is a <b>matrix</b> such that A x A^(-1) = I. If A^(-1) exists, then A is invertible or non-singular. Otherwise, it is singular. <b>Machine</b> <b>Learning</b>. 1 \u2014 Main Approaches. The 3 major approaches to <b>machine</b> <b>learning</b> are:", "dateLastCrawled": "2022-01-04T16:12:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>GitHub</b> - DCtheTall/<b>introduction-to-machine-learning</b>: My own ...", "url": "https://github.com/DCtheTall/introduction-to-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/DCtheTall/<b>introduction-to-machine-learning</b>", "snippet": "<b>Introduction to Machine Learning</b> with Python Table of Contents Chapter 1 Introduction Chapter 2 Supervised <b>Learning</b> k-Nearest Neighbors Linear Regression Ridge Regression Lasso Regression Logistic Regression Naive Bayes Classifiers Decision Trees Kernelized Support Vector Machines Neural Networks Predicting Uncertainty Chapter 3 Unsupervised <b>Learning</b> Preprocessing and Scaling Principal Component Analysis Non-negative Matrix Factorization Manifold <b>Learning</b> k-Means Clustering Agglomerative ...", "dateLastCrawled": "2021-09-16T10:45:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "when using matrix factorization is it will work because there is a low ...", "url": "https://www.coursehero.com/file/pastgfv/when-using-matrix-factorization-is-it-will-work-because-there-is-a-low-rank/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/pastgfv/when-using-matrix-factorization-is-it-will...", "snippet": "when using matrix factorization is it will work because there is a low rank from CS 188 at Columbia University", "dateLastCrawled": "2021-12-25T11:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Singular Value decomposition (<b>SVD</b>) in recommender systems for Non-math ...", "url": "https://medium.com/@m_n_malaeb/singular-value-decomposition-svd-in-recommender-systems-for-non-math-statistics-programming-4a622de653e9", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@m_n_malaeb/singular-value-decomposition-<b>svd</b>-in-recommender-systems...", "snippet": "From a high level, <b>matrix factorization can be thought of as</b> finding 2 matrices whose product is the original matrix. Each item can be represented by a vector ` qi `.", "dateLastCrawled": "2022-01-28T23:02:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(matrix factorization)  is like +(reducing a large number to a smaller number)", "+(matrix factorization) is similar to +(reducing a large number to a smaller number)", "+(matrix factorization) can be thought of as +(reducing a large number to a smaller number)", "+(matrix factorization) can be compared to +(reducing a large number to a smaller number)", "machine learning +(matrix factorization AND analogy)", "machine learning +(\"matrix factorization is like\")", "machine learning +(\"matrix factorization is similar\")", "machine learning +(\"just as matrix factorization\")", "machine learning +(\"matrix factorization can be thought of as\")", "machine learning +(\"matrix factorization can be compared to\")"]}