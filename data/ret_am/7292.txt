{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Neural Networks are Function Approximation</b> Algorithms", "url": "https://machinelearningmastery.com/neural-networks-are-function-approximators/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>neural-networks-are-function</b>-approximators", "snippet": "\u2026 the <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feedforward network with a linear output layer and at least one hidden layer with <b>any</b> \u201csquashing\u201d activation function (such as the logistic sigmoid activation function) <b>can</b> <b>approximate</b> <b>any</b> [\u2026] function from one finite-dimensional space to another with <b>any</b> desired non-zero amount of error, provided that the network is given <b>enough</b> hidden units", "dateLastCrawled": "2022-01-30T03:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Are Deep Neural Networks Dramatically Overfitted?", "url": "https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically-overfitted.html", "isFamilyFriendly": true, "displayUrl": "https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically...", "snippet": "The <b>Universal</b> <b>Approximation</b> <b>Theorem</b> states that a feedforward network with: 1) a linear output layer, 2) at least one hidden layer containing a finite number of neurons and 3) some activation function <b>can</b> <b>approximate</b> <b>any</b> continuous functions on a compact subset of \\(\\mathbb{R}^n\\) to <b>arbitrary</b> accuracy. The <b>theorem</b> was first proved for sigmoid ...", "dateLastCrawled": "2022-02-01T06:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Recent progress of <b>machine</b> <b>learning</b> in flow modeling and active flow ...", "url": "https://www.sciencedirect.com/science/article/pii/S100093612100306X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S100093612100306X", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> proposed by Hornik et al. 18 shows that if a feedforward neural network has a linear output layer and at least one hidden layer with <b>any</b> kind of \u201csqueezing\u201d activation function (such as logistic or sigmoid), it <b>can</b> <b>approximate</b> <b>any</b> measurable function from one finite-dimensional space to another with <b>arbitrary</b> precision. The most classical model of the neural network in the context of pattern recognition is the feed-forward neural network, also known as ...", "dateLastCrawled": "2022-01-18T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Chapter <b>7 Neural networks</b> | <b>Machine</b> <b>Learning</b> for Factor Investing", "url": "http://www.mlfactor.com/NN.html", "isFamilyFriendly": true, "displayUrl": "www.mlfactor.com/NN.html", "snippet": "The raw results on <b>universal</b> <b>approximation</b> imply that <b>any</b> well-behaved function \\(f\\) <b>can</b> be approached sufficiently closely by a simple neural network, as long as the number of units <b>can</b> be arbitrarily large. Now, they do not directly relate to the <b>learning</b> phase, i.e., when the model is optimized with respect to a particular dataset. In a series of papers", "dateLastCrawled": "2022-01-30T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Extreme learning</b> machines: a survey - International Journal of <b>Machine</b> ...", "url": "https://link.springer.com/article/10.1007/s13042-011-0019-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13042-011-0019-y", "snippet": "<b>Universal</b> <b>approximation</b> <b>theorem</b>. Huang et al. ... OS-ELM is a simple and efficient online sequential <b>learning</b> <b>algorithm</b> that <b>can</b> handle both additive and RBF nodes in a unified framework. OS-ELM <b>can</b> <b>learn</b> the <b>training</b> <b>data</b> not only one-by-one but also chunk by chunk (with fixed or varying length) and discard the <b>data</b> for which the <b>training</b> has already been done. The <b>training</b> observations are sequentially presented to the <b>learning</b> <b>algorithm</b> (one-by-one or chunk-by-chunk with varying or fixed ...", "dateLastCrawled": "2022-01-29T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep, Skinny Neural <b>Networks are not Universal Approximators</b> | DeepAI", "url": "https://deepai.org/publication/deep-skinny-neural-networks-are-not-universal-approximators", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/deep-skinny-neural-<b>networks-are-not-universal-approximators</b>", "snippet": "A number of papers [3, 6, 11, 13] have shown that neural networks with a single hidden layer are a <b>universal</b> approximator, i.e. that they <b>can</b> <b>approximate</b> <b>any</b> continuous function on a compact domain to <b>arbitrary</b> accuracy if the hidden layer is allowed to have an arbitrarily high dimension. In practice, however, the neural networks that have proved most effective tend to have a large number of relatively low-dimensional hidden layers.", "dateLastCrawled": "2021-12-29T03:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What does it mean by the statement, &#39;neural networks are <b>universal</b> ...", "url": "https://www.quora.com/What-does-it-mean-by-the-statement-neural-networks-are-universal-approximators-in-a-mathematical-intuitive-sense", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-does-it-mean-by-the-statement-neural-networks-are-<b>universal</b>...", "snippet": "Answer (1 of 4): Consider the set of all continuous functions which are defined on the unit hypercube (i.e. the unit square in two dimensions, the unit cube in three dimensions, etc.). Call this set C. Given two functions in C, it is possible to define a metric which calculates a notion of dist...", "dateLastCrawled": "2022-01-06T20:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Universal Approximation of Extreme Learning Machine With Adaptive</b> ...", "url": "https://www.researchgate.net/publication/220279841_Universal_Approximation_of_Extreme_Learning_Machine_With_Adaptive_Growth_of_Hidden_Nodes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220279841_<b>Universal</b>_<b>Approximation</b>_of_Extreme...", "snippet": "In this paper, a local discriminant preserving extreme <b>learning</b> <b>machine</b> autoencoder (LDELM-AE) is proposed to <b>learn</b> <b>data</b> representations with the local geometry and local discriminant exploited ...", "dateLastCrawled": "2021-10-13T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Train Neural network with infinite amount of <b>data</b> ...", "url": "https://cs.stackexchange.com/questions/44831/train-neural-network-with-infinite-amount-of-data", "isFamilyFriendly": true, "displayUrl": "https://cs.stackexchange.com/.../44831/train-neural-network-with-infinite-amount-of-<b>data</b>", "snippet": "There&#39;s a standard <b>theorem</b> saying that <b>any</b> function <b>can</b> be approximated arbitrarily well by a multi-layer perceptron-based neural network, if given <b>enough</b> <b>training</b> <b>data</b> and <b>enough</b> layers and <b>enough</b> neurons. However, this <b>theorem</b> isn&#39;t terribly useful or relevant to practice, as it doesn&#39;t make <b>any</b> promises about how many layers or neurons or <b>training</b> samples are needed. See", "dateLastCrawled": "2022-01-18T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "4. Model <b>Training</b> Patterns - <b>Machine Learning Design Patterns</b> [Book]", "url": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/<b>machine</b>-<b>learning</b>-design/9781098115777/ch04.html", "snippet": "This ML <b>approximation</b> <b>can</b> be made close <b>enough</b> to the solution of the model that was originally achieved <b>by using</b> more classical methods. The advantage is that inference <b>using</b> the learned ML <b>approximation</b> (which needs to just calculate a closed formula) takes only a fraction of the time required to carry out ray tracing (which would require numerical methods). At the same time, the <b>training</b> dataset is too large (multiple terabytes) and too unwieldy to use as a lookup table in production ...", "dateLastCrawled": "2022-01-30T15:03:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Approximation Theorem</b>. The power of Neural Networks | by ...", "url": "https://medium.com/swlh/universal-approximation-theorem-d1a1a67c1b5b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>universal-approximation-theorem</b>-d1a1a67c1b5b", "snippet": "<b>Universal Approximation Theorem</b>, in its lose form, states that a feed-forward network with a single hidden layer containing a finite number of neurons <b>can</b> <b>approximate</b> <b>any</b> continuous function. Whoa ...", "dateLastCrawled": "2022-01-28T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Neural Networks are Function Approximation</b> Algorithms", "url": "https://machinelearningmastery.com/neural-networks-are-function-approximators/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>neural-networks-are-function</b>-approximators", "snippet": "\u2026 the <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feedforward network with a linear output layer and at least one hidden layer with <b>any</b> \u201csquashing\u201d activation function (such as the logistic sigmoid activation function) <b>can</b> <b>approximate</b> <b>any</b> [\u2026] function from one finite-dimensional space to another with <b>any</b> desired non-zero amount of error, provided that the network is given <b>enough</b> hidden units", "dateLastCrawled": "2022-01-30T03:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Learning nonlinear operators via DeepONet</b> based on the <b>universal</b> ...", "url": "https://www.researchgate.net/publication/350158010_Learning_nonlinear_operators_via_DeepONet_based_on_the_universal_approximation_theorem_of_operators", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350158010_<b>Learning</b>_nonlinear_operators_via...", "snippet": "This <b>universal approximation theorem of operators</b> is suggestive of the structure and potential of deep neural networks (DNNs) in <b>learning</b> continuous operators or complex systems from streams of ...", "dateLastCrawled": "2022-02-03T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is required for neural network <b>to approximate</b> discontinuous ...", "url": "https://stats.stackexchange.com/questions/364917/what-is-required-for-neural-network-to-approximate-discontinuous-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/364917/what-is-required-for-neural-network...", "snippet": "Wikipedia provides a synopsis of the <b>universal</b> <b>approximation</b> <b>theorem</b>. In the mathematical theory of artificial neural networks, the <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feed-forward network with a single hidden layer containing a finite number of neurons <b>can</b> <b>approximate</b> continuous functions on compact subsets of $\\mathbb{R}^n$, under mild assumptions on the activation function.", "dateLastCrawled": "2022-01-27T01:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Recent progress of <b>machine</b> <b>learning</b> in flow modeling and active flow ...", "url": "https://www.sciencedirect.com/science/article/pii/S100093612100306X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S100093612100306X", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> proposed by Hornik et al. 18 shows that if a feedforward neural network has a linear output layer and at least one hidden layer with <b>any</b> kind of \u201csqueezing\u201d activation function (such as logistic or sigmoid), it <b>can</b> <b>approximate</b> <b>any</b> measurable function from one finite-dimensional space to another with <b>arbitrary</b> precision. The most classical model of the neural network in the context of pattern recognition is the feed-forward neural network, also known as ...", "dateLastCrawled": "2022-01-18T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Extreme learning</b> machines: a survey - International Journal of <b>Machine</b> ...", "url": "https://link.springer.com/article/10.1007/s13042-011-0019-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13042-011-0019-y", "snippet": "<b>Universal</b> <b>approximation</b> <b>theorem</b>. Huang et al. ... OS-ELM is a simple and efficient online sequential <b>learning</b> <b>algorithm</b> that <b>can</b> handle both additive and RBF nodes in a unified framework. OS-ELM <b>can</b> <b>learn</b> the <b>training</b> <b>data</b> not only one-by-one but also chunk by chunk (with fixed or varying length) and discard the <b>data</b> for which the <b>training</b> has already been done. The <b>training</b> observations are sequentially presented to the <b>learning</b> <b>algorithm</b> (one-by-one or chunk-by-chunk with varying or fixed ...", "dateLastCrawled": "2022-01-29T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Chapter <b>7 Neural networks</b> | <b>Machine</b> <b>Learning</b> for Factor Investing", "url": "http://www.mlfactor.com/NN.html", "isFamilyFriendly": true, "displayUrl": "www.mlfactor.com/NN.html", "snippet": "The raw results on <b>universal</b> <b>approximation</b> imply that <b>any</b> well-behaved function \\(f\\) <b>can</b> be approached sufficiently closely by a simple neural network, as long as the number of units <b>can</b> be arbitrarily large. Now, they do not directly relate to the <b>learning</b> phase, i.e., when the model is optimized with respect to a particular dataset. In a series of papers", "dateLastCrawled": "2022-01-30T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Deep, Skinny Neural <b>Networks are not Universal Approximators</b> | DeepAI", "url": "https://deepai.org/publication/deep-skinny-neural-networks-are-not-universal-approximators", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/deep-skinny-neural-<b>networks-are-not-universal-approximators</b>", "snippet": "A number of papers [3, 6, 11, 13] have shown that neural networks with a single hidden layer are a <b>universal</b> approximator, i.e. that they <b>can</b> <b>approximate</b> <b>any</b> continuous function on a compact domain to <b>arbitrary</b> accuracy if the hidden layer is allowed to have an arbitrarily high dimension. In practice, however, the neural networks that have proved most effective tend to have a large number of relatively low-dimensional hidden layers.", "dateLastCrawled": "2021-12-29T03:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Artificial Neural Network</b> | Brilliant Math &amp; Science Wiki", "url": "https://brilliant.org/wiki/artificial-neural-network/", "isFamilyFriendly": true, "displayUrl": "https://brilliant.org/wiki/<b>artificial-neural-network</b>", "snippet": "Online <b>learning</b> is especially useful in scenarios where <b>training</b> <b>data</b> is arriving sequentially over time, such as speech <b>data</b> or the movement of stock prices. With a system capable of online <b>learning</b>, one doesn&#39;t have to wait until the system has received a ton of <b>data</b> before it <b>can</b> make a prediction or decision. If the human brain learned by batch <b>learning</b>, then human children would take 10 years before they could <b>learn</b> to speak, mostly just to gather <b>enough</b> speech <b>data</b> and grammatical ...", "dateLastCrawled": "2022-01-29T01:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the difference between a <b>neural network</b> and a deep <b>neural</b> ...", "url": "https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/182734", "snippet": "Although, in theory, one <b>can</b> <b>approximate</b> an <b>arbitrary</b> NN <b>using</b> a shallow NN with only one hidden layer, however, this does not mean that the two networks will perform similarly when trained <b>using</b> the same <b>algorithm</b> and <b>training</b> <b>data</b>. In fact there is a growing interest in <b>training</b> shallow networks that perform similarly to deep networks. The <b>way</b> this is done, however, is by <b>training</b> a deep <b>network</b> first, and then <b>training</b> the shallow <b>network</b> to", "dateLastCrawled": "2022-01-29T02:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Logistic Sigmoid - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/logistic-sigmoid", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/logistic-sigmoid", "snippet": "More specifically, the <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feedforward network with a linear output layer and at least one hidden layer with <b>any</b> squashing activation function (like Sigmoid or tanh) <b>can</b> <b>approximate</b> <b>any</b> Borel measurable function from one finite-dimensional space to another with <b>any</b> desired nonzero amount of error, provided that the network is given <b>enough</b> hidden units. For the purpose of our discussion, it is <b>enough</b> to know <b>any</b> continuous function on a closed and ...", "dateLastCrawled": "2022-01-08T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Artificial Neural Network</b> | Brilliant Math &amp; Science Wiki", "url": "https://brilliant.org/wiki/artificial-neural-network/", "isFamilyFriendly": true, "displayUrl": "https://brilliant.org/wiki/<b>artificial-neural-network</b>", "snippet": "Online <b>learning</b> is especially useful in scenarios where <b>training</b> <b>data</b> is arriving sequentially over time, such as speech <b>data</b> or the movement of stock prices. With a system capable of online <b>learning</b>, one doesn&#39;t have to wait until the system has received a ton of <b>data</b> before it <b>can</b> make a prediction or decision. If the human brain learned by batch <b>learning</b>, then human children would take 10 years before they could <b>learn</b> to speak, mostly just to gather <b>enough</b> speech <b>data</b> and grammatical ...", "dateLastCrawled": "2022-01-29T01:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Neural networks <b>with a continuous squashing function in the</b> output are ...", "url": "https://www.researchgate.net/publication/12333968_Neural_networks_with_a_continuous_squashing_function_in_the_output_are_universal_approximators", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/12333968_Neural_networks_with_a_continuous...", "snippet": "The first version of the <b>theorem</b> [51] proved that a single-layer NN, exclusively <b>using</b> sigmoid AFs, <b>can</b> <b>approximate</b> <b>any</b> continuous function defined on a compact subset of R , if the number of ...", "dateLastCrawled": "2022-01-16T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How a <b>Machine</b> Learns and Fails \u2013 A <b>Grammar of Error for Artificial</b> ...", "url": "https://spheres-journal.org/contribution/how-a-machine-learns-and-fails-a-grammar-of-error-for-artificial-intelligence/", "isFamilyFriendly": true, "displayUrl": "https://spheres-journal.org/contribution/how-a-<b>machine</b>-<b>learns</b>-and-fails-a-grammar-of...", "snippet": "When one says that \u201cneural networks <b>can</b> solve <b>any</b> problem\u201d, it means that they <b>can</b> <b>approximate</b> the shape of <b>any</b> <b>curve</b> (<b>any</b> non-linear function) in a multi-dimensional space of <b>data</b>. 32 Nota bene: In these passages, the dividing line between input and output datapoints has been described as a <b>curve</b>. Actually, <b>machine</b> <b>learning</b> calculates such differential approximations in n-dimensional spaces by drawing, then, hyperplanes (rather than a <b>curve</b> on a two-dimensional matrix).", "dateLastCrawled": "2022-01-31T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>What is supply chain management</b>? | <b>IBM</b>", "url": "https://www.ibm.com/topics/supply-chain-management", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ibm.com</b>/topics", "snippet": "Retail shelves <b>can</b> then be restocked almost as quickly as product is sold. One <b>way</b> to further improve on this process is to analyze the <b>data</b> from supply chain partners to see where further improvements <b>can</b> be made. By analyzing partner <b>data</b>, the CIO.com post identifies three scenarios where effective supply chain management increases value to the supply chain cycle: Identifying potential problems. When a customer orders more product than the manufacturer <b>can</b> deliver, the buyer <b>can</b> complain ...", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Expressive Power of Neural Networks: A View from the Width", "url": "https://www.researchgate.net/publication/338048349_The_Expressive_Power_of_Neural_Networks_A_View_from_the_Width", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338048349_The_Expressive_Power_of_Neural...", "snippet": "The success of deep <b>learning</b> in the computer vision and natural language processing communities <b>can</b> be attributed to <b>training</b> of very deep neural networks with millions or billions of parameters ...", "dateLastCrawled": "2021-12-22T20:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is the difference between a <b>neural network</b> and a deep <b>neural</b> ...", "url": "https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/182734", "snippet": "Although, in theory, one <b>can</b> <b>approximate</b> an <b>arbitrary</b> NN <b>using</b> a shallow NN with only one hidden layer, however, this does not mean that the two networks will perform similarly when trained <b>using</b> the same <b>algorithm</b> and <b>training</b> <b>data</b>. In fact there is a growing interest in <b>training</b> shallow networks that perform similarly to deep networks. The <b>way</b> this is done, however, is by <b>training</b> a deep <b>network</b> first, and then <b>training</b> the shallow <b>network</b> to", "dateLastCrawled": "2022-01-29T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Is there any mathematical proof on why deep</b> <b>learning</b> works? - Quora", "url": "https://www.quora.com/Is-there-any-mathematical-proof-on-why-deep-learning-works", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Is-there-any-mathematical-proof-on-why-deep</b>-<b>learning</b>-works", "snippet": "Answer (1 of 8): I\u2019ve answered a similar question before, but I\u2019ll give it another whirl. As I have explained earlier, it all depends on what you mean by \u201cworks\u201d. Depending on how you define that word, deep <b>learning</b> either \u201cworks\u201d or it doesn&#39;t \u201cwork\u201d. Let\u2019s clarify with some examples. 1. \u201cWork\u201d...", "dateLastCrawled": "2022-01-17T20:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Section 01 - Massey University", "url": "https://www.massey.ac.nz/~mjjohnso/notes/59302/all.html", "isFamilyFriendly": true, "displayUrl": "https://www.massey.ac.nz/~mjjohnso/notes/59302/all.html", "snippet": "<b>machine</b> <b>learning</b>; This test avoids physical contact and concentrates on &quot;higher level&quot; mental faculties. A total Turing test ... We <b>can</b> see that a three layer MLP <b>can</b> <b>learn</b> <b>arbitrary</b> areas while a two layer MLP <b>can</b> <b>learn</b> convex regions. (if you <b>can</b> draw a line from <b>any</b> point in the region to <b>any</b> other in the region and the line passes out of the region then that region is not convex). <b>Training</b> the MLP last time we saw that the delta rule <b>can</b> be used to train a perceptron. When <b>training</b> the ...", "dateLastCrawled": "2022-02-02T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "5.1 MLBasics-<b>Learning</b>.ppt - Pennsylvania State University", "url": "http://clgiles.ist.psu.edu/IST597/materials/slides/lect2/ch5.pptx", "isFamilyFriendly": true, "displayUrl": "clgiles.ist.psu.edu/IST597/materials/slides/lect2/ch5.pptx", "snippet": "Use a single, <b>arbitrary</b> <b>learning</b> <b>algorithm</b> but manipulate <b>training</b> <b>data</b> to make it <b>learn</b> multiple models. Data1 Data2 \u2026 <b>Data</b> m. Learner1 = Learner2 = \u2026 = Learner m. Different methods for changing <b>training</b> <b>data</b>: Bagging: Resample <b>training</b> <b>data</b>. Boosting: Reweight <b>training</b> <b>data</b>. DECORATE: Add additional artificial <b>training</b> <b>data</b>. In WEKA, these are called . meta-learners, they take a <b>learning</b> <b>algorithm</b> as an argument (base learner) and create a new <b>learning</b> <b>algorithm</b>. Bagging. Create ...", "dateLastCrawled": "2022-01-31T17:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Are Deep Neural Networks Dramatically Overfitted?", "url": "https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically-overfitted.html", "isFamilyFriendly": true, "displayUrl": "https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically...", "snippet": "The <b>Universal</b> <b>Approximation</b> <b>Theorem</b> states that a feedforward network with: 1) a linear output layer, 2) at least one hidden layer containing a finite number of neurons and 3) some activation function <b>can</b> <b>approximate</b> <b>any</b> continuous functions on a compact subset of \\(\\mathbb{R}^n\\) to <b>arbitrary</b> accuracy. The <b>theorem</b> was first proved for sigmoid ...", "dateLastCrawled": "2022-02-01T06:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Logistic Sigmoid - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/logistic-sigmoid", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/logistic-sigmoid", "snippet": "More specifically, the <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feedforward network with a linear output layer and at least one hidden layer with <b>any</b> squashing activation function (like Sigmoid or tanh) <b>can</b> <b>approximate</b> <b>any</b> Borel measurable function from one finite-dimensional space to another with <b>any</b> desired nonzero amount of error, provided that the network is given <b>enough</b> hidden units. For the purpose of our discussion, it is <b>enough</b> to know <b>any</b> continuous function on a closed and ...", "dateLastCrawled": "2022-01-08T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep learning theory</b> lecture notes", "url": "https://mjt.cs.illinois.edu/dlt/", "isFamilyFriendly": true, "displayUrl": "https://mjt.cs.illinois.edu/dlt", "snippet": "2.2 <b>Universal</b> <b>approximation</b> with a single hidden layer. The proof of <b>Theorem</b> 2.1 use two layers to construct g_\\gamma such that g_\\gamma(x) \\approx \\mathbf{1}\\left[{ x\\in \\times_i [a_i,b_i] }\\right].If instead we had a <b>way</b> <b>to approximate</b> multiplication we could instead <b>approximate</b> x \\mapsto \\prod_i \\mathbf{1}\\left[{ x_i \\in [a_i,b_i] }\\right] = \\mathbf{1}\\left[{ x\\in \\times_i [a_i,b_i] }\\right]. <b>Can</b> we do this and then form a linear combination, all with just one hidden layer?", "dateLastCrawled": "2022-02-03T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Extreme learning</b> machines: a survey - International Journal of <b>Machine</b> ...", "url": "https://link.springer.com/article/10.1007/s13042-011-0019-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13042-011-0019-y", "snippet": "<b>Universal</b> <b>approximation</b> <b>theorem</b>. Huang et al. ... OS-ELM <b>can</b> <b>learn</b> the <b>training</b> <b>data</b> not only one-by-one but also chunk by chunk (with fixed or varying length) and discard the <b>data</b> for which the <b>training</b> has already been done. The <b>training</b> observations are sequentially presented to the <b>learning</b> <b>algorithm</b> (one-by-one or chunk-by-chunk with varying or fixed chunk length). A single or a chunk of <b>training</b> observations is discarded and may not be used <b>any</b> more as soon as the <b>learning</b> procedure ...", "dateLastCrawled": "2022-01-29T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Chapter <b>7 Neural networks</b> | <b>Machine</b> <b>Learning</b> for Factor Investing", "url": "http://www.mlfactor.com/NN.html", "isFamilyFriendly": true, "displayUrl": "www.mlfactor.com/NN.html", "snippet": "The raw results on <b>universal</b> <b>approximation</b> imply that <b>any</b> well-behaved function \\(f\\) <b>can</b> be approached sufficiently closely by a simple neural network, as long as the number of units <b>can</b> be arbitrarily large. Now, they do not directly relate to the <b>learning</b> phase, i.e., when the model is optimized with respect to a particular dataset. In a series of papers", "dateLastCrawled": "2022-01-30T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Studying the micromechanical behaviors of a polycrystalline</b> metal by ...", "url": "https://www.sciencedirect.com/science/article/pii/S1359645421003864", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1359645421003864", "snippet": "In the mathematical theory of ANNs, the <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feed-forward network with a single hidden layer containing a finite number of neurons <b>can</b> <b>approximate</b> continuous functions under mild assumptions on the activation function . Usually, a maximum of three hidden layers is sufficient to solve most of the problems. ANNs were extensively used for solving various material science problems, such as predicting material properties, optimizing processing parameters, etc.", "dateLastCrawled": "2022-01-30T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Neural Networks on Steroids", "url": "https://www.slideshare.net/AdamBlevins1/neural-networks-on-steroids", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/AdamBlevins1/neural-networks-on-steroids", "snippet": "With the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> in mind we <b>can</b> conclude that if a single hidden layer MLP fails to <b>learn</b> a mapping under the de\ufb01ned constraints, it is not down to the architecture of the network but the parameters that de\ufb01ne it. For example, this could be poorly initialised weights, it could be the <b>learning</b> rate or it could even be down to an insu\ufb03cient number of hidden nodes to suitably <b>approximate</b> the function with too few degrees of freedom to produce a complex <b>enough</b> ...", "dateLastCrawled": "2022-01-23T15:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What does it mean by the statement, &#39;neural networks are <b>universal</b> ...", "url": "https://www.quora.com/What-does-it-mean-by-the-statement-neural-networks-are-universal-approximators-in-a-mathematical-intuitive-sense", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-does-it-mean-by-the-statement-neural-networks-are-<b>universal</b>...", "snippet": "Answer (1 of 4): Consider the set of all continuous functions which are defined on the unit hypercube (i.e. the unit square in two dimensions, the unit cube in three dimensions, etc.). Call this set C. Given two functions in C, it is possible to define a metric which calculates a notion of dist...", "dateLastCrawled": "2022-01-06T20:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the difference between a <b>neural network</b> and a deep <b>neural</b> ...", "url": "https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/182734", "snippet": "Although, in theory, one <b>can</b> <b>approximate</b> an <b>arbitrary</b> NN <b>using</b> a shallow NN with only one hidden layer, however, this does not mean that the two networks will perform similarly when trained <b>using</b> the same <b>algorithm</b> and <b>training</b> <b>data</b>. In fact there is a growing interest in <b>training</b> shallow networks that perform similarly to deep networks. The <b>way</b> this is done, however, is by <b>training</b> a deep <b>network</b> first, and then <b>training</b> the shallow <b>network</b> to", "dateLastCrawled": "2022-01-29T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>can</b> we use <b>artificial neural networks to approximate</b> a piecewise ...", "url": "https://www.quora.com/How-can-we-use-artificial-neural-networks-to-approximate-a-piecewise-continuous-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-we-use-<b>artificial-neural-networks-to-approximate</b>-a...", "snippet": "Answer (1 of 4): In short, yes you <b>can</b> (almost uniformly) <b>approximate</b> a piece-wise continuous function <b>using</b> a neural network with single hidden layer\u2014as long as you know (or <b>can</b> figure out) the positions of the discontinuities and the corresponding \u201cjumps\u201d of the discontinuous function. A const...", "dateLastCrawled": "2022-01-13T03:19:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Approximation Theorem</b>. The power of Neural Networks | by ...", "url": "https://medium.com/swlh/universal-approximation-theorem-d1a1a67c1b5b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>universal-approximation-theorem</b>-d1a1a67c1b5b", "snippet": "<b>Universal Approximation Theorem</b>, in its lose form, states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate any continuous function. Whoa ...", "dateLastCrawled": "2022-01-28T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal Approximation Theorem</b>, Neural Nets &amp; Lego Blocks | by ...", "url": "https://medium.com/analytics-vidhya/universal-approximation-theorem-neural-nets-lego-blocks-1f5a7d93542a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>universal-approximation-theorem</b>-neural-nets-lego...", "snippet": "In this post, we will look at the <b>Universal Approximation Theorem</b> \u2014 one of the fundamental theorems on which the entire concept of Deep <b>Learning</b> is based upon. We will make use of lego blocks ...", "dateLastCrawled": "2022-01-28T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> Complex Functions using <b>Universal</b> Approximate <b>Theorem</b> - Ai Nxt", "url": "https://ainxt.co.in/learning-complex-functions-using-universal-approximate-theorem/", "isFamilyFriendly": true, "displayUrl": "https://ainxt.co.in/<b>learning</b>-complex-functions-using-<b>universal</b>-approximate-<b>theorem</b>", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b>. No matter how complex our output logic is, we can use collection of neurons and form Dense Neural Network to approximate our function. This is known as \u201c<b>UNIVERSAL</b> <b>APPROXIMATION</b> <b>THEOREM</b>\u201c. Lets take an example of Two Dimensional data where y = f(x) i.e. y is some function of x. Now, we need to find that ...", "dateLastCrawled": "2022-01-21T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Universal Approximation Theorems</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336361517_Universal_Approximation_Theorems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336361517_<b>Universal_Approximation_Theorems</b>", "snippet": "In the <b>machine</b> <b>learning</b> literature, <b>universal</b> <b>approximation</b> refers to a model class\u2019 ability. to generically approximate any member of a large topological space whose elements are. functions, or ...", "dateLastCrawled": "2022-01-25T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Illustrative Proof of <b>Universal Approximation Theorem</b> | HackerNoon", "url": "https://hackernoon.com/illustrative-proof-of-universal-approximation-theorem-5845c02822f6", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/illustrative-proof-of-<b>universal-approximation-theorem</b>-5845c02822f6", "snippet": "We will talk about the <b>Universal approximation theorem</b> and we will also prove the <b>theorem</b> graphically. The most commonly used sigmoid function is the logistic function, which has a characteristic of an \u201cS\u201d shaped curve. In real life, we deal with complex functions where the relationship between input and output might be complex. To solve this problem, let&#39;s take an <b>analogy</b> of building a house. The way we are going to create complex functions is that we will combine the sigmoids neurons ...", "dateLastCrawled": "2022-02-01T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "ne.neural evol - <b>Universal Approximation Theorem</b> \u2014 Neural Networks ...", "url": "https://cstheory.stackexchange.com/questions/17545/universal-approximation-theorem-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/17545", "snippet": "<b>Universal approximation theorem</b> states that &quot;the standard multilayer feed-forward network with a single hidden layer, ... There is an advanced result, key to <b>machine</b> <b>learning</b>, known as Kolmogorov&#39;s <b>theorem</b> [1]; I have never seen an intuitive sketch of why it works. This may have to do with the different cultures that approach it. The applied <b>learning</b> crowd regards Kolmogorov&#39;s <b>theorem</b> as an existence <b>theorem</b> that merely indicates that NNs may exist, so at least the structure is not overly ...", "dateLastCrawled": "2022-02-03T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "neural networks - <b>Universal Approximation Theorem and high dimension</b> ...", "url": "https://stats.stackexchange.com/questions/298622/universal-approximation-theorem-and-high-dimension-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/298622/<b>universal</b>-<b>approximation</b>-<b>theorem</b>-and...", "snippet": "Cross Validated is a question and answer site for people interested in statistics, <b>machine</b> <b>learning</b>, data analysis, data mining, and data visualization. It only takes a minute to sign up. Sign up to join this community", "dateLastCrawled": "2022-01-17T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Neural Networks and Learning Machines</b> - etsmtl.ca", "url": "https://cours.etsmtl.ca/sys843/REFS/Books/ebook_Haykin09.pdf", "isFamilyFriendly": true, "displayUrl": "https://cours.etsmtl.ca/sys843/REFS/Books/ebook_Haykin09.pdf", "snippet": "15.3 <b>Universal</b> <b>Approximation</b> <b>Theorem</b> 797 15.4 Controllability and Observability 799 15.5 Computational Power of Recurrent Networks 804 15.6 <b>Learning</b> Algorithms 806 15.7 Back Propagation Through Time 808 15.8 Real-Time Recurrent <b>Learning</b> 812 15.9 Vanishing Gradients in Recurrent Networks 818", "dateLastCrawled": "2022-01-31T06:23:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(universal approximation theorem)  is like +(the way that a machine learning algorithm can learn to approximate any arbitrary curve by using enough training data)", "+(universal approximation theorem) is similar to +(the way that a machine learning algorithm can learn to approximate any arbitrary curve by using enough training data)", "+(universal approximation theorem) can be thought of as +(the way that a machine learning algorithm can learn to approximate any arbitrary curve by using enough training data)", "+(universal approximation theorem) can be compared to +(the way that a machine learning algorithm can learn to approximate any arbitrary curve by using enough training data)", "machine learning +(universal approximation theorem AND analogy)", "machine learning +(\"universal approximation theorem is like\")", "machine learning +(\"universal approximation theorem is similar\")", "machine learning +(\"just as universal approximation theorem\")", "machine learning +(\"universal approximation theorem can be thought of as\")", "machine learning +(\"universal approximation theorem can be compared to\")"]}