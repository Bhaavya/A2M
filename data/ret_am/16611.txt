{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Neural (Pre-Trained) Language Models</b>", "url": "https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2020/lectures/15-neurallm.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2020/lectures/15-neurallm.pdf", "snippet": "GPT/<b>language</b> <b>model</b> task is <b>unidirectional</b>. ... Issue with <b>Language</b> modelling - <b>Unidirectional</b> Cannot <b>train</b> <b>model</b> on bidirectional context - required for many end tasks Solution 2: Masked <b>Language</b> Modelling Randomly mask a word in the sentence <b>Train</b> the <b>model</b> to predict it *Image Credits: [1] BERT vs. OpenAI-GPT vs. ELMo <b>Unidirectional</b> De-coupled Bidirectionality Bidirectional. Word-Piece tokenizer Middle ground between character level and word level representations tweeting \u2192 tweet + ##ing ...", "dateLastCrawled": "2021-08-28T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding Masked <b>Language</b> Models (MLM) and Causal <b>Language</b> Models ...", "url": "https://towardsdatascience.com/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-masked-<b>language</b>-<b>models</b>-mlm-and-causal...", "snippet": "Masked <b>Language</b> <b>Model</b> Explained. Under Masked <b>Lan g uage</b> Modelling, we typically mask a certain % of words in a given sentence and the <b>model</b> is expected to predict those masked words based on other words in that sentence. Such a training scheme makes this <b>model</b> bidirectional in nature because the representation of the masked word is learnt based on the words that occur it\u2019s left as well as right.You can also visualize this <b>like</b> a fill-in-the-blanks kind of a problem statement.. Below ...", "dateLastCrawled": "2022-02-02T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>NLP Language Models BERT, GPT2</b>/3, T-NLG: <b>Changing the rules of the</b> game ...", "url": "https://medium.com/analytics-vidhya/nlp-language-models-bert-gpt2-t-nlg-changing-the-rules-of-the-game-3334b23020a9", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/analytics-vidhya/<b>nlp-language-models-bert-gpt2</b>-t-nlg-changing-the...", "snippet": "This pretrained <b>model</b> are mainly used content over internet, Wikipedia, Reddit and its basically developed to do content writing or generating new text. Its <b>unidirectional</b> <b>language</b> <b>model</b>. Unlike ...", "dateLastCrawled": "2022-01-30T00:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Language</b> Models as Knowledge Bases?", "url": "https://aclanthology.org/D19-1250.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/D19-1250.pdf", "snippet": "2.1 <b>Unidirectional</b> <b>Language</b> Models Given an input sequence of tokens w = [w 1;w 2;:::;w N], <b>unidirectional</b> <b>language</b> models commonly assign a probability p(w) to the se-quence by factorizing it as follows p(w) = Y t p(w t jw t1;:::;w 1): (1) A common way to estimate this probability is us-ing neural <b>language</b> models (Mikolov and Zweig, 2012;Melis et al.,2017;Bengio et al.,2003) with p(w t jw t1;:::;w 1) = softmax(Wh t + b) (2) where h t 2Rk is the output vector of a neural net-work at position ...", "dateLastCrawled": "2022-01-31T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "50+ <b>NLP Interview Questions and Answers</b> in 2022", "url": "https://www.mygreatlearning.com/blog/nlp-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/nlp-interview-questions", "snippet": "Trains two independent LSTM <b>language</b> <b>model</b> left to right and right to left and shallowly concatenates them a. GPT b. BERT c. ULMFit d. ELMo Ans: d) ELMo tries to <b>train</b> two independent LSTM <b>language</b> models (left to right and right to left) and concatenates the results to produce word embedding. 44. Uses <b>unidirectional</b> <b>language</b> <b>model</b> for producing word embedding a. BERT b. GPT c. ELMo d. Word2Vec Ans: b) GPT is a idirectional <b>model</b> and word embedding are produced by training on information ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "SimpleTOD: A Simple <b>Language</b> <b>Model</b> for Task-Oriented Dialogue - <b>GitHub</b>", "url": "https://github.com/salesforce/simpletod", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/salesforce/simpletod", "snippet": "We propose recasting task-oriented dialogue as a simple, causal (<b>unidirectional</b>) <b>language</b> modeling task. We show that such an approach can solve all the sub-tasks in a unified way using multi-task maximum likelihood training. The proposed Simple Task-Oriented Dialogue (SimpleTOD) approach enables modeling of the inherent dependencies between the sub-tasks of task-oriented dialogue, by optimizing for all tasks in an end-to-end manner.", "dateLastCrawled": "2022-01-31T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Intuitive Explanation of BERT- <b>Bidirectional</b> Transformers for NLP | by ...", "url": "https://towardsdatascience.com/intuitive-explanation-of-bert-bidirectional-transformers-for-nlp-cdc1efc69c1e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/intuitive-explanation-of-bert-<b>bidirectional</b>...", "snippet": "Masked <b>Language</b> <b>Model</b>(MLM) <b>Bidirectional</b> conditioning in BERT allows each word to indirectly \u201csee itself.\u201d To <b>train</b> a deep <b>bidirectional</b> representation, we use MLM to mask 15% of the input tokens at random and then predict those masked tokens.", "dateLastCrawled": "2022-01-30T07:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Simple <b>Language</b> <b>Model</b> for Task-Oriented Dialogue", "url": "https://proceedings.neurips.cc/paper/2020/file/e946209592563be0f01c844ab2170f0c-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/e946209592563be0f01c844ab2170f0c-Paper.pdf", "snippet": "We propose recasting task-oriented dialogue as a simple, causal (<b>unidirectional</b>) <b>language</b> modeling task. We show that such an approach can solve all the sub-tasks in a uni\ufb01ed way using multi-task 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. Figure 1: SimpleTOD is a simple approach to task-oriented dialogue that uses a single causal <b>language</b> <b>model</b> to generate all outputs given the dialogue context and retrieved database search results. The ...", "dateLastCrawled": "2022-01-21T21:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "GPT-2: How to Build &quot;The AI That&#39;s Too Dangerous to Release\u201d", "url": "https://blog.floydhub.com/gpt2/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/gpt2", "snippet": "In other words, the <b>model</b> was thrown a whole lot of raw text data and asked to figure out the statistical features of the text to create more text. \u201cPretrained\u201d means OpenAI created a large and powerful <b>language</b> <b>model</b>, which they fine-tuned for specific tasks <b>like</b> machine translation later on. This is kind of <b>like</b> transfer learning with ...", "dateLastCrawled": "2022-01-31T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How Transformer is Bidirectional - Machine Learning</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/55158554/how-transformer-is-bidirectional-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/55158554", "snippet": "The use of BERT as a <b>language</b> <b>model</b> is realy a transfer learning As stated in your post, <b>unidirectional</b> can be done with a certain type of mask, bidirec is better. And it is used because the go from a full sentence to a full sentence but not the way classic Seq2seq is made (with LSTM and RNN) and as such can be used for LM.", "dateLastCrawled": "2022-01-13T22:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On the comparability of pre-trained <b>language</b> models", "url": "http://ceur-ws.org/Vol-2624/paper2.pdf", "isFamilyFriendly": true, "displayUrl": "ceur-ws.org/Vol-2624/paper2.pdf", "snippet": "It is a <b>unidirectional</b> <b>language</b> <b>model</b> and it al-lows stacking task speci\ufb01c layers on top after pre-training, i.e. it is fully end-to-end trainable. The major difference between them is the internal ar-chitecture, where GPT uses a Transformer decoder architecture (Vaswani et al.,2017). Instead of processing one input token at a time, like recurrent architectures (LSTMs, GRUs) do, Trans-formers process whole sequences all at once. This is possible because they utilize a variant of the At ...", "dateLastCrawled": "2022-01-02T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Enabling <b>Language</b> Models to Fill in the Blanks", "url": "https://aclanthology.org/2020.acl-main.225.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.225.pdf", "snippet": "We then <b>train</b> <b>unidirectional</b> <b>language</b> mod-els on the concatenation of each pair. Once trained, a <b>model</b> takes text input with blanks, predicts the an-swers, and then combines them to produce the output. ing remarkably coherent text (Zellers et al.,2019; See et al.,2019), (2) ef\ufb01cient at generating text, and (3) conceptually simple, but cannot in\ufb01ll ef-fectively as they can only leverage context in a single direction (usually the past). On the other hand, strategies such as BERT (Devlin et ...", "dateLastCrawled": "2022-01-29T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>NLP Language Models BERT, GPT2</b>/3, T-NLG: <b>Changing the rules of the</b> game ...", "url": "https://medium.com/analytics-vidhya/nlp-language-models-bert-gpt2-t-nlg-changing-the-rules-of-the-game-3334b23020a9", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/analytics-vidhya/<b>nlp-language-models-bert-gpt2</b>-t-nlg-changing-the...", "snippet": "This pretrained <b>model</b> are mainly used content over internet, Wikipedia, Reddit and its basically developed to do content writing or generating new text. Its <b>unidirectional</b> <b>language</b> <b>model</b>. Unlike ...", "dateLastCrawled": "2022-01-30T00:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Transformer-based <b>Language</b> Models | The Ezra Tech Blog", "url": "https://techblog.ezra.com/an-overview-of-different-transformer-based-language-models-c9d3adafead8", "isFamilyFriendly": true, "displayUrl": "https://techblog.ezra.com/an-<b>overview-of-different-transformer-based-language-models</b>-c...", "snippet": "One limitation of this <b>model</b>, however, is its <b>unidirectional</b> nature because the <b>model</b> was only trained to predict the next word from the current word and not the other way around. Bidirectional Encoder Representations from Transformers (BERT) Google AI introduced an encoder-based <b>language</b> <b>model</b> which unlike GPT is trained in both directions. Two versions of this <b>model</b> are investigated in the paper, BERT_BASE which is the size of GPT, and a larger <b>model</b> BERT_LARGE with 340M parameters and 24 ...", "dateLastCrawled": "2022-01-27T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Neural (Pre-Trained) Language Models</b>", "url": "https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2020/lectures/15-neurallm.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2020/lectures/15-neurallm.pdf", "snippet": "<b>Similar</b> to pretraining ResNet on ImageNet and finetune on specific tasks Pretrained using <b>Language</b> modelling task Finetuned on End-Task (such as Sentiment Analysis) Uses the same architecture for both pretraining and finetuning ELMo is added as additional component to existing task-specific architectures Universal <b>Language</b> <b>Model</b> Fine-tuning for Text Classification. <b>Model</b> 2: Generative Pre-Training (Transformers) 73 GPT - Uses Transformer decoder instead of LSTM for <b>Language</b> Modeling GPT-2 ...", "dateLastCrawled": "2021-08-28T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "50+ <b>NLP Interview Questions and Answers</b> in 2022", "url": "https://www.mygreatlearning.com/blog/nlp-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/nlp-interview-questions", "snippet": "Trains two independent LSTM <b>language</b> <b>model</b> left to right and right to left and shallowly concatenates them a. GPT b. BERT c. ULMFit d. ELMo Ans: d) ELMo tries to <b>train</b> two independent LSTM <b>language</b> models (left to right and right to left) and concatenates the results to produce word embedding. 44. Uses <b>unidirectional</b> <b>language</b> <b>model</b> for producing word embedding a. BERT b. GPT c. ELMo d. Word2Vec Ans: b) GPT is a idirectional <b>model</b> and word embedding are produced by training on information ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "Specifically, we <b>train</b> GPT-3, an autoregressive <b>language</b> <b>model</b> with 175 billion parameters, 10\u00d7 more than any previous non-sparse <b>language</b> <b>model</b>, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the <b>model</b>. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Word Acquisition in Neural <b>Language</b> Models | Transactions of the ...", "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00444/109271/Word-Acquisition-in-Neural-Language-Models", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/.../tacl_a_00444/109271/Word-Acquisition-in-Neural-<b>Language</b>-<b>Models</b>", "snippet": "A quadratic <b>model</b> of log-frequency also provided a slightly better fit for <b>unidirectional</b> <b>language</b> models (R 2 = 0.93 to 0.94), particularly for high-frequency words; in <b>language</b> models, this could be due either to a floor effect on age of acquisition for high-frequency words or to slower learning of function words. Regardless, significant effects of other predictors remained the same when using a quadratic <b>model</b> for log-frequency.", "dateLastCrawled": "2022-02-02T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "GPT-2: How to Build &quot;The AI That&#39;s Too Dangerous to Release\u201d", "url": "https://blog.floydhub.com/gpt2/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/gpt2", "snippet": "GPT was great. But not for long. Another <b>similar</b> approach \u2014 BERT was introduced by the google <b>language</b> team right after GPT came out, and like a kid in a candy store, the NLP folks sent GPT to the grave. RIP. But not for long. OpenAI quickly bounced back with a revolutionary idea \u2014 doing absolutely nothing at all. You see, what made BERT so great was that it used what\u2019s called a bidirectional <b>language</b> <b>model</b>, as opposed to GPT\u2019s <b>unidirectional</b> <b>language</b> <b>model</b>. I\u2019m not going to ...", "dateLastCrawled": "2022-01-31T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to Use <b>Transformer</b>-based NLP Models | Towards Data Science", "url": "https://towardsdatascience.com/how-to-use-transformer-based-nlp-models-a42adbc292e5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-use-<b>transformer</b>-based-nlp-<b>models</b>-a42adbc292e5", "snippet": "<b>Similar</b> to multi-task learning, no fine-tuning is necessary for meta-learning models. ... They also walk you through more general examples of how to <b>train</b> and fine-tune a <b>model</b>. With the pipeline objects, you only need to write a few lines of code to do inference for a variety of different tasks. These pipelines leverage already fine-tuned models, and the documentation explains which models work with which task-specific pipeline. Hugging Face even created a zero-shot classification pipeline ...", "dateLastCrawled": "2022-01-30T13:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "GPT-2: How to Build &quot;The AI That&#39;s Too Dangerous to Release\u201d", "url": "https://blog.floydhub.com/gpt2/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/gpt2", "snippet": "Here\u2019s where we\u2019re at \u2014 we have a really good <b>language</b> <b>model</b> that (hopefully) has learned dynamics of the English <b>language</b> after being trained on a vast text corpus. Now, in theory, if we stick a task-specific layer or two on top of the <b>language</b> <b>model</b>, we should **get something that leverages the <b>language</b> <b>model</b>\u2019s linguistic capabilities while also adapting to the task at hand.", "dateLastCrawled": "2022-01-31T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal Language Model Fine-tuning for Text Classification</b>", "url": "https://www.researchgate.net/publication/334116365_Universal_Language_Model_Fine-tuning_for_Text_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334116365_Universal_<b>Language</b>_<b>Model</b>_Fine...", "snippet": "<b>Language</b> adaptive fine-tuning (LAFT) is an effective method of adapting PLMs to a new <b>language</b> by finetuning PLMs MLM on unlabeled texts in the new <b>language</b> (Pfeiffer et al., 2020). The approach ...", "dateLastCrawled": "2022-02-02T23:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "KoreALBERT: Pretraining a Lite BERT <b>Model for Korean Language Understanding</b>", "url": "https://www.researchgate.net/publication/348832468_KoreALBERT_Pretraining_a_Lite_BERT_Model_for_Korean_Language_Understanding/fulltext/60122a5a45851517ef1ea6ec/KoreALBERT-Pretraining-a-Lite-BERT-Model-for-Korean-Language-Understanding.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348832468_KoreALBERT_Pre<b>train</b>ing_a_Lite_BERT...", "snippet": "predominantly <b>unidirectional</b> training of a <b>language</b> <b>model</b> by using the masked <b>language</b> <b>model</b> (MLM) training objective. MLM is an old concept dating back to the 1950s [4]. By jointly conditioning ...", "dateLastCrawled": "2021-12-21T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Everything Product People Need to Know About Transformers (Part 3: BERT ...", "url": "https://towardsdatascience.com/everything-product-people-need-to-know-about-transformers-part-3-bert-a1227cead488", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/everything-product-people-need-to-know-about...", "snippet": "BERT is first trained as a masked <b>language</b> <b>model</b> (MLM). MLM entails passing BERT a sentence like \u201cI sat [MASK] my chair\u201d and requiring BERT to predict the masked word. Next-word prediction <b>language</b> modeling <b>can</b> be considered a special case of MLM, where the last word in the sentence is always the masked word. Hence, MLM <b>can</b> <b>be thought</b> of as a more generalized form of <b>language</b> modeling than the task employed to <b>train</b> GPT.", "dateLastCrawled": "2022-02-02T06:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Transformer-based <b>Language</b> Models | The Ezra Tech Blog", "url": "https://techblog.ezra.com/an-overview-of-different-transformer-based-language-models-c9d3adafead8", "isFamilyFriendly": true, "displayUrl": "https://techblog.ezra.com/an-<b>overview-of-different-transformer-based-language-models</b>-c...", "snippet": "Skip-<b>thought</b> task; This task uses the USE embeddings of a central sentence in the training set to predict the previous and next sentences from it. Conversational input-response task ; For this question-answering task, data from the web consisting of question and answer pairs are used to predict the response to a given question. Natural <b>Language</b> Inference (NLI) This task allows for the <b>model</b> to detect the relationship between two input sentences by classifying sentence pairs into three ...", "dateLastCrawled": "2022-01-27T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "Specifically, we <b>train</b> GPT-3, an autoregressive <b>language</b> <b>model</b> with 175 billion parameters, 10\u00d7 more than any previous non-sparse <b>language</b> <b>model</b>, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the <b>model</b>. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Medical Report Generation Using Deep Learning</b> | by Vinithavn ...", "url": "https://medium.com/analytics-vidhya/medical-report-generation-using-deep-learning-87b50096ead0", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/analytics-vidhya/<b>medical-report-generation-using-deep-learning</b>-87b...", "snippet": "The <b>model</b> understands the contents of the image and generates the corresponding textual description. The image captioning process, therefore, <b>can</b> <b>be thought</b> of as a combination of two types of models-", "dateLastCrawled": "2022-02-02T21:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "BERT: Pre-training of Deep Bidirectional Transformers for <b>Language</b> ...", "url": "https://computational-linguistics-class.org/slides/old/90-guest_lecture_jacob_devlin_bert_presentations.pdf", "isFamilyFriendly": true, "displayUrl": "https://computational-linguistics-class.org/slides/old/90-guest_lecture_jacob_devlin...", "snippet": "Translate <b>Train</b>: MT English <b>Train</b> into Foreign, then fine-tune. Translate Test: MT Foreign Test into English, use English <b>model</b>. Zero Shot: Use Foreign test on English <b>model</b>. System English Chinese Spanish XNLI Baseline - Translate <b>Train</b> 73.7 67.0 68.8 XNLI Baseline - Translate Test 73.7 68.4 70.7 BERT - Translate <b>Train</b> 81.9 76.6 77.8", "dateLastCrawled": "2022-02-01T22:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "machine learning - Training NLP with multiple text input features ...", "url": "https://datascience.stackexchange.com/questions/46420/training-nlp-with-multiple-text-input-features", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/46420/<b>train</b>ing-nlp-with-multiple-text...", "snippet": "How <b>can</b> I <b>train</b> a NLP <b>model</b> with discrete labels that is based on multiple text input features? Background: I&#39;m trying to predict the difficulty of a 4-option multiple choice exam question (probability of a test-taker selecting the correct response) based on the text of the question along with its possible responses. I&#39;m hoping to be able to take into account how some incorrect yet convincing responses, the exact subject of which is relative to the content of the question, <b>can</b> skew the ...", "dateLastCrawled": "2022-01-29T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Is BERT a revolutionary approach for the NLP</b>? - Quora", "url": "https://www.quora.com/Is-BERT-a-revolutionary-approach-for-the-NLP", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Is-BERT-a-revolutionary-approach-for-the-NLP</b>", "snippet": "Answer (1 of 3): BERT brought transfer learning to NLP world. You <b>can</b> pre-<b>train</b> a general purpose <b>model</b> on a large body of <b>language</b> corpus and then fine tune that to build task specific models (like Named entity, sequence classification, etc.). You <b>can</b> achieve stunning results with very little tr...", "dateLastCrawled": "2022-01-14T16:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sentence Representation</b> | SpringerLink", "url": "https://link.springer.com/chapter/10.1007%2F978-981-15-5573-2_4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-981-15-5573-2_4", "snippet": "<b>Compared</b> with the standard <b>unidirectional</b> conditional <b>language</b> <b>model</b>, which <b>can</b> only be trained in one direction, MLM aims to <b>train</b> a deep bidirectional representation <b>model</b>. This task is inspired by Cloze . (2) The objective of NSP is to capture relationships between sentences for some sentence-based downstream tasks such as natural <b>language</b> inference (NLI) and question answering (QA). In this task, a binary classifier is trained to predict whether the sentence is the next sentence for the ...", "dateLastCrawled": "2021-12-21T11:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "On the comparability of pre-trained <b>language</b> models", "url": "http://ceur-ws.org/Vol-2624/paper2.pdf", "isFamilyFriendly": true, "displayUrl": "ceur-ws.org/Vol-2624/paper2.pdf", "snippet": "ing approach <b>compared</b> to ELMo, as the ELMo-embeddings are extracted from the pre-trained <b>model</b> and are not \ufb01ne-tuned in conjunction with the weights of the task-speci\ufb01c architecture. The OpenAI GPT (Generative Pre-Training,Rad-ford et al.,2018) is a <b>model</b> which resembles the characteristics of ULMFiT in two crucial points. It is a <b>unidirectional</b> <b>language</b> <b>model</b> and it al-lows stacking task speci\ufb01c layers on top after pre-training, i.e. it is fully end-to-end trainable. The major ...", "dateLastCrawled": "2022-01-02T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Enabling <b>Language</b> Models to Fill in the Blanks", "url": "https://aclanthology.org/2020.acl-main.225.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.225.pdf", "snippet": "We then <b>train</b> <b>unidirectional</b> <b>language</b> mod-els on the concatenation of each pair. Once trained, a <b>model</b> takes text input with blanks, predicts the an-swers, and then combines them to produce the output. ing remarkably coherent text (Zellers et al.,2019; See et al.,2019), (2) ef\ufb01cient at generating text, and (3) conceptually simple, but cannot in\ufb01ll ef-fectively as they <b>can</b> only leverage context in a single direction (usually the past). On the other hand, strategies such as BERT (Devlin et ...", "dateLastCrawled": "2022-01-29T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Comparison between <b>BERT</b>, GPT-2 and ELMo | by Gaurav Ghati | Medium", "url": "https://medium.com/@gauravghati/comparison-between-bert-gpt-2-and-elmo-9ad140cd1cda", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@gauravghati/comparison-between-<b>bert</b>-gpt-2-and-elmo-9ad140cd1cda", "snippet": "<b>Compared</b> to GPT, the largest ... (NSP) to <b>train</b>. Task 1: Masked <b>Language</b> <b>Model</b> (MLM) Learning the context around a word rather than learning just after the word makes it able to better capture its ...", "dateLastCrawled": "2022-02-02T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "50+ <b>NLP Interview Questions and Answers</b> in 2022", "url": "https://www.mygreatlearning.com/blog/nlp-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/nlp-interview-questions", "snippet": "NLTK supports a wider range of languages <b>compared</b> to Spacey (Spacey supports only 7 languages) ... ELMo tries to <b>train</b> two independent LSTM <b>language</b> models (left to right and right to left) and concatenates the results to produce word embedding. 44. Uses <b>unidirectional</b> <b>language</b> <b>model</b> for producing word embedding a. BERT b. GPT c. ELMo d. Word2Vec Ans: b) GPT is a idirectional <b>model</b> and word embedding are produced by training on information flow from left to right. ELMo is bidirectional but ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "Specifically, we <b>train</b> GPT-3, an autoregressive <b>language</b> <b>model</b> with 175 billion parameters, 10\u00d7 more than any previous non-sparse <b>language</b> <b>model</b>, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the <b>model</b>. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Language</b> Models as Knowledge Bases? - ACL Member Portal", "url": "https://aclanthology.org/D19-1250.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/D19-1250.pdf", "snippet": "<b>compared</b> to 63.5% of a knowledge base con-structed using a task-speci\ufb01c supervised relation extraction system. 2 Background In this section we provide background on <b>language</b> models. Statistics for the models that we include in our investigation are summarized in Table1. 2.1 <b>Unidirectional</b> <b>Language</b> Models Given an input sequence of tokens w = [w 1;w 2;:::;w N], <b>unidirectional</b> <b>language</b> models commonly assign a probability p(w) to the se-quence by factorizing it as follows p(w) = Y t p(w t jw ...", "dateLastCrawled": "2022-01-31T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "SimpleTOD: A Simple <b>Language</b> <b>Model</b> for Task-Oriented Dialogue - <b>GitHub</b>", "url": "https://github.com/salesforce/simpletod", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/salesforce/simpletod", "snippet": "We propose recasting task-oriented dialogue as a simple, causal (<b>unidirectional</b>) <b>language</b> modeling task. We show that such an approach <b>can</b> solve all the sub-tasks in a unified way using multi-task maximum likelihood training. The proposed Simple Task-Oriented Dialogue (SimpleTOD) approach enables modeling of the inherent dependencies between the sub-tasks of task-oriented dialogue, by optimizing for all tasks in an end-to-end manner.", "dateLastCrawled": "2022-01-31T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Simple <b>Language</b> <b>Model</b> for Task-Oriented Dialogue", "url": "https://proceedings.neurips.cc/paper/2020/file/e946209592563be0f01c844ab2170f0c-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/e946209592563be0f01c844ab2170f0c-Paper.pdf", "snippet": "We propose recasting task-oriented dialogue as a simple, causal (<b>unidirectional</b>) <b>language</b> modeling task. We show that such an approach <b>can</b> solve all the sub-tasks in a uni\ufb01ed way using multi-task 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. Figure 1: SimpleTOD is a simple approach to task-oriented dialogue that uses a single causal <b>language</b> <b>model</b> to generate all outputs given the dialogue context and retrieved database search results. The ...", "dateLastCrawled": "2022-01-21T21:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to Use <b>Transformer</b>-based NLP Models | Towards Data Science", "url": "https://towardsdatascience.com/how-to-use-transformer-based-nlp-models-a42adbc292e5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-use-<b>transformer</b>-based-nlp-<b>models</b>-a42adbc292e5", "snippet": "The same <b>model</b> <b>can</b> perform various downstream NLP tasks without fine-tuning or otherwise changing the <b>model</b> parameters or architecture . GPT-2 was trained on a large and diverse text dataset using only a <b>language</b> modeling objective. It reached SOTA results for various NLP tasks without being fine-tuned on those tasks. Only a few examples were provided during inference time to help the <b>model</b> understand what task was requested . Meta-learning is particularly well suited for situations where a ...", "dateLastCrawled": "2022-01-30T13:17:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Language</b> Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>language</b>", "snippet": "A term used to describe a system that evaluates the text that both precedes and follows a target section of text. In contrast, a <b>unidirectional</b> system only evaluates the text that precedes a target section of text. For example, consider a masked <b>language</b> <b>model</b> that must determine probabilities for the word(s) representing the underline in the following question:. What is the _____ with you? A <b>unidirectional</b> <b>language</b> <b>model</b> would have to base its probabilities only on the context provided by ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Word Acquisition in Neural <b>Language</b> Models | Transactions of the ...", "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00444/109271/Word-Acquisition-in-Neural-Language-Models", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/.../tacl_a_00444/109271/Word-Acquisition-in-Neural-<b>Language</b>-<b>Models</b>", "snippet": "A quadratic <b>model</b> of log-frequency also provided a slightly better fit for <b>unidirectional</b> <b>language</b> models (R 2 = 0.93 to 0.94), particularly for high-frequency words; in <b>language</b> models, this could be due either to a floor effect on age of acquisition for high-frequency words or to slower <b>learning</b> of function words. Regardless, significant effects of other predictors remained the same when using a quadratic <b>model</b> for log-frequency.", "dateLastCrawled": "2022-02-02T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Fine-tuned <b>Language Models for Text Classification</b> | DeepAI", "url": "https://deepai.org/publication/fine-tuned-language-models-for-text-classification", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fine-tuned-<b>language-models-for-text-classification</b>", "snippet": "In <b>analogy</b>, a hypercolumn for a word or sentence in NLP is the concatenation of embeddings at different layers in a pretrained <b>model</b>. and is used by peters2017semi, deepcontext2017, Wieting2017, Conneau2017, and Mccann2017 who use <b>language</b> modeling, paraphrasing, entailment, and <b>Machine</b> Translation (MT) respectively for pretraining. Specifically, deepcontext2017 require engineered custom architectures, while we show state-of-the-art performance with the same basic architecture across a range ...", "dateLastCrawled": "2021-12-23T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Machine learning, artificial neural networks and social</b> research", "url": "https://www.researchgate.net/publication/344171463_Machine_learning_artificial_neural_networks_and_social_research", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344171463_<b>Machine</b>_<b>learning</b>_artificial_neural...", "snippet": "<b>Machine</b> <b>Learning</b> (ML) is an automatic <b>learning</b> process in which data sets are processed (Di Franco and Santurro, 2020). An ML system learns directly from the data and learns to connect one or more ...", "dateLastCrawled": "2022-02-02T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "13. Newton\u2019s method is seldom used in <b>machine</b> <b>learning</b> because a. common loss functions are not self-concordant b. Newton\u2019s method does not work well on noisy data c. <b>machine</b> <b>learning</b> researchers don\u2019t really understand linear algebra d. it is generally not practical to form or store the Hessian in such problems, due to large problem size ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "On the character of Indian Stock Markets: A <b>Machine</b> <b>Learning</b> Approach ...", "url": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-machine-learning-approach", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-<b>machine</b>-<b>learning</b>-approach", "snippet": "On the character of Indian Stock Markets: A <b>Machine</b> <b>Learning</b> Approach. Shubham popli Northcap University Gurgaon, Haryana. Abstract- The enterprise of forecasting the stock market is as old as the market itself, ranging from the many traditional approaches like regression analysis and linear methods like AR, MA, ARIMA and ARMA, and of course fuzzier methods like experts intuitions and sentiment analysis of news cycles.", "dateLastCrawled": "2021-12-29T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Artificial intelligence and machine learning</b> in design of mechanical ...", "url": "https://pubs.rsc.org/en/content/articlelanding/2021/mh/d0mh01451f#!", "isFamilyFriendly": true, "displayUrl": "https://pubs.rsc.org/en/content/articlelanding/2021/mh/d0mh01451f", "snippet": "Artificial intelligence, especially <b>machine</b> <b>learning</b> (ML) and deep <b>learning</b> (DL) algorithms, is becoming an important tool in the fields of materials and mechanical engineering, attributed to its power to predict materials properties, design de novo materials and discover new mechanisms beyond intuitions. As the structural complexity of novel materials soars, the material design problem to optimize mechanical behaviors can involve massive design spaces that are intractable for conventional ...", "dateLastCrawled": "2022-01-26T13:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Conceptual models of programming environments: how learners use ...", "url": "https://www.academia.edu/68126562/Conceptual_models_of_programming_environments_how_learners_use_the_glass_box", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/68126562/Conceptual_<b>models</b>_of_programming_environments_how...", "snippet": "A similar <b>model</b> of <b>learning</b> underlies much of the work in this area: in particular work on <b>learning</b> by <b>analogy</b>, but it is often not made explicit. Based on such a framework, Mayer (1975) proposed a concrete <b>model</b> for teaching a BASIC-like <b>language</b>. This provides analogies for four functional units of the computer; and can either be presented as a diagram or as a board using actual parts. The helpfulness of this <b>model</b> was investigated in a study where subjects read a short manual describing ...", "dateLastCrawled": "2022-01-24T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "We do however often want to create a <b>machine</b> <b>learning</b> <b>model</b> that can perform one task really well. This is where finetuning comes in: using a labeled corpus, which is often smaller, we can then train the pretrained <b>model</b> further, with an additional or replacing NLP task. The end result is a <b>model</b> that has been pretrained on the large unlabeled corpus and which is finetuned to a specific <b>language</b> task, such as summarization, text generation in a particular domain, or translation.", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Jiajun Zhang - ACL Anthology", "url": "https://aclanthology.org/people/j/jiajun-zhang/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/people/j/jiajun-zhang", "snippet": "But the current dominant paradigm of <b>machine</b> <b>learning</b> is still to train a <b>model</b> that works well on static datasets. When <b>learning</b> tasks in a stream where data distribution may fluctuate, fitting on new tasks often leads to forgetting on the previous ones. We propose a simple yet effective framework that continually learns natural <b>language</b> understanding tasks with one <b>model</b>. Our framework distills knowledge and replays experience from previous tasks when fitting on a new task, thus named DnR ...", "dateLastCrawled": "2022-01-16T04:50:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(unidirectional language model)  is like +(a train)", "+(unidirectional language model) is similar to +(a train)", "+(unidirectional language model) can be thought of as +(a train)", "+(unidirectional language model) can be compared to +(a train)", "machine learning +(unidirectional language model AND analogy)", "machine learning +(\"unidirectional language model is like\")", "machine learning +(\"unidirectional language model is similar\")", "machine learning +(\"just as unidirectional language model\")", "machine learning +(\"unidirectional language model can be thought of as\")", "machine learning +(\"unidirectional language model can be compared to\")"]}