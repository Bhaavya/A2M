{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention Mechanism Quick Notes", "url": "https://libzx.so/main/learning/2016/03/04/attention-reparaphrase.html", "isFamilyFriendly": true, "displayUrl": "https://libzx.so/main/learning/2016/03/04/attention-reparaphrase.html", "snippet": "Here the <b>sequence-to-sequence</b> <b>task</b> (usually RNN is used) is discussed more than others. In traditional RNN, one tries to encode the all input items into one final code. Then decode the code into mulitple output items. The problems for this method are ignored here. But if we adopt attention mechanism, we may selectively choose some input items, encode and decode it for every output step. Therefore we may only look at some of the input rather than a full and complete representation. In general ...", "dateLastCrawled": "2021-12-12T20:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Hi, robot: Why robotics and <b>language</b> need each other", "url": "https://knowablemagazine.org/article/technology/2020/teaching-robots-to-talk", "isFamilyFriendly": true, "displayUrl": "https://knowablemagazine.org/article/technology/2020/<b>teaching</b>-robots-to-talk", "snippet": "Implementing a \u201c<b>sequence-to-sequence</b>\u201d architecture, this system takes in a sequence of words and outputs a sequence of action commands, rather <b>like</b> translating from one <b>language</b> to another. In between is a neural network, an arrangement of simple computing elements roughly mimicking the brain\u2019s wiring. The network has sub-networks specialized for handling <b>language</b> (the instructions) and images (what the virtual robot sees). When it succeeds during training, the active neural ...", "dateLastCrawled": "2022-02-03T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "BART: Denoising <b>Sequence-to-Sequence</b> Pre-training for Natural <b>Language</b> ...", "url": "https://www.arxiv-vanity.com/papers/1910.13461/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1910.13461", "snippet": "These layers are trained to essentially translate the <b>foreign</b> <b>language</b> to noised English, by propagation through BART, thereby using BART as a pre-trained target-side <b>language</b> model. This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark. To better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives. This study allows us to carefully control for a ...", "dateLastCrawled": "2022-01-28T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Natural <b>Language</b> Processing and Machine Learning", "url": "http://www.derczynski.com/sheffield/teaching/au/nlpml/1.pdf", "isFamilyFriendly": true, "displayUrl": "www.derczynski.com/sheffield/<b>teaching</b>/au/nlpml/1.pdf", "snippet": "Natural <b>Language</b> Processing Basic AI <b>task</b> \u2013 <b>Language</b> presumed unique \u2013 Still a sign of intellect Replicating languge comprehension and production is difficult. Natural <b>Language</b> Processing What is <b>language</b>? \u2013 Physiological \u2013 Vocal apparatus: velar \u2013 Arose in humans 2M-300k years ago. Natural <b>Language</b> Processing Written <b>language</b> much newer \u2013 ~3000 years old \u2013 Not every <b>language</b> has it \u2013 Transiency of spoken vs. intransiency of written \u2013 Allows communication without speaker&#39;s ...", "dateLastCrawled": "2021-08-27T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Learning The Conversation Techniques In <b>Teaching</b> Romanian As <b>A Foreign</b> ...", "url": "https://www.researchgate.net/publication/335190624_Learning_The_Conversation_Techniques_In_Teaching_Romanian_As_A_Foreign_Language", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335190624_Learning_The_Conversation...", "snippet": "The aim of this paper is to apply the Val.Es.Co. group&#39;s theory on units of conversation and colloquial Spanish to the study of some <b>teaching</b> techniques such as dialogues in Spanish as <b>a Foreign</b> ...", "dateLastCrawled": "2021-12-21T18:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Artificial Intelligence (AI) Chatbot as Language Learning Medium</b> ...", "url": "https://www.researchgate.net/publication/337711693_Artificial_Intelligence_AI_Chatbot_as_Language_Learning_Medium_An_inquiry", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337711693_Artificial_Intelligence_AI_Chatbot...", "snippet": "The experimental study was conducted with students in <b>foreign</b> <b>language</b> classes (n=122): a 12-week experimental trial that included pre- and post-course interest, and a sequence of <b>task</b> interest ...", "dateLastCrawled": "2022-01-30T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Developing <b>a Task-Based Dialogue System</b> for English <b>Language</b> ... - MDPI", "url": "https://www.mdpi.com/2227-7102/10/11/306/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2227-7102/10/11/306/htm", "snippet": "Willis outlined the <b>teaching</b> process of <b>task</b>-based <b>language</b> <b>teaching</b> as three stages: pre-<b>task</b> stage, <b>task</b> cycle stage, and <b>language</b> focus stage [18,19]. Stage activities can be used to construct a complete <b>language</b> learning process. The pre-<b>task</b> stage pre-approves the learner\u2019s <b>task</b> instructions and provide the student with clear instructions on what must be done during the <b>task</b> phase", "dateLastCrawled": "2021-11-12T15:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "7 Applications of Deep Learning for Natural <b>Language</b> Processing", "url": "https://machinelearningmastery.com/applications-of-deep-learning-for-natural-language-processing/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/applications-of-deep-learning-for-natural-<b>language</b>...", "snippet": "The <b>task</b> is fundamental to speech or optical character recognition, and is also used for spelling correction, handwriting recognition, and statistical machine translation. \u2014 Page 191, Foundations of Statistical Natural <b>Language</b> Processing, 1999. In addition to the academic interest in <b>language</b> modeling, it is a key component of many deep learning natural <b>language</b> processing architectures. A <b>language</b> model learns the probabilistic relationship between words such that new sequences of words ...", "dateLastCrawled": "2022-01-31T10:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Speech Generator: Main AI technologies</b> - Diplo", "url": "https://www.diplomacy.edu/blog/speech-generator-main-ai-technologies/", "isFamilyFriendly": true, "displayUrl": "https://www.diplomacy.edu/blog/<b>speech-generator-main-ai-technologies</b>", "snippet": "\u2018GPT-2 is a large transformer-based <b>language</b> model trained using the simple <b>task</b> of predicting the next word in 40GB of high-quality text from the internet. This simple objective proves sufficient to train the model to learn a variety of tasks due to the diversity of the dataset. In addition to its incredible <b>language</b> generation capabilities, it is also capable of performing tasks <b>like</b> question answering, reading comprehension, summarisation, and translation. While GPT-2 does not beat the ...", "dateLastCrawled": "2022-01-30T08:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Technological disruption in <b>foreign</b> <b>language</b> <b>teaching</b>: The rise of ...", "url": "https://www.cambridge.org/core/journals/language-teaching/article/technological-disruption-in-foreign-language-teaching-the-rise-of-simultaneous-machine-translation/A31BA5AB690B370B01535EF2D1AFAE42", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>language</b>-<b>teaching</b>/article/technological...", "snippet": "Such structural changes to the <b>teaching</b> profession may impact the private sector particularly hard, especially in English as <b>a foreign</b> <b>language</b> environments. Much <b>like</b> newspapers, which have seen a rapid decline brought on by technology, only the top FL schools may survive and prosper. For these schools, there will likely be an abundance of trained teachers for the relatively small number of <b>teaching</b> positions that remain. Once FL <b>teaching</b> positions begin to disappear, the institutions that ...", "dateLastCrawled": "2021-08-08T08:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The 7 <b>NLP Techniques That Will Change How You Communicate</b> in the Future ...", "url": "https://heartbeat.comet.ml/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-ii-636ab06da258", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/the-7-<b>nlp-techniques-that-will-change-how-you-communicate</b>...", "snippet": "Google Brain\u2019s <b>Sequence-to-Sequence</b> model follows an encoder-decoder architecture. The encoder is responsible for reading the source document and encoding it to an internal representation. The decoder is a <b>language</b> model responsible for generating each word in the output summary using the encoded representation of the source document. IBM Watson uses the <b>similar</b> <b>Sequence-to-Sequence</b> model, but with attention and bidirectional recurrent neural network features. Technique 7: Attention ...", "dateLastCrawled": "2022-01-20T13:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Attention Mechanism Quick Notes", "url": "https://libzx.so/main/learning/2016/03/04/attention-reparaphrase.html", "isFamilyFriendly": true, "displayUrl": "https://libzx.so/main/learning/2016/03/04/attention-reparaphrase.html", "snippet": "Here the <b>sequence-to-sequence</b> <b>task</b> (usually RNN is used) is discussed more than others. In traditional RNN, one tries to encode the all input items into one final code. Then decode the code into mulitple output items. The problems for this method are ignored here. But if we adopt attention mechanism, we may selectively choose some input items, encode and decode it for every output step. Therefore we may only look at some of the input rather than a full and complete representation. In general ...", "dateLastCrawled": "2021-12-12T20:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Data Recombination for Neural Semantic Parsing</b> | DeepAI", "url": "https://deepai.org/publication/data-recombination-for-neural-semantic-parsing", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>data-recombination-for-neural-semantic-parsing</b>", "snippet": "We cast semantic parsing as a <b>sequence-to-sequence</b> <b>task</b>. The input utterance x is a sequence of words x 1, \u2026, x m \u2208 V (in), the input vocabulary; similarly, the output logical form y is a sequence of tokens y 1, \u2026, y n \u2208 V (out), the output vocabulary. A linear sequence of tokens might appear to lose the hierarchical structure of a ...", "dateLastCrawled": "2022-01-24T00:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Review of the Neural <b>History of Natural Language Processing</b> - AYLIEN ...", "url": "https://aylien.com/blog/a-review-of-the-recent-history-of-natural-language-processing", "isFamilyFriendly": true, "displayUrl": "https://aylien.com/blog/a-review-of-the-recent-<b>history-of-natural-language-processing</b>", "snippet": "<b>Sequence-to-sequence</b> learning can even be applied to structured prediction tasks common in NLP where the output has a particular structure. For simplicity, the output is linearized as can be seen for constituency parsing in Figure 10 below. Neural networks have demonstrated the ability to directly learn to produce such a linearized output given sufficient amount of training data for constituency parsing (Vinyals et al, 2015), and named entity recognition (Gillick et al., 2016), among others.", "dateLastCrawled": "2022-01-30T00:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lecture$6:$ Finish$Transformers;$ SequenceHtoHSequence$Modeling$ and ...", "url": "https://home.ttic.edu/~kgimpel/teaching/31210-s19/lectures/6-seq2seq.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~kgimpel/<b>teaching</b>/31210-s19/lectures/6-seq2seq.pdf", "snippet": "\u2022 considering$aenKon$as$query/key/value$suggests$ using$di\ufb00erentspaces$for$di\ufb00erentroles$ \u2022 e.g.,$we$could$use$separate$transformaons$of$the$", "dateLastCrawled": "2021-08-30T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "BART: Denoising <b>Sequence-to-Sequence</b> Pre-training for Natural <b>Language</b> ...", "url": "https://www.arxiv-vanity.com/papers/1910.13461/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1910.13461", "snippet": "These layers are trained to essentially translate the <b>foreign</b> <b>language</b> to noised English, by propagation through BART, thereby using BART as a pre-trained target-side <b>language</b> model. This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark. To better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives. This study allows us to carefully control for a ...", "dateLastCrawled": "2022-01-28T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Artificial Intelligence (AI) Chatbot as Language Learning Medium</b> ...", "url": "https://www.researchgate.net/publication/337711693_Artificial_Intelligence_AI_Chatbot_as_Language_Learning_Medium_An_inquiry", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337711693_Artificial_Intelligence_AI_Chatbot...", "snippet": "The experimental study was conducted with students in <b>foreign</b> <b>language</b> classes (n=122): a 12-week experimental trial that included pre- and post-course interest, and a sequence of <b>task</b> interest ...", "dateLastCrawled": "2022-01-30T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Developing <b>a Task-Based Dialogue System</b> for English <b>Language</b> ... - MDPI", "url": "https://www.mdpi.com/2227-7102/10/11/306/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2227-7102/10/11/306/htm", "snippet": "Willis outlined the <b>teaching</b> process of <b>task</b>-based <b>language</b> <b>teaching</b> as three stages: pre-<b>task</b> stage, <b>task</b> cycle stage, and <b>language</b> focus stage [18,19]. Stage activities can be used to construct a complete <b>language</b> learning process. The pre-<b>task</b> stage pre-approves the learner\u2019s <b>task</b> instructions and provide the student with clear instructions on what must be done during the <b>task</b> phase", "dateLastCrawled": "2021-11-12T15:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Natural <b>Language</b> Processing and Machine Learning", "url": "http://www.derczynski.com/sheffield/teaching/au/nlpml/1.pdf", "isFamilyFriendly": true, "displayUrl": "www.derczynski.com/sheffield/<b>teaching</b>/au/nlpml/1.pdf", "snippet": "Basic AI <b>task</b> \u2013 <b>Language</b> presumed unique \u2013 Still a sign of intellect Replicating languge comprehension and production is difficult. Natural <b>Language</b> Processing What is <b>language</b>? \u2013 Physiological \u2013 Vocal apparatus: velar \u2013 Arose in humans 2M-300k years ago. Natural <b>Language</b> Processing Written <b>language</b> much newer \u2013 ~3000 years old \u2013 Not every <b>language</b> has it \u2013 Transiency of spoken vs. intransiency of written \u2013 Allows communication without speaker&#39;s presence. Natural <b>Language</b> ...", "dateLastCrawled": "2021-08-27T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Technological disruption in <b>foreign</b> <b>language</b> <b>teaching</b>: The rise of ...", "url": "https://www.cambridge.org/core/journals/language-teaching/article/technological-disruption-in-foreign-language-teaching-the-rise-of-simultaneous-machine-translation/A31BA5AB690B370B01535EF2D1AFAE42", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>language</b>-<b>teaching</b>/article/technological...", "snippet": "Such structural changes to the <b>teaching</b> profession may impact the private sector particularly hard, especially in English as <b>a foreign</b> <b>language</b> environments. Much like newspapers, which have seen a rapid decline brought on by technology, only the top FL schools may survive and prosper. For these schools, there will likely be an abundance of trained teachers for the relatively small number of <b>teaching</b> positions that remain. Once FL <b>teaching</b> positions begin to disappear, the institutions that ...", "dateLastCrawled": "2021-08-08T08:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Review of the Neural <b>History of Natural Language Processing</b> - AYLIEN ...", "url": "https://aylien.com/blog/a-review-of-the-recent-history-of-natural-language-processing", "isFamilyFriendly": true, "displayUrl": "https://aylien.com/blog/a-review-of-the-recent-<b>history-of-natural-language-processing</b>", "snippet": "<b>Sequence-to-sequence</b> learning <b>can</b> even be applied to structured prediction tasks common in NLP where the output has a particular structure. For simplicity, the output is linearized as <b>can</b> be seen for constituency parsing in Figure 10 below. Neural networks have demonstrated the ability to directly learn to produce such a linearized output given sufficient amount of training data for constituency parsing (Vinyals et al, 2015), and named entity recognition (Gillick et al., 2016), among others ...", "dateLastCrawled": "2022-01-30T00:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\ud83e\udd84\ud83e\udd1d\ud83e\udd84 <b>Encoder-decoders in Transformers: a</b> hybrid pre-trained architecture ...", "url": "https://medium.com/huggingface/encoder-decoders-in-transformers-a-hybrid-pre-trained-architecture-for-seq2seq-af4d7bf14bb8", "isFamilyFriendly": true, "displayUrl": "https://medium.com/huggingface/<b>encoder-decoders-in-transformers-a</b>-hybrid-pre-trained...", "snippet": "Instead of <b>teaching</b> the model to ... to evaluate people\u2019s abilities in <b>a foreign</b> <b>language</b>). This pre-trained model <b>can</b> then be fine-tuned on many <b>language</b> understanding tasks such as named ...", "dateLastCrawled": "2022-01-27T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Technological disruption in <b>foreign</b> <b>language</b> <b>teaching</b>: The rise of ...", "url": "https://www.cambridge.org/core/journals/language-teaching/article/technological-disruption-in-foreign-language-teaching-the-rise-of-simultaneous-machine-translation/A31BA5AB690B370B01535EF2D1AFAE42", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>language</b>-<b>teaching</b>/article/technological...", "snippet": "For this article, I will concentrate on the effects technology may have on <b>language</b> teachers instructing adults in <b>a foreign</b> <b>language</b> (FL) setting (i.e., <b>teaching</b> a <b>language</b> in a context where the <b>language</b> is not commonly spoken). Here technology may play an even more important role in radically changing the economic dynamics of FL learning and <b>teaching</b>. However, the concern is not technology replacing the FL teacher, but rather technology replacing the need and the demand for FL learning in ...", "dateLastCrawled": "2021-08-08T08:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Computer Vision + Natural Language Processing</b>", "url": "https://github.com/gujiuxiang/CV-NLP_Practice.PyTorch", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gujiuxiang/CV-NLP_Practice.PyTorch", "snippet": "<b>Sequence-to-Sequence</b> Models <b>Can</b> Directly Transcribe <b>Foreign</b> Speech: 20170326: Learning Simpler <b>Language</b> Models with the Delta Recurrent Neural Network Framework: 20160501 : Delving Deeper into Convolutional Networks for Learning Video Representations: 20151001: Holistically-Nested Edge Detection: 20160819: Pixel Recurrent Neural Networks: 20161125: Semantic Segmentation using Adversarial Networks: 20161201: Learning to Generate Images of Outdoor Scenes from Attributes and Semantic Layouts ...", "dateLastCrawled": "2021-09-14T04:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sequence-to-Sequence</b>-101/Google-10000-English.txt at master \u00b7 zake7749 ...", "url": "https://github.com/zake7749/Sequence-to-Sequence-101/blob/master/Epoch1-BasicSeq2Seq/dataset/Google-10000-English.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zake7749/<b>Sequence-to-Sequence</b>-101/blob/master/Epoch1-BasicSeq2Seq/...", "snippet": "Cannot retrieve contributors at this time. 9914 lines (9914 sloc) 73.6 KB Raw Blame", "dateLastCrawled": "2021-08-21T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is a good way to build a machine learning algorithm to ... - Quora", "url": "https://www.quora.com/What-is-a-good-way-to-build-a-machine-learning-algorithm-to-recognize-patterns", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-a-good-way-to-build-a-machine-learning-algorithm-to...", "snippet": "Answer (1 of 4): Machine learning (ML) algorithms are not currently good at the kind of pattern recognition you are referring to: &gt; It9s = It\u2019s You9ll = You\u2019ll Don9t = ? This seems to require reasoning and not just mapping from one vector space to another. Current ML models are excellent at m...", "dateLastCrawled": "2022-01-24T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Speech Recognition Research Topics Ideas | T4Tutorials.com", "url": "https://t4tutorials.com/speech-recognition-research-topics-ideas/", "isFamilyFriendly": true, "displayUrl": "https://t4tutorials.com/speech-recognition-research-topics-ideas", "snippet": "Progressive Co-<b>Teaching</b> for Ambiguous Speech Emotion Recognition 145. TransMask: A Compact and Fast Speech Separation Model Based on Transformer 146. Bionic optimization of MFCC features based on speaker fast recognition 147. Speech Perception Across The Lifespan by Means of Artificial Intelligence 148. Encoding and decoding of meaning through structured variability in intonational speech prosody 149. Introducing the Talk Markup <b>Language</b> (TalkML): Adding a little social intelligence to ...", "dateLastCrawled": "2022-01-09T17:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Understanding Artificial Intelligence</b> | by Sholto Douglas | Zeroth.ai ...", "url": "https://medium.com/zeroth-ai/understanding-artificial-intelligence-b9b58f9b25c2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/zeroth-ai/<b>understanding-artificial-intelligence</b>-b9b58f9b25c2", "snippet": "In order to do more complex tasks like translation or responding to sentences, RNNs need to be modified to give an output sequence after the input sequence. This is a <b>sequence to sequence</b> RNN. At ...", "dateLastCrawled": "2021-08-01T08:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "AQA English <b>Language</b> A-Level - Taverham High School", "url": "https://www.readkong.com/page/aqa-english-language-a-level-taverham-high-school-9580444", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/aqa-english-<b>language</b>-a-level-taverham-high-school-9580444", "snippet": "<b>Task</b> 3: euphemisms and dysphemisms A euphemism is the substitution of a polite expression for one <b>thought</b> to be offensive, harsh or blunt (e.g. \u201cspending a penny\u201d). A dysphemism is when we use a harsh expression instead of a more neutral one (e.g. animal names when they are applied to people, such as coot, old bat, pig, chicken, snake, and bitch). We might call someone a pig when we actually mean that his or her table manners are not very delicate! A. Pick an area where euphemisms are ...", "dateLastCrawled": "2022-01-18T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Will advances in machine learning lessen the importance of learning ...", "url": "https://www.quora.com/Will-advances-in-machine-learning-lessen-the-importance-of-learning-foreign-languages", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Will-advances-in-machine-learning-lessen-the-importance-of...", "snippet": "Answer (1 of 2): This reminds me of a TED talk by Conrad Wolfram, about how we teach math [1]. His point was that math is about posing the right question, formulate it in math and compute the answer. The computation is what computers are best at, but paradoxically that is also what kids in school...", "dateLastCrawled": "2022-01-19T21:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The 7 <b>NLP Techniques That Will Change How You Communicate</b> in the Future ...", "url": "https://heartbeat.comet.ml/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-ii-636ab06da258", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/the-7-<b>nlp-techniques-that-will-change-how-you-communicate</b>...", "snippet": "Google Brain\u2019s <b>Sequence-to-Sequence</b> model follows an encoder-decoder architecture. The encoder is responsible for reading the source document and encoding it to an internal representation. The decoder is a <b>language</b> model responsible for generating each word in the output summary using the encoded representation of the source document. IBM Watson uses the similar <b>Sequence-to-Sequence</b> model, but with attention and bidirectional recurrent neural network features. Technique 7: Attention ...", "dateLastCrawled": "2022-01-20T13:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Developing a <b>Task</b>-Based Dialogue System for English <b>Language</b> ...", "url": "https://www.academia.edu/44664451/Developing_a_Task_Based_Dialogue_System_for_English_Language_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/44664451/Developing_a_<b>Task</b>_Based_Dialogue_System_for_English...", "snippet": "Developing a <b>Task</b>-Based Dialogue System for English <b>Language</b> Learning. 20 Pages. Developing a <b>Task</b>-Based Dialogue System for English <b>Language</b> Learning. Education Sciences, 2020. Education Sciences. Maiga Chang. Kuo-Chen Li . \u51a0\u8208 \u5433. Education Sciences. Maiga Chang. Kuo-Chen Li. \u51a0\u8208 \u5433. Download PDF. Download Full PDF Package. This paper. A short summary of this paper. 37 Full PDFs related to this paper. READ PAPER. Developing a <b>Task</b>-Based Dialogue System for English <b>Language</b> Learning ...", "dateLastCrawled": "2021-05-22T22:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Taris: An online speech recognition framework with <b>sequence to sequence</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0885230822000018", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0885230822000018", "snippet": "First, this <b>task</b> would not be impossible for humans if it was formulated as a puzzle for finding patterns in <b>a foreign</b> <b>language</b>. <b>Language</b> acquisition in humans involves a long term process of <b>teaching</b> simpler, isolated words before gradually increasing the difficulty. These learning strategies have not fully matured in our machine learning technology. On the other hand, it is very common, and cheap, to produce a speech dataset annotated at the sentence level, without intermediate phone-level ...", "dateLastCrawled": "2022-01-20T06:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Developing <b>a Task-Based Dialogue System</b> for English <b>Language</b> ... - MDPI", "url": "https://www.mdpi.com/2227-7102/10/11/306/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2227-7102/10/11/306/htm", "snippet": "Willis outlined the <b>teaching</b> process of <b>task</b>-based <b>language</b> <b>teaching</b> as three stages: pre-<b>task</b> stage, <b>task</b> cycle stage, and <b>language</b> focus stage [18,19]. Stage activities <b>can</b> be used to construct a complete <b>language</b> learning process. The pre-<b>task</b> stage pre-approves the learner\u2019s <b>task</b> instructions and provide the student with clear instructions on what must be done during the <b>task</b> phase", "dateLastCrawled": "2021-11-12T15:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Leveraging Pre-trained Checkpoints for Sequence Generation Tasks</b> ...", "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00313/96450/Leveraging-Pre-trained-Checkpoints-for-Sequence", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00313/96450/Leveraging-Pre...", "snippet": "We developed a Transformer-based <b>sequence-to-sequence</b> model that is compatible with publicly available pre-trained BERT, GPT-2, and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both encoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation, Text Summarization, Sentence Splitting, and Sentence Fusion.", "dateLastCrawled": "2022-01-25T14:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Proceedings of the 6th Workshop on Natural <b>Language</b> Processing ...", "url": "https://aclanthology.org/volumes/2020.nlptea-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2020.nlptea-1", "snippet": "For the detection subtask, we propose two BERT-based approaches 1) with syntactic dependency trees enhancing the model performance and 2) under the multi-<b>task</b> learning framework to combine the sequence labeling and the <b>sequence-to-sequence</b> (seq2seq) models. For the correction subtask, we utilize the masked <b>language</b> model, the seq2seq model and the spelling check model to generate corrections based on the detection results. Finally, our system achieves the highest recall rate on the top-3 ...", "dateLastCrawled": "2022-01-11T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Netflix - New Methods for Memory, Attention, and Efficiency in Neural ...", "url": "https://slides.com/smerity/netflix-new-methods-for-mem-attn-efficiency-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://slides.com/smerity/netflix-new-methods-for-mem-attn-efficiency-in-<b>neural-networks</b>", "snippet": "<b>Sequence to Sequence</b> (Sutskever et al. 2014) Neural Turing Machines (Graves et al. 2014) <b>Teaching</b> Machines to Read and Comprehend (Hermann et al. 2015) Learning to Transduce with Unbounded Memory (Grefenstette 2015) Structured Memory for Neural Turing Machines (Wei Zhang 2015) Memory Networks (Weston et al. 2015) End to end memory networks (Sukhbaatar et al. 2015) DMN Overview. Original input module: a simple uni-directional GRU. QA for Dynamic Memory Networks. A modular and flexible DL ...", "dateLastCrawled": "2022-01-11T22:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "\ud83e\udd84\ud83e\udd1d\ud83e\udd84 <b>Encoder-decoders in Transformers: a</b> hybrid pre-trained architecture ...", "url": "https://medium.com/huggingface/encoder-decoders-in-transformers-a-hybrid-pre-trained-architecture-for-seq2seq-af4d7bf14bb8", "isFamilyFriendly": true, "displayUrl": "https://medium.com/huggingface/<b>encoder-decoders-in-transformers-a</b>-hybrid-pre-trained...", "snippet": "Boss2SQL (patent pending). The encoder is a Bert model pre-trained on the English <b>language</b> (you <b>can</b> even use pre-trained weights!), the decoder a Bert model pre-trained on the SQL <b>language</b>. Fine ...", "dateLastCrawled": "2022-01-27T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Artificial Intelligence (AI) Chatbot as Language Learning Medium</b> ...", "url": "https://www.researchgate.net/publication/337711693_Artificial_Intelligence_AI_Chatbot_as_Language_Learning_Medium_An_inquiry", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337711693_Artificial_Intelligence_AI_Chatbot...", "snippet": "The experimental study was conducted with students in <b>foreign</b> <b>language</b> classes (n=122): a 12-week experimental trial that included pre- and post-course interest, and a sequence of <b>task</b> interest ...", "dateLastCrawled": "2022-01-30T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Cracking the <b>Language</b> Barrier for a Multilingual Africa | Knowledge 4 ...", "url": "https://www.k4all.org/project/language-dataset-fellowship/", "isFamilyFriendly": true, "displayUrl": "https://www.k4all.org/project/<b>language</b>-dataset-fellowship", "snippet": "The <b>task</b> is similar to a machine translation <b>task</b> where we need to translate from a source <b>language</b> to a target <b>language</b>, ADA takes a source text that is non-diacriticized (e.g \u201cbi o tile je pe egbeegberun ti pada sile\u201d) and outputs target texts with diacritics (e.g. \u201cb\u00ed \u00f3 til\u1eb9\u0300 j\u1eb9\u0301 p\u00e9 \u1eb9gb\u1eb9\u1eb9gb\u1eb9\u0300r\u00fan ti pad\u00e0 s\u00edl\u00e9\u00e9\u201d). The first attempt of applying deep learning models to Yor\u00f9b\u00e1 ADA was by Iroro Orife [5].", "dateLastCrawled": "2022-01-30T02:20:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Week 3: <b>Sequence to sequence</b> architectures. <b>Sequence to sequence</b> models Language translation for example; Image captioning, caption an image; Picking the most likely model <b>Machine</b> Transation Model Split into a model encoding the sentence; and then a language model. Calculate the probability of an English sentence conditioned on a French sentence.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Week 1 Quiz - Introduction to deep <b>learning</b>. What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI is powering personal devices in our homes and offices, similar to electricity. Through the \u201csmart grid\u201d, AI is delivering a new wave of electricity. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Note: Andrew ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(teaching a foreign language)", "+(sequence-to-sequence task) is similar to +(teaching a foreign language)", "+(sequence-to-sequence task) can be thought of as +(teaching a foreign language)", "+(sequence-to-sequence task) can be compared to +(teaching a foreign language)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}