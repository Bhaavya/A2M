{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Cross-Lingual Word Embeddings</b> | Computational Linguistics | MIT Press", "url": "https://direct.mit.edu/coli/article/46/1/245/93388/Cross-Lingual-Word-Embeddings", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/46/1/245/93388/<b>Cross-Lingual-Word-Embeddings</b>", "snippet": "Word embeddings represent the words in the vocabulary of a <b>language</b> as vectors in n-dimensional <b>space</b>, where words that are similar being located close to each other. <b>Cross-lingual word embeddings</b> (CLWE for short) extend the idea, and represent translation-equivalent words from two (or more) languages close to each other in a common, cross-lingual <b>space</b>. ...", "dateLastCrawled": "2022-02-02T04:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "&#39;A Passage to India&#39;: Pre-trained Word Embeddings for Indian Languages", "url": "https://aclanthology.org/2020.sltu-1.49.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.sltu-1.49.pdf", "snippet": "<b>language</b>. Additionally, NLP tasks that rely on utilizing common linguistic properties of more than one <b>language</b> need cross-lingual word embeddings, i.e., embeddings for multiple languages projected into a common vector <b>space</b>. These cross-lingual word embeddings have shown to help the task of cross-lingual information extraction (Levy et al.,", "dateLastCrawled": "2022-01-24T22:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Glyph2Vec: <b>Learning</b> Chinese Out-of-Vocabulary Word <b>Embedding</b> from Glyphs", "url": "https://aclanthology.org/2020.acl-main.256.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.256.pdf", "snippet": "of <b>foreign</b> <b>language</b>, which often appears as OOV words. In all the models above, just <b>like</b> Word2Vec (Mikolov et al.,2013c)), the embeddings meed to learned by training over a large corpus. The most similar work is Mimick model (Pin-ter et al.,2017). By <b>learning</b> a character <b>lan-guage</b> generating model, guided by minimizing", "dateLastCrawled": "2022-02-03T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Language</b> <b>Translation</b> with RNNs. Build a recurrent neural network (RNN ...", "url": "https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>language</b>-<b>translation</b>-with-rnns-d84d43b40571", "snippet": "ELMo is context-based (not word-based), so different meanings for a word occupy different vectors within the <b>embedding</b> <b>space</b>. With GloVe and word2vec, each word has only one representation in the <b>embedding</b> <b>space</b>. For example, the word \u201cqueen\u201d could refer to the matriarch of a royal family, a bee, a chess piece, or the 1970s rock band. With traditional embeddings, all of these meanings are tied to a single vector for the word", "dateLastCrawled": "2022-02-02T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning</b> a <b>language</b> \u2013 The <b>10 most effective learning strategies</b>", "url": "http://www.flashcardlearner.com/articles/learning-a-language-the-10-most-effective-learning-strategies/", "isFamilyFriendly": true, "displayUrl": "www.flashcardlearner.com/articles/<b>learning</b>-a-<b>language</b>-the-10-most-effective-<b>learning</b>...", "snippet": "<b>Learning</b> a <b>language</b> requires a huge effort. But there are ways, how you can speed up your progress. The list of the <b>10 most effective learning strategies</b> of <b>learning</b> a <b>language</b> can give you a head start and will prevent you from taking unnecessary detours.", "dateLastCrawled": "2022-01-28T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Defining and <b>Developing Digital Literacy \u2013 Embedding literacies into</b> ...", "url": "https://www.linkinglearning.com.au/defining-and-developing-digital-literacy-embedding-literacies-into-learning-and-life/", "isFamilyFriendly": true, "displayUrl": "https://www.linking<b>learning</b>.com.au/defining-and-developing-digital-literacy-<b>embedding</b>...", "snippet": "Think about how different it is <b>learning</b> the <b>language</b> and culture of <b>a foreign</b> country in a classroom, when compared with going on an immersion tour and living in that country. Having the opportunity to be speaking and hearing the <b>language</b> everyday, interacting with the people and experiencing the culture, observing the ways that they go about their life. It is the small things that we probably don\u2019t even think to teach, <b>like</b> local mannerisms, slang, jokes that are what makes us truly ...", "dateLastCrawled": "2022-01-29T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>The Best Disney Movies to Learn</b> <b>a Foreign</b> <b>Language</b> According to Data ...", "url": "https://towardsdatascience.com/the-best-disney-movies-to-learn-a-foreign-language-according-to-data-science-45e7fd084a78", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>the-best-disney-movies-to-learn</b>-<b>a-foreign</b>-<b>language</b>...", "snippet": "<b>Learning</b> common words is a good opportunity for <b>learning</b> <b>a foreign</b> <b>language</b>! Now that you know my definition of \u201cbest movies,\u201d let\u2019s find out which are <b>the best Disney movies to learn</b> <b>a foreign</b> <b>language</b>. More details about the analysis are in the following sections. <b>The Best Disney Movies to Learn</b> <b>a Foreign</b> <b>Language</b>. The following are the 10 best Disney movies, in which you only need 1000 words to recognize at least 93% of the dialogues. There are 19 movies in the first bar previously ...", "dateLastCrawled": "2022-01-29T09:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>The Relationship Between Language &amp; Culture and</b> the Implications for ...", "url": "https://www.tefl.net/elt/articles/teacher-technique/language-culture/", "isFamilyFriendly": true, "displayUrl": "https://www.tefl.net/elt/articles/teacher-technique/<b>language</b>-culture", "snippet": "When <b>learning</b> or teaching a <b>language</b>, it is important that the culture where the <b>language</b> belongs be referenced, because <b>language</b> is very much ingrained in the culture. Boushra says: Different <b>language</b> with one culture and the whole inter wining of these relationship start at one birth day.The understanding of a culture and its people can be enhanced by the knowledge of their <b>language</b>", "dateLastCrawled": "2022-02-02T22:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Language learning</b> is still in decline in England&#39;s schools | British ...", "url": "https://www.britishcouncil.org/voices-magazine/language-learning-decline-england-schools", "isFamilyFriendly": true, "displayUrl": "https://www.britishcouncil.org/voices-magazine/<b>language-learning</b>-decline-england-schools", "snippet": "The <b>Language</b> Trends Survey 2020 is the latest in a series of annual reports by the British Council, started in 2002, which chart the health of <b>language</b> teaching in English schools. This year\u2019s research is based on an online survey completed between early March to mid-April by teachers in 608 primary schools and 320 secondary schools, of which ...", "dateLastCrawled": "2022-01-31T01:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine <b>learning</b> - <b>How to represent ELMo embeddings as</b> a 1D array ...", "url": "https://stackoverflow.com/questions/53061423/how-to-represent-elmo-embeddings-as-a-1d-array", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/53061423", "snippet": "This will return a list of two numpy arrays, one for each sentence, and each token in the sentence will be represented as one vector of size 1024. And since the default parameter of sents2elmo (output_layer) is -1, this vector represents the average of the 3 internal layers in the <b>language</b> model. How can the embeddings be represented as a 1D array?", "dateLastCrawled": "2022-01-18T23:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On the Role of Seed Lexicons in <b>Learning</b> Bilingual Word Embeddings", "url": "https://aclanthology.org/P16-1024.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/P16-1024.pdf", "snippet": "A shared bilingual word <b>embedding</b> <b>space</b> (SBWES) is an indispensable resource in a variety of cross-<b>language</b> NLP and IR tasks. A common approach to the SB- WES induction is to learn a mapping func-tion between monolingual semantic spaces, where the mapping critically relies on a seed word lexicon used in the <b>learning</b> pro-cess. In this work, we analyze the impor-tance and properties of seed lexicons for the SBWES induction across different di-mensions (i.e., lexicon source, lexicon size ...", "dateLastCrawled": "2022-01-31T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross-Lingual Word Embeddings</b> | Computational Linguistics | MIT Press", "url": "https://direct.mit.edu/coli/article/46/1/245/93388/Cross-Lingual-Word-Embeddings", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/46/1/245/93388/<b>Cross-Lingual-Word-Embeddings</b>", "snippet": "Word embeddings represent the words in the vocabulary of a <b>language</b> as vectors in n-dimensional <b>space</b>, where words that are <b>similar</b> being located close to each other. <b>Cross-lingual word embeddings</b> (CLWE for short) extend the idea, and represent translation-equivalent words from two (or more) languages close to each other in a common, cross-lingual <b>space</b>. ...", "dateLastCrawled": "2022-02-02T04:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> Indonesian-Chinese Lexicon with Bilingual Word <b>Embedding</b> ...", "url": "https://aclanthology.org/W16-3720.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/W16-3720.pdf", "snippet": "bilingual dictionary with cross-lingual word <b>embedding</b> <b>space</b>. The general steps involve 1) building a word <b>space</b> for each individual <b>language</b>; 2) projecting the two spaces into one shared <b>space</b> or from one to the other; and 3) <b>learning</b> or retrieving the target <b>language</b> word most <b>similar</b> to the source <b>language</b> word in the projection.", "dateLastCrawled": "2021-12-23T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep <b>Learning</b> #4: Why You Need to Start Using <b>Embedding</b> Layers | by ...", "url": "https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/deep-<b>learning</b>-4-<b>embedding</b>-layers-f9a02d55ac12", "snippet": "Upon introduction the concept of the <b>embedding</b> layer can be quite <b>foreign</b>. For example, the Keras documentation provides no explanation other than \u201cTurns positive integers (indexes) into dense vectors of fixed size\u201d. A quick Google search might not get you much further either since these type of documentations are the first things to pop-up. However, in a sense Keras\u2019 documentation describes all that happens. So why should you use an <b>embedding</b> layer? Here are the two main reasons: One ...", "dateLastCrawled": "2022-01-29T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Beginner&#39;s Guide to <b>Word2Vec</b> and Neural Word Embeddings | Pathmind", "url": "https://wiki.pathmind.com/word2vec", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/<b>word2vec</b>", "snippet": "So a neural word <b>embedding</b> represents a word with numbers. It\u2019s a simple, yet unlikely, translation. <b>Word2vec</b> <b>is similar</b> to an autoencoder, ... of sunflowers is a two-dimensional mixture of oil on canvas that represents vegetable matter in a three-dimensional <b>space</b> in Paris in the late 1880s, so 500 numbers arranged in a vector can represent a word or group of words. Those numbers locate each word as a point in 500-dimensional vectorspace. Spaces of more than three dimensions are difficult ...", "dateLastCrawled": "2022-02-03T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Language</b> <b>Translation</b> with RNNs. Build a recurrent neural network (RNN ...", "url": "https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>language</b>-<b>translation</b>-with-rnns-d84d43b40571", "snippet": "ELMo is context-based (not word-based), so different meanings for a word occupy different vectors within the <b>embedding</b> <b>space</b>. With GloVe and word2vec, each word has only one representation in the <b>embedding</b> <b>space</b>. For example, the word \u201cqueen\u201d could refer to the matriarch of a royal family, a bee, a chess piece, or the 1970s rock band. With traditional embeddings, all of these meanings are tied to a single vector for the word", "dateLastCrawled": "2022-02-02T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "ESL Immersion vs. <b>Submersion</b>: Models &amp; Approaches - Video &amp; Lesson ...", "url": "https://study.com/academy/lesson/esl-immersion-vs-submersion-models-approaches.html", "isFamilyFriendly": true, "displayUrl": "https://study.com/academy/lesson/esl-immersion-vs-<b>submersion</b>-models-approaches.html", "snippet": "4. <b>Foreign</b> <b>Language</b> Immersion. In <b>foreign</b> <b>language</b> immersion programs, your focus should be on <b>language</b> <b>learning</b> rather than content. Only 1 to 15 percent of class time is spent in English. Such ...", "dateLastCrawled": "2022-02-03T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning</b> a <b>language</b> \u2013 The <b>10 most effective learning strategies</b>", "url": "http://www.flashcardlearner.com/articles/learning-a-language-the-10-most-effective-learning-strategies/", "isFamilyFriendly": true, "displayUrl": "www.flashcardlearner.com/articles/<b>learning</b>-a-<b>language</b>-the-10-most-effective-<b>learning</b>...", "snippet": "<b>Learning</b> a <b>language</b> requires a huge effort. But there are ways, how you can speed up your progress. The list of the <b>10 most effective learning strategies</b> of <b>learning</b> a <b>language</b> can give you a head start and will prevent you from taking unnecessary detours.", "dateLastCrawled": "2022-01-28T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A BERT model generates diagnostically relevant semantic embeddings from ...", "url": "https://www.nature.com/articles/s43856-021-00008-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s43856-021-00008-0", "snippet": "We found that the embeddings tended to cluster meaningfully according to the semantic labels assigned in the development phase, suggesting a <b>similar</b> semantic <b>embedding</b> <b>space</b>. For example, the ...", "dateLastCrawled": "2022-01-14T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>The Relationship Between Language &amp; Culture and</b> the Implications for ...", "url": "https://www.tefl.net/elt/articles/teacher-technique/language-culture/", "isFamilyFriendly": true, "displayUrl": "https://www.tefl.net/elt/articles/teacher-technique/<b>language</b>-culture", "snippet": "This idea, which describes all people as <b>similar</b> at birth, has been around for thousands of years and was discussed by Confucius as recorded in the book by his followers, Analects (Xu, 1997). From birth, the child\u2019s life, opinions, and <b>language</b> are shaped by what it comes in contact with. Brooks (1968) argues that physically and mentally everyone is the same, while the interactions between persons or groups vary widely from place to place. Patterns which emerge from these group behaviours ...", "dateLastCrawled": "2022-02-02T22:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Embedding Technology in Education: The Potential</b> of India\u2019s Solutions ...", "url": "https://www.orfonline.org/research/embedding-technology-in-education/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>orf</b>online.org/research/<b>embedding</b>-technology-in-education", "snippet": "<b>Embedding Technology in Education: The Potential</b> of India\u2019s Solutions in East Africa. Anurag Reddy; India and the countries of Africa share common challenges in their education systems, among them, weak teacher capacities and lack of basic infrastructure. In India, technological interventions have been able to address some of the challenges at scale. Africa has met with less success in this regard, as governments prioritise the provision of hardware without addressing the systemic issues ...", "dateLastCrawled": "2022-02-01T11:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Beginner&#39;s Guide to <b>Word2Vec</b> and Neural Word Embeddings | Pathmind", "url": "https://wiki.pathmind.com/word2vec", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/<b>word2vec</b>", "snippet": "It\u2019s like numbers are <b>language</b>, like all the letters in the <b>language</b> are turned into numbers, and so it\u2019s something that everyone understands the same way. You lose the sounds of the letters and whether they click or pop or touch the palate, or go ooh or aah, and anything that <b>can</b> be misread or con you with its music or the pictures it puts in your mind, all of that is gone, along with the accent, and you have a new understanding entirely, a <b>language</b> of numbers, and everything becomes as ...", "dateLastCrawled": "2022-02-03T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>FACTORS INFLUENCING SECOND LANGUAGE ACQUISITION</b>", "url": "https://www.researchgate.net/publication/322708295_FACTORS_INFLUENCING_SECOND_LANGUAGE_ACQUISITION", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/322708295_FACTORS_INFLUENCING_SECOND_<b>LANGUAGE</b>...", "snippet": "<b>Learning</b> <b>a foreign</b> <b>language</b> enables learners to go further after graduation. In today&#39;s world, there is a need for students to be able to communicate in at least one <b>foreign</b> <b>language</b> so that they ...", "dateLastCrawled": "2022-01-30T05:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning</b> a <b>language</b> \u2013 The <b>10 most effective learning strategies</b>", "url": "http://www.flashcardlearner.com/articles/learning-a-language-the-10-most-effective-learning-strategies/", "isFamilyFriendly": true, "displayUrl": "www.flashcardlearner.com/articles/<b>learning</b>-a-<b>language</b>-the-10-most-effective-<b>learning</b>...", "snippet": "<b>Learning</b> a <b>language</b> requires a huge effort. But there are ways, how you <b>can</b> speed up your progress. The list of the <b>10 most effective learning strategies</b> of <b>learning</b> a <b>language</b> <b>can</b> give you a head start and will prevent you from taking unnecessary detours. If you decide to embark on such a journey of expanding your mind, trying to understand ...", "dateLastCrawled": "2022-01-28T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Why <b>teach foreign languages in Early Years settings</b>? | PACEY", "url": "https://www.pacey.org.uk/news-and-views/pacey-blog/2017/july-2017/why-teach-foreign-languages-in-early-years-setting/", "isFamilyFriendly": true, "displayUrl": "https://www.pacey.org.uk/.../july-2017/why-<b>teach-foreign-languages-in-early-years</b>-setting", "snippet": "<b>Learning</b> <b>a foreign</b> <b>language</b> is becoming a more prominent topic on the early years agenda, as nurseries, childminders and primary schools are beginning to realise the huge benefits it <b>can</b> bring. This has been reflected by England\u2019s 2014 curriculum modification that saw the introduction of a second <b>language</b> in primary school, and by Scotland\u2019s one plus two approach to <b>learning</b> languages.", "dateLastCrawled": "2022-01-26T08:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>The Relationship Between Language &amp; Culture and</b> the Implications for ...", "url": "https://www.tefl.net/elt/articles/teacher-technique/language-culture/", "isFamilyFriendly": true, "displayUrl": "https://www.tefl.net/elt/articles/teacher-technique/<b>language</b>-culture", "snippet": "From this, one <b>can</b> see that <b>learning</b> a new <b>language</b> involves the <b>learning</b> of a new culture (Allwright &amp; Bailey 1991). Consequently, teachers of a <b>language</b> are also teachers of culture (Byram 1989). The implications of <b>language</b> being completely entwined in culture, in regards for <b>language</b> teaching and <b>language</b> policy are far reaching. <b>Language</b> teachers must instruct their students on the cultural background of <b>language</b> usage, choose culturally appropriate teaching styles, and explore ...", "dateLastCrawled": "2022-02-02T22:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A novel <b>embedding</b> approach to learn word vectors by weighting semantic ...", "url": "https://www.sciencedirect.com/science/article/pii/S095741742100587X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S095741742100587X", "snippet": "The state of the art <b>embedding</b> studies widely use deep <b>learning</b> tools today. In deep <b>learning</b>-based natural <b>language</b> processing applications, words are represented by vectors, and the neural network systems are trained through mathematical vector operations fastly. With Neural Network <b>Language</b> Model (NNLM), it is possible to represent words in a huge corpus through low dimensional vectors Pennington et al., 2014). Although it takes much effort to obtain embeddings from large corpora, once ...", "dateLastCrawled": "2022-01-04T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Digital literacies and language learning</b>", "url": "https://www.researchgate.net/publication/285926202_Digital_literacies_and_language_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../285926202_<b>Digital_literacies_and_language_learning</b>", "snippet": "create new <b>learning</b> needs that <b>can</b> be addressed in second and <b>foreign</b> <b>language</b> education; 2. New contexts of <b>language</b> <b>learning</b>: the idea that globalized, online spaces create new, mult ilingual", "dateLastCrawled": "2021-12-24T09:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "3.4 <b>Language, Society, and Culture</b> \u2013 Communication in the Real World", "url": "https://open.lib.umn.edu/communication/chapter/3-4-language-society-and-culture/", "isFamilyFriendly": true, "displayUrl": "https://open.lib.umn.edu/communication/chapter/3-4-<b>language-society-and-culture</b>", "snippet": "<b>Learning</b> Objectives. Discuss some of the social norms that guide conversational interaction. Identify some of the ways in which <b>language</b> varies based on cultural context. Explain the role that accommodation and code-switching play in communication. Discuss cultural bias in relation to specific cultural identities. Society and culture influence the words that we speak, and the words that we speak influence society and culture. Such a cyclical relationship <b>can</b> be difficult to understand, but ...", "dateLastCrawled": "2022-02-02T16:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>Language</b> Reflects Culture &amp; Affects Meaning - Video &amp; Lesson ...", "url": "https://study.com/academy/lesson/how-language-reflects-culture-affects-meaning.html", "isFamilyFriendly": true, "displayUrl": "https://study.com/academy/lesson/how-<b>language</b>-reflects-culture-affects-meaning.html", "snippet": "As George Orwell states in his book 1984, &#39;&#39;But if <b>thought</b> corrupts <b>language</b>, <b>language</b> <b>can</b> also corrupt <b>thought</b>.&#39;&#39; Lesson Summary Let&#39;s take a few moments to review what we&#39;ve learned about ...", "dateLastCrawled": "2022-02-01T05:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Glyph2Vec: <b>Learning</b> Chinese Out-of-Vocabulary Word <b>Embedding</b> from Glyphs", "url": "https://aclanthology.org/2020.acl-main.256.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.256.pdf", "snippet": "of <b>foreign</b> <b>language</b>, which often appears as OOV words. In all the models above, just like Word2Vec (Mikolov et al.,2013c)), the embeddings meed to learned by training over a large corpus. The most similar work is Mimick model (Pin-ter et al.,2017). By <b>learning</b> a character <b>lan-guage</b> generating model, guided by minimizing the distance between the output <b>embedding</b> of LSTMs and pre-trained word embeddings, Mim-ick shows feasibility of generating OOV word <b>em-bedding</b> from character compositions ...", "dateLastCrawled": "2022-02-03T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross-Lingual Word Embeddings</b> | Computational Linguistics | MIT Press", "url": "https://direct.mit.edu/coli/article/46/1/245/93388/Cross-Lingual-Word-Embeddings", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/46/1/245/93388/<b>Cross-Lingual-Word-Embeddings</b>", "snippet": "Word embeddings represent the words in the vocabulary of a <b>language</b> as vectors in n-dimensional <b>space</b>, where words that are similar being located close to each other. <b>Cross-lingual word embeddings</b> (CLWE for short) extend the idea, and represent translation-equivalent words from two (or more) languages close to each other in a common, cross-lingual <b>space</b>. ...", "dateLastCrawled": "2022-02-02T04:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep <b>Learning</b> #4: Why You Need to Start Using <b>Embedding</b> Layers | by ...", "url": "https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/deep-<b>learning</b>-4-<b>embedding</b>-layers-f9a02d55ac12", "snippet": "Let\u2019s assume that we are doing Natural <b>Language</b> Processing (NLP) and have a dictionary of 2000 words. This means that, when using one-hot encoding, each word will be represented by a vector containing 2000 integers. And 1999 of these integers are zeros. In a big dataset this approach is not computationally efficient. The vectors of each <b>embedding</b> get updated while training the neural network. If you have seen the image at the top of this post you <b>can</b> see how similarities between words <b>can</b> ...", "dateLastCrawled": "2022-01-29T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Language</b> <b>Translation</b> with RNNs. Build a recurrent neural network (RNN ...", "url": "https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>language</b>-<b>translation</b>-with-rnns-d84d43b40571", "snippet": "ELMo is context-based (not word-based), so different meanings for a word occupy different vectors within the <b>embedding</b> <b>space</b>. With GloVe and word2vec, each word has only one representation in the <b>embedding</b> <b>space</b>. For example, the word \u201cqueen\u201d could refer to the matriarch of a royal family, a bee, a chess piece, or the 1970s rock band. With traditional embeddings, all of these meanings are tied to a single vector for the word", "dateLastCrawled": "2022-02-02T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Word/Sentence Embeddings \u2013 Random (Ecostate) vs Trained Models", "url": "https://staff.cdms.westernsydney.edu.au/~jianhua/301384_PPA_Spr/Modules/4.%20Technical%20Session%20-%20Dissertation%20Writing/301384%20PPA%20-%20Sample%20Project%20Report%20-%20Word_Sentence%20Embeddings.pdf", "isFamilyFriendly": true, "displayUrl": "https://staff.cdms.westernsydney.edu.au/~jianhua/301384_PPA_Spr/Modules/4. Technical...", "snippet": "Word embeddings is one of the major advancements in the fields of machine <b>learning</b> and Natural <b>Language</b> processing systems. word <b>embedding</b> is the conversion words to high dimensional vectors, where all the words with similar meaning stays close to each other in the vector <b>space</b>. Word embeddings deals with the possibility that the words <b>can</b> have multiple degrees of similarities. One approach to Word embeddings is to train the model using large set of documents corpus that consumes a lot of ...", "dateLastCrawled": "2022-01-02T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep <b>learning for spoken language identification</b> - MeMAD", "url": "https://memad.eu/2020/04/29/deep-learning-spoken-language-identification/", "isFamilyFriendly": true, "displayUrl": "https://memad.eu/2020/04/29/deep-<b>learning</b>-spoken-<b>language</b>-identification", "snippet": "But this <b>can</b> not be done by ASR without knowing the <b>language</b> first. This is the problem called spoken <b>language</b> identification (SLI). This is the problem called spoken <b>language</b> identification (SLI). In MeMAD the spoken <b>language</b> identification turned out to be an important problem to solve: Large quantities of audiovisual data are frequently processed by <b>language</b> dependent tools.", "dateLastCrawled": "2022-01-19T12:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Defining and <b>Developing Digital Literacy \u2013 Embedding literacies into</b> ...", "url": "https://www.linkinglearning.com.au/defining-and-developing-digital-literacy-embedding-literacies-into-learning-and-life/", "isFamilyFriendly": true, "displayUrl": "https://www.linking<b>learning</b>.com.au/defining-and-developing-digital-literacy-<b>embedding</b>...", "snippet": "Think about how different it is <b>learning</b> the <b>language</b> and culture of <b>a foreign</b> country in a classroom, when <b>compared</b> with going on an immersion tour and living in that country. Having the opportunity to be speaking and hearing the <b>language</b> everyday, interacting with the people and experiencing the culture, observing the ways that they go about their life. It is the small things that we probably don\u2019t even think to teach, like local mannerisms, slang, jokes that are what makes us truly ...", "dateLastCrawled": "2022-01-29T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Embedding Technology in Education: The Potential</b> of India\u2019s Solutions ...", "url": "https://www.orfonline.org/research/embedding-technology-in-education/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>orf</b>online.org/research/<b>embedding</b>-technology-in-education", "snippet": "<b>Embedding Technology in Education: The Potential</b> of India\u2019s Solutions in East Africa. Anurag Reddy ; India and the countries of Africa share common challenges in their education systems, among them, weak teacher capacities and lack of basic infrastructure. In India, technological interventions have been able to address some of the challenges at scale. Africa has met with less success in this regard, as governments prioritise the provision of hardware without addressing the systemic issues ...", "dateLastCrawled": "2022-02-01T11:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What are <b>Language</b> Models in NLP? - Daffodil", "url": "https://insights.daffodilsw.com/blog/what-are-language-models-in-nlp", "isFamilyFriendly": true, "displayUrl": "https://insights.daffodilsw.com/blog/what-are-<b>language</b>-models-in-nlp", "snippet": "<b>Compared</b> to the n-gram model, an exponential or continuous <b>space</b> model proves to be a better option for NLP tasks because they are designed to handle ambiguity and <b>language</b> variation. Meanwhile, <b>language</b> models should be able to manage dependencies.", "dateLastCrawled": "2022-02-03T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Word2Vec word <b>embedding</b> tutorial in Python and TensorFlow \u2013 Adventures ...", "url": "https://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/", "isFamilyFriendly": true, "displayUrl": "https://adventuresinmachine<b>learning</b>.com/word2vec-tutorial-tensorflow", "snippet": "One of the key ideas in NLP is how we <b>can</b> efficiently convert words into numeric vectors which <b>can</b> then be \u201cfed into\u201d various machine <b>learning</b> models to perform predictions. The current key technique to do this is called \u201cWord2Vec\u201d and this is what will be covered in this tutorial. After discussing the relevant background material, we will be implementing Word2Vec <b>embedding</b> using TensorFlow (which makes our lives a lot easier). To get up to speed in TensorFlow, check out my", "dateLastCrawled": "2022-02-02T08:55:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Finding the Word <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-word-<b>analogy</b>-from-given-words-using-word2vec...", "snippet": "In the word <b>analogy</b> task, ... What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the word <b>analogy</b>. In this tutorial, we will be using Word2Vec model and a pre-trained model named \u2018GoogleNews-vectors-negative300.bin\u2018 which is trained on over 50 Billion words by Google. Each word inside the pre-trained dataset is embedded in a 300-dimensional <b>space</b> and the words which are similar in context/meaning are placed closer to each other in the <b>space</b>. Methodology to ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "snippet": "A suitable representation is therefore essential for the success of <b>analogy</b>-based <b>learning</b> to rank. Therefore, we propose a method for analogical <b>embedding</b>, i.e., for <b>embedding</b> the data in a target <b>space</b> such that, in this <b>space</b>, the aforementioned <b>analogy</b> assumption is as valid and strongly pronounced as possible. This is accomplished by means of a neural network with a quadruple Siamese structure, which is trained on a suitably designed set of examples in the form of quadruples of objects ...", "dateLastCrawled": "2022-01-17T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://homepages.uni-paderborn.de/ahmadim/IDA%202021.pdf", "isFamilyFriendly": true, "displayUrl": "https://homepages.uni-paderborn.de/ahmadim/IDA 2021.pdf", "snippet": "7 Intelligent Systems and <b>Machine</b> <b>Learning</b> <b>Embedding</b> By ignoring irrelevant or noisy features, the performance can often be improved Common feature selection techniques tailored for the case of <b>analogy</b>-based <b>learning</b> to rank. <b>Analogy</b>-based <b>learning</b> to rank (able2rank) 8 Intelligent Systems and <b>Machine</b> <b>Learning</b> Extension to feature vectors Degree of <b>analogy</b>. Analogical <b>Embedding</b> 9 Intelligent Systems and <b>Machine</b> <b>Learning</b> Positive example: preferences on both sides are coherent Negative ...", "dateLastCrawled": "2022-01-06T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "snippet": "With the emergence of word <b>embedding</b> models, a lot of progress has been made in NLP, essentially assuming that a word <b>analogy</b> like m a n: k i n g:: w o m a n: q u e e n is an instance of a parallelogram within the underlying vector <b>space</b>. In this paper, we depart from this assumption to adopt a <b>machine</b> <b>learning</b> approach, i.e., <b>learning</b> a substitute of the parallelogram model. To achieve our goal, we first review the formal modeling of analogical proportions, highlighting the properties which ...", "dateLastCrawled": "2021-11-13T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-word2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, word <b>embedding</b> is used to map words into vectors of real numbers. There are various word <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce word embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector <b>space</b>, with each unique word in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting ...", "url": "https://www.researchgate.net/figure/In-the-word-embedding-space-the-analogy-pairs-exhibit-interesting-algebraic_fig1_319370400", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/In-the-word-<b>embedding</b>-<b>space</b>-the-<b>analogy</b>-pairs...", "snippet": "Download scientific diagram | In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting algebraic relationships. from publication: Visual Exploration of Semantic Relationships in Neural ...", "dateLastCrawled": "2021-12-21T11:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Word</b> <b>Embeddings</b> for NLP. Understanding <b>word</b> <b>embeddings</b> and their\u2026 | by ...", "url": "https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>word</b>-<b>embeddings</b>-for-nlp-5b72991e01d4", "snippet": "<b>Word</b> <b>Embedding</b> is solution to these problems. <b>Embeddings</b> translate large sparse vectors into a lower-dimensional <b>space</b> that preserves semantic relationships. <b>Word</b> <b>embeddings</b> is a technique where individual words of a domain or language are represented as real-valued vectors in a lower dimensional <b>space</b>.", "dateLastCrawled": "2022-02-02T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Word Embeddings with Word2Vec <b>Tutorial: All you Need to</b> Know", "url": "https://www.h2kinfosys.com/blog/word-embeddings-with-word2vec-tutorial-all-you-need-to-know/", "isFamilyFriendly": true, "displayUrl": "https://www.h2kinfosys.com/blog/word-<b>embeddings</b>-with-word2vec-<b>tutorial-all-you-need-to</b>...", "snippet": "The <b>learning</b> process of the <b>embedding</b> layer requires a lot of training data hence, can be extremely slow. But this approach will learn an <b>embedding</b>, targeted both to the textual data and the NLP task. GloVe: The GloVe (coined from Global Vector) algorithm is an unsupervised <b>machine</b> <b>learning</b> algorithm used for <b>learning</b> word vectors efficiently ...", "dateLastCrawled": "2022-02-03T00:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-<b>embeddings</b>-in-nlp", "snippet": "Word <b>Embedding</b> or Word Vector is a numeric vector input that represents a word in a lower-dimensional <b>space</b>. It allows words with similar meaning to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features. Features: Anything that relates words to one another. Eg: Age, Sports, Fitness, Employed etc. Each word vector has values corresponding to these features. Goal of Word Embeddings. To reduce dimensionality; To use a ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Zero-shot <b>learning</b> via discriminative representation extraction ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "snippet": "The pioneer work in ZSL can be traced to Larochelle et al. , where it verified that when test images belong to some classes that are not available at training stage, a <b>machine</b> <b>learning</b> system can still figure out what a test image is. Due to the importance of zero-shot <b>learning</b>, the number of proposed approaches has increased steadily recently.The number of new zero-shot <b>learning</b> approaches proposed every year was increasing.", "dateLastCrawled": "2021-10-30T07:08:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A self-supervised domain-general <b>learning</b> framework for human ventral ...", "url": "https://www.nature.com/articles/s41467-022-28091-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-022-28091-4", "snippet": "On this view, the <b>embedding space can be thought of as</b> a high-fidelity perceptual interface, with useful visual primitives over which separate conceptual representational systems can operate.", "dateLastCrawled": "2022-01-25T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Spectral Af\ufb01ne-Kernel Embeddings</b> - NSF", "url": "https://par.nsf.gov/servlets/purl/10039348", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10039348", "snippet": "Since <b>machine</b> <b>learn-ing</b> algorithms struggle with high dimensions (an issue known as the curse of dimensionality in this context), one typically needs to map these data points from their high-dimensional space into a lower dimensional space without signi\ufb01cant distortion. Mapping data (living in RD with D\u02db1 but sampling a manifold of low in-trinsic dimensionality d \u02ddD) into a low-dimensional <b>embedding space can be thought of as</b> a preliminary feature extraction step in <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-29T21:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting affinity ties in a surname network", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "snippet": "<b>Machine</b> <b>learning</b>-based approaches for knowledge graph completion To cover the broadest possible range of methods and architectures in the evaluation, we identified representative methods of different model families, taking care that these methods achieve state-of-the-art performances in knowledge graph completion and have open-source implementations that favor the reproducibility of the reported results.", "dateLastCrawled": "2021-09-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(embedding space)  is like +(learning a foreign language)", "+(embedding space) is similar to +(learning a foreign language)", "+(embedding space) can be thought of as +(learning a foreign language)", "+(embedding space) can be compared to +(learning a foreign language)", "machine learning +(embedding space AND analogy)", "machine learning +(\"embedding space is like\")", "machine learning +(\"embedding space is similar\")", "machine learning +(\"just as embedding space\")", "machine learning +(\"embedding space can be thought of as\")", "machine learning +(\"embedding space can be compared to\")"]}