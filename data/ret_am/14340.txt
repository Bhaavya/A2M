{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) A unified analysis of <b>value-function-based reinforcement-learning</b> ...", "url": "https://www.academia.edu/630062/A_unified_analysis_of_value_function_based_reinforcement_learning_algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/630062/A_unified_analysis_of_<b>value</b>_<b>function</b>_based...", "snippet": "In the class of yalue-<b>function</b>-based algorithms, an estimate of the optimal <b>value</b> <b>function</b> is built gradually from the <b>decision</b> <b>maker&#39;s</b> experience and sometimes this estimate is used for control. To define how a <b>value</b>-<b>function</b>-based RL algorithm works, assume we have an .\\lDP and that the <b>decision</b> maker has access to unbiased samples from Pr(\u00b7lx,a) and c; we assume that when the system&#39;s <b>state-action</b> transition is (x,a, y), the <b>decision</b> maker receives a random <b>value</b> c, called the ...", "dateLastCrawled": "2021-11-05T01:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "learning-notes/RL.md at master \u00b7 <b>andrew27xu/learning-notes</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/andrew27xu/learning-notes/blob/master/RL.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/andrew27xu/learning-notes/blob/master/RL.md", "snippet": "From state, <b>value</b> <b>function</b>, still need MDP to find next best action In Q learning, tells best action to take without having to look at MDP . R can be different presentation/format. If R is R(s,a,s&#39;), meaning it also depends on the next state, Q can be rewrite as Q= gama* sum (T(s,a,s&#39;)*(V(s,a,s&#39;)+R(s,a,s&#39;))) Markov <b>Decision</b> Process: state, model T, actions, rewards Markovian property: only present matters, stationary. rewards: encompasses our domain knowledge. So, the Rewards you get from ...", "dateLastCrawled": "2021-08-08T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>Value</b> of Foresight: How Prospection Affects <b>Decision</b>-Making", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3129535/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3129535", "snippet": "1. Introduction. In line with the expected <b>utility</b> theory (EUT), most economic and neuroeconomic models view <b>decision</b>-making as aimed at the maximization of expected <b>utility</b> (von Neumann and Morgenstern, 1944).With regard to the computational processes involved in <b>utility</b> assignment and choice, it has been proposed that the brain can use at least two instrumental controllers: a habitual mechanism, which retrieves the cached values of actions that have successfully led to reward in similar ...", "dateLastCrawled": "2021-11-18T17:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Bellman equation</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Bellman_equation", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Bellman_equation</b>", "snippet": "The best possible <b>value</b> of the objective, written as a <b>function</b> of the state, is called the <b>value</b> <b>function</b>. ... although what constitutes an optimal policy in this case is conditioned on the <b>decision</b>-<b>maker&#39;s</b> opponents choosing similarly optimal policies from their points of view. As suggested by the principle of optimality, we will consider the first <b>decision</b> separately, setting aside all future decisions (we will start afresh from time 1 with the new state ). Collecting the future decisions ...", "dateLastCrawled": "2022-02-03T02:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Fundamentals of <b>Decision</b> Theory - University of Washington", "url": "https://courses.cs.washington.edu/courses/cse573/12sp/lectures/25-decisiontheory.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.washington.edu/courses/cse573/12sp/lectures/25-<b>decision</b>theory.pdf", "snippet": "Select the <b>decision</b> with the highest weighted <b>value</b> States of Nature Criterion of <b>Decision</b> Favorable Unfavorable Realism Large plant $200,000 -$180,000 $124,000 Small plant $100,000 -$20,000 No plant $0 $0 (0.8)($100,000) + (0.2)(-$20,000) = $76,000 (0.8)($0) + (0.2)($0) = $0 $76,000 $0 . Minimax Regret \u2022Regret/Opportunity Loss: \u201cthe difference between the optimal reward and the actual reward received\u201d \u2022Choose the alternative that minimizes the maximum regret associated with each ...", "dateLastCrawled": "2022-02-02T19:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 5, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Markov decision process</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Markov_decision_process", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Markov_decision_process</b>", "snippet": "A <b>Markov decision process</b> is a 4-tuple (,,,), where: is a set of states called the state space,; is a set of actions called the action space (alternatively, is the set of actions available from state ), (, \u2032) = (+ = \u2032 =, =) is the probability that action in state at time will lead to state \u2032 at time +,(, \u2032) is the immediate reward (or expected immediate reward) received after transitioning from state to state \u2032, due to action The state and action spaces may be finite or infinite ...", "dateLastCrawled": "2022-02-07T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Reinforcement learning and inverse reinforcement learning with system 1 ...", "url": "https://deepai.org/publication/reinforcement-learning-and-inverse-reinforcement-learning-with-system-1-and-system-2", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/reinforcement-learning-and-inverse-reinforcement...", "snippet": "Modeling human <b>decision</b> making and inferring a person\u2019s latent reward <b>function</b> from their behavior are important problems across many fields. 1 1 1 Important applications of these techniques include goal recognition [Keren, Gal, and Karpas2014], AI/human cooperation [Hadfield-Menell et al.2016], and many products such as artificial assistants, and recommender systems. Typically such inference is performed using the rational actor model.", "dateLastCrawled": "2022-01-25T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Frontiers | A normative inference approach for optimal sample sizes in ...", "url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01342/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01342", "snippet": "If the <b>decision</b> <b>maker&#39;s</b> goal is merely to maximize the expected <b>utility</b> <b>function</b>, she may, equivalently, find the action a \u2208 A that minimizes the expected terminal opportunity loss. To determine the <b>value</b> of the expected terminal <b>utility</b> <b>function</b> based on the minimization of the expected terminal opportunity loss <b>function</b>, however, the <b>decision</b> maker requires the evaluation of the second term in (22). Put succinctly: if the <b>decision</b> maker is merely interested in choosing the optimal action ...", "dateLastCrawled": "2022-01-29T13:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Human <b>group coordination in a sensorimotor task</b> with neuron-<b>like</b> ...", "url": "https://www.nature.com/articles/s41598-020-64091-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-020-64091-4", "snippet": "This <b>value</b> is then used in a sigmoidal <b>decision</b> <b>function</b> that determines the <b>decision</b>-<b>maker\u2019s</b> action. ( b ) Schematic of the binary <b>decision</b>-<b>maker\u2019s</b> structure.", "dateLastCrawled": "2022-01-29T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "PragerU&#39;s Attempt To Violate YouTube&#39;s 1st Amendment Rights Shot Down ...", "url": "https://reason.com/2020/02/26/pragerus-attempt-to-violate-youtubes-1st-amendment-rights-shot-down-by-9th-circuit-court-of-appeals/", "isFamilyFriendly": true, "displayUrl": "https://<b>reason.com</b>/2020/02/26/pragerus-attempt-to-violate-youtubes-1st-amendment...", "snippet": "PragerU&#39;s argument boils down to the following: YouTube performs a <b>function</b> of <b>value</b> to the public, so it is therefore a public <b>utility</b> bound by the same rules as any other government agency.", "dateLastCrawled": "2022-01-23T00:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CHAPTER 16", "url": "https://www.princeton.edu/~ndaw/d13.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.princeton.edu</b>/~ndaw/d13.pdf", "snippet": "called the <b>state-action</b> <b>value</b> <b>function</b>, depends not only on the current state and action, but also on the actions we take in subsequent states. First, let us define a pol-icy \u03c0(s) as any mapping from states to actions. We write the <b>value</b> of taking an action in a state, and then following some policy \u03c0 thereafter: Q\u03c0\u00f0 s t;a t\u00de5E\u00bd r t 1\u03b3 ...", "dateLastCrawled": "2022-01-20T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "learning-notes/RL.md at master \u00b7 <b>andrew27xu/learning-notes</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/andrew27xu/learning-notes/blob/master/RL.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/andrew27xu/learning-notes/blob/master/RL.md", "snippet": "From state, <b>value</b> <b>function</b>, still need MDP to find next best action In Q learning, tells best action to take without having to look at MDP . R can be different presentation/format. If R is R(s,a,s&#39;), meaning it also depends on the next state, Q can be rewrite as Q= gama* sum (T(s,a,s&#39;)*(V(s,a,s&#39;)+R(s,a,s&#39;))) Markov <b>Decision</b> Process: state, model T, actions, rewards Markovian property: only present matters, stationary. rewards: encompasses our domain knowledge. So, the Rewards you get from ...", "dateLastCrawled": "2021-08-08T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) A unified analysis of <b>value-function-based reinforcement-learning</b> ...", "url": "https://www.academia.edu/630062/A_unified_analysis_of_value_function_based_reinforcement_learning_algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/630062/A_unified_analysis_of_<b>value</b>_<b>function</b>_based...", "snippet": "In the class of yalue-<b>function</b>-based algorithms, an estimate of the optimal <b>value</b> <b>function</b> is built gradually from the <b>decision</b> <b>maker&#39;s</b> experience and sometimes this estimate is used for control. To define how a <b>value</b>-<b>function</b>-based RL algorithm works, assume we have an .\\lDP and that the <b>decision</b> maker has access to unbiased samples from Pr(\u00b7lx,a) and c; we assume that when the system&#39;s <b>state-action</b> transition is (x,a, y), the <b>decision</b> maker receives a random <b>value</b> c, called the ...", "dateLastCrawled": "2021-11-05T01:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>Value</b> of Foresight: How Prospection Affects <b>Decision</b>-Making", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3129535/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3129535", "snippet": "1. Introduction. In line with the expected <b>utility</b> theory (EUT), most economic and neuroeconomic models view <b>decision</b>-making as aimed at the maximization of expected <b>utility</b> (von Neumann and Morgenstern, 1944).With regard to the computational processes involved in <b>utility</b> assignment and choice, it has been proposed that the brain can use at least two instrumental controllers: a habitual mechanism, which retrieves the cached values of actions that have successfully led to reward in <b>similar</b> ...", "dateLastCrawled": "2021-11-18T17:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Bellman equation</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Bellman_equation", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Bellman_equation</b>", "snippet": "The best possible <b>value</b> of the objective, written as a <b>function</b> of the state, is called the <b>value</b> <b>function</b>. ... although what constitutes an optimal policy in this case is conditioned on the <b>decision</b>-<b>maker&#39;s</b> opponents choosing similarly optimal policies from their points of view. As suggested by the principle of optimality, we will consider the first <b>decision</b> separately, setting aside all future decisions (we will start afresh from time 1 with the new state ). Collecting the future decisions ...", "dateLastCrawled": "2022-02-03T02:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Regret-based Reward Elicitation for Markov <b>Decision</b> Processes", "url": "https://www.cs.toronto.edu/~kmregan/files/uai09.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~kmregan/files/uai09.pdf", "snippet": "de\ufb01ne E to be the nk\u00d7n-matrixwith a row foreach <b>state-action</b> pair and one column per state, with E sa,t = P sa(t) if t $= s, and E sa,t = P sa(t) \u22121 if t = s. Our aim is to \ufb01nd an optimal policy that maximizes ex-pected discounted reward. A deterministic policy \u03c0 : S \u2192 A has <b>value</b> <b>function</b> V\u03c0 satisfying: V\u03c0(s)=r(s,\u03c0(s)) +\u03b3! s! P ...", "dateLastCrawled": "2021-12-19T00:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Action Space</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/mathematics/action-space", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/mathematics/<b>action-space</b>", "snippet": "There is also a <b>utility</b> <b>function</b> U(\u03b8, a), 9 which represents the <b>value</b> to the <b>decision</b> maker of choosing act a when \u0398 is the true <b>value</b> of the parameter. If the true <b>value</b> of \u0398 were known, then one would need no data. One would simply choose, for that \u0398, the act that maximizes <b>utility</b>, U (\u03b8, a), or minimizes loss, L(\u03b8, a). When \u0398 is unknown, however, one may still have to choose an action. To help in such a choice, one can first observe relevant data, which reduces the degree of ...", "dateLastCrawled": "2021-12-16T19:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Frontiers | A normative inference approach for optimal sample sizes in ...", "url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01342/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01342", "snippet": "If the <b>decision</b> <b>maker&#39;s</b> goal is merely to maximize the expected <b>utility</b> <b>function</b>, she may, equivalently, find the action a \u2208 A that minimizes the expected terminal opportunity loss. To determine the <b>value</b> of the expected terminal <b>utility</b> <b>function</b> based on the minimization of the expected terminal opportunity loss <b>function</b>, however, the <b>decision</b> maker requires the evaluation of the second term in (22). Put succinctly: if the <b>decision</b> maker is merely interested in choosing the optimal action ...", "dateLastCrawled": "2022-01-29T13:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Modulators of <b>decision</b> making | Nature Neuroscience", "url": "https://www.nature.com/articles/nn2077", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/nn2077", "snippet": "The <b>function</b> f is called the &#39;<b>utility</b> <b>function</b>&#39;. For example, the <b>value</b> of a perishable food should saturate depending on how much the animal can eat. Thus the <b>function</b> is often regarded as a ...", "dateLastCrawled": "2022-02-03T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Reinforcement learning improves behaviour from evaluative feedback | Nature", "url": "https://www.nature.com/articles/nature14540", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/nature14540", "snippet": "The <b>decision</b> <b>maker&#39;s</b> task is defined by a reward <b>function</b> R(s, a) and discount factor \u03b3 \u0404 [0, 1]. Rewards are delivered to the <b>decision</b> maker with each transition and maximizing the cumulative ...", "dateLastCrawled": "2022-02-02T20:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement learning and inverse reinforcement learning with system 1 ...", "url": "https://deepai.org/publication/reinforcement-learning-and-inverse-reinforcement-learning-with-system-1-and-system-2", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/reinforcement-learning-and-inverse-reinforcement...", "snippet": "It <b>can</b> also <b>be thought</b> of as a converged version of a ... the planner gets to change the doer\u2019s <b>utility</b> <b>function</b> (reward <b>function</b> for each <b>state, action</b>), and the doer chooses the policy consistent with this new <b>utility</b> <b>function</b>. This no longer has a simple recursive form because V 1 and V 2 have different discount rates. However, we now show that policy iteration <b>can</b> be used even in this compound problem. First, we substitute the definitions of the <b>value</b> functions into the equations above ...", "dateLastCrawled": "2022-01-25T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Exploring <b>the Impact of</b> Artificial Intelligence: Prediction versus Judgment", "url": "https://ide.mit.edu/sites/default/files/publications/w24626.pdf", "isFamilyFriendly": true, "displayUrl": "https://ide.mit.edu/sites/default/files/publications/w24626.pdf", "snippet": "These expected payoffs represent the <b>decision</b>-<b>maker\u2019s</b> <b>utility</b> from each action. Which action should be taken depends on the prediction of how likely the good rather than the bad state will arise. To keep things simple, we will suppose that the probability of each state is \u00bd. Prediction is of <b>value</b> because it makes taking the risky action less risky. To capture this, we assume that: ! &quot; ($+&amp;)&lt;) (A1) Then in the absence of a prediction, the <b>decision</b>-maker will take the safe action. Better ...", "dateLastCrawled": "2022-02-03T06:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Exploring the Impact of Artificial Intelligence: Prediction versus Judgment", "url": "https://techpolicyinstitute.org/wp-content/uploads/2018/02/Gans-et-al-prediction-vs-judgment.pdf", "isFamilyFriendly": true, "displayUrl": "https://techpolicyinstitute.org/wp-content/uploads/2018/02/Gans-et-al-prediction-vs...", "snippet": "These expected payoffs represent the <b>decision</b>-<b>maker\u2019s</b> <b>utility</b> from each action. Which action should be taken depends on the prediction of how likely the good rather than the bad state will arise. To keep things simple, we will suppose that the probability of each state is \u00bd. Prediction is of <b>value</b> because it makes taking the risky action less risky. To capture this, we assume that: ! &quot; #+%&lt;\u2019 (A1) Then in the absence of a prediction, the <b>decision</b>-maker will take the safe action. Better ...", "dateLastCrawled": "2022-02-02T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Does temporal discounting explain unhealthy behavior? A systematic ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3950931/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3950931", "snippet": "Formally, an outcome which has <b>utility</b> A if received immediately (t = 0) is worth A\u00b7 ... Discounted <b>value</b>, V(A, t, \u03c4) under three discount functions is plotted of as a <b>function</b> of the <b>decision</b> <b>maker&#39;s</b> position in time, \u03c4, where A is the magnitude of the outcome (its instantaneous <b>utility</b>) and t the time at which it is due to be delivered. A larger-later reward, LL, of magnitude, l, is due to be received at t 3 and a smaller-sooner reward, SS, of magnitude, s, is due to be received at t 2 ...", "dateLastCrawled": "2022-01-25T21:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Fundamentals of <b>Decision</b> Theory - University of Washington", "url": "https://courses.cs.washington.edu/courses/cse573/12sp/lectures/25-decisiontheory.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.washington.edu/courses/cse573/12sp/lectures/25-<b>decision</b>theory.pdf", "snippet": "Select the <b>decision</b> with the highest weighted <b>value</b> States of Nature Criterion of <b>Decision</b> Favorable Unfavorable Realism Large plant $200,000 -$180,000 $124,000 Small plant $100,000 -$20,000 No plant $0 $0 (0.8)($100,000) + (0.2)(-$20,000) = $76,000 (0.8)($0) + (0.2)($0) = $0 $76,000 $0 . Minimax Regret \u2022Regret/Opportunity Loss: \u201cthe difference between the optimal reward and the actual reward received\u201d \u2022Choose the alternative that minimizes the maximum regret associated with each ...", "dateLastCrawled": "2022-02-02T19:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Using reinforcement learning to optimize the acceptance threshold of</b> a ...", "url": "https://www.sciencedirect.com/science/article/pii/S1568494619304788", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1568494619304788", "snippet": "Up to 50 episodes are needed to sufficiently approximate the <b>value</b> <b>function</b> model for the whole <b>state\u2013action</b> space. The final model shape is displayed in Fig. 9 . As was explained in Section 3.3.2 , the <b>value</b> of a <b>state\u2013action</b> pair reflects the expected profitability of performing a particular action in a particular state and subsequently following an optimal policy.", "dateLastCrawled": "2022-01-04T12:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Asymmetries in Information Processing in a <b>Decision</b> Theory Framework", "url": "http://www.hec.unil.ch/lspinto/Papers%20&%20CV/positiveresponsive.pdf", "isFamilyFriendly": true, "displayUrl": "www.hec.unil.ch/lspinto/Papers &amp; CV/positiveresponsive.pdf", "snippet": "<b>decision</b>-<b>maker\u2019s</b> information structure and has generic element F(W = \u03c9,X= x) or, in condensed notation, F(\u03c9,x). Note that the two random variables, X and W, are fully characterized by F since from F one <b>can</b> ob-tain (1) the marginal probability distribution of the signal, F X: X \u2192[0,1], with generic element F X(X = x) or, in condensed notation, F X(x), (2) the <b>decision</b> <b>maker\u2019s</b> prior beliefs or the marginal probability distribution of the state of nature, F W: \u03a9\u2192[0,1], with generic ", "dateLastCrawled": "2022-01-28T23:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Systems of Bounded Rational Agents with Information-Theoretic ...", "url": "https://direct.mit.edu/neco/article/31/2/440/8448/Systems-of-Bounded-Rational-Agents-with", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/neco/article/31/2/440/8448/Systems-of-Bounded-Rational-Agents-with", "snippet": "The posterior P <b>can</b> be seen as a <b>state-action</b> policy that selects the best action a \u2208 A with respect to a <b>utility</b> <b>function</b> U given the state w \u2208 W of the world. 2.3 Bounded Rational Agents In the information-theoretic model of bounded rationality (Ortega &amp; Braun, 2011 , 2013 ; Genewein et al., 2015 ), an agent is bounded rational if its posterior P maximizes equation 2.1 , subject to the constraint", "dateLastCrawled": "2022-01-30T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Thermodynamics as a theory of <b>decision</b>-making with information ...", "url": "https://www.researchgate.net/publication/224871866_Thermodynamics_as_a_theory_of_decision-making_with_informationprocessing_costs", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/224871866_Thermodynamics_as_a_theory_of...", "snippet": "Therefore, in this case, free energy <b>can</b> be seen as a certainty-equivalent <b>value</b> of the subordinate <b>decision</b> problems, i.e., the amount of <b>utility</b> the agent would have to receive to be indifferent ...", "dateLastCrawled": "2022-01-27T09:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Human <b>group coordination in a sensorimotor task</b> with neuron-like ...", "url": "https://www.nature.com/articles/s41598-020-64091-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-020-64091-4", "snippet": "This <b>value</b> is then used in a sigmoidal <b>decision</b> <b>function</b> that determines the <b>decision</b>-<b>maker\u2019s</b> action. ( b ) Schematic of the binary <b>decision</b>-<b>maker\u2019s</b> structure.", "dateLastCrawled": "2022-01-29T21:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Learning the opportunity cost of time in a patch-foraging task ...", "url": "https://europepmc.org/article/MED/25917000", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/25917000", "snippet": "An alternative model, the Q-learning algorithm, instead learns a <b>state-action</b> <b>value</b> <b>function</b>, which represents the cumulative future expected <b>value</b> of harvesting or leaving a tree at each state (where the state is given by the number of previous harvests on a tree). These action values are <b>compared</b> in each state to reach a <b>decision</b> (Watkins, 1989).", "dateLastCrawled": "2022-02-02T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>FORECASTING</b> AND <b>DECISION</b> THEORY", "url": "https://econweb.ucsd.edu/~mmachina/papers/Granger-Machina_Handbook_of_Econ_Forecasting.pdf", "isFamilyFriendly": true, "displayUrl": "https://econweb.ucsd.edu/~mmachina/papers/Granger-Machina_Handbook_of_Econ_<b>Forecasting</b>.pdf", "snippet": "A <b>decision</b> maker will typically have a payoff or <b>utility</b> <b>function</b> U(x,\u03b1), which de-pends upon some uncertain variable or vectorx which will be realized and observed at a future time T, as well as some <b>decision</b> variable or vector \u03b1 which must be chosen out of a set A at some earlier time t&lt;T. The <b>decision</b> maker <b>can</b> base their choice of \u03b1 upon a current scalar forecast (a \u201cpoint forecast\u201d) xF of the variable x,andmake the choice \u03b1(xF) \u2261 argmax \u03b1\u2208AU(xF,\u03b1). Given the realized <b>value</b> ...", "dateLastCrawled": "2022-01-30T23:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Preference Elicitation and Inverse Reinforcement Learning</b>", "url": "https://www.researchgate.net/publication/215558830_Preference_Elicitation_and_Inverse_Reinforcement_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/215558830_Preference_Elicitation_and_Inverse...", "snippet": "<b>state-action</b> <b>value</b> <b>function</b>. In general, maximum entropy approac h es have good minimax guarantees [7]. Consequently , the estimated p olicy is guaranteed to be close to the agent\u2019s.", "dateLastCrawled": "2021-11-27T14:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Chapter 2 <b>Forecasting and Decision Theory</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1574070605010025", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1574070605010025", "snippet": "Given the realized <b>value</b> x R, the <b>decision</b> <b>maker&#39;s</b> ex post <b>utility</b> U (x R, \u03b1 (x F)) <b>can</b> <b>be compared</b> with the maximum possible <b>utility</b> they could have attained, namely U (x R, \u03b1 (x R)). This shortfall <b>can</b> be averaged over a number of such situations, to obtain the <b>decision</b> <b>maker&#39;s</b> average loss in terms of foregone payoff or <b>utility</b>. If one is forecasting in a stochastic environment, perfect forecasting will not be possible and this average long-term loss will be strictly positive. In a ...", "dateLastCrawled": "2022-01-18T17:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The <b>Value</b> of Foresight: How Prospection Affects <b>Decision</b>-Making", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3129535/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3129535", "snippet": "1. Introduction. In line with the expected <b>utility</b> theory (EUT), most economic and neuroeconomic models view <b>decision</b>-making as aimed at the maximization of expected <b>utility</b> (von Neumann and Morgenstern, 1944).With regard to the computational processes involved in <b>utility</b> assignment and choice, it has been proposed that the brain <b>can</b> use at least two instrumental controllers: a habitual mechanism, which retrieves the cached values of actions that have successfully led to reward in similar ...", "dateLastCrawled": "2021-11-18T17:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A tutorial on recursive <b>models for analyzing and predicting</b> ... - DeepAI", "url": "https://deepai.org/publication/a-tutorial-on-recursive-models-for-analyzing-and-predicting-path-choice-behavior", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-tutorial-on-recursive-models-for-analyzing-and...", "snippet": "05/02/19 - The problem at the heart of this tutorial consists in modeling the path choice behavior of network users. This problem has extensi...", "dateLastCrawled": "2022-01-23T12:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Quality and Quantity of Information Exchange", "url": "https://www.jstor.org/stable/pdf/40167354.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.jstor.org/stable/pdf/40167354.pdf", "snippet": "<b>Decision</b> Criteria and the <b>Value</b> of Information The idea is that we ask questions not just to get new information, but rather to get new information that might help to resolve a particular <b>decision</b> problem. By relating questions <b>to decision</b> problems, we <b>can</b> measure the <b>utility</b> of questions and answers/assertions. It turns out that this measure depends crucially on the <b>decision</b> criterion used. <b>Decision</b> problems are conventionally categorized according to the <b>decision</b> <b>maker&#39;s</b> knowledge of the ...", "dateLastCrawled": "2022-02-02T08:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Preference elicitation and inverse reinforcement learning | DeepAI", "url": "https://deepai.org/publication/preference-elicitation-and-inverse-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/preference-elicitation-and-inverse-reinforcement-learning", "snippet": "The second main assumption is the expected <b>utility</b> hypothesis. This posits that if we <b>can</b> assign a numerical <b>utility</b> to each event, such that events with larger utilities are preferred, then the <b>decision</b> <b>maker\u2019s</b> preferred choice from a set of possible gambles will be the gamble with the highest expected <b>utility</b>. The corresponding problem is ...", "dateLastCrawled": "2022-01-02T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Using reinforcement learning to optimize the acceptance threshold of</b> a ...", "url": "https://www.sciencedirect.com/science/article/pii/S1568494619304788", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1568494619304788", "snippet": "<b>Compared</b> to simpler gradient descent algorithms, RBF provide a more flexible shape of the approximated <b>function</b> and fit the expectation that the <b>value</b> <b>function</b> is bell-shaped both in states and actions. As opposed to complex artificial neural network models, RBF converge faster, are much easier to set up and work with, and are computationally cheaper. Finally, RBF have been found to perform well in small state spaces", "dateLastCrawled": "2022-01-04T12:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "An Approximate Dynamic Programming Algorithm for Monotone <b>Value</b> ...", "url": "https://www.researchgate.net/publication/259604285_An_Approximate_Dynamic_Programming_Algorithm_for_Monotone_Value_Functions", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/259604285_An_Approximate_Dynamic_Programming...", "snippet": "Abstract. Many sequential <b>decision</b> problems <b>can</b> be formulated as Markov <b>Decision</b> Processes (MDPs) where the optimal <b>value</b> <b>function</b> (or cost-to-go <b>function</b>) <b>can</b> be shown to satisfy a monotone ...", "dateLastCrawled": "2022-01-28T02:48:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Relationship between state (V) and action(Q) <b>value</b> <b>function</b> in ...", "url": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and-action-q-value-function-in-reinforcement-learning-bb9a988c0127", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and...", "snippet": "<b>Value</b> <b>function</b> can be defined as the expected <b>value</b> of an agent in a certain state. There are two types of <b>value</b> functions in RL: State-<b>value</b> and action-<b>value</b>. It is important to understand the\u2026", "dateLastCrawled": "2022-02-03T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Value</b>. State-<b>value</b> <b>function</b> v_\u03c0: gives us the <b>value</b> of a state under \u03c0; Action-<b>value</b> <b>function</b> q_\u03c0: gives us the <b>value</b> of an action under \u03c0. q_\u03c0 is referred to as the Q-<b>function</b>, and the output from the <b>function</b> for any given <b>state-action</b> pair is called a Q-<b>value</b>.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Value</b>. State-<b>value</b> <b>function</b> v_\u03c0: gives us the <b>value</b> of a state under \u03c0; Action-<b>value</b> <b>function</b> q_\u03c0: gives us the <b>value</b> of an action under \u03c0. q_\u03c0 is referred to as the Q-<b>function</b>, and the output from the <b>function</b> for any given <b>state-action</b> pair is called a Q-<b>value</b>.", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AI and Reinforcement <b>Learning</b> \u2014 Machines that Learn through Experience ...", "url": "https://www.cantorsparadise.com/ai-and-reinforcement-learning-machines-that-learn-through-experience-e7eea7bb6765", "isFamilyFriendly": true, "displayUrl": "https://www.cantorsparadise.com/ai-and-reinforcement-<b>learning</b>-<b>machines</b>-that-learn...", "snippet": "To align the policy with the updated <b>value</b> <b>function</b>, the algorithm modifies the policy so it would greedily follow the <b>value</b> <b>function</b> (meaning, choosing to perform actions that has the highest <b>value</b>). The algorithm continues by generating a new episode, now under the improved policy, which, in turn, derives a more accurate <b>value</b> estimation and so on. In this process, both the policy and the <b>value</b> <b>function</b> converge to their optimal values, until sufficient accuracy is reached, or when no more ...", "dateLastCrawled": "2022-01-25T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Value</b>-<b>Function</b>-<b>Based Transfer for Reinforcement Learning</b> Using ...", "url": "https://www.researchgate.net/publication/221604435_Value-Function-Based_Transfer_for_Reinforcement_Learning_Using_Structure_Mapping", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221604435_<b>Value</b>-<b>Function</b>-Based_Transfer_for...", "snippet": "chological and computational theory about <b>analogy</b> making, ... the form of a <b>state-action</b> <b>value</b> <b>function</b>, or a q-<b>functio n</b>. A. q-<b>function</b> q: S \u00d7 A 7\u2192 R maps from <b>state-action</b> pairs to. real ...", "dateLastCrawled": "2021-10-16T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine learning for biochemical engineering: A</b> review - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1369703X21001303", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1369703X21001303", "snippet": "<b>Value</b>-based algorithms, typically represented by Q-<b>learning</b>, explicitly learn and optimise the <b>state-action</b> <b>value</b> <b>function</b> and generate the optimal policy by acting greedily with respect to it i.e. choosing the control corresponding to the maximum Q \u03c0 x, u <b>value</b> (<b>state-action</b> <b>value</b>). There are also hybrid algorithms, such as actor-critic methods, which combine policy optimisation methods and <b>value</b>-based methods. Although RL has shown success in game-based control benchmarks, such as AlphaGo", "dateLastCrawled": "2022-01-26T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Reinforcement Learning: Prediction, Control and</b> <b>Value</b> <b>Function</b> ...", "url": "https://deepai.org/publication/reinforcement-learning-prediction-control-and-value-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>reinforcement-learning-prediction-control-and</b>-<b>value</b>...", "snippet": "<b>Reinforcement Learning: Prediction, Control and Value Function Approximation</b>. With the increasing power of computers and the rapid development of self-<b>learning</b> methodologies such as <b>machine</b> <b>learning</b> and artificial intelligence, the problem of constructing an automatic Financial Trading Systems (FTFs) becomes an increasingly attractive research ...", "dateLastCrawled": "2022-01-16T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>SARSA</b> vs Q - <b>learning</b>", "url": "https://tcnguyen.github.io/reinforcement_learning/sarsa_vs_q_learning.html", "isFamilyFriendly": true, "displayUrl": "https://tcnguyen.github.io/reinforcement_<b>learning</b>/<b>sarsa</b>_vs_q_<b>learning</b>.html", "snippet": "<b>SARSA</b> will learn the optimal $\\epsilon$-greedy policy, i.e, the Q-<b>value</b> <b>function</b> will converge to a optimal Q-<b>value</b> <b>function</b> but in the space of $\\epsilon$-greedy policy only (as long as each <b>state action</b> pair will be visited infinitely). We expect that in the limit of $\\epsilon$ decaying to $0$, <b>SARSA</b> will converge to the overall optimal policy. I quote here a paragraph from", "dateLastCrawled": "2022-01-30T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Value</b>-<b>function-based transfer for reinforcement</b> <b>learning</b> using ...", "url": "https://www.academia.edu/2661041/Value_function_based_transfer_for_reinforcement_learning_using_structure_mapping", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2661041/<b>Value</b>_<b>function_based_transfer_for_reinforcement</b>...", "snippet": "Abstract Transfer <b>learning</b> concerns applying knowledge learned in one task (the source) to improve <b>learning</b> another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about <b>analogy</b> making, to . \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset ...", "dateLastCrawled": "2022-01-19T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>learning</b> and AI <b>in marketing \u2013 Connecting computing power to</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167811620300410", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167811620300410", "snippet": "<b>State-Action</b>-Reward-<b>State-Action</b>: 2.2.3: SVD: Singular <b>Value</b> Decomposition: 2.2.2: SVM: Support Vector <b>Machine</b> : 2.2.1: TD: Temporal-Difference: 2.2.3: UGC: User-Generated Content: 3.1: Table 3. Strengths and weaknesses of <b>machine</b> <b>learning</b> methods. Strength \u2022 Ability to handle unstructured data and data of hybrid formats \u2022 Ability to handle large data volume \u2022 Flexible model structure \u2022 Strong predictive performance. Weakness \u2022 Not easy to interpret \u2022 Relationship typically ...", "dateLastCrawled": "2022-01-12T18:25:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(state-action value function)  is like +(decision maker's utility function)", "+(state-action value function) is similar to +(decision maker's utility function)", "+(state-action value function) can be thought of as +(decision maker's utility function)", "+(state-action value function) can be compared to +(decision maker's utility function)", "machine learning +(state-action value function AND analogy)", "machine learning +(\"state-action value function is like\")", "machine learning +(\"state-action value function is similar\")", "machine learning +(\"just as state-action value function\")", "machine learning +(\"state-action value function can be thought of as\")", "machine learning +(\"state-action value function can be compared to\")"]}