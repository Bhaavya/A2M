{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Programming Fairness in Algorithms</b> | by Matthew Stewart, PhD Researcher ...", "url": "https://towardsdatascience.com/programming-fairness-in-algorithms-4943a13dd9f8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>programming-fairness-in-algorithms</b>-4943a13dd9f8", "snippet": "It is fairly easy to prevent <b>disparate</b> <b>treatment</b> in algorithms \u2014 the explicit differential <b>treatment</b> of ... (e.g. race, gender). However, it is much less easy to prevent <b>disparate</b> impact \u2014implicit differential <b>treatment</b> <b>of one</b> <b>group</b> <b>over</b> <b>another</b>, usually caused by something called redundant encodings in the data. Illustration of <b>disparate</b> impact \u2014 in this diagram the data distribution of two groups is very different, which leads to differences in the output of the algorithm without any ...", "dateLastCrawled": "2022-01-23T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Fairness Beyond <b>Disparate</b> <b>Treatment</b> &amp; <b>Disparate</b> Impact: Learning ...", "url": "https://www.researchgate.net/publication/315871857_Fairness_Beyond_Disparate_Treatment_Disparate_Impact_Learning_Classification_without_Disparate_Mistreatment", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315871857_Fairness_Beyond_<b>Disparate</b>_<b>Treatment</b>...", "snippet": "<b>unfair</b> <b>treatment</b>) and there is no ground truth about the corr ectness of the historical decisions ( i.e. , <b>one</b> cannot tell whether a historical decision used during the training phase", "dateLastCrawled": "2022-01-26T15:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Managing <b>bias and unfairness</b> in data for decision support: a survey of ...", "url": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "snippet": "Essentially, <b>group</b> fairness reflects averages <b>over</b> sets of individuals\u2014if the averages across groups are similar, then the model is considered fair\u2014while individual fairness is interested in each of the individuals and how they are treated in comparison with all other individuals\u2014while a <b>group</b> average might seem high, two individuals within the same <b>group</b> might receive <b>disparate</b> <b>treatment</b>, which in average look fair. In our example, an unfairness measure such as <b>disparate</b> impact could ...", "dateLastCrawled": "2022-02-01T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "4 <b>Theories of Discrimination</b> | Measuring Racial <b>Discrimination</b> | The ...", "url": "https://www.nap.edu/read/10887/chapter/7", "isFamilyFriendly": true, "displayUrl": "https://www.nap.edu/read/10887/chapter/7", "snippet": "Avoidance entails choosing the comfort <b>of one</b>\u2019s own racial <b>group</b> (the \u201cingroup\u201d in social psychological terms) <b>over</b> interaction with <b>another</b> racial <b>group</b> (the \u201coutgroup\u201d). In settings of discretionary contact\u2014that is, in which people may choose to associate or not\u2014members of disadvantaged racial groups may be isolated. In social situations, people may self-segregate along racial lines. In work settings, discretionary contact may force out-<b>group</b> members into lower-status ...", "dateLastCrawled": "2022-02-03T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Algorithmic Fairness: Tackling Bias in City Algorithms | Data-Smart ...", "url": "https://datasmart.ash.harvard.edu/news/article/algorithmic-fairness-tackling-bias-city-algorithms", "isFamilyFriendly": true, "displayUrl": "https://datasmart.ash.harvard.edu/news/article/algorithmic-fairness-tackling-bias-city...", "snippet": "This might solve the issue of <b>disparate</b> impact, but would raise the issue of <b>disparate</b> <b>treatment</b>, whereby two people are treated differently because they belong to different classes. Historically, the Courts have not looked fondly upon examples of <b>disparate</b> <b>treatment</b>, and so implementing this kind of technique would likely require a strong legal justification, especially in the context of criminal justice. However, the increases in accuracy that Ustun found in his analysis may provide enough ...", "dateLastCrawled": "2022-02-03T10:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Big Data, Machine Learning, and Bias - The Consumer Protection ...", "url": "https://www.treliant.com/knowledge-center/big-data-machine-learning-and-bias-the-consumer-protection-implications-of-emerging-technologies/", "isFamilyFriendly": true, "displayUrl": "https://www.treliant.com/knowledge-center/big-data-machine-learning-and-bias-the...", "snippet": "<b>Treatment</b> of these groups is reviewed when determining whether marketing, underwriting, pricing, or servicing decisions have resulted in a <b>disparate</b> impact on a prohibited basis, or whether there is evidence of <b>disparate</b> <b>treatment</b> during the lending process or its outcome. The Dodd-Frank Act expanded the existing prohibition against <b>unfair</b> or deceptive acts or practices to also include abusive practices. \u201cUDAAPs can cause significant financial injury to consumers, erode consumer confidence ...", "dateLastCrawled": "2022-01-27T07:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Making hospital readmission classifier fair What</b> is the cost?", "url": "http://archive.ceciis.foi.hr/app/public/conferences/2019/Proceedings/SPDM/SPDM1.pdf", "isFamilyFriendly": true, "displayUrl": "archive.ceciis.foi.hr/app/public/conferences/2019/Proceedings/SPDM/SPDM1.pdf", "snippet": "If fairness is to be achieved in <b>disparate</b> <b>treatment</b> and parity then <b>one</b> can hide sensitive attributes from the learning process. This way algorithm is unaware of sensitive information, thus learned model will not explicitly use sensitive information in the decision-making process. For some specific tasks this approach did work, but there are many unacceptable models in practice (Marx, 2005; Taslitz, 2007). <b>One</b> can find critiques of this approach because sensitive attribute can be deduced ...", "dateLastCrawled": "2021-11-07T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Algorithmic bias detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/algorithmic-bias-detection-and-mitigation-best-", "snippet": "Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms. Nicol Turner Lee, Paul Resnick, and Genie Barton Wednesday, May 22, 2019. For media inquiries ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fairness metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "The first step in the process was to formulate a research question to define the aim of the study and map answers from the literature. The research question that this semi-systematic literature review address is the RQ1, presented in Section 1.It aims to determine what concepts from classification-based fair machine learning can be mapped to fairness metrics in rating-based recommender system settings.", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "4. Fairness Pre-Processing - Practical Fairness [Book]", "url": "https://www.oreilly.com/library/view/practical-fairness/9781492075721/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/practical-fairness/9781492075721/ch04.html", "snippet": "This data set includes around 6,000 <b>data points</b>, with such sensitive attributes as race and sex. German credit data set . This widely cited data set contains <b>one</b> thousand decisions regarding whether to extend credit to an applicant. It became public in 1994 and includes around 20 inputs and sensitive attributes including sex, age, and citizenship status. AIF360 is useful because it provides an easy and accessible interface for loading data and running discrimination metrics on that data ...", "dateLastCrawled": "2022-01-13T02:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Fairness Beyond <b>Disparate</b> <b>Treatment</b> &amp; <b>Disparate</b> Impact: Learning ...", "url": "https://www.researchgate.net/publication/315871857_Fairness_Beyond_Disparate_Treatment_Disparate_Impact_Learning_Classification_without_Disparate_Mistreatment", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315871857_Fairness_Beyond_<b>Disparate</b>_<b>Treatment</b>...", "snippet": "ment, <b>disparate</b> <b>treatment</b> arises when a decision making system provides di\ufb00erent outputs for groups of people with the same (or <b>similar</b>) values of non-sensitiv e attributes (or", "dateLastCrawled": "2022-01-26T15:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Programming Fairness in Algorithms</b> | by Matthew Stewart, PhD Researcher ...", "url": "https://towardsdatascience.com/programming-fairness-in-algorithms-4943a13dd9f8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>programming-fairness-in-algorithms</b>-4943a13dd9f8", "snippet": "It is fairly easy to prevent <b>disparate</b> <b>treatment</b> in algorithms \u2014 the explicit differential <b>treatment</b> of ... it is much less easy to prevent <b>disparate</b> impact \u2014implicit differential <b>treatment</b> <b>of one</b> <b>group</b> <b>over</b> <b>another</b>, usually caused by something called redundant encodings in the data. Illustration of <b>disparate</b> impact \u2014 in this diagram the data distribution of two groups is very different, which leads to differences in the output of the algorithm without any explicit association of the ...", "dateLastCrawled": "2022-01-23T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Managing <b>bias and unfairness</b> in data for decision support: a survey of ...", "url": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "snippet": "Essentially, <b>group</b> fairness reflects averages <b>over</b> sets of individuals\u2014if the averages across groups are <b>similar</b>, then the model is considered fair\u2014while individual fairness is interested in each of the individuals and how they are treated in comparison with all other individuals\u2014while a <b>group</b> average might seem high, two individuals within the same <b>group</b> might receive <b>disparate</b> <b>treatment</b>, which in average look fair. In our example, an unfairness measure such as <b>disparate</b> impact could ...", "dateLastCrawled": "2022-02-01T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Big Data, Machine Learning, and Bias - The Consumer Protection ...", "url": "https://www.treliant.com/knowledge-center/big-data-machine-learning-and-bias-the-consumer-protection-implications-of-emerging-technologies/", "isFamilyFriendly": true, "displayUrl": "https://www.treliant.com/knowledge-center/big-data-machine-learning-and-bias-the...", "snippet": "<b>Treatment</b> of these groups is reviewed when determining whether marketing, underwriting, pricing, or servicing decisions have resulted in a <b>disparate</b> impact on a prohibited basis, or whether there is evidence of <b>disparate</b> <b>treatment</b> during the lending process or its outcome. The Dodd-Frank Act expanded the existing prohibition against <b>unfair</b> or deceptive acts or practices to also include abusive practices. \u201cUDAAPs can cause significant financial injury to consumers, erode consumer confidence ...", "dateLastCrawled": "2022-01-27T07:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Algorithmic Fairness: Tackling Bias in City Algorithms | Data-Smart ...", "url": "https://datasmart.ash.harvard.edu/news/article/algorithmic-fairness-tackling-bias-city-algorithms", "isFamilyFriendly": true, "displayUrl": "https://datasmart.ash.harvard.edu/news/article/algorithmic-fairness-tackling-bias-city...", "snippet": "This might solve the issue of <b>disparate</b> impact, but would raise the issue of <b>disparate</b> <b>treatment</b>, whereby two people are treated differently because they belong to different classes. Historically, the Courts have not looked fondly upon examples of <b>disparate</b> <b>treatment</b>, and so implementing this kind of technique would likely require a strong legal justification, especially in the context of criminal justice. However, the increases in accuracy that Ustun found in his analysis may provide enough ...", "dateLastCrawled": "2022-02-03T10:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Highly Accurate, But Still Discriminatory: A Fairness <b>Evaluation</b> ...", "url": "https://www.researchgate.net/publication/344992667_Highly_Accurate_But_Still_Discriminatory_A_Fairness_Evaluation_of_Algorithmic_Video_Analysis_in_the_Recruitment_Context", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344992667_Highly_Accurate_But_Still...", "snippet": "the <b>group</b> membership of a certain person in a certain <b>group</b> (such as \u2018 \u2018non-white\u2019 \u2019 for G \u00bc 1 and \u2018 \u2018white\u2019 \u2019 for G \u00bc 0). Then, <b>disparate</b> impact may be formalized as", "dateLastCrawled": "2021-12-02T18:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fairness metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "For both datasets, we adopt an <b>evaluation</b> strategy <b>similar</b> to Yao and Huang (2017) and follow the general idea of a 5-fold cross validation <b>evaluation</b> approach. Specifically, the Movielens dataset is divided into 5 folds, and each fold is used for testing the collaborative filtering approach (trained on the other 4 folds) exactly once. For the synthetic data, we independently create the synthetic dataset 5 times. In both cases, the reported results represent the average for the 5 test folds ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Making hospital readmission classifier fair What</b> is the cost?", "url": "http://archive.ceciis.foi.hr/app/public/conferences/2019/Proceedings/SPDM/SPDM1.pdf", "isFamilyFriendly": true, "displayUrl": "archive.ceciis.foi.hr/app/public/conferences/2019/Proceedings/SPDM/SPDM1.pdf", "snippet": "If fairness is to be achieved in <b>disparate</b> <b>treatment</b> and parity then <b>one</b> can hide sensitive attributes from the learning process. This way algorithm is unaware of sensitive information, thus learned model will not explicitly use sensitive information in the decision-making process. For some specific tasks this approach did work, but there are many unacceptable models in practice (Marx, 2005; Taslitz, 2007). <b>One</b> can find critiques of this approach because sensitive attribute can be deduced ...", "dateLastCrawled": "2021-11-07T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Federal Register :: Small Business Lending Data Collection Under the ...", "url": "https://www.federalregister.gov/documents/2021/10/08/2021-19274/small-business-lending-data-collection-under-the-equal-credit-opportunity-act-regulation-b", "isFamilyFriendly": true, "displayUrl": "https://<b>www.federalregister.gov</b>/documents/2021/10/08/2021-19274/small-business-lending...", "snippet": "These <b>data points</b> include, for all applications: A unique identifier for each application for or extension of credit; ... As of 2018, minorities owned <b>over</b> <b>one</b> million employer firms in the U.S. (amounting to 18.3 percent of all employer firms) and, as of 2017, approximately 8.2 million non-employer firms. Likewise, as of 2018, women owned about 1.1 million employer firms (19.9 percent of all employer firms) and, as of 2017, approximately 10.6 million non-employer firms. Businesses are ...", "dateLastCrawled": "2022-01-30T17:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "ARM 401 Exam Practice Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/525165208/arm-401-exam-practice-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/525165208/arm-401-exam-practice-flash-cards", "snippet": "A. <b>Data points</b>. B. Severity distributions. C. Distribution values. D. Density functions. D. In a normal distribution, fewer than 5% of outcomes are A. Between <b>one</b> and two standard deviations above or below the mean. B. Outside two standard deviations above or below the mean. C. Within <b>one</b> standard deviation above or below the mean. D. Between the mean and two standard deviations above or below the mean. B. Which <b>one</b> of the following statements about event tree analysis is true? A. Event ...", "dateLastCrawled": "2022-01-03T00:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "4 <b>Theories of Discrimination</b> | Measuring Racial <b>Discrimination</b> | The ...", "url": "https://www.nap.edu/read/10887/chapter/7", "isFamilyFriendly": true, "displayUrl": "https://www.nap.edu/read/10887/chapter/7", "snippet": "Avoidance entails choosing the comfort <b>of one</b>\u2019s own racial <b>group</b> (the \u201cingroup\u201d in social psychological terms) <b>over</b> interaction with <b>another</b> racial <b>group</b> (the \u201coutgroup\u201d). In settings of discretionary contact\u2014that is, in which people may choose to associate or not\u2014members of disadvantaged racial groups may be isolated. In social situations, people may self-segregate along racial lines. In work settings, discretionary contact may force out-<b>group</b> members into lower-status ...", "dateLastCrawled": "2022-02-03T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Does mitigating ML&#39;s <b>disparate impact require disparate treatment</b>?", "url": "https://www.researchgate.net/publication/321180707_Does_mitigating_ML's_disparate_impact_require_disparate_treatment", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321180707_Does_mitigating_ML", "snippet": "<b>Disparate</b> impact <b>can</b> arise unintentionally and absent <b>disparate</b> <b>treatment</b>. The natural way to reduce <b>disparate</b> impact would be to apply <b>disparate</b> <b>treatment</b> in favor of the disadvantaged <b>group</b>, i.e ...", "dateLastCrawled": "2021-10-19T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Programming Fairness in Algorithms</b> - TOPBOTS", "url": "https://www.topbots.com/programming-fairness-in-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/<b>programming-fairness-in-algorithms</b>", "snippet": "<b>One</b> <b>group</b> may be much more likely to be predicted as positive than <b>another</b>, and hence we might obtain large disparities between the false positive and true positive rates for each <b>group</b>. This itself <b>can</b> cause a <b>disparate</b> impact as qualified individuals from <b>one</b> <b>group</b> ( p=0 ) may be missed out in favor of unqualified individuals from <b>another</b> <b>group</b> ( p=1 ).", "dateLastCrawled": "2022-01-30T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Algorithmic Fairness in Mortgage Lending: from Absolute Conditions</b> to ...", "url": "https://link.springer.com/article/10.1007/s11023-020-09529-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11023-020-09529-4", "snippet": "<b>Disparate</b> <b>treatment</b> <b>can</b> be sub-divided into taste-based discrimination and statistical discrimination. The former is the exercise of prejudicial tastes, sometimes forfeiting profit. The latter derives from using the applicant\u2019s <b>group</b> (e.g. race) to estimate the credit-worthiness, given the expected difference in statistical distribution of credit-worthiness between groups. <b>Disparate</b> impact goes beyond the intent to discriminate and focuses on the outcome: any policy or practice that puts ...", "dateLastCrawled": "2022-01-29T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Fairness metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "Biases in algorithmic decision making <b>can</b> lead to <b>unfair</b> predictions for users in different groups, e.g., based on gender, age, or race. Fairness is generally used as a (often quantitative) measure to describe the effects of bias in such settings. Though many definitions have been coined for fairness Verma &amp; Rubin, 2018), generically it is the concept that all groups and/or users are treated equally; i.e. with equality or equity or justice (Baeza-Yates, 2020). For the subsequent ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "CONFAIR: Configurable and Interpretable Algorithmic Fairness | DeepAI", "url": "https://deepai.org/publication/confair-configurable-and-interpretable-algorithmic-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/confair-configurable-and-interpretable-algorithmic-fairness", "snippet": "The different definitions of statistical fairness lead to the <b>evaluation</b> of an algorithm\u2019s performance in fundamentally different ways. For instance, the <b>disparate</b> impact measures the ratio of true positive rates across groups while equal opportunity measures the difference between the true positive rates and false positive rates across groups with the restriction that they are less than a specified threshold. It has been shown that the different fairness metrics are often incompatible ...", "dateLastCrawled": "2022-01-22T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Testing discrimination in practice", "url": "https://fairmlbook.org/testing.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/testing.html", "snippet": "Here is a tempting argument based on the outcome test: if <b>one</b> <b>group</b> (say women) who receive loans have a lower rate of default than <b>another</b> (men), it suggests that the bank applies a higher bar for loan qualification for women. Indeed, this type of argument was the original motivation behind the outcome test. But it is a logical fallacy; sufficiency does not imply predictive parity (or vice versa). To see why, consider a <b>thought</b> experiment involving the Bayes optimal predictor. In the ...", "dateLastCrawled": "2022-01-30T14:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "4. Fairness Pre-Processing - Practical Fairness [Book]", "url": "https://www.oreilly.com/library/view/practical-fairness/9781492075721/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/practical-fairness/9781492075721/ch04.html", "snippet": "Chapter 4. Fairness Pre-Processing. As discussed in the previous chapter, fairness <b>can</b> affect three stages of the data modeling pipeline. This chapter focuses on the earliest stage, adjusting the way that data is translated into inputs for a machine learning training process, also called pre-processing the data.. The advantages of pre-processing a data set are numerous.", "dateLastCrawled": "2022-01-13T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fairer machine learning in the real world: Mitigating discrimination ...", "url": "https://journals.sagepub.com/doi/full/10.1177/2053951717743530", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/2053951717743530", "snippet": "Many of the fairness issues in machine learning are primarily <b>thought</b> to arise from data. Some think, falling for what could be called the \u2018neutrality fallacy\u2019, that machine learning will provide a more even and objective <b>treatment</b> of individuals (Sandvig, 2015).As Latour indicates, we are often more than happy to declare value-laden issues as matters of fact, and let machines settle them for us (1999).", "dateLastCrawled": "2022-01-31T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "4 Key Questions &amp; Answers on Workplace Diversity &amp; Inclusion | <b>GoodHire</b>", "url": "https://www.goodhire.com/blog/4-honest-answers-on-diversity-and-inclusion/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>goodhire</b>.com/blog/4-h<b>one</b>st-answers-on-diversity-and-inclusion", "snippet": "The ability to build a diverse workforce <b>can</b> be hindered if requirements are too stringent and have a <b>disparate</b> impact on various types of employees. That <b>can</b> impede organizations\u2019 ability to capitalize on the value of diverse viewpoints, backgrounds and experience. Today\u2019s organizations increasingly recognize that the greatest insights and innovations come not from teams of like-minded individuals, but from teams with a wide range of diverse viewpoints and perspectives. The more complex ...", "dateLastCrawled": "2022-02-02T04:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Fairness Beyond <b>Disparate</b> <b>Treatment</b> &amp; <b>Disparate</b> Impact: Learning ...", "url": "https://www.researchgate.net/publication/315871857_Fairness_Beyond_Disparate_Treatment_Disparate_Impact_Learning_Classification_without_Disparate_Mistreatment", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315871857_Fairness_Beyond_<b>Disparate</b>_<b>Treatment</b>...", "snippet": "<b>unfair</b> <b>treatment</b>) and there is no ground truth about the corr ectness of the historical decisions ( i.e. , <b>one</b> cannot tell whether a historical decision used during the training phase", "dateLastCrawled": "2022-01-26T15:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Disparity evaluation for binary classifiers</b> \u2013 Beal Projects", "url": "https://bealprojects.com/resources/disparity-evaluation-for-binary-classifiers/", "isFamilyFriendly": true, "displayUrl": "https://bealprojects.com/resources/<b>disparity-evaluation-for-binary-classifiers</b>", "snippet": "<b>Disparity evaluation for binary classifiers</b>. September 3, 2021. May 14, 2021 by Adriana Beal. Photo by T ingey Injury Law Firm on Unsplash. Imagine that your company has developed a machine learning model to bring into a decision process. The idea is to make a business process run faster in an environment of rapidly increasing volumes of data ...", "dateLastCrawled": "2022-01-28T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Managing <b>bias and unfairness</b> in data for decision support: a survey of ...", "url": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "snippet": "Essentially, <b>group</b> fairness reflects averages <b>over</b> sets of individuals\u2014if the averages across groups are similar, then the model is considered fair\u2014while individual fairness is interested in each of the individuals and how they are treated in comparison with all other individuals\u2014while a <b>group</b> average might seem high, two individuals within the same <b>group</b> might receive <b>disparate</b> <b>treatment</b>, which in average look fair. In our example, an unfairness measure such as <b>disparate</b> impact could ...", "dateLastCrawled": "2022-02-01T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "CONFAIR: Configurable and Interpretable Algorithmic Fairness | DeepAI", "url": "https://deepai.org/publication/confair-configurable-and-interpretable-algorithmic-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/confair-configurable-and-interpretable-algorithmic-fairness", "snippet": "The different definitions of statistical fairness lead to the <b>evaluation</b> of an algorithm\u2019s performance in fundamentally different ways. For instance, the <b>disparate</b> impact measures the ratio of true positive rates across groups while equal opportunity measures the difference between the true positive rates and false positive rates across groups with the restriction that they are less than a specified threshold. It has been shown that the different fairness metrics are often incompatible ...", "dateLastCrawled": "2022-01-22T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Making hospital readmission classifier fair What</b> is the cost?", "url": "http://archive.ceciis.foi.hr/app/public/conferences/2019/Proceedings/SPDM/SPDM1.pdf", "isFamilyFriendly": true, "displayUrl": "archive.ceciis.foi.hr/app/public/conferences/2019/Proceedings/SPDM/SPDM1.pdf", "snippet": "<b>Another</b> approach for <b>disparate</b> <b>treatment</b> and parity fairness is called counterfactual measures (Kusner et al., 2017). Namely, the algorithm is counterfactually fair if prediction remains the same no matter what the value of the sensitive attribute is. Machine learning fairness is often understood in term Statistical parity which <b>can</b> be interpreted such that predictions should be approximately the same for individuals across the subgroups based on a sensitive attribute (Dwork et al., 2012 ...", "dateLastCrawled": "2021-11-07T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "4 <b>Theories of Discrimination</b> | Measuring Racial <b>Discrimination</b> | The ...", "url": "https://www.nap.edu/read/10887/chapter/7", "isFamilyFriendly": true, "displayUrl": "https://www.nap.edu/read/10887/chapter/7", "snippet": "Avoidance entails choosing the comfort <b>of one</b>\u2019s own racial <b>group</b> (the \u201cingroup\u201d in social psychological terms) <b>over</b> interaction with <b>another</b> racial <b>group</b> (the \u201coutgroup\u201d). In settings of discretionary contact\u2014that is, in which people may choose to associate or not\u2014members of disadvantaged racial groups may be isolated. In social situations, people may self-segregate along racial lines. In work settings, discretionary contact may force out-<b>group</b> members into lower-status ...", "dateLastCrawled": "2022-02-03T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fairness metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "This type of approach, as <b>compared</b> to integrated in-processing approaches such as the <b>one</b> suggested by Yao and Huang (2017), has two main advantages: First, the algorithm itself does not have to be adjusted, i.e., it <b>can</b> be applied to any algorithm used for calculating the predicted ratings. Second, it is agnostic with respect to the applied fairness metric and thus <b>can</b> be readily applied to any quantitative fairness metric definition that is considered important for a given scenario.", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Fairer machine learning in the real world: Mitigating discrimination ...", "url": "https://journals.sagepub.com/doi/full/10.1177/2053951717743530", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/2053951717743530", "snippet": "Decisions based on algorithmic, machine learning models <b>can</b> be <b>unfair</b>, reproducing biases in historical data used to train them. While computational techniques are emerging to address aspects of these concerns through communities such as discrimination-aware data mining (DADM) and fairness, accountability and transparency machine learning (FATML), their practical implementation faces real-world challenges.", "dateLastCrawled": "2022-01-31T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "4. Fairness Pre-Processing - Practical Fairness [Book]", "url": "https://www.oreilly.com/library/view/practical-fairness/9781492075721/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/practical-fairness/9781492075721/ch04.html", "snippet": "Chapter 4. Fairness Pre-Processing. As discussed in the previous chapter, fairness <b>can</b> affect three stages of the data modeling pipeline. This chapter focuses on the earliest stage, adjusting the way that data is translated into inputs for a machine learning training process, also called pre-processing the data.. The advantages of pre-processing a data set are numerous.", "dateLastCrawled": "2022-01-13T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Algorithmic bias detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/algorithmic-bias-detection-and-mitigation-best-", "snippet": "Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms. Nicol Turner Lee, Paul Resnick, and Genie Barton Wednesday, May 22, 2019. For media inquiries ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>learning</b> and applications in microbiology", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8498514/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8498514", "snippet": "<b>Machine</b> <b>learning</b> has two main <b>learning</b> modes: supervised (also known as predictive) to make future predictions from training data, and unsupervised (descriptive), which is exploratory in nature without training data, defined target or output (Mitchell 1997). Training data are the initial information used to teach supervised ML algorithms in the process of developing a model, from which the model creates and refines its rules required for prediction. Typically, training data comprises a set ...", "dateLastCrawled": "2021-12-06T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b>: Science and Technology, Volume 2, Number 1, March ...", "url": "https://iopscience.iop.org/issue/2632-2153/2/1", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/issue/2632-2153/2/1", "snippet": "<b>Machine</b> <b>learning</b> has been used in high energy physics (HEP) for a long time, primarily at the analysis level with supervised classification. Quantum computing was postulated in the early 1980s as way to perform computations that would not be tractable with a classical computer. With the advent of noisy intermediate-scale quantum computing devices, more quantum algorithms are being developed with the aim at exploiting the capacity of the hardware for <b>machine</b> <b>learning</b> applications. An ...", "dateLastCrawled": "2021-12-16T00:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Realistically Integrating <b>Machine</b> <b>Learning</b> Into Clinical Pra ...", "url": "https://journals.lww.com/anesthesia-analgesia/fulltext/2020/05000/realistically_integrating_machine_learning_into.4.aspx", "isFamilyFriendly": true, "displayUrl": "https://<b>journals.lww.com</b>/.../05000/realistically_integrating_<b>machine</b>_<b>learning</b>_into.4.aspx", "snippet": "<b>Machine</b>-<b>learning</b> models have been created to predict an increasing number of clinical outcomes, such as diagnoses and mortality, with applications including C. difficile infection in the inpatient hospital setting, 2 identifying molecular markers for cancer treatments, 3 and postoperative surgical outcomes. 4 Examples of <b>machine</b> <b>learning</b> include a cardiologist using an automated interpretation of an ECG and a radiologist using an automated detection of a lung nodule in a chest x-ray. In both ...", "dateLastCrawled": "2021-11-22T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning</b>, artificial neural networks and social research ...", "url": "https://link.springer.com/article/10.1007/s11135-020-01037-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11135-020-01037-y", "snippet": "<b>Machine learning</b> (ML), and particularly algorithms based on artificial neural networks (ANNs), constitute a field of research lying at the intersection of different disciplines such as mathematics, statistics, computer science and neuroscience. This approach is characterized by the use of algorithms to extract knowledge from large and heterogeneous data sets. In addition to offering a brief introduction to ANN algorithms-based ML, in this paper we will focus our attention on its possible ...", "dateLastCrawled": "2022-01-27T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Immune System Computes the State of the Body: Crowd Wisdom, <b>Machine</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fimmu.2019.00010/full", "isFamilyFriendly": true, "displayUrl": "https://www.<b>frontiers</b>in.org/articles/10.3389/fimmu.2019.00010", "snippet": "The correlations between the components comprising a set of data can be very subtle and obscure to the human observer, yet such correlations are detectable by <b>machine</b> <b>learning</b> algorithms, and, by <b>analogy</b>, by networks of cells and antibodies in the immune system. As a consequence of exposure to training sets of input, the computer algorithm\u2014and the immune system\u2014can accumulate a bank of learned correlations. These formative correlations can then be used by the computer or the repertoire ...", "dateLastCrawled": "2022-01-30T04:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Protein function in precision medicine: deep understanding with <b>machine</b> ...", "url": "https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.12307", "isFamilyFriendly": true, "displayUrl": "https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.12307", "snippet": "<b>disparate</b> parts may ultimately lead to the same observable effect. In this <b>analogy</b>, we might argue that medicine has so far been often investing into mitigat- ing the inconvenience with lemons and much less into improving and augmenting the protocols for \ufb01nding the individual causes of problems. In his recent State-of-the-Union address, the US Pres-ident Barack Obama announced the Precision Medicine Initiative, making this challenge a national and interna-tional priority. Precision ...", "dateLastCrawled": "2021-12-12T19:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Solution Manual Alpaydin Introduction To <b>Machine</b> <b>Learning</b>", "url": "https://se.rangle.io/solution+manual+alpaydin+introduction+to+machine+learning+pdf", "isFamilyFriendly": true, "displayUrl": "https://se.rangle.io/solution+manual+alpaydin+introduction+to+<b>machine</b>+<b>learning</b>+pdf", "snippet": "With in-depth Python and MATLAB/OCTAVE-based computational exercises and a complete <b>treatment</b> of cutting edge numerical optimization techniques, this is an essential resource for students and an ideal reference for researchers and practitioners working in <b>machine</b> <b>learning</b>, computer science, electrical engineering, signal processing, and numerical optimization. Handbook of Research on Innovations in Database Technologies and Applications One of the currently most active research areas within ...", "dateLastCrawled": "2022-01-11T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Latent bias and the implementation of <b>artificial intelligence</b> in ...", "url": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "snippet": "<b>Artificial intelligence</b> (AI) in general, and <b>machine</b> <b>learning</b> in particular, by all accounts, appear poised to revolutionize medicine. 1\u20133 With a wide spectrum of potential uses across translational research (from bench to bedside to health policy), clinical medicine (including diagnosis, <b>treatment</b>, prediction, and healthcare resource allocation), and public health, every area of medicine will be affected.", "dateLastCrawled": "2022-01-28T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "47 <b>Analogy</b> Examples To Make You As Sharp As A Tack (and then some)", "url": "https://www.greetingcardpoet.com/good-analogy-examples-and-definition/", "isFamilyFriendly": true, "displayUrl": "https://www.greetingcardpoet.com/<b>good-analogy-examples-and-definition</b>", "snippet": "The essence of this literary device is to set up a comparison that highlights similarities between two seemingly <b>disparate</b> items. It is by examining how the two items are alike in some way that leads to a clear understanding. Differences between Similes, Metaphors, and Analogies . While similes, metaphors, and analogies are similar in that they all compare two different things, similes and metaphors are figures of speech. In contrast, an <b>analogy</b> is more akin to a logical argument. A writer ...", "dateLastCrawled": "2022-02-02T03:57:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A primer on AI <b>fairness</b>. What it is and the tradeoffs to be made | by ...", "url": "https://towardsdatascience.com/artificial-intelligence-fairness-and-tradeoffs-ce11ac284b63", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/artificial-intelligence-<b>fairness</b>-and-tradeoffs-ce11ac284b63", "snippet": "A <b>machine</b> <b>learning</b> algorithms value is being able to increase the number of true positives and true negatives, which each have a value attached. Each false positive and false negative is costly. The value assigned to each depends on each context. A false negative is more costly in medical situations while a false positive is costlier in death penalty decisions. Expected value is profits that businesses can expect from using the algorithm. The more accurate the model, the higher the profits.", "dateLastCrawled": "2022-02-02T02:42:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(disparate treatment)  is like +(unfair evaluation of one group of data-points over another)", "+(disparate treatment) is similar to +(unfair evaluation of one group of data-points over another)", "+(disparate treatment) can be thought of as +(unfair evaluation of one group of data-points over another)", "+(disparate treatment) can be compared to +(unfair evaluation of one group of data-points over another)", "machine learning +(disparate treatment AND analogy)", "machine learning +(\"disparate treatment is like\")", "machine learning +(\"disparate treatment is similar\")", "machine learning +(\"just as disparate treatment\")", "machine learning +(\"disparate treatment can be thought of as\")", "machine learning +(\"disparate treatment can be compared to\")"]}