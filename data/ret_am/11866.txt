{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Guide to Generalization and Regularization in Machine Learning", "url": "https://analyticsindiamag.com/a-guide-to-generalization-and-regularization-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/a-guide-to-generalization-and-regularization-in-machine...", "snippet": "The <b>L2</b> norm is used for regularization in this sort of regularization. As a punishment, it employs the <b>L2</b>-norm. The <b>L2</b> <b>penalty</b> is equal to the square of the magnitudes of the beta coefficients. It is also referred to as <b>L2</b>-regularization. <b>L2</b> reduces the coefficients but never brings them to zero. <b>L2</b> regularization produces non-sparse results.", "dateLastCrawled": "2022-01-31T23:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Regularization techniques</b>. Regularization in a Neural Network\u2026 | by ...", "url": "https://laabidigh.medium.com/regularization-techniques-9db4214bfb4a", "isFamilyFriendly": true, "displayUrl": "https://laabidigh.medium.com/<b>regularization-techniques</b>-9db4214bfb4a", "snippet": "the <b>L2</b> regularization adds the squared value of coefficient as <b>penalty</b> term to the <b>loss</b> function. It is also called ridge regression. The main difference between L1 and <b>L2</b> is that Lasso shrinks the less important feature\u2019s coefficient to zero therefore, removing the feature completely . while <b>L2</b> minimizes the value of the irrelevant weights enough too be close to 0 but still exist in the calculation. In L1 regularization, to minimize the <b>loss</b> function, we try to estimate a value that ...", "dateLastCrawled": "2022-01-31T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "While other <b>loss</b> functions <b>like</b> squared <b>loss</b> penalize wrong predictions, cross entropy gives a greater <b>penalty</b> when incorrect predictions are predicted with high confidence. What differentiates it ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction to Machine Learning", "url": "https://las.inf.ethz.ch/courses/introml-s20/slides/introml-06-featureselection.pdf", "isFamilyFriendly": true, "displayUrl": "https://las.inf.ethz.ch/courses/introml-s20/slides/introml-06-featureselection.pdf", "snippet": "<b>l2</b>-r <b>l2</b>-r <b>Loss</b> funct. <b>Loss</b> funct. <b>Loss</b> funct. Supervised learning summary so far 20 Model/ objective: <b>Loss</b>-function + Regularization Squared <b>loss</b>, 0/1 <b>loss</b>, Perceptron <b>loss</b>, Hinge <b>loss</b> <b>L2</b> norm Method: Exact solution, Gradient Descent, (mini-batch) SGD, Convex Programming, \u2026 Model selection:K-fold Cross-Validation, Monte Carlo CV Representation/ features Linear hypotheses; nonlinear hypotheses with nonlinear feature transforms Evaluation metric: Mean squared error, Accuracy. Introduction to ...", "dateLastCrawled": "2021-09-30T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Project 1: Gradent Descent for Logistic Regression from Scratch ...", "url": "https://www.cs.tufts.edu/comp/135/2019s/proj1.html", "isFamilyFriendly": true, "displayUrl": "https://www.cs.tufts.edu/comp/135/2019s/proj1.html", "snippet": "Implementing calc_<b>loss</b> method Implementing calc_grad method Implementing fit method Implementing predict_proba method Problem 1: Toy ... Consider 5 possible values for <b>L2</b> <b>penalty</b> strength \\(\\alpha\\) (denoted as alpha in the code): 0.0001, 0.01, 1.0, 100.0, 10000.0 For each value of \\(\\alpha\\), fit a fresh LRGradientDescent model to each of the K possible train/validation splits of your data (each split sets one fold aside as validation, with the remaining K-1 folds for training). After ...", "dateLastCrawled": "2021-09-03T19:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "python - Is this a bug of the sklearn logistic regression? - Stack Overflow", "url": "https://stackoverflow.com/questions/60006968/is-this-a-bug-of-the-sklearn-logistic-regression", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/60006968", "snippet": "It has by default <b>a penalty</b>=&#39;<b>l2</b>&#39;, which is a regularisation of the coefficient estimates, a &#39;Ridge&#39;-<b>like</b> or LASSO-<b>like</b> penalisation. If I set <b>penalty</b>=&#39;none&#39; I get the expected results! \u2013 Konstantinos Paraschakis. Feb 10 &#39;20 at 10:49. Add a comment | Active Oldest Votes. Know someone who can answer? Share a link to this question via email, Twitter, or Facebook. Your Answer Thanks for contributing an answer to Stack Overflow! Please be sure to answer the question. Provide details and share ...", "dateLastCrawled": "2022-01-18T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Inference and Decisions \u2014 Introduction to Computational Statistics with ...", "url": "https://sjster.github.io/introduction_to_computational_statistics/docs/Production/Fundamentals-lecture5b-inference-decisions-unGRADED.html", "isFamilyFriendly": true, "displayUrl": "https://sjster.github.io/introduction_to_computational_statistics/docs/Production/...", "snippet": "Knowing we are going to make <b>mistakes</b>, we introduce the idea of a <b>loss</b> function. <b>Loss</b> functions (also called cost functions or as a positive, a utility functions), is a function that we create that captures the goal of minimizing errors, but allows for differences in magnitude for consequences of the <b>mistakes</b>. The goal is now to minimize the average <b>loss</b>: \\[E[<b>loss</b>] = \\sum_k \\sum_j \\int_{\\mathbb{R_j}} L_{kj}p(x,C_k) dx\\] <b>Loss</b> functions\u00b6 <b>Loss</b> functions are functions created to match our goals ...", "dateLastCrawled": "2022-01-30T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "L1 Regularization or Lasso or L1 norm - 911 WeKnow", "url": "https://911weknow.com/l1-regularization-or-lasso-or-l1-norm", "isFamilyFriendly": true, "displayUrl": "https://911weknow.com/l1-regularization-or-lasso-or-l1-norm", "snippet": "L1 regularization is also referred as L1 norm or Lasso. In L1 norm we shrink the parameters to zero. When input features have weights closer to zero that leads to sparse L1 norm. In Sparse solution majority of the input features have zero weights and very few features have non zero weights. To predict ACT score not all input features have the ...", "dateLastCrawled": "2022-01-23T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Regularization techniques for training deep</b> neural networks | AI Summer", "url": "https://theaisummer.com/regularization/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/regularization", "snippet": "Similarly to the previous methods, we add <b>a penalty</b> term to the <b>loss</b> function. ... The best he can do is something <b>like</b> [0.0003, 0.999, 0.0003, 0.0003]. As a result, the model will continue to be trained, pushing the output values as high and as low as possible. The model will never converge. That, of course, will cause overfitting. To address that, label smoothing replaces the hard 0 and 1 targets by a small margin. Specifically, 0 are replaced with \u03f5 k \u2212 1 \\frac{\\epsilon}{ k-1} k \u2212 1 ...", "dateLastCrawled": "2022-02-03T12:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>You are killing the server</b> - General Classic Discussion - Forums", "url": "https://forums.lineage2.com/topic/14029-you-are-killing-the-server/", "isFamilyFriendly": true, "displayUrl": "https://forums.lineage2.com/topic/14029-<b>you-are-killing-the-server</b>", "snippet": "But it is literally the worst <b>L2</b> server i have ever played. It <b>is like</b> NCSoft West intentionally wants to kill this server. I have 4 different VIP 4 characters in this game and have decided to give it a chance before i quit. I still have hope. So i decided to list some of the big problems i observed in hope that it can reach someone <b>making</b> decisions (although unlikely). - You advertise the server as free to play and with no Pay-to-Win items. This is a lie and very misleading. How is a server ...", "dateLastCrawled": "2021-12-25T14:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "If x &gt; 0 <b>loss</b> will be x itself (higher value), if 0&lt;x&lt;1 <b>loss</b> will be 1 \u2014 x (smaller value) and if x &lt; 0 <b>loss</b> will be 0 (minimum value). For y =1, the <b>loss</b> is as high as the value of x .", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "python - Captcha recognizing with convnet, how to define <b>loss</b> function ...", "url": "https://stackoverflow.com/questions/38724286/captcha-recognizing-with-convnet-how-to-define-loss-function", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/38724286", "snippet": "The accuracy graph looks weird, however, note that your cross entropy <b>loss</b> is huge, when it&#39;s in the millions, then decreases in cross-entropy won&#39;t necessarily improve accuracy. I would add <b>L2</b> <b>penalty</b> regularizer and try to wait until cross entropy is closer to zero. Also it helps to start with simpler problem (ie, only digits) to get a sense ...", "dateLastCrawled": "2022-01-26T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Support Vector Machine and <b>regularization</b>", "url": "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/lec4.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/electrical-engineering-and-computer-science/6-867-machine...", "snippet": "before, we have a base <b>loss</b> function, here log[1 + exp(\u2212z)] (Figure 1b), <b>similar</b> to the hinge <b>loss</b> (Figure 1a), and this <b>loss</b> depends only on the value of the \u201cmargin\u201d y t(\u03b8T x t + \u03b8 0) for each example. The di\ufb00erence here is that we have a clear probabilistic interpretation of the \u201cstrength\u201d of the prediction, i.e., how high P (y ...", "dateLastCrawled": "2022-01-30T04:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "machine learning - What is the way to understand Proximal Policy ...", "url": "https://stackoverflow.com/questions/46422845/what-is-the-way-to-understand-proximal-policy-optimization-algorithm-in-rl", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/46422845", "snippet": "With PPO, we simplify the problem by turning the KL divergence from a constraint <b>to a penalty</b> term, <b>similar</b> to for example to L1, <b>L2</b> weight <b>penalty</b> (to prevent a weights from growing large values). PPO makes additional modifications by removing the need to compute KL divergence all together, by hard clipping the policy ratio (ratio of updated policy with old) to be within a small range around 1.0, where 1.0 means the new policy is same as old.", "dateLastCrawled": "2022-01-23T12:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to prevent overfitting \u00b7 GitBook", "url": "https://ztlevi.github.io/Gitbook_Machine_Learning_Questions/basics/how_to_prevent_overfitting.html", "isFamilyFriendly": true, "displayUrl": "https://ztlevi.github.io/Gitbook_Machine_Learning_Questions/basics/how_to_prevent_over...", "snippet": "From the figure, we can find out that for L1, the gradient is either 1 or -1, except for when w 1 = 0 w_{1}=0 w 1 = 0, however, for the same \u03bb \\lambda \u03bb, it is possible that the weight for <b>L2</b> norm will never reach zero for the gradient of the weight is also very small, which may result in a smaller <b>penalty</b> for the weight. Choosing \u03bb. Plot \u03bb vs.", "dateLastCrawled": "2021-12-28T02:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "6 Common <b>Spanning Tree Mistakes</b> and How to Avoid Them", "url": "https://www.auvik.com/franklyit/blog/spanning-tree-mistakes/", "isFamilyFriendly": true, "displayUrl": "https://www.auvik.com/franklyit/blog/<b>spanning-tree-mistakes</b>", "snippet": "Following <b>L2</b> VLAN are presents on respective switches: Core Switches: 32,33,35,36,38,39,40,41,136,138,139,142,1000,2000 (One of the Core switch priority is 0 for mst instance 0, so that is root bridge) First <b>L2</b> switch: 32,33,35,40,41,142 Second <b>L2</b> switch: 32,33,36,38,39,41,138,139,142. So my query is: 1. Am I supposed to pass all the unique ...", "dateLastCrawled": "2022-02-02T06:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Adam</b> \u2014 latest trends in deep learning optimization. | by Vitaly Bushaev ...", "url": "https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>adam</b>-latest-trends-in-deep-learning-optimization-6be9a...", "snippet": "When using <b>L2</b> regularization the <b>penalty</b> we use for large weights gets scaled by moving average of the past and current squared gradients and therefore weights with large typical gradient magnitude are regularized by a smaller relative amount than other weights. In contrast, weight decay regularizes all weights by the same factor. To use weight decay with <b>Adam</b> we need to modify the update rule as follows:", "dateLastCrawled": "2022-02-02T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Increase Validation Accuracy in Cases of High Variance - programmer \u2229 ...", "url": "https://avishaan.com/machine%20learning/overfitting-accuracy/", "isFamilyFriendly": true, "displayUrl": "https://avishaan.com/machine learning/overfitting-accuracy", "snippet": "Increase Validation Accuracy in Cases of High Variance 3 minute read On this page. Intro; Strategies. Cost Regularization. Ridge - <b>L2</b> Regularization", "dateLastCrawled": "2021-12-08T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Learning <b>from Mistakes: What do Inconsistent Choices</b> Over Risk ...", "url": "https://www.researchgate.net/publication/46547721_Learning_from_Mistakes_What_do_Inconsistent_Choices_Over_Risk_Tell_us", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/46547721_Learning_from_<b>Mistakes</b>_What_do...", "snippet": "<b>Making</b> <b>mistakes</b> in one task may be linked to less than . optimal behavior in another. Being able to observe these types of behavioral biases is. essential to understanding financial decisions. The ...", "dateLastCrawled": "2021-10-22T09:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "1.1. Linear Models \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/linear_model.html", "snippet": "1.1.3.1. Setting regularization parameter\u00b6. The alpha parameter controls the degree of sparsity of the estimated coefficients.. 1.1.3.1.1. Using cross-validation\u00b6. scikit-learn exposes objects that set the Lasso alpha parameter by cross-validation: LassoCV and LassoLarsCV. LassoLarsCV is based on the Least Angle Regression algorithm explained below.. For high-dimensional datasets with many collinear features, LassoCV is most often preferable. However, LassoLarsCV has the advantage of ...", "dateLastCrawled": "2022-02-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to prevent overfitting \u00b7 GitBook", "url": "https://ztlevi.github.io/Gitbook_Machine_Learning_Questions/basics/how_to_prevent_overfitting.html", "isFamilyFriendly": true, "displayUrl": "https://ztlevi.github.io/Gitbook_Machine_Learning_Questions/basics/how_to_prevent_over...", "snippet": "Shrinkage <b>can</b> <b>be thought</b> <b>of as &quot;a penalty</b> of complexity.&quot; Why? If we set some parameters of the model to exactly zero, then the model is effectively shrunk to have lower-dimensionality and less complex. Analogously, if we use a shrinkage mechanism to zero out some of the parameters or smooth the parameters (the difference of parameters will not be very large), then we are decreasing complexity by reducing dimensions or <b>making</b> it more continuous. L1 Regularization or Lasso or L1 norm. L (x, y ...", "dateLastCrawled": "2021-12-28T02:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Lecture 3: Regularization For Deep Models", "url": "http://wavelab.uwaterloo.ca/wp-content/uploads/2017/04/Lecture_3.pdf", "isFamilyFriendly": true, "displayUrl": "wavelab.uwaterloo.ca/wp-content/uploads/2017/04/Lecture_3.pdf", "snippet": "This sparsity property <b>can</b> <b>be thought</b> of as a feature selection mechanism. 16/64. ME 780 Regularization Strategies: Parameter Norm Penalties Conclusion <b>L2</b> norm <b>penalty</b> <b>can</b> be interpreted as a MAP Bayesian Inference with a Gaussian prior on the weights. On the other hand, L1 norm <b>penalty</b> <b>can</b> be interpreted as a MAP Bayesian Inference with a Isotropic Laplace Distribution prior on the weights. 17/64. ME 780 Regularization Strategies: Dataset Augmentation Section 3 Regularization Strategies ...", "dateLastCrawled": "2022-01-25T19:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Project 1: Gradent Descent for Logistic Regression from Scratch ...", "url": "https://www.cs.tufts.edu/comp/135/2019s/proj1.html", "isFamilyFriendly": true, "displayUrl": "https://www.cs.tufts.edu/comp/135/2019s/proj1.html", "snippet": "Consider 5 possible values for <b>L2</b> <b>penalty</b> strength \\(\\alpha\\) (denoted as alpha in the code): 0.0001, 0.01, 1.0, 100.0, 10000.0 For each value of \\(\\alpha\\) , fit a fresh LRGradientDescent model to each of the K possible train/validation splits of your data (each split sets one fold aside as validation, with the remaining K-1 folds for training).", "dateLastCrawled": "2021-09-03T19:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>LASSO Regression</b>: A Complete Understanding (2021)", "url": "https://www.jigsawacademy.com/blogs/ai-ml/lasso-regression", "isFamilyFriendly": true, "displayUrl": "https://www.jigsawacademy.com/blogs/ai-ml/<b>lasso-regression</b>", "snippet": "Ridge regression is also known as <b>L2</b> Regularization. But let us understand the difference between ridge and <b>lasso regression</b>: Ridge regression has an introduction of a small level of bias to get long-term predictions. This amount of bias is known as the Ridge Regression <b>penalty</b>. By the addition of the <b>penalty</b> term, the alteration of the cost function takes place. As discussed earlier, the <b>penalty</b> term in <b>lasso regression</b> contains the absolute weights. Therefore due to the use of absolute ...", "dateLastCrawled": "2022-02-02T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lecture+5+-+Regularization+and+Logistic+Regression.pdf - General ...", "url": "https://www.coursehero.com/file/128225620/Lecture5-RegularizationandLogisticRegressionpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/128225620/Lecture5-RegularizationandLogisticRegressionpdf", "snippet": "\u2022 Recall <b>can</b> <b>be thought</b> of as a measure of a classifiers completeness. \u2022 A low recall indicates many False Negatives. Accuracy \u2022 Don\u2019t be tricked by high accuracy \u2022 Accuracy alone doesn&#39;t tell the full story when you&#39;re working with a class-imbalanced data set , where there is a significant disparity between the number of positive and negative labels.", "dateLastCrawled": "2022-02-02T07:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(DOC) The Pros <b>and Cons of Capital Punishment</b> | Pauline Tracy Batiles ...", "url": "https://www.academia.edu/9775156/The_Pros_and_Cons_of_Capital_Punishment", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/9775156/The_Pros_<b>and_Cons_of_Capital_Punishment</b>", "snippet": "Death <b>Penalty</b> <b>Can</b> Prolong Suffering for Victims&#39; Families Many family members who have lost love ones to murder feel that the death <b>penalty</b> will not heal their wounds nor will it end their pain; the extended legal process prior to executions <b>can</b> prolong the agony experienced by the victims&#39; families. International Views on the Death <b>Penalty</b> The vast majority of countries in Western Europe, North America and South America - more than 139 nations worldwide - have abandoned capital punishment ...", "dateLastCrawled": "2022-02-03T01:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: Data ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "Precision <b>can</b> <b>be thought</b> of as a measure of a classifiers exactness. A low precision <b>can</b> also indicate a large number of False Positives. Recall: A measure of a classifiers completeness. Recall is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called Sensitivity or the True Positive Rate. Recall <b>can</b> be ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "regression - <b>How is L1 regularization derived</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/210849/how-is-l1-regularization-derived", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/210849/<b>how-is-l1-regularization-derived</b>", "snippet": "But here I don&#39;t understand why is derivative of a <b>loss</b> function is divided by itself? <b>Can</b> some one please help me with this? derivative of (3) should be $2X^T(Xw-y)$ dividing it by (2) should be $2X^T(Xw-y) / \\sum\\limits_{i=1}^n(y_i-w_0 - \\sum\\limits_{j=1}^p x_{ij}w_j)^2$ firs of all I don&#39;t understand why is it done? what is the purpose of this step and also I don&#39;t see how that is simplified to (4). regression lasso regularization ridge-regression. Share. Cite. Improve this question ...", "dateLastCrawled": "2022-01-15T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "11 <b>mistakes that can get your</b> visa denied (and how to avoid them ...", "url": "https://www.visatraveler.com/blog/mistakes-that-can-get-your-visa-application-denied/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>visatraveler</b>.com/blog/mi", "snippet": "Being smart and avoiding these <b>mistakes</b> will spare you time, money and visa denials. So, without further due, let\u2019s look at these 11 <b>mistakes that can get your</b> visa denied. And the ways to avoid them. $14.95. ***. 01. Not following the visa rules strictly. The visa rules are there for a reason.", "dateLastCrawled": "2022-02-02T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>March-June 2019 Learning</b> | Thoughts on learning and work", "url": "https://bessiechu.wordpress.com/2019/07/08/march-june-2019-learning/", "isFamilyFriendly": true, "displayUrl": "https://bessiechu.wordpress.com/2019/07/08/<b>march-june-2019-learning</b>", "snippet": "Deal ID <b>can</b> <b>be thought</b> of as an automated insertion order, better flexibility but controlling for the parameters of an ad deal. Responses to Negative Data: Four Senior Leadership Archetypes. Most senior leaders in org came up when data wasn\u2019t so accurate and available; You have bubble kings who ignore the data and Attackers on the other end Deal with Bubbles: form relationships and justify decisions. Deal with Attackers: get out or provide solutions and not just data; Rationalizer who sow ", "dateLastCrawled": "2021-10-13T13:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>LASSO Regression</b>: A Complete Understanding (2021)", "url": "https://www.jigsawacademy.com/blogs/ai-ml/lasso-regression", "isFamilyFriendly": true, "displayUrl": "https://www.jigsawacademy.com/blogs/ai-ml/<b>lasso-regression</b>", "snippet": "Ridge regression is also known as <b>L2</b> Regularization. But let us understand the difference between ridge and <b>lasso regression</b>: Ridge regression has an introduction of a small level of bias to get long-term predictions. This amount of bias is known as the Ridge Regression <b>penalty</b>. By the addition of the <b>penalty</b> term, the alteration of the cost function takes place. As discussed earlier, the <b>penalty</b> term in <b>lasso regression</b> contains the absolute weights. Therefore due to the use of absolute ...", "dateLastCrawled": "2022-02-02T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is <b>Regression</b> and how it works | <b>Definition of Regression</b>", "url": "https://www.mygreatlearning.com/blog/what-is-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/what-is-<b>regression</b>", "snippet": "<b>L2</b> <b>Loss</b> function or <b>L2</b> Regularization; In <b>L2</b> regularization we try to minimize the objective function by adding a <b>penalty</b> term to the sum of the squares of coefficients. Ridge <b>Regression</b> or shrinkage <b>regression</b> makes use of <b>L2</b> regularization. This model assumes the square of the absolute values if coefficient.", "dateLastCrawled": "2022-02-02T12:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Classification - Data Science in Python", "url": "https://mapattacker.github.io/datascience/model-supervised1/", "isFamilyFriendly": true, "displayUrl": "https://mapattacker.github.io/datascience/model-supervised1", "snippet": "from sklearn.svm import LinearSVC clf = LinearSVC (<b>penalty</b> = &#39;<b>l2</b>&#39;, <b>loss</b> = &#39;squared_hinge&#39;, C = 1.0) Logistic Regression. While it is a type of regression, it only outputs a binary value hence it is considered a classification model. Key hyperparameter(s) Desc; <b>penalty</b>: l1/<b>l2</b>/elasticnet: C: lower C more regularization: from sklearn.linear_model import LogisticRegression clf = LogisticRegression (C = 100). fit (X_train, y_train) acc_train = clf. score (X_train, y_train) acc_test = clf. score ...", "dateLastCrawled": "2022-01-06T07:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "If x &gt; 0 <b>loss</b> will be x itself (higher value), if 0&lt;x&lt;1 <b>loss</b> will be 1 \u2014 x (smaller value) and if x &lt; 0 <b>loss</b> will be 0 (minimum value). For y =1, the <b>loss</b> is as high as the value of x .", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "1.1. Linear Models \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/linear_model.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/linear_model.html", "snippet": "1.1.3.1. Setting regularization parameter\u00b6. The alpha parameter controls the degree of sparsity of the estimated coefficients.. 1.1.3.1.1. Using cross-validation\u00b6. scikit-learn exposes objects that set the Lasso alpha parameter by cross-validation: LassoCV and LassoLarsCV. LassoLarsCV is based on the Least Angle Regression algorithm explained below.. For high-dimensional datasets with many collinear features, LassoCV is most often preferable. However, LassoLarsCV has the advantage of ...", "dateLastCrawled": "2022-02-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "Although an MLP is used in these examples, the same <b>loss</b> functions <b>can</b> be used when training CNN and RNN models for binary classification. Binary Cross-Entropy <b>Loss</b>. Cross-entropy is the default <b>loss</b> function to use for binary classification problems. It is intended for use with binary classification where the target values are in the set {0, 1}.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Nominal stance construction in L1 and</b> <b>L2</b> students&#39; writing ...", "url": "https://www.academia.edu/14734886/Nominal_stance_construction_in_L1_and_L2_students_writing", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/14734886/<b>Nominal_stance_construction_in_L1_and</b>_<b>L2</b>_students...", "snippet": "(7) They are under the false assumption that many more people would commit more murders with only the threat of life imprisonment, than if the death <b>penalty</b> was implemented. &lt;L1&gt; (8) With these three advantages of reading a book, you may support my opinion that books will never be substituted by Internet. &lt;<b>L2</b>&gt; (9) Because the men&#39;s teams serve as the only role models, women have a better chance of avoiding many of the <b>mistakes</b> made by the men teams. &lt;L1&gt; (10) In conclusion, it is our duty to ...", "dateLastCrawled": "2022-01-23T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "\u201c<b>Deep learning without going down the rabbit</b> holes. (Part 2)\u201d", "url": "https://jhui.github.io/2017/03/17/Deep-learning-tutorial-2/", "isFamilyFriendly": true, "displayUrl": "https://jhui.github.io/2017/03/17/Deep-learning-tutoria<b>l-2</b>", "snippet": "iteration 0: <b>loss</b>=553.5 layer 0: gradient = 2.337481559834108e-05 layer 1: gradient = 0.00010808796151264163 <b>layer 2</b>: gradient = 0.0012733936924033608 layer 3: gradient = 0.01758514040640722 layer 4: gradient = 0.20165907211476816 layer 5: gradient = 3.3937365923146308 layer 6: gradient = 49.335409914253 iteration 1000: <b>loss</b>=170.4 layer 0: gradient = 0.0005143399278199742 layer 1: gradient = 0.0031069449720360883 <b>layer 2</b>: gradient = 0.03744160389724748 layer 3: gradient = 0.7458109132993136 ...", "dateLastCrawled": "2021-11-29T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "assembly - Why is <b>x86</b> ugly? Why is it considered inferior when <b>compared</b> ...", "url": "https://stackoverflow.com/questions/2679882/why-is-x86-ugly-why-is-it-considered-inferior-when-compared-to-others", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/2679882", "snippet": "&quot;<b>Compared</b> to most architectures, <b>X86</b> sucks pretty badly.&quot; &quot;It&#39;s definitely the conventional wisdom that <b>X86</b> is inferior to MIPS, SPARC ... the slight power <b>penalty</b> for <b>x86</b> decode is a reason not to use <b>x86</b> for something like a mobile phone, but that&#39;s little argument for a full sized desktop or notebook. \u2013 Billy ONeal. Apr 23 &#39;10 at 3:31 . 5 @Billy: size is more than just code size or instruction size. Intel pays quite a <b>penalty</b> in chip surface area to implement the hardware logic for all ...", "dateLastCrawled": "2022-01-25T00:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) Learning <b>from Mistakes: What do Inconsistent Choices</b> Over Risk ...", "url": "https://www.researchgate.net/publication/46547721_Learning_from_Mistakes_What_do_Inconsistent_Choices_Over_Risk_Tell_us", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/46547721_Learning_from_<b>Mistakes</b>_What_do...", "snippet": "<b>Mistakes</b>, or inconsistent choic es, have been linked to non-co gnitive abilities, e.g. motivation, lack of attention, and impatience. And both cognitive and n on-cognitive. ability have been ...", "dateLastCrawled": "2021-10-22T09:29:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Regularization \u2014 Understanding L1 and <b>L2</b> regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-<b>l2</b>...", "snippet": "Understanding what regularization is and why it is required for <b>machine</b> <b>learning</b> and diving deep to clarify the importance of L1 and <b>L2</b> regularization in Deep <b>learning</b>.", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why <b>Machine Learning Is A Metaphor For</b> Life \u2013 Adit Deshpande ...", "url": "https://adeshpande3.github.io/Why-Machine-Learning-is-a-Metaphor-For-Life", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/Why-<b>Machine-Learning-is-a-Metaphor-For</b>-Life", "snippet": "Well, in <b>machine</b> <b>learning</b>, we use regularizers. The first form (and the most common) of regularization that I first learned about was <b>L2</b> regularization or weight decay. This type of regularization is basically imposing a soft constraint on the cost function. We\u2019re telling the network \u201cHey, we want you to minimize the <b>loss</b> from the training ...", "dateLastCrawled": "2022-01-31T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "is known as L1-norm, while the latter is known as the <b>L2</b>-norm. Keep in mind that <b>L2</b>-norm is more sensitive than L1-norm to large-valued outliers. Ridge and LASSO regularizations are based on <b>L2</b>-norm and L1-norm, respectively, while Elastic Net regularization is based on the mix of two. 2.6 What does a <b>machine</b> <b>learning</b> <b>learning</b>-curve measure ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, the optimizer , which in this mountain <b>analogy</b> roughly describes stochastic gradient descent (SGD) optimization, continually tries new weights and biases until it reaches its goal of finding the optimal values for the model to make accurate predictions.", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - L1-norm vs <b>l2</b>-norm as cost function when ...", "url": "https://stackoverflow.com/questions/43301036/l1-norm-vs-l2-norm-as-cost-function-when-standardizing", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43301036", "snippet": "The basic idea/motivation is how to penalize deviations. L1-norm does not care much about outliers, while <b>L2</b>-norm penalize these heavily. This is the basic difference and you will find a lot of pros and cons, even on wikipedia. So in regards to your question if it makes sense when the expected deviations are small: sure, it behaves the same.", "dateLastCrawled": "2022-01-24T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bias-Variance Decomposition</b> - mlxtend", "url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/", "isFamilyFriendly": true, "displayUrl": "rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp", "snippet": "<b>Bias variance decomposition</b> of <b>machine</b> <b>learning</b> algorithms for various <b>loss</b> functions. from mlxtend.evaluate import bias_variance_decomp. Overview. Often, researchers use the terms bias and variance or &quot;bias-variance tradeoff&quot; to describe the performance of a model -- i.e., you may stumble upon talks, books, or articles where people say that a model has a high variance or high bias. So, what does that mean? In general, we might say that &quot;high variance&quot; is proportional to overfitting, and ...", "dateLastCrawled": "2022-01-31T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, squared hinge <b>loss</b> function (as against hinge <b>loss</b> function) and <b>l2</b> penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are some <b>examples in everyday life analogous to &#39;overfitting</b>&#39; in ...", "url": "https://www.quora.com/What-are-some-examples-in-everyday-life-analogous-to-overfitting-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-some-<b>examples-in-everyday-life-analogous-to-overfitting</b>...", "snippet": "Answer (1 of 3): Exam overfitting - When you study for an exam, only by practicing questions from previous years&#39; exams. You then discover to your horror that xx% of this year&#39;s questions are new, and you get a much lower score than on your practice ones. If you are a bit older, you can expand th...", "dateLastCrawled": "2022-01-06T06:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[D] Looking for papers on treating regression as classification vs ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7gun87/d_looking_for_papers_on_treating_regression_as/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7gun87/d_looking_for_papers_on...", "snippet": "Doing the <b>L2 loss is like</b> doing maximum likelihood on a gaussian with a fixed variance - so the bad regression here is largely coming from the gaussian being mis-specified. I think the richer question would involve comparing approaches that consider the ordering vs. approaches that don t consider the ordering but where both have flexible enough distributions.", "dateLastCrawled": "2021-01-17T22:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep learning</b> - lectures.alex.balgavy.eu", "url": "https://lectures.alex.balgavy.eu/ml-notes/deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://lectures.alex.balgavy.eu/ml-notes/<b>deep-learning</b>", "snippet": "<b>Deep learning</b> <b>Deep learning</b> systems (autodiff engines) Tensors. To scale up backpropagation, want to move from operations on scalars to tensors. Tensor: generalisation of vectors/matrices to higher dimensions. e.g. a 2-tensor has two dimensions, a 4-tensor has 4 dimensions. You can represent data as a tensor. e.g. an RGB image is a 3-tensor of the red, green, and blue values for each pixel. Functions on tensors. Functions have inputs and outputs, all of which are tensors. They implement ...", "dateLastCrawled": "2021-12-15T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A deep <b>learning</b> framework for constitutive modeling based on temporal ...", "url": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "snippet": "These two features meet the requirement for sequence modeling in <b>machine</b> <b>learning</b>. Therefore, the nonlinear constitutive models may be classified as sequence modeling from the viewpoint of <b>machine</b> <b>learning</b>. Concrete material and steel material both exhibit significant ultra-long-term memory effects and many model-driven constitutive relationships were developed to simulate stress-strain curves of materials , , , , with ultra-long-term memory effect. For steel material, the traditional ...", "dateLastCrawled": "2022-01-20T12:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(l2 loss)  is like +(a penalty for making mistakes)", "+(l2 loss) is similar to +(a penalty for making mistakes)", "+(l2 loss) can be thought of as +(a penalty for making mistakes)", "+(l2 loss) can be compared to +(a penalty for making mistakes)", "machine learning +(l2 loss AND analogy)", "machine learning +(\"l2 loss is like\")", "machine learning +(\"l2 loss is similar\")", "machine learning +(\"just as l2 loss\")", "machine learning +(\"l2 loss can be thought of as\")", "machine learning +(\"l2 loss can be compared to\")"]}