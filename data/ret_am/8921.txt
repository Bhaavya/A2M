{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An Introduction to Inter-Annotator <b>Agreement</b> and Cohen&#39;s Kappa Statistic", "url": "http://blog.echen.me/2021/12/23/an-introduction-to-inter-annotator-agreement-and-cohens-kappa-statistic/", "isFamilyFriendly": true, "displayUrl": "blog.echen.me/2021/12/23/an-introduction-to-inter-annotator-<b>agreement</b>-and-cohens-kappa...", "snippet": "The problem with simple measures of <b>inter-rater</b> reliability, <b>like</b> the percentage of samples both raters label identically, is that they don\u2019t account for the likelihood that <b>two</b> <b>people</b> would agree by random chance. To understand how this works, let\u2019s consider the confusion matrix for the 100 essays Alix and Bob graded in their first week: This is clearly a talented bunch of students, because 86% earned passes from both raters. The raters also rarely disagreed about which essays were good ...", "dateLastCrawled": "2022-01-23T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interrater</b> <b>Agreement</b> Measures: Comments on Kappa n , Cohen&#39;s Kappa ...", "url": "https://www.researchgate.net/publication/232604628_Interrater_Agreement_Measures_Comments_on_Kappa_n_Cohen's_Kappa_Scott's_p_and_Aickin's_a", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/232604628_<b>Interrater</b>_<b>Agreement</b>_Measures...", "snippet": "Huddleston (2003) and Rashed (2010) There are <b>two</b> measures commonly used in <b>inter-rater</b> reliability, namely Cohen-Kappa (K) and percentage of <b>agreement</b> (%) (Gwet, 2002; Hsu and Field, 2003). Their ...", "dateLastCrawled": "2022-01-28T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Intercoder <b>Reliability</b> in Qualitative Research: Debates and Practical ...", "url": "https://journals.sagepub.com/doi/10.1177/1609406919899220", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/10.1177/1609406919899220", "snippet": "ICR is a numerical measure of the <b>agreement</b> between different coders regarding how the same data should be coded. ICR is sometimes conflated with <b>interrater</b> <b>reliability</b> (IRR), and the <b>two</b> terms are often used interchangeably. However, technically IRR refers to cases where data are rated on some ordinal or interval scale (e.g., the intensity of an emotion), whereas ICR is appropriate when categorizing data at a nominal level (e.g., the presence or absence of an emotion).", "dateLastCrawled": "2022-02-02T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) When Human Coders (and Machines) Disagree on the Meaning of ...", "url": "https://www.academia.edu/4149622/When_Human_Coders_and_Machines_Disagree_on_the_Meaning_of_Facial_Affect_in_Spontaneous_Videos", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/4149622/When_Human_Coders_and_Machines_Disagree_on_the...", "snippet": "<b>Inter- rater</b> reliability was measured using Kohen\u2019s kappa between self vs. peer (Coder 1 vs. Coder 2), self vs. judge1 (Coder 1 vs. Coder 3), self vs. judge2 (Coder 1 vs. Coder 4), peer vs. judge1 (Coder 2 vs. Coder 3), peer vs. judge2 (Coder 2 vs. Coder 4), judge1 vs. judge2 (Coder 3 vs. Coder 4). Among all these pairs, judge1 and judge2 had the highest <b>agreement</b> (kappa =0.71). We took the subset of videos where these <b>two</b> judges perfectly agreed and used these videos with the judges ...", "dateLastCrawled": "2021-12-17T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "David <b>Bassiuni asking them to maintain the momentum of dialogue</b> and ...", "url": "https://chicagorazom.com/david-bassiuni-asking-them-to-maintain-the-momentum-of-dialogue-and-agreement/", "isFamilyFriendly": true, "displayUrl": "https://chicagorazom.com/david-<b>bassiuni-asking-them-to-maintain</b>-the-momentum-of...", "snippet": "In statistics, <b>inter-rater</b> reliability (also called by various similar names, such as <b>inter-rater</b> <b>agreement</b>, <b>inter-rater</b> concordance, inter-observer reliability, and so on) is the degree of <b>agreement</b> among raters. It is a score of how much homogeneity or consensus exists in the ratings given by various judges. When comparing <b>two</b> methods of measurement, it is not only of interest to estimate both bias and limits of <b>agreement</b> between the <b>two</b> methods (<b>inter-rater</b> <b>agreement</b>), but also to assess ...", "dateLastCrawled": "2022-01-18T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The noun <b>agreement</b> can be countable or uncountable. - Indian Thoughts", "url": "https://indianthoughts.in/the-noun-agreement-can-be-countable-or-uncountable/", "isFamilyFriendly": true, "displayUrl": "https://indianthoughts.in/the-noun-<b>agreement</b>-can-be-countable-or-uncountable", "snippet": "A letter of <b>agreement</b> is an <b>agreement</b> between <b>two</b> parties that puts the terms of the <b>agreement</b> in writing as a means of resolving later disputes that may arise.3 min read <b>AGREEMENT</b>, contract. The consent of <b>two</b> or more persons concurring, respecting the transmission of some property, right or benefit, with a view of contracting an obligation. Bac. Ab. h.t.; Com. Dig. h.t.; Vin. Ab. h.t.; Plowd. 17; 1 Com. Contr. 2; 5 East\u2019s R. 16. It will be proper to consider, 1, the requisites of an ...", "dateLastCrawled": "2022-01-28T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Memory underpinnings of future intentions: Would you <b>like</b> to see the ...", "url": "https://www.europepmc.org/articles/PMC5407789/", "isFamilyFriendly": true, "displayUrl": "https://www.europepmc.org/articles/PMC5407789", "snippet": "Scenes were evaluated by <b>two</b> independent judges with good <b>inter-rater</b> <b>agreement</b> (Cohen\u2019s kappa = .71, computed on a random selection\u201410%\u2014of retrieved scenes). Disagreements were reconciled through the raters\u2019 joint discussion. The analysis showed that over the 95% of the retrieved scenes were valid episodic recollections. This shows that episodic information was highly accessible and accurate after one week from the experience. The episodic-derived score was computed on the ...", "dateLastCrawled": "2022-01-26T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Developmental differences in affective representation between ...", "url": "https://academic.oup.com/scan/advance-article/doi/10.1093/scan/nsab093/6332882", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/scan/advance-article/doi/10.1093/scan/nsab093/6332882", "snippet": "Calculating <b>inter-rater</b> reliability of stimuli Media content is often subjective, and, in confirming <b>agreement</b> on the constructs represented in the content, Krippendorff\u2019s alpha ( Lombard et al. , 2002 ; Krippendorff, 2004 ) is a common statistical comparison test used by media scholars to assess content constancy ( Lombard et al. , 2002 ; Lombard, 2013 ).", "dateLastCrawled": "2022-01-01T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Conceptualizations of Hooking Up Among Male Soldiers: A Qualitative ...", "url": "https://academic.oup.com/milmed/article/185/Supplement_1/355/5740688", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/milmed/article/185/Supplement_1/355/5740688", "snippet": "At subsequent meetings, the code book was further refined and augmented using the constant comparison method. 24 Once the final set of codes was established, focused coding was applied to the full set of transcripts. 24 Cohen\u2019s kappa was used as an index of <b>inter-rater</b> <b>agreement</b>, with kappas calculated in NVIVO for the most frequently coded themes in each transcript. Adequate <b>agreement</b> between coders was represented by the kappa value of 0.7 or above.", "dateLastCrawled": "2022-01-31T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Agreement between two machine learning models</b> - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/315313/agreement-between-two-machine-learning-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/315313", "snippet": "So my plan is to compare the outcome of <b>two</b> models: One that uses a biased sample and another one that uses an unbiased sample (both using the same sample size n). I would <b>like</b> to compare both the outcomes of the model, but what I&#39;m also very interested in is to to say something about the level of <b>agreement</b> of both models. For this last part I&#39;m having difficulty finding more information. Both models will output a probability on the same test set, so I&#39;m looking for a method to quantify ...", "dateLastCrawled": "2022-01-25T12:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An Introduction to Inter-Annotator <b>Agreement</b> and Cohen&#39;s Kappa Statistic", "url": "http://blog.echen.me/2021/12/23/an-introduction-to-inter-annotator-agreement-and-cohens-kappa-statistic/", "isFamilyFriendly": true, "displayUrl": "blog.echen.me/2021/12/23/an-introduction-to-inter-annotator-<b>agreement</b>-and-cohens-kappa...", "snippet": "The problem with simple measures of <b>inter-rater</b> reliability, like the percentage of samples both raters label identically, is that they don\u2019t account for the likelihood that <b>two</b> <b>people</b> would agree by random chance. To understand how this works, let\u2019s consider the confusion matrix for the 100 essays Alix and Bob graded in their first week:", "dateLastCrawled": "2022-01-23T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interrater</b> <b>Agreement</b> Measures: Comments on Kappa n , Cohen&#39;s Kappa ...", "url": "https://www.researchgate.net/publication/232604628_Interrater_Agreement_Measures_Comments_on_Kappa_n_Cohen's_Kappa_Scott's_p_and_Aickin's_a", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/232604628_<b>Interrater</b>_<b>Agreement</b>_Measures...", "snippet": "Huddleston (2003) and Rashed (2010) There are <b>two</b> measures commonly used in <b>inter-rater</b> reliability, namely Cohen-Kappa (K) and percentage of <b>agreement</b> (%) (Gwet, 2002; Hsu and Field, 2003). Their ...", "dateLastCrawled": "2022-01-28T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "David <b>Bassiuni asking them to maintain the momentum of dialogue</b> and ...", "url": "https://chicagorazom.com/david-bassiuni-asking-them-to-maintain-the-momentum-of-dialogue-and-agreement/", "isFamilyFriendly": true, "displayUrl": "https://chicagorazom.com/david-<b>bassiuni-asking-them-to-maintain</b>-the-momentum-of...", "snippet": "In statistics, <b>inter-rater</b> reliability (also called by various <b>similar</b> names, such as <b>inter-rater</b> <b>agreement</b>, <b>inter-rater</b> concordance, inter-observer reliability, and so on) is the degree of <b>agreement</b> among raters. It is a score of how much homogeneity or consensus exists in the ratings given by various judges. When comparing <b>two</b> methods of measurement, it is not only of interest to estimate both bias and limits of <b>agreement</b> between the <b>two</b> methods (<b>inter-rater</b> <b>agreement</b>), but also to assess ...", "dateLastCrawled": "2022-01-18T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "SimLex-999: <b>Evaluating Semantic Models With (Genuine) Similarity</b> ...", "url": "https://direct.mit.edu/coli/article/41/4/665/1517/SimLex-999-Evaluating-Semantic-Models-With-Genuine", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/41/4/665/1517/SimLex-999-Evaluating-Semantic...", "snippet": "The SimLex-999 <b>inter-rater</b> <b>agreement</b> suggests that participants were able to understand the (single) characterization of similarity presented in the instructions and to apply it to concepts of various types consistently. This conclusion was supported by inspection of the brief feedback offered by the majority of annotators in a final text field in the questionnaire: 78% expressed sentiment that the test was clear, easy to complete, or some <b>similar</b> sentiment.", "dateLastCrawled": "2022-01-19T23:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Intercoder <b>Reliability</b> in Qualitative Research: Debates and Practical ...", "url": "https://journals.sagepub.com/doi/10.1177/1609406919899220", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/10.1177/1609406919899220", "snippet": "ICR is a numerical measure of the <b>agreement</b> between different coders regarding how the same data should be coded. ICR is sometimes conflated with <b>interrater</b> <b>reliability</b> (IRR), and the <b>two</b> terms are often used interchangeably. However, technically IRR refers to cases where data are rated on some ordinal or interval scale (e.g., the intensity of an emotion), whereas ICR is appropriate when categorizing data at a nominal level (e.g., the presence or absence of an emotion).", "dateLastCrawled": "2022-02-02T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) When Human Coders (and Machines) Disagree on the Meaning of ...", "url": "https://www.academia.edu/4149622/When_Human_Coders_and_Machines_Disagree_on_the_Meaning_of_Facial_Affect_in_Spontaneous_Videos", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/4149622/When_Human_Coders_and_Machines_Disagree_on_the...", "snippet": "<b>Inter- rater</b> reliability was measured using Kohen\u2019s kappa between self vs. peer (Coder 1 vs. Coder 2), self vs. judge1 (Coder 1 vs. Coder 3), self vs. judge2 (Coder 1 vs. Coder 4), peer vs. judge1 (Coder 2 vs. Coder 3), peer vs. judge2 (Coder 2 vs. Coder 4), judge1 vs. judge2 (Coder 3 vs. Coder 4). Among all these pairs, judge1 and judge2 had the highest <b>agreement</b> (kappa =0.71). We took the subset of videos where these <b>two</b> judges perfectly agreed and used these videos with the judges ...", "dateLastCrawled": "2021-12-17T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "PSY105 L4b Reliability &amp; Validity in Research (Student - Big).pdf ...", "url": "https://www.coursehero.com/file/93888834/PSY105-L4b-Reliability-Validity-in-Research-Student-Bigpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/93888834/PSY105-L4b-Reliability-Validity-in-Research...", "snippet": "Form equivalence - Test-equating-<b>Two</b> different tests on the same content (but different items) 4. <b>Inter-rater</b> reliability -<b>Agreement</b> between <b>two</b> raters / coders / observers-Useful for subjective measurements (e.g., for qualitative observational research) -Measured by Pearson\u2019s r (continuous scores), Cohen\u2019s Kappa (categorical scores), etc. Types of Validity 1. Content validity (= logical validity)-Extent to which a test represents all facets of a variable-Content of test items should be ...", "dateLastCrawled": "2021-12-05T08:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Content Analysis</b> in the Study of Crime, Media, and Popular Culture ...", "url": "https://oxfordre.com/criminology/view/10.1093/acrefore/9780190264079.001.0001/acrefore-9780190264079-e-23", "isFamilyFriendly": true, "displayUrl": "https://oxfordre.com/criminology/view/10.1093/acrefore/9780190264079.001.0001/acrefore...", "snippet": "In brief, <b>inter-rater</b> reliability is the extent to which different <b>people</b> code the same text in the same way. Differences, for example, may occur when coding rules or categories are not clear, or when there are cognitive differences across coders. The pre-test process and adequate coder training may reduce these differences, but <b>inter-rater</b> reliability should also be assessed at the end of coding. Various statistical tests exist to assess <b>inter-rater</b> reliability.", "dateLastCrawled": "2022-02-01T08:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The noun <b>agreement</b> can be countable or uncountable. - Indian Thoughts", "url": "https://indianthoughts.in/the-noun-agreement-can-be-countable-or-uncountable/", "isFamilyFriendly": true, "displayUrl": "https://indianthoughts.in/the-noun-<b>agreement</b>-can-be-countable-or-uncountable", "snippet": "A Hold Harmless <b>Agreement</b>, or <b>similar</b> agreements, are used in many instances. The NRLCA National Board believes this is a fair and reasonable <b>agreement</b> that is in the best interest of the 131,000 hardworking rural letter carriers across the country providing for substantial gains in wages and benefits for all classifications of rural carriers. The NRLCA has received calls from carriers that indicate management has been using carriers\u2019 personal cell phones to communicate service directions ...", "dateLastCrawled": "2022-01-28T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Posttraumatic Stress and the Comprehension</b> of Everyday Activity ...", "url": "https://online.ucpress.edu/collabra/article/2/1/11/112685/Posttraumatic-Stress-and-the-Comprehension-of", "isFamilyFriendly": true, "displayUrl": "https://online.ucpress.edu/collabra/article/2/1/11/112685/Posttraumatic-Stress-and-the...", "snippet": "Posttraumatic Stress Disorder (PTSD) is a disabling disorder with a lifetime prevalence of 6.8% in the United States [].Symptoms of PTSD include reexperiencing (e.g., flashbacks), avoidance and numbing (e.g., avoidance of thoughts or places related to the trauma), and increased arousal (e.g., hypervigilance; []).<b>People</b> with PTSD also often report impairments in attention and memory on tasks that are not directly related to their traumatic event, such as difficulty remembering a phone number ...", "dateLastCrawled": "2022-01-16T13:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An Introduction to Inter-Annotator <b>Agreement</b> and Cohen&#39;s Kappa Statistic", "url": "http://blog.echen.me/2021/12/23/an-introduction-to-inter-annotator-agreement-and-cohens-kappa-statistic/", "isFamilyFriendly": true, "displayUrl": "blog.echen.me/2021/12/23/an-introduction-to-inter-annotator-<b>agreement</b>-and-cohens-kappa...", "snippet": "<b>Can</b> be easily adapted to measure <b>agreement</b> about more than <b>two</b> labels. (For example, if Alix and Bob gave every essay an A-F grade instead of just pass/fail.) Negative scores <b>can</b> be used to identify raters with diverse viewpoints. Cons. <b>Can</b> only compare <b>two</b> raters, not three or more. (Unlike next week&#39;s spotlight metric, Fleiss\u2019 kappa!)", "dateLastCrawled": "2022-01-23T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interrater</b> <b>Agreement</b> Measures: Comments on Kappa n , Cohen&#39;s Kappa ...", "url": "https://www.researchgate.net/publication/232604628_Interrater_Agreement_Measures_Comments_on_Kappa_n_Cohen's_Kappa_Scott's_p_and_Aickin's_a", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/232604628_<b>Interrater</b>_<b>Agreement</b>_Measures...", "snippet": "Huddleston (2003) and Rashed (2010) There are <b>two</b> measures commonly used in <b>inter-rater</b> reliability, namely Cohen-Kappa (K) and percentage of <b>agreement</b> (%) (Gwet, 2002; Hsu and Field, 2003). Their ...", "dateLastCrawled": "2022-01-28T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The noun <b>agreement</b> <b>can</b> be countable or uncountable. - Indian Thoughts", "url": "https://indianthoughts.in/the-noun-agreement-can-be-countable-or-uncountable/", "isFamilyFriendly": true, "displayUrl": "https://indian<b>thoughts</b>.in/the-noun-<b>agreement</b>-<b>can</b>-be-countable-or-uncountable", "snippet": "A verbal <b>agreement</b> <b>can</b> also be a fully binding <b>agreement</b>, but it <b>can</b> be difficult to prove a verbal <b>agreement</b> in situations where you may need this . 3.6.1 The Employer agrees to translate each clause of this <b>agreement</b> from the language in which the clause was negotiated at the bargaining table into the other official language of Canada at the time of their ratification at the negotiation table. Within three months of the ratification of the new <b>agreement</b>, the Employer shall provide a ...", "dateLastCrawled": "2022-01-28T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Establishing <b>Agreement</b> An Analysis of Proposal-Acceptance Sequences", "url": "https://cs.write-manual.net.ru/642", "isFamilyFriendly": true, "displayUrl": "https://cs.write-manual.net.ru/642", "snippet": "First, <b>inter-rater</b> reliability both within and across subgroups is assessed using the intra-class correlation coefficient (icc). Next, based on this analysis of reliability and on the test-retest reliability of the employed tool, <b>inter-rater</b> <b>agreement</b> is analyzed, magnitude and direction of rating differences are considered. Non-analysis agreements (naas) allow for a party to share its patented technology with another party for a pre-negotiated purpose, such as testing or further research ...", "dateLastCrawled": "2022-01-25T18:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "PSY105 L4b Reliability &amp; Validity in Research (Student - Big).pdf ...", "url": "https://www.coursehero.com/file/93888834/PSY105-L4b-Reliability-Validity-in-Research-Student-Bigpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/93888834/PSY105-L4b-Reliability-Validity-in-Research...", "snippet": "Form equivalence - Test-equating-<b>Two</b> different tests on the same content (but different items) 4. <b>Inter-rater</b> reliability -<b>Agreement</b> between <b>two</b> raters / coders / observers-Useful for subjective measurements (e.g., for qualitative observational research)-Measured by Pearson\u2019s r (continuous scores), Cohen\u2019s Kappa (categorical scores), etc. Types of Validity 1. Content validity (= logical validity)-Extent to which a test represents all facets of a variable-Content of test items should be ...", "dateLastCrawled": "2021-12-05T08:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Most agreements actually work. - Indian Thoughts", "url": "https://indianthoughts.in/most-agreements-actually-work/", "isFamilyFriendly": true, "displayUrl": "https://indian<b>thoughts</b>.in/most-<b>agreements</b>-actually-work", "snippet": "A letter of <b>agreement</b> is an <b>agreement</b> between <b>two</b> parties that puts the terms of the <b>agreement</b> in writing as a means of resolving later disputes that may arise.3 min read <b>AGREEMENT</b>, contract. The consent of <b>two</b> or more persons concurring, respecting the transmission of some property, right or benefit, with a view of contracting an obligation. Bac. Ab. h.t.; Com. Dig. h.t.; Vin. Ab. h.t.; Plowd. 17; 1 Com. Contr. 2; 5 East\u2019s R. 16. It will be proper to consider, 1, the requisites of an ...", "dateLastCrawled": "2022-01-29T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Midlife Eriksonian Psychosocial Development: Setting the Stage for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5398200/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5398200", "snippet": "The Word Recall Task or list learning task asks participants to read 10 words aloud and then recall any words they <b>can</b> remember in 90 seconds. This is repeated <b>two</b> more times. The Delayed Recall task asks participants to remember any words from the list after a period of five minutes. The Recognition task asks participants to identify if any ...", "dateLastCrawled": "2022-02-02T08:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>The emotional and communicative significance of</b> head nods and ...", "url": "https://www.researchgate.net/publication/233812507_The_emotional_and_communicative_significance_of_head_nods_and_shakes_in_a_naturalistic_database", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/233812507_The_emotional_and_communicative...", "snippet": "which is generally <b>thought</b> of as a ... <b>Inter-rater</b> <b>agreement</b> was significant for most of the . SEMAINE variables, though the strength of the . relationship varied. It was highest for arousal (r =0 ...", "dateLastCrawled": "2021-12-09T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Self-fulfilling prophecy, Social Evolution, Chapter</b> 10, Chapter 9 ...", "url": "https://quizlet.com/210247383/self-fulfilling-prophecy-social-evolution-chapter-10-chapter-9-chapter-5-chapter-3-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/210247383/<b>self-fulfilling-prophecy-social-evolution-chapter</b>-10...", "snippet": "- High level of <b>inter-rater</b> <b>agreement</b> - Significant cross-cultural and historical <b>agreement</b> - Variability in physical adornments and ideal body weight . Boom and Bust. Men in cultures with scarce resources prefer heavier women - Symons 1979; Anderson et al., 1992. Hungry for More-Nelson and Morrison (2005)-Men going into the dining hall prefer heavier women than men coming out of the dining hall-No significant effect for women - More specifically, hungry males have been found to prefer ...", "dateLastCrawled": "2018-11-09T14:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What does <b>agreement</b> mean - Definition of <b>agreement</b> - Word finder", "url": "https://findwords.info/term/agreement", "isFamilyFriendly": true, "displayUrl": "https://findwords.info/term/<b>agreement</b>", "snippet": "<b>People</b> sometimes sign credit agreements and then realize they <b>can</b>\u2019t afford the payments. a disarmament treaty/<b>agreement</b> . There will be US-Russian talks on a new disarmament treaty. a lease <b>agreement</b>. The organization has signed a lease <b>agreement</b> on a 50-acre site. a loan <b>agreement</b> (=that says how much the loan will be, how much you will pay back each month etc) Read the terms of your loan <b>agreement</b> carefully. a peace treaty/<b>agreement</b>/accord. The formal signing of the peace <b>agreement</b> took ...", "dateLastCrawled": "2021-07-21T03:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An Introduction to Inter-Annotator <b>Agreement</b> and Cohen&#39;s Kappa Statistic", "url": "http://blog.echen.me/2021/12/23/an-introduction-to-inter-annotator-agreement-and-cohens-kappa-statistic/", "isFamilyFriendly": true, "displayUrl": "blog.echen.me/2021/12/23/an-introduction-to-inter-annotator-<b>agreement</b>-and-cohens-kappa...", "snippet": "<b>Can</b> be easily adapted to measure <b>agreement</b> about more than <b>two</b> labels. (For example, if Alix and Bob gave every essay an A-F grade instead of just pass/fail.) Negative scores <b>can</b> be used to identify raters with diverse viewpoints. Cons. <b>Can</b> only compare <b>two</b> raters, not three or more. (Unlike next week&#39;s spotlight metric, Fleiss\u2019 kappa!)", "dateLastCrawled": "2022-01-23T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interrater</b> <b>Agreement</b> Measures: Comments on Kappa n , Cohen&#39;s Kappa ...", "url": "https://www.researchgate.net/publication/232604628_Interrater_Agreement_Measures_Comments_on_Kappa_n_Cohen's_Kappa_Scott's_p_and_Aickin's_a", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/232604628_<b>Interrater</b>_<b>Agreement</b>_Measures...", "snippet": "Huddleston (2003) and Rashed (2010) There are <b>two</b> measures commonly used in <b>inter-rater</b> reliability, namely Cohen-Kappa (K) and percentage of <b>agreement</b> (%) (Gwet, 2002; Hsu and Field, 2003). Their ...", "dateLastCrawled": "2022-01-28T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Intercoder <b>Reliability</b> in Qualitative Research: Debates and Practical ...", "url": "https://journals.sagepub.com/doi/10.1177/1609406919899220", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/10.1177/1609406919899220", "snippet": "ICR is a numerical measure of the <b>agreement</b> between different coders regarding how the same data should be coded. ICR is sometimes conflated with <b>interrater</b> <b>reliability</b> (IRR), and the <b>two</b> terms are often used interchangeably. However, technically IRR refers to cases where data are rated on some ordinal or interval scale (e.g., the intensity of an emotion), whereas ICR is appropriate when categorizing data at a nominal level (e.g., the presence or absence of an emotion).", "dateLastCrawled": "2022-02-02T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) When Human Coders (and Machines) Disagree on the Meaning of ...", "url": "https://www.academia.edu/4149622/When_Human_Coders_and_Machines_Disagree_on_the_Meaning_of_Facial_Affect_in_Spontaneous_Videos", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/4149622/When_Human_Coders_and_Machines_Disagree_on_the...", "snippet": "<b>Inter- rater</b> reliability was measured using Kohen\u2019s kappa between self vs. peer (Coder 1 vs. Coder 2), self vs. judge1 (Coder 1 vs. Coder 3), self vs. judge2 (Coder 1 vs. Coder 4), peer vs. judge1 (Coder 2 vs. Coder 3), peer vs. judge2 (Coder 2 vs. Coder 4), judge1 vs. judge2 (Coder 3 vs. Coder 4). Among all these pairs, judge1 and judge2 had the highest <b>agreement</b> (kappa =0.71). We took the subset of videos where these <b>two</b> judges perfectly agreed and used these videos with the judges ...", "dateLastCrawled": "2021-12-17T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "SimLex-999: <b>Evaluating Semantic Models With (Genuine) Similarity</b> ...", "url": "https://direct.mit.edu/coli/article/41/4/665/1517/SimLex-999-Evaluating-Semantic-Models-With-Genuine", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/41/4/665/1517/SimLex-999-Evaluating-Semantic...", "snippet": "Overall <b>agreement</b> was \u03c1 = 0.67. This compares favorably with the <b>agreement</b> on WS-353 (\u03c1 = 0.61 using the same method). The design of the MEN rating system precludes a conventional calculation of <b>inter-rater</b> <b>agreement</b> (Bruni et al. 2012b). However, <b>two</b> of the creators of MEN who independently rated the data set achieved an <b>agreement</b> of \u03c1 = 0 ...", "dateLastCrawled": "2022-01-19T23:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Tool for Observing Play Outdoors (TOPO): A New Typology for ...", "url": "https://www.academia.edu/63669481/Tool_for_Observing_Play_Outdoors_TOPO_A_New_Typology_for_Capturing_Childrens_Play_Behaviors_in_Outdoor_Environments", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/63669481/Tool_for_Observing_Play_Outdoors_TOPO_A_New_Typology...", "snippet": "During reliability rounds, all observers independently code the same play behaviors to compare levels of <b>agreement</b>; Kappa analysis <b>can</b> then be conducted to compare the reliability of <b>inter-rater</b> observations across the full dataset. Int. J. Environ. Res. Public Health 2020, 17, 5611 30 of 34 See Tables A1 and A2 for sample templates for both the TOPO-9 and TOPO-32 versions of the tool. Table A1. Sample Template for TOPO-9 (Collapsed Version). Sample Template for TOPO-9 Play Type Codes: 1 ...", "dateLastCrawled": "2022-01-15T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The noun <b>agreement</b> <b>can</b> be countable or uncountable. - Indian Thoughts", "url": "https://indianthoughts.in/the-noun-agreement-can-be-countable-or-uncountable/", "isFamilyFriendly": true, "displayUrl": "https://indianthoughts.in/the-noun-<b>agreement</b>-<b>can</b>-be-countable-or-uncountable", "snippet": "A verbal <b>agreement</b> <b>can</b> also be a fully binding <b>agreement</b>, but it <b>can</b> be difficult to prove a verbal <b>agreement</b> in situations where you may need this . 3.6.1 The Employer agrees to translate each clause of this <b>agreement</b> from the language in which the clause was negotiated at the bargaining table into the other official language of Canada at the time of their ratification at the negotiation table. Within three months of the ratification of the new <b>agreement</b>, the Employer shall provide a ...", "dateLastCrawled": "2022-01-28T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Content Analysis</b> in the Study of Crime, Media, and Popular Culture ...", "url": "https://oxfordre.com/criminology/view/10.1093/acrefore/9780190264079.001.0001/acrefore-9780190264079-e-23", "isFamilyFriendly": true, "displayUrl": "https://oxfordre.com/criminology/view/10.1093/acrefore/9780190264079.001.0001/acrefore...", "snippet": "The open-ended format of an ECA protocol precludes the use of standard metrics to assess <b>inter-rater</b> reliability, but efforts <b>can</b> be taken to ensure a level of <b>agreement</b> among coders. For example, as part of the training process, coders should also become familiar with the production process of the medium under study, as well as with the patterns in key examples. Familiarity with the sources facilitates more detailed coding. Research teams employing ECA <b>can</b> promote consistency in coding ...", "dateLastCrawled": "2022-02-01T08:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>FINAL psy 301</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/55062703/final-psy-301-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/55062703/<b>final-psy-301</b>-flash-cards", "snippet": "4. <b>inter-rater</b> - give <b>two</b> <b>people</b> one measure at one time. test-retest reliability . test administered to same person on <b>two</b> separate occasions and correlation between <b>two</b> tests is calculated - time one and time <b>two</b> advantages: uses same test items, simple to do disadvantages: testing effects, maturation, delay of info. alternate forms of reliability. correlation between <b>two</b> different (but similar versions of a measure) - form a and form b (ex: parallel forms of reliability; measuring the ...", "dateLastCrawled": "2021-05-25T14:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "For mining leases, they <b>are known as compensation agreements</b>. - Enjoii ...", "url": "http://enjoii.dk/for-mining-leases-they-are-known-as-compensation-agreements/", "isFamilyFriendly": true, "displayUrl": "enjoii.dk/for-mining-leases-they-<b>are-known-as-compensation-agreements</b>", "snippet": "The matrimonial <b>agreement</b> <b>can</b> include a clause dealing with how and when divorce proceedings may be subsequently issued e.g. you both agree after <b>two</b> years separation either party may apply to the Court for divorce.As a part of the divorce proceedings an application will be made on the date of the divorce hearing to make the <b>agreement</b> a Rule of Court or an Order of Court.This means the original <b>agreement</b> is lodged with the Court and is thus enforceable through the Courts so if any part of ...", "dateLastCrawled": "2022-01-30T23:35:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding Interobserver <b>Agreement</b>: The Kappa Statistic", "url": "http://web2.cs.columbia.edu/~julia/courses/CS6998/Interrater_agreement.Kappa_statistic.pdf", "isFamilyFriendly": true, "displayUrl": "web2.cs.columbia.edu/~julia/courses/CS6998/<b>Interrater</b>_<b>agreement</b>.Kappa_statistic.pdf", "snippet": "call the <b>analogy</b> of a target and how close we get to the bull\u2019s-eye (Figure 1). If we actually hit the bull\u2019s-eye (representing <b>agreement</b> with the gold standard), we are accurate. If all our shots land together, we have good precision (good reliability). If all our shots land together and we hit the bull\u2019s-eye, we are accurate as well as precise. It is possible, however, to hit the bull\u2019s-eye purely by chance. Referring to Figure 1, only the center black dot in target A is accurate ...", "dateLastCrawled": "2022-01-28T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Leveraging Inter-rater Agreement for Audio-Visual Emotion Recognition</b>", "url": "https://www.researchgate.net/publication/283487589_Leveraging_Inter-rater_Agreement_for_Audio-Visual_Emotion_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/283487589_Leveraging_<b>Inter-rater</b>_<b>Agreement</b>...", "snippet": "In <b>machine</b> <b>learning</b> tasks an actual \u2018ground truth\u2019 may not be available. Then, machines often have to rely on human labelling of data. This becomes challenging the more subjective the <b>learning</b> ...", "dateLastCrawled": "2021-08-28T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>learning</b> glossary - DataTime", "url": "https://www.dtalg.com/article-1/", "isFamilyFriendly": true, "displayUrl": "https://www.dtalg.com/article-1", "snippet": "See also Cohen\u2019s kappa, which is one of the most popular <b>inter-rater</b> <b>agreement</b> measurements. intersection over union (IoU) #image. The intersection of two sets divided by their union. In <b>machine</b>-<b>learning</b> image-detection tasks, IoU is used to measure the accuracy of the model\u2019s predicted bounding box with respect to the ground-truth bounding ...", "dateLastCrawled": "2022-01-25T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Multilingual <b>Twitter Sentiment Classification</b>: The Role of Human ... - PLOS", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155036", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155036", "snippet": "The researchers in the fields of <b>inter-rater</b> <b>agreement</b> and <b>machine</b> <b>learning</b> typically employ different evaluation measures. We report all the results in terms of four selected measures which we deem appropriate for the three-valued sentiment classification task (the details are in the Evaluation measures subsection in Methods). In this section, however, the results are summarized only in terms of Krippendorff\u2019s Alpha-reliability Alpha) , to highlight the main conclusions. Alpha is a ...", "dateLastCrawled": "2021-03-30T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Clinician perspectives on <b>machine</b> <b>learning</b> prognostic algorithms in the ...", "url": "https://link.springer.com/article/10.1007/s00520-021-06774-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00520-021-06774-w", "snippet": "<b>Machine</b> <b>learning</b> algorithms may accurately predict mortality risk in cancer, but it is unclear how oncology clinicians would use such algorithms in practice. The purpose of this qualitative study was to assess oncology clinicians\u2019 perceptions on the utility and barriers of <b>machine</b> <b>learning</b> prognostic algorithms to prompt advance care planning. Participants included medical oncology physicians and advanced practice providers (APPs) practicing in tertiary and community practices within a ...", "dateLastCrawled": "2022-01-30T10:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Analyzing and Interpreting Data From Rating Scales</b> | by Kevin C Lee ...", "url": "https://towardsdatascience.com/analyzing-and-interpreting-data-from-rating-scales-d169d66211db", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>analyzing-and-interpreting-data-from-rating-scales</b>-d169...", "snippet": "<b>Inter-Rater</b> Reliability. In B), we plot the pairwise correlations between the students with a heatmap. Most of the correlations are &gt; 0.6 with a few exceptions. A small number of respondents showing low correlations with others is acceptable as long as most students are able to respond similarly. P.S. The use of Pearson Correlation is only ...", "dateLastCrawled": "2022-01-29T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Target <b>analogy</b> of accuracy and precision | Download Scientific Diagram", "url": "https://researchgate.net/figure/Target-analogy-of-accuracy-and-precision_fig1_24399044", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Target-<b>analogy</b>-of-accuracy-and-precision_fig1_24399044", "snippet": "The intraclass correlation coefficient (ICC) was calculated to assess intra-rater and <b>inter-rater</b> <b>agreement</b> of I 3M . 31 A sample of OPTs was randomly divided into training dataset (819) and test ...", "dateLastCrawled": "2021-06-28T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Use of analogies, metaphors, and similes by students and reviewers at ...", "url": "https://www.cambridge.org/core/journals/ai-edam/article/use-of-analogies-metaphors-and-similes-by-students-and-reviewers-at-an-undergraduate-architectural-design-review/FB80EB57099A898FE15564497D5B06C7", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/ai-edam/article/use-of-analogies-metaphors-and...", "snippet": "We used the Delphi Method to determine the <b>inter-rater</b> <b>agreement</b>. In the first step after the second round of discussion, there was 66.67% <b>agreement</b> between the authors\u2019 coding and that of the independent coder. In the second step, <b>agreement</b> on the type of similarities was determined using the Delphi Method. At the end of second round of discussions, there was 90.1% <b>agreement</b>. Table 1. Categories and sub-categories used for coding the reviews. Any statement which explicitly or implicitly ...", "dateLastCrawled": "2022-02-02T16:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Frontiers | From What to Why, the Growing Need for a Focus Shift Toward ...", "url": "https://www.frontiersin.org/articles/10.3389/fphys.2021.821217/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fphys.2021.821217", "snippet": "Explainable AI is far from a novel concept in the <b>machine</b> <b>learning</b> (ML) community (Goebel et al., 2018; Tosun et al., 2020a,b). While the presentation of new approaches for post-hoc explainers of deep convolutional neural networks (CNNs) is outside of the scope of this review, there are a few simple steps that can increase the interpretability and explainability of an AI-driven study ( Figure 1 ).", "dateLastCrawled": "2022-02-03T06:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Quadratic weighted kappa</b> strength of <b>agreement</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/46296/quadratic-weighted-kappa-strength-of-agreement", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/46296", "snippet": "In the case of the kappa-value there are some attempts to qualify how good or bad the agreements are. For example Landis &amp; Koch in the article The Measurement of Observer <b>Agreement</b> for Categorical Data talks about &quot;strength of <b>agreement</b>&quot; based on kappa values:. Kappa Strength of <b>agreement</b> ===== ===== 0.0-0.20 Slight 0.21-0.40 Fair 0.41-0.60 Moderate 0.61-0.80 Substantial 0.81-0.90 Almost perfect", "dateLastCrawled": "2022-01-20T17:56:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reliability and Learnability of Human Bandit Feedback for Sequence-to ...", "url": "https://aclanthology.org/P18-1165.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/P18-1165.pdf", "snippet": "intra- and <b>inter-rater agreement is similar</b> for both tasks, with highest inter-rater reliability for stan-dardized 5-point ratings. In a next step, we address the issue of <b>machine</b> learnability of human rewards. We use deep learn- ing models to train reward estimators by regres-sion against cardinal feedback, and by \ufb01tting a Bradley-Terry model (Bradley and Terry,1952) to ordinal feedback. Learnability is understood by a slight misuse of the <b>machine</b> <b>learning</b> notion of learnability (Shalev ...", "dateLastCrawled": "2021-12-22T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "arXiv:1805.10627v3 [cs.CL] 13 Dec 2018", "url": "https://www.researchgate.net/profile/Joshua-Uyheng/publication/325413588_Reliability_and_Learnability_of_Human_Bandit_Feedback_for_Sequence-to-Sequence_Reinforcement_Learning/links/5ea04de5a6fdccd7cee0eebe/Reliability-and-Learnability-of-Human-Bandit-Feedback-for-Sequence-to-Sequence-Reinforcement-Learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Joshua-Uyheng/publication/325413588_Reliability...", "snippet": "\ufb01ed by bandit <b>learning</b> for neural <b>machine</b> trans-lation (NMT). Our aim is to show that successful <b>learning</b> from simulated bandit feedback (Sokolov et al.,2016b;Kreutzer et al.,2017;Nguyen et al ...", "dateLastCrawled": "2021-08-22T12:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(inter-rater agreement)  is like +(two people agreeing on a movie)", "+(inter-rater agreement) is similar to +(two people agreeing on a movie)", "+(inter-rater agreement) can be thought of as +(two people agreeing on a movie)", "+(inter-rater agreement) can be compared to +(two people agreeing on a movie)", "machine learning +(inter-rater agreement AND analogy)", "machine learning +(\"inter-rater agreement is like\")", "machine learning +(\"inter-rater agreement is similar\")", "machine learning +(\"just as inter-rater agreement\")", "machine learning +(\"inter-rater agreement can be thought of as\")", "machine learning +(\"inter-rater agreement can be compared to\")"]}