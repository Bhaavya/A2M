{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS231n Summary \u00b7 Imron Rosyadi", "url": "https://irosyadi.github.io/course/cs231n-summary.html", "isFamilyFriendly": true, "displayUrl": "https://irosyadi.github.io/course/cs231n-summary.html", "snippet": "As a revision here are the <b>Mini batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm steps: Loop: Sample a batch of data. Forward prop it through the graph (network) and get loss. Backprop to calculate the gradients. Update the parameters using the gradients. Activation functions: Different choices for activation function includes Sigmoid, tanh, RELU, Leaky RELU, Maxout, and ELU. Sigmoid: Squashes the numbers between [0,1] Used as a firing rate <b>like</b> human brains. Sigmoid(x) = 1 / (1 + e^-x) Problems ...", "dateLastCrawled": "2022-01-30T07:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An <b>Empirical Evaluation of Deep Learning on Highway Driving</b> | DeepAI", "url": "https://deepai.org/publication/an-empirical-evaluation-of-deep-learning-on-highway-driving", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/an-<b>empirical-evaluation-of-deep-learning-on-highway-driving</b>", "snippet": "Similar to the vehicle detector, we use L1 loss to train the regressor. We use <b>mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> for optimization. The learning rate is controlled by a variant of the momentum scheduler . To obtain semantic lane information, we use DBSCAN to cluster the line segments into lanes.", "dateLastCrawled": "2021-12-07T11:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Convolutional neural networks for 5G-enabled Intelligent Transportation ...", "url": "https://www.sciencedirect.com/science/article/pii/S0140366419316846", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0140366419316846", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b>: OP2: <b>Mini batch</b> <b>gradient</b> <b>descent</b>: OP3: Adagrad: OP4: AdaDelta: OP5: Adam: OP6: Nesterov accelerated <b>gradient</b>: OP7: RmsProp: Note: Sr. No. is used to refer the Table entry. Table 10. List of regularization techniques. Sr. No. Regularization technique; R1: L1 regularization: R2: L2 regularization: R3: Dropout: R4: Data augmentation: R5: Early stopping: Note: Sr. No. is used to refer the Table entry. There are few parameters which help to optimize the results ...", "dateLastCrawled": "2021-12-02T23:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An algorithm for <b>highway</b> <b>vehicle detection</b> based on convolutional ...", "url": "https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-018-0350-2", "isFamilyFriendly": true, "displayUrl": "https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-018-0350-2", "snippet": "During training phase, <b>stochastic</b> <b>gradient</b> <b>descent</b> (SGD) is used to optimize our network. We initialize the parameters for all the newly added layers by drawing weights from a zero-mean Gaussian distribution with standard deviation 0.01. We set the learning rate as 0.001 for the first 60k iterations and 0.0001 for the next 60k iterations. The batch size is 16 for a 320 \u00d7 320 model. We use a momentum of 0.9 and a weight decay of 0.005.", "dateLastCrawled": "2022-01-30T17:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>ICML 2018 Abstracts</b> \u00b7 GitHub", "url": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (SGD) is the optimization algorithm of choice in many machine learning applications such as regularized empirical risk minimization and training deep neural networks. The classical convergence analysis of SGD is carried out under the assumption that the norm of the <b>stochastic</b> <b>gradient</b> is uniformly bounded. While this might hold for some loss functions, it is always violated for cases where the objective function is strongly convex. In (Bottou et al.,2016), a new ...", "dateLastCrawled": "2022-01-19T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) An Improved Transfer learning Approach for Intrusion Detection", "url": "https://www.researchgate.net/publication/320437772_An_Improved_Transfer_learning_Approach_for_Intrusion_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320437772_An_Improved_Transfer_learning...", "snippet": "It was proposed in works [8, 9] to perform fine-tuning of object detector based on convolutional network VGG-16 using <b>mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b>. However, this would require a ...", "dateLastCrawled": "2022-01-10T05:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Hands-<b>On Machine Learning with Scikit-Learn &amp; TensorFlow</b> | Hanwen ...", "url": "https://www.academia.edu/39335333/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39335333/Hands_<b>On_Machine_Learning_with_Scikit_Learn</b>_and...", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2021-12-25T13:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "CS230 Deep Learning", "url": "http://cs230.stanford.edu/projects_winter_2019/reports/15813361.pdf", "isFamilyFriendly": true, "displayUrl": "cs230.stanford.edu/projects_winter_2019/reports/15813361.pdf", "snippet": "NGSIM contains <b>highway</b> <b>driving</b> trajectories for US <b>Highway</b> 101 and Interstate 80, and consists of 45 minutes of <b>driving</b> at 10 I-IZ for each roadway. The US <b>Highway</b> 101 data set covers an area in Los Angeles approximately 640 m in length with five mainline lanes and a sixth auxiliary lane for <b>highway</b> entrance and exit. The", "dateLastCrawled": "2021-09-15T19:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "arxiv-cs-analysis/cluster_phrase_semicolon_50.txt at master \u00b7 tf-dbis ...", "url": "https://github.com/tf-dbis-uni-freiburg/arxiv-cs-analysis/blob/master/Noun%20Phrase%20Frequencies%20Visualization/NPFreqSolrDash/cluster_phrase_semicolon_50.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/tf-dbis-uni-freiburg/arxiv-cs-analysis/blob/master/Noun Phrase...", "snippet": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.", "dateLastCrawled": "2022-01-31T10:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Driving</b> <b>Behavior for ADAS and Autonomous Driving VI</b>", "url": "https://www.slideshare.net/yuhuang/driving-behavior-for-adas-and-autonomous-driving-vi", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/yuhuang/<b>driving</b>-<b>behavior-for-adas-and-autonomous-driving-vi</b>", "snippet": "\u2022 In this work, develop a multi-modal architecture that includes the environmental modeling of ego surrounding and train a deep reinforcement learning (DRL) agent that yields consistent performance in <b>stochastic</b> <b>highway</b> <b>driving</b> scenarios. \u2022 To this end, feed the occupancy grid of the ego surrounding into the DRL agent and obtain the high-level sequential commands (i.e. lane change) to send them to lower-level controllers. \u2022 Dividing the autonomous <b>driving</b> problem into a multi-layer ...", "dateLastCrawled": "2022-02-01T02:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Lane &amp; Vehicle Detection in Autonomous Driving</b> | Intel DevMesh ...", "url": "https://devmesh.intel.com/projects/cnn-based-lane-vechile-detection", "isFamilyFriendly": true, "displayUrl": "https://devmesh.intel.com/projects/cnn-based-lane-vechile-detection", "snippet": "<b>Similar</b> to vehicle detection, we use L1 loss to train the regressor. <b>Gradient</b> <b>Descent</b> : <b>mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> for optimisation. Learning rate : controlled by a variant of the momentum scheduler. DBSCAN to cluster the line segments into lanes. Technologies Used. Framework(s) / topics or concepts/ systems. Convolutional Neural ...", "dateLastCrawled": "2022-01-14T10:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An <b>Empirical Evaluation of Deep Learning on Highway Driving</b> | DeepAI", "url": "https://deepai.org/publication/an-empirical-evaluation-of-deep-learning-on-highway-driving", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/an-<b>empirical-evaluation-of-deep-learning-on-highway-driving</b>", "snippet": "<b>Similar</b> to the vehicle detector, we use L1 loss to train the regressor. We use <b>mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> for optimization. The learning rate is controlled by a variant of the momentum scheduler . To obtain semantic lane information, we use DBSCAN to cluster the line segments into lanes.", "dateLastCrawled": "2021-12-07T11:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "CS231n Summary \u00b7 Imron Rosyadi", "url": "https://irosyadi.github.io/course/cs231n-summary.html", "isFamilyFriendly": true, "displayUrl": "https://irosyadi.github.io/course/cs231n-summary.html", "snippet": "As a revision here are the <b>Mini batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm steps: Loop: Sample a batch of data. Forward prop it through the graph (network) and get loss. Backprop to calculate the gradients. Update the parameters using the gradients. Activation functions: Different choices for activation function includes Sigmoid, tanh, RELU, Leaky RELU, Maxout, and ELU. Sigmoid: Squashes the numbers between [0,1] Used as a firing rate like human brains. Sigmoid(x) = 1 / (1 + e^-x) Problems ...", "dateLastCrawled": "2022-01-30T07:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Learning from Human Generated Rewards", "url": "https://www.in.tum.de/fileadmin/w00bws/i06/Teaching/SS18/RLAD_F.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.in.tum.de/fileadmin/w00bws/i06/Teaching/SS18/RLAD_F.pdf", "snippet": "\u2022Perform <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> updates a) Whenever the human provides new feedback b) At a fixed rate, using sampled data \u2022In practice, <b>mini-batch</b> updates were performed SS 2018 Seminar Reinforcement Learning for Autonomous <b>Driving</b> (Alexander Kurz, Julian Kunze) 27 (Warnell et al., 2017) Robotics and Embedded Systems Department of Computer Science Technical University of Munich Prof. Dr.-Ing. habil. Alois Knoll Strategies to efficiently learn the Parameters 3) Predict human reward ...", "dateLastCrawled": "2022-01-27T14:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Equality of opportunity in travel behavior prediction with deep neural ...", "url": "https://www.sciencedirect.com/science/article/pii/S0968090X21004058", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0968090X21004058", "snippet": "Experiments conducted in TensorFlow all use the <b>mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> method with a batch size of 1,000 and a step size of 0.0001 in each training. We draw samples without replacement to generate the mini-batches within an epoch. After a <b>mini-batch</b> is generated, the algorithm calculates the prediction loss and updates the coefficients. The model that produces the lowest training loss among the 50 epochs is chosen and later performs prediction over the test data. We ran 5 ...", "dateLastCrawled": "2022-01-20T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An algorithm for <b>highway</b> <b>vehicle detection</b> based on convolutional ...", "url": "https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-018-0350-2", "isFamilyFriendly": true, "displayUrl": "https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-018-0350-2", "snippet": "During training phase, <b>stochastic</b> <b>gradient</b> <b>descent</b> (SGD) is used to optimize our network. We initialize the parameters for all the newly added layers by drawing weights from a zero-mean Gaussian distribution with standard deviation 0.01. We set the learning rate as 0.001 for the first 60k iterations and 0.0001 for the next 60k iterations. The batch size is 16 for a 320 \u00d7 320 model. We use a momentum of 0.9 and a weight decay of 0.005.", "dateLastCrawled": "2022-01-30T17:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "notes-1/Deep Learning.md at master \u00b7 kirk86/notes-1 \u00b7 <b>GitHub</b>", "url": "https://github.com/kirk86/notes-1/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kirk86/notes-1/blob/master/Deep Learning.md", "snippet": "scalable and parallelizable training with <b>stochastic</b> <b>gradient</b> <b>descent</b> using similarity ranking loss or log-likelihood loss; more data and bigger the models, the more impressive the results; limitations: fixed capacity: one has to choose a dimension before training and it determines the capacity of the representation, then one can&#39;t increase it as more data comes in without retraining the whole thing from the scratch; partial lack of interpretability: distributed representations are black ...", "dateLastCrawled": "2021-12-24T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "notes-2/Deep Learning.md at master \u00b7 rsantana-isg/notes-2", "url": "https://github.com/rsantana-isg/notes-2/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rsantana-isg/notes-2/blob/master/Deep Learning.md", "snippet": "&quot;Dumb stuff like <b>stochastic</b> <b>gradient</b> <b>descent</b> working so well raises huge problems for GOFAI advocates. These techniques are always going to beat a smart system which can&#39;t learn, provided they can learn a huge number of parameters. So the real lesson here is that dumb systems that learn are better than smart systems that don&#39;t. And the other lesson is, of course, that smart systems that learn billions of parameters are going to be even better. Models should be bigger than the data. Ex: your ...", "dateLastCrawled": "2022-01-02T20:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving</b>", "url": "https://www.researchgate.net/publication/309008927_Safe_Multi-Agent_Reinforcement_Learning_for_Autonomous_Driving", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/309008927_Safe_Multi-Agent_Reinforcement...", "snippet": "Endowing a robotic <b>car</b> with the ability to form long term <b>driving</b> strategies, ... from a <b>mini-batch</b> of episodes. and then set . b \u00b7,i. to be. X \u2212 1 y. A more ef\ufb01cient approach is to think ...", "dateLastCrawled": "2022-01-21T21:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Driving</b> <b>Behavior for ADAS and Autonomous Driving VI</b>", "url": "https://www.slideshare.net/yuhuang/driving-behavior-for-adas-and-autonomous-driving-vi", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/yuhuang/<b>driving</b>-<b>behavior-for-adas-and-autonomous-driving-vi</b>", "snippet": "\u2022 Most of the current work in IRL utilizes the maximum entropy principle that allows training of a probabilistic model by <b>gradient</b> <b>descent</b>. \u2022 The <b>gradient</b> calculation depends on the state visitation frequency, which is often calculated by an algorithm <b>similar</b> to backward value iteration in reinforcement learning. \u2022 Due to the curse of dimensionality, this algorithm is intractable for <b>driving</b> style optimization in high-dimensional continuous spaces.", "dateLastCrawled": "2022-02-01T02:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS231n Summary \u00b7 Imron Rosyadi", "url": "https://irosyadi.github.io/course/cs231n-summary.html", "isFamilyFriendly": true, "displayUrl": "https://irosyadi.github.io/course/cs231n-summary.html", "snippet": "As a revision here are the <b>Mini batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm steps: Loop: Sample a batch of data. Forward prop it through the graph (network) and get loss. Backprop to calculate the gradients. Update the parameters using the gradients. Activation functions: Different choices for activation function includes Sigmoid, tanh, RELU, Leaky RELU, Maxout, and ELU. Sigmoid: Squashes the numbers between [0,1] Used as a firing rate like human brains. Sigmoid(x) = 1 / (1 + e^-x) Problems ...", "dateLastCrawled": "2022-01-30T07:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Towards a machine-learning based approach for splitting cities in ...", "url": "https://www.researchgate.net/publication/358300050_Towards_a_machine-learning_based_approach_for_splitting_cities_in_freight_logistics_context_Benchmarks_of_clustering_and_prediction_models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/358300050_Towards_a_machine-learning_based...", "snippet": "Using our method with a <b>mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> inference procedure, we are able to outperform prior work on clustering millions of ImageNet images by 15 points of dendrogram purity ...", "dateLastCrawled": "2022-02-07T10:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Drive-Net: Convolutional Network for Driver Distraction Detection - DeepAI", "url": "https://deepai.org/publication/drive-net-convolutional-network-for-driver-distraction-detection", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/drive-net-convolutional-network-for-driver-distraction...", "snippet": "The classifier was trained using the <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm, ... and updated in a <b>mini-batch</b> scheme of 128 candidates. The biases were initialized with zero, and the learning rate was set to \u03b1 = 0.001. The exponential decay rates for the first- and second-moment estimates were set as . \u03b2 1 = 0.9 and \u03b2 2 = 0.99, respectively. We used \u03f5 = 10 \u2212 8 to prevent division by zero. A dropout rate of 0.5 was implemented as regularization, applied to the output of the last ...", "dateLastCrawled": "2021-12-27T11:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "notes-2/Deep Learning.md at master \u00b7 rsantana-isg/notes-2 \u00b7 GitHub", "url": "https://github.com/rsantana-isg/notes-2/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rsantana-isg/notes-2/blob/master/Deep Learning.md", "snippet": "We <b>can</b> then use <b>gradient</b> <b>descent</b> on a continuous representation of the blueprint to optimize for the fastest <b>car</b>. Right now, this approach doesn\u2019t work very well, because you don\u2019t get an input that is actually optimal in the real world. Instead, you get an adversarial example that the model thinks will perform great but turns out to perform poorly in the real world. For example, if you start your optimization with a photo of an airplane, then use <b>gradient</b> <b>descent</b> to search for an image ...", "dateLastCrawled": "2022-01-02T20:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "notes-1/Deep Learning.md at master \u00b7 kirk86/notes-1 \u00b7 <b>GitHub</b>", "url": "https://github.com/kirk86/notes-1/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kirk86/notes-1/blob/master/Deep Learning.md", "snippet": "scalable and parallelizable training with <b>stochastic</b> <b>gradient</b> <b>descent</b> using similarity ranking loss or log-likelihood loss; more data and bigger the models, the more impressive the results ; limitations: fixed capacity: one has to choose a dimension before training and it determines the capacity of the representation, then one <b>can</b>&#39;t increase it as more data comes in without retraining the whole thing from the scratch; partial lack of interpretability: distributed representations are black ...", "dateLastCrawled": "2021-12-24T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "GitHub - amsks/amsks.github.io: Code and contents of my website, format ...", "url": "https://github.com/amsks/amsks.github.io", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/amsks/amsks.github.io", "snippet": "One of the places where Global Bayesian Optimization <b>can</b> show good results is the optimization of hyperparameters for Neural Networks. So, let&#39;s implement this approach to tune the learning rate of an Image Classifier! I will use the KMNIST dataset and a small ResNet-9 Model with a <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> optimizer. Our plan of attack is as ...", "dateLastCrawled": "2021-08-12T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Comprehensive survey of deep learning in remote sensing: theories</b> ...", "url": "https://www.spiedigitallibrary.org/journals/journal-of-applied-remote-sensing/volume-11/issue-04/042609/Comprehensive-survey-of-deep-learning-in-remote-sensing--theories/10.1117/1.JRS.11.042609.full?SSO=1", "isFamilyFriendly": true, "displayUrl": "https://www.spiedigitallibrary.org/journals/journal-of-applied-remote-sensing/volume...", "snippet": "A DBN <b>can</b> also <b>be thought</b> of as a type of deep NN. In Ref. ... CNNs are typically trained using <b>stochastic</b> <b>gradient</b> <b>descent</b> (SGD), SGD with momentum, 52 AdaGrad, 53 RMSProp, 54 and ADAM. 55 For details on the pros and cons of these algorithms, refer to Secs. 8.3 and 8.5 of Ref. 23. There are also second-order methods, and these are discussed in Sec. 8.6 of Ref. 23. A good history of DL is provided in Ref. 56, and training is discussed in Sec. 5.24. Further discussions in this paper <b>can</b> be ...", "dateLastCrawled": "2022-01-31T11:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "iclr2017-reviews-dataset/iclr2017_papers.csv at master - <b>GitHub</b>", "url": "https://github.com/ahmaurya/iclr2017-reviews-dataset/blob/master/iclr2017_papers.csv", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ahmaurya/iclr2017-reviews-dataset/blob/master/iclr2017_papers.csv", "snippet": "&quot;The <b>stochastic</b> <b>gradient</b> <b>descent</b> (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$--$512$ data points, is sampled to compute an approximation to the <b>gradient</b>. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization ...", "dateLastCrawled": "2021-08-31T02:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Driving</b> <b>Behavior for ADAS and Autonomous Driving VI</b>", "url": "https://www.slideshare.net/yuhuang/driving-behavior-for-adas-and-autonomous-driving-vi", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/yuhuang/<b>driving</b>-<b>behavior-for-adas-and-autonomous-driving-vi</b>", "snippet": "Combining Planning and Deep Reinforcement Learning in Tactical Decision Making for Autonomous <b>Driving</b> \u2022 The framework for combining planning and reinforcement learning <b>can</b> be applied to autonomous <b>driving</b>. \u2022 In study, the properties of the framework were investigated for two <b>highway</b> <b>driving</b> cases. \u2022 Then the driver and physical modeling of the two cases, which is used both as a generative model and for simulating the environment. \u2022 The Intelligent Driver Model (IDM) was to govern the ...", "dateLastCrawled": "2022-02-01T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "arxiv-cs-analysis/cluster_phrase_semicolon_50.txt at master \u00b7 tf-dbis ...", "url": "https://github.com/tf-dbis-uni-freiburg/arxiv-cs-analysis/blob/master/Noun%20Phrase%20Frequencies%20Visualization/NPFreqSolrDash/cluster_phrase_semicolon_50.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/tf-dbis-uni-freiburg/arxiv-cs-analysis/blob/master/Noun Phrase...", "snippet": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.", "dateLastCrawled": "2022-01-31T10:39:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS231n Summary \u00b7 Imron Rosyadi", "url": "https://irosyadi.github.io/course/cs231n-summary.html", "isFamilyFriendly": true, "displayUrl": "https://irosyadi.github.io/course/cs231n-summary.html", "snippet": "As a revision here are the <b>Mini batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm steps: Loop: Sample a batch of data. Forward prop it through the graph (network) and get loss. Backprop to calculate the gradients. Update the parameters using the gradients. Activation functions: Different choices for activation function includes Sigmoid, tanh, RELU, Leaky RELU, Maxout, and ELU. Sigmoid: Squashes the numbers between [0,1] Used as a firing rate like human brains. Sigmoid(x) = 1 / (1 + e^-x) Problems ...", "dateLastCrawled": "2022-01-30T07:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Equality of opportunity in travel behavior prediction with deep neural ...", "url": "https://www.sciencedirect.com/science/article/pii/S0968090X21004058", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0968090X21004058", "snippet": "Experiments conducted in TensorFlow all use the <b>mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> method with a batch size of 1,000 and a step size of 0.0001 in each training. We draw samples without replacement to generate the mini-batches within an epoch. After a <b>mini-batch</b> is generated, the algorithm calculates the prediction loss and updates the coefficients. The model that produces the lowest training loss among the 50 epochs is chosen and later performs prediction over the test data. We ran 5 ...", "dateLastCrawled": "2022-01-20T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Review of deep learning: concepts, CNN architectures, challenges ...", "url": "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00444-8", "isFamilyFriendly": true, "displayUrl": "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00444-8", "snippet": "<b>Mini-batch</b> <b>Gradient</b> <b>Descent</b>: In this approach, the training samples are partitioned into several mini-batches, in which every <b>mini-batch</b> <b>can</b> be considered an under-sized collection of samples with no overlap between them . Next, parameter updating is performed following <b>gradient</b> computation on every <b>mini-batch</b>. The advantage of this method comes from combining the advantages of both BGD and SGD techniques. Thus, it has a steady convergence, more computational efficiency and extra memory ...", "dateLastCrawled": "2022-02-02T04:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "VIL-100: A New Dataset and A Baseline Model for Video Instance Lane ...", "url": "https://deepai.org/publication/vil-100-a-new-dataset-and-a-baseline-model-for-video-instance-lane-detection", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/vil-100-a-new-dataset-and-a-baseline-model-for-video...", "snippet": "In the second training stage, <b>stochastic</b> <b>gradient</b> <b>descent</b> optimizer is employed to optimize the whole network with the learning rate as . 10 \u2212 3, a momentum value of 0.9, a weight decay of 10 \u2212 6, and a <b>mini-batch</b> size of 1. The first training stage takes about 14 hours with 100 epochs while the second training stages takes about 7 hours ...", "dateLastCrawled": "2022-01-20T08:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An algorithm for <b>highway</b> <b>vehicle detection</b> based on convolutional ...", "url": "https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-018-0350-2", "isFamilyFriendly": true, "displayUrl": "https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-018-0350-2", "snippet": "During training phase, <b>stochastic</b> <b>gradient</b> <b>descent</b> (SGD) is used to optimize our network. We initialize the parameters for all the newly added layers by drawing weights from a zero-mean Gaussian distribution with standard deviation 0.01. We set the learning rate as 0.001 for the first 60k iterations and 0.0001 for the next 60k iterations. The batch size is 16 for a 320 \u00d7 320 model. We use a momentum of 0.9 and a weight decay of 0.005.", "dateLastCrawled": "2022-01-30T17:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "notes-2/Deep Learning.md at master \u00b7 rsantana-isg/notes-2 \u00b7 GitHub", "url": "https://github.com/rsantana-isg/notes-2/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rsantana-isg/notes-2/blob/master/Deep Learning.md", "snippet": "We <b>can</b> then use <b>gradient</b> <b>descent</b> on a continuous representation of the blueprint to optimize for the fastest <b>car</b>. Right now, this approach doesn\u2019t work very well, because you don\u2019t get an input that is actually optimal in the real world. Instead, you get an adversarial example that the model thinks will perform great but turns out to perform poorly in the real world. For example, if you start your optimization with a photo of an airplane, then use <b>gradient</b> <b>descent</b> to search for an image ...", "dateLastCrawled": "2022-01-02T20:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An Empirical Evaluation of Deep Learning on <b>Highway</b> <b>Driving</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1504.01716/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1504.01716", "snippet": "We collect a large data set of <b>highway</b> data and apply deep learning and computer vision algorithms to problems such as <b>car</b> and lane detection. We show how existing convolutional neural networks (CNNs) <b>can</b> be used to perform lane and vehicle detection while running at frame rates required for a real-time system. Our results lend credence to the hypothesis that deep learning holds promise for autonomous <b>driving</b>. I Introduction. Since the DARPA Grand Challenges for autonomous vehicles, there ...", "dateLastCrawled": "2022-01-15T11:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "An <b>Empirical Evaluation of Deep Learning on Highway Driving</b> | DeepAI", "url": "https://deepai.org/publication/an-empirical-evaluation-of-deep-learning-on-highway-driving", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/an-<b>empirical-evaluation-of-deep-learning-on-highway-driving</b>", "snippet": "Lastly, assuming our <b>car</b> drives close to the center of the lane, we filter out ground paint other than the ego-lane boundaries, such as other lane boundaries, <b>car</b>-pool or directional signs, by only considering markings whose absolute lateral distances from the <b>car</b> are smaller than 2.2 meters and greater than 1.4 meters. We <b>can</b> also distinguish the left boundary from the right one using the sign of the lateral distance. After obtaining the points in the left and right boundaries, we fit a ...", "dateLastCrawled": "2021-12-07T11:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving</b>", "url": "https://www.researchgate.net/publication/309008927_Safe_Multi-Agent_Reinforcement_Learning_for_Autonomous_Driving", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/309008927_Safe_Multi-Agent_Reinforcement...", "snippet": "Endowing a robotic <b>car</b> with the ability to form long term <b>driving</b> strategies, ... from a <b>mini-batch</b> of episodes. and then set . b \u00b7,i. to be. X \u2212 1 y. A more ef\ufb01cient approach is to think ...", "dateLastCrawled": "2022-01-21T21:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) Hands-<b>On Machine Learning with Scikit-Learn &amp; TensorFlow</b> | J k ...", "url": "https://www.academia.edu/37010160/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/37010160/Hands_<b>On_Machine_Learning_with_Scikit_Learn</b>_and...", "snippet": "Machine Learning <b>can</b> help humans learn To summarize, Machine Learning is great for: \u2022 Problems for which existing solutions require a lot of hand-tuning or long lists of rules: one Machine Learning algorithm <b>can</b> often simplify code and perform bet\u2010 ter. \u2022 Complex problems for which there is no good solution at all using a traditional approach: the best Machine Learning techniques <b>can</b> find a solution. \u2022 Fluctuating environments: a Machine Learning system <b>can</b> adapt to new data ...", "dateLastCrawled": "2022-01-30T23:52:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Batch vs <b>Mini-batch</b> vs <b>Stochastic Gradient Descent</b> with Code Examples ...", "url": "https://medium.datadriveninvestor.com/batch-vs-mini-batch-vs-stochastic-gradient-descent-with-code-examples-cd8232174e14", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/batch-vs-<b>mini-batch</b>-vs-<b>stochastic</b>-<b>gradient</b>...", "snippet": "It is possible to use only the <b>Mini-batch</b> <b>Gradient Descent</b> code to implement all versions of <b>Gradient Descent</b>, you just need to set the <b>mini_batch</b>_size equals one to <b>Stochastic</b> GD or the number of training examples to Batch GD. Thus, the main difference between Batch, <b>Mini-batch</b>, and <b>Stochastic Gradient Descent</b> is the number of examples used for each epoch and the time and effort necessary to reach the global minimum value of the Cost Function.", "dateLastCrawled": "2022-01-27T13:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Batch, <b>Mini Batch</b> &amp; <b>Stochastic</b> <b>Gradient Descent</b> | by Sushant Patrikar ...", "url": "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/batch-<b>mini-batch</b>-<b>stochastic</b>-<b>gradient-descent</b>-7a62ecba642a", "snippet": "<b>Mini Batch</b> <b>Gradient Descent</b>. We have seen the Batch <b>Gradient Descent</b>. We have also seen the <b>Stochastic</b> <b>Gradient Descent</b>. Batch <b>Gradient Descent</b> can be used for smoother curves. SGD can be used when the dataset is large. Batch <b>Gradient Descent</b> converges directly to minima. SGD converges faster for larger datasets. But, since in SGD we use only ...", "dateLastCrawled": "2022-02-02T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Batch vs <b>Mini-batch vs Stochastic Gradient Descent</b> with Code Examples", "url": "https://edgeaiguru.com/Batch-vs-Mini-batch-vs-Stochastic-Gradient-Descent", "isFamilyFriendly": true, "displayUrl": "https://edgeaiguru.com/Batch-vs-<b>Mini-batch-vs-Stochastic-Gradient-Descent</b>", "snippet": "It is possible to use only the <b>Mini-batch</b> <b>Gradient</b> <b>Descent</b> code to implement all versions of <b>Gradient</b> <b>Descent</b>, you just need to set the <b>mini_batch</b>_size equals one to <b>Stochastic</b> GD or to the number of training examples to Batch GD. Thus, the main difference between Batch, <b>Mini-batch</b> and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> is the number of examples used for each epoch and the time and effort necessary to reach the global minimum value of the Cost Function.", "dateLastCrawled": "2022-01-27T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "13.6 <b>Stochastic and mini-batch gradient descent</b>", "url": "https://kenndanielso.github.io/mlrefined/blog_posts/13_Multilayer_perceptrons/13_6_Stochastic_and_minibatch_gradient_descent.html", "isFamilyFriendly": true, "displayUrl": "https://kenndanielso.github.io/.../13_6_<b>Stochastic_and_minibatch_gradient_descent</b>.html", "snippet": "It is noteworthy that because moderately accurate solutions (provided by a moderate amount of minimization of a cost function) tend to perform reasonably well in <b>machine</b> <b>learning</b> applications, and because with large datasets a random initialization will tend to lie far from a convergent point, in many cases even a single iteration of <b>stochastic</b>/<b>mini-batch</b> <b>gradient</b> <b>descent</b> can provide a good solution.", "dateLastCrawled": "2022-02-02T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient</b> <b>Descent</b>: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-<b>descent</b>", "snippet": "Types of <b>gradient</b> <b>descent</b>: batch, <b>stochastic</b>, <b>mini-batch</b>; Introduction to <b>Gradient</b> <b>Descent</b>. <b>Gradient</b> <b>descent</b> is an optimization algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum. What is <b>Gradient</b> <b>Descent</b>? <b>Gradient</b> <b>Descent</b> is an optimization algorithm for finding a local minimum of a differentiable function. <b>Gradient</b> <b>descent</b> is simply used in <b>machine</b> <b>learning</b> to ...", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Empirical Risk Minimization and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "models, <b>mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> (SGD) can e\ufb03ciently solve the minimization problem (albeit, approximately). The ease of SGD comes from the de\ufb01- nition of the empirical risk as the expectation over a randomly subsampled example: the <b>gradient</b> of the loss on a randomly subsampled example is an unbiased es-timate of the <b>gradient</b> of the empirical risk. Combined with automatic di\ufb00erentiation, this provides a turnkey approach to \ufb01tting <b>machine</b>-<b>learning</b> models. Returning to ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding the 3 Primary <b>Types of Gradient Descent</b> | by ODSC - Open ...", "url": "https://medium.com/odscjournal/understanding-the-3-primary-types-of-gradient-descent-987590b2c36", "isFamilyFriendly": true, "displayUrl": "https://medium.com/odscjournal/understanding-the-3-primary-<b>types-of-gradient-descent</b>...", "snippet": "<b>Mini Batch</b> <b>Gradient Descent</b> is commonly used for deep <b>learning</b> problems. Conclusion This article should give you the basic motivation for the <b>gradient descent</b> process in <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2022-02-03T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! | by Aishwarya V ...", "url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>stochastic-gradient-descent</b>-clearly-explained-53d239905d31", "snippet": "<b>Stochastic gradient descent</b> is a very popular and common algorithm used in various <b>Machine</b> <b>Learning</b> algorithms, most importantly forms the basis of Neural Networks. In this article, I have tried my\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! Aishwarya V Srinivasan. Sep 7, 2019 \u00b7 4 min read. <b>Stochastic gradient descent</b> is a ...", "dateLastCrawled": "2022-02-02T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Basics and Beyond: <b>Gradient Descent</b> | by Kumud Lakara | The Startup ...", "url": "https://medium.com/swlh/basics-and-beyond-gradient-descent-87fa964c31dd", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/basics-and-beyond-<b>gradient-descent</b>-87fa964c31dd", "snippet": "3. <b>Mini-batch Gradient Descent</b>. This is actually the best of both worlds. It accounts for the computational expenses in case of <b>batch gradient descent</b> and the high variance in case of SGD. Mini ...", "dateLastCrawled": "2021-05-02T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "11.5. <b>Minibatch</b> <b>Stochastic</b> <b>Gradient Descent</b> \u2014 Dive into Deep <b>Learning</b> 0 ...", "url": "http://d2l.ai/chapter_optimization/minibatch-sgd.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_optimization/<b>minibatch</b>-sgd.html", "snippet": "So far we encountered two extremes in the approach to <b>gradient</b> based <b>learning</b>: Section 11.3 uses the full dataset to compute gradients and to update parameters, one pass at a time. Conversely Section 11.4 processes one observation at a time to make progress. Each of them has its own drawbacks. <b>Gradient Descent</b> is not particularly data efficient whenever data is very similar. <b>Stochastic</b> <b>Gradient Descent</b> is not particularly computationally efficient since CPUs and GPUs cannot exploit the full ...", "dateLastCrawled": "2022-02-01T18:13:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(mini-batch stochastic gradient descent)  is like +(driving a car on a highway)", "+(mini-batch stochastic gradient descent) is similar to +(driving a car on a highway)", "+(mini-batch stochastic gradient descent) can be thought of as +(driving a car on a highway)", "+(mini-batch stochastic gradient descent) can be compared to +(driving a car on a highway)", "machine learning +(mini-batch stochastic gradient descent AND analogy)", "machine learning +(\"mini-batch stochastic gradient descent is like\")", "machine learning +(\"mini-batch stochastic gradient descent is similar\")", "machine learning +(\"just as mini-batch stochastic gradient descent\")", "machine learning +(\"mini-batch stochastic gradient descent can be thought of as\")", "machine learning +(\"mini-batch stochastic gradient descent can be compared to\")"]}