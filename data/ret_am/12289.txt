{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The <b>Algorithm Behind the Curtain: Reinforcement Learning Concepts</b> (2 of ...", "url": "https://randomant.net/reinforcement-learning-concepts/", "isFamilyFriendly": true, "displayUrl": "https://randomant.net/reinforcement-learning-concepts", "snippet": "<b>Action</b>-<b>Value</b> <b>Function</b>: The mathematical sister of the <b>state</b>-<b>value</b> <b>function</b> is the <b>action</b>-<b>value</b> <b>function</b>, represented as Q. Whereas the <b>state</b>-<b>value</b> <b>function</b> is concerned with determining the total future reward of each <b>state</b>, the <b>action</b>-<b>value</b> <b>function</b> seeks to determine the total future reward of each <b>state/action</b> pair. Although both functions accomplish a similar objective, they are used in different ways in different RL algorithms. The equation for the <b>action</b>-<b>value</b> <b>function</b> is below. It is ...", "dateLastCrawled": "2022-01-31T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Policy Gradient</b> Algorithms - Lil&#39;Log", "url": "https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html", "isFamilyFriendly": true, "displayUrl": "https://lilianweng.github.io/lil-log/2018/04/08/<b>policy-gradient</b>-algorithms.html", "snippet": "<b>Action</b>-<b>value</b> <b>function</b> is similar to \\(V(s)\\), but it assesses the expected return of a pair of <b>state</b> and <b>action</b> \\((s, a)\\); \\(Q_w(.)\\) is a <b>action</b> <b>value</b> <b>function</b> parameterized by \\(w\\). \\(Q^\\pi(s, a)\\) Similar to \\(V^\\pi(.)\\), the <b>value</b> of (<b>state, action</b>) pair when we follow a <b>policy</b> \\(\\pi\\); \\(Q^\\pi(s, a) = \\mathbb{E}_{a\\sim \\pi} [G_t \\vert S_t = s, A_t = a]\\). \\(A(s, a)\\) Advantage <b>function</b>, \\(A(s, a) = Q(s, a) - V(s)\\); it can be considered as another version of Q-<b>value</b> with lower ...", "dateLastCrawled": "2022-02-03T21:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Playing <b>Pong</b> using Reinforcement Learning | by Omkar Vedpathak ...", "url": "https://towardsdatascience.com/intro-to-reinforcement-learning-pong-92a94aa0f84d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/intro-to-reinforcement-learning-<b>pong</b>-92a94aa0f84d", "snippet": "<b>Value</b> functions are a way to evaluate how good a <b>given</b> <b>state</b>, or <b>state-action</b> pair is, which can be used to select an <b>action</b> that tries to transition the Agent into the future states. There are multiple types of <b>value</b> functions which make different assumptions about the policy will be used in calculating the <b>value</b>. For a great resource, look at", "dateLastCrawled": "2022-02-02T10:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Markov Decision Processes and Reinforcement Learning</b>", "url": "https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/2015-08-02-<b>markov-decision-processes-and-reinforcement</b>...", "snippet": "It is also common to define a new quantity called a Q-<b>value</b> with respect to <b>state-action</b> pairs: \\[Q^*(s,a) = \\sum_{s&#39;} P(s,a,s&#39;)[R(s,a,s&#39;) + \\gamma V^*(s&#39;)]\\] In words, \\(Q(s,a)\\) is the expected utility starting at <b>state</b> \\(s\\), <b>taking</b> <b>action</b> \\(a\\), and hereafter, playing optimally. We can therefore relate the \\(V\\)s and \\(Q\\)s with the following equation: \\[V^*(s) = \\max_a Q^*(s,a)\\] I find it easiest to think of these in terms of expectimax trees with chance nodes. The \u201cnormal\u201d nodes ...", "dateLastCrawled": "2022-01-30T21:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Deep Reinforcement Learning</b> for Trading | The Journal of Financial Data ...", "url": "https://jfds.pm-research.com/content/early/2020/03/16/jfds.2020.1.030", "isFamilyFriendly": true, "displayUrl": "https://jfds.pm-research.com/content/early/2020/03/16/jfds.2020.1.030", "snippet": "A <b>state-action</b> <b>value</b> <b>function</b>, Q, is constructed to represent how good <b>a particular</b> <b>action</b> is in a <b>state</b>. Discrete <b>action</b> spaces are adopted in these works, and an agent is trained to fully go long or short a position. However, a fully invested position is risky during high-volatility periods, exposing one to severe risk when opposite moves occur. Ideally, one would <b>like</b> to scale positions up or down according to current market conditions. Doing this requires one to have large <b>action</b> spaces ...", "dateLastCrawled": "2022-01-30T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Damped Anderson Mixing for Deep Reinforcement Learning: Acceleration ...", "url": "https://deepai.org/publication/damped-anderson-mixing-for-deep-reinforcement-learning-acceleration-convergence-and-stabilization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/damped-anderson-mixing-for-deep-reinforcement-learning...", "snippet": "The Q-<b>value</b> <b>function</b> evaluates the expected return starting from a <b>given</b> <b>state-action</b> pair (s, a), that is, Q \u03c0 (s, a) = E [\u2211 \u221e t = 0 \u03b3 t R t + 1 \u2223 s 0 = s, a 0 = a]. A policy \u03c0 (a | s) is a distribution mapping the <b>state</b> space S to the <b>action</b> space A. 2.1 Anderson Acceleration in Policy Iteration. We focus on the tabular case to enable the theoretical analysis of Anderson acceleration in <b>value</b> (policy) iteration, which can be naturally applied to <b>function</b> approximation. Both the ...", "dateLastCrawled": "2022-01-29T03:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Playing a <b>toy poker game with Reinforcement Learning</b>", "url": "http://willtipton.com/coding/poker/2017/06/06/shove-fold-with-reinforcement-learning.html", "isFamilyFriendly": true, "displayUrl": "willtipton.com/coding/poker/2017/06/06/shove-fold-with-reinforcement-learning.html", "snippet": "Linear <b>function</b> approximator: We learned a linear <b>function</b> to map from our representation of the <b>state-action</b> pair to the <b>value</b>. Alternatives include simple tables which store a separate estimate of the <b>value</b> of every <b>action</b> in every <b>state</b> as well as many other types of <b>function</b> approximators. Neural networks, in <b>particular</b>, have been very successful. To some degree, this is because they don\u2019t require much feature engineering to get good results. Neural nets can often learn both a good set ...", "dateLastCrawled": "2022-02-03T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - <b>readyforchaos/Reinforcement-QLearning-Epsilon-Greedy</b>: Pirate ...", "url": "https://github.com/readyforchaos/Reinforcement-QLearning-Epsilon-Greedy", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/readyforchaos/Reinforcement-QLearning-Epsilon-Greedy", "snippet": "For a <b>given</b> <b>state</b>, we need the ability to retrieve the index of the <b>action</b> with the highest Q-<b>value</b>. This is used to communicate to the environment which <b>action</b> the agent should take. We also need to get the <b>value</b> of the <b>action</b> with the highest Q-<b>value</b> (in a <b>state</b>), as we use in when calculating the new Q-<b>value</b> for another <b>state</b>. The reason is ...", "dateLastCrawled": "2022-01-31T03:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Applications of <b>Reinforcement Learning</b> in Real World | by garychl ...", "url": "https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/applications-of-<b>reinforcement-learning</b>-in-real-world-1a...", "snippet": "For example, to find a good policy we could use valued-based methods <b>like</b> Q-learning to measure how good an <b>action</b> is in <b>a particular</b> <b>state</b> or policy-based methods to directly find out what actions to take under different states without knowing how good the actions are. However, the problems we face in the real world can be extremely complicated in many different ways and therefore a typical RL algorithm has no clue to solve. For example, the <b>state</b> space is very large in the game of GO ...", "dateLastCrawled": "2022-02-02T20:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "reactjs - Why is immutability so important (or needed) in JavaScript ...", "url": "https://stackoverflow.com/questions/34385243/why-is-immutability-so-important-or-needed-in-javascript", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/34385243", "snippet": "Another <b>benefit</b> of Immutability in Javascript is that it reduces Temporal Coupling, which has substantial benefits for design generally. Consider the interface of an object with two methods: It may be the case that a call to baz () is required to get the object in a valid <b>state</b> for a call to bar () to work correctly.", "dateLastCrawled": "2022-01-28T15:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Policy Gradient</b> Algorithms", "url": "https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html", "isFamilyFriendly": true, "displayUrl": "https://lilianweng.github.io/lil-log/2018/04/08/<b>policy-gradient</b>-algorithms.html", "snippet": "<b>Action</b>-<b>value</b> <b>function</b> <b>is similar</b> to \\(V(s)\\), but it assesses the expected return of a pair of <b>state</b> and <b>action</b> \\((s, a)\\); \\(Q_w(.)\\) is a <b>action</b> <b>value</b> <b>function</b> parameterized by \\(w\\). \\(Q^\\pi(s, a)\\) <b>Similar</b> to \\(V^\\pi(.)\\), the <b>value</b> of (<b>state, action</b>) pair when we follow a <b>policy</b> \\(\\pi\\); \\(Q^\\pi(s, a) = \\mathbb{E}_{a\\sim \\pi} [G_t \\vert S_t = s, A_t = a]\\). \\(A(s, a)\\) Advantage <b>function</b>, \\(A(s, a) = Q(s, a) - V(s)\\); it can be considered as another version of Q-<b>value</b> with lower ...", "dateLastCrawled": "2022-02-03T21:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Algorithm Behind the Curtain: Reinforcement Learning Concepts</b> (2 of ...", "url": "https://randomant.net/reinforcement-learning-concepts/", "isFamilyFriendly": true, "displayUrl": "https://randomant.net/reinforcement-learning-concepts", "snippet": "<b>Action</b>-<b>Value</b> <b>Function</b>: The mathematical sister of the <b>state</b>-<b>value</b> <b>function</b> is the <b>action</b>-<b>value</b> <b>function</b>, represented as Q. Whereas the <b>state</b>-<b>value</b> <b>function</b> is concerned with determining the total future reward of each <b>state</b>, the <b>action</b>-<b>value</b> <b>function</b> seeks to determine the total future reward of each <b>state/action</b> pair. Although both functions accomplish a <b>similar</b> objective, they are used in different ways in different RL algorithms. The equation for the <b>action</b>-<b>value</b> <b>function</b> is below. It is ...", "dateLastCrawled": "2022-01-31T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Playing <b>Pong</b> using Reinforcement Learning | by Omkar Vedpathak ...", "url": "https://towardsdatascience.com/intro-to-reinforcement-learning-pong-92a94aa0f84d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/intro-to-reinforcement-learning-<b>pong</b>-92a94aa0f84d", "snippet": "<b>Value</b> functions are a way to evaluate how good a <b>given</b> <b>state</b>, or <b>state-action</b> pair is, which can be used to select an <b>action</b> that tries to transition the Agent into the future states. There are multiple types of <b>value</b> functions which make different assumptions about the policy will be used in calculating the <b>value</b>. For a great resource, look at", "dateLastCrawled": "2022-02-02T10:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>SmartIX: A database indexing agent based on reinforcement learning</b>", "url": "https://www.researchgate.net/publication/339937862_SmartIX_A_database_indexing_agent_based_on_reinforcement_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339937862_<b>SmartIX_A_database_indexing_agent</b>...", "snippet": "by a <b>state-action</b> <b>value</b> <b>function</b> Q \u03c0 that, for each <b>state- action</b> pair, returns a <b>value</b> computed based on the amount of reward an agent might expect in the long run by <b>taking</b>", "dateLastCrawled": "2022-01-27T01:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Deep Reinforcement Learning</b> for Trading | The Journal of Financial Data ...", "url": "https://jfds.pm-research.com/content/early/2020/03/16/jfds.2020.1.030", "isFamilyFriendly": true, "displayUrl": "https://jfds.pm-research.com/content/early/2020/03/16/jfds.2020.1.030", "snippet": "A <b>state-action</b> <b>value</b> <b>function</b>, Q, is constructed to represent how good <b>a particular</b> <b>action</b> is in a <b>state</b>. Discrete <b>action</b> spaces are adopted in these works, and an agent is trained to fully go long or short a position. However, a fully invested position is risky during high-volatility periods, exposing one to severe risk when opposite moves occur. Ideally, one would like to scale positions up or down according to current market conditions. Doing this requires one to have large <b>action</b> spaces ...", "dateLastCrawled": "2022-01-30T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Automated Database Indexing using Model-free Reinforcement</b> ... - DeepAI", "url": "https://deepai.org/publication/automated-database-indexing-using-model-free-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>automated-database-indexing-using-model</b>-free...", "snippet": "This method learns the values of <b>state-action</b> pairs, denoted by Q (s, a), <b>representing</b> the <b>value</b> <b>of taking</b> <b>action</b> a in a <b>state</b> s [23, Ch. 6]. Assuming that states can be described in terms of features that are well informative, such problem can be handled by using linear <b>function</b> approximation, which is to use a parameterized representation for the <b>state-action</b> <b>value</b> <b>function</b> other than a look-up table [ 25 ]", "dateLastCrawled": "2021-12-23T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Teaching machines to behave: Reinforcement Learning | by Diego Gomez ...", "url": "https://towardsdatascience.com/rl-4-all-1-3edac941fe37", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/rl-4-all-1-3edac941fe37", "snippet": "Specifically, we will rephrase the <b>value</b> <b>function</b> of a <b>given</b> <b>state</b> depend on the <b>value</b> functions of other states. Mathematically speaking, we will define them recursively. By definition, the optimal <b>value</b> <b>function</b> v*(s) is the expressed as the expected return G, starting at <b>state</b> s and <b>taking</b> an <b>action</b> a that maximizes this <b>value</b>.", "dateLastCrawled": "2022-01-08T13:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Lecture 7: Reinforcement learning</b> | CS236605: Deep Learning", "url": "https://vistalab-technion.github.io/cs236605/lecture_notes/lecture_07/", "isFamilyFriendly": true, "displayUrl": "https://vistalab-technion.github.io/cs236605/lecture_notes/lecture_07", "snippet": "<b>Given</b> an MDP and having the agent behavior fixed to some policy $\\pi$, we may predict how beneficial it is for the environment to be in a certain <b>state</b>, or for the agent to take a certain <b>action</b> in <b>a particular</b> <b>state</b>.", "dateLastCrawled": "2021-07-30T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "GitHub - Valdinious/Carbon-Footprint-Multi-Agent-Reinforcement-Learning ...", "url": "https://github.com/Valdinious/Carbon-Footprint-Multi-Agent-Reinforcement-Learning", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Valdinious/Carbon-Footprint-Multi-Agent-Reinforcement-Learning", "snippet": ", calculates Q-values, <b>representing</b> the reward r of an <b>action</b> a <b>in a given</b> <b>state</b> s, plus the maximum reward achievable by the best <b>action</b> a\u2019 in the next <b>state</b> s\u2019, discounted by the factor gamma $\\gamma$ (temporal-difference learning). Q* denotes the highest Q-<b>value</b> for a <b>given</b> row, thus defines the <b>action</b> <b>in a given</b> <b>state</b> that achieves the highest Q-<b>value</b> (Valkov, 2017).", "dateLastCrawled": "2022-01-30T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "reactjs - Why is immutability so important (or needed) in JavaScript ...", "url": "https://stackoverflow.com/questions/34385243/why-is-immutability-so-important-or-needed-in-javascript", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/34385243", "snippet": "Another <b>benefit</b> of Immutability in Javascript is that it reduces Temporal Coupling, which has substantial benefits for design generally. Consider the interface of an object with two methods: It may be the case that a call to baz () is required to get the object in a valid <b>state</b> for a call to bar () to work correctly.", "dateLastCrawled": "2022-01-28T15:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Processes and Reinforcement Learning</b>", "url": "https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/2015-08-02-<b>markov-decision-processes-and-reinforcement</b>...", "snippet": "It is also common to define a new quantity called a Q-<b>value</b> with respect to <b>state-action</b> pairs: \\[Q^*(s,a) = \\sum_{s&#39;} P(s,a,s&#39;)[R(s,a,s&#39;) + \\gamma V^*(s&#39;)]\\] In words, \\(Q(s,a)\\) is the expected utility starting at <b>state</b> \\(s\\), <b>taking</b> <b>action</b> \\(a\\), and hereafter, playing optimally. We <b>can</b> therefore relate the \\(V\\)s and \\(Q\\)s with the following equation: \\[V^*(s) = \\max_a Q^*(s,a)\\] I find it easiest to think of these in terms of expectimax trees with chance nodes. The \u201cnormal\u201d nodes ...", "dateLastCrawled": "2022-01-30T21:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Lecture 7: Reinforcement learning | CS236781: Deep Learning", "url": "https://vistalab-technion.github.io/cs236781/lecture_notes/lecture_07/", "isFamilyFriendly": true, "displayUrl": "https://vistalab-technion.github.io/cs236781/lecture_notes/lecture_07", "snippet": "Since in most implementations the <b>action</b> <b>value</b> <b>function</b> is used, the method is also known under the name of. q. -learning (when a deep neural network is used as the approximator, it is also known as deep. q. network or DQN). In practice, the network is realized as a vector-valued <b>function</b>. q\u03b8(s) receiving the <b>state</b>.", "dateLastCrawled": "2021-10-18T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Playing <b>Pong</b> using Reinforcement Learning | by Omkar Vedpathak ...", "url": "https://towardsdatascience.com/intro-to-reinforcement-learning-pong-92a94aa0f84d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/intro-to-reinforcement-learning-<b>pong</b>-92a94aa0f84d", "snippet": "In RL land, a policy is a rule, strategy, or behavior <b>function</b>, that evaluates and recommends the next <b>action</b> <b>given</b> a specific <b>state</b>; effectively it <b>can</b> <b>be thought</b> of as a map from <b>state</b> to <b>action</b>. A policy may be deterministic or stochastic in nature, and since the ultimate goal of a RL agent is to maximize its rewards, we want to select the policy that maximizes the future expected reward for a <b>given</b> <b>action</b>.", "dateLastCrawled": "2022-02-02T10:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Applications of <b>Reinforcement Learning</b> in Real World | by garychl ...", "url": "https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/applications-of-<b>reinforcement-learning</b>-in-real-world-1a...", "snippet": "The <b>state</b> was defined as eight-dimensional vector with each element <b>representing</b> the relative traffic flow of each lane. Eight choices were available to the agent, each <b>representing</b> a phase combination, and the reward <b>function</b> was defined as reduction in delay compared with previous time step. The authors used DQN to learn the Q <b>value</b> of the {<b>state, action</b>} pairs.", "dateLastCrawled": "2022-02-02T20:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>SmartIX: A database indexing agent based on reinforcement learning</b> ...", "url": "https://link.springer.com/article/10.1007/s10489-020-01674-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10489-020-01674-8", "snippet": "The expected reward is estimated by a <b>state-action</b> <b>value</b> <b>function</b> Q \u03c0 that, for each <b>state-action</b> pair, returns a <b>value</b> computed based on the amount of reward an agent might expect in the long run by <b>taking</b> <b>a particular</b> <b>action</b> on that <b>state</b>. The <b>value</b> <b>function</b> for a <b>state-action</b> pair, following a policy \u03c0, is computed using the Bellman ...", "dateLastCrawled": "2022-01-30T02:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A survey and critique of multiagent deep <b>reinforcement learning</b> ...", "url": "https://link.springer.com/article/10.1007/s10458-019-09421-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-019-09421-1", "snippet": "In the case when the critic learns a <b>state-action</b> <b>function</b> (Q <b>function</b>) and a <b>state</b> <b>value</b> <b>function</b> (V <b>function</b>), an advantage <b>function</b> <b>can</b> be computed by subtracting <b>state</b> values from the <b>state-action</b> values [283, 315]. The advantage <b>function</b> indicates the relative quality of an <b>action</b> compared to other available actions computed from the baseline, i.e., <b>state</b> <b>value</b> <b>function</b>. An example of an actor-critic algorithm is Deterministic Policy Gradient (DPG)", "dateLastCrawled": "2022-01-29T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Lecture 7: Reinforcement learning</b> | CS236605: Deep Learning", "url": "https://vistalab-technion.github.io/cs236605/lecture_notes/lecture_07/", "isFamilyFriendly": true, "displayUrl": "https://vistalab-technion.github.io/cs236605/lecture_notes/lecture_07", "snippet": "<b>Given</b> an MDP and having the agent behavior fixed to some policy $\\pi$, we may predict how beneficial it is for the environment to be in a certain <b>state</b>, or for the agent to take a certain <b>action</b> in <b>a particular</b> <b>state</b>.", "dateLastCrawled": "2021-07-30T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Approximating <b>Value</b> Functions in Classifier Systems", "url": "https://www.researchgate.net/publication/225423043_Approximating_Value_Functions_in_Classifier_Systems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/225423043_Approximating_<b>Value</b>_<b>Functions</b>_in...", "snippet": "of the <b>value</b> <b>of taking</b> its designated <b>action</b> in the states that match its condition. F rom this standpoint, each rule is treated as a separate <b>function</b> approximator.", "dateLastCrawled": "2022-01-31T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "reactjs - Why is immutability so important (or needed) in JavaScript ...", "url": "https://stackoverflow.com/questions/34385243/why-is-immutability-so-important-or-needed-in-javascript", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/34385243", "snippet": "Your <b>function</b> <b>can</b> be pure even your <b>state</b> is mutable. Immutability is just a one of 100+ ways to guarantee that you <b>function</b> will be pure. Actually 95% of your functions will be pure. You shouldn&#39;t use any limitations (like immutability) if you actually don&#39;t have a serious reason. If you want to &quot;Undo&quot; some <b>state</b>, you <b>can</b> create transactions ...", "dateLastCrawled": "2022-01-28T15:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Thomas Hill Green: Bio, Life and Political Ideas</b>", "url": "https://www.politicalsciencenotes.com/political-thinkers/thomas-hill/thomas-hill-green-bio-life-and-political-ideas/1159", "isFamilyFriendly": true, "displayUrl": "https://www.politicalsciencenotes.com/political-thinkers/thomas-hill/thomas-hill-green...", "snippet": "The implication is the chief <b>function</b> of <b>state</b> is to remove the hindrances which stand on the way of implemen\u00adtation or enjoyment of rights. Green felt that any other association could perform the job. But he had doubt about the capacity or eligibility of those associations. Only a society having supreme coercive power <b>can</b> resist the aggression or disturbance from within or without which may intensify the assault upon the rights. The <b>state</b> has the supreme coercive power and it exercises ...", "dateLastCrawled": "2022-02-01T12:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The <b>Algorithm Behind the Curtain: Reinforcement Learning Concepts</b> (2 of ...", "url": "https://randomant.net/reinforcement-learning-concepts/", "isFamilyFriendly": true, "displayUrl": "https://randomant.net/reinforcement-learning-concepts", "snippet": "<b>Action</b>-<b>Value</b> <b>Function</b>: The mathematical sister of the <b>state</b>-<b>value</b> <b>function</b> is the <b>action</b>-<b>value</b> <b>function</b>, represented as Q. Whereas the <b>state</b>-<b>value</b> <b>function</b> is concerned with determining the total future reward of each <b>state</b>, the <b>action</b>-<b>value</b> <b>function</b> seeks to determine the total future reward of each <b>state/action</b> pair. Although both functions accomplish a similar objective, they are used in different ways in different RL algorithms. The equation for the <b>action</b>-<b>value</b> <b>function</b> is below. It is ...", "dateLastCrawled": "2022-01-31T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Q-Learning</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/engineering/q-learning", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/engineering/<b>q-learning</b>", "snippet": "The <b>state\u2013action</b> pair <b>value</b> <b>function</b>, Q <b>function</b>, is constructed in the algorithm. The definition of the Q <b>function</b> is shown in Eq. (7.12). Theory suggests that the <b>Q learning</b> algorithm will converge to the optimal <b>state\u2013action</b> pair <b>value</b> <b>function</b> when the learning rate \u03b1 satisfies a definite condition. The <b>Q learning</b> algorithm is one of the most popular RL algorithms. (7.12) Q \u03c0 (s, a) = \u2211 s \u2032 P s s \u2032 a [R s s \u2032 a + \u03b3 V \u03c0 (s \u2032)] Algorithm 7.2: <b>Q learning</b> algorithm ...", "dateLastCrawled": "2022-01-24T01:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Value</b>-Based Reinforcement Learning for Continuous Control Robotic ...", "url": "https://deepai.org/publication/value-based-reinforcement-learning-for-continuous-control-robotic-manipulation-in-multi-task-sparse-reward-settings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>value</b>-based-reinforcement-learning-for-continuous...", "snippet": "The <b>action</b>-<b>value</b> <b>function</b> Q \u03c0 (s, a), with s \u2208 S and a \u2208 A, is defined as the maximum expected return achievable by following <b>a particular</b> policy \u03c0: S \u2192 A , after seeing some <b>state</b> s and then <b>taking</b> some <b>action</b> a. The optimal <b>action</b>-<b>value</b> <b>function</b> Q \u2217 (s, a) = max \u03c0 E [R t | s t = s, a t = a, \u03c0] corresponds with the optimal policy ...", "dateLastCrawled": "2022-01-28T11:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reinforcement learning with guided policy search using Gaussian</b> ...", "url": "https://www.researchgate.net/publication/261464877_Reinforcement_learning_with_guided_policy_search_using_Gaussian_processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261464877_Reinforcement_learning_with_guided...", "snippet": "Gradient based policy search algorithms <b>benefit</b> largely from the availability of a properly estimated <b>state</b> or <b>state-action</b> <b>value</b> <b>function</b> which <b>can</b> be used to reduce the variance of the gradient ...", "dateLastCrawled": "2021-12-23T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Deep Reinforcement Learning</b> for Trading | The Journal of Financial Data ...", "url": "https://jfds.pm-research.com/content/early/2020/03/16/jfds.2020.1.030", "isFamilyFriendly": true, "displayUrl": "https://jfds.pm-research.com/content/early/2020/03/16/jfds.2020.1.030", "snippet": "A <b>state-action</b> <b>value</b> <b>function</b>, Q, is constructed to represent how good <b>a particular</b> <b>action</b> is in a <b>state</b>. Discrete <b>action</b> spaces are adopted in these works, and an agent is trained to fully go long or short a position. However, a fully invested position is risky during high-volatility periods, exposing one to severe risk when opposite moves occur. Ideally, one would like to scale positions up or down according to current market conditions. Doing this requires one to have large <b>action</b> spaces ...", "dateLastCrawled": "2022-01-30T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Reinforcement Learning in Multi-dimensional</b> <b>State-action</b> Space Using ...", "url": "https://www.researchgate.net/publication/280213361_Reinforcement_Learning_in_Multi-dimensional_State-action_Space_Using_Random_Tiling_and_Gibbs_Sampling", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/280213361_Reinforcement_Learning_in_Multi...", "snippet": "The purpose of this <b>function</b> is to measure the long-term utility or <b>value</b> of any <b>given</b> <b>state</b> and it is important because an agent <b>can</b> use it to decide what to do next. A common problem in ...", "dateLastCrawled": "2022-01-11T15:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Competitive reinforcement learning in continuous control tasks ...", "url": "https://www.academia.edu/7104048/Competitive_reinforcement_learning_in_continuous_control_tasks", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/7104048/Competitive_reinforcement_learning_in_continuous...", "snippet": "It does therefore make such as SLVQ, the <b>action</b> <b>value</b> of the next control algorithm move taken will be used to update the <b>state-action</b> pair sense for those problems to activate neurons defined by a \u261e . In addition, temporal credit assignment to previous neighborhood <b>function</b> or kernel to learn from the same input. moves, modulated by or eligibility trace[11], is not limited Spreading the reward around makes learning converge faster and less sensitive to changes in the parameters. By doing ...", "dateLastCrawled": "2022-01-19T07:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Importance sampling in reinforcement learning</b> with an estimated ...", "url": "https://link.springer.com/article/10.1007/s10994-020-05938-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05938-9", "snippet": "We use B for the random variable <b>representing</b> all k <b>state-action</b> pairs observed in D. Footnote 1 A policy also induces a distribution over <b>state</b> visitation frequencies, \\(d_\\pi : \\mathcal {S} \\rightarrow [0,1]\\). We define the <b>value</b> of a policy, \\(v(\\pi ) :=\\mathbf {E}[g(H) | H \\sim \\pi ]\\), as the expected discounted return when sampling a trajectory with policy \\(\\pi\\). Expectation evaluation <b>in reinforcement learning</b>. An important problem that arises across the reinforcement <b>learning</b> ...", "dateLastCrawled": "2022-01-13T11:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Applications of <b>Reinforcement Learning</b> in Real World | by garychl ...", "url": "https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/applications-of-<b>reinforcement-learning</b>-in-real-world-1a...", "snippet": "For example, to find a good policy we could use valued-based methods like Q-learning to measure how good an <b>action</b> is in <b>a particular</b> <b>state</b> or policy-based methods to directly find out what actions to take under different states without knowing how good the actions are. However, the problems we face in the real world <b>can</b> be extremely complicated in many different ways and therefore a typical RL algorithm has no clue to solve. For example, the <b>state</b> space is very large in the game of GO ...", "dateLastCrawled": "2022-02-02T20:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "GitHub - Valdinious/Carbon-Footprint-Multi-Agent-Reinforcement-Learning ...", "url": "https://github.com/Valdinious/Carbon-Footprint-Multi-Agent-Reinforcement-Learning", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Valdinious/Carbon-Footprint-Multi-Agent-Reinforcement-Learning", "snippet": ", calculates Q-values, <b>representing</b> the reward r of an <b>action</b> a <b>in a given</b> <b>state</b> s, plus the maximum reward achievable by the best <b>action</b> a\u2019 in the next <b>state</b> s\u2019, discounted by the factor gamma $\\gamma$ (temporal-difference learning). Q* denotes the highest Q-<b>value</b> for a <b>given</b> row, thus defines the <b>action</b> <b>in a given</b> <b>state</b> that achieves the highest Q-<b>value</b> (Valkov, 2017).", "dateLastCrawled": "2022-01-30T10:09:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Relationship between state (V) and action(Q) <b>value</b> <b>function</b> in ...", "url": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and-action-q-value-function-in-reinforcement-learning-bb9a988c0127", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and...", "snippet": "<b>Value</b> <b>function</b> can be defined as the expected <b>value</b> of an agent in a certain state. There are two types of <b>value</b> functions in RL: State-<b>value</b> and action-<b>value</b>. It is important to understand the\u2026", "dateLastCrawled": "2022-02-03T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Value</b>. State-<b>value</b> <b>function</b> v_\u03c0: gives us the <b>value</b> of a state under \u03c0; Action-<b>value</b> <b>function</b> q_\u03c0: gives us the <b>value</b> of an action under \u03c0. q_\u03c0 is referred to as the Q-<b>function</b>, and the output from the <b>function</b> for any given <b>state-action</b> pair is called a Q-<b>value</b>.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Teaching machines to behave: Reinforcement <b>Learning</b> | by Diego Gomez ...", "url": "https://towardsdatascience.com/rl-4-all-1-3edac941fe37", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/rl-4-all-1-3edac941fe37", "snippet": "Notice that, as in all <b>machine</b> <b>learning</b> sub-fields, RL programs are not coded explicitly to perform optimally in some given task. ... Similarly, a* is the optimal <b>state-action</b>-<b>value</b> <b>function</b>, obtained if followed an optimal policy \u03c0*. Optimal <b>value</b> functions: Obtained when following a policy \u03c0 that maximizes v(s) and a(s,a) Assuming that the optimal <b>value</b> functions v* and a* are known, but the optimal policy is not, it is possible to build an optimal policy in the following ways: When v*(s ...", "dateLastCrawled": "2022-01-08T13:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Value</b>-<b>Function</b>-<b>Based Transfer for Reinforcement Learning</b> Using ...", "url": "https://www.researchgate.net/publication/221604435_Value-Function-Based_Transfer_for_Reinforcement_Learning_Using_Structure_Mapping", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221604435_<b>Value</b>-<b>Function</b>-Based_Transfer_for...", "snippet": "chological and computational theory about <b>analogy</b> making, ... the form of a <b>state-action</b> <b>value</b> <b>function</b>, or a q-<b>functio n</b>. A. q-<b>function</b> q: S \u00d7 A 7\u2192 R maps from <b>state-action</b> pairs to. real ...", "dateLastCrawled": "2021-10-16T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning for biochemical engineering: A</b> review - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1369703X21001303", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1369703X21001303", "snippet": "<b>Value</b>-based algorithms, typically represented by Q-<b>learning</b>, explicitly learn and optimise the <b>state-action</b> <b>value</b> <b>function</b> and generate the optimal policy by acting greedily with respect to it i.e. choosing the control corresponding to the maximum Q \u03c0 x, u <b>value</b> (<b>state-action</b> <b>value</b>). There are also hybrid algorithms, such as actor-critic methods, which combine policy optimisation methods and <b>value</b>-based methods. Although RL has shown success in game-based control benchmarks, such as AlphaGo", "dateLastCrawled": "2022-01-26T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "AI and Reinforcement <b>Learning</b> \u2014 Machines that Learn through Experience ...", "url": "https://www.cantorsparadise.com/ai-and-reinforcement-learning-machines-that-learn-through-experience-e7eea7bb6765", "isFamilyFriendly": true, "displayUrl": "https://www.cantorsparadise.com/ai-and-reinforcement-<b>learning</b>-<b>machines</b>-that-learn...", "snippet": "To align the policy with the updated <b>value</b> <b>function</b>, the algorithm modifies the policy so it would greedily follow the <b>value</b> <b>function</b> (meaning, choosing to perform actions that has the highest <b>value</b>). The algorithm continues by generating a new episode, now under the improved policy, which, in turn, derives a more accurate <b>value</b> estimation and so on. In this process, both the policy and the <b>value</b> <b>function</b> converge to their optimal values, until sufficient accuracy is reached, or when no more ...", "dateLastCrawled": "2022-01-25T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>SARSA</b> vs Q - <b>learning</b>", "url": "https://tcnguyen.github.io/reinforcement_learning/sarsa_vs_q_learning.html", "isFamilyFriendly": true, "displayUrl": "https://tcnguyen.github.io/reinforcement_<b>learning</b>/<b>sarsa</b>_vs_q_<b>learning</b>.html", "snippet": "<b>SARSA</b> will learn the optimal $\\epsilon$-greedy policy, i.e, the Q-<b>value</b> <b>function</b> will converge to a optimal Q-<b>value</b> <b>function</b> but in the space of $\\epsilon$-greedy policy only (as long as each <b>state action</b> pair will be visited infinitely). We expect that in the limit of $\\epsilon$ decaying to $0$, <b>SARSA</b> will converge to the overall optimal policy. I quote here a paragraph from", "dateLastCrawled": "2022-01-30T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning Analogy for Meditation (illustrated</b>) - LessWrong 2.0 ...", "url": "https://www.greaterwrong.com/posts/chHhuCvmZqYLM32gz/machine-learning-analogy-for-meditation-illustrated", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/chHhuCvmZqYLM32gz/<b>machine</b>-<b>learning</b>-<b>analogy</b>-for...", "snippet": "<b>Machine Learning Analogy for Meditation (illustrated</b>) ... and the algorithm we use includes a <b>value</b> table: [picture: table, actions on x-axis, states on y-axis, cells of table are estimated values of taking actions in states] A <b>value</b> isn\u2019t just the learned estimate of the immediate reward which you get by taking an action in a state, but rather, the estimate of the eventual rewards, in total, from that action. This makes the values difficult to estimate. An estimate is improved by <b>value</b> it", "dateLastCrawled": "2022-01-17T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Reinforcement Learning: Prediction, Control and</b> <b>Value</b> <b>Function</b> ...", "url": "https://deepai.org/publication/reinforcement-learning-prediction-control-and-value-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>reinforcement-learning-prediction-control-and</b>-<b>value</b>...", "snippet": "<b>Reinforcement Learning: Prediction, Control and Value Function Approximation</b>. With the increasing power of computers and the rapid development of self-<b>learning</b> methodologies such as <b>machine</b> <b>learning</b> and artificial intelligence, the problem of constructing an automatic Financial Trading Systems (FTFs) becomes an increasingly attractive research ...", "dateLastCrawled": "2022-01-16T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>value</b> of a <b>function</b>?", "url": "https://psichologyanswers.com/library/lecture/read/57841-what-is-value-of-a-function", "isFamilyFriendly": true, "displayUrl": "https://psichologyanswers.com/library/lecture/read/57841-what-is-<b>value</b>-of-a-<b>function</b>", "snippet": "What is a <b>value</b> <b>function</b> reinforcement <b>learning</b>? <b>Value</b> <b>function</b> Many reinforcement <b>learning</b> introduce the notion of `<b>value</b>-<b>function</b>` which often denoted as V(s) . The <b>value</b> <b>function</b> represent how good is a state for an agent to be in. It is equal to expected total reward for an agent starting from state s . What is optimal <b>value</b> <b>function</b>? The optimal <b>Value</b> <b>function</b> is one which yields maximum <b>value</b> compared to all other <b>value</b> <b>function</b>. When we say we are solving an MDP it actually means we ...", "dateLastCrawled": "2022-01-15T22:23:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(state-action value function)  is like +(representing the benefit of taking a particular action in a given state)", "+(state-action value function) is similar to +(representing the benefit of taking a particular action in a given state)", "+(state-action value function) can be thought of as +(representing the benefit of taking a particular action in a given state)", "+(state-action value function) can be compared to +(representing the benefit of taking a particular action in a given state)", "machine learning +(state-action value function AND analogy)", "machine learning +(\"state-action value function is like\")", "machine learning +(\"state-action value function is similar\")", "machine learning +(\"just as state-action value function\")", "machine learning +(\"state-action value function can be thought of as\")", "machine learning +(\"state-action value function can be compared to\")"]}