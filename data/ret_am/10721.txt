{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Another Look at the <b>Data</b> <b>Sparsity</b> Problem - ResearchGate", "url": "https://www.researchgate.net/publication/221152250_Another_Look_at_the_Data_Sparsity_Problem", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221152250_Another_Look_at_the_<b>Data</b>_<b>Sparsity</b>...", "snippet": "<b>When there</b> is not enough <b>data</b> available to model language correctly this is called <b>data</b> <b>sparsity</b> [2]. In this case we address possible <b>data</b> <b>sparsity</b> when using the domain ontology by making use of ...", "dateLastCrawled": "2022-01-28T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Machine Learning in Medical Imaging", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4220564/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4220564", "snippet": "<b>Like</b> SVM, RVM uses a subset of the <b>training</b> <b>data</b> called relevance vectors, but usually <b>there</b> are far fewer relevance vectors than support vectors. <b>Like</b> SVM, RVM starts with a kernel model . f (x) = \u2211 i = 1 N w i K (x, x i), (4) however, whereas SVM is based on the maximum-margin principle, RVM instead takes a Bayesian approach. RVM assumes a Gaussian prior on the kernel weights w i, which are assumed to have zero mean and variance a i \u2212 1. RVM further assumes a gamma hyperprior on a i ...", "dateLastCrawled": "2022-02-03T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Sparsity</b> Algorithm with Applications to Corporate Credit Rating | DeepAI", "url": "https://deepai.org/publication/a-sparsity-algorithm-with-applications-to-corporate-credit-rating", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-<b>sparsity</b>-algorithm-with-applications-to-corporate...", "snippet": "If <b>there</b> is no solution to the <b>sparsity</b> algorithm, ... In this section we apply the <b>sparsity</b> algorithm to <b>data</b> obtained from financial statements. Given a particular financial statement, <b>there</b> may be many ways in which to improve the financial stability of a company and thus increasing its credit rating. In this work, we are trying to provide a <b>data</b> driven answer which is based purely on the machine learning technique used. To this end, we have to assume that the machine learning technique ...", "dateLastCrawled": "2021-12-19T06:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Frontiers | A <b>Sparsity</b>-Driven Backpropagation-Less Learning Framework ...", "url": "https://www.frontiersin.org/articles/10.3389/fnins.2021.715451/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fnins.2021.715451", "snippet": "Domain described by the network when the upper bound on the <b>number</b> of <b>training</b> <b>data</b> <b>points</b> rejected as outliers is (A) 0%, (B) 25%, and (C) 50% of the <b>training</b> dataset. Blue circles indicate <b>training</b> <b>data</b> <b>points</b>, and blue and gray shaded regions indicate the areas identified as members and anomalies, respectively. 3.3. Supervised Learning. In this section, we exploit the framework outlined in (40) to design networks that can solve linear classification problems using the GT network. Consider ...", "dateLastCrawled": "2022-01-26T18:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "All about Feature <b>Scaling</b>. Scale <b>data</b> for better performance of\u2026 | by ...", "url": "https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/all-about-feature-<b>scaling</b>-bcc0ad75cb35", "snippet": "Machine learning algorithm just sees <b>number</b> \u2014 if <b>there</b> is a vast difference in the range say <b>few</b> ranging in thousands and <b>few</b> ranging in the tens, and it makes the underlying assumption that higher ranging numbers have superiority of some sort. So these more significant <b>number</b> starts playing a more decisive role while <b>training</b> the model. The machine learning algorithm works on numbers and does not know what that <b>number</b> represents. A weight of 10 grams and a price of 10 dollars represents ...", "dateLastCrawled": "2022-01-29T06:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "If <b>very</b> <b>few</b> <b>data</b> samples are <b>there</b>, we can make use of oversampling to produce new <b>data</b> <b>points</b>. In this way, we can have new <b>data</b> <b>points</b>. 116. What are the hyperparameters of an SVM? Ans. The gamma value, c value and the type of kernel are the hyperparameters of an SVM model. 117. What is Pandas Profiling? Ans. Pandas profiling is a step to find the effective <b>number</b> of usable <b>data</b>. It gives us the statistics of NULL values and the usable values and thus makes variable selection and <b>data</b> ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - dontless/<b>Machine-Learning-Foundations-A-Case-Study-Approach</b> ...", "url": "https://github.com/dontless/Machine-Learning-Foundations-A-Case-Study-Approach", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dontless/<b>Machine-Learning-Foundations-A-Case-Study-Approach</b>", "snippet": "not interested in what a person does not <b>like</b> want to quickly discover relatively <b>few</b> liked items ^ imbalanced class problem Users have short attention span, so we want to recommend fewer items . higher cost to missing liked item Recall = (# liked &amp; shown) / (# liked) e.g 3/5 Precision = (# liked &amp; shown) / (# shown) e.g. 3/11 Optimal recommenders. How to maximize recall? recommend all my liked items. in this case precision is really small. e.g. 1000 total products, of which i liked 6 items ...", "dateLastCrawled": "2022-01-30T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: <b>Data</b> ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "<b>There</b> are several ways to make a model more robust to outliers, from different <b>points</b> of view (<b>data</b> preparation or model building). An outlier in the question and answer is assumed being unwanted, unexpected, or a must-be-wrong value to the human\u2019s knowledge so far (e.g. no one is 200 years old) rather than a rare event which is possible but rare.", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What are L1, L2 <b>and Elastic Net Regularization in neural</b> ... - MachineCurve", "url": "https://www.machinecurve.com/index.php/2020/01/21/what-are-l1-l2-and-elastic-net-regularization-in-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/01/21/what-are-l1-l2-and-elastic-net...", "snippet": "<b>Training</b> <b>data</b> is fed to the network in a feedforward fashion. The predictions generated by this process are stored, and compared to the actual targets, or the \u201cground truth\u201d. The difference between the predictions and the targets can be computed and is known as the loss value. Through computing gradients and subsequent gradient based optimization techniques, the weights of your neural network can be adapted, possibly improving the model. This means that optimizing a model equals ...", "dateLastCrawled": "2022-02-02T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The <b>Curse of Dimensionality</b> in Classification", "url": "https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/", "isFamilyFriendly": true, "displayUrl": "https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification", "snippet": "Together with the 3D mean of the distribution this means that we <b>need</b> to estimate 9 <b>parameters</b> based on our <b>training</b> <b>data</b>, to obtain the Gaussian density that represent the likelihood of our <b>data</b>. In the 1D case, only 2 <b>parameters</b> <b>need</b> <b>to be estimated</b> (mean and variance), whereas in the 2D case 5 <b>parameters</b> are needed (2D mean, two variances and a covariance). Again we can see that the <b>number</b> <b>of parameters</b> <b>to be estimated</b> grows quadratic with the <b>number</b> of dimensions.", "dateLastCrawled": "2022-02-02T08:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deformable models with <b>sparsity</b> constraints for cardiac motion analysis ...", "url": "https://europepmc.org/article/PMC/PMC4876050", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC4876050", "snippet": "Therefore, <b>there</b> is no <b>need</b> for <b>training</b> <b>data</b>. Go to: 3. Methodology. Consider a set of <b>points</b> , where each point has a neighborhood structure 1, and a subset of as control <b>points</b> c that are computed from the observations (e.g., image information). Denote the homogeneous coordinate of the point i as v i = [x i, y i, z i, 1] T and its position after deformation as v \u2032 i = [x \u2032 i, y \u2032 i, z \u2032 i] T v i \u2032 = [x i \u2032, y i \u2032, z i \u2032] T, where i = 1,2,,n. Then the coordinates of all the ...", "dateLastCrawled": "2021-06-24T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Statistical Learning with Sparsity The Lasso and Generalizations</b> Pages ...", "url": "https://fliphtml5.com/mofx/mxtu/basic/", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/mofx/mxtu/basic", "snippet": "A sparse statistical model is onehaving only a small <b>number</b> of nonzero <b>parameters</b> or weights. It represents aclassic case of \u201cless is more\u201d: a sparse model can be much easier to estimateand interpret than a dense model. In this age of big <b>data</b>, the <b>number</b> offeatures measured on a person or object can be large, and might be largerthan the <b>number</b> of observations. The <b>sparsity</b> assumption allows us to tacklesuch problems and extract useful and reproducible patterns from big datasets. The ...", "dateLastCrawled": "2022-01-19T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Sparsity</b> Algorithm with Applications to Corporate Credit Rating | DeepAI", "url": "https://deepai.org/publication/a-sparsity-algorithm-with-applications-to-corporate-credit-rating", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-<b>sparsity</b>-algorithm-with-applications-to-corporate...", "snippet": "In this section we apply the <b>sparsity</b> algorithm to <b>data</b> obtained from financial statements. Given a particular financial statement, <b>there</b> may be many ways in which to improve the financial stability of a company and thus increasing its credit rating. In this work, we are trying to provide a <b>data</b> driven answer which is based purely on the machine learning technique used. To this end, we have to assume that the machine learning technique used to determine the original f is <b>very</b> accurate ...", "dateLastCrawled": "2021-12-19T06:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machine Learning in Medical Imaging", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4220564/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4220564", "snippet": "For example, Figure 10 (adapted from ) illustrates that a simple linear model can outperform a flexible nonlinear model (in this case an ANN) until <b>there</b> are enough <b>data</b> examples to support estimation of the greater <b>number</b> <b>of parameters</b> inherent in the nonlinear model. Nevertheless, these issues are frequently ignored in the current brain mapping literature when discussing or comparing different analysis techniques.", "dateLastCrawled": "2022-02-03T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Statistical Learning with Sparsity The Lasso and Generalizations</b> Pages ...", "url": "https://fliphtml5.com/mofx/mxtu/basic/201-250", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/mofx/mxtu/basic/201-250", "snippet": "<b>Points</b> xi areassociated with prototypes \u00b5i of the same color. The <b>estimated</b> prototypes <b>need</b> notbe close to the centroids of their clusters.true clusters; further details are given in the caption. The convexity of theobjective function as well as its ability to choose the <b>number</b> of clusters andthe informative <b>features</b>, makes this approach ...", "dateLastCrawled": "2022-01-11T15:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sparse identification</b> of nonlinear dynamics for model predictive ...", "url": "https://royalsocietypublishing.org/doi/10.1098/rspa.2018.0335", "isFamilyFriendly": true, "displayUrl": "https://royalsocietypublishing.org/doi/10.1098/rspa.2018.0335", "snippet": "We varied the <b>number</b> of hidden layers (up to 3), the <b>number</b> of neurons (up to 100), the type of activation function, the <b>number</b> of delays (up to 100) in the state and input variables and the amount of <b>training</b> <b>data</b> (\u2248 600 different initial conditions). However, these did not significantly change the performance of the model. The type of <b>data</b> (not just the amount) is particularly critical for <b>training</b> a NN. Designing experiments, i.e. a good forcing signal that explores the system behaviour ...", "dateLastCrawled": "2022-02-03T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: <b>Data</b> ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "<b>There</b> are several ways to make a model more robust to outliers, from different <b>points</b> of view (<b>data</b> preparation or model building). An outlier in the question and answer is assumed being unwanted, unexpected, or a must-be-wrong value to the human\u2019s knowledge so far (e.g. no one is 200 years old) rather than a rare event which is possible but rare.", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "All about Feature <b>Scaling</b>. Scale <b>data</b> for better performance of\u2026 | by ...", "url": "https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/all-about-feature-<b>scaling</b>-bcc0ad75cb35", "snippet": "Machine learning algorithm just sees <b>number</b> \u2014 if <b>there</b> is a vast difference in the range say <b>few</b> ranging in thousands and <b>few</b> ranging in the tens, and it makes the underlying assumption that higher ranging numbers have superiority of some sort. So these more significant <b>number</b> starts playing a more decisive role while <b>training</b> the model. The machine learning algorithm works on numbers and does not know what that <b>number</b> represents. A weight of 10 grams and a price of 10 dollars represents ...", "dateLastCrawled": "2022-01-29T06:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GitHub</b> - dontless/<b>Machine-Learning-Foundations-A-Case-Study-Approach</b> ...", "url": "https://github.com/dontless/Machine-Learning-Foundations-A-Case-Study-Approach", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dontless/<b>Machine-Learning-Foundations-A-Case-Study-Approach</b>", "snippet": "train_<b>data</b>,test_<b>data</b> = sales.random_split(.8,seed=0) Use random_split to split <b>training</b> and test <b>data</b> 0.8 =&gt; 80% <b>training</b> and 20% test set seed to 0 in this case. we should use a random seed or let GL pick it for you", "dateLastCrawled": "2022-01-30T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "A <b>data</b> point that is considerably distant from the other <b>similar</b> <b>data</b> <b>points</b> is known as an outlier. They may occur due to experimental errors or variability in measurement. They are problematic and can mislead a <b>training</b> process, which eventually results in longer <b>training</b> time, inaccurate models, and poor results. The three methods to deal with outliers are: Univariate method \u2013 looks for <b>data</b> <b>points</b> having extreme values on a single variable Multivariate method \u2013 looks for unusual ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Musical note onset detection based on a spectral <b>sparsity</b> measure ...", "url": "https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00214-7", "isFamilyFriendly": true, "displayUrl": "https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00214-7", "snippet": "The NINOS 2 feature <b>can</b> <b>be thought</b> of as a spectral <b>sparsity</b> measure, aiming to exploit the difference in spectral <b>sparsity</b> between the different parts of a musical note. This spectral structure is revealed when focusing on low-magnitude spectral components that are traditionally filtered out when computing note onset <b>features</b>. We present an extensive set of NOD simulation results covering a wide range of instruments, playing styles, and mixing options. The proposed algorithm consistently ...", "dateLastCrawled": "2022-02-01T09:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Statistical Learning with Sparsity The Lasso and Generalizations</b> Pages ...", "url": "https://fliphtml5.com/mofx/mxtu/basic/201-250", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/mofx/mxtu/basic/201-250", "snippet": "A <b>number</b> of au-thors have explored how <b>sparsity</b> <b>can</b> still allow for consistent estimation ofprincipal components even when p N . Johnstone and Lu (2009) propose atwo-stage procedure, based on thresholding the diagonal of the sample covari-ance matrix in order to isolate the highest variance coordinates, and then per-forming PCA in the reduced-dimensional space. They prove consistency of thismethod even when p/N stays bounded away from zero, but allow only polyno-mial growth of p as a ...", "dateLastCrawled": "2022-01-11T15:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Machine Learning in Medical Imaging", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4220564/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4220564", "snippet": "For example, Figure 10 (adapted from ) illustrates that a simple linear model <b>can</b> outperform a flexible nonlinear model (in this case an ANN) until <b>there</b> are enough <b>data</b> examples to support estimation of the greater <b>number</b> <b>of parameters</b> inherent in the nonlinear model. Nevertheless, these issues are frequently ignored in the current brain mapping literature when discussing or comparing different analysis techniques.", "dateLastCrawled": "2022-02-03T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Statistical Learning with <b>Sparsity</b> The Lasso and Generalizations Pages ...", "url": "https://fliphtml5.com/mofx/mxtu/basic/151-200", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/mofx/mxtu/basic/151-200", "snippet": "On theother hand, the con\ufb01dence intervals from PoSI <b>can</b> be <b>very</b> wide. In the di-abetes dataset <b>there</b> are four <b>very</b> strong predictors: their lasso intervals areessentially una\ufb00ected by the selection and look much like the standard least-squares intervals. Even with a Bonferroni adjustment from 0.05 to 0.01, theintervals have approximate length \u00b12.33 \u00b7 \u03c3vj\u00b7M compared to \u00b14.42 \u00b7 \u03c3vj\u00b7M forPoSI. However the authors of PoSI make the point that their method providesmuch stronger ...", "dateLastCrawled": "2021-12-12T19:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Deep learning</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Deep_learning", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Deep_learning</b>", "snippet": "Word embedding, such as word2vec, <b>can</b> <b>be thought</b> of as a representational layer in a <b>deep learning</b> architecture that transforms an atomic word into a positional representation of the word <b>relative</b> to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar <b>can</b> <b>be thought</b> of as", "dateLastCrawled": "2022-02-02T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Make Some ROOM for the Zeros: <b>Data</b> <b>Sparsity</b> in Secure Distributed ...", "url": "https://www.researchgate.net/publication/331864762_Make_Some_ROOM_for_the_Zeros_Data_Sparsity_in_Secure_Distributed_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/331864762_Make_Some_ROOM_for_the_Zeros_<b>Data</b>...", "snippet": "Exploiting <b>data</b> <b>sparsity</b> is crucial for the scalability of many <b>data</b> analysis tasks. However, while <b>there</b> is an increasing interest in efficient secure computation protocols for distributed ...", "dateLastCrawled": "2022-01-31T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: <b>Data</b> ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "This overfitting of the <b>training</b> <b>data</b> <b>can</b> negatively affect the modeling power of the method and cripple the predictive accuracy. 2. Explain what regularization is and why it is useful. Regularization is the process of adding a tuning parameter to a model to induce smoothness in order to prevent overfitting. This is most often done by adding a constant multiple to an existing weight vector. This constant is often either the L1 (Lasso) or L2 (ridge), but <b>can</b> in actuality <b>can</b> be any norm. The ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "In Depth: <b>Principal Component Analysis</b> | Python <b>Data</b> Science Handbook", "url": "https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html", "isFamilyFriendly": true, "displayUrl": "https://jakevdp.github.io/Python<b>Data</b>ScienceHandbook/05.09-<b>principal-component-analysis</b>...", "snippet": "PCA <b>can</b> <b>be thought</b> of as a process of choosing optimal basis functions, such that adding together just the first <b>few</b> of them is enough to suitably reconstruct the bulk of the elements in the dataset. The principal components, which act as the low-dimensional representation of our <b>data</b>, are simply the coefficients that multiply each of the elements in this series. This figure shows a similar depiction of reconstructing this digit using the mean plus the first eight PCA basis functions:", "dateLastCrawled": "2022-02-02T20:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What are L1, L2 <b>and Elastic Net Regularization in neural</b> ... - MachineCurve", "url": "https://www.machinecurve.com/index.php/2020/01/21/what-are-l1-l2-and-elastic-net-regularization-in-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/01/21/what-are-l1-l2-and-elastic-net...", "snippet": "The <b>need</b> for regularization during model <b>training</b>. When you are <b>training</b> a machine learning model, at a high level, you\u2019re learning a function \\(\\hat{y}: f(x) \\) which transforms some input value \\(x\\) (often a vector, so \\(\\textbf{x}\\)) into some output value \\(\\hat{y}\\) (often a scalar value, such as a class when classifying and a real <b>number</b> when regressing).. Contrary to a regular mathematical function, the exact mapping (to \\(y\\)) is not known in advance, but is learnt based on the ...", "dateLastCrawled": "2022-02-02T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is ridge regression useless in high dimensions ($n \\\\ll p$)? How <b>can</b> ...", "url": "https://stats.stackexchange.com/questions/328630/is-ridge-regression-useless-in-high-dimensions-n-ll-p-how-can-ols-fail-to", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/328630", "snippet": "Also why the minimum norm solution is more effective as the <b>number</b> of <b>features</b> grows larger. My intuition is that problems with more <b>parameters</b> <b>need</b> more regularisation (all things being otherwise equal) rather than less.", "dateLastCrawled": "2022-01-28T01:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Another Look at the <b>Data</b> <b>Sparsity</b> Problem - ResearchGate", "url": "https://www.researchgate.net/publication/221152250_Another_Look_at_the_Data_Sparsity_Problem", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221152250_Another_Look_at_the_<b>Data</b>_<b>Sparsity</b>...", "snippet": "When <b>there</b> is not enough <b>data</b> available to model language correctly this is called <b>data</b> <b>sparsity</b> [2]. In this case we address possible <b>data</b> <b>sparsity</b> when using the domain ontology by making use of ...", "dateLastCrawled": "2022-01-28T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Sparsity</b> Algorithm with Applications to Corporate Credit Rating | DeepAI", "url": "https://deepai.org/publication/a-sparsity-algorithm-with-applications-to-corporate-credit-rating", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-<b>sparsity</b>-algorithm-with-applications-to-corporate...", "snippet": "If <b>there</b> is no solution to the <b>sparsity</b> algorithm, ... In this section we apply the <b>sparsity</b> algorithm to <b>data</b> obtained from financial statements. Given a particular financial statement, <b>there</b> may be many ways in which to improve the financial stability of a company and thus increasing its credit rating. In this work, we are trying to provide a <b>data</b> driven answer which is based purely on the machine learning technique used. To this end, we have to assume that the machine learning technique ...", "dateLastCrawled": "2021-12-19T06:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Resolving <b>data</b> <b>sparsity</b> by multi-type auxiliary <b>implicit feedback</b> for ...", "url": "https://www.sciencedirect.com/science/article/pii/S0950705117304653", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705117304653", "snippet": "The auxiliary <b>data</b> for this section is the <b>training</b> set as well as the related <b>data</b> with auxiliary feedback. For simplicity, we treat each kind of auxiliary and target feedback as a feature. In Sobazaar, for each record, <b>there</b> is at least 1 feature and at most 6 <b>features</b> (may or may not include the target feedback). The average <b>number</b> of <b>features</b> is around 1.53 (less than 2) for all the users. To train a regression model, we retrieve only the records that contain both auxiliary and target ...", "dateLastCrawled": "2021-11-21T11:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Statistical Learning with Sparsity The Lasso and Generalizations</b> Pages ...", "url": "https://fliphtml5.com/mofx/mxtu/basic/", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/mofx/mxtu/basic", "snippet": "A sparse statistical model is onehaving only a small <b>number</b> of nonzero <b>parameters</b> or weights. It represents aclassic case of \u201cless is more\u201d: a sparse model <b>can</b> be much easier to estimateand interpret than a dense model. In this age of big <b>data</b>, the <b>number</b> offeatures measured on a person or object <b>can</b> be large, and might be largerthan the <b>number</b> of observations. The <b>sparsity</b> assumption allows us to tacklesuch problems and extract useful and reproducible patterns from big datasets. The ...", "dateLastCrawled": "2022-01-19T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machine Learning in Medical Imaging", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4220564/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4220564", "snippet": "For example, Figure 10 (adapted from ) illustrates that a simple linear model <b>can</b> outperform a flexible nonlinear model (in this case an ANN) until <b>there</b> are enough <b>data</b> examples to support estimation of the greater <b>number</b> <b>of parameters</b> inherent in the nonlinear model. Nevertheless, these issues are frequently ignored in the current brain mapping literature when discussing or comparing different analysis techniques.", "dateLastCrawled": "2022-02-03T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: <b>Data</b> ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "This overfitting of the <b>training</b> <b>data</b> <b>can</b> negatively affect the modeling power of the method and cripple the predictive accuracy. 2. Explain what regularization is and why it is useful. Regularization is the process of adding a tuning parameter to a model to induce smoothness in order to prevent overfitting. This is most often done by adding a constant multiple to an existing weight vector. This constant is often either the L1 (Lasso) or L2 (ridge), but <b>can</b> in actuality <b>can</b> be any norm. The ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What are L1, L2 <b>and Elastic Net Regularization in neural</b> ... - MachineCurve", "url": "https://www.machinecurve.com/index.php/2020/01/21/what-are-l1-l2-and-elastic-net-regularization-in-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/01/21/what-are-l1-l2-and-elastic-net...", "snippet": "The <b>need</b> for regularization during model <b>training</b>. When you are <b>training</b> a machine learning model, at a high level, you\u2019re learning a function \\(\\hat{y}: f(x) \\) which transforms some input value \\(x\\) (often a vector, so \\(\\textbf{x}\\)) into some output value \\(\\hat{y}\\) (often a scalar value, such as a class when classifying and a real <b>number</b> when regressing).. Contrary to a regular mathematical function, the exact mapping (to \\(y\\)) is not known in advance, but is learnt based on the ...", "dateLastCrawled": "2022-02-02T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Race to Improve Radar Imagery: An overview of recent progress in ...", "url": "https://www.researchgate.net/publication/309706444_The_Race_to_Improve_Radar_Imagery_An_overview_of_recent_progress_in_statistical_sparsity-based_techniques", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/309706444_The_Race_to_Improve_Radar_Imagery...", "snippet": "Simultaneously, it is able to better facilitate ISAR imaging with limited <b>training</b> samples than <b>data</b>-driven methods owing to its simple structure and small <b>number</b> of adjustable <b>parameters</b> ...", "dateLastCrawled": "2021-11-09T13:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "96 questions with answers in <b>DIMENSIONALITY REDUCTION</b> | Science topic", "url": "https://www.researchgate.net/topic/Dimensionality-Reduction", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/<b>Dimensionality-Reduction</b>", "snippet": "1- Choice of algorithm: Kmeans is acting <b>very</b> poorly. 2-Distance metrics: In light of typr of <b>data</b> and algorithm. 3-Additional steps to reduce <b>number</b> offeatures. 4- <b>Number</b> of dimenssions: or an ...", "dateLastCrawled": "2022-01-30T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Over-parameterization: Pitfalls and Opportunities", "url": "https://icml.cc/virtual/2021/workshop/8357", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2021/workshop/8357", "snippet": "We show that the min 2-norm interpolator of <b>training</b> <b>data</b> <b>can</b> be susceptible even to adversaries who <b>can</b> only perturb the low-dimensional inputs and not the high-dimensional lifted <b>features</b> directly. The adversarial vulnerability arises because of a phenomena we term spatial localization: the predictions of the learned model are markedly more sensitive in the vicinity of <b>training</b> <b>points</b> than elsewhere. This sensitivity is crucially a consequence of feature lifting and <b>can</b> have consequences ...", "dateLastCrawled": "2022-02-02T15:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Sparsity</b> is an essential feature of many contemporary data problems. Remote sensing, various forms of automated screening and other high throughput measurement devices collect a large amount of ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Regularization \u2014 Understanding L1 and L2 regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2...", "snippet": "The <b>sparsity</b> feature used in L1 regularization has been used extensively as a feature selection mechanism in <b>machine</b> <b>learning</b>. Feature selection is a mechanism which inherently simplifies a ...", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "An E\ufb03cient Sparse Metric <b>Learning</b> in High ... - <b>Machine</b> <b>Learning</b>", "url": "http://machinelearning.org/archive/icml2009/papers/46.pdf", "isFamilyFriendly": true, "displayUrl": "<b>machinelearning</b>.org/archive/icml2009/papers/46.pdf", "snippet": "This <b>sparsity</b> prior of <b>learning</b> distance metric serves to regularize the com-plexity of the distance model especially in the \u201cless example number p and high dimension d\u201d setting. Theoretically, by <b>analogy</b> to the covariance estimation problem, we \ufb01nd the proposed distance <b>learning</b> algorithm has a consistent result at rate O!&quot;# m2 logd $% n &amp; to the target distance matrix with at most m nonzeros per row. Moreover, from the imple-mentation perspective, this! 1-penalized log-determinant ...", "dateLastCrawled": "2021-11-19T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Dynamical <b>machine</b> <b>learning</b> volumetric reconstruction of objects ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8027224/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8027224", "snippet": "The sequence index in the angle of illumination plays the role of discrete time in the dynamical system <b>analogy</b>. Thus, the imaging problem turns into a problem of nonlinear system identification, which also suggests dynamical <b>learning</b> as a better fit to regularize the reconstructions. We devised a Recurrent Neural Network (RNN) architecture with a novel Separable-Convolution Gated Recurrent Unit (SC-GRU) as the fundamental building block. Through a comprehensive comparison of several ...", "dateLastCrawled": "2022-01-08T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Dynamical machine learning volumetric reconstruction of</b> objects ...", "url": "https://www.nature.com/articles/s41377-021-00512-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41377-021-00512-x", "snippet": "Recently, thanks to a ground-breaking observation from 2010 that <b>sparsity</b> can be learnt by a deep neural network 48, the idea of using <b>machine</b> <b>learning</b> to approximate solutions to inverse problems ...", "dateLastCrawled": "2022-02-02T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Dynamical <b>machine</b> <b>learning</b> volumetric reconstruction of objects ...", "url": "https://thetalkingmachines.com/sites/default/files/2021-04/s41377-021-00512-x.pdf", "isFamilyFriendly": true, "displayUrl": "https://thetalking<b>machines</b>.com/sites/default/files/2021-04/s41377-021-00512-x.pdf", "snippet": "The <b>analogy</b> of the BPM computa-tional structure with a neural network was exploited, in conjunction with gradient descent optimization, to obtain the 3D refractive index as the\u201cweights\u201d of the analogous neural network in the <b>learning</b> tomography approach41\u201343. BPM has also been used with more tra-ditional <b>sparsity</b>-based inverse methods33,44. Later, a <b>machine</b> <b>learning</b> approach with a Convolutional Neural Network (CNN) replacing the iterative gradient descent algorithm exhibited even ...", "dateLastCrawled": "2021-10-11T18:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Discovering governing equations from data</b> by sparse identification of ...", "url": "https://www.pnas.org/content/pnas/113/15/3932.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/<b>pnas</b>/113/15/3932.full.pdf", "snippet": "examples. In this work, we combin e <b>sparsity</b>-promoting techniques and <b>machine</b> <b>learning</b> with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only as-sumption about the structureof the model is that there are onlya few important terms that govern the dy namics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to ...", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Dendritic Computing: Branching Deeper into <b>Machine</b> <b>Learning</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0306452221005017", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306452221005017", "snippet": "We link these studies to applications in <b>machine</b> <b>learning</b> and deep <b>learning</b>. ... but relied on their <b>analogy</b> with continuous time hidden markov model (HMM) formulations (Hasler et al., 2007, George et al., 2013). In this line of work, the authors draw an <b>analogy</b> between the activation along a dendrite and a traveling wave equation that is similar to the continuous time formulation of hidden markov models (HMM) (Lazzaro et al., 1996). The traveling wave effect was found to be more prominent ...", "dateLastCrawled": "2022-01-29T08:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "The result is a <b>learning</b> model that may result in generally better word embeddings. GloVe, is a new global log-bilinear regression model for the unsupervised <b>learning</b> of word representations that outperforms other models on word <b>analogy</b>, word similarity, and named entity recognition tasks. \u2014 GloVe: Global Vectors for Word Representation, 2014.", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Learning Neural Representations for Network Anomaly Detection</b>", "url": "https://www.researchgate.net/publication/325797465_Learning_Neural_Representations_for_Network_Anomaly_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/325797465_<b>Learning</b>_Neural_Representations_for...", "snippet": "Many <b>machine</b> <b>learning</b> algorithms have been. Manuscript received December 22, 2017; revised March 13, 2018. This. work is funded by Vietnam International Education De velopment (VIED) and. by ...", "dateLastCrawled": "2021-12-06T22:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Self-representation based dual-graph regularized <b>feature selection</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation ... Her current research interests include pattern recognition and <b>machine</b> <b>learning</b>. Licheng Jiao (SM\u05f389) received the B.S. degree from Shanghai Jiaotong University, Shanghai, China, in 1982, the M.S. and Ph.D. degrees from Xi\u05f3an Jiaotong University, Xi\u05f3an, China, in 1984 and 1990, respectively. From 1990 to 1991, he was a postdoctoral Fellow in the National Key Laboratory for Radar Signal ...", "dateLastCrawled": "2021-11-22T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Self-representation based dual-graph regularized feature selection ...", "url": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "snippet": "<b>machine</b> <b>learning</b> and computer vision \ufb01elds [41]. <b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation [41]. Taking into account of manifold <b>learning</b> and feature selection, and inspired by the self-representation property and the idea of dual-regularization <b>learning</b> [44,45], we propose a novel feature selection algorithm for clustering, named self-representation based dual-graph regularized feature selection clustering (DFSC). This algorithm ...", "dateLastCrawled": "2022-02-02T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Unsupervised feature selection</b> by <b>regularized self-representation</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation. With the above considerations, in this paper we propose a simple yet very effective <b>unsupervised feature selection</b> method by exploiting the self-representation ability of features. The feature matrix is represented over itself to find the representative feature components. The representation residual is minimized by L 2, 1-norm loss to reduce the effect of outlier samples. Different from the ...", "dateLastCrawled": "2022-01-24T12:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Talk <b>Archive</b> - Research on Algorithms and Incentives in Networks", "url": "https://rain.stanford.edu/schedule/archive.shtml", "isFamilyFriendly": true, "displayUrl": "https://rain.stanford.edu/schedule/<b>archive</b>.shtml", "snippet": "McFowland\u2019s research interests\u2014which lie at the intersection of Information Systems, <b>Machine</b> <b>Learning</b>, and Public Policy\u2014include the development of computationally efficient algorithms for large-scale statistical <b>machine</b> <b>learning</b> and \u201cbig data\u201d analytics. More specifically, his research seeks to demonstrate that many real-world problems faced by organizations, and society more broadly, can be reduced to the tasks of anomalous pattern detection and discovery. As a data and ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Talks - <b>sites.google.com</b>", "url": "https://sites.google.com/view/dssseminarseries/talks", "isFamilyFriendly": true, "displayUrl": "https://<b>sites.google.com</b>/view/dssseminarseries/talks", "snippet": "Abstracts &amp; Bios for upcoming talks", "dateLastCrawled": "2022-01-27T14:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Sparse representations for text categorization</b>", "url": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text_categorization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text...", "snippet": "<b>Machine</b> <b>learning</b> for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is ...", "dateLastCrawled": "2021-12-10T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Continual Learning via Neural Pruning</b> | DeepAI", "url": "https://deepai.org/publication/continual-learning-via-neural-pruning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>continual-learning-via-neural-pruning</b>", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more efficient use of resources in machines with memory constraints.", "dateLastCrawled": "2021-12-30T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Non-negative data-<b>driven mapping of structural connections</b> with ...", "url": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "snippet": "For ICA, <b>sparsity can be thought of as</b> a proxy for independence. 3.5. In-vivo data decompositions. For real data, we decomposed group-average tractography matrices, using independent component analysis (ICA) and non-negative matrix factorisation (NMF), with a range of model orders K. ICA was initialised with regular PCA, in which the first 500 components were retained (explaining 97% of the total variance). ICA was applied to the reduced dataset using the FastICA algorithm (Hyv\u00e4rinen and ...", "dateLastCrawled": "2021-10-11T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Sparse Representations for Text Categorization</b> | Dimitri Kanevsky ...", "url": "https://www.academia.edu/2738730/Sparse_Representations_for_Text_Categorization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2738730/<b>Sparse_Representations_for_Text_Categorization</b>", "snippet": "A Comparative Study of <b>Machine</b> <b>Learning</b> Methods for Verbal Autopsy Text Classification. By Eric S Atwell and Samuel Danso. CSC435 book proposal. By Russell Frith. Higher-Order Smoothing: A Novel Semantic Smoothing Method for Text Classification. By Murat C Ganiz, Mitat Poyraz, and Zeynep Kilimci. INFORMATION RETRIEVAL. By febi k. Introduction to information retrieval. By Valeria Mesi. Download pdf. \u00d7 Close Log In. Log In with Facebook Log In with Google. Sign Up with Apple. or. Email ...", "dateLastCrawled": "2021-10-13T23:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Continual <b>Learning</b> via Neural Pruning \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1903.04476/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1903.04476", "snippet": "We introduce Continual <b>Learning</b> via Neural Pruning (CLNP), a new method aimed at lifelong <b>learning</b> in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the ...", "dateLastCrawled": "2021-11-07T10:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Continual <b>Learning</b> via Neural Pruning", "url": "https://openreview.net/pdf?id=Hyl_XXYLIB", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=Hyl_XXYLIB", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much at-tention from the <b>machine</b> <b>learning</b> community in recent years. The main obstacle for effective continual <b>learning</b> is the problem of cata-strophic forgetting: machines trained on new problems forget about", "dateLastCrawled": "2022-01-05T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Abstract - arXiv.org e-Print archive", "url": "https://arxiv.org/pdf/1903.04476", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1903.04476", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more ef\ufb01cient use of resources in machines with memory constraints. There is also great interest in continual <b>learning</b> from a more long term ...", "dateLastCrawled": "2021-10-25T14:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Introduction to compressed sensing</b>", "url": "https://www.researchgate.net/publication/220043734_Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220043734_<b>Introduction_to_compressed_sensing</b>", "snippet": "systems control, clustering, and <b>machine</b> <b>learning</b> [14, 15, 58, 61, 89, 193, 217, 240, 244]. Low-dimensional manifolds hav e also been prop osed as approximate mod-", "dateLastCrawled": "2022-01-14T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Introduction to compressed sensing</b> | Marco Duarte - Academia.edu", "url": "https://www.academia.edu/1443164/Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/1443164/<b>Introduction_to_compressed_sensing</b>", "snippet": "<b>Introduction to Compressed Sensing</b> For any x \u2208 \u03a3k , we can associate a k-face of C n with the support and sign pattern of x. One can show that the number of k-faces of AC n is precisely the number of index sets of size k for which signals supported on them can be recovered by (1.12) with B (y) = {z : Az = y}.", "dateLastCrawled": "2022-01-21T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Compressed Sensing : Theory and Applications</b> | Kutyniok, Gitta Eldar ...", "url": "https://b-ok.africa/book/2086657/84a688", "isFamilyFriendly": true, "displayUrl": "https://b-ok.africa/book/2086657/84a688", "snippet": "You can write a book review and share your experiences. Other readers will always be interested in your opinion of the books you&#39;ve read. Whether you&#39;ve loved the book or not, if you give your honest and detailed thoughts then people will find new books that are right for them.", "dateLastCrawled": "2021-12-26T07:22:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparsity)  is like +(when there are very few training data points relative to the number of parameters or features that need to be estimated)", "+(sparsity) is similar to +(when there are very few training data points relative to the number of parameters or features that need to be estimated)", "+(sparsity) can be thought of as +(when there are very few training data points relative to the number of parameters or features that need to be estimated)", "+(sparsity) can be compared to +(when there are very few training data points relative to the number of parameters or features that need to be estimated)", "machine learning +(sparsity AND analogy)", "machine learning +(\"sparsity is like\")", "machine learning +(\"sparsity is similar\")", "machine learning +(\"just as sparsity\")", "machine learning +(\"sparsity can be thought of as\")", "machine learning +(\"sparsity can be compared to\")"]}