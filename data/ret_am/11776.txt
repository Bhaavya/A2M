{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "<b>Hinge</b> Embedding <b>Loss</b>. torch.nn.HingeEmbeddingLoss. Measures the <b>loss</b> given an input tensor x and a labels tensor y containing <b>values</b> (1 or -1). It is used for measuring whether two inputs are ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "LossFunctions in Deep Learning-DeepVidhya", "url": "https://deepvidhya.com/blog/lossfunctions-in-deep-learning-1205", "isFamilyFriendly": true, "displayUrl": "https://deepvidhya.com/blog/<b>loss</b>functions-in-deep-learning-1205", "snippet": "Note: Disadvantage of The L2 standard is that if are typical <b>values</b>, these points will explain the main component of the <b>loss</b>. for example, the real value is 1, the prediction is 10-foldThe prediction value is 1000 once, and the prediction value of the other times is about 1, obvious, the <b>loss</b> value is mainly through 1000", "dateLastCrawled": "2022-01-28T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Common <b>Loss</b> functions in machine learning | by Ravindra Parmar ...", "url": "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/common-<b>loss</b>-functions-in-machine-learning-46af0ffc4d23", "snippet": "<b>Hinge</b> <b>Loss</b>/Multi class SVM <b>Loss</b>. In simple terms, the score of correct category should be greater than sum of scores of all incorrect categories by some safety margin (usually one). And hence <b>hinge</b> <b>loss</b> is used for maximum-margin classification, most notably for support vector machines.", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "3.3. Metrics and scoring: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "<b>hinge</b>_<b>loss</b> (y_<b>true</b>, pred_decision, *[, ...]) Average <b>hinge</b> <b>loss</b> (non-regularized). ... <b>like</b> binary targets, as an array of class labels, multilabel data is specified as an indicator matrix, in which cell [i, j] has value 1 if sample i has label j and value 0 otherwise. 3.3.2.2. Accuracy <b>score</b>\u00b6 The accuracy_<b>score</b> function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions. In multilabel classification, the function returns the subset ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "MITx_6.86x/Unit 01 - Linear Classifiers and Generalizations.md at ...", "url": "https://github.com/sylvaticus/MITx_6.86x/blob/master/Unit%2001%20-%20Linear%20Classifiers%20and%20Generalizations/Unit%2001%20-%20Linear%20Classifiers%20and%20Generalizations.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/sylvaticus/MITx_6.86x/blob/master/Unit 01 - Linear Classifiers and...", "snippet": "Lecture 3 <b>Hinge</b> <b>loss</b>, Margin boundaries and Regularization. Slides. 3.1. Objective. <b>Hinge</b> <b>loss</b>, Margin boundaries, and Regularization . At the end of this lecture, you will be able to. understand the need for maximizing the margin; pose linear classification as an optimization problem; understand <b>hinge</b> <b>loss</b>, margin boundaries and regularization; 3.2. Introduction. Today, we will talk about how to turn machine learning problems into optimization problems. That is, we are going to turn the ...", "dateLastCrawled": "2021-12-20T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning using Python Interview Questions</b> &amp; Answers | Beginner ...", "url": "https://www.zeolearn.com/interview-questions/machine-learning-using-python", "isFamilyFriendly": true, "displayUrl": "https://www.zeolearn.com/interview-questions/machine-learning-using-python", "snippet": "SVM uses <b>hinge</b> <b>loss</b> function: Where w^2 is the regularize and is the <b>loss</b> function. Add Bookmark 10. Discuss some of the pre-processing techniques used to prepare the data in python? Mean removal - It involves removing the mean from each feature so that it is centred on zero. Mean removal helps in removing any bias from the features. Feature scaling - The <b>values</b> of every feature in a data point can vary <b>between</b> random <b>values</b>. So, it is important to scale them so that this matches specified ...", "dateLastCrawled": "2022-02-02T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Keras</b> <b>Loss</b> Functions: Everything You Need to Know - neptune.ai", "url": "https://neptune.ai/blog/keras-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>keras</b>-<b>loss</b>-functions", "snippet": "The CategoricalCrossentropy also computes the cross-entropy <b>loss</b> <b>between</b> <b>the true</b> classes and predicted classes. The labels are given in an one_hot format. cce = tf.<b>keras</b>.losses.CategoricalCrossentropy() cce(y_<b>true</b>, y_pred).numpy() Sparse Categorical Crossentropy. If you have two or more classes and the labels are integers, the SparseCategoricalCrossentropy should be used. y_<b>true</b> = [0, 1, 2] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1],[0.1, 0.8, 0.1]] scce = tf.<b>keras</b>.losses ...", "dateLastCrawled": "2022-02-02T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Compare accuracies of two classification models by repeated cross ...", "url": "https://www.mathworks.com/help/stats/testckfold.html", "isFamilyFriendly": true, "displayUrl": "https://www.mathworks.com/help/stats/<b>testckfold</b>.html", "snippet": "Estimate the classification <b>loss</b> by comparing the two sets of <b>estimated</b> labels to <b>the true</b> labels. Denote ... Compute the <b>difference</b> <b>between</b> the classification losses of the two models: \u03b4 ^ r k = e 1 r k \u2212 e 2 r k. At the end of a run, there are K classification losses per classification model. Combine the results of step 2. For each r = 1 through R: Estimate the within-fold averages of the differences and their average: \u03b4 \u00af r = 1 K \u2211 k = 1 K \u03b4 ^ k r. Estimate the overall average of ...", "dateLastCrawled": "2022-02-02T15:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "Cross-entropy will calculate a score that summarizes the average <b>difference</b> <b>between</b> the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0. Cross-entropy can be specified as the <b>loss</b> function in Keras by specifying \u2018binary_crossentropy\u2018 when compiling the model. 1. model. compile (<b>loss</b> = &#39;binary_crossentropy&#39;, optimizer = opt, metrics = [&#39;accuracy&#39;]) The function requires that the output layer is configured ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "10-701/15-781 Machine Learning - Midterm Exam, Fall 2010", "url": "https://cs.cmu.edu/~aarti/Class/10701/exams/midterm2010f_sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.cmu.edu/~aarti/Class/10701/exams/midterm2010f_sol.pdf", "snippet": "a model selection procedure <b>like</b> cross validation can be used to select the appropriate model complexity and reduce the possibility of over\ufb01tting. 6. The kernel density estimator is equivalent to performing kernel regression with the value Y i= 1 n at each point X i in the original data set. False: Kernel regression predicts the value of a point as the weighted average of the <b>values</b> at nearby points, therefore if all of the points have the same value, then kernel regression will predict a ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "<b>Hinge</b> Embedding <b>Loss</b>. torch.nn.HingeEmbeddingLoss. Measures the <b>loss</b> given an input tensor x and a labels tensor y containing <b>values</b> (1 or -1). It is used for measuring whether two inputs are ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Loss functions for classification</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Loss_functions_for_classification", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Loss_functions_for_classification</b>", "snippet": "This conclusion makes the <b>hinge</b> <b>loss</b> quite attractive, as bounds can be placed on the <b>difference</b> <b>between</b> expected risk and the sign of <b>hinge</b> <b>loss</b> function. The <b>Hinge</b> <b>loss</b> cannot be derived from (2) since is not invertible. Generalized smooth <b>hinge</b> <b>loss</b>. The generalized smooth <b>hinge</b> <b>loss</b> function with parameter is defined as", "dateLastCrawled": "2022-02-03T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Cost Functions In Machine Learning</b> - The Click Reader", "url": "https://www.theclickreader.com/cost-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.theclickreader.com/<b>cost-functions-in-machine-learning</b>", "snippet": "is the probability distribution of the predicted <b>values</b>, and is the total number of observations taken. 7. <b>Hinge</b> <b>loss</b>. The <b>hinge</b> <b>loss</b> function is a common cost function used in Support Vector Machines (SVM) for classification. It maps the output to <b>values</b> <b>between</b> 1, 0, -1. The <b>Hinge</b> <b>loss</b> function is calculated as, where is actual value of the ...", "dateLastCrawled": "2022-02-02T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning using Python Interview Questions</b> &amp; Answers | Beginner ...", "url": "https://www.zeolearn.com/interview-questions/machine-learning-using-python", "isFamilyFriendly": true, "displayUrl": "https://www.zeolearn.com/interview-questions/machine-learning-using-python", "snippet": "SVM uses <b>hinge</b> <b>loss</b> function: Where w^2 is the regularize and is the <b>loss</b> function. Add Bookmark 10. Discuss some of the pre-processing techniques used to prepare the data in python? Mean removal - It involves removing the mean from each feature so that it is centred on zero. Mean removal helps in removing any bias from the features. Feature scaling - The <b>values</b> of every feature in a data point can vary <b>between</b> random <b>values</b>. So, it is important to scale them so that this matches specified ...", "dateLastCrawled": "2022-02-02T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Scikit Learn - Quick Guide</b> - <b>Tutorialspoint</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_quick_guide.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_quick_guide</b>.htm", "snippet": "squared_<b>hinge</b> \u2212 <b>similar</b> to \u2018<b>hinge</b>\u2019 <b>loss</b> but it is quadratically penalized. perceptron \u2212 as the name suggests, it is a linear <b>loss</b> which is used by the perceptron algorithm. 2: penalty \u2212 str, \u2018none\u2019, \u2018l2\u2019, \u2018l1\u2019, \u2018elasticnet\u2019 It is the regularization term used in the model. By default, it is L2. We can use L1 or \u2018elasticnet; as well but both might bring sparsity to the model, hence not achievable with L2. 3: alpha \u2212 float, default = 0.0001. Alpha, the constant ...", "dateLastCrawled": "2022-01-30T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Compare accuracies of two classification models by repeated cross ...", "url": "https://www.mathworks.com/help/stats/testckfold.html", "isFamilyFriendly": true, "displayUrl": "https://www.mathworks.com/help/stats/<b>testckfold</b>.html", "snippet": "Estimate the classification <b>loss</b> by comparing the two sets of <b>estimated</b> labels to <b>the true</b> labels. Denote ... Exponential <b>loss</b> <b>is similar</b> to binomial deviance and has the form. e 1 = \u2211 j = 1 n t e s t w j exp (\u2212 y j f (X j)) \u2211 j = 1 n t e s t w j. y j and f (X j) take the same forms here as in the binomial deviance formula. <b>Hinge</b> <b>loss</b> has the form. e 1 = \u2211 j = 1 n w j max {0, 1 \u2212 y j \u2032 f (X j)} \u2211 j = 1 n w j, y j and f (X j) take the same forms here as in the binomial deviance ...", "dateLastCrawled": "2022-02-02T15:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "MITx_6.86x/Unit 01 - Linear Classifiers and Generalizations.md at ...", "url": "https://github.com/sylvaticus/MITx_6.86x/blob/master/Unit%2001%20-%20Linear%20Classifiers%20and%20Generalizations/Unit%2001%20-%20Linear%20Classifiers%20and%20Generalizations.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/sylvaticus/MITx_6.86x/blob/master/Unit 01 - Linear Classifiers and...", "snippet": "Lecture 3 <b>Hinge</b> <b>loss</b>, Margin boundaries and Regularization. Slides. 3.1. Objective. <b>Hinge</b> <b>loss</b>, Margin boundaries, and Regularization . At the end of this lecture, you will be able to. understand the need for maximizing the margin; pose linear classification as an optimization problem; understand <b>hinge</b> <b>loss</b>, margin boundaries and regularization; 3.2. Introduction. Today, we will talk about how to turn machine learning problems into optimization problems. That is, we are going to turn the ...", "dateLastCrawled": "2021-12-20T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "LossFunctions in Deep Learning-DeepVidhya", "url": "https://deepvidhya.com/blog/lossfunctions-in-deep-learning-1205", "isFamilyFriendly": true, "displayUrl": "https://deepvidhya.com/blog/<b>loss</b>functions-in-deep-learning-1205", "snippet": "Note: Disadvantage of The L2 standard is that if are typical <b>values</b>, these points will explain the main component of the <b>loss</b>. for example, the real value is 1, the prediction is 10-foldThe prediction value is 1000 once, and the prediction value of the other times is about 1, obvious, the <b>loss</b> value is mainly through 1000", "dateLastCrawled": "2022-01-28T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "Cross-entropy will calculate a score that summarizes the average <b>difference</b> <b>between</b> the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0. Cross-entropy can be specified as the <b>loss</b> function in Keras by specifying \u2018binary_crossentropy\u2018 when compiling the model. 1. model. compile (<b>loss</b> = &#39;binary_crossentropy&#39;, optimizer = opt, metrics = [&#39;accuracy&#39;]) The function requires that the output layer is configured ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "10-701/15-781 Machine Learning - Midterm Exam, Fall 2010", "url": "https://cs.cmu.edu/~aarti/Class/10701/exams/midterm2010f_sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.cmu.edu/~aarti/Class/10701/exams/midterm2010f_sol.pdf", "snippet": "<b>True</b>: Estimate the joint density P(Y;X), then use it to calculate P(YjX). 2. The correspondence <b>between</b> logistic regression and Gaussian Na ve Bayes (with iden- tity class covariances) means that there is a one-to-one correspondence <b>between</b> the parameters of the two classi ers. False: Each LR model parameter corresponds to a whole set of possible GNB classi\ufb01er parameters, there is no one-to-one correspondence because logistic regression is discrimi-native and therefore doesn\u2019t model P(X ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Cost Functions In Machine Learning</b> - The Click Reader", "url": "https://www.theclickreader.com/cost-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.theclickreader.com/<b>cost-functions-in-machine-learning</b>", "snippet": "It maps the output to <b>values</b> <b>between</b> 1, 0, -1. The <b>Hinge</b> <b>loss</b> function is calculated as, where is actual value of the output, is the classification score predicted by the model. From the function, it <b>can</b> be determined that when , then the cost function is zero. However, when , the cost function increases.", "dateLastCrawled": "2022-02-02T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Concepts</b> | Machine Learning - Michael Clark", "url": "https://m-clark.github.io/introduction-to-machine-learning/concepts.html", "isFamilyFriendly": true, "displayUrl": "https://m-clark.github.io/introduction-to-machine-learning/<b>concepts</b>.html", "snippet": "In the following, <b>the true</b> <b>values</b> for the intercept and other coefficients are ... <b>Hinge</b> <b>Loss</b>. A final <b>loss</b> function to consider, typically used with support vector machines, is the <b>hinge</b> <b>loss</b> function. \\[L(Y, f(X)) = \\max(1-yf, 0)\\] Here negative <b>values</b> of \\(yf\\) are misclassifications, and so correct classifications do not contribute to the <b>loss</b>. We could also note it as \\(\\sum (1-yf)_+\\), i.e. summing only those positive <b>values</b> of \\(1-yf\\). The following image compares these (from Hastie ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "3.3. Model <b>Evaluation</b> - Scikit-learn - W3cubDocs", "url": "https://docs.w3cub.com/scikit_learn/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://docs.w3cub.com/scikit_learn/modules/model_<b>evaluation</b>.html", "snippet": "3.3.2.10. <b>Hinge</b> <b>loss</b>. The <b>hinge</b>_<b>loss</b> function computes the average distance <b>between</b> the model and the data using <b>hinge</b> <b>loss</b>, a one-sided metric that considers only prediction errors. (<b>Hinge</b> <b>loss</b> is used in maximal margin classifiers such as support vector machines.)", "dateLastCrawled": "2022-01-28T16:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Cost Function and <b>Loss</b> Function in Machine Learning - Shishir Kant Singh", "url": "http://shishirkant.com/cost-function-and-loss-function-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "shishirkant.com/cost-function-and-<b>loss</b>-function-in-machine-learning", "snippet": "The Cost Function calculates the <b>difference</b> <b>between</b> anticipated and expected <b>values</b> and shows it as a single real number. Cost Functions may be created in a variety of methods depending on the situation. To estimate how poorly models perform, cost functions are employed. Simply put, a cost function is a measure of how inaccurate the model is in estimating the connection <b>between</b> X and y. This is usually stated as a <b>difference</b> or separation <b>between</b> the expected and actual <b>values</b>. The term ...", "dateLastCrawled": "2022-01-29T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Keras</b> <b>Loss</b> Functions: Everything You Need to Know - neptune.ai", "url": "https://neptune.ai/blog/keras-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>keras</b>-<b>loss</b>-functions", "snippet": "The CategoricalCrossentropy also computes the cross-entropy <b>loss</b> <b>between</b> <b>the true</b> classes and predicted classes. The labels are given in an one_hot format. cce = tf.<b>keras</b>.losses.CategoricalCrossentropy() cce(y_<b>true</b>, y_pred).numpy() Sparse Categorical Crossentropy. If you have two or more classes and the labels are integers, the SparseCategoricalCrossentropy should be used. y_<b>true</b> = [0, 1, 2] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1],[0.1, 0.8, 0.1]] scce = tf.<b>keras</b>.losses ...", "dateLastCrawled": "2022-02-02T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Scikit Learn - Quick Guide</b> - <b>Tutorialspoint</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_quick_guide.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_quick_guide</b>.htm", "snippet": "If <b>loss</b> = \u2018epsilon-insensitive\u2019, any <b>difference</b>, <b>between</b> current prediction and the correct label, less than the threshold would be ignored. 10: max_iter \u2212 int, optional, default = 1000. As name suggest, it represents the maximum number of passes over the epochs i.e. training data. 11: warm_start \u2212 bool, optional, default = false. With this parameter set to <b>True</b>, we <b>can</b> reuse the solution of the previous call to fit as initialization. If we choose default i.e. false, it will erase ...", "dateLastCrawled": "2022-01-30T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Stealing Hyperparameters in Machine Learning</b>", "url": "https://www.researchgate.net/publication/323218177_Stealing_Hyperparameters_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/323218177_Stealing_Hyperparameters_in_Machine...", "snippet": "and square <b>hinge</b> <b>loss</b> <b>can</b> more effecti vely defend against our. attacks than re gular <b>hinge</b> <b>loss</b> using rounding. The cross-entropy <b>loss</b> function is adopted by logistic regression [20], while ...", "dateLastCrawled": "2022-02-01T01:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Generative vs. Discriminative; Bayesian vs. Frequentist</b> | LingPipe Blog", "url": "https://lingpipe-blog.com/2013/04/12/generative-vs-discriminative-bayesian-vs-frequentist/", "isFamilyFriendly": true, "displayUrl": "https://lingpipe-blog.com/2013/04/12/<b>generative-vs-discriminative-bayesian-vs-frequentist</b>", "snippet": "Rather, the <b>difference</b> is in the training (or model estimation) procedure; the <b>loss</b> function for the HMM is joint, while the <b>loss</b> function for the CRF is conditional. Of course, if you optimise conditional log likelihood, you shouldn\u2019t expect the resulting model to do a good job of predicting the joint distribution (even though you could in fact technically use a conditionally-<b>estimated</b> model to do so).", "dateLastCrawled": "2022-01-12T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "3.3. <b>Model evaluation: quantifying the quality of predictions</b> \u2014 scikit ...", "url": "https://scikit-learn-docs-chs.readthedocs.io/zh_CN/latest/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn-docs-chs.readthedocs.io/zh_CN/latest/modules/model_evaluation.html", "snippet": "The actual outcome has to be 1 or 0 (<b>true</b> or false), while the predicted probability of the actual outcome <b>can</b> be a value <b>between</b> 0 and 1. The brier score <b>loss</b> is also <b>between</b> 0 to 1 and the lower the score (the mean square <b>difference</b> is smaller), the more accurate the prediction is. It <b>can</b> <b>be thought</b> of as a measure of the \u201ccalibration\u201d of ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Quizzes Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/423155597/quizzes-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/423155597/quizzes-flash-cards", "snippet": "Which of the following statements about purchased customer data is <b>true</b>? The data <b>can</b> be very broad. The data paint a picture of a business&#39;s own customers. The data may be more useful for new customer acquisition. The data are useful in managing relationships with existing customers. The data may be more useful for new customer acquisition. Data purchased from companies such as database marketing companies provide information about customers outside a company&#39;s own customer base. These data ...", "dateLastCrawled": "2022-01-14T08:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Types of Loss Function</b> - OpenGenus IQ: Computing Expertise", "url": "https://iq.opengenus.org/types-of-loss-function/", "isFamilyFriendly": true, "displayUrl": "https://iq.opengenus.org/<b>types-of-loss-function</b>", "snippet": "keras.losses.<b>hinge</b>(y_<b>true</b>, y_pred) The <b>hinge</b> <b>loss</b> provides a relatively tight, convex upper bound on the 0\u20131 indicator function. In addition, the empirical risk minimization of this <b>loss</b> is equivalent to the classical formulation for support vector machines (SVMs). Correctly classified points lying outside the margin boundaries of the support vectors are not penalized, whereas points within the margin boundaries or on the wrong side of the hyperplane are penalized in a linear fashion ...", "dateLastCrawled": "2022-01-28T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "3.3. Metrics and scoring: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "<b>hinge</b>_<b>loss</b> (y_<b>true</b>, pred_decision, *[, ...]) Average <b>hinge</b> <b>loss</b> (non-regularized). ... <b>Compared</b> with the ranking <b>loss</b>, NDCG <b>can</b> take into account relevance scores, rather than a ground-truth ranking. So if the ground-truth consists only of an ordering, the ranking <b>loss</b> should be preferred; if the ground-truth consists of actual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very relevant), NDCG <b>can</b> be used. For one sample, given the vector of continuous ground-truth <b>values</b> ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Common <b>Loss</b> functions in machine learning | by Ravindra Parmar ...", "url": "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/common-<b>loss</b>-functions-in-machine-learning-46af0ffc4d23", "snippet": "<b>Hinge</b> <b>Loss</b>/Multi class SVM <b>Loss</b>. In simple terms, the score of correct category should be greater than sum of scores of all incorrect categories by some safety margin (usually one). And hence <b>hinge</b> <b>loss</b> is used for maximum-margin classification, most notably for support vector machines.", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Loss functions for classification</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Loss_functions_for_classification", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Loss_functions_for_classification</b>", "snippet": "The third equality follows from the fact that 1 and \u22121 are the only possible <b>values</b> for , and the fourth ... SVMs utilizing the <b>hinge</b> <b>loss</b> function <b>can</b> also be solved using quadratic programming. The minimizer of [] for the <b>hinge</b> <b>loss</b> function is (\u2192) = {(\u2192) &gt; (\u2192) (\u2192) &lt; (\u2192) when (), which matches that of the 0\u20131 indicator function. This conclusion makes the <b>hinge</b> <b>loss</b> quite attractive, as bounds <b>can</b> be placed on the <b>difference</b> <b>between</b> expected risk and the sign of <b>hinge</b> <b>loss</b> ...", "dateLastCrawled": "2022-02-03T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "<b>Hinge</b> Embedding <b>Loss</b>. torch.nn.HingeEmbeddingLoss. Measures the <b>loss</b> given an input tensor x and a labels tensor y containing <b>values</b> (1 or -1). It is used for measuring whether two inputs are ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning using Python Interview Questions</b> &amp; Answers | Beginner ...", "url": "https://www.zeolearn.com/interview-questions/machine-learning-using-python", "isFamilyFriendly": true, "displayUrl": "https://www.zeolearn.com/interview-questions/machine-learning-using-python", "snippet": "The key <b>difference</b> <b>between</b> these two is the penalty term. ... SVM uses <b>hinge</b> <b>loss</b> function: Where w^2 is the regularize and is the <b>loss</b> function. Add Bookmark 10. Discuss some of the pre-processing techniques used to prepare the data in python? Mean removal - It involves removing the mean from each feature so that it is centred on zero. Mean removal helps in removing any bias from the features. Feature scaling - The <b>values</b> of every feature in a data point <b>can</b> vary <b>between</b> random <b>values</b>. So ...", "dateLastCrawled": "2022-02-02T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Cost Functions In Machine Learning</b> - The Click Reader", "url": "https://www.theclickreader.com/cost-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.theclickreader.com/<b>cost-functions-in-machine-learning</b>", "snippet": "It maps the output to <b>values</b> <b>between</b> 1, 0, -1. The <b>Hinge</b> <b>loss</b> function is calculated as, where is actual value of the output, is the classification score predicted by the model. From the function, it <b>can</b> be determined that when , then the cost function is zero. However, when , the cost function increases.", "dateLastCrawled": "2022-02-02T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Understanding Loss Functions in Machine Learning</b> | Engineering ...", "url": "https://www.section.io/engineering-education/understanding-loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.section.io/engineering-education/<b>understanding-loss-functions-in-machine</b>...", "snippet": "Huber <b>Loss</b>. A comparison <b>between</b> L1 and L2 <b>loss</b> yields the following results: L1 <b>loss</b> is more robust than its counterpart. On taking a closer look at the formulas, one <b>can</b> observe that if the <b>difference</b> <b>between</b> the predicted and the actual value is high, L2 <b>loss</b> magnifies the effect when <b>compared</b> to L1. Since L2 succumbs to outliers, L1 <b>loss</b> ...", "dateLastCrawled": "2022-02-01T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "LossFunctions in Deep Learning-DeepVidhya", "url": "https://deepvidhya.com/blog/lossfunctions-in-deep-learning-1205", "isFamilyFriendly": true, "displayUrl": "https://deepvidhya.com/blog/<b>loss</b>functions-in-deep-learning-1205", "snippet": "<b>Compared</b> with L2 <b>loss</b>, Huber <b>Loss</b> is less sensitive to outliers (because if the residual is too large, it is a piecewise function, <b>loss</b> is a linear function of the residual). The huber <b>Loss</b> combines the best properties of MSE and MAE.", "dateLastCrawled": "2022-01-28T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "Cross-entropy will calculate a score that summarizes the average <b>difference</b> <b>between</b> the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0. Cross-entropy <b>can</b> be specified as the <b>loss</b> function in Keras by specifying \u2018binary_crossentropy\u2018 when compiling the model. 1. model. compile (<b>loss</b> = &#39;binary_crossentropy&#39;, optimizer = opt, metrics = [&#39;accuracy&#39;]) The function requires that the output layer is configured ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Main concepts behind <b>Machine</b> <b>Learning</b> | by Leven.co.in | Medium", "url": "https://in-leven.medium.com/main-concepts-behind-machine-learning-848ec516ef94", "isFamilyFriendly": true, "displayUrl": "https://in-leven.medium.com/main-concepts-behind-<b>machine</b>-<b>learning</b>-848ec516ef94", "snippet": "The two most common <b>loss</b> function are <b>hinge</b>-<b>loss</b> and cross-entropy. The first one is used in SVM (Supported Vector Machines) classifiers and it concerns in getting the correct class score greater than the other scores by a margin \u0394. Formula for <b>hinge</b>-<b>loss</b>. s\u1d62 is the correct score category. The second one is used in Softmax classifiers which interprets the scores as probabilities, always trying to get the correct class close to 1. Formula for cross-entropy. s\u1d62 the correct category score ...", "dateLastCrawled": "2022-01-14T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Main concepts behind Machine Learning</b> | by Bruno Eidi Nishimoto ...", "url": "https://medium.com/neuronio/main-concepts-behind-machine-learning-22cd81d68a11", "isFamilyFriendly": true, "displayUrl": "https://medium.com/neuronio/<b>main-concepts-behind-machine-learning</b>-22cd81d68a11", "snippet": "The two most common <b>loss</b> function are <b>hinge</b>-<b>loss</b> and cross-entropy. The first one is used in SVM (Supported Vector Machines) classifiers and it concerns in getting the correct class score greater ...", "dateLastCrawled": "2022-01-19T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Metrics to Evaluate Classification and Regression Algorithms | by ...", "url": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and-regression-algorithms-1554f1e00a75", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and...", "snippet": "<b>Hinge</b> <b>Loss</b>(Binary Classification): An alternative to cross-entropy for binary classification problems is the <b>hinge</b> <b>loss</b> function, primarily developed for use with support vector <b>machine</b> (SVM ...", "dateLastCrawled": "2022-01-17T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "In contrast, in <b>machine</b> <b>learning</b> methodology, log <b>loss</b> will be minimized with respect to ... <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the following stages occur: Stage 1 ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "CPSC 340: Data Mining <b>Machine</b> <b>Learning</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "snippet": "\u2022Exam <b>analogy</b> for types of supervised/semi-supervised <b>learning</b>: \u2013Regular supervised <b>learning</b>: ... \u2022For non-separable data, <b>hinge</b> <b>loss</b> minimizes penalizes violations: Kernel Trick \u2022Non-separable data can be separable in high-dimensional space: \u2022Kernel trick: linear regression using similarities instead of features. \u2013If you can compute inner product, you dont to store basis z i. \u2013Can have exponential/infinite basis. Stochastic Gradient \u2022Stochastic gradient methods are ...", "dateLastCrawled": "2021-11-22T01:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, squared <b>hinge</b> <b>loss</b> function (as against <b>hinge</b> <b>loss</b> function) and l2 penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "Optimization methods are applied to minimize the <b>loss</b> function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one <b>loss</b> is L0-1 = 1 (m &lt;= 0); in zero-one <b>loss</b>, value of <b>loss</b> is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this <b>loss</b> is it is not differentiable, non-convex, and also NP-hard. Hence, in order to make optimization feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Models 1.1 Support vector <b>machine</b> 1.1.1 Principle 1.1.2 Kernel 1.1.3 Soft margin SVM 1.1.4 <b>Hinge</b> <b>loss</b> view 1.1.5 Multi-class SVM 1.1.6 Extensions 1.2 Tree-based models 1.2.1 Decision tree 1.2.2 Random forest 1.2.3 Gradient boosted decision trees 1.2.4 Tools 1.3 EM Principle 1.4 MaxEnt 1.4.1 Entropy 1.5 Model selection 1.5.1 Under-fitting / Over-fitting 1.5.2 Model ensemble, sklearn 2.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Statistical <b>Learning</b> Theory and the C-<b>Loss</b> cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/c<b>loss</b>.pdf", "snippet": "Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. Empirical Risk Minimization (ERM) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the Risk functional as L(.) is called the <b>Loss</b> function, and minimize it w.r.t. w achieving the best possible <b>loss</b>. But we can not do this integration because the joint is normally not ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "We\u2019re then using <b>machine</b> <b>learning</b> for ... The squared <b>hinge loss is like</b> the hinge formula displayed above, but then the \\(max()\\) function output is squared. This helps achieving two things: Firstly, it makes the loss value more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add to the loss more significantly than smaller errors. Note that simiarly, this may also mean that you\u2019ll need to inspect your dataset for the presence of such outliers first. Secondly ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - <b>hinge loss</b> vs logistic loss advantages and ...", "url": "https://stats.stackexchange.com/questions/146277/hinge-loss-vs-logistic-loss-advantages-and-disadvantages-limitations", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/146277/<b>hinge-loss</b>-vs-logistic-loss...", "snippet": "<b>machine</b>-<b>learning</b> svm loss-functions computer-vision. Share. Cite. Improve this question. Follow edited Jul 23 &#39;18 at 15:41. DHW. 644 3 3 silver badges 13 13 bronze badges. asked Apr 14 &#39;15 at 11:18. user570593 user570593. 1,059 2 2 gold badges 12 12 silver badges 19 19 bronze badges $\\endgroup$ Add a comment | 3 Answers Active Oldest Votes. 31 $\\begingroup$ Logarithmic loss minimization leads to well-behaved probabilistic outputs. <b>Hinge loss</b> leads to some (not guaranteed) sparsity on the ...", "dateLastCrawled": "2022-01-26T09:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>A Course in Machine Learning</b> | AZERTY UIOP - Academia.edu", "url": "https://www.academia.edu/11902068/A_Course_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/11902068/<b>A_Course_in_Machine_Learning</b>", "snippet": "<b>A Course in Machine Learning</b>. \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset password. Enter the email address you signed up with and we&#39;ll email you a reset link. Need an account? Click here to sign up. Log In Sign ...", "dateLastCrawled": "2022-01-23T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Course in <b>Machine</b> <b>Learning</b>", "url": "http://ciml.info/dl/v0_8/ciml-v0_8-ch12.pdf", "isFamilyFriendly": true, "displayUrl": "ciml.info/dl/v0_8/ciml-v0_8-ch12.pdf", "snippet": "160 a course in <b>machine</b> <b>learning</b> fortunately, not only is the zero-norm non-convex, it\u2019s also discrete. Optimizing it is NP-hard. A reasonable middle-ground is the one-norm: jjwjj 1 = \u00e5 djw j. It is indeed convex: in fact, it is the tighest \u2018p norm that is convex. Moreover, its gradients do not go to zero as in the two-norm. <b>Just as hinge-loss</b> is the tightest convex upper bound on zero-one error, the one-norm is the tighest convex upper bound on the zero-norm. At this point, you should ...", "dateLastCrawled": "2021-09-07T20:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Course in <b>Machine</b> <b>Learning</b> | PDF | <b>Machine</b> <b>Learning</b> | Prediction", "url": "https://www.scribd.com/document/346469890/a-course-in-machine-learning-pdf", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/346469890/a-course-in-<b>machine</b>-<b>learning</b>-pdf", "snippet": "The <b>machine</b> <b>learning</b> algorithm has succeeded if its performance on the test data is high. 1.2 Some Canonical <b>Learning</b> Problems. There are a large number of typical inductive <b>learning</b> problems. The primary difference between them is in what type of thing theyre trying to predict. Here are some examples: Regression: trying to predict a real value. For instance, predict the value of a stock tomorrow given its past performance. Or predict Alices score on the <b>machine</b> <b>learning</b> final exam based on ...", "dateLastCrawled": "2021-12-06T00:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Course in <b>Machine</b> <b>Learning</b>", "url": "http://ciml.info/dl/v0_9/ciml-v0_9-ch12.pdf", "isFamilyFriendly": true, "displayUrl": "ciml.info/dl/v0_9/ciml-v0_9-ch12.pdf", "snippet": "162 a course in <b>machine</b> <b>learning</b> pect the algorithm to converge. Unfortunately, in comparisong to gradient descent, stochastic gradient is quite sensitive to the selection of a good <b>learning</b> rate. There is one more practical issues related to the use of SGD as a <b>learning</b> algorithm: do you really select a random point (or subset of random points) at each step, or do you stream through the data in order. The answer is akin to the answer of the same question for the perceptron algorithm ...", "dateLastCrawled": "2021-09-20T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "- <b>A Course in Machine Learning</b> - Studylib", "url": "https://studylib.net/doc/8792694/--a-course-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://studylib.net/doc/8792694/--<b>a-course-in-machine-learning</b>", "snippet": "Free essays, homework help, flashcards, research papers, book reports, term papers, history, science, politics", "dateLastCrawled": "2021-12-27T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Ciml <b>v0 - 8 All Machine Learning</b> | <b>Machine Learning</b> | Prediction", "url": "https://www.scribd.com/document/172987143/Ciml-v0-8-All-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/172987143/Ciml-<b>v0-8-All-Machine-Learning</b>", "snippet": "The <b>machine learning</b> algorithm has succeeded if its performance on the test data is high. 1.2 Some Canonical <b>Learning</b> Problems. There are a large number of typical inductive <b>learning</b> problems. The primary difference between them is in what type of thing theyre trying to predict. Here are some examples: Regression: trying to predict a real value. For instance, predict the value of a stock tomorrow given its past performance. Or predict Alices score on the <b>machine learning</b> nal exam based on ...", "dateLastCrawled": "2022-01-19T05:02:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(hinge loss)  is like +(difference between the true values and the estimated values)", "+(hinge loss) is similar to +(difference between the true values and the estimated values)", "+(hinge loss) can be thought of as +(difference between the true values and the estimated values)", "+(hinge loss) can be compared to +(difference between the true values and the estimated values)", "machine learning +(hinge loss AND analogy)", "machine learning +(\"hinge loss is like\")", "machine learning +(\"hinge loss is similar\")", "machine learning +(\"just as hinge loss\")", "machine learning +(\"hinge loss can be thought of as\")", "machine learning +(\"hinge loss can be compared to\")"]}