{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Discontinuous Grammar as <b>a Foreign</b> <b>Language</b>", "url": "https://deepai.org/publication/discontinuous-grammar-as-a-foreign-language", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/discontinuous-grammar-as-<b>a-foreign</b>-<b>language</b>", "snippet": "10/20/21 - In order to achieve deep natural <b>language</b> understanding, syntactic constituent parsing is a vital step, highly demanded by many ar...", "dateLastCrawled": "2022-01-11T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "15 <b>Other Sequence-to-sequence Applications</b>", "url": "http://www.phontron.com/class/mtandseq2seq2017/mt-spring2017.chapter15.pdf", "isFamilyFriendly": true, "displayUrl": "www.phontron.com/class/mtandseq2seq2017/mt-spring2017.chapter15.pdf", "snippet": "15 <b>Other Sequence-to-sequence Applications</b> Up until now, we have largely used machine translation as an example of <b>sequence-to-sequence</b> <b>learning</b> tasks. However, as mentioned at the beginning of the course, <b>sequence-to-sequence</b> models are quite general, and can be used for a large number of tasks. This chapter provides", "dateLastCrawled": "2021-12-15T18:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequence-To-Sequence Speech Recognition</b> | Papers With Code", "url": "https://paperswithcode.com/task/sequence-to-sequence-speech-recognition", "isFamilyFriendly": true, "displayUrl": "https://paperswithcode.com/<b>task</b>/<b>sequence-to-sequence-speech-recognition</b>", "snippet": "Lingvo is a Tensorflow framework offering a complete solution for collaborative deep <b>learning</b> research, with a particular focus towards <b>sequence-to-sequence</b> models. 3. Paper Code <b>Sequence-to-Sequence</b> Models Can Directly Translate <b>Foreign</b> Speech. colaprograms/speechify \u2022 \u2022 24 Mar 2017. We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in one <b>language</b> into text in another. 1. Paper Code Syllable-Based <b>Sequence-to-Sequence</b> Speech ...", "dateLastCrawled": "2022-01-23T19:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Sequence to Sequence</b> <b>Learning</b> for Event Prediction", "url": "https://www.researchgate.net/publication/319895741_Sequence_to_Sequence_Learning_for_Event_Prediction", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319895741_<b>Sequence_to_Sequence</b>_<b>Learning</b>_for...", "snippet": "This paper presents an approach to the <b>task</b> of predicting an event description from a preceding sentence in a text. Our approach explores <b>sequence-to-sequence</b> <b>learning</b> using a bidirectional multi ...", "dateLastCrawled": "2021-12-15T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sequence-to-Sequence Models Can</b> Zhifeng Chen Directly Translate <b>Foreign</b> ...", "url": "https://ronw.net/pubs/interspeech2017-speech_translation-slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://ronw.net/pubs/interspeech2017-speech_translation-slides.pdf", "snippet": "<b>Sequence-to-Sequence Models Can</b> <b>Directly Translate Foreign Speech</b> -- Interspeech 2017 References [B\u00e9rard et al, 2016] A. B\u00e9rard, O. Pietquin, C. Servan, and L. Besacier, \u201cListen and translate: A proof of concept for end-to-end speech-to-text translation,\u201d NIPS Workshop on End-to-end <b>Learning</b> for Speech and Audio Processing, 2016.", "dateLastCrawled": "2021-11-02T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "BART: Denoising <b>Sequence-to-Sequence</b> Pre-training for Natural <b>Language</b> ...", "url": "https://www.arxiv-vanity.com/papers/1910.13461/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1910.13461", "snippet": "We present BART, a denoising autoencoder for pretraining <b>sequence-to-sequence</b> models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) <b>learning</b> a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a ...", "dateLastCrawled": "2022-01-28T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Multi-task Sequence to Sequence Learning</b> - ResearchGate", "url": "https://www.researchgate.net/publication/284218905_Multi-task_Sequence_to_Sequence_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../284218905_<b>Multi-task_Sequence_to_Sequence_Learning</b>", "snippet": "<b>Sequence to sequence learning</b> has recently emerged as a new paradigm in supervised <b>learning</b>. To date, most of its applications focused on only one <b>task</b> and not much work explored this framework ...", "dateLastCrawled": "2022-02-03T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "LSTMs + Grammar as <b>a Foreign</b> <b>Language</b> - Anil Keshwani / paeninsula", "url": "https://anilkeshwani.github.io/LSTM-grammar-foreign-language/", "isFamilyFriendly": true, "displayUrl": "https://anilkeshwani.github.io/LSTM-grammar-<b>foreign</b>-<b>language</b>", "snippet": "LSTMs + Grammar as <b>a Foreign</b> <b>Language</b>. 9 minute read. Published: September 06, 2021 In their paper Grammar as <b>a Foreign</b> <b>Language</b> from 2015, Oriol Vinyals and colleagues 1 use a <b>sequence-to-sequence</b> model with attention to accomplish syntactic constituency parsing, a problem in computational linguistics (or natural <b>language</b> processing, NLP) wherein for a given input text, we would <b>like</b> to return the grammatical structure of that text as a syntax tree or similar.. The approach they used is ...", "dateLastCrawled": "2022-01-12T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Introduction to Machine Translation - <b>Sequence to sequence</b> tasks | Coursera", "url": "https://pt.coursera.org/lecture/language-processing/introduction-to-machine-translation-nv7Cr", "isFamilyFriendly": true, "displayUrl": "https://pt.coursera.org/lecture/<b>language</b>-processing/introduction-to-machine...", "snippet": "Now, you can think about some other examples. For example, summarization is also a <b>sequence to sequence</b> <b>task</b> and you can think about it as machine translation but for one <b>language</b>, monolingual machine translation. Well we will cover these examples in the end of the week but now let us start with statistical machine translation, and neural machine translation. We will see that actually there are some techniques, that are super similar in both these approaches. For example, we will see ...", "dateLastCrawled": "2022-01-27T06:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "AI Quantification of <b>Language</b> Puzzle to <b>Language</b> <b>Learning</b> ...", "url": "http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/CS298_Report.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/CS298_Report.pdf", "snippet": "Besides my advisor, I would <b>like</b> to thank the rest of my project committee: Dr. Katerina Potika and Prof. Kevin Smith, for their encouragement, guidance and valuable feedback. Finally, I thank my wonderful family for their countless hours of support and encouragement along the way. 5 ABSTRACT Online <b>language</b> <b>learning</b> applications provide users multiple ways/games to learn a new <b>language</b>. Some of the ways include rearranging words in the <b>foreign</b> <b>language</b> sentences, filling in the blanks ...", "dateLastCrawled": "2021-08-05T15:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Multi-<b>task</b> <b>Sequence to Sequence Learning</b> | DeepAI", "url": "https://deepai.org/publication/multi-task-sequence-to-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/multi-<b>task</b>-<b>sequence-to-sequence-learning</b>", "snippet": "Despite the popularity of multi-<b>task</b> <b>learning</b> and <b>sequence to sequence learning</b>, there has been little work in combining MTL with seq2seq <b>learning</b>. To the best of our knowledge, there is only one recent publication by Dong et al. which applies a seq2seq models for machine translation, where the goal is to translate from one <b>language</b> to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach \u2013 for tasks that can have an ...", "dateLastCrawled": "2022-02-02T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Sequence to Sequence</b> <b>Learning</b> for Event Prediction", "url": "https://www.researchgate.net/publication/319895741_Sequence_to_Sequence_Learning_for_Event_Prediction", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319895741_<b>Sequence_to_Sequence</b>_<b>Learning</b>_for...", "snippet": "This paper presents an approach to the <b>task</b> of predicting an event description from a preceding sentence in a text. Our approach explores <b>sequence-to-sequence</b> <b>learning</b> using a bidirectional multi ...", "dateLastCrawled": "2021-12-15T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequence-to-Sequence</b> Models Can Directly Translate <b>Foreign</b> Speech ...", "url": "https://ui.adsabs.harvard.edu/abs/2017arXiv170308581W/abstract", "isFamilyFriendly": true, "displayUrl": "https://ui.adsabs.harvard.edu/abs/2017arXiv170308581W/abstract", "snippet": "<b>Sequence-to-Sequence</b> Models Can Directly Translate <b>Foreign</b> Speech. We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in one <b>language</b> into text in another. The model does not explicitly transcribe the speech into text in the source <b>language</b>, nor does it require supervision from the ground ...", "dateLastCrawled": "2020-11-14T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Deep Memory-based Architecture for <b>Sequence-to-Sequence</b> <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/a-deep-memory-based-architecture-for-sequence-to-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../a-deep-memory-based-architecture-for-<b>sequence-to-sequence</b>-<b>learning</b>", "snippet": "<b>Sequence-to-sequence</b> <b>learning</b> is a fundamental problem in natural <b>language</b> processing, with many important applications such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), part-of-speech tagging (Collobert et al., 2011; Vinyals et al., 2014) and dependency parsing (Chen &amp; Manning, 2014)Recently, there has been significant progress in development of technologies for the <b>task</b> using purely neural network-based models.", "dateLastCrawled": "2021-12-12T22:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Multi-task Sequence to Sequence Learning</b> - ResearchGate", "url": "https://www.researchgate.net/publication/284218905_Multi-task_Sequence_to_Sequence_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../284218905_<b>Multi-task_Sequence_to_Sequence_Learning</b>", "snippet": "<b>Sequence to sequence learning</b> has recently emerged as a new paradigm in supervised <b>learning</b>. To date, most of its applications focused on only one <b>task</b> and not much work explored this framework ...", "dateLastCrawled": "2022-02-03T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Building Music Playlists Recommendation System</b> | by Piyush Papreja ...", "url": "https://towardsdatascience.com/building-music-playlists-recommendation-system-564a3e63ef64", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>building-music-playlists-recommendation-system</b>-564a3e63ef64", "snippet": "<b>Sequence-to-Sequence</b> <b>Learning</b>. The name <b>sequence-to-sequence</b> <b>learning</b> in its very core implies that the network is trained to take in sequences and output sequences. So instead of predicting single word, the network outputs the entire sentence, which could be a translated in <b>a foreign</b> <b>language</b>, or the next predicted sentence from the corpus, or even the same sentence if the network is trained like an autoencoder.", "dateLastCrawled": "2022-01-29T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "LSTMs + Grammar as <b>a Foreign</b> <b>Language</b> - Anil Keshwani / paeninsula", "url": "https://anilkeshwani.github.io/LSTM-grammar-foreign-language/", "isFamilyFriendly": true, "displayUrl": "https://anilkeshwani.github.io/LSTM-grammar-<b>foreign</b>-<b>language</b>", "snippet": "LSTMs + Grammar as <b>a Foreign</b> <b>Language</b>. 9 minute read. Published: September 06, 2021 In their paper Grammar as <b>a Foreign</b> <b>Language</b> from 2015, Oriol Vinyals and colleagues 1 use a <b>sequence-to-sequence</b> model with attention to accomplish syntactic constituency parsing, a problem in computational linguistics (or natural <b>language</b> processing, NLP) wherein for a given input text, we would like to return the grammatical structure of that text as a syntax tree or <b>similar</b>.. The approach they used is ...", "dateLastCrawled": "2022-01-12T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "2016 MultiTaskSequencetoSequenceLear - GM-RKB", "url": "https://www.gabormelli.com/RKB/2016_MultiTaskSequencetoSequenceLear", "isFamilyFriendly": true, "displayUrl": "https://www.gabormelli.com/RKB/2016_Multi<b>TaskSequencetoSequence</b>Lear", "snippet": "This paper examines three multi-<b>task</b> <b>learning</b> (MTL) settings for <b>sequence to sequence</b> models: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and ...", "dateLastCrawled": "2021-07-06T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "AI Quantification of <b>Language</b> Puzzle to <b>Language</b> <b>Learning</b> ...", "url": "http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "snippet": "<b>Learning</b> <b>a foreign</b> <b>language</b> is always a challenging <b>task</b>. The spread of the internet not only connected different parts of the world but also provided some powerful tools. Online <b>language</b> <b>learning</b> is one such tool people find appealing nowadays. There are many tools available on the internet which can help a user learn a new <b>language</b> at their own pace. According to [5], very little research has been done to study the effectiveness of these platforms. These platforms use a different set of ...", "dateLastCrawled": "2021-11-13T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Semantics as <b>a Foreign</b> <b>Language</b>", "url": "https://aclanthology.org/D18-1263.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/D18-1263.pdf", "snippet": "Semantics as <b>a Foreign</b> <b>Language</b> Gabriel Stanovsky 2,3 and Ido Dagan1 1Bar-Ilan University Computer Science Department, Ramat Gan, Israel 2Paul G. Allen School of Computer Science &amp; Engineering, University of Washington, Seattle, WA 3Allen Institute for Arti\ufb01cial Intelligence, Seattle, WA gabis@cs.washington.edu dagan@cs.biu.ac.il Abstract We propose a novel approach to semantic de-pendency parsing (SDP) by casting the <b>task</b> as an instance of multi-lingual machine transla-tion, where each ...", "dateLastCrawled": "2022-02-02T18:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Multi-<b>task</b> <b>Sequence to Sequence Learning</b> | DeepAI", "url": "https://deepai.org/publication/multi-task-sequence-to-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/multi-<b>task</b>-<b>sequence-to-sequence-learning</b>", "snippet": "Despite the popularity of multi-<b>task</b> <b>learning</b> and <b>sequence to sequence learning</b>, there has been little work in combining MTL with seq2seq <b>learning</b>. To the best of our knowledge, there is only one recent publication by Dong et al. which applies a seq2seq models for machine translation, where the goal is to translate from one <b>language</b> to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach \u2013 for tasks that <b>can</b> have an ...", "dateLastCrawled": "2022-02-02T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Sequence to Sequence</b> <b>Learning</b> for Event Prediction", "url": "https://www.researchgate.net/publication/319895741_Sequence_to_Sequence_Learning_for_Event_Prediction", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319895741_<b>Sequence_to_Sequence</b>_<b>Learning</b>_for...", "snippet": "This paper presents an approach to the <b>task</b> of predicting an event description from a preceding sentence in a text. Our approach explores <b>sequence-to-sequence</b> <b>learning</b> using a bidirectional multi ...", "dateLastCrawled": "2021-12-15T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Multi-task Sequence to Sequence Learning</b>", "url": "https://www.researchgate.net/publication/284218905_Multi-task_Sequence_to_Sequence_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../284218905_<b>Multi-task_Sequence_to_Sequence_Learning</b>", "snippet": "<b>Sequence to sequence learning</b> has recently emerged as a new paradigm in supervised <b>learning</b>. To date, most of its applications focused on only one <b>task</b> and not much work explored this framework ...", "dateLastCrawled": "2022-02-03T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Review of the Neural <b>History of Natural Language Processing</b> - AYLIEN ...", "url": "https://aylien.com/blog/a-review-of-the-recent-history-of-natural-language-processing", "isFamilyFriendly": true, "displayUrl": "https://aylien.com/blog/a-review-of-the-recent-<b>history-of-natural-language-processing</b>", "snippet": "<b>Sequence-to-sequence</b> <b>learning</b> <b>can</b> even be applied to structured prediction tasks common in NLP where the output has a particular structure. For simplicity, the output is linearized as <b>can</b> be seen for constituency parsing in Figure 10 below. Neural networks have demonstrated the ability to directly learn to produce such a linearized output given sufficient amount of training data for constituency parsing (Vinyals et al, 2015), and named entity recognition (Gillick et al., 2016), among others.", "dateLastCrawled": "2022-01-30T00:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Direct speech-to-speech translation with a <b>sequence-to-sequence</b> model ...", "url": "https://deepai.org/publication/direct-speech-to-speech-translation-with-a-sequence-to-sequence-model", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/direct-speech-to-speech-translation-with-a-sequence-to...", "snippet": "We present an attention-based <b>sequence-to-sequence</b> neural network which <b>can</b> directly translate speech from one <b>language</b> into speech in another <b>language</b>, without relying on an intermediate text representation. The network is trained end-to-end, <b>learning</b> to map speech spectrograms into target spectrograms in another <b>language</b>, corresponding to the translated content (in a different canonical voice).", "dateLastCrawled": "2021-12-17T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "2016 MultiTaskSequencetoSequenceLear - GM-RKB", "url": "https://www.gabormelli.com/RKB/2016_MultiTaskSequencetoSequenceLear", "isFamilyFriendly": true, "displayUrl": "https://www.gabormelli.com/RKB/2016_Multi<b>TaskSequencetoSequence</b>Lear", "snippet": "This paper examines three multi-<b>task</b> <b>learning</b> (MTL) settings for <b>sequence to sequence</b> models: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder <b>can</b> be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and ...", "dateLastCrawled": "2021-07-06T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep <b>Learning</b>, Structure and Innate Priors | SAIL Blog", "url": "https://ai.stanford.edu/blog/deep-learning-structure-and-innate-priors/", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/blog/deep-<b>learning</b>-structure-and-innate-priors", "snippet": "At 19:17, Manning discusses the paper Grammar as <b>a Foreign</b> <b>Language</b>, which tackled a highly recursive linguistic <b>task</b> (parsing) with a surprisingly unstructured method (<b>sequence-to-sequence</b>). The question at 39:15 refers to the idea that Stochastic Gradient Descent acts as a kind of implicit regularization. To read more about this idea, see for ...", "dateLastCrawled": "2021-12-29T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "\ud83e\udd84\ud83e\udd1d\ud83e\udd84 <b>Encoder-decoders in Transformers: a</b> hybrid pre-trained architecture ...", "url": "https://medium.com/huggingface/encoder-decoders-in-transformers-a-hybrid-pre-trained-architecture-for-seq2seq-af4d7bf14bb8", "isFamilyFriendly": true, "displayUrl": "https://medium.com/huggingface/<b>encoder-decoders-in-transformers-a</b>-hybrid-pre-trained...", "snippet": "To answer the first question, I would say that there is one thing that might be much easier to do with encoder-decoders: transfer <b>learning</b> on every <b>task</b> that <b>can</b> be mapped to a translation <b>task</b> ...", "dateLastCrawled": "2022-01-27T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An Overview of Multi-<b>Task</b> <b>Learning</b> in <b>Speech Recognition</b>", "url": "http://jrmeyer.github.io/asr/2020/03/21/overview-mtl-in-asr.html", "isFamilyFriendly": true, "displayUrl": "jrmeyer.github.io/asr/2020/03/21/overview-mtl-in-asr.html", "snippet": "The bottleneck features themselves are the product of Multi-<b>Task</b> <b>Learning</b>. Source <b>Language</b> Model as Teacher. Instead of using data from multiple languages, it is possible to use the predictions from a pre-trained source model as targets in an auxiliary <b>task</b>. In this way, knowledge located in the source dataset is transferred indirectly via a source model, as opposed to directly from the dataset itself. In a very recent approach, He (2018) 39 trained a classifier on a well-resourced <b>language</b> ...", "dateLastCrawled": "2022-01-29T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Technological disruption in <b>foreign</b> <b>language</b> teaching: The rise of ...", "url": "https://www.cambridge.org/core/journals/language-teaching/article/technological-disruption-in-foreign-language-teaching-the-rise-of-simultaneous-machine-translation/A31BA5AB690B370B01535EF2D1AFAE42", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>language</b>-teaching/article/technological...", "snippet": "With large enough corpora, the machine <b>learning</b> algorithms <b>can</b> distinguish patterns between the two corpora and develop rules to produce one <b>language</b> given input from another <b>language</b>. More importantly, because simultaneous translation relies on deep <b>learning</b> algorithms that allow for feedback within the system, mistranslations <b>can</b> be highlighted and that data <b>can</b> be flagged for future improvement. Feedback to the system <b>can</b> also be shared across large numbers of users and other potential ...", "dateLastCrawled": "2021-08-08T08:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Multi-<b>task</b> <b>Sequence to Sequence Learning</b> | DeepAI", "url": "https://deepai.org/publication/multi-task-sequence-to-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/multi-<b>task</b>-<b>sequence-to-sequence-learning</b>", "snippet": "Despite the popularity of multi-<b>task</b> <b>learning</b> and <b>sequence to sequence learning</b>, there has been little work in combining MTL with seq2seq <b>learning</b>. To the best of our knowledge, there is only one recent publication by Dong et al. which applies a seq2seq models for machine translation, where the goal is to translate from one <b>language</b> to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach \u2013 for tasks that <b>can</b> have an ...", "dateLastCrawled": "2022-02-02T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequence-to-Sequence</b> Models <b>Can</b> Directly Transcribe <b>Foreign</b> Speech ...", "url": "https://www.researchgate.net/publication/315667124_Sequence-to-Sequence_Models_Can_Directly_Transcribe_Foreign_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315667124_<b>Sequence-to-Sequence</b>_Models_<b>Can</b>...", "snippet": "We apply a slightly modified <b>sequence-to-sequence</b> with attention architecture that has previously been used for speech recognition and show that it <b>can</b> be repurposed for this more complex <b>task</b> ...", "dateLastCrawled": "2022-01-18T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "2016 MultiTaskSequencetoSequenceLear - GM-RKB", "url": "https://www.gabormelli.com/RKB/2016_MultiTaskSequencetoSequenceLear", "isFamilyFriendly": true, "displayUrl": "https://www.gabormelli.com/RKB/2016_Multi<b>TaskSequencetoSequence</b>Lear", "snippet": "This paper examines three multi-<b>task</b> <b>learning</b> (MTL) settings for <b>sequence to sequence</b> models: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder <b>can</b> be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and ...", "dateLastCrawled": "2021-07-06T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "LSTMs + Grammar as <b>a Foreign</b> <b>Language</b> - Anil Keshwani / paeninsula", "url": "https://anilkeshwani.github.io/LSTM-grammar-foreign-language/", "isFamilyFriendly": true, "displayUrl": "https://anilkeshwani.github.io/LSTM-grammar-<b>foreign</b>-<b>language</b>", "snippet": "LSTMs + Grammar as <b>a Foreign</b> <b>Language</b>. 9 minute read. Published: September 06, 2021 In their paper Grammar as <b>a Foreign</b> <b>Language</b> from 2015, Oriol Vinyals and colleagues 1 use a <b>sequence-to-sequence</b> model with attention to accomplish syntactic constituency parsing, a problem in computational linguistics (or natural <b>language</b> processing, NLP) wherein for a given input text, we would like to return the grammatical structure of that text as a syntax tree or similar.. The approach they used is ...", "dateLastCrawled": "2022-01-12T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Spelling Correction as a Foreign Language</b> | DeepAI", "url": "https://deepai.org/publication/spelling-correction-as-a-foreign-language", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>spelling-correction-as-a-foreign-language</b>", "snippet": "3 <b>Spelling Correction as a Foreign Language</b>. It is easy to see that spelling correction problem <b>can</b> be formulated as a <b>sequence to sequence</b> <b>learning</b> problem as mentioned in section 2. In this sense, it is very similar to a machine translation problem, where the input is the misspelled text and the output is the corresponding correct spellings.", "dateLastCrawled": "2021-11-30T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Building Music Playlists Recommendation System</b> | by Piyush Papreja ...", "url": "https://towardsdatascience.com/building-music-playlists-recommendation-system-564a3e63ef64", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>building-music-playlists-recommendation-system</b>-564a3e63ef64", "snippet": "The name <b>sequence-to-sequence</b> <b>learning</b> in its very core implies that the network is trained to take in sequences and output sequences. So instead of predicting single word, the network outputs the entire sentence, which could be a translated in <b>a foreign</b> <b>language</b>, or the next predicted sentence from the corpus, or even the same sentence if the network is trained like an autoencoder. For this work, we use seq2seq framework as an autoencoder where the <b>task</b> of the network is to reconstruct the ...", "dateLastCrawled": "2022-01-29T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Universal Lemmatizer: A <b>sequence-to-sequence</b> model for lemmatizing ...", "url": "https://www.cambridge.org/core/journals/natural-language-engineering/article/universal-lemmatizer-a-sequencetosequence-model-for-lemmatizing-universal-dependencies-treebanks/9341ECA9B562DAF55E2F3F966554A667", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/natural-<b>language</b>-engineering/article/universal...", "snippet": "Taking inspiration from the top systems in the CoNLL-SIGMORPHON 2017 Shared <b>Task</b>, we cast lemmatization as a <b>sequence-to-sequence</b> rewrite problem where lemma characters are generated one at a time from the given sequence of word characters and morphosyntatic tags. We diverge from previous work on lemmatization by utilizing morphosyntactic features predicted by a tagger to represent the salient information from the context, instead of using, for example, contextualized word representations or ...", "dateLastCrawled": "2022-01-17T17:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "AI Quantification of <b>Language</b> Puzzle to <b>Language</b> <b>Learning</b> ...", "url": "http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "snippet": "<b>Learning</b> <b>a foreign</b> <b>language</b> is always a challenging <b>task</b>. The spread of the internet not only connected different parts of the world but also provided some powerful tools. Online <b>language</b> <b>learning</b> is one such tool people find appealing nowadays. There are many tools available on the internet which <b>can</b> help a user learn a new <b>language</b> at their own pace. According to [5], very little research has been done to study the effectiveness of these platforms. These platforms use a different set of ...", "dateLastCrawled": "2021-11-13T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "AI Quantification of <b>Language</b> Puzzle to <b>Language</b> <b>Learning</b> ... - CORE", "url": "https://core.ac.uk/download/pdf/323868806.pdf", "isFamilyFriendly": true, "displayUrl": "https://core.ac.uk/download/pdf/323868806.pdf", "snippet": "Online <b>language</b> <b>learning</b> applications provide users multiple ways/games to learn a new <b>language</b>. Some of the ways include rearranging words in the <b>foreign</b> <b>language</b> sentences, filling in the blanks, providing flashcards, and many more. Primarily this research focused on quantifying the effectiveness of these games in <b>learning</b> a new <b>language</b> ...", "dateLastCrawled": "2020-11-27T05:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Language</b> <b>Translation</b> with RNNs. Build a recurrent neural network (RNN ...", "url": "https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>language</b>-<b>translation</b>-with-rnns-d84d43b40571", "snippet": "This investment and recent advancements in deep <b>learning</b> have yielded major improvements in <b>translation</b> quality. According to Google, switching to deep <b>learning</b> produced a 60% increase in <b>translation</b> accuracy <b>compared</b> to the phrase-based approach previously used in Google Translate. Today, Google and Microsoft <b>can</b> translate over 100 different ...", "dateLastCrawled": "2022-02-02T17:05:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Read the model framework <b>Encoder-Decoder and Seq2Seq</b> in NLP - easyAI", "url": "https://easyai.tech/en/ai-definition/encoder-decoder-seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://easyai.tech/en/ai-definition/encoder-decoder-seq2seq", "snippet": "Encoder-Decoder This framework is a good illustration of the core ideas of <b>machine</b> <b>learning</b>: ... Seq2Seq (short for <b>Sequence-to-sequence</b>), as literally, enters a sequence and outputs another sequence. The most important aspect of this structure is that the length of the input sequence and the output sequence are variable. For example, the following picture: As shown above: 6 Chinese characters are input, and 3 English words are output. The length of the input and output are different. The ...", "dateLastCrawled": "2022-01-31T17:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Week 3: <b>Sequence to sequence</b> architectures. <b>Sequence to sequence</b> models Language translation for example; Image captioning, caption an image; Picking the most likely model <b>Machine</b> Transation Model Split into a model encoding the sentence; and then a language model. Calculate the probability of an English sentence conditioned on a French sentence.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Week 1 Quiz - Introduction to deep <b>learning</b>. What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI is powering personal devices in our homes and offices, similar to electricity. Through the \u201csmart grid\u201d, AI is delivering a new wave of electricity. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Note: Andrew ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Andrew-NG-Notes/andrewng-p-5-sequence-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence...", "snippet": "<b>Machine</b> translation (<b>sequence to sequence</b>): X: text sequence (in one language) Y: text sequence (in other language) Video activity recognition (sequence to one): X: video frames ; Y: label (activity) Name entity recognition (<b>sequence to sequence</b>): X: text sequence; Y: label sequence; Can be used by seach engines to index different type of words inside a text. All of these problems with different input and output (sequence or not) can be addressed as supervised <b>learning</b> with label data X, Y ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(learning a foreign language)", "+(sequence-to-sequence task) is similar to +(learning a foreign language)", "+(sequence-to-sequence task) can be thought of as +(learning a foreign language)", "+(sequence-to-sequence task) can be compared to +(learning a foreign language)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}