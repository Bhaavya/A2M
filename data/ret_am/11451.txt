{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>BLEU</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/BLEU", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>BLEU</b>", "snippet": "<b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is an algorithm for evaluating the quality of text which has been machine-translated from one natural <b>language</b> to another. Quality is considered to be the correspondence between a machine&#39;s output and that of a human: &quot;the closer a machine translation is to a professional human translation, the better it is&quot; \u2013 this is the central idea behind <b>BLEU</b>. <b>BLEU</b> was one of the first metrics to claim a high correlation with human judgements of quality, and ...", "dateLastCrawled": "2022-02-02T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>BLEU</b>: a Method for Automatic <b>Evaluation</b> of Machine Translation", "url": "https://aclanthology.org/P02-1040.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/P02-1040.pdf", "snippet": "1So we call our method the <b>bilingual</b> <b>evaluation</b> <b>understudy</b>, <b>BLEU</b>. the <b>evaluation</b> bottleneck. Developers would bene-\ufb01t from an inexpensive automatic <b>evaluation</b> that is quick, <b>language</b>-independent, and correlates highly with human <b>evaluation</b>. We propose such an <b>evalua-tion</b> method in this paper. 1.2 Viewpoint How does one measure translation performance? The closer a machine translation is to a professional human translation, the better it is. This is the cen-tral idea behind our proposal. To ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching algorithm that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to Calculating the <b>BLEU</b> Score for Text in Python", "url": "https://machinelearningmastery.com/calculate-bleu-score-for-text-python/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/calculate-<b>bleu</b>-score-for-text-pyth", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for comparing a candidate translation of text to one or more reference translations. Although developed for translation, it can be used to evaluate text generated for a suite of natural <b>language</b> processing tasks. In this tutorial, you will discover the <b>BLEU</b> score for evaluating and scoring candidate text using the NLTK library in", "dateLastCrawled": "2022-01-30T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bilingual Evaluation Understudy</b> (<b>BLEU</b>) - <b>Lei Mao</b>&#39;s Log Book", "url": "https://leimao.github.io/blog/BLEU-Score/", "isFamilyFriendly": true, "displayUrl": "https://<b>leimao</b>.github.io/blog/<b>BLEU</b>-Score", "snippet": "<b>Bilingual Evaluation Understudy</b> (<b>BLEU</b>) ... Natural <b>Language</b> Processing, Machine Translation . <b>Like</b> this article? Support the author with. Paypal. Alipay Wechat Buy me a coffee Patreon. Correlation VS Causation. RSA Algorithm. Comments. <b>Lei Mao</b>. Artificial Intelligence Machine <b>Learning</b> Computer Science Santa Clara, California. Posts. 295. Categories. 6. Tags. 168 Follow. Catalogue. 1 Introduction; 2 English Translation Example. 2.1 Example 1; 2.2 Example 2; 3 Precision ...", "dateLastCrawled": "2022-02-02T07:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise machine translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching algorithm that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "This measure, looking at n-grams overlap between the output and reference translations with a penalty for shorter outputs, is known as <b>BLEU</b> (short for \u201c<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>\u201d which people literally only ever say when explaining the acronym) and was developed by Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu at IBM in 2002. It\u2019s a very popular metric in NLP, particularly for tasks where the output of a system is a text string rather than a classification. This ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Natural Language Processing Performance Metrics (Benchmarks</b>) - DEV ...", "url": "https://dev.to/amananandrai/natural-language-processing-performance-metrics-benchmarks-4jel", "isFamilyFriendly": true, "displayUrl": "https://dev.to/amananandrai/<b>natural-language-processing-performance-metrics-benchmarks</b>...", "snippet": "<b>BLEU</b> <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b> It is a performance metric to measure the performance of machine translation models. It evaluates how good a model translates from one <b>language</b> to another. It assigns a score for machine translation based on the unigrams, bigrams or trigrams present in the generated output and comparing it with the ground ...", "dateLastCrawled": "2022-02-02T16:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Metrics for NLG <b>evaluation</b>. Simple natural <b>language</b> processing\u2026 | by ...", "url": "https://medium.com/explorations-in-language-and-learning/metrics-for-nlg-evaluation-c89b6a781054", "isFamilyFriendly": true, "displayUrl": "https://medium.com/explorations-in-<b>language</b>-and-<b>learning</b>/<b>metric</b>s-for-nlg-<b>evaluation</b>-c...", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) This is by far the most popular <b>metric</b> for evaluating machine translation system. In <b>BLEU</b>, precision and recall are approximated by modified n-gram precision ...", "dateLastCrawled": "2022-02-03T08:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "Abstract and Figures. Our research extends the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical machine translation to make it more adjustable and robust. We intend to ...", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>BLEU</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/BLEU", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>BLEU</b>", "snippet": "<b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is an algorithm for evaluating the quality of text which has been machine-translated from one natural <b>language</b> to another. Quality is considered to be the correspondence between a machine&#39;s output and that of a human: &quot;the closer a machine translation is to a professional human translation, the better it is&quot; \u2013 this is the central idea behind <b>BLEU</b>. <b>BLEU</b> was one of the first metrics to claim a high correlation with human judgements of quality, and ...", "dateLastCrawled": "2022-02-02T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "BLUE Score - Machine <b>Learning</b> Interviews", "url": "https://machinelearninginterview.com/topics/natural-language-processing/bleu-score/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>interview.com/topics/natural-<b>language</b>-processing/<b>bleu</b>-score", "snippet": "<b>BLEU</b> stands for <b>Bilingual</b> <b>evaluation</b> <b>Understudy</b>. It is a metric used to evaluate the quality of machine generated text by comparing it with a reference text that is supposed to be generated. Usually, the reference text is generated by a manual evaluator or a translator. Where is <b>BLEU</b> Score used? <b>BLEU</b> score was traditionally used to evaluate machine translation. Hence the <b>Bilingual</b>. (Note: Take a look at our article on a brief history of machine translation, that quickly summarizes various ...", "dateLastCrawled": "2022-01-28T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bilingual Evaluation Understudy (BLEU) Performance Measure</b> - GM-RKB", "url": "https://www.gabormelli.com/RKB/Bilingual_Evaluation_Understudy_(BLEU)_Performance_Measure", "isFamilyFriendly": true, "displayUrl": "https://www.gabormelli.com/RKB/<b>Bilingual_Evaluation_Understudy_(BLEU)_Performance_Measure</b>", "snippet": "<b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is an algorithm for evaluating the quality of text which has been machine-translated from one natural <b>language</b> to another. Quality is considered to be the correspondence between a machine&#39;s output and that of a human: &quot;the closer a machine translation is to a professional human translation, the better it is&quot; \u2013 this is the central idea behind <b>BLEU</b>. <b>BLEU</b> was one of the first metrics to claim a high correlation with human judgements of quality, and ...", "dateLastCrawled": "2022-01-23T07:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to Calculating the <b>BLEU</b> Score for Text in Python", "url": "https://machinelearningmastery.com/calculate-bleu-score-for-text-python/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/calculate-<b>bleu</b>-score-for-text-pyth", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for comparing a candidate translation of text to one or more reference translations. Although developed for translation, it can be used to evaluate text generated for a suite of natural <b>language</b> processing tasks. In this tutorial, you will discover the <b>BLEU</b> score for evaluating and scoring candidate text using the NLTK library in", "dateLastCrawled": "2022-01-30T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching algorithm that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise machine translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching algorithm that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Machine Translation <b>Evaluation</b> with sacreBLEU and BERTScore | by Ng Wai ...", "url": "https://towardsdatascience.com/machine-translation-evaluation-with-sacrebleu-and-bertscore-d7fdb0c47eb3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/machine-translation-<b>evaluation</b>-with-sacre<b>bleu</b>-and-bert...", "snippet": "For your information, <b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is one of the most popular metric for evaluating machine-translated text. It can be used to evaluate translations of any <b>language</b> provided that there exists some form of word boundary in the text. <b>BLEU</b>\u2019s output is usually a score between 0 and 100, indicating the similarity value between the reference text and hypothesis text. The higher the value, the better the translations. Having said th a t, one of the major downside for ...", "dateLastCrawled": "2022-01-29T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "This measure, looking at n-grams overlap between the output and reference translations with a penalty for shorter outputs, is known as <b>BLEU</b> (short for \u201c<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>\u201d which people literally only ever say when explaining the acronym) and was developed by Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu at IBM in 2002. It\u2019s a very popular metric in NLP, particularly for tasks where the output of a system is a text string rather than a classification. This ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Confusion matrix of linear classifier with one <b>BLEU</b> score | Download ...", "url": "https://researchgate.net/figure/Confusion-matrix-of-linear-classifier-with-one-BLEU-score_tbl1_307587565", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Confusion-matrix-of-linear-classifier-with-one-<b>BLEU</b>...", "snippet": "We propose a novel classification method of recognized second <b>language</b> learners utterances into three classes of acceptability for dialogue-based computer assisted <b>language</b> <b>learning</b> (CALL) systems.", "dateLastCrawled": "2021-06-19T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>language</b> agnostic - How do I evaluate a text <b>summarization</b> tool ...", "url": "https://stackoverflow.com/questions/9879276/how-do-i-evaluate-a-text-summarization-tool", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/9879276", "snippet": "The traditional problems (or goals) of AI research include reasoning, knowledge, planning, <b>learning</b>, natural <b>language</b> processing, perception and the ability to move and manipulate objects. General intelligence is among the field&#39;s long-term goals. Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, neural networks and methods based on statistics, probability and ...", "dateLastCrawled": "2022-01-24T15:39:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bilingual Evaluation Understudy</b> (<b>BLEU</b>) - <b>Lei Mao</b>&#39;s Log Book", "url": "https://leimao.github.io/blog/BLEU-Score/", "isFamilyFriendly": true, "displayUrl": "https://<b>leimao</b>.github.io/blog/<b>BLEU</b>-Score", "snippet": "<b>Bilingual Evaluation Understudy</b> (<b>BLEU</b>) ... At first I <b>thought</b> it should be very straightforward to use. However, it turns out that there are a lot of caveats. In this blog post, I am going to show the <b>BLEU</b> algorithm in detail and talk about the caveats. English Translation Example. We will use the following examples to illustrate how to compute the <b>BLEU</b> scores. Example 1. Chinese: \u732b\u5750\u5728\u57ab\u5b50\u4e0a. Reference 1: the cat is on the mat. Reference 2: there is a cat on the mat. Candidate: the ...", "dateLastCrawled": "2022-02-02T07:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "<b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical machine translation to make it more adjustable and robust . We in tend to adapt it to resemble human <b>evaluatio n</b> mor e.", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Attention mechanism in Deep Learning, Explained</b>", "url": "https://www.theaidream.com/post/attention-mechanism-in-deep-learning-explained", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>attention-mechanism-in-deep-learning-explained</b>", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is a score for comparing a candidate translation of text to one or more reference translations. The above graph shows that the encoder-decoder unit fails to memorize the whole long sentence. Hence, what\u2019s reflected from the graph above is that the encoder-decoder unit works well for shorter sentences (high <b>bleu</b> score).", "dateLastCrawled": "2022-01-26T06:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Attention mechanism in Deep Learning, Explained</b> - KDnuggets", "url": "https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2021/01/attention-mechanism-deep-<b>learning</b>-explained.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is a score for comparing a candidate translation of text to one or more reference translations. The above graph shows that the encoder-decoder unit fails to memorize the whole long sentence. Hence, what\u2019s reflected from the graph above is that the encoder-decoder unit works well for shorter sentences (high <b>bleu</b> score). The basic idea behind Attention . Attention was presented by Dzmitry Bahdanau, et al. in their 2014 paper \u201cNeural Machine ...", "dateLastCrawled": "2022-01-30T12:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>GitHub</b> - <b>gcunhase/NLPMetrics</b>: Python code for various NLP metrics", "url": "https://github.com/gcunhase/NLPMetrics", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gcunhase/NLPMetrics", "snippet": "<b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>) Papineni 2002 &#39;Measures how many words overlap in a given translation when compared to a reference translation, giving higher scores to sequential words.&#39; (recall) Limitation: Doesn&#39;t consider different types of errors (insertions, substitutions, synonyms, paraphrase, stems) Designed to be a corpus measure, so it has undesirable properties when used for single sentences. GLEU (Google-<b>BLEU</b>) Wu et al. 2016; Minimum of <b>BLEU</b> recall and precision applied to ...", "dateLastCrawled": "2022-01-27T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Essential KPIs for SMT: F-Measure</b> \u2013 KantanMT \u2013 Machine <b>Learning</b> ...", "url": "https://kantanmtblog.com/2014/01/30/essential-kpis-for-smt-f-measure/", "isFamilyFriendly": true, "displayUrl": "https://kantanmtblog.com/2014/01/30/<b>essential-kpis-for-smt-f-measure</b>", "snippet": "In the next post I will look at <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) and examine how this metric helps us to further understand the quality of SMT engines. KantanMT\u2019s new BuildAnalytics technology illustrates the distribution of F-Measure, <b>BLEU</b>, and TER score across our members SMT engines. It also generates a Gap Analysis, highlighting ...", "dateLastCrawled": "2022-01-31T23:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Automatic Assessment of Open Ended</b> Questions with a <b>Bleu</b>-Inspired ...", "url": "https://www.researchgate.net/publication/221418794_Automatic_Assessment_of_Open_Ended_Questions_with_a_Bleu-Inspired_Algorithm_and_Shallow_NLP", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221418794_<b>Automatic_Assessment_of_Open_Ended</b>...", "snippet": "For the <b>evaluation</b> we use <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) and NIST (National Institute of Standards and Technology) scoring techniques. A detailed <b>evaluation</b> of these models is performed by ...", "dateLastCrawled": "2021-12-24T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What are <b>the benefits/disadvantages associated with word</b> error rate ...", "url": "https://www.quora.com/What-are-the-benefits-disadvantages-associated-with-word-error-rate-WER-compared-to-the-BLEU-bilingual-evaluation-understudy-metric", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>the-benefits-disadvantages-associated-with-word</b>-error...", "snippet": "Answer: WER tends to predicate a raw metric of how many updates must be performed - in terms of deletion, editing, etc. Where as of <b>BLEU</b> tends to measure the translation strength - in comparison to what would be judged as being \u201cfavored by people\u201d. However - I think both of them run into distin...", "dateLastCrawled": "2022-01-21T01:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Subjective</b> Answer <b>Evaluation</b> Using Machine <b>Learning</b>", "url": "https://www.acadpubl.eu/hub/2018-118-24/3/577.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.acadpubl.eu/hub/2018-118-24/3/577.pdf", "snippet": "a human-grader. One major natural <b>language</b> processing technique we ought to look is <b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>)[19-20] is a basically an algorithm for <b>evaluation</b> of the text quality which has been translated with the help of a machine from one <b>language</b> to another. Though it is built to mimick human evaluations at a", "dateLastCrawled": "2022-01-30T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Neural <b>Machine Translation</b>: Inner Workings, Seq2Seq, and Transformers ...", "url": "https://towardsdatascience.com/neural-machine-translation-inner-workings-seq2seq-and-transformers-229faff5895b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-<b>machine-translation</b>-inner-workings-seq2seq-and...", "snippet": "Alternatively, people mostly use the <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score. It produces a score between [0,1], in some cases, we multiply it with 100 for convenience. It uses n-grams (1, 2, 3, and 4 mostly) to evaluate the translation with some additional internal tricks. But this <b>evaluation</b> mechanism might slightly fail in this example ...", "dateLastCrawled": "2022-01-29T08:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching algorithm that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise machine translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching algorithm that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "This measure, looking at n-grams overlap between the output and reference translations with a penalty for shorter outputs, is known as <b>BLEU</b> (short for \u201c<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>\u201d which people literally only ever say when explaining the acronym) and was developed by Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu at IBM in 2002. It\u2019s a very popular metric in NLP, particularly for tasks where the output of a system is a text string rather than a classification. This ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bilingual Evaluation Understudy</b> (<b>BLEU</b>) - <b>Lei Mao</b>&#39;s Log Book", "url": "https://leimao.github.io/blog/BLEU-Score/", "isFamilyFriendly": true, "displayUrl": "https://<b>leimao</b>.github.io/blog/<b>BLEU</b>-Score", "snippet": "BP = 1 BP = 1. When we plugin these values to the <b>BLEU</b> equation, actually we would need to compute log 0 log \u2061 0 which is not mathematically defined. We use a small number 10 \u2212 100 10 \u2212 100 instead of 0 0 for p 2 p 2, p 3 p 3 and p 4 p 4. The <b>BLEU</b> is.", "dateLastCrawled": "2022-02-02T07:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Essential KPIs For Your SMT Engines \u2013 KantanMT \u2013 Machine <b>Learning</b> ...", "url": "https://kantanmtblog.com/2014/01/28/essential-kpis-for-your-smt-engines/", "isFamilyFriendly": true, "displayUrl": "https://kantanmtblog.com/2014/01/28/essential-kpis-for-your-smt-engines", "snippet": "<b>BLEU</b>. <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is a KPI that measures the fluency of the translated output of an SMT engine, which means it measures how many words overlap in a given translation when <b>compared</b> to a reference translation. Higher scores are given to segments which contain a greater number of sequential words. <b>BLEU</b> is a major improvement on F-Measure as it takes word-order in account! A high <b>BLEU</b> KPI means that an SMT engine is producing highly fluent translations; a low score ...", "dateLastCrawled": "2022-01-20T17:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Machine Translation <b>Evaluation</b> with sacreBLEU and BERTScore | by Ng Wai ...", "url": "https://towardsdatascience.com/machine-translation-evaluation-with-sacrebleu-and-bertscore-d7fdb0c47eb3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/machine-translation-<b>evaluation</b>-with-sacre<b>bleu</b>-and-bert...", "snippet": "For your information, <b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is one of the most popular metric for evaluating machine-translated text. It <b>can</b> be used to evaluate translations of any <b>language</b> provided that there exists some form of word boundary in the text. <b>BLEU</b>\u2019s output is usually a score between 0 and 100, indicating the similarity value between the reference text and hypothesis text. The higher the value, the better the translations. Having said th a t, one of the major downside for ...", "dateLastCrawled": "2022-01-29T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Google Introduces BLEURT \u2014 a BERT-Based NLG <b>Evaluation</b> Metric | by ...", "url": "https://synced.medium.com/google-introduces-bleurt-a-bert-based-nlg-evaluation-metric-a441debdfae6", "isFamilyFriendly": true, "displayUrl": "https://synced.medium.com/google-introduces-<b>bleu</b>rt-a-bert-based-nlg-<b>evaluation</b>-metric...", "snippet": "Another available <b>evaluation</b> method is automatic metrics such as the popular <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score, but these are oftentimes unreliable when <b>compared</b> to human <b>evaluation</b>. The challenge is to develop a novel and automatic metric that <b>can</b> be robust and reliable enough to match the <b>evaluation</b> quality that human annotation <b>can</b> deliver. In the recently published paper BLEURT: <b>Learning</b> Robust Metrics for Text Generation, a Google Research team proposes the automatic metric ...", "dateLastCrawled": "2022-01-09T08:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Confusion matrix of linear classifier with one <b>BLEU</b> score | Download ...", "url": "https://researchgate.net/figure/Confusion-matrix-of-linear-classifier-with-one-BLEU-score_tbl1_307587565", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Confusion-matrix-of-linear-classifier-with-one-<b>BLEU</b>...", "snippet": "We propose a novel classification method of recognized second <b>language</b> learners utterances into three classes of acceptability for dialogue-based computer assisted <b>language</b> <b>learning</b> (CALL) systems.", "dateLastCrawled": "2021-06-19T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>gcunhase/NLPMetrics</b>: Python code for various NLP metrics - <b>GitHub</b>", "url": "https://github.com/gcunhase/NLPMetrics", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gcunhase/NLPMetrics", "snippet": "<b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>) Papineni 2002 &#39;Measures how many words overlap in a given translation when <b>compared</b> to a reference translation, giving higher scores to sequential words.&#39; (recall) Limitation: Doesn&#39;t consider different types of errors (insertions, substitutions, synonyms, paraphrase, stems) Designed to be a corpus measure, so it has undesirable properties when used for single sentences. GLEU (Google-<b>BLEU</b>) Wu et al. 2016; Minimum of <b>BLEU</b> recall and precision applied to ...", "dateLastCrawled": "2022-01-27T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are <b>the benefits/disadvantages associated with word</b> error rate ...", "url": "https://www.quora.com/What-are-the-benefits-disadvantages-associated-with-word-error-rate-WER-compared-to-the-BLEU-bilingual-evaluation-understudy-metric", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>the-benefits-disadvantages-associated-with-word</b>-error...", "snippet": "Answer: WER tends to predicate a raw metric of how many updates must be performed - in terms of deletion, editing, etc. Where as of <b>BLEU</b> tends to measure the translation strength - in comparison to what would be judged as being \u201cfavored by people\u201d. However - I think both of them run into distin...", "dateLastCrawled": "2022-01-21T01:28:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language <b>Evaluation</b> | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) #language. A score between 0.0 and 1.0, inclusive, indicating the quality of a translation between two human languages (for example, between English and Russian). A <b>BLEU</b> score of 1.0 indicates a perfect translation; a <b>BLEU</b> score of 0.0 indicates a terrible translation. C. causal language model. #language. Synonym for unidirectional language model. See bidirectional language model to contrast different directional approaches in language modeling. crash ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Evaluation</b> of an <b>NLP</b> model \u2014 latest benchmarks | by Ria Kulshrestha ...", "url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>evaluation</b>-of-an-<b>nlp</b>-model-latest-benchmarks-90fd8ce6fae5", "snippet": "<b>BLEU</b> Score \u2014 <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>. As the name suggests, it was originally used to evaluate translations from one language to another. How to calculate <b>BLEU</b> score? Calculating unigram precision: Step 1: Look at each word in the output sentence and assign it a score of 1 if it shows up in any of the reference sentences and 0 if it doesn\u2019t. Step 2: Normalize that count, so that it\u2019s always between 0 and 1, by dividing the number of words that showed up in one of the reference ...", "dateLastCrawled": "2022-01-28T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Natrual language processing basic concepts - language model - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "<b>BLEU</b> stands for <b>bilingual</b> <b>evaluation</b> <b>understudy</b>. It&#39;s an automatic metric to evaluate how close a sequence of text generated by a language model is to a reference. At first, it&#39;s used to evaluate the quality of <b>machine</b> translation text. Now other natural language processing tasks such as task-oriented dialogue generation adopt it as well. For a reference &quot;The man returned to the store&quot;, a generated text &quot;the the man the&quot; would get a BLUE score as below. For each word in the generated text ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>Computational</b> Limits of Deep <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-computational-limits-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>computational</b>-limits-of-deep-<b>learning</b>", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) [papineni2002bleu] score is a metric for translation and computes the similarity between human translation and <b>machine</b> translation based on n-gram. An n-gram is a continuous sequence of n items from a given text. The score is based on precision, brevity penalty, and clipping. The modified n-gram precision ...", "dateLastCrawled": "2022-01-28T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "9.7. <b>Sequence</b> to <b>Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>), though originally proposed for evaluating <b>machine</b> translation results [Papineni et al., 2002], has been extensively used in measuring the quality of output sequences for different applications.", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Arti cial Intelligence Master Thesis", "url": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "snippet": "2:87 <b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>) score over the baseline; attention model, for German-English translation, and 0:34 <b>BLEU</b> score improvement for Catalan-Spanish trans-lation. Keywords <b>Machine</b> <b>Learning</b>, Deep <b>Learning</b>, Natural Language Processing, Neural <b>Machine</b> Transla-tion", "dateLastCrawled": "2021-12-27T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>The Evolution of Machine Translation</b> | LLM Law Review", "url": "https://www.llmlawreview.com/2018/01/26/the-evolution-of-machine-translation/", "isFamilyFriendly": true, "displayUrl": "https://www.llmlawreview.com/2018/01/26/<b>the-evolution-of-machine-translation</b>", "snippet": "Using the <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) method to score the outcome, they found that NMT scored consistently higher than PBSMT for accuracy. In addition, human translators whose native language was Catalan but who were also fluent in English, evaluated sections of the MT translation in three of the books. Once again, the NMT outperformed its rival. It was estimated that \u201cbetween 17% and 34% of the translations \u2026 are perceived by native speakers of the target language to be of ...", "dateLastCrawled": "2022-01-19T23:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Machine</b> Translation Development for Indian Languages and its ...", "url": "https://www.academia.edu/12395564/Machine_Translation_Development_for_Indian_Languages_and_its_Approaches", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/12395564", "snippet": "Urdu to English 2013 Urdu-English General Corpus based Explained <b>Machine</b> Translation methodology of using <b>Bilingual</b> each system and <b>Evaluation</b> found their <b>Understudy</b> [32] comparison based on their respective outputs using <b>BLEU</b>. The EBMT approach produced accuracy of 84.21% whereas the accuracy of the online SMT system is 62.68%. 4. Developing English- -- English-Urdu General Interlingua The English-Hindi Urdu <b>Machine</b> based Rule- lexical database is Translation Via Hindi based approach used ...", "dateLastCrawled": "2022-01-10T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Evaluation of machine translation systems and related procedures</b>", "url": "https://www.researchgate.net/publication/326320090_Evaluation_of_machine_translation_systems_and_related_procedures", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/326320090_<b>Evaluation</b>_of_<b>machine</b>_translation...", "snippet": "<b>Evaluation of machine translation systems and related procedures</b>. June 2018 ; Journal of Engineering and Applied Sciences 13(12):3961-3972; Project: <b>Machine</b> <b>learning</b>; Authors: Musatafa Albadr ...", "dateLastCrawled": "2022-01-15T04:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Learning</b> by <b>Analogy</b>: A Classification Rule for Binary and Nominal ...", "url": "https://www.researchgate.net/publication/220812160_Learning_by_Analogy_A_Classification_Rule_for_Binary_and_Nominal_Data", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220812160_<b>Learning</b>_by_<b>Analogy</b>_A...", "snippet": "We are interested here in the use of analogical proportions for making predictions, in a <b>machine</b> <b>learning</b> context. In recent works, <b>analogy</b>-based classifiers have achieved noteworthy performances ...", "dateLastCrawled": "2022-01-05T06:35:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bleu (bilingual evaluation understudy))  is like +(learning a language)", "+(bleu (bilingual evaluation understudy)) is similar to +(learning a language)", "+(bleu (bilingual evaluation understudy)) can be thought of as +(learning a language)", "+(bleu (bilingual evaluation understudy)) can be compared to +(learning a language)", "machine learning +(bleu (bilingual evaluation understudy) AND analogy)", "machine learning +(\"bleu (bilingual evaluation understudy) is like\")", "machine learning +(\"bleu (bilingual evaluation understudy) is similar\")", "machine learning +(\"just as bleu (bilingual evaluation understudy)\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be thought of as\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be compared to\")"]}