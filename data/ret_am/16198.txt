{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Mean Average Precision</b> (<b>mAP</b>) Explained | Paperspace Blog", "url": "https://blog.paperspace.com/mean-average-precision/", "isFamilyFriendly": true, "displayUrl": "https://blog.paperspace.com/<b>mean-average-precision</b>", "snippet": "Due to the importance of both precision and recall, there is a <b>precision-recall</b> <b>curve</b> the shows the tradeoff between the precision and recall values for different thresholds. This <b>curve</b> helps to select the best threshold to maximize both metrics. There are some inputs needed to create the <b>precision-recall</b> <b>curve</b>: The ground-truth labels.", "dateLastCrawled": "2022-02-03T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Precision-recall</b> curves \u2013 what are they and how are they used?", "url": "https://acutecaretesting.org/en/articles/precision-recall-curves-what-are-they-and-how-are-they-used", "isFamilyFriendly": true, "displayUrl": "https://acutecaretesting.org/en/articles/<b>precision-recall</b>-<b>curves</b>-what-are-they-and-how...", "snippet": "FIG. 7: <b>Precision-recall curve</b> for a test with complete overlap of results between persons with and without disease \u2013 imbalanced distribution Y:N equal to 1:9. A PCR plot for a test with complete overlap of results between persons with and without disease will be determined by the ratio between the two groups: \u2022 For the balanced data set with Y:N equal to 1:1, you will due to the complete overlap of data for each cut-off have the same number of persons with disease as persons without ...", "dateLastCrawled": "2022-02-02T06:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>What is Mean Average Precision</b> (<b>mAP</b>) in Object Detection?", "url": "https://blog.roboflow.com/mean-average-precision/", "isFamilyFriendly": true, "displayUrl": "https://blog.roboflow.com/<b>mean-average-precision</b>", "snippet": "In order to calculate <b>mAP</b>, we draw a series of <b>precision recall</b> curves with the IoU threshold set at varying levels of difficulty. A sketch of <b>mAP</b> <b>precision-recall</b> curves by yours truly. In my sketch, red is drawn with the highest requirement for IoU (perhaps 90 percent) and the orange line is drawn with the most lenient requirement for IoU (perhaps 10 percent).", "dateLastCrawled": "2022-02-02T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Precision-Recall Curve | ML - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/precision-recall-curve-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>precision-recall</b>-<b>curve</b>-ml", "snippet": "Let us briefly understand what is a <b>Precision-Recall</b> <b>curve</b>. <b>Precision-Recall</b> (PR) <b>Curve</b> \u2013 A PR <b>curve</b> is simply a graph with Precision values on the y-axis and Recall values on the x-axis. In other words, the PR <b>curve</b> contains TP/(TP+FN) on the y-axis and TP/(TP+FP) on the x-axis. It is important to note that Precision is also called the Positive Predictive Value (PPV). Recall is also called Sensitivity, Hit Rate or True Positive Rate (TPR). The figure below shows a juxtaposition of sample ...", "dateLastCrawled": "2022-02-02T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Breaking Down Mean <b>Average Precision</b> (<b>mAP</b>) | by Ren Jie Tan | Towards ...", "url": "https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/breaking-down-mean-<b>average-precision</b>-<b>map</b>-ae462f623a52", "snippet": "<b>Precision/Recall</b> <b>Curve</b> (PR <b>Curve</b>) With the TP, FP and FN formally defined, we can now calculate the precision and recall of our detection for a given class across the test set. Each BB would have its confidence level, usually given by its softmax layer, and would be used to rank the output. Note that this is very similar to the information ...", "dateLastCrawled": "2022-02-02T18:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How To <b>Calculate the mean Average Precision</b> (<b>mAP</b>) in object detection ...", "url": "https://hungsblog.de/en/technology/how-to-calculate-mean-average-precision-map/", "isFamilyFriendly": true, "displayUrl": "https://hungsblog.de/en/technology/how-to-calculate-mean-average-precision-<b>map</b>", "snippet": "The Average Precision (AP) is meant to summarize the <b>Precision-Recall</b> <b>Curve</b> by averaging the precision across all recall values between 0 and 1. Efffectively it is the area under the <b>Precision-Recall</b> <b>curve</b>. Because the <b>curve</b> is a characterized by zick zack lines it is best to approximate the area using interpolation. At this point i would again <b>like</b> to refer to the already comprehensive work of Padilla et al., 2020 and also EL Aidouni, 2019 on how to interpolate the precision from the recall ...", "dateLastCrawled": "2022-02-01T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding the <b>mAP</b> Evaluation Metric for Object <b>Detection</b> | by ...", "url": "https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@timothycarlen/understanding-the-<b>map</b>-evaluation-metric-for-object...", "snippet": "<b>Precision-Recall</b> <b>curve</b> for an example classifier. A point on the <b>precision-recall</b> <b>curve</b> is determined by considering all objects above a given model score threshold as a positive prediction, then ...", "dateLastCrawled": "2022-02-03T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-<b>precision-recall</b>-<b>curves</b>-for...", "snippet": "A <b>precision-recall</b> <b>curve</b> (or PR <b>Curve</b>) is a plot of the precision (y-axis) and the recall (x-axis) for different probability thresholds. PR <b>Curve</b>: Plot of Recall (x) vs Precision (y). A model with perfect skill is depicted as a point at a coordinate of (1,1). A skillful model is represented by a <b>curve</b> that bows towards a coordinate of (1,1). A no-skill classifier will be a horizontal line on the plot with a precision that is proportional to the number of positive examples in the dataset. For ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Segmentation And Recongnition Metrics</b> | Penguinflys\u2019 Log", "url": "https://penguinflys.github.io/penguinflys/fundamental/Segmentation-And-Recongnition-Metrics/", "isFamilyFriendly": true, "displayUrl": "https://penguinflys.github.io/penguinflys/fundamental/Segmentation-And-Recongnition...", "snippet": "<b>Precision Recall</b> <b>curve</b>(PR-<b>curve</b>) is used to measure the performance of a model by tuning confidence score, precision, and recall from a <b>curve</b> showing the compromise of precision and recall. The larger the area under that <b>curve</b> is, the better performance the model has. The area is what we call average precision. However, such a <b>curve</b> is not convenient to compare models, especially when the <b>curve</b> is noisy and intersects with the saw-tooth shape.", "dateLastCrawled": "2022-01-20T00:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "sklearn.metrics.<b>precision_recall</b>_<b>curve</b> \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/.../modules/generated/sklearn.metrics.<b>precision_recall</b>_<b>curve</b>.html", "snippet": "sklearn.metrics. <b>precision_recall</b>_<b>curve</b> (y_true, probas_pred, *, pos_label = None, sample_weight = None) [source] \u00b6 Compute <b>precision-recall</b> pairs for different probability thresholds. Note: this implementation is restricted to the binary classification task. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The ...", "dateLastCrawled": "2022-02-03T02:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Difference between <b>MAP</b>@K for recommendations, <b>MAP</b> from <b>Precision Recall</b> ...", "url": "https://datascience.stackexchange.com/questions/93011/difference-between-mapk-for-recommendations-map-from-precision-recall-curves-a", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/93011/difference-between-<b>map</b>k-for...", "snippet": "I have been using the 3 metrics independently for a while now, but trying to figure out if they are actually 3 separate things (with <b>similar</b>-looking definitions/names) or there is some underlying connection between them. <b>mAP</b> - Mean Average Precision is the average of the <b>Precision-Recall</b> <b>curve</b> over various thresholds. 1 2", "dateLastCrawled": "2022-01-14T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Breaking Down Mean <b>Average Precision</b> (<b>mAP</b>) | by Ren Jie Tan | Towards ...", "url": "https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/breaking-down-mean-<b>average-precision</b>-<b>map</b>-ae462f623a52", "snippet": "<b>Precision/Recall</b> <b>Curve</b> (PR <b>Curve</b>) With the TP, FP and FN formally defined, we can now calculate the precision and recall of our detection for a given class across the test set. Each BB would have its confidence level, usually given by its softmax layer, and would be used to rank the output. Note that this is very <b>similar</b> to the information ...", "dateLastCrawled": "2022-02-02T18:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>MRR</b> vs <b>MAP</b> vs NDCG: Rank-Aware Evaluation Metrics And When To Use Them ...", "url": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "snippet": "We get the <b>precision-recall</b> <b>curve</b> by computing the precision as a function of recall values. In this ... The goal of the <b>MAP</b> measure <b>is similar</b> to the goal of the NDCG metric. They both value ...", "dateLastCrawled": "2022-01-31T10:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>ROC and precision-recall curves</b> \u2013 way to be a data scientist", "url": "https://datascience103579984.wordpress.com/2019/04/30/roc-and-precision-recall-curves/", "isFamilyFriendly": true, "displayUrl": "https://datascience103579984.wordpress.com/2019/04/30/<b>roc-and-precision-recall-curves</b>", "snippet": "ROC curves are quite useful for comparing methods. However, they have one weakness, and it is that neither of the measures plotted depend on prevalence. In cases in which prevalence matters, we may instead. make a <b>precision recall</b> plot. The idea <b>is similar</b>, but we instead plot precision against recall. 1.", "dateLastCrawled": "2021-11-03T09:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>PRECISION-RECALL</b> <b>CURVE</b> \u00b7 Issue #898 \u00b7 ultralytics/<b>yolov3</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/ultralytics/yolov3/issues/898", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ultralytics/<b>yolov3</b>/issues/898", "snippet": "<b>Precision Recall</b> curves may be plotted by uncommenting code here when running test.py: For <b>yolov3</b>-spp-ultralytics.pt on COCO, the curves for all 80 classes look like this: For a single class 0, or person, the <b>curve</b> looks like this. During testing we evaluate the area under the <b>curve</b> as average <b>precision</b>, AP.", "dateLastCrawled": "2022-01-26T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>information retrieval evaluation python precision, recall</b>, f score, AP,<b>MAP</b>", "url": "https://stackoverflow.com/questions/40457331/information-retrieval-evaluation-python-precision-recall-f-score-ap-map", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/40457331", "snippet": "I have a <b>similar</b> question to yours @M.R.. I&#39;ve been wondering how I plot a <b>Precision-Recall</b> <b>curve</b> that represents an entire set of queries, instead of a single query. Should I take, for example, the mean (between all queries) of the precision and recall values at different points, and plot that? \u2013", "dateLastCrawled": "2022-01-27T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "machine learning - Why does <b>precision_recall</b>_<b>curve</b>() return <b>similar</b> but ...", "url": "https://stats.stackexchange.com/questions/559203/why-does-precision-recall-curve-return-similar-but-not-equal-values-than-confu", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/559203/why-does-<b>precision-recall</b>-<b>curve</b>...", "snippet": "Why does <b>precision_recall</b>_<b>curve</b>() return <b>similar</b> but not equal values than confusion matrix? Ask Question Asked 29 days ago. Active 29 days ago. Viewed 63 times 2 $\\begingroup$ INTRO: I wrote a very simple machine learning project which classifies numbers based on the minst dataset: from sklearn.datasets import fetch_openml import numpy as np from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.linear_model import SGDClassifier from sklearn.metrics ...", "dateLastCrawled": "2022-02-02T17:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-<b>precision-recall</b>-<b>curves</b>-for...", "snippet": "A <b>precision-recall</b> <b>curve</b> (or PR <b>Curve</b>) is a plot of the precision (y-axis) and the recall (x-axis) for different probability thresholds. PR <b>Curve</b>: Plot of Recall (x) vs Precision (y). A model with perfect skill is depicted as a point at a coordinate of (1,1). A skillful model is represented by a <b>curve</b> that bows towards a coordinate of (1,1). A no-skill classifier will be a horizontal line on the plot with a precision that is proportional to the number of positive examples in the dataset. For ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Ml Roc Pr - Plotly", "url": "https://plotly.com/python/roc-and-pr-curves/", "isFamilyFriendly": true, "displayUrl": "https://plotly.com/python/<b>roc-and-pr-curves</b>", "snippet": "Basic binary ROC <b>curve</b>\u00b6. Notice how this ROC <b>curve</b> looks <b>similar</b> to the True Positive Rate <b>curve</b> from the previous plot. This is because they are the same <b>curve</b>, except the x-axis consists of increasing values of FPR instead of threshold, which is why the line is flipped and distorted.", "dateLastCrawled": "2022-01-31T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to Use ROC Curves and <b>Precision-Recall Curves for Classification</b> in ...", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-<b>precision-recall</b>-<b>curves</b>-for...", "snippet": "A <b>precision-recall</b> <b>curve</b> is a plot of the precision (y-axis) and the recall (x-axis) for different thresholds, much like the ROC <b>curve</b>. A no-skill classifier is one that cannot discriminate between the classes and would predict a random class or a constant class in all cases. The no-skill line changes based on the distribution of the positive to negative classes. It is a horizontal line with the value of the ratio of positive cases in the dataset. For a balanced dataset, this is 0.5. While ...", "dateLastCrawled": "2022-02-03T04:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Precision-recall curves</b> - Andreas Beger", "url": "https://www.andybeger.com/2015/03/16/precision-recall-curves/", "isFamilyFriendly": true, "displayUrl": "https://www.andybeger.com/2015/03/16/<b>precision-recall-curves</b>", "snippet": "The plot below is a <b>precision-recall</b> <b>curve</b> that does this, for the same example as before. Instead of FPR we now have precision, and I&#39;ve also flipped the axes as it seems to be convention to plot recall on the x-axis. <b>Precision-recall</b> <b>curve</b> for the same example data with 0.4 positives. Simulations! Since the example I used had a positive rate of 0.4, the plot doesn&#39;t really make it obvious why one would want to look at <b>precision-recall curves</b> for sparse data. To illustrate that better ...", "dateLastCrawled": "2022-01-26T04:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>precision recall</b> <b>curve</b> in r", "url": "https://davenue.in/8kgelue/precision-recall-curve-in-r.html", "isFamilyFriendly": true, "displayUrl": "https://davenue.in/8kgelue/<b>precision-recall</b>-<b>curve</b>-in-r.html", "snippet": "The measurement and &quot;truth&quot; data must have the same two possible outcomes and one of the outcomes must <b>be thought</b> of as a &quot;relevant&quot; results. Non-linear interpolation. You will explore how the probabilities output by your classifier <b>can</b> be used to trade-off precision with recall, and dive into this spectrum, using <b>precision-recall</b> curves. The area under the <b>precision-recall</b> <b>curve</b> as a performance metric for rare binary events. These functions calculate the recall, precision or F values of a ...", "dateLastCrawled": "2022-01-23T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Intro to Deep Learning \u2014 performance metrics(<b>Precision, Recall</b>, F1, ROC ...", "url": "https://hk3342.medium.com/intro-to-deep-learning-performance-metrics-precision-recall-f1-roc-pr-prg-87f5073f9354", "isFamilyFriendly": true, "displayUrl": "https://hk3342.medium.com/intro-to-deep-learning-performance-metrics-<b>precision-recall</b>...", "snippet": "It <b>can</b> <b>be thought</b> of as the fraction the model correctly predicted among the positive classes. Precision and recall don\u2019t consider the true negative. To get high precision, the model needs to reduce false positive(i.e. when the model incorrectly predicts as positive which was actually negative class). A good example application where precision could be an appropriate metric would be a spam email scanner. To get a high recall, the model needs to decrease false negative(i.e. when the model ...", "dateLastCrawled": "2022-01-30T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How NOT to use ROC, <b>Precision-Recall curves &amp; MCC (Matthews Correlation</b> ...", "url": "https://towardsdatascience.com/how-not-to-use-roc-precision-recall-curves-mcc-matthews-correlation-coefficient-f68a33108f8b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-not-to-use-roc-<b>precision-recall</b>-<b>curves</b>-mcc-matthews...", "snippet": "ROC <b>curve</b>; <b>Precision-Recall</b> (PR) <b>curve</b>; Something else? <b>Thought</b> Process. First of all, let\u2019s eliminate ROC <b>curve</b> as it is not best suited for imbalanced class problems. Here\u2019s a great video to get one\u2019s foundation right for this. On to <b>Precision-Recall</b> <b>curve</b> now - this is what my colleague had chosen. It has Precision as y-axis and Recall as x-axis. If I use the above confusion matrix at various thresholds to plot this <b>curve</b>, do you see any problem in the approach? The <b>Precision-Recall</b> ...", "dateLastCrawled": "2022-01-20T17:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How To <b>Calculate the mean Average Precision</b> (<b>mAP</b>) in object detection ...", "url": "https://hungsblog.de/en/technology/how-to-calculate-mean-average-precision-map/", "isFamilyFriendly": true, "displayUrl": "https://hungsblog.de/en/technology/how-to-calculate-mean-average-precision-<b>map</b>", "snippet": "The Average Precision (AP) is meant to summarize the <b>Precision-Recall</b> <b>Curve</b> by averaging the precision across all recall values between 0 and 1. Efffectively it is the area under the <b>Precision-Recall</b> <b>curve</b>. Because the <b>curve</b> is a characterized by zick zack lines it is best to approximate the area using interpolation. At this point i would again like to refer to the already comprehensive work of Padilla et al., 2020", "dateLastCrawled": "2022-02-01T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "classification - &quot;Good&quot; <b>classifier destroyed my Precision-Recall</b> <b>curve</b> ...", "url": "https://stats.stackexchange.com/questions/201750/good-classifier-destroyed-my-precision-recall-curve-what-happened", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/201750/good-classifier-destroyed-my...", "snippet": "If a model shows good AUC, but still has poor early retrieval, the <b>Precision-Recall</b> <b>curve</b> will leave a lot to be desired. You <b>can</b> see a great example of this happening in this answer to a similar question. For this reason, Saito et al. recommend using area under the <b>Precision-Recall</b> <b>curve</b> rather than AUC when you have imbalanced classes.", "dateLastCrawled": "2022-01-25T11:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "r - <b>Custom Precision-Recall AUC measure in</b> mlr3 - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/61612931/custom-precision-recall-auc-measure-in-mlr3", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/61612931", "snippet": "Browse other questions tagged r machine-learning <b>precision-recall</b> mlr3 or ask your own question. The Overflow Blog Plan for tradeoffs: You <b>can</b>\u2019t optimize all software quality attributes", "dateLastCrawled": "2022-01-19T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Evaluation Metrics for Object Detection</b> - DebuggerCafe", "url": "https://debuggercafe.com/evaluation-metrics-for-object-detection/", "isFamilyFriendly": true, "displayUrl": "https://debuggercafe.com/<b>evaluation-metrics-for-object-detection</b>", "snippet": "The intention in interpolating the <b>precision/recall</b> <b>curve</b> in this way is to reduce the impact of the \u201cwiggles\u201d in the <b>precision/recall</b> <b>curve</b>, caused by small variations in the ranking of examples. It should be noted that to obtain a high score, a method must have precision at all levels of recall\u2014 this penalises methods which retrieve only a subset of examples with high precision (e.g. side views of cars). The PASCAL Visual Object Classes (VOC) Challenge. I think that the above words ...", "dateLastCrawled": "2022-02-03T01:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>MRR</b> vs <b>MAP</b> vs NDCG: Rank-Aware Evaluation Metrics And When To Use Them ...", "url": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "snippet": "The following works here and here provide nice deep dives into the <b>MAP</b> metric. <b>MAP</b> Pros. Gives a single metric that represents the complex Area under the <b>Precision-Recall</b> <b>curve</b>. This provides the ...", "dateLastCrawled": "2022-01-31T10:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Can</b> somebody provide me with a practical explanation of the mean ...", "url": "https://www.reddit.com/r/artificial/comments/s9giwb/can_somebody_provide_me_with_a_practical/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/artificial/comments/s9giwb/<b>can</b>_somebody_provide_me_with_a...", "snippet": "<b>Can</b> somebody provide me with a practical explanation of the mean average precision (<b>mAP</b>) evaluation metric? Question I am working with object detection algorithms (I&#39;m not a computer scientist), and have computed the <b>precision, recall</b> and therefore the mean average precision <b>curve</b> for about 450 test images.", "dateLastCrawled": "2022-01-21T17:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Precision-recall</b> curves \u2013 what are they and how are they used?", "url": "https://acutecaretesting.org/en/articles/precision-recall-curves-what-are-they-and-how-are-they-used", "isFamilyFriendly": true, "displayUrl": "https://acutecaretesting.org/en/articles/<b>precision-recall</b>-<b>curves</b>-what-are-they-and-how...", "snippet": "A <b>precision-recall curve</b> shows the relationship between precision (= positive predictive value) and recall (= sensitivity) for every possible cut-off. The PRC is a graph with: \u2022 The x-axis showing recall (= sensitivity = TP / (TP + FN)) \u2022 The y-axis showing precision (= positive predictive value = TP / (TP + FP)) Thus every point on the PRC represents a chosen cut-off even though you cannot see this cut-off. What you <b>can</b> see is the precision and the recall that you will get when you ...", "dateLastCrawled": "2022-02-02T06:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Precision-Recall Curve | ML - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/precision-recall-curve-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>precision-recall</b>-<b>curve</b>-ml", "snippet": "Let us briefly understand what is a <b>Precision-Recall</b> <b>curve</b>. <b>Precision-Recall</b> (PR) <b>Curve</b> \u2013 A PR <b>curve</b> is simply a graph with Precision values on the y-axis and Recall values on the x-axis. In other words, the PR <b>curve</b> contains TP/(TP+FN) on the y-axis and TP/(TP+FP) on the x-axis. It is important to note that Precision is also called the Positive Predictive Value (PPV). Recall is also called Sensitivity, Hit Rate or True Positive Rate (TPR). The figure below shows a juxtaposition of sample ...", "dateLastCrawled": "2022-02-02T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to Use ROC Curves and <b>Precision-Recall Curves for Classification</b> in ...", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-<b>precision-recall</b>-<b>curves</b>-for...", "snippet": "The curves of different models <b>can</b> <b>be compared</b> directly in general or for different thresholds. The area under the <b>curve</b> (AUC) <b>can</b> be used as a summary of the model skill. The shape of the <b>curve</b> contains a lot of information, including what we might care about most for a problem, the expected false positive rate, and the false negative rate. To make this clear: Smaller values on the x-axis of the plot indicate lower false positives and higher true negatives. Larger values on the y-axis of ...", "dateLastCrawled": "2022-02-03T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-<b>precision-recall</b>-<b>curves</b>-for...", "snippet": "Now that we have seen the <b>Precision-Recall</b> <b>Curve</b>, let\u2019s take a closer look at the ROC area under <b>curve</b> score. <b>Precision-Recall</b> Area Under <b>Curve</b> (AUC) Score. The <b>Precision-Recall</b> AUC is just like the ROC AUC, in that it summarizes the <b>curve</b> with a range of threshold values as a single score. The score <b>can</b> then be used as a point of comparison between different models on a binary classification problem where a score of 1.0 represents a model with perfect skill. The <b>Precision-Recall</b> AUC score ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The area under the <b>precision\u2010recall</b> <b>curve</b> as a performance metric for ...", "url": "https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13140", "isFamilyFriendly": true, "displayUrl": "https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13140", "snippet": "The area under the <b>precision-recall</b> <b>curve</b> as a performance metric for rare binary events Helen R. Sofaer1 | Jennifer A. Hoeting2 ... index that <b>can</b> <b>be compared</b> across datasets. Our results indicate AUC-PR and precision <b>can</b> provide useful and intuitive metrics for evaluating a model&#39;s utility for guiding sampling, and <b>can</b> complement other metrics to help delineate a mod-el&#39;s appropriate use. This article has been contributed to by US employees and their work is in the public domain in the USA ...", "dateLastCrawled": "2022-01-20T02:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Precision Recall Curve Simplified</b> - ListenData", "url": "https://www.listendata.com/2019/07/precision-recall-curve-simplified.html", "isFamilyFriendly": true, "displayUrl": "https://www.listendata.com/2019/07/<b>precision-recall-curve-simplified</b>.html", "snippet": "This article outlines <b>precision recall</b> <b>curve</b> and how it is used in real-world data science application. It includes explanation of how it is different from ROC <b>curve</b>. It also highlights limitation of ROC <b>curve</b> and how it <b>can</b> be solved via area under <b>precision-recall</b> <b>curve</b>. This article also covers implementation of area under <b>precision recall</b> ...", "dateLastCrawled": "2022-01-30T14:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>MRR</b> vs <b>MAP</b> vs NDCG: Rank-Aware Evaluation Metrics And When To Use Them ...", "url": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "snippet": "We get the <b>precision-recall</b> <b>curve</b> by computing the precision as a function of recall values. In this ... <b>Compared</b> to the <b>MAP</b> metric it does a good job at evaluating the position of ranked items ...", "dateLastCrawled": "2022-01-31T10:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "computer vision - High <b>mAP</b>@50 with low precision and recall. What does ...", "url": "https://stackoverflow.com/questions/62973155/high-map50-with-low-precision-and-recall-what-does-it-mean-and-what-metric-sho", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/62973155", "snippet": "In your case faster rcnn results indicate that <b>precision-recall</b> <b>curve</b> metric is bad <b>compared</b> to that of Yolov3, which means that either faster rcnn has very bad recall at higher confidence thresholds or very bad precision at lower confidence threshold <b>compared</b> to that of Yolov3 (especially for small objects). <b>Precision, Recall</b> and F1 score are computed for given confidence threshold. I&#39;m assuming you&#39;re running the model with default confidence threshold (could be 0.25). So higher Precision ...", "dateLastCrawled": "2022-01-27T12:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to <b>plot a precision-recall curve in MATLAB</b> - Quora", "url": "https://www.quora.com/How-can-I-plot-a-precision-recall-curve-in-MATLAB", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-I-<b>plot-a-precision-recall-curve-in-MATLAB</b>", "snippet": "Answer (1 of 3): Using perfcurve() from the Statistics Toolbox: [code] scores = rand(1000, 1); targets = round(targets + 0.5*(rand(1000,1) - 0.5)); figure [Xpr,Ypr ...", "dateLastCrawled": "2022-01-30T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Link prediction</b> - Neo4j Graph Data Science", "url": "https://neo4j.com/docs/graph-data-science/current/algorithms/ml-models/linkprediction/", "isFamilyFriendly": true, "displayUrl": "https://neo4j.com/docs/graph-data-science/current/algorithms/ml-models/<b>linkprediction</b>", "snippet": "The area under the <b>Precision-Recall</b> <b>curve</b> <b>can</b> also be interpreted as an average precision where the average is over different classification thresholds. 1.4.1. Class imbalance . Most graphs have far more non-connected node pairs than connected ones (e.g. sparse graphs). Thus, typically we have an issue with class imbalance. There are multiple strategies to account for imbalanced data. In our procedure, the AUCPR metric is used which is considered more suitable than the commonly used AUROC ...", "dateLastCrawled": "2022-01-29T15:17:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> <b>Evaluation Metrics</b> - GitHub Pages", "url": "https://kevalnagda.github.io/evaluation-metrics", "isFamilyFriendly": true, "displayUrl": "https://kevalnagda.github.io/<b>evaluation-metrics</b>", "snippet": "This is where Average Precision (AP), which is based on the <b>precision-recall</b> <b>curve</b>, comes into play. In essence, AP is the precision averaged across all unique recall levels. where, r1, r2, r3, \u2026, rn are the recall levels at which the precision is first interpolated. ROC <b>Curve</b> The Receiver Operating Characteristic <b>curve</b> is a plot that shows the performance of a binary classifier as a function of its cut-off threshold. It essentially shows the True Positive Rate (TPR) against the False ...", "dateLastCrawled": "2021-10-13T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "My Review Note for Applied <b>Machine</b> <b>Learning</b> (Second Half) - Alan Duan", "url": "https://alanduan.me/machine%20learning/cheatsheetappliedMLpart2/", "isFamilyFriendly": true, "displayUrl": "https://alanduan.me/<b>machine</b> <b>learning</b>/cheatsheetappliedMLpart2", "snippet": "<b>Precision-Recall</b> <b>Curve</b>. Area under it is the average precision (ignoring some technical differences). Ideal <b>curve</b> \u2013upper right. Receiver operating characterisitics (ROC), which is FPR (\\(FP/(FP+TN)\\))v.s. TPR (recall). AUC is the area under the <b>curve</b>, which does not depend on threshold selection. AUC is always 0.5 for random prediction ...", "dateLastCrawled": "2021-04-04T07:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Learning</b> Curves in <b>Machine</b> <b>Learning</b> - ResearchGate", "url": "https://www.researchgate.net/publication/247934703_Learning_Curves_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/247934703_<b>Learning</b>_<b>Curves</b>_in_<b>Machine</b>_<b>Learning</b>", "snippet": "The area under the receiver operating characteristic (ROC) <b>curve</b> (AUC) was 0.62 (95% confidence interval [CI]: 0.57, 0.68) and the area under the <b>precision\u2010recall</b> <b>curve</b> was 0.58. <b>Learning</b> <b>curve</b> ...", "dateLastCrawled": "2021-12-15T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Interpret your Regression</b>. A walk through Logistic Regression | by ...", "url": "https://towardsdatascience.com/interpret-your-regression-d5f93908327b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpret-your-regression</b>-d5f93908327b", "snippet": "The <b>precision-recall</b> <b>curve</b> calls attention to the point that the model is just slightly above the no skill line for most thresholds. The no skill line is a line parallel to the x-axis with the value of the ratio of positive cases in the dataset, which is, in this case, 0.06. But this contradicts the high accuracy of 93%.", "dateLastCrawled": "2022-02-01T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>machine</b> <b>learning</b> - How to calculate precision and recall in a 3 x 3 ...", "url": "https://stats.stackexchange.com/questions/91044/how-to-calculate-precision-and-recall-in-a-3-x-3-confusion-matrix", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/91044/how-to-calculate-precision-and-recall...", "snippet": "<b>machine</b>-<b>learning</b> <b>precision-recall</b>. Share. Cite. Improve this question. Follow edited Mar 23 &#39;14 at 11:58. TooTone. 3,621 ... I already understand the <b>analogy</b> described in your solution. I will read paper. I will accept this as a answer. I don&#39;t understand PPV AND NPV.Please explain these concept as graphic as the Sens and Spec were explained and I will accept your answer. $\\endgroup$ \u2013 user22149. Mar 23 &#39;14 at 22:27. Add a comment | 3 $\\begingroup$ By reducing the data down to forced ...", "dateLastCrawled": "2022-01-30T05:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>An Intuitive Explanation to Precision, Recall and</b> Accuracy", "url": "https://www.linkedin.com/pulse/intuitive-explanation-precision-recall-accuracy-daniel-d-souza/", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/intuitive-explanation-<b>precision-recall</b>-accuracy-daniel...", "snippet": "Earlier this year, at an interview in New York I was asked about the recall and precision of one of my <b>Machine</b> <b>Learning</b> Projects. For a couple of minutes following that, the interviewer sat back ...", "dateLastCrawled": "2021-10-21T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Bias -Variance &amp; <b>Precision-Recall</b> Trade-offs: How to aim for the sweet ...", "url": "https://towardsdatascience.com/tradeoffs-how-to-aim-for-the-sweet-spot-c20b40d5e6b6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/tradeoffs-how-to-aim-for-the-sweet-spot-c20b40d5e6b6", "snippet": "<b>Machine</b> <b>Learning</b> mostly have to deal with two Trade-offs, Bias-Variance Trade-offs; <b>Precision-Recall</b> Trade-offs; Part 1: Bias-Variance Trade-offs 1.1 First thing first, What is Bias, What is Variance? 1.1.1 Bias: To understand it, we must know its general meaning. Cambridge dictionary states as, The action of supporting or opposing a particular person or thing in an unfair way, because of allowing personal opinions to influence your judgment. \u2192 So in the world of stats, it is defined as ...", "dateLastCrawled": "2022-01-30T08:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "3.3. Metrics and scoring: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "The <b>precision_recall</b>_<b>curve</b> computes a <b>precision-recall</b> <b>curve</b> from the ground truth label and a <b>score</b> given by the classifier by varying a decision threshold. The average_precision_<b>score</b> function computes the average precision (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as \\[\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\\] where \\(P_n\\) and \\(R_n\\) are the precision and recall at the nth threshold. With random predictions, the AP is the fraction of ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Prediction of Chronic Kidney Disease - A Machine Learning Perspective</b>", "url": "https://www.researchgate.net/publication/348697674_Prediction_of_Chronic_Kidney_Disease_-_A_Machine_Learning_Perspective", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348697674_Prediction_of_Chronic_Kidney...", "snippet": "Along with accuracy, <b>precision, recall</b>, F-measure, area under the <b>curve</b> and GINI coefficient have been computed and compared results of various algorithms have been shown in the graph. Least ...", "dateLastCrawled": "2022-02-03T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "More Performance Evaluation Metrics for Classification Problems You ...", "url": "https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2020/04/performance-evaluation-metrics-classification.html", "snippet": "Decision Thresholds and Receiver Operating Characteristic (ROC) <b>curve</b> . Warming up: The flow of <b>Machine</b> <b>Learning</b> model . In any binary classification task, we model can only achieve two results, either our model is correct or incorrect in the prediction where we only have two classes. Imagine we now have a classification task to predict if an image is a dog or cat. In supervised <b>learning</b>, we first fit/train a model on training data, then test the model on testing data. Once we have the model ...", "dateLastCrawled": "2022-01-26T05:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine learning - precision recall curve is like</b> stairs - Data Science ...", "url": "https://datascience.stackexchange.com/questions/86830/precision-recall-curve-is-like-stairs", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/86830/<b>precision-recall-curve-is-like</b>...", "snippet": "<b>precision recall curve is like</b> stairs [closed] Ask Question Asked 1 year ago. Active 1 year ago. Viewed 83 times 0 $\\begingroup$ Closed. This question needs details or clarity. It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post. Closed 1 year ago. Improve this question I am training an ensemble model using a 400 data set sample this led to a precision recall curve that looks like stairs ? what would be the reason ...", "dateLastCrawled": "2022-01-14T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Newest &#39;ensemble-modeling&#39; Questions</b> - <b>Data Science Stack Exchange</b>", "url": "https://datascience.stackexchange.com/questions/tagged/ensemble-modeling", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/tagged/ensemble-modeling", "snippet": "Q&amp;A for Data science professionals, <b>Machine</b> <b>Learning</b> specialists, and those interested in <b>learning</b> more about the field. Stack Exchange Network. Stack Exchange network consists of 178 Q&amp;A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange. Loading\u2026 0 +0; Tour Start here for a quick overview of the site Help Center Detailed answers to any questions you might have Meta ...", "dateLastCrawled": "2022-01-10T07:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Future Internet | Free Full-Text | <b>Machine</b> <b>Learning</b> in Detecting COVID ...", "url": "https://www.mdpi.com/1999-5903/13/10/244/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1999-5903/13/10/244/htm", "snippet": "Area under precision\u2013recall curve (PR-AUC): The <b>precision\u2013recall curve is similar</b> to the ROC curve, which is also a performance evaluation metric, especially when the supplied data are heavily imbalanced. PR-AUC is generally used to summarize the precision\u2013recall curve into a single value. If the value of PR-AUC is small, it indicates a bad classifier; a higher value such as 1 indicates an excellent classifier.", "dateLastCrawled": "2022-01-25T13:07:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(precision-recall curve)  is like +(map)", "+(precision-recall curve) is similar to +(map)", "+(precision-recall curve) can be thought of as +(map)", "+(precision-recall curve) can be compared to +(map)", "machine learning +(precision-recall curve AND analogy)", "machine learning +(\"precision-recall curve is like\")", "machine learning +(\"precision-recall curve is similar\")", "machine learning +(\"just as precision-recall curve\")", "machine learning +(\"precision-recall curve can be thought of as\")", "machine learning +(\"precision-recall curve can be compared to\")"]}