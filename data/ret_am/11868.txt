{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "Types of <b>Loss Functions in Machine Learning</b>. Below are the different types of the <b>loss</b> function in <b>machine</b> <b>learning</b> which are as follows: 1. Regression <b>loss</b> functions. Linear regression is a fundamental concept of this function. Regression <b>loss</b> functions establish a linear relationship between a dependent variable (Y) and an independent ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>ML | Common Loss Functions - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/ml-common-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/ml-common-<b>loss</b>-functions", "snippet": "(5) . Hinge <b>Loss</b> also known as Multi-class SVM <b>Loss</b>.Hinge <b>loss</b> is applied for maximum-margin classification, prominently for support vector machines. It is a convex function used in the convex optimizer.", "dateLastCrawled": "2022-01-29T08:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions in Neural Networks - theaidream.com", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Neural networks are trained using stochastic gradient descent and require that you choose a <b>loss</b> function when designing and configuring your model. There are many <b>loss</b> functions to choose from and it can be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when training a neural network. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "python - How can I use <b>L2</b>-<b>Loss</b> function for Object-detection CNN in ...", "url": "https://stackoverflow.com/questions/55963192/how-can-i-use-l2-loss-function-for-object-detection-cnn-in-tensorflow-framework", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/55963192", "snippet": "Although some object detection frameworks do look <b>like</b> a regression model (YOLO, SSD), but the <b>loss</b> function is not as simple as a <b>L2</b> <b>loss</b>. In fact, the <b>loss</b> function consists of two parts, crossentropy <b>loss</b> for classification and regression <b>loss</b> for localization, and <b>L2</b> <b>loss</b> is usually used for regression <b>loss</b> here.", "dateLastCrawled": "2022-01-28T22:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Analyzing l1-<b>loss</b> and <b>l2</b>-<b>loss Support Vector Machines Implemented</b> ...", "url": "https://www.researchgate.net/publication/332402217_Analyzing_l1-loss_and_l2-loss_Support_Vector_Machines_Implemented_in_PERMON_Toolbox", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/332402217_Analyzing_l1-<b>loss</b>_and_<b>l2</b>-<b>loss</b>...", "snippet": "Thesupport-vector network is a new <b>learning</b> <b>machine</b> for two-group classification problems. The <b>machine</b> conceptually implements the following idea: input vectors are non-linearly mapped to a very ...", "dateLastCrawled": "2022-01-01T08:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - What&#39;s the difference between <b>Error</b>, Risk and <b>Loss</b> ...", "url": "https://datascience.stackexchange.com/questions/35928/whats-the-difference-between-error-risk-and-loss", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/35928", "snippet": "Data Science Stack Exchange is a question and answer site for Data science professionals, <b>Machine</b> <b>Learning</b> specialists, and those interested in <b>learning</b> more about the field.", "dateLastCrawled": "2022-02-03T06:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Understanding <b>Loss</b> Functions in Computer Vision! | by Sowmya ...", "url": "https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/ml-cheat-sheet/winning-at-<b>loss</b>-functions-2-important-<b>loss</b>-functions...", "snippet": "Typically for <b>machine</b> <b>learning</b> models, convergence is observed as the minimization of the chosen <b>loss</b> function on the training dataset. In a GAN, convergence signals the end of the two-player game ...", "dateLastCrawled": "2022-02-03T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to choose between <b>L2</b> and L1-<b>smoothed loss function in regression</b> ...", "url": "https://www.quora.com/How-do-I-choose-between-L2-and-L1-smoothed-loss-function-in-regression-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-I-choose-between-<b>L2</b>-and-L1-smoothed-<b>loss</b>-function-in...", "snippet": "Answer: The difference between L1 and <b>L2</b> is mainly how they prioritize between reducing big errors and small errors. As an example, let\u2019s say 5 of your data points ...", "dateLastCrawled": "2022-01-08T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "linkedin-skill-assessments-quizzes/<b>machine</b>-<b>learning</b>-quiz.md at master ...", "url": "https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes/blob/master/machine-learning/machine-learning-quiz.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/.../blob/master/<b>machine</b>-<b>learning</b>/<b>machine</b>-<b>learning</b>-quiz.md", "snippet": "supervised <b>machine</b> <b>learning</b> <b>system</b> that classifies applicants into existing groups // we do not need to classify best candidates we just need to classify job applicants in to existing categories; Q49. Someone of your data science team recommends that you use decision trees, naive Bayes and K-nearest neighbor, all at the same time, on the same training data, and then average the results. What is this an example of? regression analysis; unsupervised <b>learning</b>; high -variance modeling; ensemble ...", "dateLastCrawled": "2022-02-02T14:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions in <b>Machine</b> <b>Learning</b>: An Easy Overview(2021)", "url": "https://www.jigsawacademy.com/blogs/ai-ml/loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.<b>jigsawacademy</b>.com/blogs/ai-ml/<b>loss</b>-function", "snippet": "It has all the advantages of Huber <b>loss</b>, and some <b>Learning</b> algorithms like XGBoost use Newton\u2019s method to find the optimum. Conclusion. Above, we have mentioned the various types of <b>loss</b> function example, which will give a clear understanding of What is a <b>loss</b> function in <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2022-01-26T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Ridge and Lasso <b>Regression (L1 and L2 regularization</b>) Explained Using ...", "url": "https://www.excelr.com/blog/data-science/regression/l1_and_l2_regularization", "isFamilyFriendly": true, "displayUrl": "https://www.excelr.com/blog/data-science/<b>regression/l1_and_l2_regularization</b>", "snippet": "In the domain of <b>machine</b> <b>learning</b>, regularization is the process which prevents overfitting by discouraging developers <b>learning</b> a more complex or flexible model, and finally, which regularizes or shrinks the coefficients towards zero. The basic idea is to penalize the complex models i.e. adding a complexity term in such a way that it tends to give a bigger <b>loss</b> for evaluating complex models.", "dateLastCrawled": "2022-02-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Activation and <b>loss</b> functions (part 1) \u00b7 <b>Deep Learning</b>", "url": "https://atcold.github.io/pytorch-Deep-Learning/en/week11/11-1/", "isFamilyFriendly": true, "displayUrl": "https://atcold.github.io/pytorch-<b>Deep-Learning</b>/en/week11/11-1", "snippet": "Use Case: L1 <b>loss</b> is more robust against outliers and noise compared to <b>L2</b> <b>loss</b>. In <b>L2</b>, the errors of those outlier/noisy points are squared, so the cost function gets very sensitive to outliers. Problem: The L1 <b>loss</b> is not differentiable at the bottom (0). We need to be careful when handling its gradients (namely Softshrink). This motivates ...", "dateLastCrawled": "2022-02-02T09:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "The high-level supervised <b>learning</b> process. Before we can actually introduce the concept of <b>loss</b>, we\u2019ll have to take a look at the high-level supervised <b>machine</b> <b>learning</b> process.All supervised training approaches fall under this process, which means that it is equal for deep neural networks such as MLPs or ConvNets, but also for SVMs.. Let\u2019s take a look at this training process, which is cyclical in nature.", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding <b>Loss</b> Functions in Computer Vision! | by Sowmya ...", "url": "https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/ml-cheat-sheet/winning-at-<b>loss</b>-functions-2-important-<b>loss</b>-functions...", "snippet": "Typically for <b>machine</b> <b>learning</b> models, convergence is observed as the minimization of the chosen <b>loss</b> function on the training dataset. In a GAN, convergence signals the end of the two-player game ...", "dateLastCrawled": "2022-02-03T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to choose between <b>L2</b> and L1-<b>smoothed loss function in regression</b> ...", "url": "https://www.quora.com/How-do-I-choose-between-L2-and-L1-smoothed-loss-function-in-regression-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-I-choose-between-<b>L2</b>-and-L1-smoothed-<b>loss</b>-function-in...", "snippet": "Answer: The difference between L1 and <b>L2</b> is mainly how they prioritize between reducing big errors and small errors. As an example, let\u2019s say 5 of your data points ...", "dateLastCrawled": "2022-01-08T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - Validation <b>Error</b> less than <b>training</b> <b>error</b>? - Cross ...", "url": "https://stats.stackexchange.com/questions/187335/validation-error-less-than-training-error/205831", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/187335/validation-<b>error</b>-less-than-<b>training</b>...", "snippet": "When I deleted them - <b>training</b> <b>loss</b> became <b>similar</b> to validation <b>loss</b>. Probably, that happened because during <b>training</b> batch-norm uses mean and variance of the given input batch, which might be different from batch to batch. But during evaluation batch-norm uses running mean and variance, both of which reflect properties of the whole <b>training</b> set much better than mean and variance of a single batch during <b>training</b>. At least, that is how batch-norm is implemented in pytorch", "dateLastCrawled": "2022-02-02T09:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Regularization</b> in <b>Machine</b> <b>Learning</b> | by Prashant Gupta | Towards Data ...", "url": "https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>regularization</b>-in-<b>machine</b>-<b>learning</b>-76441ddcf99a", "snippet": "One of the major aspects of training your <b>machine</b> <b>learning</b> model is avoiding overfitting. The model will have a low accuracy if it is overfitting. This happens because your model is trying too hard to capture the noise in your training dataset. By noise we mean the data points that don\u2019t really represent the true properties of your data, but random chance. <b>Learning</b> such data points, makes your model more flexible, at the risk of overfitting. The concept of balancing bias and variance, is ...", "dateLastCrawled": "2022-02-02T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Part-3: Structuring <b>Machine</b> <b>Learning</b> Projects - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-3-structuring-ml-projects.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-3-structuring...", "snippet": "Understand how to diagnose errors <b>in a machine</b> <b>learning</b> <b>system</b>, and; Be able to prioritize the most promising directions for reducing <b>error</b>; Understand complex ML settings, such as mismatched training/test sets, and comparing to and/or surpassing human-level performance ; Know how to apply end-to-end <b>learning</b>, transfer <b>learning</b>, and multi-task <b>learning</b>; I&#39;ve seen teams waste months or years through not understanding the principles taught in this course. I hope this two week course will save ...", "dateLastCrawled": "2022-02-03T01:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the relationship between the <b>accuracy</b> and the <b>loss</b> in deep ...", "url": "https://datascience.stackexchange.com/questions/42599/what-is-the-relationship-between-the-accuracy-and-the-loss-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/42599", "snippet": "I have created three different models using deep <b>learning</b> for multi-class classification and each model gave me a different <b>accuracy</b> and <b>loss</b> value. The results of the testing model as the following: First Model: <b>Accuracy</b>: 98.1% <b>Loss</b>: 0.1882. Second Model: <b>Accuracy</b>: 98.5% <b>Loss</b>: 0.0997. Third Model: <b>Accuracy</b>: 99.1% <b>Loss</b>: 0.2544. My questions are:", "dateLastCrawled": "2022-01-26T03:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding <b>Loss</b> Functions in Computer Vision! | by Sowmya ...", "url": "https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/ml-cheat-sheet/winning-at-<b>loss</b>-functions-2-important-<b>loss</b>-functions...", "snippet": "Most of the <b>loss</b> functions discussed in the previous article such as MSE or <b>L2</b> <b>loss</b>, MAE or L1 <b>loss</b>, cross-entropy <b>loss</b>, etc, <b>can</b> be applied between every pair of pixels of the prediction and ...", "dateLastCrawled": "2022-02-03T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Beyond <b>L2</b> <b>Loss</b> \u2014 How <b>we experiment with loss functions at</b> Lyft | by ...", "url": "https://eng.lyft.com/beyond-l2-loss-how-we-experiment-with-loss-functions-at-lyft-51f9303f5d2d", "isFamilyFriendly": true, "displayUrl": "https://eng.lyft.com/beyond-<b>l2</b>-<b>loss</b>-how-<b>we-experiment-with-loss-functions-at</b>-lyft-51f...", "snippet": "<b>Loss</b> functions <b>can</b> <b>be thought</b> of as trails through a mountain range that <b>can</b> lead to different peaks. Each trail varies in difficulty. The amount of training data is the amount of energy one has to hike up the mountain. The optimization method is the trail guide. The result of all this is a prediction model or a trained hiker. Therefore, to take any trail we need to consider the energy and have the right guide, just as for each <b>loss</b> function we need to consider the amount of training data ...", "dateLastCrawled": "2022-01-23T07:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions in <b>Machine</b> <b>Learning</b>: An Easy Overview(2021)", "url": "https://www.jigsawacademy.com/blogs/ai-ml/loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.<b>jigsawacademy</b>.com/blogs/ai-ml/<b>loss</b>-function", "snippet": "It has all the advantages of Huber <b>loss</b>, and some <b>Learning</b> algorithms like XGBoost use Newton\u2019s method to find the optimum. Conclusion. Above, we have mentioned the various types of <b>loss</b> function example, which will give a clear understanding of What is a <b>loss</b> function in <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2022-01-26T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>machine</b> <b>learning</b> - Interpretation of <b>Loss</b> and validation <b>Loss</b> in Keras ...", "url": "https://datascience.stackexchange.com/questions/93013/interpretation-of-loss-and-validation-loss-in-keras", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/93013/interpretation-of-<b>loss</b>-and...", "snippet": "And these are the plots for the <b>loss</b> (in Blue) and validation <b>loss</b> (in Red): The <b>loss</b> function (mse) is minimized after two epochs which I guess means that the model &#39;learned&#39; at the point. I don&#39;t understand however why the validation <b>loss</b> has huge fluctuations. I <b>thought</b> that it would have a similar distribution to the <b>loss</b> function.", "dateLastCrawled": "2022-01-26T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "linkedin-skill-assessments-quizzes/<b>machine</b>-<b>learning</b>-quiz.md at master ...", "url": "https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes/blob/master/machine-learning/machine-learning-quiz.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/.../blob/master/<b>machine</b>-<b>learning</b>/<b>machine</b>-<b>learning</b>-quiz.md", "snippet": "This extremely complex game is <b>thought</b> to have more gameplay possibilities than there are atoms of the universe. The first version of the <b>system</b> won by observing hundreds of thousands of hours of human gameplay; the second version learned how to play by getting rewards while playing against itself. How would you describe this transition to different <b>machine</b> <b>learning</b> approaches? The <b>system</b> went from supervised <b>learning</b> to reinforcement <b>learning</b>. The <b>system</b> evolved from supervised <b>learning</b> to ...", "dateLastCrawled": "2022-02-02T14:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Winter Quarter 2019 Stanford University - CS230 Deep <b>Learning</b>", "url": "https://cs230.stanford.edu/files/cs230exam_win19_soln.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs230.stanford.edu/files/cs230exam_win19_soln.pdf", "snippet": "of the L1 <b>loss</b>. Since even small weights are penalised the same amount as large weights, more weight values will tend closer to 0. <b>L2</b> on the other hand penalizes smaller weights less, which leads to smaller weights but does not ensure sparsity. (c) (2 points) You are designing a deep <b>learning</b> <b>system</b> to detect driver fatigue in cars.", "dateLastCrawled": "2022-01-25T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Active Optical Control with Machine Learning</b>: A Proof of Concept for ...", "url": "https://ml4physicalsciences.github.io/2020/files/NeurIPS_ML4PS_2020_147.pdf", "isFamilyFriendly": true, "displayUrl": "https://ml4physicalsciences.github.io/2020/files/NeurIPS_ML4PS_2020_147.pdf", "snippet": "The Active Optics <b>System</b> of the Vera C. Rubin Observatory (Rubin) uses infor-mation provided by four wavefront sensors to determine deviations between the reconstructed wavefront and the ideal wavefront. The observed deviations are used to adjust the control parameters of the optical <b>system</b> to maintain image quality across the 3.5 degree \ufb01eld of view. In this paper, we use deep <b>learning</b> methods to extract the control parameters directly from the images captured by the wavefront sensors ...", "dateLastCrawled": "2022-02-03T07:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>L2</b> <b>Vs L1 Regularization in Xgboost</b>? : MLQuestions", "url": "https://www.reddit.com/r/MLQuestions/comments/77r9lu/l2_vs_l1_regularization_in_xgboost/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MLQuestions/comments/77r9lu/<b>l2</b>_<b>vs_l1_regularization_in_xgboost</b>", "snippet": "The objective is then equal to the sum of the two losses. Your regularisation <b>loss</b> <b>can</b> take a number of forms, e.g. it could be the L1 <b>loss</b>, the <b>L2</b> <b>loss</b>, whatever. I believe that for XGBoost, the regularization <b>loss</b> is given as a * L1 + b* <b>L2</b>, where a and b are your respective coefficients. XGBoost then learns the best trees that minimise that ...", "dateLastCrawled": "2022-01-30T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>LinkedIn: Machine Learning | Skill Assessment Quiz Solutions</b>", "url": "https://www.apdaga.com/2021/03/linkedin-machine-learning-skill-assessment-quiz-solutions.html", "isFamilyFriendly": true, "displayUrl": "https://www.apdaga.com/2021/03/<b>linkedin-machine-learning-skill-assessment-quiz</b>...", "snippet": "Click here to see solutions for all HackerRank SQL practice questions. &amp; Click here to see solutions for all <b>Machine</b> <b>Learning</b> Coursera Assignments. &amp; Click here to see more codes for Raspberry Pi 3 and similar Family. &amp; Click here to see more codes for NodeMCU ESP8266 and similar Family. &amp; Click here to see more codes for Arduino Mega (ATMega 2560) and similar Family. Feel free to ask doubts in the comment section. I will try my best to answer it. If you find this helpful by any mean like ...", "dateLastCrawled": "2022-01-30T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "17 <b>Common Issues In Machine Learning: Simplified</b>", "url": "https://www.jigsawacademy.com/blogs/ai-ml/issues-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.jigsawacademy.com/blogs/ai-ml/issues-in-<b>machine</b>-<b>learning</b>", "snippet": "<b>Machine</b> <b>Learning</b> or ML is one of the most successful applications of Artificial intelligence which provides systems with automated <b>learning</b> without being constantly programmed. It has acquired a ton of noticeable quality lately due to its capacity to be applied across scores of ventures to tackle complex issues quickly and effectively. From Digital assistants that play your music to the products being recommended based on prior search, <b>Machine</b> <b>Learning</b> has taken over many aspects of our life ...", "dateLastCrawled": "2022-02-02T14:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "L1 and <b>L2: loss function and regularization</b> | Develop Paper", "url": "https://developpaper.com/l1-and-l2-loss-function-and-regularization/", "isFamilyFriendly": true, "displayUrl": "https://developpaper.com/l1-and-<b>l2-loss-function-and-regularization</b>", "snippet": "We often see an additional term added after the <b>loss</b> function, which is usually L1 norm, <b>L2</b> norm, which is called L1 regularization and <b>L2</b> regularization in Chinese, or L1 norm and <b>L2</b> function. L1 regularization and <b>L2</b> regularization <b>can</b> be regarded as penalty terms of <b>loss</b> function. The so-called \u201cpunishment\u201d refers to the limitation of ...", "dateLastCrawled": "2022-02-03T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "Types of <b>Loss Functions in Machine Learning</b>. Below are the different types of the <b>loss</b> function in <b>machine</b> <b>learning</b> which are as follows: 1. Regression <b>loss</b> functions. Linear regression is a fundamental concept of this function. Regression <b>loss</b> functions establish a linear relationship between a dependent variable (Y) and an independent ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "L1 vs <b>L2</b> <b>Regularization</b>: The intuitive difference | by Dhaval Taunk ...", "url": "https://medium.com/analytics-vidhya/l1-vs-l2-regularization-which-is-better-d01068e6658c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/l1-vs-<b>l2</b>-<b>regularization</b>-which-is-better-d01068e6658c", "snippet": "As we <b>can</b> see from the formula of L1 and <b>L2</b> <b>regularization</b>, L1 <b>regularization</b> adds the penalty term in cost function by adding the absolute value of weight (Wj) parameters, while <b>L2</b> <b>regularization</b> ...", "dateLastCrawled": "2022-01-29T09:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Ridge and Lasso <b>Regression (L1 and L2 regularization</b>) Explained Using ...", "url": "https://www.excelr.com/blog/data-science/regression/l1_and_l2_regularization", "isFamilyFriendly": true, "displayUrl": "https://www.excelr.com/blog/data-science/<b>regression/l1_and_l2_regularization</b>", "snippet": "In the domain of <b>machine</b> <b>learning</b>, regularization is the process which prevents overfitting by discouraging developers <b>learning</b> a more complex or flexible model, and finally, which regularizes or shrinks the coefficients towards zero. The basic idea is to penalize the complex models i.e. adding a complexity term in such a way that it tends to give a bigger <b>loss</b> for evaluating complex models.", "dateLastCrawled": "2022-02-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions in Neural Networks - theaidream.com", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> Functions in <b>Machine</b> <b>Learning</b>: An Easy Overview(2021)", "url": "https://www.jigsawacademy.com/blogs/ai-ml/loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.<b>jigsawacademy</b>.com/blogs/ai-ml/<b>loss</b>-function", "snippet": "It has all the advantages of Huber <b>loss</b>, and some <b>Learning</b> algorithms like XGBoost use Newton\u2019s method to find the optimum. Conclusion. Above, we have mentioned the various types of <b>loss</b> function example, which will give a clear understanding of What is a <b>loss</b> function in <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2022-01-26T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Neural networks are trained using stochastic gradient descent and require that you choose a <b>loss</b> function when designing and configuring your model. There are many <b>loss</b> functions to choose from and it <b>can</b> be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when training a neural network. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Choose <b>Loss</b> Functions When Training Deep <b>Learning</b> Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "model.compile(<b>loss</b>=&#39;...&#39;, optimizer=opt) # fit model. history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0) Now that we have the basis of a problem and model, we <b>can</b> take a look evaluating three common <b>loss</b> functions that are appropriate for a regression predictive modeling problem.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "Newton\u2019s method is seldom used in <b>machine</b> <b>learning</b> because a. common <b>loss</b> functions are not self-concordant b. Newton\u2019s method does not work well on noisy data c. <b>machine</b> <b>learning</b> researchers don\u2019t really understand linear algebra d. it is generally not practical to form or store the Hessian in such problems, due to large problem size answer : d. <b>Learning</b> with Regression and trees. Module 04. 1. In practice, Line of best fit or regression line is found when _____ a) Sum of residuals ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "linkedin-skill-assessments-quizzes/<b>machine</b>-<b>learning</b>-quiz.md at master ...", "url": "https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes/blob/master/machine-learning/machine-learning-quiz.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/.../blob/master/<b>machine</b>-<b>learning</b>/<b>machine</b>-<b>learning</b>-quiz.md", "snippet": "The supervisor asks to create a <b>machine</b> <b>learning</b> <b>system</b> that will help your hr dep. classify job applicants into well-defined groups.What type of <b>system</b> are more likely to recommend? Q49. Someone of your data science team recommends that you use decision trees, naive Bayes and K-nearest neighbor, all at the same time, on the same training data, and then average the results.", "dateLastCrawled": "2022-02-02T14:44:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Regularization \u2014 Understanding L1 and <b>L2</b> regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-<b>l2</b>...", "snippet": "Understanding what regularization is and why it is required for <b>machine</b> <b>learning</b> and diving deep to clarify the importance of L1 and <b>L2</b> regularization in Deep <b>learning</b>.", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Beyond <b>L2</b> <b>Loss</b> \u2014 How <b>we experiment with loss functions at</b> Lyft | by ...", "url": "https://eng.lyft.com/beyond-l2-loss-how-we-experiment-with-loss-functions-at-lyft-51f9303f5d2d", "isFamilyFriendly": true, "displayUrl": "https://eng.lyft.com/beyond-<b>l2</b>-<b>loss</b>-how-<b>we-experiment-with-loss-functions-at</b>-lyft-51f...", "snippet": "To provide reliable and accurate ETA estimates, the Mapping team built a sophisticated system that includes a <b>machine</b> <b>learning</b> ETA prediction model. The main component in any prediction model is the \u201c<b>loss</b>\u201d (or \u201cobjective\u201d) function, and choosing the right <b>loss</b> function for the data can improve prediction accuracy. This post discusses a recently developed method that generates and compares <b>loss</b> functions that are suitable for calculating averages, such as the ETA.", "dateLastCrawled": "2022-01-23T07:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "is known as L1-norm, while the latter is known as the <b>L2</b>-norm. Keep in mind that <b>L2</b>-norm is more sensitive than L1-norm to large-valued outliers. Ridge and LASSO regularizations are based on <b>L2</b>-norm and L1-norm, respectively, while Elastic Net regularization is based on the mix of two. 2.6 What does a <b>machine</b> <b>learning</b> <b>learning</b>-curve measure ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, the optimizer , which in this mountain <b>analogy</b> roughly describes stochastic gradient descent (SGD) optimization, continually tries new weights and biases until it reaches its goal of finding the optimal values for the model to make accurate predictions.", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Why <b>Machine Learning Is A Metaphor For</b> Life \u2013 Adit Deshpande ...", "url": "https://adeshpande3.github.io/Why-Machine-Learning-is-a-Metaphor-For-Life", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/Why-<b>Machine-Learning-is-a-Metaphor-For</b>-Life", "snippet": "Well, in <b>machine</b> <b>learning</b>, we use regularizers. The first form (and the most common) of regularization that I first learned about was <b>L2</b> regularization or weight decay. This type of regularization is basically imposing a soft constraint on the cost function. We\u2019re telling the network \u201cHey, we want you to minimize the <b>loss</b> from the training ...", "dateLastCrawled": "2022-01-31T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective function to perform maximum or minimum evaluation. In reality, optimization is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Orthogonalization - Adjust one knob to adjust one parameter, to solve one problem - The TV knob <b>analogy</b> and the car <b>analogy</b>. Chain of assumptions in <b>Machine</b> <b>Learning</b> and different knobs to say improve performance on train/dev set. Andrew Ng does not recommend Early stopping, as it is a knob that affects multiple thing at once. Setting up your goal", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are some <b>examples in everyday life analogous to &#39;overfitting</b>&#39; in ...", "url": "https://www.quora.com/What-are-some-examples-in-everyday-life-analogous-to-overfitting-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-some-<b>examples-in-everyday-life-analogous-to-overfitting</b>...", "snippet": "Answer (1 of 3): Exam overfitting - When you study for an exam, only by practicing questions from previous years&#39; exams. You then discover to your horror that xx% of this year&#39;s questions are new, and you get a much lower score than on your practice ones. If you are a bit older, you can expand th...", "dateLastCrawled": "2022-01-06T06:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[D] Looking for papers on treating regression as classification vs ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7gun87/d_looking_for_papers_on_treating_regression_as/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7gun87/d_looking_for_papers_on...", "snippet": "Doing the <b>L2 loss is like</b> doing maximum likelihood on a gaussian with a fixed variance - so the bad regression here is largely coming from the gaussian being mis-specified. I think the richer question would involve comparing approaches that consider the ordering vs. approaches that don t consider the ordering but where both have flexible enough distributions.", "dateLastCrawled": "2021-01-17T22:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep learning</b> - lectures.alex.balgavy.eu", "url": "https://lectures.alex.balgavy.eu/ml-notes/deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://lectures.alex.balgavy.eu/ml-notes/<b>deep-learning</b>", "snippet": "<b>Deep learning</b> <b>Deep learning</b> systems (autodiff engines) Tensors. To scale up backpropagation, want to move from operations on scalars to tensors. Tensor: generalisation of vectors/matrices to higher dimensions. e.g. a 2-tensor has two dimensions, a 4-tensor has 4 dimensions. You can represent data as a tensor. e.g. an RGB image is a 3-tensor of the red, green, and blue values for each pixel. Functions on tensors. Functions have inputs and outputs, all of which are tensors. They implement ...", "dateLastCrawled": "2021-12-15T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A deep <b>learning</b> framework for constitutive modeling based on temporal ...", "url": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "snippet": "These two features meet the requirement for sequence modeling in <b>machine</b> <b>learning</b>. Therefore, the nonlinear constitutive models may be classified as sequence modeling from the viewpoint of <b>machine</b> <b>learning</b>. Concrete material and steel material both exhibit significant ultra-long-term memory effects and many model-driven constitutive relationships were developed to simulate stress-strain curves of materials , , , , with ultra-long-term memory effect. For steel material, the traditional ...", "dateLastCrawled": "2022-01-20T12:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(l2 loss)  is like +(error in a machine learning system)", "+(l2 loss) is similar to +(error in a machine learning system)", "+(l2 loss) can be thought of as +(error in a machine learning system)", "+(l2 loss) can be compared to +(error in a machine learning system)", "machine learning +(l2 loss AND analogy)", "machine learning +(\"l2 loss is like\")", "machine learning +(\"l2 loss is similar\")", "machine learning +(\"just as l2 loss\")", "machine learning +(\"l2 loss can be thought of as\")", "machine learning +(\"l2 loss can be compared to\")"]}