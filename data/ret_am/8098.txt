{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Nonnegative <b>Matrix</b> <b>Factorization</b>: An Analytical and Interpretive Tool ...", "url": "https://www.ncbi.nlm.nih.gov/sites/ppmc/articles/PMC2447881/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/sites/ppmc/articles/PMC2447881", "snippet": "The NMF Approach. Lee and Seung , introduced NMF in its modern form as an unsupervised, parts-based learning paradigm in which a nonnegative <b>matrix</b> V is decomposed into two nonnegative matrices V\u223cWH by a multiplicative updates algorithm. They applied it for text mining and facial pattern recognition. Prior to Lee and Seung&#39;s work, a similar approach called positive <b>matrix</b> <b>factorization</b> from Paatero and Tapper was applied as a dimension reduction tool to problems in the environmental ...", "dateLastCrawled": "2021-11-06T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Nonnegative <b>Matrix</b> <b>Factorization</b>: An Analytical and Interpretive Tool ...", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000029", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000029", "snippet": "Nonnegative <b>matrix</b> <b>factorization</b> (NMF) was introduced as an unsupervised, parts-based learning paradigm involving the decomposition of a nonnegative <b>matrix</b> V into two nonnegative matrices, W and H, via a multiplicative updates algorithm. In the context of a p\u00d7n gene expression <b>matrix</b> V consisting of observations on p genes from n samples, each column of W defines a metagene, and each column of H represents the metagene expression pattern of the corresponding sample. NMF has been primarily ...", "dateLastCrawled": "2021-11-02T01:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "[PDF] Discovering <b>Hidden</b> Structure in High Dimensional Human Behavioral ...", "url": "https://www.semanticscholar.org/paper/Discovering-Hidden-Structure-in-High-Dimensional-Hosseinmardi-Kao/52cfc69207eea3c485fdcfef1d7f28a0ebd57029", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/Discovering-<b>Hidden</b>-Structure-in-High-Dimensional...", "snippet": "Rich multimodal data, from wearables <b>like</b> Fitbit, online social networks, mobile phones etc. can be collected in natural environments. <b>Uncovering</b> the underlying low-dimensional structure of noisy multi-way data in an unsupervised setting is a challenging problem. Tensor <b>factorization</b> has been successful in extracting the interconnected low-dimensional descriptions of multi-way data. In this paper, we apply non-negative tensor <b>factorization</b> on a real-word wearable sensor data, StudentLife, to ...", "dateLastCrawled": "2021-10-03T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "OPEN SparRec: <b>An efective matrix completion framework of missing</b> data ...", "url": "https://par.nsf.gov/servlets/purl/10064362", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10064362", "snippet": "We wo uld <b>like</b> to point out that the low-rank <b>matrix</b> completion (LRMC) model is similar to Mendel-Impute, but the <b>matrix</b> co-clustering <b>factorization</b> (MCCF) model is completely new. We will illustrate how our approach i s able to efec - tively ind <b>patterns</b> for imputation within study data, both with and without reference pan els, and even with data", "dateLastCrawled": "2021-09-19T01:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "SparRec: An effective <b>matrix</b> completion framework of missing data ...", "url": "https://www.nature.com/articles/srep35534", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/srep35534", "snippet": "In (3), the Frobenius norm of a <b>matrix</b> X is defined as Our imputation approach, based on the <b>matrix</b> co-clustering <b>factorization</b>, aims to complete <b>matrix</b> M by using a low-rank <b>matrix</b> <b>factorization</b> ...", "dateLastCrawled": "2021-11-25T05:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "InterSIM: Simulation tool for multiple integrative &#39;omic datasets ...", "url": "https://www.semanticscholar.org/paper/InterSIM%3A-Simulation-tool-for-multiple-integrative-Chalise-Raghavan/5a679debb399766d7536fe41a949d439e4a7640c", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/InterSIM:-Simulation-tool-for-multiple...", "snippet": "This study provides an useful protocol for <b>uncovering</b> <b>hidden</b> <b>patterns</b> and their biological implications in multi-dimensional \u2018omic\u2019 data by adopting a joint <b>matrix</b> <b>factorization</b> technique to address the challenge of integrative analysis ofMulti-dimensional genomics data for the discovery of combinatorial <b>patterns</b>. Expand. 240. PDF. Save. Alert. Integrative Model-based clustering of microarray methylation and expression data. Matthias Kormaksson, J. Booth, M. Figueroa, A. Melnick ...", "dateLastCrawled": "2021-11-05T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Implicit Feedback Recommendation for Plant-pollinator Networks", "url": "https://agsci.oregonstate.edu/sites/agscid7/files/xxx/2014-lambertj_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://agsci.oregonstate.edu/sites/agscid7/files/xxx/2014-lambertj_paper.pdf", "snippet": "This method applies <b>matrix</b> <b>factorization</b> to decompose the original <b>matrix</b> into two latent factors, one for users and one for items. Latent factor models attempt to explain rat-ings or preferences by charaterizing items and users on a set of factors inferred from the rat-ing or preference <b>patterns</b> [9]. For music, the", "dateLastCrawled": "2021-08-10T01:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[Frontiers in Bioscience 13, 263-275, January 1, 2008] Inferring ...", "url": "http://sli.ics.uci.edu/pmwiki/uploads/Ihler-Private/2677.pdf", "isFamilyFriendly": true, "displayUrl": "sli.ics.uci.edu/pmwiki/uploads/Ihler-Private/2677.pdf", "snippet": "Non-negative <b>matrix</b> <b>factorization</b> (NMF) is a recently developedmachine learning technique, capable of finding smaller and more localized <b>patterns</b> as well as global <b>patterns</b>. The approach can be particularly useful in identifying biological subsystems (i.e., sets of genes that function in concert in a relatively tightly regulated manner) (61, 62).", "dateLastCrawled": "2021-08-24T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Detecting the community structure and activity <b>patterns</b> of temporal ...", "url": "https://www.europepmc.org/articles/PMC3908891/", "isFamilyFriendly": true, "displayUrl": "https://www.europepmc.org/articles/PMC3908891", "snippet": "<b>Uncovering</b> Latent Structures by Tensor <b>Factorization</b>. The tensor , where is the number of nodes of the network and the number of network snapshots, encodes both the topological and temporal information on the network under study. <b>Uncovering</b> structures that may correspond to communities or correlated activity <b>patterns</b> requires the identification and extraction of lower-dimensional factors. To this end, we use tensor <b>factorization</b> techniques, i.e., we choose to represent the tensor as a ...", "dateLastCrawled": "2022-01-17T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "User-based representation of time-resolved multimodal public ...", "url": "https://royalsocietypublishing.org/doi/10.1098/rsos.160156", "isFamilyFriendly": true, "displayUrl": "https://royalsocietypublishing.org/doi/10.1098/rsos.160156", "snippet": "After selecting privileged connections, we apply non-negative <b>matrix</b> <b>factorization</b> (NMF) to the graph of the privileged connections to identify underlying <b>patterns</b>, which may not be present due to overall design. Finally, we compare our findings with independent measures of commuting <b>patterns</b>, which allow us to give an estimation of the efficiency of the PT systems.", "dateLastCrawled": "2021-12-19T03:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Nonnegative <b>Matrix</b> <b>Factorization</b>: An Analytical and Interpretive Tool ...", "url": "https://www.ncbi.nlm.nih.gov/sites/ppmc/articles/PMC2447881/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/sites/ppmc/articles/PMC2447881", "snippet": "The NMF Approach. Lee and Seung , introduced NMF in its modern form as an unsupervised, parts-based learning paradigm in which a nonnegative <b>matrix</b> V is decomposed into two nonnegative matrices V\u223cWH by a multiplicative updates algorithm. They applied it for text mining and facial pattern recognition. Prior to Lee and Seung&#39;s work, a <b>similar</b> approach called positive <b>matrix</b> <b>factorization</b> from Paatero and Tapper was applied as a dimension reduction tool to problems in the environmental ...", "dateLastCrawled": "2021-11-06T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Uncovering</b> <b>Hidden</b> Structure through Parallel Problem Decomposition for ...", "url": "https://www.cs.purdue.edu/homes/yexiang/publications/ijcai15_set_basis.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.purdue.edu/homes/yexiang/publications/ijcai15_set_basis.pdf", "snippet": "special type of <b>matrix</b> <b>factorization</b> technique [Miettinen et al., 2008]. It also has applications in system security and pro-tection, where it is referred to as the role mining problem in access control [Vaidya et al., 2007]. It also has applications in secure broadcasting [Shu et al., 2006] and computational biology [Nau et al., 1978]. While having many natural applications, our work is mo-tivated by a novel application in the \ufb01eld of computational sustainability [Gomes, Winter 2009 ...", "dateLastCrawled": "2021-09-19T06:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Finding the Number of Latent Topics With Semantic Non-Negative <b>Matrix</b> ...", "url": "https://www.osti.gov/pages/biblio/1836317", "isFamilyFriendly": true, "displayUrl": "https://www.osti.gov/pages/biblio/1836317", "snippet": "Non-negative <b>Matrix</b> <b>Factorization</b> (NMF) has proven to be a powerful unsupervised learning method for <b>uncovering</b> <b>hidden</b> features in complex and noisy datasets with applications in data mining, text recognition, dimension reduction, face recognition, anomaly detection, blind source separation, and many other fields. An important input for NMF is the latent dimensionality of the data, that is, the number of <b>hidden</b> features, K, present in the explored dataset. Unfortunately, and this quantity is ...", "dateLastCrawled": "2022-01-29T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "OPEN SparRec: <b>An efective matrix completion framework of missing</b> data ...", "url": "https://par.nsf.gov/servlets/purl/10064362", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10064362", "snippet": "our low-rank <b>matrix</b> completion (LRMC) model <b>is similar</b> to Mendel-Impute, our <b>matrix</b> co-clustering <b>factorization</b> (MCCF) model is completely new. (2) SparRec, as other <b>matrix</b> completion methods, is lexible to be applied to missing data imputation for large meta-analysis with diferent cohorts genotyped on diferent sets of SNPs, even when there is no reference panel. This kind of meta-analysis is very challenging for current statistics based methods. (3) SparRec has consistent performance and ...", "dateLastCrawled": "2021-09-19T01:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Detecting heterogeneity in single-cell RNA-Seq data by non-negative ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5251935/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5251935", "snippet": "Here we investigate the performance of non-negative <b>matrix</b> <b>factorization</b> (NMF) method to analyze a wide variety of scRNA-Seq datasets, ranging from mouse hematopoietic stem cells to human glioblastoma data. In comparison to other unsupervised clustering methods including K-means and hierarchical clustering, NMF has higher accuracy in separating <b>similar</b> groups in various datasets. We ranked genes by their importance scores D-scores) in separating these groups, and discovered that NMF uniquely ...", "dateLastCrawled": "2021-12-17T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A neural network for determination of latent dimensionality in ...", "url": "https://www.osti.gov/pages/biblio/1716806", "isFamilyFriendly": true, "displayUrl": "https://www.osti.gov/pages/biblio/1716806", "snippet": "Non-negative <b>Matrix</b> <b>Factorization</b> (NMF) has proven to be a powerful unsupervised learning method for <b>uncovering</b> <b>hidden</b> features in complex and noisy datasets with applications in data mining, text recognition, dimension reduction, face recognition, anomaly detection, blind source separation, and many other fields. An important input for NMF is the latent dimensionality of the data, that is, the number of <b>hidden</b> features, K, present in the explored dataset. Unfortunately, and this quantity is ...", "dateLastCrawled": "2022-01-22T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "SparRec: An effective <b>matrix</b> completion framework of missing data ...", "url": "https://www.nature.com/articles/srep35534", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/srep35534", "snippet": "In (3), the Frobenius norm of a <b>matrix</b> X is defined as Our imputation approach, based on the <b>matrix</b> co-clustering <b>factorization</b>, aims to complete <b>matrix</b> M by using a low-rank <b>matrix</b> <b>factorization</b> ...", "dateLastCrawled": "2021-11-25T05:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[Frontiers in Bioscience 13, 263-275, January 1, 2008] Inferring ...", "url": "http://sli.ics.uci.edu/pmwiki/uploads/Ihler-Private/2677.pdf", "isFamilyFriendly": true, "displayUrl": "sli.ics.uci.edu/pmwiki/uploads/Ihler-Private/2677.pdf", "snippet": "Non-negative <b>matrix</b> <b>factorization</b> (NMF) is a recently developedmachine learning technique, capable of finding smaller and more localized <b>patterns</b> as well as global <b>patterns</b>. The approach can be particularly useful in identifying biological subsystems (i.e., sets of genes that function in concert in a relatively tightly regulated manner) (61, 62).", "dateLastCrawled": "2021-08-24T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GitHub</b> - aliizadi/Data-Science-with-Python: Including all python codes ...", "url": "https://github.com/aliizadi/Data-Science-with-Python", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/aliizadi/Data-Science-with-Python", "snippet": "This is the world of unsupervised learning, called as such because you are not guiding, or supervising, the pattern discovery by some prediction task, but instead <b>uncovering</b> <b>hidden</b> structure from unlabeled data. Unsupervised learning encompasses a variety of techniques in machine learning, from clustering to dimension reduction to <b>matrix</b> <b>factorization</b>. In this course, you&#39;ll learn the fundamentals of unsupervised learning and implement the essential algorithms using scikit-learn and scipy ...", "dateLastCrawled": "2021-11-11T04:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Detecting the Community Structure and Activity <b>Patterns</b> of Temporal ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0086028", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0086028", "snippet": "Similarly, groups of nodes may exist that share <b>similar</b> activity <b>patterns</b> over time (due to, e.g., an externally imposed activity schedule): an aggregated view on the network will only retain the topology of the interactions and lose the activity <b>patterns</b> and temporal correlations. Overall, detecting structures that involve topological features and correlated activity <b>patterns</b> over time is an outstanding challenge that bears relevance to many fields of research and needs a principled ...", "dateLastCrawled": "2022-01-27T09:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Neural Network for Determination of Latent Dimensionality in ...", "url": "https://deepai.org/publication/a-neural-network-for-determination-of-latent-dimensionality-in-nonnegative-matrix-factorization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-neural-network-for-determination-of-latent...", "snippet": "Non-negative <b>Matrix</b> <b>Factorization</b> (NMF) has proven to be a powerful unsupervised learning method for <b>uncovering</b> <b>hidden</b> features in complex and noisy data sets with applications in data mining, text recognition, dimension reduction, face recognition, anomaly detection, blind source separation, and many other fields. An important input for NMF is the latent dimensionality of the data, that is, the number of <b>hidden</b> features, K, present in the explored data set. Unfortunately, this quantity is ...", "dateLastCrawled": "2021-12-01T06:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) A Neural Network for Determination of Latent Dimensionality in ...", "url": "https://www.researchgate.net/publication/342378085_A_Neural_Network_for_Determination_of_Latent_Dimensionality_in_Nonnegative_Matrix_Factorization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/342378085_A_Neural_Network_for_Determination...", "snippet": "Non-negative <b>Matrix</b> <b>Factorization</b> (NMF) has proven to be a powerful unsupervised learning method for <b>uncovering</b> <b>hidden</b> features in complex and noisy data sets with applications in data mining ...", "dateLastCrawled": "2021-12-30T18:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Incremental Multiresolution <b>Matrix</b> <b>Factorization</b> Algorithm ...", "url": "https://europepmc.org/articles/PMC5798492", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/articles/PMC5798492", "snippet": "Consider a symmetric <b>matrix</b> C m\u00d7m.PCA decomposes C as Q T \u039bQ where Q is an orthogonal <b>matrix</b>, which, in general, is dense. On the other hand, sparse PCA (sPCA) [] imposes sparsity on the columns of Q, allowing for fewer dimensions to interact that may not capture global <b>patterns</b>.The <b>factorization</b> resulting from such individual low-rank decompositions cannot capture hierarchical relationships among data dimensions.", "dateLastCrawled": "2021-10-15T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Unsupervised learning explained</b> | <b>InfoWorld</b>", "url": "https://www.infoworld.com/article/3429017/unsupervised-learning-explained.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.infoworld.com</b>/article/3429017", "snippet": "<b>Uncovering</b> <b>patterns</b> rather than carrying out a pre-defined task <b>can</b> yield surprising and useful results, as demonstrated when researchers at Lawrence Berkeley Lab ran a text processing algorithm ...", "dateLastCrawled": "2022-01-30T06:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Definitions, methods, and applications in interpretable machine ... - <b>PNAS</b>", "url": "https://www.pnas.org/content/116/44/22071", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/116/44/22071", "snippet": "In particular, it uses stability-driven nonnegative <b>matrix</b> <b>factorization</b> to decompose images of complex spatial gene expression <b>patterns</b> into a library of 21 \u201cprincipal <b>patterns</b>,\u201d which <b>can</b> be viewed as preorgan regions. This decomposition, which is interpretable to biologists, allows the study of gene\u2013gene interactions in preorgan regions of the developing embryo.", "dateLastCrawled": "2022-02-02T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Probabilistic Topic Models</b> | April 2012 | Communications of the ACM", "url": "https://cacm.acm.org/magazines/2012/4/147361-probabilistic-topic-models/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2012/4/147361-<b>probabilistic-topic-models</b>", "snippet": "The central computational problem for topic modeling is to use the observed documents to infer the <b>hidden</b> topic structure. This <b>can</b> <b>be thought</b> of as &quot;reversing&quot; the generative processwhat is the <b>hidden</b> structure that likely generated the observed collection? Figure 2 illustrates example inference using the same example document from Figure 1. Here, we took 17,000 articles from Science magazine and used a topic modeling algorithm to infer the <b>hidden</b> topic structure. (The algorithm assumed ...", "dateLastCrawled": "2022-01-29T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding Genotype-Phenotype Effects in Cancer via Network Approaches", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004747", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004747", "snippet": "The topic model-based method <b>can</b> <b>be thought</b> of as a subtype-centric approach, as it allows probabilistic disease subtypes to be defined based on the frequencies of putative causal features while representing each patient as a mixture of these subtypes. On the other hand, the network fusion algorithm is a more patient-centric approach, which allows, for each patient, other individuals that are most similar to the patient in question to be identified and therefore enabling the information from ...", "dateLastCrawled": "2021-12-04T12:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Artificial intelligence in drug design: algorithms, applications ...", "url": "https://www.future-science.com/doi/10.4155/fdd-2020-0028", "isFamilyFriendly": true, "displayUrl": "https://www.future-science.com/doi/10.4155/fdd-2020-0028", "snippet": "Probabilistic <b>matrix</b> <b>factorization</b> was employed in the prediction of drug\u2013target interactions . This is a method that needs large amounts of data input for accurate predictions and is therefore potentially good for applications with a plethora of data (e.g., enzymes and ion channels). However, neither G-protein-coupled receptors nor nuclear receptors are good systems to be studied using this method given the scarcity of sufficient data to feed the prediction platform.", "dateLastCrawled": "2022-01-29T08:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Application of Graph Theory for Identifying Connectivity <b>Patterns</b> in ...", "url": "https://www.frontiersin.org/articles/10.3389/fnins.2019.00585/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fnins.2019.00585", "snippet": "In graph theory, an N \u00d7 N adjacency <b>matrix</b> (also called a connection <b>matrix</b>) with the elements of zero or non-zero indicates the absence or presence of a relationship between the vertices of a network with N nodes. By extracting different metrics from this <b>matrix</b>, one <b>can</b> obtain a topological analysis of the desired graph (e.g., the human brain network). A brain graph may be classified as either directed or undirected", "dateLastCrawled": "2022-02-02T21:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Center for Theoretical Neuroscience | <b>Publications</b>", "url": "http://www.columbia.edu/cu/neurotheory/publications.html", "isFamilyFriendly": true, "displayUrl": "<b>www.columbia.edu</b>/cu/neurotheory/<b>publications</b>.html", "snippet": "These quantities <b>can</b> also <b>be thought</b> of as characterizing the stability and the magnitude of the linear response of a nonlinear network to small perturbations about a fixed point. We derive these formulae and work them out analytically for some examples of M, L and R motivated by neurobiological models. We also argue that the persistence as N goes to infinity of a finite number of randomly distributed outlying eigenvalues outside the support of the eigenvalue density of A, as previously ...", "dateLastCrawled": "2021-10-15T07:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[PDF] A non-negative <b>matrix</b> <b>factorization</b> method for detecting modules ...", "url": "https://www.semanticscholar.org/paper/A-non-negative-matrix-factorization-method-for-in-Yang-Michailidis/eda066fed23fdb472978f845c124252e4fdf03a7", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/A-non-negative-<b>matrix</b>-<b>factorization</b>-method-for...", "snippet": "Nonnegative <b>matrix</b> <b>factorization</b> is described, an algorithm based on decomposition by parts that <b>can</b> reduce the dimension of expression data from thousands of genes to a handful of metagenes, and found less sensitive to a priori selection of genes or initial conditions and able to detect alternative or context-dependent <b>patterns</b> of gene expression in complex biological systems.", "dateLastCrawled": "2021-11-11T00:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Nonnegative <b>Matrix</b> <b>Factorization</b>: An Analytical and Interpretive Tool ...", "url": "https://www.ncbi.nlm.nih.gov/sites/ppmc/articles/PMC2447881/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/sites/ppmc/articles/PMC2447881", "snippet": "The NMF Approach. Lee and Seung , introduced NMF in its modern form as an unsupervised, parts-based learning paradigm in which a nonnegative <b>matrix</b> V is decomposed into two nonnegative matrices V\u223cWH by a multiplicative updates algorithm. They applied it for text mining and facial pattern recognition. Prior to Lee and Seung&#39;s work, a similar approach called positive <b>matrix</b> <b>factorization</b> from Paatero and Tapper was applied as a dimension reduction tool to problems in the environmental ...", "dateLastCrawled": "2021-11-06T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting Protein\u2013Protein Interactions Using Symmetric Logistic <b>Matrix</b> ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8253547/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8253547", "snippet": "<b>Matrix</b> <b>factorization</b> (MF) models present several advantages that may address these limitations. MF models have been highly popular in recommender systems due to their simplicity and superior performance and have proven to be efficiently parallelizable and scalable, thus facilitating the applications to large datasets. MF models have been successfully applied to predictions of drug\u2013target interactions, 34\u201338 synthetic lethality, 39 and PPIs 40 in previous studies. A major advantage of MF ...", "dateLastCrawled": "2022-01-27T05:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "SSMF: Shifting Seasonal <b>Matrix</b> <b>Factorization</b>", "url": "https://proceedings.neurips.cc/paper/2021/file/1fb2a1c37b18aa4611c3949d6148d0f8-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2021/file/1fb2a1c37b18aa4611c3949d6148d0f8-Paper.pdf", "snippet": "<b>Matrix</b> <b>Factorization</b> approach, namely SSMF, that <b>can</b> adaptively learn multiple seasonal <b>patterns</b> (called regimes), as well as switching between them. Our pro- posed method has the following properties: (a) it accurately forecasts future events by detecting regime shifts in seasonal <b>patterns</b> as the data stream evolves; (b) it works in an online setting, i.e., processes each observation in constant time and memory; (c) it effectively realizes regime shifts without human intervention by using a ...", "dateLastCrawled": "2022-01-31T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A neural network for determination of latent dimensionality in ...", "url": "https://www.osti.gov/pages/biblio/1716806", "isFamilyFriendly": true, "displayUrl": "https://www.osti.gov/pages/biblio/1716806", "snippet": "Non-negative <b>Matrix</b> <b>Factorization</b> (NMF) has proven to be a powerful unsupervised learning method for <b>uncovering</b> <b>hidden</b> features in complex and noisy datasets with applications in data mining, text recognition, dimension reduction, face recognition, anomaly detection, blind source separation, and many other fields. An important input for NMF is the latent dimensionality of the data, that is, the number of <b>hidden</b> features, K, present in the explored dataset. Unfortunately, and this quantity is ...", "dateLastCrawled": "2022-01-22T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Nonnegative <b>Matrix</b> <b>Factorization</b>: An Analytical and Interpretive Tool ...", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000029", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000029", "snippet": "Nonnegative <b>matrix</b> <b>factorization</b> (NMF) was introduced as an unsupervised, parts-based learning paradigm involving the decomposition of a nonnegative <b>matrix</b> V into two nonnegative matrices, W and H, via a multiplicative updates algorithm. In the context of a p\u00d7n gene expression <b>matrix</b> V consisting of observations on p genes from n samples, each column of W defines a metagene, and each column of H represents the metagene expression pattern of the corresponding sample. NMF has been primarily ...", "dateLastCrawled": "2021-11-02T01:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Additional Neural <b>Matrix</b> <b>Factorization</b> model for computational drug ...", "url": "https://europepmc.org/article/MED/31412762", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/31412762", "snippet": "<b>Uncovering</b> the <b>hidden</b> features of both drugs and diseases is made possible by the use of a deep learning technique, Additional Stacked ... <b>Compared</b> with the state-of-the-art models, the ANMF model is shown to be more valid. We <b>can</b> summarize the main contributions of this paper as follows: (1) A novel Additional Neural <b>Matrix</b> <b>Factorization</b> (ANMF) model is proposed for drug repositioning. The model combines deep learning representation with the nonlinear <b>matrix</b> <b>factorization</b> technique, and ...", "dateLastCrawled": "2021-10-29T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Neural Network for Determination of Latent Dimensionality in ...", "url": "https://deepai.org/publication/a-neural-network-for-determination-of-latent-dimensionality-in-nonnegative-matrix-factorization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-neural-network-for-determination-of-latent...", "snippet": "Non-negative <b>Matrix</b> <b>Factorization</b> (NMF) has proven to be a powerful unsupervised learning method for <b>uncovering</b> <b>hidden</b> features in complex and noisy data sets with applications in data mining, text recognition, dimension reduction, face recognition, anomaly detection, blind source separation, and many other fields. An important input for NMF is the latent dimensionality of the data, that is, the number of <b>hidden</b> features, K, present in the explored data set. Unfortunately, this quantity is ...", "dateLastCrawled": "2021-12-01T06:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Interaction Dynamics in a Social Network Using <b>Hidden</b> Markov Model", "url": "https://www.scirp.org/journal/PaperInformation.aspx?PaperID=86045", "isFamilyFriendly": true, "displayUrl": "https://www.scirp.org/journal/PaperInformation.aspx?PaperID=86045", "snippet": "The initial state probabilities are estimated from the errors of the <b>matrix</b> <b>factorization</b> method when estimating the observation <b>matrix</b>. 3. <b>Hidden</b> Markov Model. The characterization of HMM is outlined [1] [21] . \u2022 The number of states ( M = 3 ) in the model with the set of states denoted as S = { S 1 , S 2 , S 3 } = { F , R , E } = { Forgetting , Reinforcement , Exploration } . \u2022 The state transition probability distribution A = { a i j } , where a i j = P [ q t + 1 = S j | q t = S i ...", "dateLastCrawled": "2022-01-28T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "SparRec: An effective <b>matrix</b> completion framework of missing data ...", "url": "https://www.nature.com/articles/srep35534", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/srep35534", "snippet": "In (3), the Frobenius norm of a <b>matrix</b> X is defined as Our imputation approach, based on the <b>matrix</b> co-clustering <b>factorization</b>, aims to complete <b>matrix</b> M by using a low-rank <b>matrix</b> <b>factorization</b> ...", "dateLastCrawled": "2021-11-25T05:51:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Matrix</b> <b>Factorization</b> for <b>Machine</b> <b>Learning</b>", "url": "https://machinelearningmastery.com/introduction-to-matrix-decompositions-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/introduction-to-<b>matrix</b>-decompositions-for-<b>machine</b>...", "snippet": "A common <b>analogy</b> for <b>matrix</b> decomposition is the factoring of numbers, such as the factoring of 10 into 2 x 5. For this reason, <b>matrix</b> decomposition is also called <b>matrix</b> <b>factorization</b>. Like factoring real values, there are many ways to decompose a <b>matrix</b>, hence there are a range of different <b>matrix</b> decomposition techniques. Two simple and widely used <b>matrix</b> decomposition methods are the LU <b>matrix</b> decomposition and the QR <b>matrix</b> decomposition. Next, we will take a closer look at each of ...", "dateLastCrawled": "2022-02-03T04:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "16.3. <b>Matrix</b> <b>Factorization</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://www.d2l.ai/chapter_recommender-systems/mf.html", "isFamilyFriendly": true, "displayUrl": "https://www.d2l.ai/chapter_recommender-systems/mf.html", "snippet": "<b>Matrix</b> <b>Factorization</b> [Koren et al., 2009] is a well-established algorithm in the recommender systems literature. The first version of <b>matrix</b> <b>factorization</b> model is proposed by Simon Funk in a famous blog post in which he described the idea of factorizing the interaction <b>matrix</b>. It then became widely known due to the Netflix contest which was held in 2006.", "dateLastCrawled": "2022-01-31T10:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to Matrices and <b>Matrix</b> Arithmetic for <b>Machine Learning</b>", "url": "https://machinelearningmastery.com/introduction-matrices-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/introduction-matrices-<b>machine-learning</b>", "snippet": "A likely first place you may encounter a <b>matrix</b> in <b>machine learning</b> is in model training data comprised of many rows and columns and often represented using the capital letter \u201cX\u201d. The geometric <b>analogy</b> used to help understand vectors and some of their operations does not hold with matrices. Further, a vector itself may be considered a <b>matrix</b> with one column and multiple rows. Often the dimensions of the <b>matrix</b> are denoted as m and n for the number of rows and the number of columns. Now ...", "dateLastCrawled": "2022-02-02T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "6 Math Foundations to Start <b>Learning</b> <b>Machine Learning</b> | by Cornellius ...", "url": "https://towardsdatascience.com/6-math-foundation-to-start-learning-machine-learning-1afef04f42bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/6-math-foundation-to-start-<b>learning</b>-<b>machine-learning</b>-1...", "snippet": "<b>Matrix</b> Decomposition aims to simplify more complex <b>matrix</b> operations on the decomposed <b>matrix</b> rather than on its original <b>matrix</b>. A common <b>analogy</b> for <b>matrix</b> decomposition is like factoring numbers, such as factoring 8 into 2 x 4. This is why <b>matrix</b> decomposition is synonymical to <b>matrix</b> <b>factorization</b>. There are many ways to decompose a <b>matrix</b> ...", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "16.9. <b>Factorization Machines</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_recommender-systems/fm.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_recommender-systems/fm.html", "snippet": "<b>Factorization machines</b> (FM) [Rendle, 2010], proposed by Steffen Rendle in 2010, is a supervised algorithm that can be used for classification, regression, and ranking tasks. It quickly took notice and became a popular and impactful method for making predictions and recommendations. Particularly, it is a generalization of the linear regression model and the <b>matrix</b> <b>factorization</b> model. Moreover, it is reminiscent of support vector machines with a polynomial kernel. The strengths of ...", "dateLastCrawled": "2022-01-30T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning</b> Word Vectors with <b>Linear Constraints: A Matrix Factorization</b> ...", "url": "https://www.ijcai.org/Proceedings/2018/0582.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/2018/0582.pdf", "snippet": "A <b>Matrix</b> <b>Factorization</b> Approach Wenye Li1;2, Jiawei Zhang1, Jianjun Zhou2 andLaizhong Cui3 1 The Chinese University of Hong Kong, Shenzhen, China 2 Shenzhen Research Institute of Big Data, Shenzhen, China 3 Shenzhen University, Shenzhen, China wyli@cuhk.edu.cn, 216019001@link.cuhk.edu.cn, benz@sribd.cn, cuilz@szu.edu.cn Abstract <b>Learning</b> vector space representation of words, or word embedding, has attracted much recent research attention. With the objective of better capturing the semantic ...", "dateLastCrawled": "2021-11-19T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Matrix Factorization</b> Intuition for Movie Recommender System | by Himang ...", "url": "https://medium.com/skyshidigital/matrix-factorization-intuition-for-movie-recommender-system-f25804836327", "isFamilyFriendly": true, "displayUrl": "https://medium.com/skyshidigital/<b>matrix-factorization</b>-intuition-for-movie-recommender...", "snippet": "The classic problem in any supervised <b>machine</b> <b>learning</b> is overfitting which is a condition where the model manage to accurately predict for the data that we use in training process but is not able ...", "dateLastCrawled": "2021-12-12T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Objective Functions: A Simple Example with <b>Matrix</b> Factorisation", "url": "https://mlatcl.github.io/mlai/slides/02-matrix-factorization.slides.html", "isFamilyFriendly": true, "displayUrl": "https://mlatcl.github.io/mlai/slides/02-<b>matrix</b>-<b>factorization</b>.slides.html", "snippet": "Objective Functions: A Simple Example with <b>Matrix</b> Factorisation. Neil D. Lawrence. Objective Function. Last week we motivated the importance of probability. This week we motivate the idea of the \u2018objective function\u2019. Introduction to Classification Classification. Wake word classification (Global Pulse Project). Breakthrough in 2012 with ImageNet result of Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. We are given a data set containing \u2018inputs\u2019, \\(\\mathbf{X}\\) and \u2018targets ...", "dateLastCrawled": "2022-02-02T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Deep Non-Negative <b>Matrix</b> <b>Factorization</b> Neural Network", "url": "https://www1.cmc.edu/pages/faculty/BHunter/papers/deep-negative-matrix.pdf", "isFamilyFriendly": true, "displayUrl": "https://www1.cmc.edu/pages/faculty/BHunter/papers/deep-negative-<b>matrix</b>.pdf", "snippet": "A Deep Non-Negative <b>Matrix</b> <b>Factorization</b> Neural Network Jennifer Flenner Blake Hunter 1 Abstract Recently, deep neural network algorithms have emerged as one of the most successful <b>machine</b> <b>learning</b> strategies, obtaining state of the art results for speech recognition, computer vision, and classi cation of large data sets. Their success is due to advancement in computing power, availability of massive amounts of data and the development of new computational techniques. Some of the drawbacks ...", "dateLastCrawled": "2022-02-03T04:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>Learning</b> Classifier: Basics and Evaluation \u2014 <b>James Le</b>", "url": "https://jameskle.com/writes/ml-basics-and-evaluation", "isFamilyFriendly": true, "displayUrl": "https://jameskle.com/writes/ml-basics-and-evaluation", "snippet": "<b>Matrix</b> transpose is when we flip a <b>matrix</b>\u2019s columns and rows, so row 1 is now column 1, and so on. Given a <b>matrix</b> A, its inverse A^(-1) is a <b>matrix</b> such that A x A^(-1) = I. If A^(-1) exists, then A is invertible or non-singular. Otherwise, it is singular. <b>Machine</b> <b>Learning</b>. 1 \u2014 Main Approaches. The 3 major approaches to <b>machine</b> <b>learning</b> are:", "dateLastCrawled": "2022-01-04T16:12:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>GitHub</b> - DCtheTall/<b>introduction-to-machine-learning</b>: My own ...", "url": "https://github.com/DCtheTall/introduction-to-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/DCtheTall/<b>introduction-to-machine-learning</b>", "snippet": "<b>Introduction to Machine Learning</b> with Python Table of Contents Chapter 1 Introduction Chapter 2 Supervised <b>Learning</b> k-Nearest Neighbors Linear Regression Ridge Regression Lasso Regression Logistic Regression Naive Bayes Classifiers Decision Trees Kernelized Support Vector Machines Neural Networks Predicting Uncertainty Chapter 3 Unsupervised <b>Learning</b> Preprocessing and Scaling Principal Component Analysis Non-negative Matrix Factorization Manifold <b>Learning</b> k-Means Clustering Agglomerative ...", "dateLastCrawled": "2021-09-16T10:45:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "when using matrix factorization is it will work because there is a low ...", "url": "https://www.coursehero.com/file/pastgfv/when-using-matrix-factorization-is-it-will-work-because-there-is-a-low-rank/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/pastgfv/when-using-matrix-factorization-is-it-will...", "snippet": "when using matrix factorization is it will work because there is a low rank from CS 188 at Columbia University", "dateLastCrawled": "2021-12-25T11:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Singular Value decomposition (<b>SVD</b>) in recommender systems for Non-math ...", "url": "https://medium.com/@m_n_malaeb/singular-value-decomposition-svd-in-recommender-systems-for-non-math-statistics-programming-4a622de653e9", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@m_n_malaeb/singular-value-decomposition-<b>svd</b>-in-recommender-systems...", "snippet": "From a high level, <b>matrix factorization can be thought of as</b> finding 2 matrices whose product is the original matrix. Each item can be represented by a vector ` qi `.", "dateLastCrawled": "2022-01-28T23:02:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(matrix factorization)  is like +(uncovering hidden patterns)", "+(matrix factorization) is similar to +(uncovering hidden patterns)", "+(matrix factorization) can be thought of as +(uncovering hidden patterns)", "+(matrix factorization) can be compared to +(uncovering hidden patterns)", "machine learning +(matrix factorization AND analogy)", "machine learning +(\"matrix factorization is like\")", "machine learning +(\"matrix factorization is similar\")", "machine learning +(\"just as matrix factorization\")", "machine learning +(\"matrix factorization can be thought of as\")", "machine learning +(\"matrix factorization can be compared to\")"]}