{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Wasserstein</b> Distance Using C# and Python -- Visual Studio Magazine", "url": "https://visualstudiomagazine.com/articles/2021/08/16/wasserstein-distance.aspx", "isFamilyFriendly": true, "displayUrl": "https://visualstudiomagazine.com/articles/2021/08/16/<b>wasserstein</b>-distance.aspx", "snippet": "The Data Science Lab. <b>Wasserstein</b> Distance Using C# and Python. Dr. James McCaffrey of Microsoft Research shows how to compute the <b>Wasserstein</b> distance and explains why it is often preferable to alternative distance functions, used to measure the distance between <b>two</b> <b>probability</b> <b>distributions</b> in machine learning projects.", "dateLastCrawled": "2022-01-24T18:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to Implement <b>Wasserstein</b> <b>Loss</b> for Generative Adversarial Networks", "url": "https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-implement-<b>wasserstein</b>-<b>loss</b>-for-generative...", "snippet": "Stable training requires finding and maintaining an equilibrium between the capabilities of the <b>two</b> models. The discriminator model is a neural network that learns a binary classification problem, using a sigmoid activation function in the output layer, and is fit using a binary cross entropy <b>loss</b> function. As such, the model predicts a <b>probability</b> that a given input is real (or fake as 1 minus the predicted) as a value between 0 and 1. The <b>loss</b> function has the effect of penalizing the ...", "dateLastCrawled": "2022-02-02T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Wasserstein Distance</b> and Textual Similarity - neptune.ai", "url": "https://neptune.ai/blog/wasserstein-distance-and-textual-similarity", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>wasserstein-distance</b>-and-textual-similarity", "snippet": "<b>Wasserstein Distance</b> and Textual Similarity. In many machine learning (ML) projects, there comes a point when we have to decide the level of similarity between different objects of interest. We might be trying to understand the similarity between different images, weather patterns, or <b>probability</b> <b>distributions</b>.", "dateLastCrawled": "2022-01-19T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>distributions</b> - What are the advantages of <b>Wasserstein distance</b> ...", "url": "https://stats.stackexchange.com/questions/490065/what-are-the-advantages-of-wasserstein-distance-compared-to-jensen-shannon-diver", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/490065/what-are-the-advantages-of...", "snippet": "<b>Wasserstein</b> metric is a distance function defined between <b>probability</b> <b>distributions</b> on a given metric space M. Intuitively, if each distribution is viewed as a unit amount of earth (soil) piled on M, the metric is the minimum &quot;cost&quot; of turning one pile into the other, which is assumed to be the amount of earth that needs to be moved times the mean distance it has to be moved.", "dateLastCrawled": "2022-01-25T17:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>distributions</b> - What is the advantages of <b>Wasserstein</b> metric compared ...", "url": "https://stats.stackexchange.com/questions/295617/what-is-the-advantages-of-wasserstein-metric-compared-to-kullback-leibler-diverg", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/295617", "snippet": "Here the measures between red and blue <b>distributions</b> are the same for KL divergence whereas <b>Wasserstein distance</b> measures the work required to transport the <b>probability</b> mass from the red state to the blue state using x-axis as a \u201croad\u201d. This measure is obviously the larger the further away the <b>probability</b> mass is (hence the alias earth mover&#39;s distance). So which one you want to use depends on your application area and what you want to measure. As a note, instead of KL divergence there ...", "dateLastCrawled": "2022-02-03T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "HETEROGENEOUS <b>WASSERSTEIN</b> DISCREPANCY FOR INCOMPARABLE <b>DISTRIBUTIONS</b>", "url": "https://openreview.net/pdf?id=UORhn0DGIT", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=UORhn0DGIT", "snippet": "Optimal Transport (OT) metrics allow for de\ufb01ning discrepancies between <b>two</b> <b>probability</b> measures. <b>Wasserstein</b> distance is for longer the celebrated OT-distance frequently-used in the literature, which seeks <b>probability</b> <b>distributions</b> to be sup-ported on the same metric space. Because of its high computational complexity, several approximate <b>Wasserstein</b> distances have been proposed based on entropy regularization or on slicing, and one-dimensional Wassserstein computation. In this paper, we ...", "dateLastCrawled": "2021-12-03T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(q,p)-<b>Wasserstein</b> GANs: <b>Comparing</b> Ground Metrics for <b>Wasserstein</b> GANs ...", "url": "https://deepai.org/publication/q-p-wasserstein-gans-comparing-ground-metrics-for-wasserstein-gans", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/q-p-<b>wasserstein</b>-gans-<b>comparing</b>-ground-metrics-for...", "snippet": "<b>Wasserstein</b> GANs (WGANs) brought Optimal Transport (OT) theory into GANs, by minimizing the 1-<b>Wasserstein</b> distance between model and data <b>distributions</b> as their objective function. Since then, WGANs have gained considerable interest due to their stability and theoretical framework. We contribute to the WGAN literature by introducing the family of (q,p)-<b>Wasserstein</b> GANs, which allow the use of more general p-<b>Wasserstein</b> metrics for p\u2265 1 in the GAN learning procedure. While the method is ...", "dateLastCrawled": "2022-01-21T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "An <b>intuitive guide to optimal transport, part II: the Wasserstein</b> GAN ...", "url": "http://modelai.gettysburg.edu/2020/wgan/Resources/Lesson4/IntuitiveGuideOT.htm", "isFamilyFriendly": true, "displayUrl": "modelai.gettysburg.edu/2020/wgan/Resources/Lesson4/IntuitiveGuideOT.htm", "snippet": "The most common example is when <b>two</b> <b>distributions</b> have different support, meaning that they assign zero <b>probability</b> to different families of sets. For example, assume that P ( x ) P ( x ) is a usual <b>probability</b> distribution on a <b>two</b> dimensional space defined by a <b>probability</b> density.", "dateLastCrawled": "2022-01-24T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Gaussian Word Embedding with a <b>Wasserstein</b> Distance <b>Loss</b> | DeepAI", "url": "https://deepai.org/publication/gaussian-word-embedding-with-a-wasserstein-distance-loss", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/gaussian-word-embedding-with-a-<b>wasserstein</b>-distance-<b>loss</b>", "snippet": "While the <b>Wasserstein</b> distance provides a natural notion of dissimilarity with <b>probability</b> measures and has a closed form solution when measuring the distance between <b>two</b> Gaussian <b>distributions</b>. Therefore, with the aim of representing words in a high-efficient way, we propose to operate the Gaussian word embedding model with the <b>loss</b> function which is based on the <b>Wasserstein</b> distance.", "dateLastCrawled": "2021-12-21T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(q,p)-<b>Wasserstein</b> GANs: <b>Comparing</b> Ground Metrics for <b>Wasserstein</b> GANs", "url": "https://www.arxiv-vanity.com/papers/1902.03642/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1902.03642", "snippet": "Generative Adversial Networks (GANs) have made a major impact in computer vision and machine learning as generative models. <b>Wasserstein</b> GANs (WGANs) brought Optimal Transport (OT) theory into GANs, by minimizing the 1-<b>Wasserstein</b> distance between model and data <b>distributions</b> as their objective function. Since then, WGANs have gained considerable interest due to their stability and theoretical framework. We contribute to the WGAN literature by introducing the family of (q,p)-<b>Wasserstein</b> GANs ...", "dateLastCrawled": "2021-12-07T19:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Wasserstein Distance</b> and Textual Similarity - neptune.ai", "url": "https://neptune.ai/blog/wasserstein-distance-and-textual-similarity", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>wasserstein-distance</b>-and-textual-<b>similar</b>ity", "snippet": "<b>Wasserstein Distance</b> and Textual Similarity. In many machine learning (ML) projects, there comes a point when we have to decide the level of similarity between different objects of interest. We might be trying to understand the similarity between different images, weather patterns, or <b>probability</b> <b>distributions</b>.", "dateLastCrawled": "2022-01-19T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>WASSERSTEIN</b> OF <b>WASSERSTEIN</b> <b>LOSS</b> FOR LEARNING GENERATIVE MODELS", "url": "https://www.researchgate.net/profile/Wuchen-Li-2/publication/330699954_Wasserstein_of_Wasserstein_Loss_for_Learning_Generative_Models/links/5c4faa6592851c22a3985217/Wasserstein-of-Wasserstein-Loss-for-Learning-Generative-Models.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Wuchen-Li-2/publication/330699954_<b>Wasserstein</b>_of...", "snippet": "<b>distributions</b> incorporates the distance between samples, via a ground metric of choice. In In this way, it provides a continuous <b>loss</b> function for learning <b>probability</b> models supported", "dateLastCrawled": "2022-01-02T04:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Wasserstein of Wasserstein Loss for Learning Generative Models</b>", "url": "http://proceedings.mlr.press/v97/dukler19a/dukler19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/dukler19a/dukler19a.pdf", "snippet": "<b>Wasserstein of Wasserstein Loss for Learning Generative Models</b> ... <b>two</b> sample images is taken to be the mean square differ-ence over the features, i.e., the L2 (Euclidean) norm. This, however, does not incorporate additional knowledge that we have about the space of natural images. In order to improve training and direct focus to selected features, other Sobolev norms in image space have been studied (Adler &amp; Lunz, 2018). Recent works are also investigating distances based on higher level ...", "dateLastCrawled": "2022-01-24T10:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "DISTANCES BETWEEN <b>PROBABILITY</b> <b>DISTRIBUTIONS</b> OF DIFFERENT DIMENSIONS", "url": "http://www.stat.uchicago.edu/~lekheng/work/probdist.pdf", "isFamilyFriendly": true, "displayUrl": "www.stat.uchicago.edu/~lekheng/work/probdist.pdf", "snippet": "<b>Comparing</b> <b>probability</b> <b>distributions</b> is an indispensable and ubiquitous task in machine learning and statistics. The most common way to compare a pair of Borel <b>probability</b> measures is to compute a metric between them, and by far the most widely used notions of metric are the <b>Wasserstein</b> metric and the total variation metric. The next most common way is to compute a divergence between them, and in this case almost every known divergences such as those of Kullback{Leibler, Jensen{Shannon, R ...", "dateLastCrawled": "2022-01-13T13:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(q,p)-<b>Wasserstein</b> GANs: <b>Comparing</b> Ground Metrics for <b>Wasserstein</b> GANs", "url": "https://www.arxiv-vanity.com/papers/1902.03642/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1902.03642", "snippet": "Generative Adversial Networks (GANs) have made a major impact in computer vision and machine learning as generative models. <b>Wasserstein</b> GANs (WGANs) brought Optimal Transport (OT) theory into GANs, by minimizing the 1-<b>Wasserstein</b> distance between model and data <b>distributions</b> as their objective function. Since then, WGANs have gained considerable interest due to their stability and theoretical framework. We contribute to the WGAN literature by introducing the family of (q,p)-<b>Wasserstein</b> GANs ...", "dateLastCrawled": "2021-12-07T19:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An <b>intuitive guide to optimal transport, part II: the Wasserstein</b> GAN ...", "url": "http://modelai.gettysburg.edu/2020/wgan/Resources/Lesson4/IntuitiveGuideOT.htm", "isFamilyFriendly": true, "displayUrl": "modelai.gettysburg.edu/2020/wgan/Resources/Lesson4/IntuitiveGuideOT.htm", "snippet": "The most common example is when <b>two</b> <b>distributions</b> have different support, meaning that they assign zero <b>probability</b> to different families of sets. For example, assume that P ( x ) P ( x ) is a usual <b>probability</b> distribution on a <b>two</b> dimensional space defined by a <b>probability</b> density.", "dateLastCrawled": "2022-01-24T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(q,p)-<b>Wasserstein</b> GANs: <b>Comparing</b> Ground Metrics for <b>Wasserstein</b> GANs ...", "url": "https://deepai.org/publication/q-p-wasserstein-gans-comparing-ground-metrics-for-wasserstein-gans", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/q-p-<b>wasserstein</b>-gans-<b>comparing</b>-ground-metrics-for...", "snippet": "<b>Wasserstein</b> GANs (WGANs) brought Optimal Transport (OT) theory into GANs, by minimizing the 1-<b>Wasserstein</b> distance between model and data <b>distributions</b> as their objective function. Since then, WGANs have gained considerable interest due to their stability and theoretical framework. We contribute to the WGAN literature by introducing the family of (q,p)-<b>Wasserstein</b> GANs, which allow the use of more general p-<b>Wasserstein</b> metrics for p\u2265 1 in the GAN learning procedure. While the method is ...", "dateLastCrawled": "2022-01-21T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Quantum <b>Wasserstein</b> GANs - NeurIPS", "url": "https://proceedings.neurips.cc/paper/8903-quantum-wasserstein-generative-adversarial-networks.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/8903-quantum-<b>wasserstein</b>-generative-adversarial...", "snippet": "of classical <b>distributions</b>, where the <b>loss</b> function measuring the difference between the real and the fake <b>distributions</b> can be borrowed directly from the classical GANs, the design of the <b>loss</b> function between real and fake quantum data as well as the ef\ufb01cient training of the corresponding GAN is much more challenging. The only existing results on quantum data either have a unique design speci\ufb01c to the 1-qubit case [13, 23], or suffer from robust training issues discussed below [4 ...", "dateLastCrawled": "2022-01-25T16:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] In what situation would one rather use KL-divergence instead of ...", "url": "https://www.reddit.com/r/MachineLearning/comments/fftpfh/d_in_what_situation_would_one_rather_use/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/fftpfh/d_in_what_situation_would_one...", "snippet": "Given <b>two</b> discrete <b>probability</b> <b>distributions</b>, the KL-divergence has a very simple and easy to evaluate closed form formula, which makes it easily usable in basically every scenario as a <b>probability</b> <b>loss</b> function. On the other hand, evaluating the <b>Wasserstein</b> distance between <b>two</b> <b>probability</b> <b>distributions</b> requires solving an optimization problem, so it is significantly harder to use, and especially so if you want your <b>loss</b> function to be differentiable (which is usually the case). When used ...", "dateLastCrawled": "2021-12-31T07:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to measure the statistical &quot;<b>distance&quot; between</b> <b>two</b> frequency ...", "url": "https://stats.stackexchange.com/questions/425040/how-to-measure-the-statistical-distance-between-two-frequency-distributions", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/425040/how-to-measure-the-statistical...", "snippet": "Put another way: <b>two</b> <b>distributions</b> (1,0,0) and (0,1,0) should be &quot;more <b>similar</b>&quot; than (1,0,0) and (0,0,1). The EMD will recognize this and assign a smaller <b>distance</b> to the first pair than to the second. The \u03c7 2 statistic will assign the same <b>distance</b> to both pairs, because it has no notion of an ordering in the distribution entries.", "dateLastCrawled": "2022-01-25T16:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Implement <b>Wasserstein</b> <b>Loss</b> for Generative Adversarial Networks", "url": "https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-implement-<b>wasserstein</b>-<b>loss</b>-for-generative...", "snippet": "The most fundamental difference between such distances is their impact on the convergence of sequences of <b>probability</b> <b>distributions</b>. \u2014 <b>Wasserstein</b> GAN, 2017. They demonstrate that a critic neural network <b>can</b> be trained to approximate the <b>Wasserstein</b> distance, and, in turn, used to effectively train a generator model. \u2026 we define a form of GAN called <b>Wasserstein</b>-GAN that minimizes a reasonable and efficient approximation of the EM distance, and we theoretically show that the corresponding ...", "dateLastCrawled": "2022-02-02T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>distributions</b> - What is the advantages of <b>Wasserstein</b> metric compared ...", "url": "https://stats.stackexchange.com/questions/295617/what-is-the-advantages-of-wasserstein-metric-compared-to-kullback-leibler-diverg", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/295617", "snippet": "<b>Wasserstein</b> (or Vaserstein) metric is a distance function defined between <b>probability</b> <b>distributions</b> on a given metric space M. and. Kullback\u2013Leibler divergence is a measure of how one <b>probability</b> distribution diverges from a second expected <b>probability</b> distribution. I&#39;ve seen KL been used in machine learning implementations, but I recently came across the <b>Wasserstein</b> metric. Is there a good guideline on when to use one or the other? (I have insufficient reputation to create a new tag with ...", "dateLastCrawled": "2022-02-03T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Comparing</b> <b>Distributions</b> by Measuring Dif- ferences that Affect Decision ...", "url": "https://openreview.net/pdf?id=KB5onONJIAU", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=KB5onONJIAU", "snippet": "<b>Comparing</b> <b>Distributions</b> by Measuring Dif-ferences that Affect Decision Making Anonymous authors Paper under double-blind review Abstract Measuring the discrepancy between <b>two</b> <b>probability</b> <b>distributions</b> is a fun-damental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal <b>loss</b> for a decision task { <b>two</b> <b>distributions</b> are di erent if the optimal decision <b>loss</b> is higher on their mixture than on each individual distribution. By suitably choosing ...", "dateLastCrawled": "2022-01-11T14:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "On the equivalence between Fourier-based and <b>Wasserstein</b> metrics", "url": "https://mate.unipv.it/toscani/publi/Immagini.pdf", "isFamilyFriendly": true, "displayUrl": "https://mate.unipv.it/tos<b>can</b>i/publi/Immagini.pdf", "snippet": "de\ufb01ned also for <b>probability</b> <b>distributions</b> with di\ufb00erent centers of mass, and for discrete <b>probability</b> measures supported over a regular grid. Among other properties, it is shown that, in the discrete setting, these new Fourier-based met- rics are equivalent either to the Euclidean-<b>Wasserstein</b> distance W 2, or to the Kantorovich-<b>Wasserstein</b> distance W 1, with explicit constants of equivalence. Keywords Fourier-based Metrics \u00b7<b>Wasserstein</b> Distance \u00b7Fourier Transform Mathematics Subject ...", "dateLastCrawled": "2021-08-29T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A generative adversarial network-based abnormality detection using only ...", "url": "https://www.nature.com/articles/s41598-021-89626-1", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-89626-1", "snippet": "The <b>Wasserstein</b> distance between <b>two</b> <b>distributions</b> <b>can</b> intuitively <b>be thought</b> of as the minimal effort needed to transport <b>probability</b> mass between these <b>distributions</b>; it is theoretically defined as", "dateLastCrawled": "2022-01-31T04:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Semantic Image Inpainting Through Improved <b>Wasserstein</b> Generative ...", "url": "https://deepai.org/publication/semantic-image-inpainting-through-improved-wasserstein-generative-adversarial-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/semantic-image-inpainting-through-improved-<b>wasserstein</b>...", "snippet": "Indeed, <b>two</b> problems <b>can</b> appear: vanishing gradients and mode collapse. Vanishing gradients are specially problematic when <b>comparing</b> <b>probability</b> <b>distributions</b> with non-overlapping supports. If the discriminator is able to perfectly distinguish between real and generated images, it reaches its optimum and thus the generator no longer improves the generated data. On the other hand, mode collapse happens when the generator only encapsulates the major nodes of the real distribution, and not the ...", "dateLastCrawled": "2022-01-21T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Wasserstein GAN</b> | DeepAI", "url": "https://deepai.org/publication/wasserstein-gan", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>wasserstein-gan</b>", "snippet": "Example 1 gives us a case where we <b>can</b> learn a <b>probability</b> distribution over a low dimensional manifold by doing gradient descent on the EM distance. This cannot be done with the other distances and divergences because the resulting <b>loss</b> function is not even continuous. Although this simple example features <b>distributions</b> with disjoint supports, the same conclusion holds when the supports have a non empty intersection contained in a set of measure zero.", "dateLastCrawled": "2022-01-30T19:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Understanding <b>Loss</b> Functions in Computer Vision! | by Sowmya ...", "url": "https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/ml-cheat-sheet/winning-at-<b>loss</b>-functions-2-important-<b>loss</b>-functions...", "snippet": "Another interesting <b>loss</b> function from recent literature, introduced by Mosinska et al (2017) is the topology-aware <b>loss</b> function. This <b>can</b> <b>be thought</b> of as an extension of perceptual <b>loss</b> ...", "dateLastCrawled": "2022-02-03T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 8, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Earth mover&#39;s distance</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Earth_mover%27s_distance", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Earth_mover&#39;s_distance</b>", "snippet": "In statistics, the <b>earth mover&#39;s distance</b> (EMD) is a measure of the distance between <b>two</b> <b>probability</b> <b>distributions</b> over a region D.In mathematics, this is known as the <b>Wasserstein</b> metric.Informally, if the <b>distributions</b> are interpreted as <b>two</b> different ways of piling up a certain amount of earth (dirt) over the region D, the EMD is the minimum cost of turning one pile into the other; where the cost is assumed to be the amount of dirt moved times the distance by which it is moved.. The above ...", "dateLastCrawled": "2022-01-30T17:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Sizing energy storage to reduce renewable power ... - Wiley Online Library", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-rpg.2020.0354", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-rpg.2020.0354", "snippet": "The uncertainty of renewable generation is described via inexact <b>probability</b> <b>distributions</b> encapsulated in a data-driven <b>Wasserstein</b>-metric based ambiguity set, based on which the renewable energy curtailment rate is formulated as a distributionally robust chance constraint. The objective is to minimise the total investment cost, and the optimal sizing problem gives rise to a distributionally robust chance-constrained program, and is reformulated as a tractable linear program via ...", "dateLastCrawled": "2022-02-03T07:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>distributions</b> - What are the advantages of <b>Wasserstein distance</b> ...", "url": "https://stats.stackexchange.com/questions/490065/what-are-the-advantages-of-wasserstein-distance-compared-to-jensen-shannon-diver", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/490065/what-are-the-advantages-of...", "snippet": "<b>Wasserstein</b> metric is a distance function defined between <b>probability</b> <b>distributions</b> on a given metric space M. Intuitively, if each distribution is viewed as a unit amount of earth (soil) piled on M, the metric is the minimum &quot;cost&quot; of turning one pile into the other, which is assumed to be the amount of earth that needs to be moved times the mean distance it has to be moved. and. Jensen-Shannon divergence is a method of measuring the similarity between <b>two</b> <b>probability</b> <b>distributions</b>. It is ...", "dateLastCrawled": "2022-01-25T17:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(q,p)-<b>Wasserstein</b> GANs: <b>Comparing</b> Ground Metrics for <b>Wasserstein</b> GANs ...", "url": "https://deepai.org/publication/q-p-wasserstein-gans-comparing-ground-metrics-for-wasserstein-gans", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/q-p-<b>wasserstein</b>-gans-<b>comparing</b>-ground-metrics-for...", "snippet": "Recently, a popular family of metrics has been provided by the theory of Optimal Transport (OT), which studies <b>probability</b> <b>distributions</b> through a geometric framework. At its heart lie the <b>Wasserstein</b> metrics, which extend the underlying metric between sample points to entire <b>distributions</b>.Consequently, the metrics <b>can</b> be used to e.g. derive statistics between populations of <b>probability</b> <b>distributions</b>, allowing the inclusion of stochastic data objects in statistical pipelines (Mallasto ...", "dateLastCrawled": "2022-01-21T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>distributions</b> - What is the advantages of <b>Wasserstein</b> metric <b>compared</b> ...", "url": "https://stats.stackexchange.com/questions/295617/what-is-the-advantages-of-wasserstein-metric-compared-to-kullback-leibler-diverg", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/295617", "snippet": "Here the measures between red and blue <b>distributions</b> are the same for KL divergence whereas <b>Wasserstein distance</b> measures the work required to transport the <b>probability</b> mass from the red state to the blue state using x-axis as a \u201croad\u201d. This measure is obviously the larger the further away the <b>probability</b> mass is (hence the alias earth mover&#39;s distance). So which one you want to use depends on your application area and what you want to measure. As a note, instead of KL divergence there ...", "dateLastCrawled": "2022-02-03T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(q,p)-<b>Wasserstein</b> GANs: <b>Comparing</b> Ground Metrics for <b>Wasserstein</b> GANs", "url": "https://www.arxiv-vanity.com/papers/1902.03642/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1902.03642", "snippet": "Generative Adversial Networks (GANs) have made a major impact in computer vision and machine learning as generative models. <b>Wasserstein</b> GANs (WGANs) brought Optimal Transport (OT) theory into GANs, by minimizing the 1-<b>Wasserstein</b> distance between model and data <b>distributions</b> as their objective function. Since then, WGANs have gained considerable interest due to their stability and theoretical framework. We contribute to the WGAN literature by introducing the family of (q,p)-<b>Wasserstein</b> GANs ...", "dateLastCrawled": "2021-12-07T19:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "HETEROGENEOUS <b>WASSERSTEIN</b> DISCREPANCY FOR INCOMPARABLE <b>DISTRIBUTIONS</b>", "url": "https://openreview.net/pdf?id=UORhn0DGIT", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=UORhn0DGIT", "snippet": "are now <b>compared</b> through <b>Wasserstein</b> distance. This point, in conjunction, with other key aspect of the method, will lead to a relevant discrepancy between <b>two</b> <b>distributions</b>, called heterogeneous <b>Wasserstein</b> discrepancy (HWD). Although we lose some properties of a distance, we show that HWD is rotation-invariant, that it is robust enough to be considered as a <b>loss</b> for learning generative models between heterogeneous spaces. We also establish that HWD boils down to the recent distributional ...", "dateLastCrawled": "2021-12-03T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Wasserstein Distance</b> and Textual Similarity - neptune.ai", "url": "https://neptune.ai/blog/wasserstein-distance-and-textual-similarity", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>wasserstein-distance</b>-and-textual-similarity", "snippet": "<b>Wasserstein Distance</b> and Textual Similarity. In many machine learning (ML) projects, there comes a point when we have to decide the level of similarity between different objects of interest. We might be trying to understand the similarity between different images, weather patterns, or <b>probability</b> <b>distributions</b>.", "dateLastCrawled": "2022-01-19T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Simulated Annealing Based Inexact Oracle for <b>Wasserstein</b> <b>Loss</b> ...", "url": "http://proceedings.mlr.press/v70/ye17b/ye17b.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v70/ye17b/ye17b.pdf", "snippet": "<b>Wasserstein</b> distance is de\ufb01ned as the cost of matching <b>two</b> <b>probability</b> measures, originated from the literature of op-timal transport (OT) (Monge, 1781). It takes into account the cross-term similarity between different support points of the <b>distributions</b>, a level of complexity beyond the usual vector data treatment, i.e., to convert the distribution into a vector of frequencies. It has been promoted for <b>comparing</b> sets of vectors (e.g. bag-of-words models) by researchers in computer vision ...", "dateLastCrawled": "2022-01-29T17:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Comparing</b> <b>Distributions</b> by Measuring Dif- ferences that Affect Decision ...", "url": "https://openreview.net/pdf?id=KB5onONJIAU", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=KB5onONJIAU", "snippet": "<b>Comparing</b> <b>Distributions</b> by Measuring Dif-ferences that Affect Decision Making Anonymous authors Paper under double-blind review Abstract Measuring the discrepancy between <b>two</b> <b>probability</b> <b>distributions</b> is a fun-damental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal <b>loss</b> for a decision task { <b>two</b> <b>distributions</b> are di erent if the optimal decision <b>loss</b> is higher on their mixture than on each individual distribution. By suitably choosing ...", "dateLastCrawled": "2022-01-11T14:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Gaussian Word Embedding with a <b>Wasserstein</b> Distance <b>Loss</b> | DeepAI", "url": "https://deepai.org/publication/gaussian-word-embedding-with-a-wasserstein-distance-loss", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/gaussian-word-embedding-with-a-<b>wasserstein</b>-distance-<b>loss</b>", "snippet": "For example, <b>compared</b> to cosine similarity and Euclidean distance represented by the points, using the distribution-based representation <b>can</b> extend the definition of indicators that measure the similarity of <b>two</b> words, such as KL divergence, <b>Wasserstein</b> distance, etc. However, vilnis2014word vilnis2014word use a <b>loss</b> function based on KL divergence. KL divergence is ill-defined for <b>two</b> very similar <b>distributions</b>, and it is not sensitive to the distance between <b>two</b> distant <b>distributions</b>. It ...", "dateLastCrawled": "2021-12-21T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Quantum <b>Wasserstein</b> GANs - NeurIPS", "url": "https://proceedings.neurips.cc/paper/8903-quantum-wasserstein-generative-adversarial-networks.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/8903-quantum-<b>wasserstein</b>-generative-adversarial...", "snippet": "of classical <b>distributions</b>, where the <b>loss</b> function measuring the difference between the real and the fake <b>distributions</b> <b>can</b> be borrowed directly from the classical GANs, the design of the <b>loss</b> function between real and fake quantum data as well as the ef\ufb01cient training of the corresponding GAN is much more challenging. The only existing results on quantum data either have a unique design speci\ufb01c to the 1-qubit case [13, 23], or suffer from robust training issues discussed below [4 ...", "dateLastCrawled": "2022-01-25T16:26:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Learning Wasserstein Embeddings</b> | DeepAI", "url": "https://deepai.org/publication/learning-wasserstein-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-wasserstein-embeddings</b>", "snippet": "The <b>Wasserstein</b> distance received a lot of attention recently in the community of <b>machine</b> <b>learning</b>, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy ...", "dateLastCrawled": "2022-01-05T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to stabilize GAN training. Understand <b>Wasserstein</b> distance and ...", "url": "https://towardsdatascience.com/wasserstein-distance-gan-began-and-progressively-growing-gan-7e099f38da96", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>wasserstein</b>-distance-gan-began-and-progressively...", "snippet": "<b>Wasserstein</b> <b>loss</b> leads to a higher quality of the gradients to train G. ... Finally, one intuitive way to understand this paper is to make an <b>analogy</b> with the gradients on the history of in-layer activation functions. Specifically, the gradients of sigmoid and tanh activations that disappeared in favor of ReLUs, because of the improved gradients in the whole range of values. BEGAN (Boundary Equilibrium Generative Adversarial Networks 2017) We often see that the discriminator progresses too ...", "dateLastCrawled": "2022-01-25T18:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Learning</b> <b>Wasserstein</b> Embeddings - ResearchGate", "url": "https://www.researchgate.net/publication/320564581_Learning_Wasserstein_Embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320564581_<b>Learning</b>_<b>Wasserstein</b>_Embeddings", "snippet": "Designed through an <b>analogy</b> with ... Fast dictionary <b>learning</b> with a smoothed <b>wasserstein</b> <b>loss</b>. In AISTA TS, pages 630\u2013638, 2016. [32] F. Santambrogio. Introduction to optimal transport theory ...", "dateLastCrawled": "2021-12-13T04:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "[D] Is the <b>Wasserstein</b> distance really what we optimize in WGAN ...", "url": "https://www.reddit.com/r/MachineLearning/comments/ew2lzs/d_is_the_wasserstein_distance_really_what_we/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/ew2lzs/d_is_the_<b>wasserstein</b>_distance...", "snippet": "The &quot;genuine&quot; <b>Wasserstein</b> <b>loss</b> relies on optimal transport, a generalization of sorting to high-dimensional feature spaces. In a nutshell: OT relies on the matrix of distances between samples to define a &quot;least action&quot; matching between any two distributions. Now, unfortunately, in spaces of images, the L2 distance is (essentially) meaningless: natural images should not be compared with each other pixel-wise. As a consequence, the baseline <b>Wasserstein</b> distance between two batches of images is ...", "dateLastCrawled": "2021-09-30T11:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "deep <b>learning</b> - How can both generator and discriminator losses ...", "url": "https://datascience.stackexchange.com/questions/32699/how-can-both-generator-and-discriminator-losses-decrease", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/32699", "snippet": "In the widely used <b>analogy</b>: ... despite the WGAN having a different <b>loss</b> function, namely the <b>Wasserstein</b> distance, one should still not expect that the discriminator and generator simultaneously monotonically increase -- generally one of them &quot;wins&quot; the round and receives a lower portion of the <b>loss</b>. $\\endgroup$ \u2013 PSub. Mar 13 &#39;21 at 6:07 $\\begingroup$ @PSub You are completely misunderstanding the question. It&#39;s not a question about the small scale changes of the <b>loss</b> values. OP is asking ...", "dateLastCrawled": "2022-01-28T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Manifold-Valued Image Generation with <b>Wasserstein</b> Generative ...", "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4277/4155", "isFamilyFriendly": true, "displayUrl": "https://ojs.aaai.org/index.php/AAAI/article/download/4277/4155", "snippet": "fundamental <b>machine</b> <b>learning</b> problems. However, few mod-ern generative models, including <b>Wasserstein</b> Generative Ad-versarial Nets (WGANs), are studied on manifold-valued im- ages that are frequently encountered in real-world applica-tions. To \ufb01ll the gap, this paper \ufb01rst formulates the problem of generating manifold-valued images and exploits three typical instances: hue-saturation-value (HSV) color image genera-tion, chromaticity-brightness (CB) color image generation, and diffusion ...", "dateLastCrawled": "2022-01-29T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Advanced <b>Machine</b> <b>Learning</b> - Computer Vision", "url": "https://www.vision.rwth-aachen.de/media/course/SS/2019/advanced-machine-learning/ml2_19-part17-gans-6on1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.vision.rwth-aachen.de/media/course/SS/2019/advanced-<b>machine</b>-<b>learning</b>/ml2...", "snippet": "<b>Analogy</b>: police investigator \u2022Both generator and discriminator are deep networks We can train them with backprop. Image sources: www.bundesbank.de, weclipart.com, Kevin McGuiness 15 Advanced <b>Machine</b> <b>Learning</b> Part 17 \u2013Generative Adversarial Networks Training the Discriminator \u2022Procedure Fix generator weights Train discriminator to distinguish between real and generated images Image credit: Kevin McGuiness 16 Visual Computing Institute | Prof. Dr . Bastian Leibe Advanced <b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2021-10-25T15:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "SpringerLink - <b>Machine Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-020-05924-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05924-1", "snippet": "for a given set \\({\\mathcal {F}}\\) of distributions, twice differentiable and convex <b>loss</b> \\(\\ell\\), and prediction \\(f_\\theta (x)\\).The set \\({\\mathcal {F}}\\) is the set of distributions on which one would like the estimator to achieve a guaranteed performance bound.. Causal inference can be seen to be a specific instance of distributional robustness, where we take \\({\\mathcal {F}}\\) to be the class of all distributions generated under do-interventions on the predictors X (Meinshausen 2018 ...", "dateLastCrawled": "2022-01-15T22:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>Tour of Generative Adversarial Network Models</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/tour-of-generative-adversarial-network-models/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>tour-of-generative-adversarial-network-models</b>", "snippet": "By <b>analogy</b> with auto-encoders, we propose Context Encoders \u2013 a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. \u2014 Context Encoders: Feature <b>Learning</b> by Inpainting, 2016. Example of the Context Encoders Encoder-Decoder Model Architecture. Taken from: Context Encoders: Feature <b>Learning</b> by Inpainting. The model is trained with a joint-<b>loss</b> that combines both the adversarial <b>loss</b> of generator and discriminator models ...", "dateLastCrawled": "2022-02-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Gentle Introduction to Pix2Pix Generative</b> Adversarial Network", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-pix2pix-generative</b>...", "snippet": "Image-to-image translation is the controlled conversion of a given source image to a target image. An example might be the conversion of black and white photographs to color photographs. Image-to-image translation is a challenging problem and often requires specialized models and <b>loss</b> functions for a given translation task or dataset. The Pix2Pix GAN is a general approach for image-to-image translation. It is based", "dateLastCrawled": "2022-02-02T13:26:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(wasserstein loss)  is like +(comparing two probability distributions)", "+(wasserstein loss) is similar to +(comparing two probability distributions)", "+(wasserstein loss) can be thought of as +(comparing two probability distributions)", "+(wasserstein loss) can be compared to +(comparing two probability distributions)", "machine learning +(wasserstein loss AND analogy)", "machine learning +(\"wasserstein loss is like\")", "machine learning +(\"wasserstein loss is similar\")", "machine learning +(\"just as wasserstein loss\")", "machine learning +(\"wasserstein loss can be thought of as\")", "machine learning +(\"wasserstein loss can be compared to\")"]}