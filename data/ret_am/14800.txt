{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias</b> in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/<b>bias</b>-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "It is important to consider both an AI model\u2019s <b>algorithm</b> as well as the <b>data</b> it\u2019s <b>trained</b> on. While in some cases there might exist flaws in the <b>algorithm</b>, oftentimes there are many potential issues that can exist in the training <b>data</b> itself or the way that training <b>data</b> is labeled. Knowing what to look for is the fastest way to identify these flaws. That is why it is important to understand the different types of <b>bias</b> that can exist when using <b>machine</b> <b>learning</b> for problem-solving. With ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How <b>to Remove Bias in Training Data</b> for <b>Machine</b> <b>Learning</b> | Appen", "url": "https://appen.com/blog/how-to-remove-bias-in-training-data/", "isFamilyFriendly": true, "displayUrl": "https://appen.com/blog/how-<b>to-remove-bias-in-training-data</b>", "snippet": "<b>Machine</b> <b>learning</b> (ML) algorithms are generally only as good as the <b>data</b> they are <b>trained</b> on. <b>Bias</b> in ML training <b>data</b> can take many forms, but the end result is that it can cause an <b>algorithm</b> to miss the relevant relations between features and target outputs. Whether your organization is a small business, global enterprise, or governmental agency, it\u2019s essential that you mitigate <b>bias</b> in your training <b>data</b> at every phase of your Artificial Intelligence (AI) initiatives.", "dateLastCrawled": "2022-02-02T01:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bias</b> in AI and <b>Machine Learning: Sources and Solutions</b> - Lexalytics", "url": "https://www.lexalytics.com/lexablog/bias-in-ai-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.lexalytics.com/lexablog/<b>bias</b>-in-ai-<b>machine</b>-<b>learning</b>", "snippet": "\u201c<b>Bias</b> in AI\u201d has long been a critical area of research and concern in <b>machine</b> <b>learning</b> circles and has grown in awareness among general consumer audiences over the past couple of years as knowledge of AI has grown. It\u2019s a term that describes situations where ML-based <b>data</b> analytics systems show <b>bias</b> against certain groups of people. These biases usually reflect widespread societal biases about race, gender, biological sex, age, and culture.", "dateLastCrawled": "2022-01-31T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "If a <b>machine learning</b> <b>algorithm</b> was <b>trained</b> solely on video of daytime driving, it would have tragic results if the model were permitted to drive at night. This is different from human <b>bias</b>, but demonstrates the issue of lacking a representative <b>data</b> set for the problem at hand. <b>Bias</b> can also show up where we don\u2019t expect it. In the case of Amazon\u2019s recruitment tool, the model penalized wording used by some candidates and rewarded words by others. In this case, the penalized words were ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Algorithms and <b>bias</b>, explained - <b>Vox</b>", "url": "https://www.vox.com/recode/2020/2/18/21121286/algorithms-bias-discrimination-facial-recognition-transparency", "isFamilyFriendly": true, "displayUrl": "https://www.<b>vox</b>.com/recode/2020/2/18/21121286", "snippet": "<b>Machine</b> <b>learning</b>-based systems are <b>trained</b> on <b>data</b>. Lots of it. When thinking about \u201c<b>machine</b> <b>learning</b>\u201d tools (<b>machine</b> <b>learning</b> is a type of artificial intelligence), it\u2019s better to think ...", "dateLastCrawled": "2022-01-30T21:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> Biases: how to avoid them - Codemotion Magazine", "url": "https://www.codemotion.com/magazine/ai-ml/machine-learning-bias-fantastic-data/", "isFamilyFriendly": true, "displayUrl": "https://www.codemotion.com/magazine/ai-ml/<b>machine</b>-<b>learning</b>-<b>bias</b>-fantastic-<b>data</b>", "snippet": "We can define <b>machine</b> <b>learning</b> as an <b>algorithm</b> that, based on <b>data</b>, creates a model to make a prediction. Then it validates such prediction against the reality, obtaining feedback to improve the model with more <b>data</b>. For example, let\u2019s say that we want a system able to predict the price of a house based on square metres. We have to choose a proper <b>algorithm</b>. In this simple case, we can choose a linear regression that has the goal of finding the best line that fits the correlation between ...", "dateLastCrawled": "2022-01-31T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "HUMAN <b>BIAS</b> THAT CAN RESULT INTO ML BIASES | by CoffeeBeans Consulting ...", "url": "https://coffeebeansconsulting.medium.com/human-bias-that-can-result-into-ml-biases-b1ea9f6d2767?source=post_internal_links---------6----------------------------", "isFamilyFriendly": true, "displayUrl": "https://coffeebeansconsulting.medium.com/human-<b>bias</b>-that-can-result-into-ml-<b>bias</b>es-b1...", "snippet": "<b>Reporting</b> <b>Bias</b>/Sample <b>Bias</b>: \u2014 <b>Reporting</b> <b>bias</b> occurs when the frequency of events, properties and the results in a <b>data</b> set do not reflect their real-world <b>data</b> accurately. This <b>bias</b> can arise because people tend to focus on documenting circumstances that are unusual or especially memorable. This should not be done and equal distribution of large datasets should be used . 2. Prejudice <b>Bias</b>: If the training <b>data</b> that is influenced by stereotypes <b>like</b> culture for an example. The training <b>data</b> ...", "dateLastCrawled": "2022-01-25T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "5 Types of <b>bias</b> &amp; how to eliminate them in your <b>machine</b> <b>learning</b> ...", "url": "https://towardsdatascience.com/5-types-of-bias-how-to-eliminate-them-in-your-machine-learning-project-75959af9d3a0", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/5-types-of-<b>bias</b>-how-to-eliminate-them-in-your-<b>machine</b>...", "snippet": "Compas is a <b>machine</b> <b>learning</b> <b>algorithm</b> that predicts the defendant\u2019s likelihoods to commit crimes, it has been shown that it makes <b>biased</b> predictions about who is more likely to recommit crimes. Research from ProPublica found that the tool was 2 times more likely to incorrectly cite black defendants as being high risk for recommitting crimes, it was also 2 times more likely to incorrectly predict that white defendants were low risk to recommitting crimes. Find the complete analysis by ...", "dateLastCrawled": "2022-01-26T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "Second, A <b>machine</b> <b>learning</b> <b>algorithm</b> is chosen, and uses historical <b>data</b> (e.g. a set of past input <b>data</b> (e.g. a set of past input <b>data</b>, the decisions reached) to build a model, optimising against ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "FTC warns it could crack down <b>on biased</b> AI - <b>The Verge</b>", "url": "https://www.theverge.com/2021/4/20/22393873/ftc-ai-machine-learning-race-gender-bias-legal-violation", "isFamilyFriendly": true, "displayUrl": "https://<b>www.theverge.com</b>/2021/4/20/22393873/ftc-ai-<b>machine</b>-<b>learning</b>-race-gender-<b>bias</b>...", "snippet": "Artificial intelligence holds the potential to mitigate human <b>bias</b> in processes <b>like</b> hiring, but it can also reproduce or exaggerate that <b>bias</b>, particularly if it\u2019s <b>trained</b> in <b>data</b> that reflects it.", "dateLastCrawled": "2022-02-02T23:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias</b> in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/<b>bias</b>-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "It is important to consider both an AI model\u2019s <b>algorithm</b> as well as the <b>data</b> it\u2019s <b>trained</b> on. While in some cases there might exist flaws in the <b>algorithm</b>, oftentimes there are many potential issues that can exist in the training <b>data</b> itself or the way that training <b>data</b> is labeled. Knowing what to look for is the fastest way to identify these flaws. That is why it is important to understand the different types of <b>bias</b> that can exist when using <b>machine</b> <b>learning</b> for problem-solving. With ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Seven Types Of <b>Data</b> <b>Bias</b> In <b>Machine</b> <b>Learning</b>", "url": "https://www.telusinternational.com/articles/7-types-of-data-bias-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.telusinternational.com/articles/7-types-of-<b>data</b>-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "How do I avoid <b>data</b> <b>bias</b> in <b>machine</b> <b>learning</b> projects? The prevention of <b>data</b> <b>bias</b> in <b>machine</b> <b>learning</b> projects is an ongoing process. Though it is sometimes difficult to know when your <b>machine</b> <b>learning</b> <b>algorithm</b>, <b>data</b> or model is <b>biased</b>, there are a number of steps you can take to help prevent <b>bias</b> or catch it early. Though far from a comprehensive list, the bullet points below provide an entry-level guide for thinking about <b>data</b> <b>bias</b> for <b>machine</b> <b>learning</b> projects.", "dateLastCrawled": "2022-02-03T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bias</b> in AI and <b>Machine Learning: Sources and Solutions</b> - Lexalytics", "url": "https://www.lexalytics.com/lexablog/bias-in-ai-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.lexalytics.com/lexablog/<b>bias</b>-in-ai-<b>machine</b>-<b>learning</b>", "snippet": "\u201c<b>Bias</b> in AI\u201d has long been a critical area of research and concern in <b>machine</b> <b>learning</b> circles and has grown in awareness among general consumer audiences over the past couple of years as knowledge of AI has grown. It\u2019s a term that describes situations where ML-based <b>data</b> analytics systems show <b>bias</b> against certain groups of people. These biases usually reflect widespread societal biases about race, gender, biological sex, age, and culture.", "dateLastCrawled": "2022-01-31T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "If a <b>machine learning</b> <b>algorithm</b> was <b>trained</b> solely on video of daytime driving, it would have tragic results if the model were permitted to drive at night. This is different from human <b>bias</b>, but demonstrates the issue of lacking a representative <b>data</b> set for the problem at hand. <b>Bias</b> can also show up where we don\u2019t expect it. In the case of Amazon\u2019s recruitment tool, the model penalized wording used by some candidates and rewarded words by others. In this case, the penalized words were ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "5 Types of <b>bias</b> &amp; how to eliminate them in your <b>machine</b> <b>learning</b> ...", "url": "https://towardsdatascience.com/5-types-of-bias-how-to-eliminate-them-in-your-machine-learning-project-75959af9d3a0", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/5-types-of-<b>bias</b>-how-to-eliminate-them-in-your-<b>machine</b>...", "snippet": "Compas is a <b>machine</b> <b>learning</b> <b>algorithm</b> that predicts the defendant\u2019s likelihoods to commit crimes, it has been shown that it makes <b>biased</b> predictions about who is more likely to recommit crimes. Research from ProPublica found that the tool was 2 times more likely to incorrectly cite black defendants as being high risk for recommitting crimes, it was also 2 times more likely to incorrectly predict that white defendants were low risk to recommitting crimes. Find the complete analysis by ...", "dateLastCrawled": "2022-01-26T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Racially Unbiased, <b>Machine</b> <b>Learning</b> Approach to Prediction of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7644374/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7644374", "snippet": "To assess whether the <b>machine</b> <b>learning</b> <b>algorithm</b> and each comparator identified <b>similar</b> at-risk individuals, the McNemar test was used, comparing performance of the two systems at a sensitivity around 0.75. Performance was assessed both on the overall sample and after stratifying by race. Racial categories were defined as white and nonwhite, where only non-Hispanic white patients were included in the white category (eg, a white Hispanic patient was considered nonwhite for the purpose of this ...", "dateLastCrawled": "2021-12-18T01:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding <b>Data</b> <b>Bias</b>. Types and sources of <b>data</b> <b>bias</b> | by Prabhakar ...", "url": "https://towardsdatascience.com/survey-d4f168791e57", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/survey-d4f168791e57", "snippet": "The huge success of <b>machine</b> <b>learning</b> (ML) applications in the past decade \u2014 in image recognition, re c ommendation systems, e-commerce and online advertising \u2014 has inspired its adoption in domains such as social justice, employment screening, and in smart interactive interfaces such as Siri, Alexa, and the like. Along with the proliferation of these applications, there has been an alarming rise in reports of gender, race and other types of <b>bias</b> in these systems.", "dateLastCrawled": "2022-02-02T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "Second, A <b>machine</b> <b>learning</b> <b>algorithm</b> is chosen, and uses historical <b>data</b> (e.g. a set of past input <b>data</b> (e.g. a set of past input <b>data</b>, the decisions reached) to build a model, optimising against ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Training Report on Machine Learning</b> | Sahdev Kansal - Academia.edu", "url": "https://www.academia.edu/42924139/Training_Report_on_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/42924139/<b>Training_Report_on_Machine_Learning</b>", "snippet": "Translate PDF. INDUSTRIAL <b>TRAINING REPORT ON \u201cMACHINE LEARNING</b>\u201d Submitted in partial fulfillment of the requirements for the award of the degree of BACHELOR OF TECHNOLOGY IN COMPUTER SCIENCE ENGINEERING Submitted By Sahdev Kansal, Enrollment no. (41015602717) Department of Computer Science Engineering Dr. Akhilesh Das Gupta Institute of ...", "dateLastCrawled": "2022-01-30T20:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Amazon scraps secret AI recruiting tool that showed <b>bias</b> against women ...", "url": "https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reuters.com</b>/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G", "snippet": "<b>Machine</b> <b>learning</b> was gaining traction in the technology world, thanks to a surge in low-cost computing power. And Amazon\u2019s Human Resources department was about to embark on a hiring spree: Since ...", "dateLastCrawled": "2022-02-02T22:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Seven Types Of <b>Data</b> <b>Bias</b> In <b>Machine</b> <b>Learning</b>", "url": "https://www.telusinternational.com/articles/7-types-of-data-bias-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.telusinternational.com/articles/7-types-of-<b>data</b>-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "How do I avoid <b>data</b> <b>bias</b> in <b>machine</b> <b>learning</b> projects? The prevention of <b>data</b> <b>bias</b> in <b>machine</b> <b>learning</b> projects is an ongoing process. Though it is sometimes difficult to know when your <b>machine</b> <b>learning</b> <b>algorithm</b>, <b>data</b> or model is <b>biased</b>, there are a number of steps you <b>can</b> take to help prevent <b>bias</b> or catch it early. Though far from a comprehensive list, the bullet points below provide an entry-level guide for thinking about <b>data</b> <b>bias</b> for <b>machine</b> <b>learning</b> projects.", "dateLastCrawled": "2022-02-03T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Explaining Bias in Your Data</b> - Blog - Dataiku", "url": "https://blog.dataiku.com/explaining-bias-in-your-data", "isFamilyFriendly": true, "displayUrl": "https://blog.<b>data</b>iku.com/<b>explaining-bias-in-your-data</b>", "snippet": "An in-depth review of unfairness and <b>bias</b> causes in <b>machine</b> <b>learning</b> and their root in <b>data</b>. ... In this article, we will deal with how to explain unfairness in a <b>machine</b> <b>learning</b> <b>algorithm</b>. In 2014, in a report called Big <b>Data</b>: Seizing Opportunities and Preserving Values, the Executive Office of President Obama pointed out the fact that \u201cbig <b>data</b> technologies <b>can</b> cause societal harms beyond damages to privacy, such as discrimination against individuals and groups.&quot; This was the first time ...", "dateLastCrawled": "2022-02-02T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "5 Types of <b>bias</b> &amp; how to eliminate them in your <b>machine</b> <b>learning</b> ...", "url": "https://towardsdatascience.com/5-types-of-bias-how-to-eliminate-them-in-your-machine-learning-project-75959af9d3a0", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/5-types-of-<b>bias</b>-how-to-eliminate-them-in-your-<b>machine</b>...", "snippet": "T he following is a devastating truth about a <b>biased</b> <b>machine</b> <b>learning</b> program that happened in real life. It is safe to say that the following is an example of the reasons why racism still exists. It\u2019s what I\u2019d like to start with to show you how important it is to fix any <b>bias</b> in your AI program. Compas. Developed by a private company called Equiv a nt (formerly Northpointe). Compas is a <b>machine</b> <b>learning</b> <b>algorithm</b> that predicts the defendant\u2019s likelihoods to commit crimes, it has been ...", "dateLastCrawled": "2022-01-26T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Explaining <b>Bias</b> in Your <b>Data</b> - Adolfo Eliaz\u00e0t - Artificial Intelligence ...", "url": "https://adolfoeliazat.com/2021/07/31/explaining-bias-in-your-data/", "isFamilyFriendly": true, "displayUrl": "https://adolfoeliazat.com/2021/07/31/explaining-<b>bias</b>-in-your-<b>data</b>", "snippet": "A.I. <b>Bias</b> A.I. News. Over the last five years, unfairness in <b>machine</b> <b>learning</b> has gone from almost unknown to hitting the headlines frequently, and new cases of unwanted <b>bias</b> introduced in automated processes are frequently discovered. However, there is still no \u201cone-size-fits-all\u201d standard <b>machine</b> <b>learning</b> tool to prevent and assess such <b>bias</b>.", "dateLastCrawled": "2022-01-30T20:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "News and Media <b>Bias</b> Detection using <b>Machine Learning</b> - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/news-and-media-bias-detection-using-machine-learning-a-potential-way-to-find-fake-news-13c766aa3988", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/news-and-media-<b>bias</b>-detection-using-<b>machine-learning</b>-a...", "snippet": "<b>Machine learning</b> has recently seen a huge increase because of a rise in both available <b>data</b> and computational power. Researchers have also been working to make even more complex neural networks with more and more layers (deep <b>learning</b>), which allows them to solve even harder problems. <b>Machine learning</b> itself has a bunch of applications in almost every field imaginable; recent advances in <b>machine learning</b> include self-driving cars, language translation, and facial recognition. I previously ...", "dateLastCrawled": "2022-01-20T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evolution and impact of <b>bias</b> in human and <b>machine learning</b> <b>algorithm</b> ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "snippet": "Traditionally, <b>machine learning</b> algorithms relied on reliable labels from experts to build predictions. More recently however, algorithms have been receiving <b>data</b> from the general population in the form of labeling, annotations, etc. The result is that algorithms are subject to <b>bias</b> that is born from ingesting unchecked information, such as <b>biased</b> samples and <b>biased</b> labels. Furthermore, people and algorithms are increasingly engaged in interactive processes wherein neither the human nor the ...", "dateLastCrawled": "2021-11-15T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "A <b>machine</b> <b>learning</b> <b>algorithm</b> takes <b>data</b> as an ... There is also risk that <b>bias</b> <b>can</b> be amplified over time by feedback loops, as models are incrementally re-<b>trained</b> on new <b>data</b> generated, either ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How AI model <b>bias</b> impacts trust | Deloitte Insights", "url": "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/ai-model-bias.html", "isFamilyFriendly": true, "displayUrl": "https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/ai-model-<b>bias</b>", "snippet": "The <b>algorithm</b> is <b>trained</b> on an initial set of <b>data</b> to give an idea of what normal versus fraudulent transactions look like. However, the training <b>data</b> becomes <b>biased</b> by oversampling applicants over 45 years of age for examples of fraudulent behavior. This oversampling continues over a period of months, with the <b>bias</b> growing and remaining ...", "dateLastCrawled": "2022-01-30T07:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Even <b>Kids Can Understand That Algorithms Can Be</b> <b>Biased</b> - Scientific ...", "url": "https://blogs.scientificamerican.com/roots-of-unity/even-kids-can-understand-that-algorithms-can-be-biased/", "isFamilyFriendly": true, "displayUrl": "https://blogs.scientificameri<b>can</b>.com/roots-of-unity/even-kids-<b>can</b>-understand-that...", "snippet": "A <b>machine</b> <b>learning</b> <b>algorithm</b> looks at <b>data</b> about how decisions were made in the past and uses it to make future decisions. For example, when you go to Amazon or YouTube and browse for the next ...", "dateLastCrawled": "2022-01-26T18:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Training Report on Machine Learning</b> | Sahdev Kansal - Academia.edu", "url": "https://www.academia.edu/42924139/Training_Report_on_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/42924139/<b>Training_Report_on_Machine_Learning</b>", "snippet": "Translate PDF. INDUSTRIAL <b>TRAINING REPORT ON \u201cMACHINE LEARNING</b>\u201d Submitted in partial fulfillment of the requirements for the award of the degree of BACHELOR OF TECHNOLOGY IN COMPUTER SCIENCE ENGINEERING Submitted By Sahdev Kansal, Enrollment no. (41015602717) Department of Computer Science Engineering Dr. Akhilesh Das Gupta Institute of ...", "dateLastCrawled": "2022-01-30T20:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "If a <b>machine learning</b> <b>algorithm</b> was <b>trained</b> solely on video of daytime driving, it would have tragic results if the model were permitted to drive at night. This is different from human <b>bias</b>, but demonstrates the issue of lacking a representative <b>data</b> set for the problem at hand. <b>Bias</b> <b>can</b> also show up where we don\u2019t expect it. In the case of Amazon\u2019s recruitment tool, the model penalized wording used by some candidates and rewarded words by others. In this case, the penalized words were ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Racially Unbiased, <b>Machine</b> <b>Learning</b> Approach to Prediction of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7644374/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7644374", "snippet": "<b>Bias</b> was assessed through the equal opportunity difference. Model performance in terms of <b>bias</b> and accuracy was <b>compared</b> with the Modified Early Warning Score (MEWS), the Simplified Acute Physiology Score II (SAPS II), and the Acute Physiologic Assessment and Chronic Health Evaluation (APACHE). Results. The <b>machine</b> <b>learning</b> <b>algorithm</b> was found to be more accurate than all comparators, with a higher sensitivity, specificity, and area under the receiver operating characteristic. The <b>machine</b> ...", "dateLastCrawled": "2021-12-18T01:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Is <b>bias a problem in machine</b> <b>learning</b>? - Aveni", "url": "https://aveni.ai/is-bias-a-problem-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://aveni.ai/is-<b>bias-a-problem-in-machine</b>-<b>learning</b>", "snippet": "In all decision-making processes, whether human or <b>machine</b>, <b>bias</b> <b>can</b> result in unfair outcomes. We discuss why this <b>can</b> be a problem for <b>machine</b> <b>learning</b>, and what we <b>can</b> do about it. <b>Machine</b> <b>Learning</b>: <b>Machine</b> <b>learning</b> (ML) is very good at capturing signals and correlations in <b>data</b>. Deep <b>learning</b>, coupled with substantial <b>data</b>, and lots of computation, allows us to build systems that exploit correlations in <b>data</b>.", "dateLastCrawled": "2022-01-30T17:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "5 Types of <b>bias</b> &amp; how to eliminate them in your <b>machine</b> <b>learning</b> ...", "url": "https://towardsdatascience.com/5-types-of-bias-how-to-eliminate-them-in-your-machine-learning-project-75959af9d3a0", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/5-types-of-<b>bias</b>-how-to-eliminate-them-in-your-<b>machine</b>...", "snippet": "T he following is a devastating truth about a <b>biased</b> <b>machine</b> <b>learning</b> program that happened in real life. It is safe to say that the following is an example of the reasons why racism still exists. It\u2019s what I\u2019d like to start with to show you how important it is to fix any <b>bias</b> in your AI program. Compas. Developed by a private company called Equiv a nt (formerly Northpointe). Compas is a <b>machine</b> <b>learning</b> <b>algorithm</b> that predicts the defendant\u2019s likelihoods to commit crimes, it has been ...", "dateLastCrawled": "2022-01-26T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Biases in <b>machine</b> <b>learning</b> models and big <b>data</b> analytics: The ...", "url": "http://international-review.icrc.org/articles/biases-machine-learning-big-data-analytics-ihl-implications-913", "isFamilyFriendly": true, "displayUrl": "international-review.icrc.org/articles/<b>bias</b>es-<b>machine</b>-<b>learning</b>-big-<b>data</b>-analytics-ihl...", "snippet": "<b>Data</b> sets often contain biases which have the potential to unfairly disadvantage certain groups or to over-focus on certain activities to the detriment of others, and ML models or big <b>data</b> analytics <b>trained</b> on such <b>data</b> sets <b>can</b> inherit these biases.22 The following section discusses human biases that most commonly appear in <b>data</b> sets used for ML models and thus are most likely to impact ICL investigations and IHL considerations: implicit <b>bias</b>, selection <b>bias</b>, <b>reporting</b> <b>bias</b>, group ...", "dateLastCrawled": "2022-02-01T10:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How <b>to Remove Bias in Training Data</b> for <b>Machine</b> <b>Learning</b> | Appen", "url": "https://appen.com/blog/how-to-remove-bias-in-training-data/", "isFamilyFriendly": true, "displayUrl": "https://appen.com/blog/how-<b>to-remove-bias-in-training-data</b>", "snippet": "<b>Machine</b> <b>learning</b> (ML) algorithms are generally only as good as the <b>data</b> they are <b>trained</b> on. <b>Bias</b> in ML training <b>data</b> <b>can</b> take many forms, but the end result is that it <b>can</b> cause an <b>algorithm</b> to miss the relevant relations between features and target outputs. Whether your organization is a small business, global enterprise, or governmental ...", "dateLastCrawled": "2022-02-02T01:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evolution and impact of <b>bias</b> in human and <b>machine learning</b> <b>algorithm</b> ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "snippet": "Traditionally, <b>machine learning</b> algorithms relied on reliable labels from experts to build predictions. More recently however, algorithms have been receiving <b>data</b> from the general population in the form of labeling, annotations, etc. The result is that algorithms are subject to <b>bias</b> that is born from ingesting unchecked information, such as <b>biased</b> samples and <b>biased</b> labels. Furthermore, people and algorithms are increasingly engaged in interactive processes wherein neither the human nor the ...", "dateLastCrawled": "2021-11-15T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Understanding <b>Data</b> <b>Bias</b>. Types and sources of <b>data</b> <b>bias</b> | by Prabhakar ...", "url": "https://towardsdatascience.com/survey-d4f168791e57", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/survey-d4f168791e57", "snippet": "The huge success of <b>machine</b> <b>learning</b> (ML) applications in the past decade \u2014 in image recognition, re c ommendation systems, e-commerce and online advertising \u2014 has inspired its adoption in domains such as social justice, employment screening, and in smart interactive interfaces such as Siri, Alexa, and the like. Along with the proliferation of these applications, there has been an alarming rise in reports of gender, race and other types of <b>bias</b> in these systems.", "dateLastCrawled": "2022-02-02T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "8 types of <b>bias</b> in <b>data</b> analysis and how to avoid them", "url": "https://searchbusinessanalytics.techtarget.com/feature/8-types-of-bias-in-data-analysis-and-how-to-avoid-them", "isFamilyFriendly": true, "displayUrl": "https://searchbusinessanalytics.techtarget.com/feature/8-types-of-<b>bias</b>-in-<b>data</b>...", "snippet": "<b>Bias</b> in <b>data</b> analysis <b>can</b> come from human sources because they use unrepresentative <b>data</b> sets, leading questions in surveys and <b>biased</b> <b>reporting</b> and measurements. Often <b>bias</b> goes unnoticed until you&#39;ve made some decision based on your <b>data</b>, such as building a predictive model that turns out to be wrong. How Diverse Talent Pools <b>Can</b> Help Solve <b>Bias</b> In AI. Learn from the head of product inclusion at Google and other leaders as they provide advice on how organizations <b>can</b> bring historically ...", "dateLastCrawled": "2022-02-02T22:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Managing <b>bias and unfairness</b> in <b>data</b> for decision support: a survey of ...", "url": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "snippet": "At training time, this would increase the likelihood that the outputs of a model <b>trained</b> on such dataset are fair (note: an \u201cunbiased\u201d training dataset does not guarantee an unbiased resulting system since new unwanted biases might arise from the <b>machine</b> <b>learning</b> <b>algorithm</b> used or small unwanted biases in the <b>data</b> might be reinforced by the <b>machine</b> <b>learning</b> model, but helps); while at deployment time, it would monitor whether the predictions made for new <b>data</b> points are fair. Constraints ...", "dateLastCrawled": "2022-02-01T12:12:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "Even humans can unintentionally amplify <b>bias</b> in <b>machine learning</b> models. <b>Bias</b> in humans can be unconscious (also called implicit <b>bias</b>), ... This permits <b>analogy</b> puzzles, such as \u201cman is to king as woman is to x.\u201d Computing x results in queen, which is a reasonable answer. But looking at other analogies identifies some potential areas of <b>bias</b>. For example, \u201cman is to computer-programmer as woman is to homemaker\u201d reflects a gender <b>bias</b>. Equally problematic is \u201cfather is to doctor as ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Artificial Intelligence, <b>Machine</b> <b>Learning</b>, and <b>Bias</b> In Finance ...", "url": "https://www.academia.edu/69033409/Artificial_Intelligence_Machine_Learning_and_Bias_In_Finance_Toward_Responsible_Innovation", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69033409/Artificial_Intelligence_<b>Machine</b>_<b>Learning</b>_and_<b>Bias</b>_In...", "snippet": "ARTIFICIAL INTELLIGENCE, <b>MACHINE</b> <b>LEARNING</b>, AND <b>BIAS</b> IN FINANCE: TOWARD RESPONSIBLE INNOVATION Kristin Johnson,* Frank Pasquale** &amp; Jennifer Chapman*** INTRODUCTION Over the last decade, a growing number of digital startups launched bids to lure business from the financial services industry.1 Armed with what they claim are vast quantities of data and sophisticated algorithmic platforms capable of interpreting the data,2 these financial technology (\u201cfintech\u201d)3 * McGlinchey Stafford ...", "dateLastCrawled": "2022-01-27T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Mitigating <b>bias</b> in <b>machine</b> <b>learning</b> for medicine", "url": "https://www.researchgate.net/publication/354073359_Mitigating_bias_in_machine_learning_for_medicine", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354073359_Mitigating_<b>bias</b>_in_<b>machine</b>_<b>learning</b>...", "snippet": "Strategies for mitigating <b>bias</b> across the different steps in <b>machine</b> <b>learning</b> systems development. Diagram outlining proposed solutions on how to mitigate <b>bias</b> across the different development ...", "dateLastCrawled": "2022-01-21T21:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Comparison of <b>machine</b>-<b>learning</b> methodologies for accurate diagnosis of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8128240/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8128240", "snippet": "The residual 3,571 genes (approximately 60%) are processed with all investigated <b>machine</b>-<b>learning</b>-based methods independently in <b>analogy</b> to the second experiment. Classification is again repeated for the native testing data and all levels of data degeneration for 100 iterations while performance is reported. In a detailed comparison of experiments 2 and 3 (Tables", "dateLastCrawled": "2022-01-26T11:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Survey on <b>Bias</b> and <b>Fairness in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/a-survey-on-bias-and-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-on-<b>bias</b>-and-<b>fairness-in-machine-learning</b>", "snippet": "Fighting against <b>bias</b> and discrimination has a long history in philosophy and psychology, and recently in <b>machine</b>-<b>learning</b>. However, in order to be able to fight against discrimination and achieve fairness, one should first define the notion of fairness. Philosophy and psychology have tried to define the concept of fairness long before computer science started exploring it. The fact that there is still no universal definition of fairness speaks for itself. Different preferences and outlooks ...", "dateLastCrawled": "2022-01-22T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Using Artificial Intelligence, Machine Learning, and Predictive</b> ...", "url": "https://www.researchgate.net/publication/329327717_Using_Artificial_Intelligence_Machine_Learning_and_Predictive_Analytics_in_Decision-making", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/329327717_Using_Artificial_Intelligence...", "snippet": "<b>Machine</b> <b>learning</b> and human <b>bias</b>. ... such as the ability to cluster related concepts and to solve <b>analogy</b> tasks. The resulting embeddings can be used in applications without amplifying gender <b>bias</b> ...", "dateLastCrawled": "2021-11-27T04:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>Learning</b> and Theological Traditions of <b>Analogy</b> - Davison - 2021 ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/moth.12682", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/moth.12682", "snippet": "12 <b>Machine</b> <b>Learning</b>, <b>Analogy</b>, and God. The texts considered in this article come from theological sources. They have offered ways to think analogically about features of the world, in this case the similarities we are beginning to see between capacities in <b>machine</b> <b>learning</b> and those in human beings and other animals. Much of the mediaeval ...", "dateLastCrawled": "2021-04-16T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Latent <b>bias</b> and the implementation of <b>artificial intelligence</b> in ...", "url": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "snippet": "<b>artificial intelligence</b>, <b>machine</b> <b>learning</b>, <b>bias</b>, ... (AI) in general, and <b>machine</b> <b>learning</b> in particular, by all accounts, appear poised to revolutionize medicine. 1\u20133 With a wide spectrum of potential uses across translational research (from bench to bedside to health policy), clinical medicine (including diagnosis, treatment, prediction, and healthcare resource allocation), and public health, every area of medicine will be affected. Estimates suggest upwards of $6 billion of investment ...", "dateLastCrawled": "2022-01-28T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Overfitting avoidance as <b>bias</b> - Springer", "url": "https://link.springer.com/content/pdf/10.1007%2FBF00993504.pdf", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/content/pdf/10.1007/BF00993504.pdf", "snippet": "to dispel in demonstrating that overfitting avoidance is a form of <b>bias</b>. To draw an <b>analogy</b>, suppose we are given time to experiment with an unfair coin and then asked to predict whether it will show heads or tails on the next flip. A basic strategy consists of flipping the coin a number of times and predicting for the next flip whichever side of the coin has appeared most often. Consider two variations of this strategy. The first calls for a prediction of heads if heads is flipped in at ...", "dateLastCrawled": "2022-01-28T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Disarming Loaded Words: Addressing Gender Bias in Political</b> <b>Reporting</b>", "url": "https://cpb-us-w2.wpmucdn.com/express.northeastern.edu/dist/d/53/files/2019/11/CJ_2020_paper_68.pdf", "isFamilyFriendly": true, "displayUrl": "https://cpb-us-w2.wpmucdn.com/express.northeastern.edu/dist/d/53/files/2019/11/CJ_2020...", "snippet": "<b>Disarming Loaded Words: Addressing Gender Bias in Political</b> <b>Reporting</b> Irena Fischer-Hwang, Dylan Grosz, ... both <b>machine</b> <b>learning</b> and human expert curation, that tags potentially biased words in a document and provides feed-back and context for why a word may be problematic. DLW provides a minimally disruptive way to nudge journalists to-ward understanding unconscious biases. It was successfully prototyped as a standalone Google Docs add-on, and the code for its model, interface, and API are ...", "dateLastCrawled": "2021-08-28T20:11:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(reporting bias)  is like +(machine learning algorithm trained on biased data)", "+(reporting bias) is similar to +(machine learning algorithm trained on biased data)", "+(reporting bias) can be thought of as +(machine learning algorithm trained on biased data)", "+(reporting bias) can be compared to +(machine learning algorithm trained on biased data)", "machine learning +(reporting bias AND analogy)", "machine learning +(\"reporting bias is like\")", "machine learning +(\"reporting bias is similar\")", "machine learning +(\"just as reporting bias\")", "machine learning +(\"reporting bias can be thought of as\")", "machine learning +(\"reporting bias can be compared to\")"]}