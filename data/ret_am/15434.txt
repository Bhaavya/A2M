{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Co-Training</b> for Deep Object Detection: Comparing Single-Modal and Multi ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8125436/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8125436", "snippet": "Briefly, <b>co-training</b> is based on <b>two</b> <b>models</b>, ... The labeled images <b>together</b> with the self-labeled by <b>co-training</b> up to the indicated cycle are used to train the corresponding \u03d5 F. Then, we plot (y-axis) the accuracy (mAP) of each \u03d5 F in the corresponding testing set, i.e., either K t t or W t t. We can see how <b>co-training</b> strategies allow improving over the LBs from early iterations and, although slightly oscillating, keep improving until stabilization is reached. No drifting to erroneous ...", "dateLastCrawled": "2021-11-15T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Co-training</b> Transformer with Videos and Images Improves Action ...", "url": "https://deepai.org/publication/co-training-transformer-with-videos-and-images-improves-action-recognition", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>co-training</b>-transformer-with-videos-and-images-improves...", "snippet": "Compared with <b>co-training</b> on just <b>two</b> datasets, <b>co-training</b> on all three improves K400 and SSv2 performance by 0.8% and 0.7%, respectively. We observe that <b>co-training</b> with MiT also improves MiT performance by 1.0%. Finally, we co-train both image and video datasets <b>together</b>. Adding ImageNet in the <b>co-training</b> datasets further improves SSv2 and MiT by 0.2% and 0.2%, establishing a new SoTA. We observe a similar improvement on the model co-trained with K600+SSv2+MiT+ImageNet and K700+SSv2+MiT ...", "dateLastCrawled": "2022-01-07T16:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "CoG: a <b>Two</b>-View <b>Co-training</b> Framework for Defending Adversarial Attacks ...", "url": "https://deepai.org/publication/cog-a-two-view-co-training-framework-for-defending-adversarial-attacks-on-graph", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/cog-a-<b>two</b>-view-<b>co-training</b>-framework-for-defending...", "snippet": "CoG: a <b>Two</b>-View <b>Co-training</b> Framework for Defending Adversarial Attacks on Graph. ... Topological attacks are <b>more</b> effective than feature attacks when attacking <b>models</b> <b>like</b> GCN; (2) Attackers tend to connect nodes with dissimilar node features to achieve a successful attack. Therefore, to fully exploit the feature information and achieve better robustness, we propose a <b>two</b>-view <b>co-training</b> framework, named <b>Co-training</b> on Graph (CoG), to learn a robust ensemble of the feature-dominant <b>models</b> ...", "dateLastCrawled": "2021-11-28T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Co-training</b> <b>Using Prosodic and Lexical Information</b> for Sentence ...", "url": "https://www.academia.edu/4804807/Co_training_Using_Prosodic_and_Lexical_Information_for_Sentence_Segmentation", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/4804807/<b>Co_training</b>_<b>Using_Prosodic_and_Lexical_Information</b>...", "snippet": "<b>Co-training</b> algorithms work by generating using self-training and <b>co-training</b> with the ICSI Meeting <b>two</b> <b>or more</b> classifiers trained on different views of the input Recorder Dialog Act (MRDA) corpus in Section 4. labeled data that are then used to label the unlabeled data separately. The most confidently labeled examples of the 2. Related Work automatically labeled data can then be added to the set of manually labeled data. The process may continue for several The <b>co-training</b> approach was ...", "dateLastCrawled": "2022-01-25T19:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Co-Training</b> as a Human Collaboration Policy", "url": "https://pages.cs.wisc.edu/~jerryzhu/pub/human_cotraining.pdf", "isFamilyFriendly": true, "displayUrl": "https://pages.cs.wisc.edu/~jerryzhu/pub/human_<b>cotraining</b>.pdf", "snippet": "ing, where <b>two</b> people work <b>together</b> to classify test items into appropriate categories based on what they learn from a train-ing set. We propose a novel collaboration policy based on the <b>Co-Training</b> algorithm in machine learning, in which the <b>two</b> people play the role of the base learners. The policy restricts each learner\u2019s view of the data and limits their communica-tion to only the exchange of their labelings on test items. In a series of empirical studies, we show that the <b>Co-Training</b> ...", "dateLastCrawled": "2022-02-02T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Co-training</b> by Committee: A <b>New Semi-supervised Learning Framework</b> ...", "url": "https://www.researchgate.net/publication/220765112_Co-training_by_Committee_A_New_Semi-supervised_Learning_Framework", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220765112_<b>Co-training</b>_by_Committee_A_New_Semi...", "snippet": "<b>Co-Training</b> is a popular semi-supervised learning algorithm. It assumes that each example is represented by <b>two</b> <b>or more</b> redundantly sufficient sets of features (views) and these views are ...", "dateLastCrawled": "2022-01-28T16:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Multimodal Co-learning: Challenges, applications with datasets, recent ...", "url": "https://www.sciencedirect.com/science/article/pii/S1566253521002530", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1566253521002530", "snippet": "<b>Co-training</b> needs <b>two</b> different views of the same data point; however, data has only one view in many cases. In some instances, multiple views are generated by adding noise or by using data augmentation techniques. Adversarial examples are employed to create different views and to stop <b>two</b> networks from collapsing. The obtained deep <b>co-training</b> model for semi-supervised image recognition showed performance improvement on Street View House Number (SVHN) , CIFAR 10/100 , and ImageNet datasets ...", "dateLastCrawled": "2022-01-27T22:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Self-paced Multi-view <b>Co-training</b>", "url": "https://jmlr.csail.mit.edu/papers/volume21/18-794/18-794.pdf", "isFamilyFriendly": true, "displayUrl": "https://jmlr.csail.mit.edu/papers/volume21/18-794/18-794.pdf", "snippet": "ing procedure of conventional <b>co-training</b> in <b>two</b>-view cases. In this study, we have made a substantial improvement to the prior work. Speci cally, this paper proposes a general framework for multi-view <b>co-training</b>, which allows rich variations for practical realizations. The SPaCo algorithm in Ma et al. (2017) is only a speci c hard implementation scheme contained in this framework only usable for <b>two</b>-view cases, by properly setting the forms of self-paced regularizer and the weight co ...", "dateLastCrawled": "2022-01-31T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "CoG: a <b>Two</b>-View <b>Co-training</b> Framework for Defending Adversarial Attacks ...", "url": "https://malwaredefinition.com/index.php/2021/09/14/cog-a-two-view-co-training-framework-for-defending-adversarial-attacks-on-graph-arxiv2109-05558v1-cs-lg/security-world-news/admin/", "isFamilyFriendly": true, "displayUrl": "https://malwaredefinition.com/index.php/2021/09/14/cog-a-<b>two</b>-view-<b>co-training</b>...", "snippet": "<b>more</b> comprehensive features. Graph data, in most cases, has <b>two</b> views of information, namely structure information and feature information. In this paper, we propose CoG, a simple yet effective <b>co-training</b> framework to combine these <b>two</b> views for the purpose of robustness. CoG trains sub-<b>models</b> from the feature view and the structure view independently and allows them to distill knowledge from each other by adding their most confident unlabeled data into the training set. The orthogonality ...", "dateLastCrawled": "2021-09-14T12:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Uncertainty-aware multi-view <b>co-training</b> for semi-supervised medical ...", "url": "https://www.sciencedirect.com/science/article/pii/S1361841520301304", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1361841520301304", "snippet": "Blum and Mitchell (1998) further proved that <b>co-training</b> has PAC-<b>like</b> guarantees on semi-supervised learning with an additional assumption that the <b>two</b> views are conditionally independent given the category. Since most computer vision tasks have only one source of data, encouraging view differences is a crucial factor for successful <b>co-training</b>. For example, deep <b>co-training</b> (Qiao et al., 2018) trains multiple deep networks to act as different views by utilizing adversarial examples ...", "dateLastCrawled": "2022-01-27T16:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Co-Training</b> for Deep Object Detection: Comparing Single-Modal and Multi ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8125436/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8125436", "snippet": "Briefly, <b>co-training</b> is based on <b>two</b> <b>models</b>, ... The labeled images <b>together</b> with the self-labeled by <b>co-training</b> up to the indicated cycle are used to train the corresponding \u03d5 F. Then, we plot (y-axis) the accuracy (mAP) of each \u03d5 F in the corresponding testing set, i.e., either K t t or W t t. We can see how <b>co-training</b> strategies allow improving over the LBs from early iterations and, although slightly oscillating, keep improving until stabilization is reached. No drifting to erroneous ...", "dateLastCrawled": "2021-11-15T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>SplitNet: Divide and Co-training</b> - ResearchGate", "url": "https://www.researchgate.net/publication/346511273_Towards_Better_Accuracy-efficiency_Trade-offs_Divide_and_Co-training/fulltext/5fc5d53fa6fdcce952691bd0/346511273_Towards_Better_Accuracy-efficiency_Trade-offs_Divide_and_Co-training.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/346511273_Towards_Better_Accuracy-efficiency...", "snippet": "inal de\ufb01nition, many works involving <b>two</b> <b>or more</b> <b>models</b> learning <b>together</b> can also be called collaborative learning, e.g., deep mutual learning (DML) [62], <b>co-training</b> [4, 41],", "dateLastCrawled": "2021-09-01T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep <b>Co-Training</b> With Task Decomposition for Semi-Supervised Domain ...", "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Deep_Co-Training_With_Task_Decomposition_for_Semi-Supervised_Domain_Adaptation_ICCV_2021_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Deep_<b>Co-Training</b>_With_Task...", "snippet": "<b>Co-training</b> vs. co-teaching. Co-teaching [17] was pro-posed for learning with noisy data, which shares a <b>similar</b> procedure to <b>co-training</b> by learning <b>two</b> <b>models</b> to \ufb01lter out noisy data for each other. There are several key differences between them and DECOTA is based on <b>co-training</b>. As", "dateLastCrawled": "2022-01-28T22:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "CoG: a <b>Two</b>-View <b>Co-training</b> Framework for Defending Adversarial Attacks ...", "url": "https://deepai.org/publication/cog-a-two-view-co-training-framework-for-defending-adversarial-attacks-on-graph", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/cog-a-<b>two</b>-view-<b>co-training</b>-framework-for-defending...", "snippet": "Nevertheless, applying vanilla <b>co-training</b> framework to graph data meets <b>two</b> challenges: (1) <b>Co-training</b> uses the softmax outputs as the indicator of sub-<b>models</b>\u2019 confidence. However, this can be inaccurate since neural networks are often miscalibrated, especially when sub-<b>models</b> are heterogenous. (2) <b>Co-training</b> selects unlabeled data simply based on their confidence. If dominant classes exist, the <b>co-training</b> process will amplify the imbalance of classes and force the sub-<b>models</b> to ...", "dateLastCrawled": "2021-11-28T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Co-training</b> Transformer with Videos and Images Improves Action ...", "url": "https://deepai.org/publication/co-training-transformer-with-videos-and-images-improves-action-recognition", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>co-training</b>-transformer-with-videos-and-images-improves...", "snippet": "Compared with <b>co-training</b> on just <b>two</b> datasets, <b>co-training</b> on all three improves K400 and SSv2 performance by 0.8% and 0.7%, respectively. We observe that <b>co-training</b> with MiT also improves MiT performance by 1.0%. Finally, we co-train both image and video datasets <b>together</b>. Adding ImageNet in the <b>co-training</b> datasets further improves SSv2 and MiT by 0.2% and 0.2%, establishing a new SoTA. We observe a <b>similar</b> improvement on the model co-trained with K600+SSv2+MiT+ImageNet and K700+SSv2+MiT ...", "dateLastCrawled": "2022-01-07T16:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Co-Training</b> as a Human Collaboration Policy", "url": "https://pages.cs.wisc.edu/~jerryzhu/pub/human_cotraining.pdf", "isFamilyFriendly": true, "displayUrl": "https://pages.cs.wisc.edu/~jerryzhu/pub/human_<b>cotraining</b>.pdf", "snippet": "\ufb01ed version of Blum and Mitchell\u2019s <b>Co-Training</b> algorithm is given in Algorithm 1. To classify a new test item ex, one can compare the predictions f(1)(ex(1)) and f(2)(ex(2)) and pick the one with higher con\ufb01dence. <b>Co-Training</b> is a \u201cwrapper\u201d method in that the <b>two</b> base learners f(1) and f(2) can be any learning systems. The only", "dateLastCrawled": "2022-02-02T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Applying <b>Co-Training</b> methods to Statistical Parsing", "url": "https://aclanthology.org/N01-1023.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/N01-1023.pdf", "snippet": "Combining the trees <b>together</b> by rewriting nodes as trees (explained in Section 2.1) gives us the parse tree in Figure 1. A history of the bi-lexical dependencies that de\ufb01ne the probability model used to construct the parse is shown in Figure 3. This history is called the derivation tree. In addition, as a byproduct of this kind of represen-tation we obtain <b>more</b> than the phrase structure of each sentence. We also produce a <b>more</b> embellished parse in which phenomena such as predicate-argument ...", "dateLastCrawled": "2021-12-25T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Self-paced Multi-view <b>Co-training</b>", "url": "https://jmlr.csail.mit.edu/papers/volume21/18-794/18-794.pdf", "isFamilyFriendly": true, "displayUrl": "https://jmlr.csail.mit.edu/papers/volume21/18-794/18-794.pdf", "snippet": "ing procedure of conventional <b>co-training</b> in <b>two</b>-view cases. In this study, we have made a substantial improvement to the prior work. Speci cally, this paper proposes a general framework for multi-view <b>co-training</b>, which allows rich variations for practical realizations. The SPaCo algorithm in Ma et al. (2017) is only a speci c hard implementation scheme contained in this framework only usable for <b>two</b>-view cases, by properly setting the forms of self-paced regularizer and the weight co ...", "dateLastCrawled": "2022-01-31T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Semi-Supervised Learning using Siamese Networks</b>", "url": "https://www.cs.waikato.ac.nz/~eibe/pubs/semi_supervised_siamese.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.waikato.ac.nz/~eibe/pubs/semi_supervised_siamese.pdf", "snippet": "providing <b>more</b> training instances for the supervised learning algorithm. <b>Co-training</b> [1] is a <b>similar</b> approach, where <b>two</b> <b>models</b> are trained on <b>two</b> separate subsets of the data features. Confident predictions from one model are then used", "dateLastCrawled": "2022-02-02T19:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "L&amp;D <b>Training</b> <b>Models</b> for Employee: The 10 Best Learning and Development ...", "url": "https://www.novoed.com/resources/blog/modern-corporate-training-models/", "isFamilyFriendly": true, "displayUrl": "https://www.novoed.com/resources/blog/modern-<b>corporate-training</b>-<b>models</b>", "snippet": "Jane Hart, director and editor of the Centre for Modern Workplace Learning, recently wrote an article about a survey that she is conducting on the valued ways of Learning in the Workforce.Her results find that the formal activities offered by most L&amp;D organizations (i.e. e-learning, conferences, classroom <b>training</b>) are actually considered the least valuable to learners\u2019 development.", "dateLastCrawled": "2022-02-01T14:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Co-Training and Expansion: Towards Bridging Theory and</b> Practice ...", "url": "https://www.researchgate.net/publication/221619974_Co-Training_and_Expansion_Towards_Bridging_Theory_and_Practice", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221619974_<b>Co-Training</b>_and_Expansion_Towards...", "snippet": "<b>Co-training</b> is a method for combining labeled and unlabeled data when examples <b>can</b> <b>be thought</b> of as containing <b>two</b> distinct sets of features. It has had a number of practical successes, yet ...", "dateLastCrawled": "2022-01-24T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Co-Training</b> as a Human Collaboration Policy", "url": "https://pages.cs.wisc.edu/~jerryzhu/pub/human_cotraining.pdf", "isFamilyFriendly": true, "displayUrl": "https://pages.cs.wisc.edu/~jerryzhu/pub/human_<b>cotraining</b>.pdf", "snippet": "The linearly non-separable classi\ufb01cation achieved by <b>Co-Training</b> is just the kind of solution that human beings have dif\ufb01culty learning without extensive supervision (Love 2002). In the current work we consider whether the <b>Co-Training</b> algorithm <b>can</b> be used to design a collaboration policy for human participants that will promote learning", "dateLastCrawled": "2022-02-02T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Co-Training</b> as a Human Collaboration Policy", "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7945/7804", "isFamilyFriendly": true, "displayUrl": "https://ojs.aaai.org/index.php/AAAI/article/view/7945/7804", "snippet": "<b>Co-Training</b> as a Human Collaboration Policy ... ate machine learning <b>models</b>. Though human learning abilities are remarkable in many respects, they are also constrained in ways that may seem puzzling to machine learning. As one example, people <b>can</b> have dif\ufb01culty learning nonlinear decision boundaries with-out extensive supervision (Love 2002). As another exam-ple, psychologists often distinguish between feature dimen-sions that are \u201cseparable\u201d versus \u201cintegral\u201d. For separable ...", "dateLastCrawled": "2022-01-01T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Uncertainty-aware multi-view <b>co-training</b> for semi-supervised medical ...", "url": "https://www.sciencedirect.com/science/article/pii/S1361841520301304", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1361841520301304", "snippet": "The <b>co-training</b> assumption encourages <b>models</b> to make similar predictions on both S and U, which potentially <b>can</b> lead to collapsed neural networks mentioned in Qiao et al. (2018), a phenomenon that results in a sudden and significant drop in validation accuracy during training of <b>co-training</b> algorithms. In our multi-view settings, this could also happen when the <b>models</b> from different views only learn the permutation or rotation of the kernels, resulting in exactly the same learned feature ...", "dateLastCrawled": "2022-01-27T16:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Co-embedding: a semi-supervised multi-view representation learning ...", "url": "https://link.springer.com/article/10.1007/s00521-021-06599-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00521-021-06599-y", "snippet": "<b>Co-training</b> first trains <b>two</b> classifiers on labeled data of <b>two</b> different views. Then, these classifiers gradually label unlabeled data and exchange new labels with each other. <b>Co-training</b> utilizes consensus property by assuming classifications yielded by <b>two</b> classifiers are consistent with each other and utilizes complementary property by exchanging complementary information between views. Moreover, Wang and Zhou proved that diversity of different views is the key to the success of co ...", "dateLastCrawled": "2022-01-20T11:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Combining Labeled and Unlabeled Data with</b> <b>Co-Training</b>", "url": "https://www.researchgate.net/publication/2457211_Combining_Labeled_and_Unlabeled_Data_with_Co-Training", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2457211_<b>Combining_Labeled_and_Unlabeled_Data</b>...", "snippet": "The <b>co-training</b> paradigm which jointly trains <b>two</b> <b>models</b> is used to improve the robustness of <b>models</b> (Blum and Mitchell, 1998; Nigam and Ghani, 2000;Kiritchenko and Matwin, 2011). Many previous ...", "dateLastCrawled": "2022-01-29T16:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Multi-Modal <b>Co-Training</b> for Fake News Identification Using Attention ...", "url": "https://easychair.org/publications/preprint_download/kfrT", "isFamilyFriendly": true, "displayUrl": "https://easychair.org/publications/preprint_download/kfrT", "snippet": "well as recent deep learning <b>models</b> [10,23,24,29], most of which rely on tex-tual content or other metadata (like news source, emotional features, number of likes, etc.) including content creator\u2019s pro le information. While a few re-cent works have proposed multimodal methods to address the task of fake news detection [11,13,15,30], majority of these methods just combine di erent mode-speci c feature vectors (visual, textual features, and semantic information), de-rived from some ...", "dateLastCrawled": "2022-01-10T04:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Combining Diverse Feature Priors | DeepAI", "url": "https://deepai.org/publication/combining-diverse-feature-priors", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/combining-diverse-feature-priors", "snippet": "We find that shape- and texture-biased <b>models</b> exhibit low correlation at the start of <b>co-training</b>, but this correlation increases as <b>co-training</b> progresses. This is in contrast to self-training each model on its own, where the correlation remains relatively low. It is also worth noting that the correlation appears to plateau at a lower value when <b>co-training</b> <b>models</b> with distinct feature priors as opposed to <b>co-training</b> <b>two</b> standard <b>models</b>.", "dateLastCrawled": "2022-01-13T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Multi-view Learning and Link Farm Discovery</b> | Lise ... - Academia.edu", "url": "https://www.academia.edu/2791414/Multi_view_Learning_and_Link_Farm_Discovery", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2791414/<b>Multi_view_Learning_and_Link_Farm_Discovery</b>", "snippet": "Abstract The first part of this abstract focuses on estimation of mixture <b>models</b> for problems in which multiple views of the instances are available. Examples of this setting include clustering web pages or research papers that have intrinsic (text)", "dateLastCrawled": "2022-01-19T07:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Read to Succeed Paper Two</b> | <b>Models</b> of Teaching 9th Edition", "url": "http://modelsofteaching.org/sample-page/read-to-succeed-paper-two/", "isFamilyFriendly": true, "displayUrl": "<b>models</b>ofteaching.org/sample-page/<b>read-to-succeed-paper-two</b>", "snippet": "Half of the studies were a year <b>or more</b> long. Most were <b>two</b> years long or longer. The mean effect size for learning science processes 0.52. Scientific content was 0.16. Attitudes toward science and process was 0.28. Smaller subsets of studies examined effects on creativity (0.42),and measures of intelligence (0.48). Computation and mathematical understanding increased modestly. The aggregated mean effect size was 0.30. \u201cThe idea that curriculums should aim at ideas, inductive and other ...", "dateLastCrawled": "2022-01-30T12:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Co-Training</b> for Deep Object Detection: Comparing Single-Modal and Multi ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8125436/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8125436", "snippet": "In these settings, we have <b>compared</b> multi-modal <b>co-training</b> and appearance-based single-modal <b>co-training</b>. We have shown that multi-modal <b>co-training</b> is effective in all settings. In the standard SSL setting, from a 5% of human-labeled training data, <b>co-training</b> <b>can</b> already lead to a final object detection accuracy relatively close to upper bounds (i.e., with the 100% of human labeling). The same observation holds when using virtual-world data, i.e., without human labeling at all. Multi ...", "dateLastCrawled": "2021-11-15T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>SplitNet: Divide and Co-training</b> - ResearchGate", "url": "https://www.researchgate.net/publication/346511273_Towards_Better_Accuracy-efficiency_Trade-offs_Divide_and_Co-training/fulltext/5fc5d53fa6fdcce952691bd0/346511273_Towards_Better_Accuracy-efficiency_Trade-offs_Divide_and_Co-training.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/346511273_Towards_Better_Accuracy-efficiency...", "snippet": "inal de\ufb01nition, many works involving <b>two</b> <b>or more</b> <b>models</b> learning <b>together</b> <b>can</b> also be called collaborative learning, e.g., deep mutual learning (DML) [62], <b>co-training</b> [4, 41],", "dateLastCrawled": "2021-09-01T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Co-Training</b> as a Human Collaboration Policy", "url": "https://pages.cs.wisc.edu/~jerryzhu/pub/human_cotraining.pdf", "isFamilyFriendly": true, "displayUrl": "https://pages.cs.wisc.edu/~jerryzhu/pub/human_<b>cotraining</b>.pdf", "snippet": "ing, where <b>two</b> people work <b>together</b> to classify test items into appropriate categories based on what they learn from a train-ing set. We propose a novel collaboration policy based on the <b>Co-Training</b> algorithm in machine learning, in which the <b>two</b> people play the role of the base learners. The policy restricts each learner\u2019s view of the data and limits their communica-tion to only the exchange of their labelings on test items. In a series of empirical studies, we show that the <b>Co-Training</b> ...", "dateLastCrawled": "2022-02-02T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Co-training</b> <b>Using Prosodic and Lexical Information</b> for Sentence ...", "url": "https://www.academia.edu/4804807/Co_training_Using_Prosodic_and_Lexical_Information_for_Sentence_Segmentation", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/4804807/<b>Co_training</b>_<b>Using_Prosodic_and_Lexical_Information</b>...", "snippet": "<b>Co-training</b> algorithms work by generating using self-training and <b>co-training</b> with the ICSI Meeting <b>two</b> <b>or more</b> classifiers trained on different views of the input Recorder Dialog Act (MRDA) corpus in Section 4. labeled data that are then used to label the unlabeled data separately. The most confidently labeled examples of the 2. Related Work automatically labeled data <b>can</b> then be added to the set of manually labeled data. The process may continue for several The <b>co-training</b> approach was ...", "dateLastCrawled": "2022-01-25T19:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Co-training</b> Transformer with Videos and Images Improves Action ...", "url": "https://deepai.org/publication/co-training-transformer-with-videos-and-images-improves-action-recognition", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>co-training</b>-transformer-with-videos-and-images-improves...", "snippet": "However, since Transformer <b>models</b> are <b>more</b> data-hungry ... We further include the MiT dataset into <b>co-training</b>. <b>Compared</b> with <b>co-training</b> on just <b>two</b> datasets, <b>co-training</b> on all three improves K400 and SSv2 performance by 0.8% and 0.7%, respectively. We observe that <b>co-training</b> with MiT also improves MiT performance by 1.0%. Finally, we co-train both image and video datasets <b>together</b>. Adding ImageNet in the <b>co-training</b> datasets further improves SSv2 and MiT by 0.2% and 0.2%, establishing a ...", "dateLastCrawled": "2022-01-07T16:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Co-Training</b> as a Human Collaboration Policy", "url": "https://ojs.aaai.org/index.php/AAAI/article/view/7945/7804", "isFamilyFriendly": true, "displayUrl": "https://ojs.aaai.org/index.php/AAAI/article/view/7945/7804", "snippet": "<b>Co-Training</b> algorithm in machine learning, in which the <b>two</b> people play the role of the base learners. The policy restricts each learner\u2019s view of the data and limits their communica-tion to only the exchange of their labelings on test items. In a series of empirical studies, we show that the <b>Co-Training</b> policy leads collaborators to jointly produce unique and po-tentially valuable classi\ufb01cation outcomes that are not gener-ated under other collaboration policies. We further demon-strate ...", "dateLastCrawled": "2022-01-01T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Development of Semi-supervised Multiple-output Soft-sensors</b> with Co ...", "url": "https://www.researchgate.net/publication/339130794_Development_of_Semi-supervised_Multiple-output_Soft-sensors_with_Co-training_and_Tri-training_MPLS_and_MRVM", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339130794_Development_of_Semi-supervised...", "snippet": "Unfortunately, most <b>cotraining</b> <b>models</b> are isomorphic <b>co-training</b> methods, even though the <b>co-training</b> algorithm <b>can</b> establish <b>two</b> <b>or more</b> independent regression <b>models</b> to enrich the model ...", "dateLastCrawled": "2022-01-21T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Multiple Feature Fusion Based on <b>Co-Training</b> Approach and Time ...", "url": "https://www.hindawi.com/journals/am/2013/175064/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/am/2013/175064", "snippet": "Interesting additional insights on the <b>Co-Training</b> algorithm <b>can</b> be gained if we perform <b>more</b> than one iteration (see Figure 10). The figures show the evolution of the performance of a single feature classifier after it was iteratively retrained from the standard baseline up to 10 iterations where a constant portion of high confidence estimates were added after each iteration. The plots show an interesting increase of performance with every iteration for both classifiers with the same trend ...", "dateLastCrawled": "2022-01-23T00:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Offline data-driven evolutionary optimization</b> based on tri-training ...", "url": "https://www.sciencedirect.com/science/article/pii/S2210650220304533", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2210650220304533", "snippet": "Basic <b>co-training</b> algorithms have <b>two</b> <b>models</b> (i.e. n = 2). In fact, n <b>can</b> be extended to three <b>or more</b>, which results in <b>co-training</b> variants such as tri-training , multi-train and co-forest . In addition, most work of <b>co-training</b> is for classification except a few for regression.", "dateLastCrawled": "2022-01-16T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "L&amp;D <b>Training</b> <b>Models</b> for Employee: The 10 Best Learning and Development ...", "url": "https://www.novoed.com/resources/blog/modern-corporate-training-models/", "isFamilyFriendly": true, "displayUrl": "https://www.novoed.com/resources/blog/modern-<b>corporate-training</b>-<b>models</b>", "snippet": "Peer feedback <b>can</b> be even <b>more</b> powerful than feedback from experts, partly because new learners <b>can</b> explain concepts to other learners better, and partly because the social connections (including professional networks and communities) bring social connections and reinforcement to the learning dynamic (learn how to optimize feedback in online courses). In addition, Hart points out that a key feature that makes this particular aspect successful is that they are chosen and organized by the ...", "dateLastCrawled": "2022-02-01T14:31:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Supervised Learning and Co-training</b> | Request PDF", "url": "https://www.researchgate.net/publication/268809884_Supervised_Learning_and_Co-training", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/268809884_<b>Supervised_Learning_and_Co-training</b>", "snippet": "\u2022 <b>Co-Training</b> [2]: It is a <b>machine</b> <b>learning</b> algorithm used when there are only some labeled data and large amounts of unlabeled data. One of its uses is in text mining for search engines. ...", "dateLastCrawled": "2021-10-24T01:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Self-Supervised Graph <b>Co-Training</b> for Session-based Recommendation | DeepAI", "url": "https://deepai.org/publication/self-supervised-graph-co-training-for-session-based-recommendation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/self-supervised-graph-<b>co-training</b>-for-session-based...", "snippet": "<b>Co-Training</b> is a classical semi-supervised <b>learning</b> paradigm to exploit unlabeled data (Blum and Mitchell, 1998; Da Costa et al., 2018; Han et al., 2020). Under this regime, two classifiers are separately trained on two views and then exchange confident pseudo labels of unlabeled instances to construct additional labeled training data for each other. Typically, the two views are two disjoint sets of features and can provide complementary information to each other. Blum", "dateLastCrawled": "2022-02-01T08:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Cooperative <b>Learning</b> of Energy-Based Model and Latent Variable Model ...", "url": "http://www.stat.ucla.edu/~ywu/CoopNets/doc/CoopNets_AAAI.pdf", "isFamilyFriendly": true, "displayUrl": "www.stat.ucla.edu/~ywu/CoopNets/doc/CoopNets_AAAI.pdf", "snippet": "3Amazon RSML (Retail System <b>Machine</b> <b>Learning</b>) Group Abstract This paper proposes a cooperative <b>learning</b> algorithm to train both the undirected energy-based model and the directed latent variable model jointly. The <b>learning</b> algorithm interweaves the maximum likelihood algorithms for <b>learning</b> the two models, and each iteration consists of the following two steps: (1) Modi\ufb01ed contrastive divergence for energy-based model: The <b>learning</b> of the energy-based model is based on the contrastive ...", "dateLastCrawled": "2022-02-03T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Interacting meaningfully with machine learning systems</b>: Three ...", "url": "https://dl.acm.org/doi/10.1016/j.ijhcs.2009.03.004", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1016/j.ijhcs.2009.03.004", "snippet": "Although <b>machine</b> <b>learning</b> is becoming commonly used in today&#39;s software, there has been little research into how end users might interact with <b>machine</b> <b>learning</b> systems, beyond communicating simple &#39;&#39;right/wrong&#39;&#39; judgments. If the users themselves could work hand-in-hand with <b>machine</b> <b>learning</b> systems, the users&#39; understanding and trust of the system could improve and the accuracy of <b>learning</b> systems could be improved as well. We conducted three experiments to understand the potential for ...", "dateLastCrawled": "2022-01-28T03:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Overfitting, PAC Learning, VC Dimension</b>, VC Bounds, Mistake Bounds ...", "url": "https://www.cs.cmu.edu/~tom/10701_sp11/recitations/Recitation_9.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~tom/10701_sp11/recitations/Recitation_9.pdf", "snippet": "PAC <b>learning</b> (finite hypothesis space) Consistent learner case, and agnostic case PAC <b>learning</b> (infinite hypothesis space) VC dimension, VC bounds, structural risk minimization Mistake bounds Find-S, Halving algorithm, weighted majority algorithm Semi-supervised <b>learning</b> The general idea, EM, <b>co-training</b>, NELL 36", "dateLastCrawled": "2022-02-02T21:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Using Machine Learning to Understand and Enhance Human Learning Capacity</b>", "url": "http://pages.cs.wisc.edu/~jerryzhu/career/", "isFamilyFriendly": true, "displayUrl": "pages.cs.wisc.edu/~jerryzhu/career", "snippet": "<b>Using Machine Learning to Understand and Enhance Human Learning Capacity</b> Research Projects The overall goal of the project is to develop computational <b>learning</b> models and theory, originally aimed at computers, to predict and influence human <b>learning</b> behaviors. Capacity measure of the human mind What is the VC-dimension of the human mind? In <b>machine</b> <b>learning</b>, the VC-dimension is a well-known capacity measure for a model family. What if the &quot;model family&quot; is the human mind, e.g., all the ...", "dateLastCrawled": "2022-02-01T10:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Learning from positive</b> and unlabeled data: a survey - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "snippet": "<b>Learning from positive</b> and unlabeled data or PU <b>learning</b> is the setting where a learner only has access to positive examples and unlabeled data. The assumption is that the unlabeled data can contain both positive and negative examples. This setting has attracted increasing interest within the <b>machine</b> <b>learning</b> literature as this type of data naturally arises in applications such as medical diagnosis and knowledge base completion. This article provides a survey of the current state of the art ...", "dateLastCrawled": "2022-02-02T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>100 Must-Read</b> NLP Papers | This is a list of 100 important natural ...", "url": "http://masatohagiwara.net/100-nlp-papers/", "isFamilyFriendly": true, "displayUrl": "masatohagiwara.net/100-nlp-papers", "snippet": "<b>Machine</b> <b>Learning</b>. Avrim Blum and Tom Mitchell: Combining Labeled and Unlabeled Data with <b>Co-Training</b>, 1998. John Lafferty, Andrew McCallum, Fernando C.N. Pereira: Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data, ICML 2001. Charles Sutton, Andrew McCallum. An Introduction to Conditional Random Fields for Relational <b>Learning</b>. Kamal Nigam, et al.: Text Classification from Labeled and Unlabeled Documents using EM. <b>Machine</b> <b>Learning</b>, 1999. Kevin Knight ...", "dateLastCrawled": "2022-01-31T22:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Hui&#39;s Homepage", "url": "https://layneins.github.io/", "isFamilyFriendly": true, "displayUrl": "https://layneins.github.io", "snippet": "My main research interests include natural language processing, text mining and <b>machine</b> <b>learning</b>. News [2021.12] ... Unsupervised Conversation Disentanglement through <b>Co-Training</b> Hui Liu, Zhan Shi, Xiaodan Zhu EMNLP 2021 main conference, long paper Retrieval, <b>Analogy</b>, and Composition: A framework for Compositional Generalization in Image Captioning Zhan Shi, Hui Liu, Martin Renqiang Min, Christopher Malon, Li Erran Li and Xiaodan Zhu Findings of EMNLP 2021, long paper Enhancing Descriptive ...", "dateLastCrawled": "2022-02-02T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Interacting meaningfully with <b>machine</b> <b>learning</b> systems : three ...", "url": "https://core.ac.uk/display/10196053", "isFamilyFriendly": true, "displayUrl": "https://core.ac.uk/display/10196053", "snippet": "Although <b>machine</b> <b>learning</b> is becoming commonly used in today&#39;s software, there has been little research into how end users might interact with <b>machine</b> <b>learning</b> systems, beyond communicating simple &quot;right/wrong&quot; judgments. If the users themselves could somehow work hand-in-hand with <b>machine</b> <b>learning</b> systems, the accuracy of <b>learning</b> systems could be improved and the users&#39; understanding and trust of the system could improve as well. We conducted three experiments to begin to understand the ...", "dateLastCrawled": "2018-09-22T01:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) A <b>literature survey of active machine learning</b> in the context of ...", "url": "https://www.researchgate.net/publication/228682097_A_literature_survey_of_active_machine_learning_in_the_context_of_natural_language_processing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228682097_A_literature_survey_of_active...", "snippet": "Active <b>machine</b> <b>learning</b> is a supervised <b>learning</b> method in which the learner. is in control of the data from which it learns. That control is used by. the learner to ask an oracle, a teacher ...", "dateLastCrawled": "2022-02-01T01:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Deep <b>Learning</b> for Sensor-based Human Activity Recognition ...", "url": "https://www.researchgate.net/publication/338737352_Deep_Learning_for_Sensor-based_Human_Activity_Recognition_Overview_Challenges_and_Opportunities", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338737352_Deep_<b>Learning</b>_for_Sensor-based...", "snippet": "Many <b>machine</b> <b>learning</b> methods have been employed in human activity recognition. However ... The process of <b>co-training is like</b> the process of human <b>learning</b>. People can learn new knowledge. from ...", "dateLastCrawled": "2022-01-09T11:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep <b>Learning</b> for Sensor-based Human Activity Recognition: Overview ...", "url": "https://deepai.org/publication/deep-learning-for-sensor-based-human-activity-recognition-overview-challenges-and-opportunities", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/deep-<b>learning</b>-for-sensor-based-human-activity...", "snippet": "Transfer <b>learning</b> is a common <b>machine</b> <b>learning</b> technique that transfers the classification ability of the <b>learning</b> model from one predefined setting to a dynamic setting. Transfer <b>learning</b> is particularly effective in solving heterogeneity problems. It avoids the decline in the performance of <b>learning</b> models when the training data and the test data follow different distributions. In the activity recognition context, this problem appears when activity recognition models are deployed for ...", "dateLastCrawled": "2022-01-11T03:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Instance labeling in semi-supervised <b>learning</b> with meaning values of ...", "url": "https://www.sciencedirect.com/science/article/pii/S0952197617300672", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0952197617300672", "snippet": "In <b>machine</b> <b>learning</b> applications, especially in the field of text classification there are two conventional strategies; supervised <b>learning</b> and unsupervised <b>learning</b>. A sufficient amount of labeled data is required as training corpus to build the classifier in conventional supervised classification methods, which will be helpful to guess the class labels of the unlabeled instances. Conversely, unsupervised <b>learning</b>, only depends on unlabeled instances, and doesn\u2019t require class labels to ...", "dateLastCrawled": "2022-01-11T19:09:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(co-training)  is like +(two (or more) models together)", "+(co-training) is similar to +(two (or more) models together)", "+(co-training) can be thought of as +(two (or more) models together)", "+(co-training) can be compared to +(two (or more) models together)", "machine learning +(co-training AND analogy)", "machine learning +(\"co-training is like\")", "machine learning +(\"co-training is similar\")", "machine learning +(\"just as co-training\")", "machine learning +(\"co-training can be thought of as\")", "machine learning +(\"co-training can be compared to\")"]}