{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Tutorial on <b>Fairness</b> in Machine <b>Learning</b> | by ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/a-tutorial-on-<b>fairness</b>-in-machine-<b>learning</b>-3ff8ba1040cb", "snippet": "4.2 <b>Demographic</b> <b>Parity</b>. <b>Demographic</b> <b>Parity</b>, also called Independence, Statistical <b>Parity</b>, is one of the most well-known criteria for <b>fairness</b>. Formulation: C is independent of A: P\u2080 [C = c] = P\u2081 [C = c] \u2200 c \u2208 {0,1} In our example, this means the acceptance rates of the applicants from the two groups must be equal. In practice, there are ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Machine <b>Learning</b> for <b>Data</b> Science (CS4786) Lecture 26", "url": "https://www.cs.cornell.edu/courses/cs4786/2019sp/lectures/lec26.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs4786/2019sp/lectures/lec26.pdf", "snippet": "<b>Demographic</b> <b>Parity</b> \u2022 preprocess to remove information about T from input features X to create feature Z, use Z as new input \u2022 Example: \u2022 Find all directions in <b>data</b> matrix X that correlate with T \u2022 Remove these directions and let Z be the <b>data</b> matrix projected on remaining directions \u2022 If X is gaussian distributed this will make T and Z", "dateLastCrawled": "2021-09-04T16:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Measuring <b>Fairness</b> in Machine <b>Learning</b> Models", "url": "https://blog.dataiku.com/measuring-fairness-in-machine-learning-models", "isFamilyFriendly": true, "displayUrl": "https://blog.<b>data</b>iku.com/measuring-<b>fairness</b>-in-machine-<b>learning</b>-models", "snippet": "<b>Demographic</b> <b>parity</b> (with laziness) Limitations. On the other hand, <b>demographic</b> <b>parity</b> has various flaws. First, it can be used in an inappropriate context, meaning where disproportionality is truly present and independent from a protected attribute, or from a proxy for the protected attribute. In our example use case, enforcing <b>demographic</b> <b>parity</b> would result in discrimination against some qualified candidates, which could be seen as unfair. This first flaw underlines one of <b>demographic</b> ...", "dateLastCrawled": "2022-01-25T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AI Fairness | <b>Data</b> Science Portfolio", "url": "https://sourestdeeds.github.io/blog/ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://sourestdeeds.github.io/blog/ai-fairness", "snippet": "The next image shows what the confusion matrices could look <b>like</b>, if the model satisfies <b>demographic</b> <b>parity</b> fairness. 10 people from each group (50% from Group A, and 50% from Group B) were considered by the model. 14 people, also equally split across groups (50% from Group A, and 50% from Group B) were approved by the model.", "dateLastCrawled": "2022-02-02T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to define <b>fairness</b> to detect and prevent ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/how-to-define-<b>fairness</b>-to-detect-and-prevent...", "snippet": "<b>Demographic</b> <b>Parity</b> states that the proportion of each segment of a protected class (e.g. gender) should receive the positive outcome at equal rates. A positive outcome is the preferred decision, such as \u201cgetting to university\u201d, \u201cgetting a loan\u201d or \u201cbeing shown the ad\u201d. As mentioned earlier, the difference should be ideally zero, but this is usually not the case. Let\u2019s assume we\u2019re building a credit model based purely on \u201cincome\u201d (see Figure 2). We may decide to use ...", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Machine <b>Learning</b> Glossary: <b>Fairness</b> | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/fairness", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-<b>learning</b>/glossary/<b>fairness</b>", "snippet": "For example, if both Lilliputians and Brobdingnagians apply to Glubbdubdrib University, <b>demographic</b> <b>parity</b> is achieved if the percentage of Lilliputians admitted is the same as the percentage of Brobdingnagians admitted, irrespective of whether one group is on average more qualified than the other. Contrast with equalized odds and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain ...", "dateLastCrawled": "2022-02-02T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What <b>does \u201cfairness\u201d mean for machine learning systems</b>?", "url": "https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf", "isFamilyFriendly": true, "displayUrl": "https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf", "snippet": "<b>demographic</b> <b>parity</b> in particular may seem <b>like</b> a good solution but is a simplistic ap- ... There are several AI fairness tools meant to help engineers and <b>data</b> scientists examine, report, and mitigate discrimination and bias in ML models. For example: \u2022 IBM\u2019s AI Fairness 360 Toolkit: a Python toolkit focusing on technical solutions through fairness metrics and algorithms to help users examine, report, and mitigate discrimi-nation and bias in ML models. \u2022 Google\u2019s What-If Tool: a tool ...", "dateLastCrawled": "2022-01-28T05:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Fairness in <b>ML 2: Equal opportunity and odds</b>", "url": "https://www2.cs.duke.edu/courses/fall18/compsci590.1/lectures/FairML2.pdf", "isFamilyFriendly": true, "displayUrl": "https://www2.cs.duke.edu/courses/fall18/compsci590.1/lectures/FairML2.pdf", "snippet": "<b>Demographic</b> <b>parity</b> Issues \u2022 Does not seem \u201cfair\u201d to allow random performance on A = 0 \u2022 Perfect classification is impossible 6 A = 1 A = 0 . Perfect Classifier and Fairness \u2022 The perfect classifier may not ensure <b>demographic</b> <b>parity</b> \u2013 Y is correlated with A \u2022 What if we did not know how the classifier C was created? \u2013 No access to the classifier (to retrain) \u2013 No access to the training <b>data</b> (human created classifier) 7. True Positive <b>Parity</b> (TPP) (or equal opportunity ...", "dateLastCrawled": "2022-01-28T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Equality of Opportunity in Supervised <b>Learning</b>", "url": "https://home.ttic.edu/~nati/Publications/HardtPriceSrebro2016.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Publications/HardtPriceSrebro2016.pdf", "snippet": "Assuming <b>data</b> about the predictor, target, and membership in the protected group are avail- ... Unlike <b>demographic</b> <b>parity</b>, our notion always allows for the perfectly accurate solution of Yb= Y:More broadly, our criterion is easier to achieve the more accurate the predictor Yb is, aligning fairness with the central goal in supervised <b>learning</b> of building more accurate predictors. The notion we propose is \u201coblivious\u201d, in that it is based only on the joint distribution, or joint statistics ...", "dateLastCrawled": "2022-01-31T14:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Data</b> Balance Analysis on Spark | SynapseML", "url": "https://microsoft.github.io/SynapseML/docs/features/responsible_ai/Data%20Balance%20Analysis/", "isFamilyFriendly": true, "displayUrl": "https://microsoft.github.io/SynapseML/docs/features/responsible_ai/<b>Data</b> Balance Analysis", "snippet": "<b>Data</b> Balance Analysis is relevant for overall understanding of datasets, but it becomes essential when thinking about building Machine <b>Learning</b> services out of such datasets. Having a well balanced <b>data</b> representation is critical when developing models in a responsible way, specially in terms of fairness. It is unfortunately all too easy to build an ML model that produces biased results for subsets of an overall population, by training or testing the model on biased ground truth <b>data</b>. There ...", "dateLastCrawled": "2022-01-31T02:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Tutorial on <b>Fairness</b> in Machine <b>Learning</b> | by ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/a-tutorial-on-<b>fairness</b>-in-machine-<b>learning</b>-3ff8ba1040cb", "snippet": "4.2 <b>Demographic</b> <b>Parity</b>. <b>Demographic</b> <b>Parity</b>, also called Independence, Statistical <b>Parity</b>, is one of the most well-known criteria for <b>fairness</b>. Formulation: C is independent of A: P\u2080 [C = c] = P\u2081 [C = c] \u2200 c \u2208 {0,1} In our example, this means the acceptance rates of the applicants from the two groups must be equal. In practice, there are ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Measuring <b>Fairness</b> in Machine <b>Learning</b> Models", "url": "https://blog.dataiku.com/measuring-fairness-in-machine-learning-models", "isFamilyFriendly": true, "displayUrl": "https://blog.<b>data</b>iku.com/measuring-<b>fairness</b>-in-machine-<b>learning</b>-models", "snippet": "As <b>demographic</b> <b>parity</b>\u2019s main flaws are all linked to the inequality of treatment it introduces among subpopulations, two research groups came up with <b>similar</b> definitions of <b>fairness</b> which took into account how each group was treated: Equality of Odds and Disparate Mistreatment. We will use the Equality of Odds denomination in this article.", "dateLastCrawled": "2022-01-25T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "AI Fairness | <b>Data</b> Science Portfolio", "url": "https://sourestdeeds.github.io/blog/ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://sourestdeeds.github.io/blog/ai-fairness", "snippet": "1. <b>Demographic</b> <b>parity</b> / statistical <b>parity</b>. <b>Demographic</b> <b>parity</b> says the model is fair if the composition of people who are selected by the model matches the group membership percentages of the applicants. A nonprofit is organizing an international conference, and 20,000 people have signed up to attend. The organizers write a ML model to select ...", "dateLastCrawled": "2022-02-02T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machine <b>Learning</b> for <b>Data</b> Science (CS4786) Lecture 26", "url": "https://www.cs.cornell.edu/courses/cs4786/2019sp/lectures/lec26.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs4786/2019sp/lectures/lec26.pdf", "snippet": "<b>Demographic</b> <b>Parity</b> \u2022 preprocess to remove information about T from input features X to create feature Z, use Z as new input \u2022 Example: \u2022 Find all directions in <b>data</b> matrix X that correlate with T \u2022 Remove these directions and let Z be the <b>data</b> matrix projected on remaining directions \u2022 If X is gaussian distributed this will make T and Z", "dateLastCrawled": "2021-09-04T16:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/interventions", "snippet": "<b>Demographic</b> <b>parity</b>. The intervention reduced <b>demographic</b> <b>parity</b> difference only slightly, from 0.193 to 0.174. Since we exclude a priori available information from the training process it is reasonable to expect some reduction in accuracy. However, the influence on achieved accuracy is small, reducing it from 85.3% to 84.9%.", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Training a biased model</b> | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/baseline/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/baseline", "snippet": "<b>Demographic</b> <b>parity</b> requires that we treat all <b>demographic</b> groups equally. We start by investigating the disparity between the sexes. We do this with box plots of the model scores. A higher score means the model thinks the individual is more likely to be a high earner. It&#39;s clear that there is a major disparity between men and women, with men being awarded systematically higher scores. This model therefore does not achieve <b>demographic</b> <b>parity</b> by some margin.", "dateLastCrawled": "2022-01-24T18:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Explaining Measures of <b>Fairness</b>. Avoid the black ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/explaining-measures-of-fairness-f0e419d4e0d7", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/explaining-measures-of-<b>fairness</b>-f0e419d4e0d7", "snippet": "While this investigation is challenging given just a single <b>demographic</b> <b>parity</b> difference value, it is much easier given the per-feature <b>demographic</b> <b>parity</b> decomposition based on SHAP. Using SHAP we can see there is a significant bias coming from the reported income feature that is increasing the risk of women disproportionately to men. This allows us to quickly identify which feature has the reporting bias that is causing our model to violate <b>demographic</b> <b>parity</b>:", "dateLastCrawled": "2022-02-01T08:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification - Fairness and machine <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "The tools to solve classification and regression problems in practice are very <b>similar</b>. In both cases, ... referred to as <b>demographic</b> <b>parity</b>, statistical <b>parity</b>, group fairness, disparate impact and others. In the case of binary classification, independence simplifies to the condition \\mathbb{P}\\{R=1\\mid A=a\\}=\\mathbb{P}\\{R=1\\mid A=b\\}\\,, for all groups a, b. Thinking of the event R=1 as \u201cacceptance,\u201d the condition requires the acceptance rate to be the same in all groups. A relaxation ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fairness in <b>ML 2: Equal opportunity and odds</b>", "url": "https://www2.cs.duke.edu/courses/fall18/compsci590.1/lectures/FairML2.pdf", "isFamilyFriendly": true, "displayUrl": "https://www2.cs.duke.edu/courses/fall18/compsci590.1/lectures/FairML2.pdf", "snippet": "\u2022 While <b>demographic</b> <b>parity</b> seems like a good fairness goal for the society, \u2026 Equal odds/opportunity seems to be measuring whether an algorithm is fair (independent of other factors like input <b>data</b>). 22", "dateLastCrawled": "2022-01-28T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Heidari et al. have written a paper comparing the three criteria \u2013 <b>demographic</b> <b>parity</b>, equality of opportunity, and predictive <b>parity</b> \u2013 to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the analogy is fascinating, it too assumes that we may take what is in the <b>data</b> at face value. In their likening predictive <b>parity</b> to luck egalitarianism, they have to go to especially great lengths, in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Equality of opportunity in supervised learning</b> | the morning paper", "url": "https://blog.acolyer.org/2018/05/07/equality-of-opportunity-in-supervised-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2018/05/07/<b>equality-of-opportunity-in-supervised-learning</b>", "snippet": "Under <b>demographic</b> <b>parity</b> we go back to choosing a different threshold for each group, ... One nice characteristic of these models is that you <b>can</b> start by <b>learning</b> a possibly discriminator learned binary predictor (or score R), and then derive an equalized odds or equal opportunity predictor from it. So we <b>can</b> keep an existing training pipeline untouched, and add an anti-discriminatory step on the back-end of it. Section 4 in the paper shows how to do this for a binary predictor, and the ...", "dateLastCrawled": "2022-02-03T12:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Impossibility Theorem</b> of Machine Fairness \u2013 A Causal ... - DeepAI", "url": "https://deepai.org/publication/the-impossibility-theorem-of-machine-fairness-a-causal-perspective", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>impossibility-theorem</b>-of-machine-fairness-a-causal...", "snippet": "This entails that <b>demographic</b> <b>parity</b> <b>can</b> not hold in such a model. ... is a lack of fairness in many <b>data</b>-driven classifiers is due to the discrepancy between what is deemed correct by the <b>data</b> and what is <b>thought</b> of as &quot;correct&quot; by the society. Many papers in the literature have mentioned that fairness in machine <b>learning</b> is not a statistical but rather a sociological issue in using machine learnt classifiers in practical settings. The exposure bias of machine learners is at odds with what ...", "dateLastCrawled": "2022-01-27T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>A Reductions Approach to Fair</b> Classification", "url": "http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf", "snippet": "The \ufb01rst de\ufb01nition\u2014<b>demographic</b> (or statistical) <b>parity</b>\u2014 <b>can</b> <b>be thought</b> of as a stronger version of the US Equal Employment Opportunity Commission\u2019s \u201cfour-\ufb01fths rule,\u201d which requires that the \u201cselection rate for any race, sex, or ethnic group [must be at least] four-\ufb01fths (4/5) (or eighty", "dateLastCrawled": "2022-02-02T14:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Handling Discriminatory Biases in <b>Data</b> for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Demographic</b> <b>parity</b> proposes that the decision (the target variable) should be independent of protected attributes \u2014 race, ... Food for <b>Thought</b>. To end such a long and serious article, I leave you with a quote from Google about <b>discrimination</b> in <b>machine learning</b> to mull over. \u201cOptimizing for equal opportunity is just one of many tools that <b>can</b> be used to improve <b>machine learning</b> systems \u2014 and mathematics alone is unlikely to lead to the best solutions. Attacking <b>discrimination</b> in ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Adversarial Removal of Demographic Attributes</b> from Text <b>Data</b> | DeepAI", "url": "https://deepai.org/publication/adversarial-removal-of-demographic-attributes-from-text-data", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../<b>adversarial-removal-of-demographic-attributes</b>-from-text-<b>data</b>", "snippet": "We show that <b>demographic</b> information of authors is encoded in -- and <b>can</b> be recovered from -- the intermediate representations learned by text-based neural classifiers. The implication is that decisions of classifiers trained on textual <b>data</b> are not agnostic to -- and likely condition on -- <b>demographic</b> attributes. When attempting to remove such <b>demographic</b> information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy ...", "dateLastCrawled": "2021-12-26T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Best Practices for <b>Data</b> Science Projects CMSC320: Introduction to <b>Data</b> ...", "url": "http://www.hcbravo.org/IntroDataSci/slides/cmsc320_best-practices.pdf", "isFamilyFriendly": true, "displayUrl": "www.hcbravo.org/Intro<b>Data</b>Sci/slides/cmsc320_best-practices.pdf", "snippet": "<b>Demographic</b> <b>parity</b>: \u2022 A decision must be independent of the protected attribute \u2022 E.g., a loan application\u2019s acceptance rate is independent of an applicant\u2019s race (but <b>can</b> be dependent on non-protected features like salary) Formally: binary decision variable C, protected attribute A \u2022 P{ C = 1 | A = 0 } = P{ C = 1 | A = 1 }", "dateLastCrawled": "2021-08-29T20:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Eight truths about <b>diversity</b> and inclusion at work | Deloitte Insights", "url": "https://www2.deloitte.com/us/en/insights/deloitte-review/issue-22/diversity-and-inclusion-at-work-eight-powerful-truths.html", "isFamilyFriendly": true, "displayUrl": "https://www2.deloitte.com/us/en/insights/deloitte-review/issue-22/<b>diversity</b>-and...", "snippet": "It\u2019s about looking beyond <b>demographic</b> <b>parity</b> to the ultimate outcome\u2014<b>diversity</b> of thinking. ... Using <b>data</b> to pinpoint leaks in the talent lifecycle. To do this, organizations <b>can</b> look at the profile of their employees from recruitment to retirement, coupled with <b>data</b> on inclusion experiences. Identifying and remodeling vulnerable moments along the talent lifecycle. These are points within specific talent processes where decision-makers are more susceptible to bias: for example, when ...", "dateLastCrawled": "2022-02-02T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Examples from UKRI\u2019s AI investment portfolio \u2013 UKRI", "url": "https://www.ukri.org/our-work/how-we-work-in-ai/examples-from-ukris-ai-investment-portfolio/", "isFamilyFriendly": true, "displayUrl": "https://www.ukri.org/our-work/how-we-work-in-ai/examples-from-ukris-ai-investment...", "snippet": "The team proposed a statistical test in their paper \u2018why fairness cannot be automated\u2019 (conditional <b>demographic</b> <b>parity</b>) ... With these powerful <b>data</b> mining techniques, researchers <b>can</b> search through medical records quickly and efficiently, helping to understand diseases and develop more effective treatments. The team analysed medical notes of COVID-19 patients. Their preliminary <b>data</b> suggests that patients taking medicines known as ACE-inhibitors to manage high blood pressure and ...", "dateLastCrawled": "2022-02-01T08:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "15-388/688 -Practical <b>Data</b> Science: The future of <b>data</b> science", "url": "http://www.datasciencecourse.org/slides/15388_S21_Lecture_27_future.pdf", "isFamilyFriendly": true, "displayUrl": "www.<b>data</b>sciencecourse.org/slides/15388_S21_Lecture_27_future.pdf", "snippet": "My <b>thought</b>: if you <b>can</b> achieve mastery of the ideas in this course, you will be well-suited for many of these positions, but you\u2019ll often need to make initial contact to convey this 8. Class survey For those who have interviewed for a <b>data</b> science position, what questions were you asked in your interview? 9. The <b>data</b> science interview There is no \u201cstandard\u201d yet for the types of questions you\u2019ll be asked (just as there is no standard as to what a <b>data</b> science position means) The ...", "dateLastCrawled": "2022-01-25T21:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Feature Choice and Fairness: Less May</b> be More - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/feature-choice-and-fairness-less-may-be-more-7809ec11772e", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>feature-choice-and-fairness-less-may</b>-be-more-7809ec11772e", "snippet": "Machine <b>learning</b> models are based on correlation, and any feature associated with an outcome <b>can</b> be used as a decision basis; there is reason for concern. However, the risks of such a scenario occurring depend on the information available to the model and on the specific algorithm used. Here, I will use sample <b>data</b> to illustrate differences in incorporation of incidental information in random forest vs. XGBoost models, and discuss the importance of considering missing information ...", "dateLastCrawled": "2022-01-25T06:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Identifying and Correcting Label <b>Bias</b> in Machine <b>Learning</b> | by Rani ...", "url": "https://towardsdatascience.com/identifying-and-correcting-label-bias-in-machine-learning-ed177d30349e", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/identifying-and-correcting-label-<b>bias</b>-in-machine...", "snippet": "<b>Demographic</b> <b>parity</b> \u2014 Classifier should make positive predictions on a protected population group at the same rate as the entire population. Disparate impact \u2014 Similar to <b>demographic</b> <b>parity</b> but without the classifier knowing which protected population groups exist and which <b>data</b> points relate to such protected groups.", "dateLastCrawled": "2022-01-30T18:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to define <b>fairness</b> to detect and prevent ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/how-to-define-<b>fairness</b>-to-detect-and-prevent...", "snippet": "<b>Demographic</b> <b>Parity</b> states that the proportion of each segment of a protected class (e.g. gender) should receive the positive outcome at equal rates. A positive outcome is the preferred decision, such as \u201cgetting to university\u201d, \u201cgetting a loan\u201d or \u201cbeing shown the ad\u201d. As mentioned earlier, the difference should be ideally zero, but this is usually not the case.", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Development of Machine <b>Learning</b> Models for Prediction of Osteoporosis ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8305021/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8305021", "snippet": "For prediction of osteoporosis with more easily available <b>data</b>, some machine <b>learning</b> models were constructed, focusing mainly on ... <b>Demographic</b> Information of the Study Population. Of the 5982 participants enrolled in the study, 3053 (51.0%) were men and 2929 (49.0%) were women. The average age was 59.3 \u00b1 7.0 years old for both gender groups. The results of DXA showed that 117 men (3.8% of men) and 304 women (10.4% of women) had osteoporosis, and 1134 men (37.1% of men) and 1369 women (46 ...", "dateLastCrawled": "2021-08-23T09:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Model understanding with Azure Machine Learning</b>", "url": "https://techcommunity.microsoft.com/t5/azure-ai-blog/model-understanding-with-azure-machine-learning/ba-p/2201141", "isFamilyFriendly": true, "displayUrl": "https://techcommunity.microsoft.com/t5/azure-ai-blog/model-understanding-with-azure...", "snippet": "fairness metric (e.g., <b>demographic</b> <b>parity</b> difference). Model assessment view . After setting the configurations, you will land on a model assessment view where you <b>can</b> see how the model is treating different <b>demographic</b> groups. Our fairness assessment shows an 18.3% disparity in the selection rate (or <b>demographic</b> group difference). According to that insight, 18.3% more males are receiving qualifications for loan acceptance <b>compared</b> to females. Now that you\u2019ve seen some unfairness ...", "dateLastCrawled": "2022-01-17T06:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Algorithmic Fairness in R | Nikita Kozodoi", "url": "https://kozodoi.me/r/fairness/packages/2020/05/01/fairness-tutorial.html", "isFamilyFriendly": true, "displayUrl": "https://kozodoi.me/r/fairness/packages/2020/05/01/fairness-tutorial.html", "snippet": "How to measure fairness of a machine <b>learning</b> model? To date, a number of algorithmic fairness metrics have been proposed. <b>Demographic</b> <b>parity</b>, proportional <b>parity</b> and equalized odds are among the most commonly used metrics to evaluate fairness across sensitive groups in binary classification problems. Multiple other metrics have been proposed based on performance measures extracted from the confusion matrix (e.g., false positive rate <b>parity</b>, false negative rate <b>parity</b>). Together with Tibor V ...", "dateLastCrawled": "2022-01-30T20:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Gender and education - UNICEF <b>DATA</b>", "url": "https://data.unicef.org/topic/gender/gender-disparities-in-education/", "isFamilyFriendly": true, "displayUrl": "https://<b>data</b>.unicef.org/topic/gender/gender-disparities-in-education", "snippet": "Primary education provides the foundation for a lifetime of <b>learning</b>. Providing universal access to, and ensuring the completion of, primary education for all girls and boys is one of the key areas of concern identified in the Beijing Platform for Action adopted in 1995. Since then, considerable progress has been made in achieving universal primary education and closing the gender gap in enrollment. More than two-thirds of countries have reached gender <b>parity</b> (defined as having a gender ...", "dateLastCrawled": "2022-02-03T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Adversarial Removal of Demographic Attributes</b> from Text <b>Data</b> | DeepAI", "url": "https://deepai.org/publication/adversarial-removal-of-demographic-attributes-from-text-data", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../<b>adversarial-removal-of-demographic-attributes</b>-from-text-<b>data</b>", "snippet": "We show that <b>demographic</b> information of authors is encoded in -- and <b>can</b> be recovered from -- the intermediate representations learned by text-based neural classifiers. The implication is that decisions of classifiers trained on textual <b>data</b> are not agnostic to -- and likely condition on -- <b>demographic</b> attributes. When attempting to remove such <b>demographic</b> information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy ...", "dateLastCrawled": "2021-12-26T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Controllable Fairness in Machine <b>Learning</b> | SAIL Blog", "url": "http://ai.stanford.edu/blog/controllable-fairness/", "isFamilyFriendly": true, "displayUrl": "ai.stanford.edu/blog/controllable-fairness", "snippet": "To complement fair machine <b>learning</b> models that corporations and governments <b>can</b> choose to use, this work takes a step towards putting control of fair machine <b>learning</b> in the hands of a party concerned with fairness, such as a <b>data</b> collector, community organizer, or regulatory body. We contribute a theoretical approach <b>to learning</b> fair representations that make it much more difficult for downstream machine <b>learning</b> models to discriminate, and we contribute a new method that allows the ...", "dateLastCrawled": "2022-01-30T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Classification - Fairness and machine <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Supervised <b>learning</b>. A classifier is a mapping from the space of possible values for X to the space of values that the target variable Y <b>can</b> assume. Supervised <b>learning</b> is the prevalent method for constructing classifiers from observed <b>data</b>. The essential idea is very simple.", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning Removes Bias from Algorithms</b> and the Hiring ... - Arena", "url": "https://arena.io/machine-learning-removes-bias-from-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://arena.io/<b>machine-learning-removes-bias-from-algorithms</b>", "snippet": "At the premiere Machine <b>Learning</b> Conference , November 6, 2020, Arena ... but that same <b>data</b> point may leak <b>demographic</b> <b>data</b>. Adjusting models after-the-fact <b>can</b> ensure the protected class performs similarly to other classes. This is how LinkedIN is correcting its \u2018predictions.\u2019 This adjusts recommendation rates so that, for example, women are recommended at the same rate as men. Success is measured by statistical significance and a 4/5th rule, as described by the Equal Employment ...", "dateLastCrawled": "2022-01-26T07:50:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An example of prediction which complies with <b>Demographic</b> <b>Parity</b> and ...", "url": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-demographic-parity-and-equalizes-group-wise-risks-in-the-context", "isFamilyFriendly": true, "displayUrl": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-<b>demographic</b>...", "snippet": "However, <b>Demographic</b> <b>Parity</b> and EGWR only define fairness on the group level and inspecting the individual level reveals a critical flow of this prediction rule. We have constrained our predictors to those that do not produce Disparate Treatment by prohibiting them from having the sensitive variable as direct input. Nevertheless, enforcing group level fairness constraints (such as DP and EGWR) forces the prediction rule to guess the sensitive attribute corresponding to a given feature vector", "dateLastCrawled": "2022-02-05T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Pandas <b>Machine</b> <b>Learning</b> Example", "url": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "snippet": "Regardless of your dataset, <b>demographic</b> <b>parity</b> is a <b>machine</b> <b>learning</b> algorithms. Data Munging It helps us to missing data of wedge form with another. Python with datetime module, i should equal to bring new example <b>machine</b>. Quite possibly the state important part clean the <b>machine</b> <b>learning</b> process is understanding the data you are working with and advantage it relates to reflect task you front to solve. Viewing the corresponding number of dropping down arrow illustrates that are not only all ...", "dateLastCrawled": "2022-01-24T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Heidari et al. have written a paper comparing the three criteria \u2013 <b>demographic</b> <b>parity</b>, equality of opportunity, and predictive <b>parity</b> \u2013 to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the <b>analogy</b> is fascinating, it too assumes that we may take what is in the data at face value. In their likening predictive <b>parity</b> to luck egalitarianism, they have to go to especially great lengths, in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Mitigating Unwanted Biases with Adversarial Learning</b>", "url": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concern- ing <b>demographic</b> groups are in the training data, well-trained models will re\ufb02ect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an ad-versary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or in ...", "dateLastCrawled": "2021-12-17T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Survey on Bias and <b>Fairness in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/a-survey-on-bias-and-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-on-bias-and-<b>fairness-in-machine-learning</b>", "snippet": "With the popularity of AI and <b>machine</b> <b>learning</b> over the past decades, and their epidemic spread in different applications, safety and fairness constraints have become a huge issue for researchers and engineers. <b>Machine</b> <b>learning</b> is used in courts to assess the likelihood of a defendant becoming a recidivist. It is used in different medical fields, in childhood welfare systems (pmlr-v81-chouldechova18a), and autonomous vehicles. All of these applications have a direct effect in our lives and ...", "dateLastCrawled": "2022-01-22T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "This includes measures such as <b>Demographic</b> <b>Parity</b> / Statistical <b>Parity</b> (Dwork et al., 2012), Equalized Odds Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are trying to test for equivalence. In another survey of fairness definitions, Verma &amp; Rubin (2018) listed 20 definitions of fairness, 13 belonging to ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "1. Introduction \u2014 <b>Dive into Deep Learning</b> 0.17.2 documentation", "url": "https://www.d2l.ai/chapter_introduction/index.html", "isFamilyFriendly": true, "displayUrl": "https://www.d2l.ai/chapter_introduction/index.html", "snippet": "<b>Machine</b> <b>learning</b> is the study of powerful techniques that can learn from experience. As a <b>machine</b> <b>learning</b> algorithm accumulates more experience, typically in the form of observational data or interactions with an environment, its performance improves. Contrast this with our deterministic e-commerce platform, which performs according to the same business logic, no matter how much experience accrues, until the developers themselves learn and decide that it is time to update the software. In ...", "dateLastCrawled": "2022-02-01T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The <b>measure and mismeasure of fairness: a critical review</b> of fair ...", "url": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness", "snippet": "In case you\u2019re wondering where on earth I\u2019m going with this\u2026 it\u2019s a very stretched <b>analogy</b> I\u2019ve been playing with in my mind. One premise of many models of fairness in <b>machine</b> <b>learning</b> is that you can measure (\u2018prove\u2019) fairness of a <b>machine</b> <b>learning</b> model from within the system \u2013 i.e. from properties of the model itself and perhaps the data it is trained on. Beyond the questions of whether any one model of fairness is better or worse than another, I\u2019m coming to the ...", "dateLastCrawled": "2022-01-30T11:52:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(demographic parity)  is like +(learning from data)", "+(demographic parity) is similar to +(learning from data)", "+(demographic parity) can be thought of as +(learning from data)", "+(demographic parity) can be compared to +(learning from data)", "machine learning +(demographic parity AND analogy)", "machine learning +(\"demographic parity is like\")", "machine learning +(\"demographic parity is similar\")", "machine learning +(\"just as demographic parity\")", "machine learning +(\"demographic parity can be thought of as\")", "machine learning +(\"demographic parity can be compared to\")"]}