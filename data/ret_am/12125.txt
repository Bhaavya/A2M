{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Gaussian mixture models</b> - Matthew N. Bernstein", "url": "https://mbernste.github.io/posts/gmm_em/", "isFamilyFriendly": true, "displayUrl": "https://mbernste.github.io/posts/gmm_em", "snippet": "Recall, the <b>Q-function</b> is defined as \\[Q_t(\\Theta) := E_{Z \\mid X; \\Theta_t}\\left[ \\log p(X, Z; \\Theta) \\right]\\] Deriving the <b>Q-function</b> entails calculating an analytical form of this expectation so that we can implement it in <b>a computer</b> <b>program</b>. For GMM\u2019s that derivation is:", "dateLastCrawled": "2022-02-01T02:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What are <b>the Q# programming language</b> &amp; QDK? - Azure Quantum | Microsoft ...", "url": "https://docs.microsoft.com/en-us/azure/quantum/overview-what-is-qsharp-and-qdk", "isFamilyFriendly": true, "displayUrl": "https://docs.microsoft.com/en-us/azure/quantum/overview-what-is-qsharp-and-qdk", "snippet": "A Q# <b>program</b> can compile into a standalone application, through Jupyter Notebooks, or be called by a host <b>program</b> that is written either in Python or a .NET language. When you compile and run the <b>program</b>, it creates an instance of the quantum simulator and passes the Q# code to it. The simulator uses the Q# code to create qubits (simulations of ...", "dateLastCrawled": "2022-02-03T03:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>C++ program to implement queue using arrays</b>", "url": "https://notesformsc.org/c-implement-queue-arrays/", "isFamilyFriendly": true, "displayUrl": "https://notesformsc.org/c-implement-queue-arrays", "snippet": "From the following flowchart, it is clear that when user input their choice, the Queue object calls specific functions <b>like</b> Insert_Q() or Delete_Q() and display results immediately using Display_<b>Q() function</b>. The function definition is given in the <b>program</b> code section. The queue operations continue until user decides to quit with choice \u2018q\u2019.", "dateLastCrawled": "2022-01-29T00:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Functions of Operating System - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/functions-of-operating-system/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/functions-of-operating-system", "snippet": "An operating system is a <b>program</b> on which application programs are executed and acts as a communication bridge (interface) between the user and the <b>computer</b> hardware. The main task an operating system carries out is the allocation of resources and services, such as the allocation of memory, devices, processors, and information.", "dateLastCrawled": "2022-02-02T22:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Operating Systems (10) | Other Quiz - Quizizz", "url": "https://quizizz.com/admin/quiz/5ca64c191f8503001e1be8b2/operating-systems-10", "isFamilyFriendly": true, "displayUrl": "https://quizizz.com/admin/quiz/5ca64c191f8503001e1be8b2/operating-systems-10", "snippet": "Q. ____________a set of programs containing instructions that work together to coordinate all the activities among <b>computer</b> hardware resources. answer choices. <b>Operating system</b>. System Software. Application software. Utility <b>program</b>. Tags: Question 6. SURVEY.", "dateLastCrawled": "2022-02-02T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Modular Approach in Programming - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/modular-approach-in-programming/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/modular-approach-in-<b>program</b>ming", "snippet": "<b>Like</b> Article. Modular Approach in Programming. Difficulty Level : Basic; Last Updated : 07 Sep, 2018. Modular programming is the process of subdividing <b>a computer</b> <b>program</b> into separate sub-programs. A module is a separate software component. It can often be used in a variety of applications and functions with other components of the system. Some programs might have thousands or millions of lines and to manage such programs it becomes quite difficult as there might be too many of syntax ...", "dateLastCrawled": "2022-01-29T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Computer</b> Science Q&amp;A: Archive 412 | bartleby", "url": "https://www.bartleby.com/subject/engineering/computer-science/questions-and-answers/archive-412", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bartleby.com</b>/subject/engineering/<b>computer</b>-science/questions-and-answers/...", "snippet": "Find expert answers to <b>Computer</b> Science questions asked by students <b>like</b> you. question_answer. Q: A pedometer trea\u2026. question_answer. Q: Complete the if-\u2026. question_answer. Q: TRUE or FALSE b.\u2026. question_answer. Q: Assign decoded_t\u2026.", "dateLastCrawled": "2022-01-17T02:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Answered: 3. Write a <b>program</b> that will accept and\u2026 | bartleby", "url": "https://www.bartleby.com/questions-and-answers/3.-write-a-program-that-will-accept-and-store-5-grades-in-an-array.-the-program-will-then-determine-/f104773a-3fad-42d2-8d56-bb991c6c71c1", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bartleby.com</b>/questions-and-answers/3.-write-a-<b>program</b>-that-will-accept-and...", "snippet": "Engineering <b>Computer</b> Engineering Q&amp;A Library 3. Write a <b>program</b> that will accept and store 5 grades in an array. The <b>program</b> will then determine highest and lowest grades. The will also display all the passing grades and failing grades from the Passing grade starts to 75 and above. Use LabOneDimArray3 as classname and filename. <b>program</b> array.", "dateLastCrawled": "2022-01-30T19:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Function of a compiler? - Answers</b>", "url": "https://www.answers.com/Q/Function_of_a_compiler", "isFamilyFriendly": true, "displayUrl": "https://www.answers.com/<b>Q/Function</b>_of_a_compiler", "snippet": "The c <b>program</b> starts with &#39;main&#39; function because it&#39;s compiler is designed that way. so during the compile time, the compiler looks for main function and through that it binds all the other user ...", "dateLastCrawled": "2022-01-27T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Function of a <b>program</b> counter? - Answers", "url": "https://www.answers.com/Q/Function_of_a_program_counter", "isFamilyFriendly": true, "displayUrl": "https://www.answers.com/<b>Q/Function</b>_of_a_<b>program</b>_counter", "snippet": "<b>program</b> counter is a register that has the address of next instruction that has to be executed after currently executing instruction. it is used for proper execution of functions of <b>computer</b> by ...", "dateLastCrawled": "2022-01-26T11:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Approximate Dynamic Programming: a $\\mathcal{<b>Q}$-Function</b> Approach", "url": "https://www.researchgate.net/publication/301844866_Approximate_Dynamic_Programming_a_mathcalQ-Function_Approach", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/301844866_Approximate_Dynamic_<b>Program</b>ming_a...", "snippet": "The <b>Q-function</b>, \ufb01rst introduced in [25], <b>is similar</b> to the cost- to-go function, but it has the property that the optimal control policy can be written in terms of the optimal <b>Q -function</b>", "dateLastCrawled": "2021-12-11T13:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Model-free LQR design by <b>Q-function</b> learning - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0005109821005884", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0005109821005884", "snippet": "Problem 4 Model-free SDP to Derive the <b>Q-function</b>. A convex <b>program</b> with variables H \u2208 S n + m and W \u2208 S n. Maximize H, W t r a c e (W) subject to H 11 \u2212 W H 12 H 12 T H 22 \u2ab0 0, X D T H 11 X D \u2212 D T (H \u2212 \u039b) D X D T H 12 H 12 T X D H 22 \u2ab0 0. Proof. It is already shown that according to the properties of matrix congruence, the constraint is equivalent to the second constraint in Problem 4. Based on Theorem 2, the exact <b>Q-function</b> is retrieved by solving Problem 2 which is ...", "dateLastCrawled": "2022-01-22T02:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Functions of Operating System - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/functions-of-operating-system/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/functions-of-operating-system", "snippet": "An operating system is a <b>program</b> on which application programs are executed and acts as a communication bridge (interface) between the user and the <b>computer</b> hardware. The main task an operating system carries out is the allocation of resources and services, such as the allocation of memory, devices, processors, and information.", "dateLastCrawled": "2022-02-02T22:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Solved Let q be the function defined by q(x, y) = | Chegg.com", "url": "https://www.chegg.com/homework-help/questions-and-answers/let-q-function-defined-q-x-y-r-ytan-xy--use-taylor-series-one-variable-tangent-function-ta-q85997930", "isFamilyFriendly": true, "displayUrl": "https://www.chegg.com/homework-help/questions-and-answers/let-<b>q-function</b>-defined-q-x-y...", "snippet": "So instead you should do this calculation using a <b>computer</b> <b>program</b>. Use the diff! command from the Matlab symbolic toolbor to write a <b>program</b> to carry out the alculations and evaluations of D&quot; q needed for the Taylor polynomial approximation. Include a printout of the Matlab commands and relevant results as part of your submissionand comment on the output. Alternatively you may use Sage Math, Python or other <b>similar</b> software for these computations. [30] Previous question Next question. Get ...", "dateLastCrawled": "2022-01-13T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Deep Q Network vs Policy Gradients - An Experiment on VizDoom with ...", "url": "https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html", "isFamilyFriendly": true, "displayUrl": "https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html", "snippet": "Deep Q Network vs Policy Gradients - An Experiment on VizDoom with Keras. October 12, 2017 After a brief stint with several interesting <b>computer</b> vision projects, include this and this, I\u2019ve recently decided to take a break from <b>computer</b> vision and explore reinforcement learning, another exciting field.<b>Similar</b> to <b>computer</b> vision, the field of reinforcement learning has experienced several important breakthroughs made possible by the deep learning revolution.", "dateLastCrawled": "2022-01-31T19:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Reinforcement Learning Tutorial</b> - Javatpoint", "url": "https://www.javatpoint.com/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/reinforcement-learning", "snippet": "Hence, we can say that &quot;Reinforcement learning is a type of machine learning method where an intelligent agent (<b>computer</b> <b>program</b>) interacts with the environment and learns to act within that.&quot; How a Robotic dog learns the movement of his arms is an example of Reinforcement learning. It is a core part of Artificial intelligence, and all AI agent works on the concept of reinforcement learning. Here we do not need to pre-<b>program</b> the agent, as it learns from its own experience without any human ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Some Integrals Involving The <b>Q Function</b> Dtic", "url": "https://www.fortifyprogram.org/some%20integrals%20involving%20the%20q%20function%20dtic%20pdf", "isFamilyFriendly": true, "displayUrl": "https://www.fortify<b>program</b>.org/some integrals involving the q function dtic pdf", "snippet": "exhibit <b>similar</b> tendencies.The problems faced by many entrepreneurial managers managing start-up or even existing enterprises are complex and require an in-depth understanding not only of the problems themselves, but also of the contextual framework in which these problems need to be solved. This book contains original cases that cover issues like cluster formation, information gathering, marketing strategies and operations, and information-technology. Examples come from industries like ...", "dateLastCrawled": "2022-01-19T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Using Keras and Deep Q-Network to Play FlappyBird | Ben Lau", "url": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html?source=post_page---------------------------", "isFamilyFriendly": true, "displayUrl": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html?source=post_page...", "snippet": "lib.cnmem=0.2 means you assign only 20% of the GPU\u2019s memory to the <b>program</b>. If you want to train the network from beginning, delete \u201cmodel.h5\u201d and run qlearn.py -m \u201cTrain\u201d What is Deep Q-Network? Deep Q-Network is a learning algorithm developed by Google DeepMind to play Atari games. They demonstrated how a <b>computer</b> learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable because it ...", "dateLastCrawled": "2022-01-10T19:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Model-free reinforcement learning \u2014 Introduction to Reinforcement Learning", "url": "https://gibberblot.github.io/rl-notes/single-agent/model-free.html", "isFamilyFriendly": true, "displayUrl": "https://gibberblot.github.io/rl-notes/single-agent/model-free.html", "snippet": "Imagine how hard it is for a <b>computer</b> that doesn\u2019t have any assumptions or intuition for this game though! It will not match the colours, nor will it really have any prior knowledge about <b>similar</b> games, unless it is explicitly told about it. Model-free reinforcement learning techniques start with no or minimal initial knowledge, and will learn a policy (e.g. a value function, a <b>Q-function</b>, or a policy directly) just by trying behaviours and seeing what happens. Most techniques (at least ...", "dateLastCrawled": "2022-01-30T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Answered: Write a <b>program</b> that asks for the\u2026 | bartleby", "url": "https://www.bartleby.com/questions-and-answers/write-a-program-that-asks-for-the-principal-the-annual-interest-rate-and-the-number-of-times-the-int/452dc789-4028-4d74-8fb1-13ff224d825a", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bartleby.com</b>/questions-and-answers/write-a-<b>program</b>-that-asks-for-the...", "snippet": "Engineering <b>Computer</b> Engineering Q&amp;A Library Write a <b>program</b> that asks for the principal, the annual interest rate, and the number of times the interest is compounded. It should display a report <b>similar</b> to the following: Interest Rate: 4.25% Times Compounded: 12 Principal: $ 1000.00 Interest: 43.33 Final balance: $ 1043.33.", "dateLastCrawled": "2021-12-27T12:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Some Integrals Involving The <b>Q Function</b> Dtic", "url": "https://www.fortifyprogram.org/some-integrals-involving-the-q-function-dtic-pdf", "isFamilyFriendly": true, "displayUrl": "https://www.fortify<b>program</b>.org/some-integrals-involving-the-<b>q-function</b>-dtic-pdf", "snippet": "Read PDF Some Integrals Involving The <b>Q Function</b> Dtic A massive compendium of useful information, this volume represents a valuable tool for applied mathematicians in many areas of academia and industry. A dozen useful tables supplement the text. 1962 edition. Integrals of Bessel Functions The Probability Integrals of the Range and of the Studentized Range The goal of this book is to describe the most powerful methods for evaluating multiloop Feynman integrals that are currently used in ...", "dateLastCrawled": "2022-02-01T12:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Some Integrals Involving The <b>Q Function</b> Dtic", "url": "https://www.fortifyprogram.org/some%20integrals%20involving%20the%20q%20function%20dtic%20pdf", "isFamilyFriendly": true, "displayUrl": "https://www.fortify<b>program</b>.org/some integrals involving the q function dtic pdf", "snippet": "Many of the integrals presented here cannot be obtained using a <b>computer</b> (except via an approximate numerical integration). Additionally, for improper integrals, this book emphasizes the necessary and sufficient conditions for the validity of the presented formulas, including trajectory for going to infinity on the complex plane; such conditions are usually not given in <b>computer</b>-assisted analytical integration and often not presented in the previously published tables of integrals. Features ...", "dateLastCrawled": "2022-01-19T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reinforcement Learning with Q tables | by Mohit Mayank | ITNEXT", "url": "https://itnext.io/reinforcement-learning-with-q-tables-5f11168862c8", "isFamilyFriendly": true, "displayUrl": "https://itnext.io/reinforcement-learning-with-q-tables-5f11168862c8", "snippet": "Now its even difficult for us to grasp the sense of right actions, what if we want the <b>computer</b> to learn this? Reinforcement learning to the rescue. Markov Decision Process . Now what is this Markov process and why do we need to learn it? Well I <b>thought</b> the same and to be clear, we don\u2019t need to deep dive into it, just a basic intuition would do. So, markov decision process is used for modeling decision making in situations where the outcomes are partly random and partly under the control ...", "dateLastCrawled": "2022-01-29T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "BUF lab: <b>Experiment 3 of deep understanding computer system</b> | Develop Paper", "url": "https://developpaper.com/buf-lab-experiment-3-of-deep-understanding-computer-system/", "isFamilyFriendly": true, "displayUrl": "https://developpaper.com/buf-lab-<b>experiment-3-of-deep-understanding-computer-system</b>", "snippet": "When the <b>Q function</b> is executed, it <b>can</b> save the value of the register and allocate space for local variables. When the call ends, all the space requested by Q will be released. When p passes less than 6 parameters to Q, it is enough to use register to save. When more than 6 parameters are passed, the extra part will be passed by stack. Refer to this article for a more detailed knowledge of stacksInterviewers don\u2019t talk about martial arts, actually let me talk about worms and canaries ...", "dateLastCrawled": "2022-02-02T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Advanced DQNs: Playing <b>Pac-man</b> with Deep Reinforcement <b>Learning</b> | by ...", "url": "https://towardsdatascience.com/advanced-dqns-playing-pac-man-with-deep-reinforcement-learning-3ffbd99e0814", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/advanced-dqns-playing-<b>pac-man</b>-with-deep-reinforcement...", "snippet": "This <b>can</b> <b>be thought</b> of as the difference between the \u2018true\u2019 or target Q values and our current estimation of them, where the target value is the immediate reward plus the Q value of the action we will take in the next state. Of course, that value is also calculated by our network, but the overall expression is inherently more accurate thanks to it having access to at least the first reward term. Even so, this is definitely the math equivalent of trying to hit a moving target, as the true ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Learning in Zero-Sum Team Markov Games Using Factored Value ... - NeurIPS", "url": "https://proceedings.neurips.cc/paper/2002/file/4ae67a7dd7e491f8fb6f9ea0cf25dfdb-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2002/file/4ae67a7dd7e491f8fb6f9ea0cf25dfdb-Paper.pdf", "snippet": "We consider the standard approach of approximating the <b>Q function</b> as the linear combination of k basis functions ... function <b>can</b> <b>be thought</b> of as an individual player\u2019s perception of the environment, so each \u02daj need not depend upon every feature of the state or the actions taken by every player in the game. In particular, we assume that each \u02daj depends only on the actions of a small subset of maximizers Aj and minimizers Oj, that is, \u02daj = \u02daj(s; aj;o j), where a j 2 A j and o j 2 O j ...", "dateLastCrawled": "2022-01-16T13:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "About | <b>ardv1</b>", "url": "https://www.espyonard.com/about", "isFamilyFriendly": true, "displayUrl": "https://www.espyonard.com/about", "snippet": "Neat <b>program</b>. The &quot;<b>Q&quot; function</b> has some pretty good text file cleanup and smarts to it (I used to be a programmer and the text algorithms obviously have some <b>thought</b> behind them). I consider Q the power of this <b>program</b>. It&#39;s not as &quot;sexy&quot; as a direct import from radio reference, but the added power of copying and pasting lists from any source to a text file really versatile. JAY (USA) Mar 2019 I think it\u2019s important that I tell all who read this that, as a beta tester, and one who has had ...", "dateLastCrawled": "2022-01-19T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Answered: Which of the following is an accurate\u2026 | bartleby", "url": "https://www.bartleby.com/questions-and-answers/which-of-the-following-is-an-accurate-description-of-a-main-difference-between-our-brains-and-a-comp/38d86562-df3d-43e3-9f7c-6a0fe6c6a1d7", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bartleby.com</b>/questions-and-answers/which-of-the-following-is-an-accurate...", "snippet": "Engineering <b>Computer</b> Science Q&amp;A Library Which of the following is an accurate description of a main difference between our brains and a <b>computer</b>? O The brain processes information in a serial fashion O The brain thinks about nothing but pizza O There are separate components to the brain The brain likely does not have a central processor", "dateLastCrawled": "2022-01-31T20:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How does <b>the computer learn to calculate the expected future return</b> ...", "url": "https://www.quora.com/How-does-the-computer-learn-to-calculate-the-expected-future-return-max-Q-s-a-using-Q-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-does-<b>the-computer-learn-to-calculate-the-expected</b>-future...", "snippet": "Answer: Q-learning is a off-Policy, model-free reinforcement learning algorithm for Temporal Difference (TD) learning. Specifically, Q-learning <b>can</b> be used to find an optimal action-selection policy for any given (finite) Markov decision process (MDP). In Q-learning, an agent tries to learn an a...", "dateLastCrawled": "2022-01-21T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>can</b> I use Deep Q learning if the action space is continuous, eg ...", "url": "https://www.quora.com/How-can-I-use-Deep-Q-learning-if-the-action-space-is-continuous-eg-action-from-water-jet-engine-of-autonomous-boat", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-I-use-Deep-Q-learning-if-the-action-space-is-continuous...", "snippet": "Answer (1 of 2): The way Q learning works, if you look at the algorithm, you are always maximizing over discrete action selection (maximum Q value output) , so in this sense you cannot utilize Q learning for continuous action spaces. You could always &quot;cheat&quot; by discretized the action space, but ...", "dateLastCrawled": "2022-01-17T14:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>introduction to Q-Learning: reinforcement learning</b>", "url": "https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/", "isFamilyFriendly": true, "displayUrl": "https://www.freecodecamp.org/news/an-<b>introduction-to-q-learning-reinforcement-learning</b>...", "snippet": "The <b>Q-function</b> uses the Bellman equation and takes two inputs: state (s) and action (a). Using the above function, we get the values of Q for the cells in the table. When we start, all the values in the Q-table are zeros. There is an iterative process of updating the values. As we start to explore the environment, the <b>Q-function</b> gives us better and better approximations by continuously updating the Q-values in the table. Now, let\u2019s understand how the updating takes place. Introducing the Q ...", "dateLastCrawled": "2022-02-02T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An Improved Approximation for the Gaussian <b>Q-Function</b>", "url": "http://users.auth.gr/geokarag/pdf/Q_Approxim.pdf", "isFamilyFriendly": true, "displayUrl": "users.auth.gr/geokarag/pdf/Q_Approxim.pdf", "snippet": "An Improved Approximation for the Gaussian <b>Q-Function</b> George K. Karagiannidis, Senior Member, IEEE, and Athanasios S. Lioumpas Student Member, IEEE Abstract\u2014We present a novel, simple and tight approximation for the Gaussian <b>Q-function</b> and its integer powers. <b>Compared</b> to other known closed-form approximations, an accuracy improve-", "dateLastCrawled": "2021-12-29T01:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Quality-functions for an uniform and comparable analysis of demand side ...", "url": "https://www.deepdyve.com/lp/springer-journals/quality-functions-for-an-uniform-and-comparable-analysis-of-demand-2teUaTww0q", "isFamilyFriendly": true, "displayUrl": "https://www.deepdyve.com/lp/springer-journals/quality-functions-for-an-uniform-and...", "snippet": "However, they cannot be easily <b>compared</b> due to a lot of different assumptions while generating the consumption curves and pursuing different goals in each DSM-algorithm. This paper describes a method to transparently generate a data-basis as input for the DSM-algorithms to produce comparable results. A number of quality-functions (Q-functions) are mathematically described and discussed. The Q-Functions allow a consistent evaluation of DSM-algorithms by analysing managed and unmanaged ...", "dateLastCrawled": "2021-08-03T20:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Highly Accurate Analytic Approximation to</b> the Gaussian <b>Q-function</b> Based ...", "url": "https://link.springer.com/article/10.1007%2Fs10957-012-0217-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10957-012-0217-0", "snippet": "In this paper, as an extension of a previous study, an improved approximation for the Gaussian <b>Q-function</b> is presented. The nonlinear least squares algorithm is employed to optimize the coefficients of the proposed approximation. The accuracy of the presented approximation is evaluated using extensive <b>computer</b> simulations. Results show that the proposed approximation has superior accuracy in high arguments\u2019 region when <b>compared</b> to the performance of other approaches introduced in the ...", "dateLastCrawled": "2022-01-20T02:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Model-free LQR design by <b>Q-function</b> learning - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0005109821005884", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0005109821005884", "snippet": "For the case of LQR, the <b>Q-function</b> <b>can</b> be expressed as a quadratic form in both x (k) and u (k) (see Bradtke et al., 1994 for details), (11) Q K x (k), u (k) = x (k) u (k) T H x (k) u (k), (12) H = Q + A T P A A T P B B T P A R + B T P B. Using the Kronecker product, (13) Q K x (k), u (k) = v e c (H) T x (k) u (k) \u2297 x (k) u (k). where v e c (H) denotes the vector formed by stacking the columns of H. Accordingly, a quadratic basis with state and input trajectories are constructed to learn ...", "dateLastCrawled": "2022-01-22T02:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Novel Approximations for the <b>Q -Function</b> with Application in SQNR ...", "url": "https://www.researchgate.net/publication/315324172_Novel_Approximations_for_the_Q_-Function_with_Application_in_SQNR_Calculation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315324172_Novel_Approximations_for_the_Q...", "snippet": "In recent years, intensive research has been performed with the goal to determine novel tight bounds on the <b>Q-function</b>, for instance in the literature. 3-6,10,11 Unlike with Jang and Nikoli\u0107, 3 ...", "dateLastCrawled": "2022-01-29T01:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Highly Accurate Analytic Approximation to</b> the Gaussian <b>Q-function</b> Based ...", "url": "https://www.researchgate.net/publication/233918996_Highly_Accurate_Analytic_Approximation_to_the_Gaussian_Q-function_Based_on_the_Use_of_Nonlinear_Least_Squares_Optimization_Algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/233918996_Highly_Accurate_Analytic...", "snippet": "It <b>can</b> be used as a graduate text in engineering, operations research, mathematics, <b>computer</b> science, and business. It also serves as a handbook for researchers and practitioners in the field. The ...", "dateLastCrawled": "2021-12-25T00:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Answered: Complete the following <b>program</b> by\u2026 | bartleby", "url": "https://www.bartleby.com/questions-and-answers/complete-the-following-program-by-writing-its-functions/66d45e58-c6c7-413e-ad48-958ed0647b4b", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bartleby.com</b>/questions-and-answers/complete-the-following-<b>program</b>-by...", "snippet": "Q: <b>Compared</b> to a manual method, what are the advantages of a basic batch <b>computer</b> system? A: Step 1 Introduction An efficient manner of processing high/large volumes of data is what you call B... question_answer", "dateLastCrawled": "2022-01-31T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Deep Q Network vs Policy Gradients - An Experiment on VizDoom with ...", "url": "https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html", "isFamilyFriendly": true, "displayUrl": "https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html", "snippet": "Deep Q Network vs Policy Gradients - An Experiment on VizDoom with Keras. October 12, 2017 After a brief stint with several interesting <b>computer</b> vision projects, include this and this, I\u2019ve recently decided to take a break from <b>computer</b> vision and explore reinforcement learning, another exciting field.Similar to <b>computer</b> vision, the field of reinforcement learning has experienced several important breakthroughs made possible by the deep learning revolution.", "dateLastCrawled": "2022-01-31T19:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Genetic Algorithms - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/genetic-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/genetic-algorithms", "snippet": "Genetic algorithms are based on the ideas of natural selection and genetics. These are intelligent exploitation of random search provided with historical data to direct the search into the region of better performance in solution space. They are commonly used to generate high-quality solutions for optimization problems and search problems.", "dateLastCrawled": "2022-02-02T23:36:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>introduction to Q-Learning: Reinforcement Learning</b>", "url": "https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/an-<b>introduction-to-q-learning-reinforcement-learning</b>", "snippet": "In this article, we are going to step into the world of reinforcement <b>learning</b>, another beautiful branch of artificial intelligence, which lets machines learn on their own in a way different from traditional <b>machine</b> <b>learning</b>. Particularly, we will be covering the simplest reinforcement <b>learning</b> algorithm i.e. the Q-<b>Learning</b> algorithm in great detail.", "dateLastCrawled": "2022-01-31T09:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Q-function</b>: input the state-atcion pair, output the Q-value. The letter \u201cQ\u201d is used to represent the quality of taking a given action in a given state. Q-<b>learning</b>. It is used for <b>learning</b> the optimal policy by <b>learning</b> the optimal Q-values for each state-action pair in a Markov Decision Process", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Q-function</b>: input the state-atcion pair, output the Q-value. The letter \u201cQ\u201d is used to represent the quality of taking a given action in a given state. Q-<b>learning</b>. It is used for <b>learning</b> the optimal policy by <b>learning</b> the optimal Q-values for each state-action pair in a Markov Decision Process", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement Q-<b>Learning</b> from Scratch in Python with OpenAI Gym ...", "url": "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/", "isFamilyFriendly": true, "displayUrl": "https://www.learndatasci.com/tutorials/reinforcement-q-<b>learning</b>-scratch-python-openai-gym", "snippet": "Q-<b>learning</b> is one of the easiest Reinforcement <b>Learning</b> algorithms. The problem with Q-<b>learning</b> however is, once the number of states in the environment are very high, it becomes difficult to implement them with Q table as the size would become very, very large. State of the art techniques uses Deep neural networks instead of the Q-table (Deep Reinforcement <b>Learning</b>). The neural network takes in state information and actions to the input layer and learns to output the right action over the ...", "dateLastCrawled": "2022-02-03T03:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Relationship between state (V) and action(Q) value function in ...", "url": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and-action-q-value-function-in-reinforcement-learning-bb9a988c0127", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and...", "snippet": "Value function can be defined as the expected value of an agent in a certain state. There are two types of value functions in RL: State-value and action-value. It is important to understand the\u2026", "dateLastCrawled": "2022-02-03T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning rate of a Q learning agent</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/33011825/learning-rate-of-a-q-learning-agent", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/33011825", "snippet": "If the <b>learning</b> rate is constant, will <b>Q function</b> converge to the optimal on or <b>learning</b> rate should necessarily decay to guarantee convergence? <b>machine</b>-<b>learning</b> reinforcement-<b>learning</b> q-<b>learning</b>. Share. Follow asked Oct 8 &#39;15 at 9:31. uduck uduck. 119 1 1 silver badge 8 8 bronze badges. 2. 4. With a sufficiently small <b>learning</b> rate you have a convergence guarantee for a convex q <b>learning</b> problem. \u2013 Thomas Jungblut. Oct 8 &#39;15 at 15:27. I assume there is also a dependence on the nature of ...", "dateLastCrawled": "2022-01-24T06:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Reinforcement <b>learning</b>: <b>Temporal-Difference</b>, SARSA, Q-<b>Learning</b> ...", "url": "https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/reinforcement-<b>learning</b>-<b>temporal-difference</b>-sarsa-q...", "snippet": "Source: Introduction to Reinforcement <b>learning</b> by Sutton and Barto \u2014Chapter 6. The action A\u2019 in the above algorithm is given by following the same policy (\u03b5-greedy over the Q values) because SARSA is an on-policy method.. \u03b5-greedy policy. Epsilon-greedy policy is this: Generate a random number r \u2208[0,1]; If r&lt;\u03b5 choose an action derived from the Q values (which yields the maximum utility); Else choose a random action", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "On using Huber loss in (Deep) Q-<b>learning</b> | \u30e4\u30ed\u30df\u30eb", "url": "https://jaromiru.wordpress.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://jaromiru.wordpress.com/2017/05/27/on-using-huber-loss-in-deep-q-<b>learning</b>", "snippet": "<b>MACHINE</b> <b>LEARNING</b> &amp; AI. Menu. Let\u2019s make a DQN. Theory; Implementation; Debugging; Full DQN; Double <b>Learning</b> and Prioritized Experience Replay; Let\u2019s make an A3C. Theory ; Implementation; About me; On using Huber loss in (Deep) Q-<b>learning</b>. Posted on May 27, 2017 May 30, 2017 by \u30e4\u30ed\u30df\u30eb. I\u2019ve been recently working on a problem where I put a plain DQN to use. The problem is very simple, deterministic, partially observable and states are quite low-dimensional. The agent however can ...", "dateLastCrawled": "2021-12-26T09:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why doesn&#39;t <b>Q-learning</b> converge when using function approximation ...", "url": "https://ai.stackexchange.com/questions/11679/why-doesnt-q-learning-converge-when-using-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/11679/why-doesnt-<b>q-learning</b>-converge-when-using...", "snippet": "$\\begingroup$ @nbro The proof doesn&#39;t say that explicitly, but it assumes an exact representation of the <b>Q-function</b> (that is, that exact values are computed and stored for every state/action pair). For infinite state spaces, it&#39;s clear that this exact representation can be infinitely large in the worst case (simple example: let Q(s,a) = sth digit of pi).", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Reinforcement Learning</b> as Heuristic Search <b>Analogy</b> - DataDrivenInvestor", "url": "https://medium.datadriveninvestor.com/reinforcement-learning-as-heuristic-search-analogy-31d92b06dadd", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/<b>reinforcement-learning</b>-as-heuristic-search...", "snippet": "I will not go over all the RL Algorithms, only a subset of those that fit my <b>analogy</b> well, nor will I be giving example code. This post is a purely theoretical outlook and assumes that you can translate the pseudo-code to actual code later. This post will work best if you have some knowledge of basic RL algorithms (TD <b>Learning</b>, Dynamic Programming etc), though I will attempt to go from scratch. Those that have prior knowledge of <b>Reinforcement Learning</b> will benefit the most from this post. On ...", "dateLastCrawled": "2022-01-21T02:55:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Using <b>Keras and Deep Q-Network to Play FlappyBird</b> | Ben Lau", "url": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "isFamilyFriendly": true, "displayUrl": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "snippet": "You just follow the guidiance from the strategy book. Here, <b>Q-function is similar</b> to a strategy guide. Suppose you are in state s and you need to decide whether you take action a or b. If you have this magical Q-function, the answers become really simple \u2013 pick the action with highest Q-value! Here, represents the policy, which you will often see in the ML literature. How do we get the Q-function? That\u2019s where Q-<b>learning</b> is coming from. Let me quickly derive here: Define total future ...", "dateLastCrawled": "2022-01-30T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Learn to Make Decision <b>with Small Data for Autonomous Driving: Deep</b> ...", "url": "https://www.hindawi.com/journals/jat/2020/8495264/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/jat/2020/8495264", "snippet": "GP is a Bayesian nonparametric <b>machine</b> <b>learning</b> framework for regression, classification, and unsupervised <b>learning</b> . A GP ... In addition, the <b>learning</b> method of <b>Q function is similar</b> to that in DQN as well. In our case, we train a deep neural network by DDPG to achieve successful loop trip. It takes about 16 hours and 4000 episodes to achieve a high performance deep neural network. And tens of thousands of data will be updated in the centralized experience replay buffer during training ...", "dateLastCrawled": "2022-01-22T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficient Navigation of Colloidal Robots in an Unknown Environment via ...", "url": "https://onlinelibrary.wiley.com/doi/10.1002/aisy.201900106", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1002/aisy.201900106", "snippet": "In free space navigation (Figure 2a), the navigation strategy derived from the learned optimal <b>Q* function is similar</b> to previous studies 18, 43, 44 and can be summarized approximately as \u03c0 * (s) = {v max, d n \u2208 [d c, \u221e) v max, d n \u2208 [0, d c), \u03b1 n \u2208 [\u2212 \u03b1 c, \u03b1 c] 0, otherwise (3) where d n is the projection of the target-particle vector onto the orientation vector n = (cos\u03b8, sin\u03b8), \u03b1 n is the angle between target-particle distance vector and n, and parameters d c and \u03b1 c are ...", "dateLastCrawled": "2022-01-20T08:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Adapting Soft Actor Critic for Discrete Action Spaces | by Felix ...", "url": "https://towardsdatascience.com/adapting-soft-actor-critic-for-discrete-action-spaces-a20614d4a50a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/adapting-soft-actor-critic-for-discrete-action-spaces-a...", "snippet": "This should accelerate <b>learning</b> in the later stages of training and help with avoiding local optima. Just as before we want to find \u03b8 that optimizes the expected return. To do so in the entropy regularized setting we can simply add an estimate of the entropy to our estimate of the expected return: Entropy Regularized Actor Cost Function. Figure 7: Entropy regularized critic cost functions. How we adapt the Bellman equation for our <b>Q-function is similar</b> to what we have seen in the definition ...", "dateLastCrawled": "2022-02-03T12:06:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep Reinforcement <b>Learning</b> for Agriculture: Principles and Use Cases ...", "url": "https://link.springer.com/chapter/10.1007/978-981-16-5847-1_4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-981-16-5847-1_4", "snippet": "In other words, the Q-function captures the expected total future rewards agent i can receive in state s t by taking action a t. <b>Q-function can be thought of as</b> a table look up, where rows of the table are states s and columns represent actions a.Ultimately, the <b>learning</b> agent i needs to find the best action given current state s.This is called a policy \u03c0(s).Policy captures the <b>learning</b> agent&#39;s behavior at any given time.", "dateLastCrawled": "2022-01-27T09:13:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(q-function)  is like +(a computer program)", "+(q-function) is similar to +(a computer program)", "+(q-function) can be thought of as +(a computer program)", "+(q-function) can be compared to +(a computer program)", "machine learning +(q-function AND analogy)", "machine learning +(\"q-function is like\")", "machine learning +(\"q-function is similar\")", "machine learning +(\"just as q-function\")", "machine learning +(\"q-function can be thought of as\")", "machine learning +(\"q-function can be compared to\")"]}