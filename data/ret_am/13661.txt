{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Implicit Bias</b> | SWD at NIH", "url": "https://diversity.nih.gov/sociocultural-factors/implicit-bias", "isFamilyFriendly": true, "displayUrl": "https://diversity.nih.gov/sociocultural-factors/<b>implicit-bias</b>", "snippet": "<b>Implicit bias</b> is a form of <b>bias</b> that occurs automatically and unintentionally, that nevertheless affects judgments, decisions, and behaviors. Research has shown <b>implicit bias</b> can pose a barrier to recruiting and retaining a diverse scientific workforce. Research Shows. The good news is that <b>implicit bias</b> can be mitigated with awareness and effective <b>bias</b>-reduction strategies. We provide a few examples of these strategies that you can use to reduce <b>implicit bias</b>. Think of counter-stereotypic ...", "dateLastCrawled": "2022-02-03T05:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding <b>Bias</b> in <b>Machine</b> <b>Learning</b> Models - Arize AI", "url": "https://arize.com/blog/understanding-bias-in-ml-models/", "isFamilyFriendly": true, "displayUrl": "https://arize.com/blog/understanding-<b>bias</b>-in-ml-models", "snippet": "For decades, <b>bias</b> in <b>machine</b> <b>learning</b> has been recognized as a potential concern, but it remains a complex and challenging issue for <b>machine</b> <b>learning</b> researchers and engineers when deploying models into production. The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm, which is utilized in US court systems to estimate the probability that a defendant will be a reoffender, is the most prominent example of AI <b>bias</b> and the negative implications on society ...", "dateLastCrawled": "2022-02-01T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Addressing Fairness, <b>Bias</b>, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "When a <b>machine</b> <b>learning</b> model is <b>being</b> <b>used</b> as part of a public service, such as the delivery of health care, it is important to consider how and why the technology is <b>being</b> <b>used</b>. Given the increasing availability of <b>machine</b> <b>learning</b> software, it has become quite popular to try applying <b>machine</b> <b>learning</b> to many different decision-making needs in health care. However, given the limitations of artificial intelligence, it is also important to consider at the outset why a computer algorithm is ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Data <b>Bias</b> and <b>Machine</b> <b>Learning</b>. I have often heard people say, \u201cthe ...", "url": "https://medium.com/skiplist-publication/data-bias-and-machine-learning-7bb021f2dc88", "isFamilyFriendly": true, "displayUrl": "https://medium.com/skiplist-publication/data-<b>bias</b>-and-<b>machine</b>-<b>learning</b>-7bb021f2dc88", "snippet": "Data <b>Bias</b> and <b>Machine</b> <b>Learning</b>. Fahad . Follow. Sep 4, 2018 \u00b7 5 min read. I have often heard people say, \u201cthe data speaks for itself.\u201d This sentiment is not only naive, it is also very ...", "dateLastCrawled": "2021-06-21T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What Do We Do About the Biases in AI?", "url": "https://hbr.org/2019/10/what-do-we-do-about-the-biases-in-ai", "isFamilyFriendly": true, "displayUrl": "https://<b>hbr.org</b>/2019/10/what-do-we-do-about-the-<b>bias</b>es-in-ai", "snippet": "<b>Bias</b> can creep into algorithms in several ways. AI systems learn to make decisions based on training data, which can include biased human decisions or reflect historical or social inequities, even ...", "dateLastCrawled": "2022-01-30T19:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Algorithmic bias: on the implicit biases of social technology</b> ...", "url": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "snippet": "An AI <b>like</b> Tay, which uses <b>machine</b> <b>learning</b> to capitalize on (or \u201clearn\u201d from) statistical regularities in human-generated datasets, tends to pick up social patterns that manifest in human behavior and that are reflected in the data on which it is trained. In many of these cases, we have reason to suspect that programmers are not explicitly writing biases toward marginalized demographics into their software\u2019s code. Footnote 2 Instead, it appears the biases in some sense implicitly ...", "dateLastCrawled": "2022-01-25T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Root Out <b>Bias</b> at Every <b>Stage of Your AI-Development Process</b>", "url": "https://hbr.org/2020/10/root-out-bias-at-every-stage-of-your-ai-development-process", "isFamilyFriendly": true, "displayUrl": "https://<b>hbr.org</b>/2020/10/root-out-<b>bias</b>-at-every-<b>stage-of-your-ai-development-process</b>", "snippet": "This data is <b>used</b> to develop <b>machine</b> <b>learning</b> models, and is often where the underlying <b>bias</b> seeps in. <b>Bias</b> can be introduced by the selection or sampling of the training data itself. This may ...", "dateLastCrawled": "2022-01-31T23:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>machine</b> <b>learning</b> approach to the potential-field method for <b>implicit</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0098300416304848", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0098300416304848", "snippet": "<b>Machine</b> <b>learning</b> <b>technique</b> for <b>implicit</b> geological modeling. ... minimal user-induced <b>bias</b>, and the straightforward incorporation of multi source information (Vollgger et al., 2015). As stated by McLennan and Deutsch (2006), a good methodology for <b>implicit</b> modeling should be simple, realistic, and provide some measure of uncertainty in its results. Geostatistics is the <b>technique</b> of choice for modeling spatial variations of properties in geoscientific related problems (Chil\u00e8s and Delfiner ...", "dateLastCrawled": "2021-12-24T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Forget Killer Robots\u2014<b>Bias</b> Is the Real AI Danger | <b>MIT Technology Review</b>", "url": "https://www.technologyreview.com/2017/10/03/241956/forget-killer-robotsbias-is-the-real-ai-danger/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.technologyreview.com</b>/2017/10/03/241956/forget-killer-robots<b>bias</b>-is-the...", "snippet": "The problem of <b>bias</b> in <b>machine</b> <b>learning</b> is likely to become more significant as the technology spreads to critical areas <b>like</b> medicine and law, and as more people without a deep technical ...", "dateLastCrawled": "2022-01-28T19:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "inductive <b>bias</b> in <b>machine</b> <b>learning</b> geeksforgeeks", "url": "http://pahurling.com/site/9zrmu5t/inductive-bias-in-machine-learning-geeksforgeeks", "isFamilyFriendly": true, "displayUrl": "pahurling.com/site/9zrmu5t/inductive-<b>bias</b>-in-<b>machine</b>-<b>learning</b>-geeksforgeeks", "snippet": "Thus, the assumption of <b>machine</b> <b>learning</b> <b>being</b> free of <b>bias</b> is a false one, <b>bias</b> <b>being</b> a fundamental property of inductive <b>learning</b> systems. 23) What is Model Selection in <b>Machine</b> <b>Learning</b>? In <b>machine</b> <b>learning</b>, one aims to construct algorithms that are able to learn to predict a certain target output. It is seen as a part of artificial intelligence.<b>Machine</b> <b>learning</b> algorithms build a model based on sample data, known as &quot;training data&quot;, in order to make predictions or decisions without <b>being</b> ...", "dateLastCrawled": "2022-01-29T07:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Mitigating <b>Bias</b> in <b>Machine Learning</b>: An introduction to ...", "url": "https://towardsdatascience.com/mitigating-bias-in-machine-learning-an-introduction-to-mlfairnesspipeline-42e007dce0a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/mitigating-<b>bias</b>-in-<b>machine-learning</b>-an-introduction-to...", "snippet": "<b>Bias</b> in <b>Machine Learning</b>. <b>Bias</b> takes many different forms and impact all groups of people. It can range from <b>implicit</b> to expli c it and is often very difficult to detect. In the field of <b>machine learning</b> <b>bias</b> is often subtle and hard to identify, let alone solve. Why is this a problem? <b>Implicit</b> <b>bias</b> in <b>machine learning</b> has very real consequences including denial of a loan, a lengthier prison sentence, and many other harmful outcomes for underprivileged groups. The data scientists designing ...", "dateLastCrawled": "2022-02-02T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Algorithmic bias: on the implicit biases of social technology</b> ...", "url": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "snippet": "Often <b>machine</b> <b>learning</b> programs inherit social patterns reflected in their training data without any directed effort by programmers to include such biases. Computer scientists call this algorithmic <b>bias</b>. This paper explores the relationship between <b>machine</b> <b>bias</b> and human cognitive <b>bias</b>. In it, I argue similarities between algorithmic and cognitive biases indicate a disconcerting sense in which sources of <b>bias</b> emerge out of seemingly innocuous patterns of information processing. The emergent ...", "dateLastCrawled": "2022-01-25T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Racially Unbiased, <b>Machine</b> <b>Learning</b> Approach to Prediction of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7644374/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7644374", "snippet": "To assess whether the <b>machine</b> <b>learning</b> algorithm and each comparator identified <b>similar</b> at-risk individuals, the McNemar test was <b>used</b>, comparing performance of the two systems at a sensitivity around 0.75. Performance was assessed both on the overall sample and after stratifying by race. Racial categories were defined as white and nonwhite, where only non-Hispanic white patients were included in the white category (eg, a white Hispanic patient was considered nonwhite for the purpose of this ...", "dateLastCrawled": "2021-12-18T01:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A <b>machine learning</b> <b>technique</b> that iteratively combines a set of simple and not very accurate classifiers (referred to as &quot;weak&quot; classifiers) ... Confirmation <b>bias</b> is a form of <b>implicit</b> <b>bias</b>. Experimenter&#39;s <b>bias</b> is a form of confirmation <b>bias</b> in which an experimenter continues training models until a preexisting hypothesis is confirmed. confusion matrix. An NxN table that summarizes how successful a classification model&#39;s predictions were; that is, the correlation between the label and the ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>machine</b> <b>learning</b> approach to the potential-field method for <b>implicit</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0098300416304848", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0098300416304848", "snippet": "<b>Machine</b> <b>learning</b> <b>technique</b> for <b>implicit</b> geological modeling. ... In that work the authors define the potential as a scalar field, with the geological surfaces of interest <b>being</b> modeled as different isovalues in that field. Here the potential field is defined as a vector field \u03d5 (x) = (\u03d5 1 (x), \u03d5 2 (x), \u2026, \u03d5 C (x)) T, and the potential components \u03d5 c (x) are all linked due to their compositional origin. <b>Implicit</b> modeling amounts to estimating \u03d5 over the region of interest, the details ...", "dateLastCrawled": "2021-12-24T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Yale researchers combat biases in <b>machine</b> <b>learning</b> algorithms - Yale ...", "url": "https://yaledailynews.com/blog/2021/11/28/yale-researchers-combat-biases-in-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://yaledailynews.com/blog/2021/11/28/yale-researchers-combat-<b>bias</b>es-in-<b>machine</b>...", "snippet": "These algorithms are <b>used</b> under the assumption that they are impartial; however, biases often become ingrained in <b>machine</b> <b>learning</b> programs through training methods and data, according to Amin Karbasi, professor of electrical engineering and computer science, and Ehsan Kazem, former Yale postdoctoral fellow. But now, researchers at the Yale School of Management have designed a novel \u201ctrain then mask\u201d <b>technique</b> for supervised <b>learning</b> to help eliminate these biases from algorithms and ...", "dateLastCrawled": "2022-01-31T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Analyzing &amp; <b>Preventing Unconscious Bias in Machine Learning</b>", "url": "https://www.infoq.com/presentations/unconscious-bias-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.infoq.com/presentations/unconscious-<b>bias</b>-<b>machine</b>-<b>learning</b>", "snippet": "So, one is that <b>machine</b> <b>learning</b> can actually amplify <b>bias</b>. There is an interesting paper called &quot;Men also like shopping&quot; where they looked at a dataset. These are kind commonly site datasets ...", "dateLastCrawled": "2022-02-03T03:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Implicit</b> Stereotypes in Pre-Trained Classifiers", "url": "https://www.researchgate.net/publication/357194307_Implicit_Stereotypes_in_Pre-Trained_Classifiers", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357194307_<b>Implicit</b>_Stereotypes_in_Pre-Trained...", "snippet": "Pre-trained deep <b>learning</b> models underpin many public-facing applications, and their propensity to reproduce <b>implicit</b> racial and gender stereotypes is an increasing source of concern.", "dateLastCrawled": "2021-12-25T07:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Top <b>10 Papers On Transfer Learning One Must Read</b> In 2020", "url": "https://analyticsindiamag.com/top-10-papers-on-transfer-learning-one-must-read-in-2020/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/top-<b>10-papers-on-transfer-learning-one-must-read</b>-in-2020", "snippet": "This <b>learning</b> is an approach to transferring a part of the network that has already been trained on a <b>similar</b> task while adding one or more layers at the end, and then re-train the model. In this article, we list down the top 10 researchers <b>papers on transfer learning one must read</b> in 2020. (The papers are listed according to the year of publishing) 1| Pay Attention to Features, Transfer Learn Faster CNNs. About: Transfer <b>learning</b> offers the chance for CNNs to learn with limited data samples ...", "dateLastCrawled": "2022-01-30T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning Exam 2</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/459481663/machine-learning-exam-2-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/459481663/<b>machine-learning-exam-2</b>-flash-cards", "snippet": "These neurons are stacked together to form a network, which can be <b>used</b> to approximate any function.-To get the best possible neural network, we can use techniques like gradient descent to update our neural network model. Given above is a description of a neural network. When does a neural network model become a deep <b>learning</b> model? A. When you ...", "dateLastCrawled": "2022-01-02T18:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Implicit</b> Racial/Ethnic <b>Bias</b> Among Health Care Professionals and Its ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4638275/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4638275", "snippet": "They also reported longer visits and experienced their visits with the provider as <b>being</b> less collaborative.32 Another study also found an association between <b>implicit</b> racial <b>bias</b> and verbal dominance by physicians during encounters with Black patients.34 Pro-White attitudes among primary care physicians were associated with lower scores by Black patients on physician warmth and friendliness, as well as lower scores by physicians regarding their feelings of \u201c<b>being</b> on the same team\u201d with ...", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Strategies to Reduce the Influence of <b>Implicit</b> <b>Bias</b>*", "url": "http://horsley.yale.edu/sites/default/files/files/IB_Strategies_033012.pdf", "isFamilyFriendly": true, "displayUrl": "horsley.yale.edu/sites/default/files/files/IB_Strategies_033012.pdf", "snippet": "<b>thought</b> that this type of training would be more effective if the program contained the following: a. ... group exercises and other experiential <b>learning</b> techniques could help make information more personally relevant, which could provide a valuable frame of reference for those who are expected to resist the idea of <b>implicit</b> <b>bias</b>. Brain teaser exercises may be <b>used</b> to introduce the topic and demonstrate its broad application beyond race to gender, class, age, weight, and other stigmatized ...", "dateLastCrawled": "2022-02-02T17:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Addressing Fairness, Bias, and Appropriate</b> Use of Artificial ...", "url": "https://europepmc.org/article/PMC/PMC8107824", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8107824", "snippet": "When a <b>machine</b> <b>learning</b> model is <b>being</b> <b>used</b> as part of a public service, such as the delivery of health care, it is important to consider how and why the technology is <b>being</b> <b>used</b>. Given the increasing availability of <b>machine</b> <b>learning</b> software, it has become quite popular to try applying <b>machine</b> <b>learning</b> to many different decision-making needs in health care. However, given the limitations of artificial intelligence, it is also important to consider at the outset why a computer algorithm is ...", "dateLastCrawled": "2021-06-01T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "deeplearning.ai&#39;s <b>Intro to TensorFlow (Week 2</b>) \u2013 Kevin Urban \u2013 Don&#39;t ...", "url": "https://krbnite.github.io/Deep-Learning-AI-Intro-to-TensorFlow-Week2/", "isFamilyFriendly": true, "displayUrl": "https://krbnite.github.io/Deep-<b>Learning</b>-AI-<b>Intro-to-TensorFlow-Week2</b>", "snippet": "This week\u2019s content got a little more into actual <b>machine</b> <b>learning</b> models, namely simple multiperceptron-style networks \u2013 i.e., going from a linear regression to a network with hidden layers and non-identity activation functions. Instead of using MNIST as a starting point, the course creators buck that trend and dive into Fashion MNIST. Very briefly, the fact that <b>implicit</b> biases may be inherent in a data set is mentioned, and it is pointed out that such biases <b>can</b> unknowingly leak into ...", "dateLastCrawled": "2022-01-29T10:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Analyzing &amp; <b>Preventing Unconscious Bias in Machine Learning</b>", "url": "https://www.infoq.com/presentations/unconscious-bias-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.infoq.com/presentations/unconscious-<b>bias</b>-<b>machine</b>-<b>learning</b>", "snippet": "So, one is that <b>machine</b> <b>learning</b> <b>can</b> actually amplify <b>bias</b>. There is an interesting paper called &quot;Men also like shopping&quot; where they looked at a dataset. These are kind commonly site datasets ...", "dateLastCrawled": "2022-02-03T03:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "inductive <b>bias</b> in <b>machine</b> <b>learning</b> geeksforgeeks", "url": "http://pahurling.com/site/9zrmu5t/inductive-bias-in-machine-learning-geeksforgeeks", "isFamilyFriendly": true, "displayUrl": "pahurling.com/site/9zrmu5t/inductive-<b>bias</b>-in-<b>machine</b>-<b>learning</b>-geeksforgeeks", "snippet": "Decision Tree is a Supervised <b>learning</b> <b>technique</b> that <b>can</b> be <b>used</b> for both classification and Regression problems, but mostly it is preferred for solving Classification problems. <b>Machine</b> <b>Learning</b> Any de\ufb01nition of <b>machine</b> <b>learning</b> is bound to be controversial. Inductive <b>Learning</b> Algorithm in <b>Machine</b> <b>Learning</b>. al, 2018) is an amazing read, which I will be referring to throughout this answer. Intuitively, <b>bias</b> <b>can</b> <b>be thought</b> as having a \u2018<b>bias</b>\u2019 towards people. In addition, the training ...", "dateLastCrawled": "2022-01-29T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "When a <b>machine</b> <b>learning</b> model is <b>being</b> <b>used</b> as part of a public service, such as the delivery of health care, it is important to consider how and why the technology is <b>being</b> <b>used</b>. Given the increasing availability of <b>machine</b> <b>learning</b> software, it has become quite popular to try applying <b>machine</b> <b>learning</b> to many different decision-making needs in health care. However, given the limitations of <b>artificial intelligence</b>, it is also important to consider at the outset why a computer algorithm is ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Gender <b>Bias</b> &amp; Artificial Intelligence: Questions and Challenges | CCTP ...", "url": "https://blogs.commons.georgetown.edu/cctp-607-spring2019/2019/05/06/gender-bias-artificial-intelligence-questions-and-challenges/", "isFamilyFriendly": true, "displayUrl": "https://blogs.commons.georgetown.edu/cctp-607-spring2019/2019/05/06/gender-<b>bias</b>...", "snippet": "Gender <b>bias</b> in AI, <b>machine</b> <b>learning</b>, and deep <b>learning</b> is the result of the replication by design of a deeply systemic, systematic, racist, sexist, gendered, class-oriented -and other axes of discrimination- <b>bias</b> embedded in most data collected by humans. Instead of erasing divisions through objectivity in decision making, this process is exacerbating inequality in the workplace, the legal and judicial systems, and other spaces of public life in which minorities interact. This happens in ...", "dateLastCrawled": "2022-01-25T09:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Bias</b>, awareness, and ignorance in deep-<b>learning</b>-based face recognition ...", "url": "https://link.springer.com/article/10.1007%2Fs43681-021-00108-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s43681-021-00108-6", "snippet": "We investigate how well <b>machine</b> <b>learning</b> models <b>can</b> predict the sensitive features, such as ethnicity and gender, based on the face embedding. The intuition is that an FR model is \u201caware\u201d of a sensitive feature if it <b>can</b> be predicted from the embedding vectors produced by the FR model. This inference is a classification task and the performance depends on the classification model at hand. If simple models, more precisely models with a low number of parameters, <b>can</b> properly infer the ...", "dateLastCrawled": "2022-02-03T10:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Root Out <b>Bias</b> at Every <b>Stage of Your AI-Development Process</b>", "url": "https://hbr.org/2020/10/root-out-bias-at-every-stage-of-your-ai-development-process", "isFamilyFriendly": true, "displayUrl": "https://<b>hbr.org</b>/2020/10/root-out-<b>bias</b>-at-every-<b>stage-of-your-ai-development-process</b>", "snippet": "This data is <b>used</b> to develop <b>machine</b> <b>learning</b> models, and is often where the underlying <b>bias</b> seeps in. <b>Bias</b> <b>can</b> be introduced by the selection or sampling of the training data itself. This may ...", "dateLastCrawled": "2022-01-31T23:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Implicit Bias</b> | SWD at NIH", "url": "https://diversity.nih.gov/sociocultural-factors/implicit-bias", "isFamilyFriendly": true, "displayUrl": "https://diversity.nih.gov/sociocultural-factors/<b>implicit-bias</b>", "snippet": "<b>Bias</b> consists of attitudes, behaviors, and actions that are prejudiced in favor of or against one person or group <b>compared</b> to another. <b>Implicit Bias</b>: Training Module Now Open!! Check Out The Training Module. <b>Implicit Bias</b> Presentation. What is <b>implicit bias</b>? <b>Implicit bias</b> is a form of <b>bias</b> that occurs automatically and unintentionally, that nevertheless affects judgments, decisions, and behaviors. Research has shown <b>implicit bias</b> <b>can</b> pose a barrier to recruiting and retaining a diverse ...", "dateLastCrawled": "2022-02-03T05:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Mitigating <b>Bias</b> in <b>Machine Learning</b>: An introduction to ...", "url": "https://towardsdatascience.com/mitigating-bias-in-machine-learning-an-introduction-to-mlfairnesspipeline-42e007dce0a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/mitigating-<b>bias</b>-in-<b>machine-learning</b>-an-introduction-to...", "snippet": "<b>Bias</b> in <b>Machine Learning</b>. <b>Bias</b> takes many different forms and impact all groups of people. It <b>can</b> range from <b>implicit</b> to expli c it and is often very difficult to detect. In the field of <b>machine learning</b> <b>bias</b> is often subtle and hard to identify, let alone solve. Why is this a problem? <b>Implicit</b> <b>bias</b> in <b>machine learning</b> has very real consequences including denial of a loan, a lengthier prison sentence, and many other harmful outcomes for underprivileged groups. The data scientists designing ...", "dateLastCrawled": "2022-02-02T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Racially Unbiased, <b>Machine</b> <b>Learning</b> Approach to Prediction of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7644374/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7644374", "snippet": "This study aims to determine whether a <b>machine</b> <b>learning</b> algorithm <b>can</b> minimize racial <b>bias</b> in patient risk predictions as <b>compared</b> with commonly <b>used</b> rules-based methods. Methods. Data Processing . Data were drawn from the Medical Information Mart for Intensive Care\u2013III (MIMIC-III) database . The database consists of data on more than 53,000 patient encounters for patients admitted to the intensive care unit at a large academic health center between 2001 and 2012. Patients were included if ...", "dateLastCrawled": "2021-12-18T01:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>How to Reduce Implicit Bias</b> - <b>IHI</b>", "url": "http://www.ihi.org/communities/blogs/how-to-reduce-implicit-bias", "isFamilyFriendly": true, "displayUrl": "<b>www.ihi.org</b>/communities/blogs/<b>how-to-reduce-implicit-bias</b>", "snippet": "<b>Implicit</b> <b>bias</b> <b>can</b> negatively affect other elements of patient interaction with the health care system. A 2015 study found that racial/ethnic minorities, individuals with lower levels of education, and unemployed individuals spend significantly longer time waiting to obtain medical care, with blacks and Latinos waiting 19 and 25 minutes more, respectively, than white patients to see a doctor. In addition, anxiety about interactions with people of color <b>can</b> result in white providers spending ...", "dateLastCrawled": "2022-02-03T18:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "On the Basis of Sex: A Review of Gender <b>Bias</b> in <b>Machine</b> <b>Learning</b> ...", "url": "https://deepai.org/publication/on-the-basis-of-sex-a-review-of-gender-bias-in-machine-learning-applications", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-the-basis-of-sex-a-review-of-gender-<b>bias</b>-in-<b>machine</b>...", "snippet": "To assist in this task of making <b>machine</b> <b>learning</b> models more gender equitable, we first introduced readers to some examples of gender <b>bias</b> in AI, followed by a formalization of the concepts of fairness. Next, we turned to a summary of representative <b>bias</b> mitigation algorithms in research domains that are particularly prone to gender discrimination. Finally, we summarized in which domains fairness has been understudied and introduced numerous open source tools for assessing and mitigating <b>bias</b>.", "dateLastCrawled": "2022-02-02T00:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Comparative Assessment of Various <b>Machine</b> <b>Learning</b>\u2010Based <b>Bias</b> ...", "url": "https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2019EA000740", "isFamilyFriendly": true, "displayUrl": "https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2019EA000740", "snippet": "Unlike MOS and KF\u2014in which <b>bias</b>-correction is required to construct a model for each station\u2014<b>machine</b> <b>learning</b> <b>can</b> be <b>used</b> to develop a model that works for a multitude of stations. Because of these advantages, the spatial distributions of the predictand (e.g., air temperature) <b>can</b> be monitored when spatially continuous input variables are fed into <b>machine</b> <b>learning</b> models. Among various <b>machine</b> <b>learning</b> classifiers, Artificial Neural Network (ANN) has been the most popular <b>technique</b> for ...", "dateLastCrawled": "2021-12-08T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Analyzing &amp; <b>Preventing Unconscious Bias in Machine Learning</b>", "url": "https://www.infoq.com/presentations/unconscious-bias-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.infoq.com/presentations/unconscious-<b>bias</b>-<b>machine</b>-<b>learning</b>", "snippet": "So, one is that <b>machine</b> <b>learning</b> <b>can</b> actually amplify <b>bias</b>. There is an interesting paper called &quot;Men also like shopping&quot; where they looked at a dataset. These are kind commonly site datasets ...", "dateLastCrawled": "2022-02-03T03:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Addressing Fairness, Bias, and Appropriate</b> Use of Artificial ...", "url": "https://europepmc.org/article/PMC/PMC8107824", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8107824", "snippet": "When a <b>machine</b> <b>learning</b> model is <b>being</b> <b>used</b> as part of a public service, such as the delivery of health care, it is important to consider how and why the technology is <b>being</b> <b>used</b>. Given the increasing availability of <b>machine</b> <b>learning</b> software, it has become quite popular to try applying <b>machine</b> <b>learning</b> to many different decision-making needs in health care. However, given the limitations of artificial intelligence, it is also important to consider at the outset why a computer algorithm is ...", "dateLastCrawled": "2021-06-01T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Introduction to <b>Machine</b> <b>Learning</b>, Neural Networks, and Deep <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7347027/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7347027", "snippet": "The aim of this review article is to provide the nontechnical readers a layman&#39;s explanation of the <b>machine</b> <b>learning</b> methods <b>being</b> <b>used</b> in medicine today. The goal is to provide the reader a better understanding of the potential and challenges of artificial intelligence within the field of medicine. Keywords: deep <b>learning</b>, <b>machine</b> <b>learning</b>, artificial intelligence. Introduction. Over the past decade, artificial intelligence (AI) has become a popular subject both within and outside of the ...", "dateLastCrawled": "2022-02-02T05:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Bias</b> and Fairness in Multimodal <b>Machine</b> <b>Learning</b>: A Case Study of ...", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897", "snippet": "While balancing the data may have reduced the severity of gender <b>bias</b>, we note that it was still present in this case study and thus enabled us to focus the analysis on manifestations of gender <b>bias</b> introduced by the <b>machine</b> <b>learning</b> process itself. As part of the study&#39;s procedure, participants were asked to provide their gender and 727 self-affiliated as either a man (n=262) or woman (n=465). Since the non-binary gender (n=6) representation was insufficient for statistical analysis, we ...", "dateLastCrawled": "2021-10-28T00:12:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "<b>Bias</b> in humans can be unconscious (also called <b>implicit</b> <b>bias</b>), which indicates humans can introduce <b>bias</b> without even knowing they are doing so. Let\u2019s explore how we can detect <b>bias</b> in <b>machine learning</b> models and how it can be eliminated. Types of <b>bias</b>. <b>Bias</b> in <b>machine learning</b> data sets and models is such a problem that you\u2019ll find tools ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Implicit/Machine learning gender bias</b> | ReberLab", "url": "https://www.reberlab.psych.northwestern.edu/2018/10/11/implicitmachine-learning-gender-bias/", "isFamilyFriendly": true, "displayUrl": "https://www.reberlab.psych.northwestern.edu/.../10/11/<b>implicitmachine-learning-gender-bias</b>", "snippet": "First, I have found myself describing on a few recent occasions that it is reasonable to think of <b>implicit</b> <b>learning</b> (IL) as the brain\u2019s <b>machine</b> <b>learning</b> (ML) algorithm. ML is a super-hot topic in AI and data science research, so this might be a useful <b>analogy</b> to help people understand what we mean by studying IL. We characterize IL as the statistical extraction of patterns in the environment and the shaping of cognitive processing to maximize efficiency and effectiveness to these patterns ...", "dateLastCrawled": "2020-11-16T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Algorithmic bias: on the implicit biases of social technology</b> ...", "url": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "snippet": "Often <b>machine</b> <b>learning</b> programs inherit social patterns reflected in their training data without any directed effort by programmers to include such biases. Computer scientists call this algorithmic <b>bias</b>. This paper explores the relationship between <b>machine</b> <b>bias</b> and human cognitive <b>bias</b>. In it, I argue similarities between algorithmic and cognitive biases indicate a disconcerting sense in which sources of <b>bias</b> emerge out of seemingly innocuous patterns of information processing. The emergent ...", "dateLastCrawled": "2022-01-25T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Harnessing data for unbiased <b>Learning</b> of Machines | by Vivek Gupta | Medium", "url": "https://gvivek.medium.com/harnessing-data-for-unbiased-learning-of-machines-dc614497a29c", "isFamilyFriendly": true, "displayUrl": "https://gvivek.medium.com/harnessing-data-for-un<b>bias</b>ed-<b>learning</b>-of-<b>machines</b>-dc614497a29c", "snippet": "In <b>Machine</b> <b>Learning</b>, the input data reflects stereotypes and biases of the broader society, then the output of the <b>learning</b> algorithm also captures these stereotypes. Here, we will discuss the gender stereotypes in word embedding. word-embedding encode semantic information, they also exhibit hidden biases inherent in the dataset they are trained on associations such as: father:doctor :: mother:nurse. man:computer programmer :: woman:homemaker. The prejudices and stereotypes in these ...", "dateLastCrawled": "2022-01-12T17:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning</b> | Gendered Innovations", "url": "http://genderedinnovations.stanford.edu/case-studies/machinelearning.html", "isFamilyFriendly": true, "displayUrl": "genderedinnovations.stanford.edu/case-studies/<b>machinelearning</b>.html", "snippet": "As <b>machine learning</b> becomes increasingly ubiquitous in everyday lives, such <b>bias</b>, if uncorrected, can lead to social inequities. Researchers need to understand how gender and ethnicity operate within the context of their algorithm in order to enhance or, at least, not reinforce social equalities. Here we suggest avenues for reducing <b>bias</b> in training data and algorithms in efforts to produce AI that enhances social equalities.", "dateLastCrawled": "2022-01-25T22:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Introduction to Natural Language Processing</b> (NLP) and <b>Bias</b> in AI | by ...", "url": "https://towardsdatascience.com/introduction-to-natural-language-processing-nlp-and-bias-in-ai-877d3f3ee680", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>introduction-to-natural-language-processing</b>-nlp-and...", "snippet": "In this post, I will introduce key concepts of NLP such as word embeddings, and we will see how an algorithm can become biased, and how we can remove that <b>bias</b>. Le t \u2019s get started! For hands-on video tutorials on <b>machine</b> <b>learning</b>, deep <b>learning</b>, and artificial intelligence, checkout my YouTube channel.", "dateLastCrawled": "2022-01-20T19:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fairness Metrics - Data Analytics - Data Science, <b>Machine</b> <b>Learning</b>, AI", "url": "https://vitalflux.com/fairness-metrics-ml-model-sensitivity-bias-detection/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/<b>fairness-metrics-ml-model-sensitivity</b>-<b>bias</b>-detection", "snippet": "There are many different ways in which <b>machine</b> <b>learning</b> ... Instinctively (based on <b>implicit</b> human <b>bias</b>), it is one of the specific \u201cgender\u201d (male or female) against which the model could get biased as like in the real world in predicting whether those with a specific gender could re-offend or not. Thus, one of the protected attributes becomes gender. For the current example, in the real world, one could get biased in favor of female and not (or fail to) classify them as the ones who ...", "dateLastCrawled": "2022-01-20T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word ...", "url": "https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf", "snippet": "signi\ufb01cant risk and challenge for <b>machine</b> <b>learning</b> and its applications. The analogies generated from these embeddings spell out the <b>bias</b> <b>implicit</b> in the data on which they were trained. Hence, word embeddings may serve as a means to extract <b>implicit</b> gender associations from a large text corpus", "dateLastCrawled": "2022-02-02T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning is Requirements Engineering</b> \u2014 On the Role of Bugs ...", "url": "https://medium.com/analytics-vidhya/machine-learning-is-requirements-engineering-8957aee55ef4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>machine-learning-is-requirements-engineering</b>-8957...", "snippet": "A <b>machine</b>-learned model is not an attempt at implementing an <b>implicit</b> specification; a <b>machine</b>-learned model is a specification! It is a learned description of how the system shall behave. The ...", "dateLastCrawled": "2022-01-19T17:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Man is to Computer Programmer as Woman is</b> to Homemaker? | by Sheldon ...", "url": "https://towardsdatascience.com/man-is-to-computer-programmer-as-woman-is-to-homemaker-e57b07cbde96", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>man-is-to-computer-programmer-as-woman-is</b>-to-homemaker...", "snippet": "The paper discusses gender <b>bias</b> in <b>machine</b> <b>learning</b> as a result of using biased training data and proposes a solution to debias the model. This article contains an overview of the paper and discusses key findings in the python implementation of the paper. Overview. The authors of the paper used Word Embe d ding Model to demonstrate gender <b>bias</b> in the training data. The Word Embedding Model was trained on Google News articles and the vector representation of all words were stored in the ...", "dateLastCrawled": "2022-01-30T22:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CSWA: <b>Unconscious Bias</b> | American Astronomical Society", "url": "https://aas.org/comms/cswa/resources/unconsciousbias", "isFamilyFriendly": true, "displayUrl": "https://aas.org/comms/cswa/resources/<b>unconsciousbias</b>", "snippet": "The intervention is based on the premise that <b>implicit bias is like</b> a habit that can be broken through a combination of awareness of implicit bias, concern about the effects of that bias, and the application of strategies to reduce bias. In a 12-week longitudinal study, people who received the intervention showed dramatic reductions in implicit race bias. People who were concerned about discrimination or who reported using the strategies showed the greatest reductions. The intervention also ...", "dateLastCrawled": "2022-02-03T07:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Patrick Forscher</b> \u2013 Research Lead \u2013 Busara Center for Behavioral ...", "url": "https://ch.linkedin.com/in/patrick-forscher-91163854", "isFamilyFriendly": true, "displayUrl": "https://ch.linkedin.com/in/<b>patrick-forscher</b>-91163854", "snippet": "The intervention is based on the premise that <b>implicit bias is like</b> a habit that can be broken through a combination of awareness of implicit bias, concern about the effects of that bias, and the application of strategies to reduce bias. In a 12-week longitudinal study, people who received the intervention showed dramatic reductions in implicit race bias. People who were\u2026 We developed a multi-faceted prejudice habit-breaking intervention to produce long-term reductions in implicit race ...", "dateLastCrawled": "2022-02-03T15:10:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Those designing healthcare algorithms must become actively</b> anti-racist ...", "url": "https://www.nature.com/articles/s41591-020-1020-3", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41591-020-1020-3", "snippet": "<b>Just as \u2018implicit bias</b>\u2019 training for police does little to change racist behavior 10 \u2014in large part because departmental cultures do not fully support the lessons of anti-racism\u2014healthcare ...", "dateLastCrawled": "2021-11-15T00:12:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(implicit bias)  is like +(being used as a machine learning technique)", "+(implicit bias) is similar to +(being used as a machine learning technique)", "+(implicit bias) can be thought of as +(being used as a machine learning technique)", "+(implicit bias) can be compared to +(being used as a machine learning technique)", "machine learning +(implicit bias AND analogy)", "machine learning +(\"implicit bias is like\")", "machine learning +(\"implicit bias is similar\")", "machine learning +(\"just as implicit bias\")", "machine learning +(\"implicit bias can be thought of as\")", "machine learning +(\"implicit bias can be compared to\")"]}