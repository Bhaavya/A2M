{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Neural Networks, the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> and Option ...", "url": "https://medium.com/analytics-vidhya/neural-networks-and-the-universal-approximation-theorem-e5c387982eed", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/neural-networks-and-the-<b>universal</b>-<b>approximation</b>...", "snippet": "Data Science or <b>Machine</b> <b>Learning</b> is basically the application of algorithms based on mathematics and statistics to real world problems. Now I know there is a lot more to <b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2021-07-20T00:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Interval <b>Universal</b> <b>Approximation</b> for Neural Networks", "url": "https://pages.cs.wisc.edu/~aws/papers/popl22.pdf", "isFamilyFriendly": true, "displayUrl": "https://pages.cs.wisc.edu/~aws/papers/popl22.pdf", "snippet": "Neural networks and <b>approximation</b>. Over the past decade, <b>machine</b> <b>learning</b> with neural networks has revolutionized a vast array of tasks\u2014from computer vision [Krizhevsky et al.2012], to natural-language processing [Mikolov et al. 2013], to program-analysis tasks [Raychev et al. 2015], and beyond. While these advances are recent, it has been well-known that neural networks are a powerful class of models: The <b>universal</b> <b>approximation</b> <b>theorem</b> [Cybenko1989;Hornik et al. 1989] states that neural ...", "dateLastCrawled": "2022-02-01T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Universal</b> Differential Equations for Scientific <b>Machine</b> <b>Learning</b> ...", "url": "https://stan-haochen.github.io/slides/universal-differential-equations-for-scientific-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://stan-haochen.github.io/slides/<b>universal</b>-differential-equations-for-scientific...", "snippet": "<b>Universal</b> Differential Equations for Scientific <b>Machine</b> <b>Learning</b>. Chris Rackauckas Massachusetts Institute of Technology, Department of Mathematics University of Maryland, Baltimore, School of Pharmacy, Center for Translational Medicine. The major advances in <b>machine</b> <b>learning</b> were due to encoding more structure into the model. More structure = Faster and better fits from less data. Convolutional Neural Networks Are Structure Assumptions What is the structure of science? Ecology Example ...", "dateLastCrawled": "2022-01-11T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Advantages and Disadvantages of Neural Networks</b> | Baeldung on Computer ...", "url": "https://www.baeldung.com/cs/neural-net-advantages-disadvantages", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/neural-net-advantages-disadvantages", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b> and Its Limitation. The second advantage of neural networks relates to their capacity to approximate unknown functions. The foundational <b>theorem</b> for neural networks states that a sufficiently large neural network with one hidden layer can approximate any continuously differentiable functions. If we know that a problem can be modeled using a continuous function, it may then make sense to use a neural network to tackle it. A function is continuous if its inputs ...", "dateLastCrawled": "2022-02-02T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Neural Networks are Function Approximation</b> Algorithms", "url": "https://machinelearningmastery.com/neural-networks-are-function-approximators/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>neural-networks-are-function</b>-approximators", "snippet": "Supervised <b>learning</b> in <b>machine</b> <b>learning</b> can be described in terms of <b>function approximation</b>. Given a dataset comprised of inputs and outputs, we assume that there is an unknown underlying function that is consistent in mapping inputs to outputs in the target domain and resulted in the dataset. We then use supervised <b>learning</b> algorithms to approximate this function. Neural networks are an example", "dateLastCrawled": "2022-01-30T03:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Opening the black box of Deep Learning</b> - DyCon Blog", "url": "https://deustotech.github.io/DyCon-Blog/tutorial/wp99/P00013", "isFamilyFriendly": true, "displayUrl": "https://deustotech.github.io/DyCon-Blog/tutorial/wp99/P00013", "snippet": "The aim is then to open the <b>black box of Deep Learning</b>, ... The problem of <b>approximation</b> is somehow resolved by the so-called <b>Universal</b> <b>Approximation</b> Theorems (look, for example, at the results by Cybenko [11 ]). This results state that the functions generated by Neural Networks are dense in the space of continuous functions. However, in principle it is not clear what do we gain by increasing the number of layers (using deeper Neural Networks), that seem to give better results in practuce ...", "dateLastCrawled": "2022-01-25T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - Can neural networks approximate any function given ...", "url": "https://stackoverflow.com/questions/25609347/can-neural-networks-approximate-any-function-given-enough-hidden-neurons", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/25609347", "snippet": "The only rigorous <b>theorem</b> that exists about the ability of neural networks to approximate different kinds of functions is the <b>Universal</b> <b>Approximation</b> <b>Theorem</b>. The UAT states that any continuous function on a compact domain can be approximated by a <b>neural network</b> with only one hidden layer provided the activation functions used are BOUNDED, continuous and monotonically increasing.", "dateLastCrawled": "2022-01-16T04:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Opening the <b>black</b> <b>box</b> \u2013 Quantile neural networks for loss given default ...", "url": "https://www.sciencedirect.com/science/article/pii/S0378426621002855", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0378426621002855", "snippet": "It is well known that from the <b>universal</b> <b>approximation</b> <b>theorem</b>, following Cybenko (1989) or Hornik (1991) ... <b>Machine</b> <b>learning</b> models are prone to the conjecture that the researcher tries many different combinations and only reports the best, without ensuring that there is a broad superiority and to some extent robustness with respect to the architecture. We alleviate this problem by reporting positive Spearman\u2019s \u03c1, indicating that a good model in-sample is also a good model out-of-sample ...", "dateLastCrawled": "2022-01-28T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "terminology - Why are <b>Machine</b> <b>Learning</b> models called <b>black</b> boxes ...", "url": "https://datascience.stackexchange.com/questions/22335/why-are-machine-learning-models-called-black-boxes", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/22335", "snippet": "The <b>black box</b> thing has nothing to do with the level of expertise of the audience (as long as the audience is human), but with the explainability of the function modelled by the <b>machine</b> <b>learning</b> algorithm.. In logistic regression, there is a very simple relationship between inputs and outputs. You can sometimes understand why a certain sample was incorrectly catalogued (e.g. because the value of certain component of the input vector was too low).", "dateLastCrawled": "2022-01-21T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Artificial neural networks EQUIVALENT to linear ...", "url": "https://stats.stackexchange.com/questions/305619/artificial-neural-networks-equivalent-to-linear-regression-with-polynomial-featu", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/305619/artificial-neural-networks-equivalent...", "snippet": "Despite the success of neural networks (NNs), there is still a concern among many over their &quot;<b>black</b> <b>box</b>&quot; nature. Why do they work? Here we present a simple analytic argument that NNs are in fact essentially polynomial regression models. This view will have various implications for NNs, e.g. providing an explanation for why convergence problems arise in NNs, and it gives rough guidance on avoiding overfitting. In addition, we use this phenomenon to predict and confirm a multicollinearity ...", "dateLastCrawled": "2022-01-21T22:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Opening the <b>black</b> <b>box</b> \u2013 Quantile neural networks for loss given default ...", "url": "https://www.sciencedirect.com/science/article/pii/S0378426621002855", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0378426621002855", "snippet": "It is well known that from the <b>universal</b> <b>approximation</b> <b>theorem</b>, following Cybenko (1989) or Hornik (1991) ... <b>Machine</b> <b>learning</b> models are prone to the conjecture that the researcher tries many different combinations and only reports the best, without ensuring that there is a broad superiority and to some extent robustness with respect to the architecture. We alleviate this problem by reporting positive Spearman\u2019s \u03c1, indicating that a good model in-sample is also a good model out-of-sample ...", "dateLastCrawled": "2022-01-28T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>machine</b> <b>learning</b> - Artificial neural networks EQUIVALENT to linear ...", "url": "https://stats.stackexchange.com/questions/305619/artificial-neural-networks-equivalent-to-linear-regression-with-polynomial-featu", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/305619/artificial-neural-networks-equivalent...", "snippet": "Despite the success of neural networks (NNs), there is still a concern among many over their &quot;<b>black</b> <b>box</b>&quot; nature. Why do they work? Here we present a simple analytic argument that NNs are in fact essentially polynomial regression models. This view will have various implications for NNs, e.g. providing an explanation for why convergence problems arise in NNs, and it gives rough guidance on avoiding overfitting. In addition, we use this phenomenon to predict and confirm a multicollinearity ...", "dateLastCrawled": "2022-01-21T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Learning nonlinear operators via DeepONet</b> based on the <b>universal</b> ...", "url": "https://www.researchgate.net/publication/350158010_Learning_nonlinear_operators_via_DeepONet_based_on_the_universal_approximation_theorem_of_operators", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350158010_<b>Learning</b>_nonlinear_operators_via...", "snippet": "This <b>universal approximation theorem of operators</b> is suggestive of the structure and potential of deep neural networks (DNNs) in <b>learning</b> continuous operators or complex systems from streams of ...", "dateLastCrawled": "2022-02-03T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Opening the <b>black</b> <b>box</b> of <b>artificial intelligence</b> for clinical decision ...", "url": "https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0231166", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231166", "snippet": "According to the <b>universal</b> <b>approximation</b> <b>theorem</b>, an MLP with one hidden layer can approximate any function . Here ... it ranked the presence of diabetes similarly strong. A <b>similar</b> ranking for diabetes can also be observed in the logistic regression models. Although diabetes is known to be an important predictor for bad stroke outcome , a feature importance score that is at a <b>similar</b> level as age is unexpected. Another striking difference is the high relative importance given to sex by the ...", "dateLastCrawled": "2021-08-27T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Representation</b> Power of Neural Networks | by ASHISH RANA | Towards Data ...", "url": "https://towardsdatascience.com/representation-power-of-neural-networks-8e99a383586", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>representation</b>-power-of-neural-networks-8e99a383586", "snippet": "Now, above interpretation of <b>Universal</b> <b>Approximation</b> <b>Theorem</b> tells us that more number of towers we use for <b>approximation</b> better will be the <b>approximation</b> behavior. Hence, parameters are tuned in sigmoidal activations with an aim of creating such <b>approximation</b> towers. Theoretically there is no limit upon the accuracy of neural networks as per this interpretation.", "dateLastCrawled": "2022-02-03T07:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "terminology - Why are <b>Machine</b> <b>Learning</b> models called <b>black</b> boxes ...", "url": "https://datascience.stackexchange.com/questions/22335/why-are-machine-learning-models-called-black-boxes", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/22335", "snippet": "The <b>black box</b> thing has nothing to do with the level of expertise of the audience (as long as the audience is human), but with the explainability of the function modelled by the <b>machine</b> <b>learning</b> algorithm.. In logistic regression, there is a very simple relationship between inputs and outputs. You can sometimes understand why a certain sample was incorrectly catalogued (e.g. because the value of certain component of the input vector was too low).", "dateLastCrawled": "2022-01-21T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Opening the <b>black</b> <b>box</b> of neural networks: methods for interpreting ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6035992/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6035992", "snippet": "Due to their \u201c<b>black</b>-<b>box</b>\u201d nature, many <b>machine</b> <b>learning</b> methods suffer from the limitation of providing meaningful interpretations that can enhance understanding in subject-matter research. This article reviewed and demonstrated several methods to help clinicians understand neural network models of important patient parameters and outcomes. One or several of the illustrated methods will be suitable for essentially any type of clinical outcome model. Applicability of interpretation ...", "dateLastCrawled": "2022-01-28T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Neural Networks are Function Approximation</b> Algorithms", "url": "https://machinelearningmastery.com/neural-networks-are-function-approximators/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>neural-networks-are-function</b>-approximators", "snippet": "Supervised <b>learning</b> in <b>machine</b> <b>learning</b> can be described in terms of <b>function approximation</b>. Given a dataset comprised of inputs and outputs, we assume that there is an unknown underlying function that is consistent in mapping inputs to outputs in the target domain and resulted in the dataset. We then use supervised <b>learning</b> algorithms to approximate this function. Neural networks are an example", "dateLastCrawled": "2022-01-30T03:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Deep <b>learning</b> for <b>universal</b> linear embeddings of <b>nonlinear dynamics</b> ...", "url": "https://www.nature.com/articles/s41467-018-07210-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-018-07210-0", "snippet": "Importantly, the <b>universal</b> <b>approximation</b> <b>theorem</b> 16,17,18 guarantees that a NN with sufficiently many hidden units and a linear output layer is capable of representing any arbitrary function ...", "dateLastCrawled": "2022-01-31T22:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] Fourier transform vs NNs as function approximators", "url": "https://www.reddit.com/r/MachineLearning/comments/ryw53x/d_fourier_transform_vs_nns_as_function/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/ryw53x/d_fourier_transform_vs_nns_as...", "snippet": "What are the pros and cons vs neural networks? I believe every practitioner of <b>machine</b> <b>learning</b> should know the answer to those questions. [Edit: in a re-read I realized I forgot to mention why global and Gibbs phenomenon are related. It&#39;s because they are both the assumption of smoothness. If you assume a function is smooth, then every point influences everywhere else in the domain. You can think about this by looking at the convergence of Taylor series, where as you get more and more ...", "dateLastCrawled": "2022-02-01T11:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Neural Networks, the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> and Option ...", "url": "https://medium.com/analytics-vidhya/neural-networks-and-the-universal-approximation-theorem-e5c387982eed", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/neural-networks-and-the-<b>universal</b>-<b>approximation</b>...", "snippet": "Data Science or <b>Machine</b> <b>Learning</b> is basically the application of algorithms based on mathematics and statistics to real world problems. Now I know there is a lot more to <b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2021-07-20T00:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Deep learning</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Deep_learning", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Deep_learning</b>", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed ... <b>can</b> <b>be thought</b> of as a representational layer in a <b>deep learning</b> architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an ...", "dateLastCrawled": "2022-02-02T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 2, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Artificial neural network</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Artificial_neural_network</b>", "snippet": "This <b>can</b> <b>be thought</b> of as <b>learning</b> with a &quot;teacher&quot;, in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. Unsupervised <b>learning</b>. In unsupervised <b>learning</b>, input data is given along with the cost function, some function of the data and the network&#39;s output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a ...", "dateLastCrawled": "2022-02-07T09:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Solving a Rubik\u2019s Cube with <b>Reinforcement Learning</b> (Part 1) | by ...", "url": "https://medium.com/analytics-vidhya/solving-a-rubiks-cube-with-reinforcement-learning-part-1-4f0405dd07f2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/solving-a-<b>rubiks-cube</b>-with-<b>reinforcement-learning</b>...", "snippet": "Specifically, we will leverage the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> of neural networks, which states under certain conditions, that a neural network <b>can</b> be used to approximate any function.", "dateLastCrawled": "2022-02-02T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Artificial Intelligence, Machine Learning, and Surgical</b> Science ...", "url": "https://www.sciencedirect.com/science/article/pii/S0022480421000834", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0022480421000834", "snippet": "Thus, the function that is represented by the trained ANN (as per the <b>Universal</b> <b>Approximation</b> <b>Theorem</b>) would not reflect the impact of the new intervention on the system. Therefore, ML/AI cannot predict the effect of a novel drug; it <b>can</b> hypothesize a novel mechanism, but a potential drug that utilizes that mechanism would still need to go through all standard steps involved in developing and testing a new drug. ML/AI cannot predict \u201cwhat if\u201d scenarios, unless someone has already tried ...", "dateLastCrawled": "2021-12-26T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Universal</b> <b>Approximation</b> by Ridge Computational Models and Neural ...", "url": "https://www.researchgate.net/publication/228696065_Universal_Approximation_by_Ridge_Computational_Models_and_Neural_Networks_A_Survey", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228696065_<b>Universal</b>_<b>Approximation</b>_by_Ridge...", "snippet": "pointed out in [40] <b>can</b> be overcome, and the C-density property <b>can</b> be proved by means on <b>Theorem</b> 1 for networks with t wo hidden layers. (Also an estimate of the number of hidden units was ...", "dateLastCrawled": "2022-01-06T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Parameterized quantum circuits as <b>machine</b> <b>learning</b> models - IOPscience", "url": "https://iopscience.iop.org/article/10.1088/2058-9565/ab4eb5", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/2058-9565/ab4eb5", "snippet": "Similar to the <b>universal</b> <b>approximation</b> <b>theorem</b> in neural networks ... gradient descent. As another example, the objective function may be itself unknown and therefore should be treated as a <b>black</b>-<b>box</b>. In these cases, <b>circuit</b> <b>learning</b> <b>can</b> be carried out by gradient-free methods. A well-known method of this type is particle swarm optimization . Here the system is initialized with a number of random solutions called particles, each one moving through solution space with a certain velocity. The ...", "dateLastCrawled": "2021-12-01T23:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Artificial intelligence with deep <b>learning</b> in nuclear medicine and ...", "url": "https://ejnmmiphys.springeropen.com/articles/10.1186/s40658-021-00426-y", "isFamilyFriendly": true, "displayUrl": "https://ejnmmiphys.springeropen.com/articles/10.1186/s40658-021-00426-y", "snippet": "The versatility of deep <b>learning</b> lies behind the <b>universal</b> <b>approximation</b> <b>theorem</b>, stating that feedforward networks with at least one hidden layer, using a nonlinear activation and a linear output layer, <b>can</b> approximate any continuous function [16, 17]. That is, these deep <b>learning</b> models should be able to fit any sufficiently well-behaved training data to arbitrary precision by expanding the hidden layer size, thereby allowing the network to model increasingly complex functions. One of the ...", "dateLastCrawled": "2022-01-29T17:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] Unfair Comparison: Neural Networks vs Taylor Polynomials and ...", "url": "https://www.reddit.com/r/MachineLearning/comments/o01ox7/d_unfair_comparison_neural_networks_vs_taylor/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/o01ox7/d_unfair_comparison_neural...", "snippet": "This is a question I have always had : Early on in <b>machine</b> <b>learning</b> history, the perceptron algorithm was invented for classification tasks (the perceptron being the predecessor to the multi layered perceptron, i.e. the modern neural network). Later on, the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> was developed that showed a simple neural network is able to well-approximate any continuous function.", "dateLastCrawled": "2022-01-29T06:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Newton versus the <b>machine</b>: solving the chaotic three-body problem using ...", "url": "https://www.repository.cam.ac.uk/bitstream/handle/1810/315691/staa713.pdf?sequence=3", "isFamilyFriendly": true, "displayUrl": "https://www.repository.cam.ac.uk/bitstream/handle/1810/315691/staa713.pdf?sequence=3", "snippet": "5Leiden Observatory, Leiden University, PO <b>Box</b> 9513, NL-2300 RA Leiden, the Netherlands Accepted 2020 March 1. Received 2020 February 28; in original form 2019 October 16 ABSTRACT Since its formulation by Sir Isaac Newton, the problem of solving the equations of motion for three bodies under their own gravitational force has remained practically unsolved. Currently, the solution for a given initialization <b>can</b> only be found by performing laborious iterative calculations that have ...", "dateLastCrawled": "2022-01-25T10:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Advantages and Disadvantages of Neural Networks</b> | Baeldung on Computer ...", "url": "https://www.baeldung.com/cs/neural-net-advantages-disadvantages", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/neural-net-advantages-disadvantages", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b> and Its Limitation. The second advantage of neural networks relates to their capacity to approximate unknown functions. The foundational <b>theorem</b> for neural networks states that a sufficiently large neural network with one hidden layer <b>can</b> approximate any continuously differentiable functions. If we know that a problem <b>can</b> be modeled using a continuous function, it may then make sense to use a neural network to tackle it. A function is continuous if its inputs ...", "dateLastCrawled": "2022-02-02T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning</b> the solution operator of parametric partial differential ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8480920/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8480920", "snippet": "DeepONets present a specialized deep <b>learning</b> architecture that encapsulates the <b>universal</b> <b>approximation</b> <b>theorem</b> for operators . Here, we illustrate how DeepONets <b>can</b> be effectively applied to <b>learning</b> the solution operator of parametric PDEs. Here, the terminology \u201cparametric PDEs\u201d refers to the fact that some parameters of a given PDE system are allowed to vary over a certain range. These input parameters may include, but are not limited to, the shape of the physical domain, the ...", "dateLastCrawled": "2022-01-30T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>machine</b> <b>learning</b> - Artificial neural networks EQUIVALENT to linear ...", "url": "https://stats.stackexchange.com/questions/305619/artificial-neural-networks-equivalent-to-linear-regression-with-polynomial-featu", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/305619/artificial-neural-networks-equivalent...", "snippet": "I want to improve my understanding of neural networks and their benefits <b>compared</b> to other <b>machine</b> <b>learning</b> algorithms. My understanding is as below and my question is: <b>Can</b> you correct and supplement my understanding please? :) My understanding: (1) Artificial neural networks = A function, which predicts output values from input values.", "dateLastCrawled": "2022-01-21T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Opening the <b>black</b> <b>box</b> \u2013 Quantile neural networks for loss given default ...", "url": "https://www.sciencedirect.com/science/article/pii/S0378426621002855", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0378426621002855", "snippet": "It is well known that from the <b>universal</b> <b>approximation</b> <b>theorem</b>, following Cybenko (1989) or Hornik (1991) ... <b>Machine</b> <b>learning</b> models are prone to the conjecture that the researcher tries many different combinations and only reports the best, without ensuring that there is a broad superiority and to some extent robustness with respect to the architecture. We alleviate this problem by reporting positive Spearman\u2019s \u03c1, indicating that a good model in-sample is also a good model out-of-sample ...", "dateLastCrawled": "2022-01-28T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Opening the <b>black</b> <b>box</b> of <b>artificial intelligence</b> for clinical decision ...", "url": "https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0231166", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231166", "snippet": "According to the <b>universal</b> <b>approximation</b> <b>theorem</b>, an MLP with one hidden layer <b>can</b> approximate any function . Here the model prediction is given by: where is the (softmax) output layer activation, k is the predicted class and c is any of the possible classes for prediction. denotes the hidden layer activation function where M represents the number of nodes in the layer.", "dateLastCrawled": "2021-08-27T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Learning nonlinear operators via DeepONet</b> based on the <b>universal</b> ...", "url": "https://www.researchgate.net/publication/350158010_Learning_nonlinear_operators_via_DeepONet_based_on_the_universal_approximation_theorem_of_operators", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350158010_<b>Learning</b>_nonlinear_operators_via...", "snippet": "This <b>universal approximation theorem of operators</b> is suggestive of the structure and potential of deep neural networks (DNNs) in <b>learning</b> continuous operators or complex systems from streams of ...", "dateLastCrawled": "2022-02-03T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Artificial intelligence with deep <b>learning</b> in nuclear medicine and ...", "url": "https://ejnmmiphys.springeropen.com/articles/10.1186/s40658-021-00426-y", "isFamilyFriendly": true, "displayUrl": "https://ejnmmiphys.springeropen.com/articles/10.1186/s40658-021-00426-y", "snippet": "The versatility of deep <b>learning</b> lies behind the <b>universal</b> <b>approximation</b> <b>theorem</b>, stating that feedforward networks with at least one hidden layer, using a nonlinear activation and a linear output layer, <b>can</b> approximate any continuous function [16, 17]. That is, these deep <b>learning</b> models should be able to fit any sufficiently well-behaved training data to arbitrary precision by expanding the hidden layer size, thereby allowing the network to model increasingly complex functions. One of the ...", "dateLastCrawled": "2022-01-29T17:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Deep <b>learning</b> for <b>universal</b> linear embeddings of <b>nonlinear dynamics</b> ...", "url": "https://www.nature.com/articles/s41467-018-07210-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-018-07210-0", "snippet": "Importantly, the <b>universal</b> <b>approximation</b> <b>theorem</b> 16,17,18 guarantees that a NN with sufficiently many hidden units and a linear output layer is capable of representing any arbitrary function ...", "dateLastCrawled": "2022-01-31T22:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Opening the <b>black</b> <b>box</b> of neural networks: methods for interpreting ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6035992/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6035992", "snippet": "Due to their \u201c<b>black</b>-<b>box</b>\u201d nature, many <b>machine</b> <b>learning</b> methods suffer from the limitation of providing meaningful interpretations that <b>can</b> enhance understanding in subject-matter research. This article reviewed and demonstrated several methods to help clinicians understand neural network models of important patient parameters and outcomes. One or several of the illustrated methods will be suitable for essentially any type of clinical outcome model. Applicability of interpretation ...", "dateLastCrawled": "2022-01-28T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What does it mean by the statement, &#39;neural networks are <b>universal</b> ...", "url": "https://www.quora.com/What-does-it-mean-by-the-statement-neural-networks-are-universal-approximators-in-a-mathematical-intuitive-sense", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-does-it-mean-by-the-statement-neural-networks-are-<b>universal</b>...", "snippet": "Answer (1 of 4): Consider the set of all continuous functions which are defined on the unit hypercube (i.e. the unit square in two dimensions, the unit cube in three dimensions, etc.). Call this set C. Given two functions in C, it is possible to define a metric which calculates a notion of dist...", "dateLastCrawled": "2022-01-06T20:17:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Approximation Theorem</b>. The power of Neural Networks | by ...", "url": "https://medium.com/swlh/universal-approximation-theorem-d1a1a67c1b5b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>universal-approximation-theorem</b>-d1a1a67c1b5b", "snippet": "<b>Universal Approximation Theorem</b>, in its lose form, states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate any continuous function. Whoa ...", "dateLastCrawled": "2022-01-28T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal Approximation Theorem</b>, Neural Nets &amp; Lego Blocks | by ...", "url": "https://medium.com/analytics-vidhya/universal-approximation-theorem-neural-nets-lego-blocks-1f5a7d93542a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>universal-approximation-theorem</b>-neural-nets-lego...", "snippet": "In this post, we will look at the <b>Universal Approximation Theorem</b> \u2014 one of the fundamental theorems on which the entire concept of Deep <b>Learning</b> is based upon. We will make use of lego blocks ...", "dateLastCrawled": "2022-01-28T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> Complex Functions using <b>Universal</b> Approximate <b>Theorem</b> - Ai Nxt", "url": "https://ainxt.co.in/learning-complex-functions-using-universal-approximate-theorem/", "isFamilyFriendly": true, "displayUrl": "https://ainxt.co.in/<b>learning</b>-complex-functions-using-<b>universal</b>-approximate-<b>theorem</b>", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b>. No matter how complex our output logic is, we can use collection of neurons and form Dense Neural Network to approximate our function. This is known as \u201c<b>UNIVERSAL</b> <b>APPROXIMATION</b> <b>THEOREM</b>\u201c. Lets take an example of Two Dimensional data where y = f(x) i.e. y is some function of x. Now, we need to find that ...", "dateLastCrawled": "2022-01-21T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Universal Approximation Theorems</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336361517_Universal_Approximation_Theorems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336361517_<b>Universal_Approximation_Theorems</b>", "snippet": "In the <b>machine</b> <b>learning</b> literature, <b>universal</b> <b>approximation</b> refers to a model class\u2019 ability. to generically approximate any member of a large topological space whose elements are. functions, or ...", "dateLastCrawled": "2022-01-25T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Illustrative Proof of <b>Universal Approximation Theorem</b> | HackerNoon", "url": "https://hackernoon.com/illustrative-proof-of-universal-approximation-theorem-5845c02822f6", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/illustrative-proof-of-<b>universal-approximation-theorem</b>-5845c02822f6", "snippet": "We will talk about the <b>Universal approximation theorem</b> and we will also prove the <b>theorem</b> graphically. The most commonly used sigmoid function is the logistic function, which has a characteristic of an \u201cS\u201d shaped curve. In real life, we deal with complex functions where the relationship between input and output might be complex. To solve this problem, let&#39;s take an <b>analogy</b> of building a house. The way we are going to create complex functions is that we will combine the sigmoids neurons ...", "dateLastCrawled": "2022-02-01T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "ne.neural evol - <b>Universal Approximation Theorem</b> \u2014 Neural Networks ...", "url": "https://cstheory.stackexchange.com/questions/17545/universal-approximation-theorem-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/17545", "snippet": "<b>Universal approximation theorem</b> states that &quot;the standard multilayer feed-forward network with a single hidden layer, ... There is an advanced result, key to <b>machine</b> <b>learning</b>, known as Kolmogorov&#39;s <b>theorem</b> [1]; I have never seen an intuitive sketch of why it works. This may have to do with the different cultures that approach it. The applied <b>learning</b> crowd regards Kolmogorov&#39;s <b>theorem</b> as an existence <b>theorem</b> that merely indicates that NNs may exist, so at least the structure is not overly ...", "dateLastCrawled": "2022-02-03T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "neural networks - <b>Universal Approximation Theorem and high dimension</b> ...", "url": "https://stats.stackexchange.com/questions/298622/universal-approximation-theorem-and-high-dimension-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/298622/<b>universal</b>-<b>approximation</b>-<b>theorem</b>-and...", "snippet": "Cross Validated is a question and answer site for people interested in statistics, <b>machine</b> <b>learning</b>, data analysis, data mining, and data visualization. It only takes a minute to sign up. Sign up to join this community", "dateLastCrawled": "2022-01-17T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Neural Networks and Learning Machines</b> - etsmtl.ca", "url": "https://cours.etsmtl.ca/sys843/REFS/Books/ebook_Haykin09.pdf", "isFamilyFriendly": true, "displayUrl": "https://cours.etsmtl.ca/sys843/REFS/Books/ebook_Haykin09.pdf", "snippet": "15.3 <b>Universal</b> <b>Approximation</b> <b>Theorem</b> 797 15.4 Controllability and Observability 799 15.5 Computational Power of Recurrent Networks 804 15.6 <b>Learning</b> Algorithms 806 15.7 Back Propagation Through Time 808 15.8 Real-Time Recurrent <b>Learning</b> 812 15.9 Vanishing Gradients in Recurrent Networks 818", "dateLastCrawled": "2022-01-31T06:23:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(universal approximation theorem)  is like +(machine learning black box)", "+(universal approximation theorem) is similar to +(machine learning black box)", "+(universal approximation theorem) can be thought of as +(machine learning black box)", "+(universal approximation theorem) can be compared to +(machine learning black box)", "machine learning +(universal approximation theorem AND analogy)", "machine learning +(\"universal approximation theorem is like\")", "machine learning +(\"universal approximation theorem is similar\")", "machine learning +(\"just as universal approximation theorem\")", "machine learning +(\"universal approximation theorem can be thought of as\")", "machine learning +(\"universal approximation theorem can be compared to\")"]}