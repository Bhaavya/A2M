{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Counterfactual</b> Inference v3 - NeurIPS", "url": "https://media.neurips.cc/Conferences/NIPS2018/Slides/Counterfactual_Inference.pdf", "isFamilyFriendly": true, "displayUrl": "https://media.neurips.cc/Conferences/NIPS2018/Slides/<b>Counterfactual</b>_Inference.pdf", "snippet": "<b>Fairness</b>/Non\u2010discrimination \u201cHuman\u2010<b>like</b>\u201d AI Reasonable decisions in never\u2010 experienced situations CAUSAL INFERENCE FRAMEWORK Goal: learn model of how the world works Impact of interventions can be context\u2010specific Model maps contexts and interventions to outcomes Formal language to separate out correlates and causes Ideal causal model is by definition stable, interpretable Transferability: straightforward for new context dist\u2019n <b>Fairness</b>: Many aspects of discrimination relate to ...", "dateLastCrawled": "2022-02-02T15:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Managing <b>bias and unfairness</b> in data for decision support: a survey of ...", "url": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "snippet": "Causal <b>Fairness</b>. A last set of metrics relies on causal relations between records and predictions and requires the establishment of a causal graph . For instance, <b>counterfactual</b> <b>fairness</b> is verified when the predictions do not depend on a descendent of the protected attribute node in the graph. In our example, using such metrics would require ...", "dateLastCrawled": "2022-02-01T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "On the Current and Emerging Challenges of Developing Fair and Ethical ...", "url": "https://deepai.org/publication/on-the-current-and-emerging-challenges-of-developing-fair-and-ethical-ai-solutions-in-financial-services", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-the-current-and-emerging-challenges-of-developing...", "snippet": "Counterintuitive Effects: In credit decisions, some scholars emphasize <b>counterfactual</b> <b>fairness</b> ... as well as reducing the complexity and subjectivity of the ethical criteria. (i) Fine-<b>tuning</b> Existing Regulations: According to the industry reports, most financial regulators are in the process of reviewing the existing financial regulations for potential adjustments and expansions to address the growing use of AI (Deloitte; OCC). In 2019, SEC announced rule amendments to modernize the risk ...", "dateLastCrawled": "2022-01-21T14:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Frontiers | Ethical and Statistical Considerations in <b>Models of Moral</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/frobt.2019.00039/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frobt.2019.00039", "snippet": "In their paper, \u201c<b>Counterfactual</b> <b>Fairness</b>,\u201d Kusner et al. describe protected attributes as variables that must not be discriminated against, relative to a particular system. Kusner et al. present <b>counterfactual</b> <b>fairness</b> as the idea that a decision is only fair if it is fair in both the real world and a <b>counterfactual</b> world, where the target individual belongs to an alternative demographic group.", "dateLastCrawled": "2022-01-16T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Addressing <b>Fairness</b>, Bias, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "While the eventual optimal <b>tuning</b> of an algorithm can thus depend on many factors, including the local system of laws and community values, the overall goal of this paper is to introduce and define the notions of bias, <b>fairness</b>, and appropriate use, as it pertains to machine learning in global health, and also to illustrate how a given machine learning model can be analyzed to identify and quantify issues of bias and <b>fairness</b>. With this goal in mind, this paper is intended for the audience ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "DeDUCE: Generating <b>Counterfactual</b> Explanations At Scale", "url": "https://xai4debugging.github.io/files/papers/deduce_generating_counterfactu.pdf", "isFamilyFriendly": true, "displayUrl": "https://xai4debugging.github.io/files/papers/deduce_generating_counterfactu.pdf", "snippet": "<b>Counterfactual</b> explanations can be useful for debugging: They can be used to answer questions <b>like</b> \u2018Why did the self-driving <b>car</b> misidentify the \ufb01re hydrant as a stop sign?\u2019 [8]. More precisely, they can answer the questions \u2019What would the image need to look <b>like</b> in order to be classi\ufb01ed as a \ufb01re hydrant?\u2019 and \u2019What changes would need to happen in other \ufb01re hydrant (stop sign) images in order to be classi\ufb01ed as a stop sign (\ufb01re hydrant)?\u2019. Depending on the model and ...", "dateLastCrawled": "2021-12-17T19:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Perception of <b>fairness</b> in algorithmic decisions: Future developers ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "snippet": "<b>Fairness</b>, accountability, transparency, and ethics (FATE) in algorithmic systems is gaining a lot of attention lately. With the continuous advancement of machine learning and artificial intelligence, research and tech companies are coming across incidents where algorithmic systems are making non-objective decisions that may reproduce and/or amplify social stereotypes and inequalities.", "dateLastCrawled": "2022-01-30T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Trustworthy AI: A Computational Perspective \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2107.06641/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2107.06641", "snippet": "Studies on <b>fairness</b> in different fields have confirmed the existence of the trade-off between <b>fairness</b> and performance of an algorithm (Corbett-Davies et al., 2017; Prost et al., 2019; Berk et al., 2021). The improvement of the <b>fairness</b> of an algorithm typically comes at the cost of performance degradation. Since both <b>fairness</b> and performance are indispensable, extensive research is needed to help people better understand an algorithm\u2019s trade-off mechanism between them, so that ...", "dateLastCrawled": "2022-01-27T23:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Counterfactual</b> state explanations for reinforcement learning agents via ...", "url": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "snippet": "Using the MDP framework, we introduce the concept of a <b>counterfactual</b> state as a <b>counterfactual</b> explanation. 1 More precisely, for an agent in state s performing action a according to its learned policy, a <b>counterfactual</b> state s \u2032 is a state that involves a minimal change to s such that the agent&#39;s policy chooses action a \u2032 instead of a.For example, a <b>counterfactual</b> state can be seen in Fig. 1 for the video game Space Invaders [Brockman et al. ].In this game, an agent exchanges fire with ...", "dateLastCrawled": "2022-01-29T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Tuning</b> EU equality law <b>to algorithmic discrimination: Three pathways to</b> ...", "url": "https://journals.sagepub.com/doi/full/10.1177/1023263X20982173", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/1023263X20982173", "snippet": "<b>Tuning</b> EU equality law <b>to algorithmic discrimination: Three pathways to resilience</b> Show all authors. Rapha\u00eble Xenidis * Rapha\u00eble Xenidis . Lecturer in European Union Law at Edinburgh University, School of Law, Old College, South Bridge, Edinburgh EH8 9YL, United Kingdom and Marie Curie Fellow at iCourts, Copenhagen University, Faculty of Law, Karen Blixens Plads 16, 2300 Copenhagen, Denmark. View ORCID profile See all articles by this author. Search Google Scholar for this author. First ...", "dateLastCrawled": "2022-01-29T21:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Machine learning glossary - DataTime", "url": "https://www.dtalg.com/article-1/", "isFamilyFriendly": true, "displayUrl": "https://www.dtalg.com/article-1", "snippet": "<b>counterfactual</b> <b>fairness</b>. #<b>fairness</b>. A <b>fairness</b> metric that checks whether a classifier produces the same result for one individual as it does for another individual who is identical to the first, except with respect to one or more sensitive attributes. Evaluating a classifier for <b>counterfactual</b> <b>fairness</b> is one method for surfacing potential sources of bias in a model. See \u201cWhen Worlds Collide: Integrating Different <b>Counterfactual</b> Assumptions in <b>Fairness</b>\u201d for a more detailed discussion of ...", "dateLastCrawled": "2022-01-25T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "DeDUCE: Generating <b>Counterfactual</b> Explanations At Scale", "url": "https://xai4debugging.github.io/files/papers/deduce_generating_counterfactu.pdf", "isFamilyFriendly": true, "displayUrl": "https://xai4debugging.github.io/files/papers/deduce_generating_counterfactu.pdf", "snippet": "inputs that are <b>similar</b> but classi\ufb01ed differently; this is the realm of <b>counterfactual</b> explanations. For de-bugging, we usually want to generate realistic counterfactuals that are just across the model\u2019s classi\ufb01cation decision boundary (\ufb01g. 1). We want a <b>counterfactual</b> that stays as close to the original input as possible but results in a correct classi\ufb01cation, in order to understand the system\u2019s decision making. At the same time, the <b>counterfactual</b> should stay on the manifold of ...", "dateLastCrawled": "2021-12-17T19:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Addressing <b>Fairness</b>, Bias, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "While the eventual optimal <b>tuning</b> of an algorithm can thus depend on many factors, including the local system of laws and community values, the overall goal of this paper is to introduce and define the notions of bias, <b>fairness</b>, and appropriate use, as it pertains to machine learning in global health, and also to illustrate how a given machine learning model can be analyzed to identify and quantify issues of bias and <b>fairness</b>. With this goal in mind, this paper is intended for the audience ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Managing <b>bias and unfairness</b> in data for decision support: a survey of ...", "url": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "snippet": "Essentially, group <b>fairness</b> reflects averages over sets of individuals\u2014if the averages across groups are <b>similar</b>, then the model is considered fair\u2014while individual <b>fairness</b> is interested in each of the individuals and how they are treated in comparison with all other individuals\u2014while a group average might seem high, two individuals within the same group might receive disparate treatment, which in average look fair. In our example, an unfairness measure such as disparate impact could ...", "dateLastCrawled": "2022-02-01T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Model Fairness &amp; Transparency</b>. A Project on Detecting, Understanding ...", "url": "https://medium.com/sfu-cspmp/model-transparency-fairness-552a747b444", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/sfu-cspmp/model-transparency-<b>fairness</b>-552a747b444", "snippet": "1. Introduction. B ias is a prejudice in favor or against a person, group, or a thing that is considered to be unfair. At present times, Machine Learning and Artificial Intelligence play a major ...", "dateLastCrawled": "2022-01-19T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Counterfactual</b> state explanations for reinforcement learning agents via ...", "url": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "snippet": "Using the MDP framework, we introduce the concept of a <b>counterfactual</b> state as a <b>counterfactual</b> explanation. 1 More precisely, for an agent in state s performing action a according to its learned policy, a <b>counterfactual</b> state s \u2032 is a state that involves a minimal change to s such that the agent&#39;s policy chooses action a \u2032 instead of a.For example, a <b>counterfactual</b> state can be seen in Fig. 1 for the video game Space Invaders [Brockman et al. ].In this game, an agent exchanges fire with ...", "dateLastCrawled": "2022-01-29T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Trustworthy AI: A Computational Perspective \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2107.06641/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2107.06641", "snippet": "A model satisfies individual <b>fairness</b> if it gives <b>similar</b> predictions to <b>similar</b> individuals (Dwork et al., 2012; Kusner et al., 2017). Formally, if individuals i and j are <b>similar</b> under a certain metric \u03b4, the difference between the predictions given by an algorithm M on them should be small enough: | f M (i) \u2212 f M (j) | &lt; \u03f5, where f M (\u22c5) is the predictive function of algorithm M that maps an individual to an outcome, and \u03f5 is a small constant. 4.2. Methods. In this subsection, we ...", "dateLastCrawled": "2022-01-27T23:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Framework and Benchmarking Study for <b>Counterfactual</b> Generating ...", "url": "https://www.mdpi.com/2076-3417/11/16/7274/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2076-3417/11/16/7274/htm", "snippet": "<b>Counterfactual</b> explanations are viewed as an effective way to explain machine learning predictions. This interest is reflected by a relatively young literature with already dozens of algorithms aiming to generate such explanations. These algorithms are focused on finding how features can be modified to change the output classification. However, this rather general objective can be achieved in different ways, which brings about the need for a methodology to test and benchmark these algorithms.", "dateLastCrawled": "2021-12-01T09:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Fair Is Better than Sensational: Man Is</b> to Doctor as Woman ... - MIT Press", "url": "https://direct.mit.edu/coli/article/46/2/487/93368/Fair-Is-Better-than-Sensational-Man-Is-to-Doctor", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/46/2/487/93368/<b>Fair-Is-Better-than-Sensational-Man</b>...", "snippet": "Abstract. Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy ...", "dateLastCrawled": "2022-02-01T21:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Tuning</b> EU equality law <b>to algorithmic discrimination: Three pathways to</b> ...", "url": "https://journals.sagepub.com/doi/full/10.1177/1023263X20982173", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/1023263X20982173", "snippet": "<b>Tuning</b> EU equality law <b>to algorithmic discrimination: Three pathways to resilience</b> Show all authors. Rapha\u00eble Xenidis * Rapha\u00eble Xenidis . Lecturer in European Union Law at Edinburgh University, School of Law, Old College, South Bridge, Edinburgh EH8 9YL, United Kingdom and Marie Curie Fellow at iCourts, Copenhagen University, Faculty of Law, Karen Blixens Plads 16, 2300 Copenhagen, Denmark. View ORCID profile See all articles by this author. Search Google Scholar for this author. First ...", "dateLastCrawled": "2022-01-29T21:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Addressing <b>Fairness</b>, Bias, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "The following sections provide an overview of different ways <b>fairness</b> <b>can</b> be defined and measured, to enable the proper <b>tuning</b> of a machine learning algorithm. Individual vs. Group <b>Fairness</b> . Given the dictionary definition of <b>fairness</b> (impartial and just treatment), we <b>can</b> consider <b>fairness</b> at the level of an individual or a group. We <b>can</b> ask whether a computer algorithm disproportionately helps or harms specific individuals or specific groups of people. Ideally, an algorithm would be ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Frontiers | Ethical and Statistical Considerations in <b>Models of Moral</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/frobt.2019.00039/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frobt.2019.00039", "snippet": "In their paper, \u201c<b>Counterfactual</b> <b>Fairness</b>,\u201d Kusner et al. describe protected attributes as variables that must not be discriminated against, relative to a particular system. Kusner et al. present <b>counterfactual</b> <b>fairness</b> as the idea that a decision is only fair if it is fair in both the real world and a <b>counterfactual</b> world, where the target individual belongs to an alternative demographic group.", "dateLastCrawled": "2022-01-16T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Addressing Fairness, Bias, and Appropriate</b> Use of Artificial ...", "url": "https://europepmc.org/article/PMC/PMC8107824", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8107824", "snippet": "The following sections provide an overview of different ways <b>fairness</b> <b>can</b> be defined and measured, to enable the proper <b>tuning</b> of a machine learning algorithm. Individual vs. Group <b>Fairness</b> . Given the dictionary definition of <b>fairness</b> (impartial and just treatment), we <b>can</b> consider <b>fairness</b> at the level of an individual or a group. We <b>can</b> ask whether a computer algorithm disproportionately helps or harms specific individuals or specific groups of people. Ideally, an algorithm would be ...", "dateLastCrawled": "2021-06-01T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Perception of <b>fairness</b> in algorithmic decisions: Future developers ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "snippet": "Students&#39; perception of <b>fairness</b> changed after an hour-long lecture and discussion on algorithmic <b>fairness</b>. 26 Education level appears to be an important predictor for comprehension of <b>fairness</b>, and negative sentiment is associated with greater comprehension of demographic parity. 43 However, in order for algorithms to become more fair, developers must be educated and aware of the possible biases and discrimination that might occur as a result of the algorithms they develop. They must be ...", "dateLastCrawled": "2022-01-30T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Perception of <b>fairness</b> in algorithmic decisions: Future ...", "url": "https://www.researchgate.net/publication/355892219_Perception_of_fairness_in_algorithmic_decisions_Future_developers'_perspective", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355892219_Perception_of_<b>fairness</b>_in...", "snippet": "Furthermore, <b>fairness</b> is most likely to be defined as the use of \u201cobjective factors,\u201d and participants identify the use of \u201csensitive attributes\u201d as the most likely cause of unfairness.", "dateLastCrawled": "2021-12-04T10:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Counterfactual</b> state explanations for reinforcement learning agents via ...", "url": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "snippet": "Using the MDP framework, we introduce the concept of a <b>counterfactual</b> state as a <b>counterfactual</b> explanation. 1 More precisely, for an agent in state s performing action a according to its learned policy, a <b>counterfactual</b> state s \u2032 is a state that involves a minimal change to s such that the agent&#39;s policy chooses action a \u2032 instead of a.For example, a <b>counterfactual</b> state <b>can</b> be seen in Fig. 1 for the video game Space Invaders [Brockman et al. ].In this game, an agent exchanges fire with ...", "dateLastCrawled": "2022-01-29T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "CausaLM: Causal Model Explanation Through <b>Counterfactual</b> Language ...", "url": "https://deepai.org/publication/causalm-causal-model-explanation-through-counterfactual-language-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/causalm-causal-model-explanation-through-<b>counterfactual</b>...", "snippet": "As their method <b>can</b> only be performed by manually creating <b>counterfactual</b> examples such as this query, it is exposed to all the problems involving <b>counterfactual</b> text generation (see Section 5.1). Also, they do not compare model predictions on examples and their counterfactuals, and only measure the difference between the two queries, neither of which are the original text. In contrast, we propose a generalized method for providing a causal explanation for any textual concept, and present ...", "dateLastCrawled": "2022-01-31T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Perception of <b>fairness</b> in algorithmic decisions: Future developers ...", "url": "https://www.cell.com/patterns/fulltext/S2666-3899(21)00247-6", "isFamilyFriendly": true, "displayUrl": "https://www.cell.com/patterns/fulltext/S2666-3899(21)00247-6", "snippet": "When asked whether they would consider <b>fairness</b> in their system (Q5), most of the participants (66.7%) responded affirmatively (4\u20135), 19.2% indicated that they would not consider <b>fairness</b> (1\u20132), and 14.1% seemed undecided (3). In line with our instructions\u2014to answer the following question only if the participant selected 3\u20135\u2014only 81 out of the 99 participants answered the question about choosing a part of the system to focus on (Q6), in order to promote <b>fairness</b>. As a reminder, the ...", "dateLastCrawled": "2021-11-22T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Tuning</b> EU equality law <b>to algorithmic discrimination: Three pathways to</b> ...", "url": "https://journals.sagepub.com/doi/full/10.1177/1023263X20982173", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/1023263X20982173", "snippet": "An additional source of inspiration to tackle this issue <b>can</b> be found in the concept of \u2018nodes of discrimination fields\u2019 developed by Schiek, who offers a re-conceptualization of EU non-discrimination law around the nodes of \u2018race\u2019, \u2018disability\u2019 and \u2018gender\u2019, arguing that a comprehensive reading of these nodes would lead to recognising that discrimination <b>can</b> happen in their \u2018centre\u2019 or in their \u2018orbit\u2019. 80 This concept, she argues, \u2018provides an opportunity to ...", "dateLastCrawled": "2022-01-29T21:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "AI + safety - DNV GL", "url": "https://ai-and-safety.dnvgl.com/", "isFamilyFriendly": true, "displayUrl": "https://ai-and-safety.dnvgl.com", "snippet": "As artificial intelligence (AI) systems begin to control safety-critical infrastructure across a growing number of industries, the need to ensure safe use of AI in systems has become a top priority. DNV GL has published a position paper to provide guidance on responsible use of AI, and why causal- and data-driven models must be combined.", "dateLastCrawled": "2022-02-02T18:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Addressing <b>Fairness</b>, Bias, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "The following sections provide an overview of different ways <b>fairness</b> <b>can</b> be defined and measured, to enable the proper <b>tuning</b> of a machine learning algorithm. Individual vs. Group <b>Fairness</b> . Given the dictionary definition of <b>fairness</b> (impartial and just treatment), we <b>can</b> consider <b>fairness</b> at the level of an individual or a group. We <b>can</b> ask whether a computer algorithm disproportionately helps or harms specific individuals or specific groups of people. Ideally, an algorithm would be ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Frontiers | Ethical and Statistical Considerations in <b>Models of Moral</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/frobt.2019.00039/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frobt.2019.00039", "snippet": "In their paper, \u201c<b>Counterfactual</b> <b>Fairness</b>,\u201d Kusner et al. describe protected attributes as variables that must not be discriminated against, relative to a particular system. Kusner et al. present <b>counterfactual</b> <b>fairness</b> as the idea that a decision is only fair if it is fair in both the real world and a <b>counterfactual</b> world, where the target individual belongs to an alternative demographic group.", "dateLastCrawled": "2022-01-16T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Model Fairness &amp; Transparency</b>. A Project on Detecting, Understanding ...", "url": "https://medium.com/sfu-cspmp/model-transparency-fairness-552a747b444", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/sfu-cspmp/model-transparency-<b>fairness</b>-552a747b444", "snippet": "We also learnt about various <b>fairness</b> metrics, the concepts behind them and how they <b>can</b> be used to test a model for bias. Technology-wise our groups achievement was to come up with two new tools ...", "dateLastCrawled": "2022-01-19T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Perception of <b>fairness</b> in algorithmic decisions: Future developers ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "snippet": "In the third scenario, participants show a preference for the proportional (\u201cratio\u201d) decision, as <b>compared</b> with the other two decisions (giving all the money to one candidate, splitting the money equally). Our results suggest that the level of education <b>can</b> change participants&#39; understating of the process, their agreement with the decision, and the appropriateness of certain factors used by the algorithm. Qualitative analysis shows that future developers, in order to judge the <b>fairness</b> ...", "dateLastCrawled": "2022-01-30T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Managing <b>bias and unfairness</b> in data for decision support: a survey of ...", "url": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00671-8", "snippet": "As there are a plethora of different <b>fairness</b> definitions, choosing the correct metric and setting the correct parameters is far from trivial due to the abstraction gap between application (<b>fairness</b> as an abstract norm) and constraint model (<b>fairness</b> as a mathematical object). Therefore, we envision a guidance component that could come in form of wizards, or an IDE that <b>can</b> provide suggestions based on data profiling of potential biases and on existing regulations.", "dateLastCrawled": "2022-02-01T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Trustworthy AI: A Computational Perspective \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2107.06641/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2107.06641", "snippet": "Furthermore, according to the object of the study, <b>fairness</b> <b>can</b> be defined as group <b>fairness</b> and individual <b>fairness</b>. Group <b>Fairness</b>. Group <b>fairness</b> requires that two groups of people with different sensitive attributes receive comparable treatments and outcomes statistically. Based on this principle, various definitions have been proposed, such as Equal Opportunity (Hardt et al., 2016a), which requires people from two groups to be equally likely to get a positive outcome when they indeed ...", "dateLastCrawled": "2022-01-27T23:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Counterfactual</b> state explanations for reinforcement learning agents via ...", "url": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "snippet": "Using the MDP framework, we introduce the concept of a <b>counterfactual</b> state as a <b>counterfactual</b> explanation. 1 More precisely, for an agent in state s performing action a according to its learned policy, a <b>counterfactual</b> state s \u2032 is a state that involves a minimal change to s such that the agent&#39;s policy chooses action a \u2032 instead of a.For example, a <b>counterfactual</b> state <b>can</b> be seen in Fig. 1 for the video game Space Invaders [Brockman et al. ].In this game, an agent exchanges fire with ...", "dateLastCrawled": "2022-01-29T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Framework and Benchmarking Study for <b>Counterfactual</b> Generating ...", "url": "https://www.mdpi.com/2076-3417/11/16/7274/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2076-3417/11/16/7274/htm", "snippet": "<b>Counterfactual</b> explanations are viewed as an effective way to explain machine learning predictions. This interest is reflected by a relatively young literature with already dozens of algorithms aiming to generate such explanations. These algorithms are focused on finding how features <b>can</b> be modified to change the output classification. However, this rather general objective <b>can</b> be achieved in different ways, which brings about the need for a methodology to test and benchmark these algorithms.", "dateLastCrawled": "2021-12-01T09:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Finding and Fixing Spurious Patterns with Explanations | DeepAI", "url": "https://deepai.org/publication/finding-and-fixing-spurious-patterns-with-explanations", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/finding-and-fixing-spurious-patterns-with-explanations", "snippet": "In order to verify that the initial model relies on a SP and quantify the impact of mitigation methods, we measure gaps in accuracy between images with and without the spurious object (e.g., there is a 45.4% accuracy drop between images of tennis rackets with and without people). Intuitively, the more a model relies on a SP, the larger these gaps will be and the less robust the model is to distribution shift.", "dateLastCrawled": "2021-12-06T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Tuning</b> EU equality law <b>to algorithmic discrimination: Three pathways to</b> ...", "url": "https://journals.sagepub.com/doi/full/10.1177/1023263X20982173", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/1023263X20982173", "snippet": "The Court <b>compared</b> the applicant to commissioning fathers and to non-disabled workers in two separate instances, see Case C-363/12 Z. v A Government department and The Board of management of a community school, para. 52 and 77\u201381. 23 This formula <b>can</b> be traced back to K. Crenshaw, The University of Chicago Legal Forum (1989), p. 140. 24", "dateLastCrawled": "2022-01-29T21:37:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Responsible AI practices \u2013 Google AI", "url": "https://ai.google/responsibilities/responsible-ai-practices/?category=fairness", "isFamilyFriendly": true, "displayUrl": "https://ai.google/responsibilities/responsible-ai-practices/?category=<b>fairness</b>", "snippet": "A case-study on the application of <b>fairness</b> in <b>machine</b> <b>learning</b> research to a production classification system, and new insights in how to measure and address algorithmic <b>fairness</b> issues. Research paper <b>Counterfactual</b> <b>fairness</b> in text classification through robustness Provides and compares multiple approaches for addressing <b>counterfactual</b> <b>fairness</b> issues in text models. Research paper Model Cards for Model Reporting Proposes a framework to encourage transparent model reporting. Research ...", "dateLastCrawled": "2022-02-02T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fairness in Machine Learning: Lessons from Political Philosophy</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-lessons-from-political-philosophy", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-lessons-from-political</b>...", "snippet": "This discussion suggests that \u2018<b>fairness</b>\u2019 as used in the fair <b>machine</b> <b>learning</b> community is best understood as a placeholder term for a variety of normative egalitarian considerations. Notably, while egalitarianism is a widely held principle, exactly what it requires is the subject of much debate. I provide an overview of some of this debate and finish with implications for the incorporation of \u2018<b>fairness</b>\u2019 into algorithmic decision-making systems.", "dateLastCrawled": "2021-12-26T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Counterfactual</b> Explanation of <b>Machine</b> <b>Learning</b> Survival Models - IOS Press", "url": "https://content.iospress.com/articles/informatica/infor468", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/informatica/infor468", "snippet": "A method for <b>counterfactual</b> explanation of <b>machine</b> <b>learning</b> survival models is proposed. One of the difficulties of solving the <b>counterfactual</b> explanation problem is that the classes of examples are implicitly defined through outcomes of a <b>machine</b> <b>learning</b> survival model in the form of survival functions. A condition that establishes the difference between survival functions of the original example and the <b>counterfactual</b> is introduced. This condition is based on using a distance between mean ...", "dateLastCrawled": "2022-01-15T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "A lot of what is discussed in the <b>machine</b> <b>learning</b> literature touches on <b>fairness</b> (or rather equivalence in certain outcomes) between groups, yet this narrowly constricts <b>fairness</b> to the notion of equality. Of course, we should think about <b>fairness</b> in the context of prejudiced groups, but we should also ask whether it is fair to an individual. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[PDF] A Survey on Bias and <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | Semantic Scholar", "url": "https://www.semanticscholar.org/paper/A-Survey-on-Bias-and-Fairness-in-Machine-Learning-Mehrabi-Morstatter/0090023afc66cd2741568599057f4e82b566137c", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/A-Survey-on-Bias-and-<b>Fairness</b>-in-<b>Machine</b>...", "snippet": "This survey investigated different real-world applications that have shown biases in various ways, and created a taxonomy for <b>fairness</b> definitions that <b>machine</b> <b>learning</b> researchers have defined to avoid the existing bias in AI systems. With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for <b>fairness</b> has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive ...", "dateLastCrawled": "2022-01-29T11:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "dimensions of <b>machine</b> Causality and the normative", "url": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "isFamilyFriendly": true, "displayUrl": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "snippet": "dimensions of <b>machine</b> <b>learning</b> Joshua Loftus (LSE Statistics) High level intro Causality, what is it good for? Causal <b>fairness</b> In prediction and ranking tasks, and with intersectionality Designing interventions Optimal fair policies, causal interference Concluding thoughts 2 / 27. Tech solutionism, using ML/AI in every situation 3 / 27. Imagination Albert Einstein: Imagination is more important than knowledge. For knowledge is limited, whereas imagination [...] stimulat[es] progress, giving ...", "dateLastCrawled": "2022-01-11T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Stable <b>Learning</b> and its Causal Implication", "url": "http://pengcui.thumedialab.com/papers/Stable%20Learning-tutorial-valse2021.pdf", "isFamilyFriendly": true, "displayUrl": "pengcui.thumedialab.com/papers/Stable <b>Learning</b>-tutorial-valse2021.pdf", "snippet": "Application --- <b>counterfactual</b> visual explanations ... Goyal, Yash, et al. &quot;<b>Counterfactual</b> visual explanations.&quot; International Conference on <b>Machine</b> <b>Learning</b>. PMLR, 2019. Explainability with Causality Application --- causal recommendation 17 He et al. \u201dCollaborative Causal Filtering for Out-of-Distribution Recommendation.&quot; Under review. Caual structure among user features and item features Example . Explainability and OOD \u2022Explainability would be a side product when pursuing OOD with ...", "dateLastCrawled": "2022-01-28T03:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Mitigating Political Bias in Language Models Through Reinforced Calibration", "url": "https://www.cs.dartmouth.edu/~rbliu/aaai_copy.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.dartmouth.edu/~rbliu/aaai_copy.pdf", "snippet": "\ufb01rst proposed <b>counterfactual</b> <b>fairness</b>, which treats data samples equally in actual and <b>counterfactual</b> demographic groups. Zhao et al. mitigated gender bias by augmenting original data with gender-swapping and training a unbiased system on the union of two datasets. Other augmentation techniques have reduced gender bias in hate speech detec-tion (Park, Shin, and Fung 2018; Liu et al. 2020), knowledge graph building (Mitchell et al. 2019) and <b>machine</b> transla-tion (Stanovsky, Smith, and ...", "dateLastCrawled": "2022-01-28T20:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Counterfactual Fairness \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1703.06856/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.06856", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however, previous decisions have been made that are unfairly biased against certain subpopulations (e.g., those of a particular race, gender, or sexual orientation). Because this past data is often biased, <b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating discriminatory practices (or ...", "dateLastCrawled": "2021-12-15T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Abstract - arXiv", "url": "https://arxiv.org/pdf/1703.06856v3.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1703.06856v3.pdf", "snippet": "<b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our de\ufb01nition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real ...", "dateLastCrawled": "2020-08-09T05:32:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(counterfactual fairness)  is like +(fairness in car tuning)", "+(counterfactual fairness) is similar to +(fairness in car tuning)", "+(counterfactual fairness) can be thought of as +(fairness in car tuning)", "+(counterfactual fairness) can be compared to +(fairness in car tuning)", "machine learning +(counterfactual fairness AND analogy)", "machine learning +(\"counterfactual fairness is like\")", "machine learning +(\"counterfactual fairness is similar\")", "machine learning +(\"just as counterfactual fairness\")", "machine learning +(\"counterfactual fairness can be thought of as\")", "machine learning +(\"counterfactual fairness can be compared to\")"]}