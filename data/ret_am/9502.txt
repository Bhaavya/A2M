{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>Tutorial to AI Ethics - Fairness, Bias &amp; Perception</b>", "url": "https://www.slideshare.net/KimKyllesbechLarsen/a-tutorial-to-ai-ethics-fairness-bias-perception", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/KimKyllesbechLarsen/a-<b>tutorial-to-ai-ethics-fairness</b>-<b>bias</b>...", "snippet": "2,106 views. This collection of slides are meant as a starting point and tutorial for the ones who want to understand AI Ethics and in particular the challenges around <b>bias</b> and fairness. Furthermore, I have also included studies on how we as <b>humans</b> perceive AI influence in our private as well as working lives. Dr. Kim (Kyllesbech Larsen) Follow.", "dateLastCrawled": "2022-02-02T01:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is it really fair or did you automate it?", "url": "https://deep1401.github.io/data-ethics-2", "isFamilyFriendly": true, "displayUrl": "https://deep1401.github.io/data-ethics-2", "snippet": "<b>Bias</b> is a term which is often used in juxtaposition with fairness. So it would be wise to understand <b>bias</b> first and then move on to fairness. Applications &amp; Biases. Starting off with some examples of <b>bias</b> which exist in products seems <b>like</b> a good idea and that\u2019s what we\u2019ll be doing! Representation <b>Bias</b> in Facial Recognition", "dateLastCrawled": "2022-01-14T15:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>ethics, fairness and accountability</b> of AI", "url": "https://www.cranfield.ac.uk/alumni/communications/alumni-news/2021/0322-potential-issues-around-ethics-fairness-and-accountability", "isFamilyFriendly": true, "displayUrl": "https://www.cranfield.ac.uk/alumni/communications/alumni-news/2021/0322-potential...", "snippet": "<b>Bias</b> and fairness have been extensively studied by AI ethics researchers, but it is still possible to unknowingly train AI to discriminate. For example, skewed data sets may actively discriminate against women or those from an ethnic minority background. There are many other prominent examples of <b>bias</b>, for example, speech and facial recognition software that fails to work appropriately for large parts of society, and recruitment AI that favours males over females in the shortlisting process.", "dateLastCrawled": "2021-12-29T04:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "17-445: <b>Ethics &amp; Fairness in AI-Enabled Systems</b>", "url": "https://ckaestne.github.io/seai/S2020/slides/14_fairness1/fairness1.html", "isFamilyFriendly": true, "displayUrl": "https://ckaestne.github.io/seai/S2020/slides/14_fairness1/fairness1.html", "snippet": "&quot;An example of this type of <b>bias</b> can be found in a 2018 image search result where searching for women CEOs ultimately resulted in fewer female CEO images due to the fact that only 5% of Fortune 500 CEOs were woman\u2014which would cause the search results to be biased towards male CEOs. These search results were of course reflecting the reality, but whether or not the search algorithms should reflect this reality is an issue worth considering.&quot;", "dateLastCrawled": "2021-11-22T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "AI <b>Bias, Fairness</b>, AI <b>Explainability</b> | AI Ethicist", "url": "https://www.aiethicist.org/bias-fairness-explainability", "isFamilyFriendly": true, "displayUrl": "https://www.aiethicist.org/<b>bias-fairness</b>-<b>explainability</b>", "snippet": "Semantics derived automatically from language corpora contain human-<b>like</b> biases. Science, 356 Calo, Ryan: Artificial Intelligence Policy: A Primer and Roadmap, (2017) Carusi, Annamaria. (2008). Beyond Anonymity: Data as representation in e-research ethics. International Journal of Internet Research Ethics. 1. 37-65. The Center for Critical Race and Digital Studies (CR+DS): Publications &amp; Public Works Centre for Data Ethics and Innovation (UK Government): Landscape Summary: <b>Bias</b> in ...", "dateLastCrawled": "2022-02-01T09:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Ethical</b> dilemmas of AI: fairness, transparency, collaboration, trust ...", "url": "https://uxdesign.cc/ethical-dilemmas-of-ai-fairness-transparency-human-machine-collaboration-trust-accountability-1fe9fc0ffff3", "isFamilyFriendly": true, "displayUrl": "https://uxdesign.cc/<b>ethical</b>-dilemmas-of-ai-fairness-transparency-human-machine...", "snippet": "Accountability is complicated because \u201ctechnologies tend to spread moral responsibility between many actors\u201d <b>like</b> a car crash requires an investigation of multiple factors <b>like</b> what the different people involved in the accident were doing, the state of the car\u2019s brakes and who performed its last service (Bowles, 2018, p.12). Besides, although the <b>bias</b> problem starts to be acknowledged by the industry, firms and developers argue that their algorithms are neutral and \u201cso complicated ...", "dateLastCrawled": "2022-01-26T09:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Algorithmic <b>Bias</b>: Why Bother?", "url": "https://cmr.berkeley.edu/assets/documents/pdf/2020-11-algorithmic-bias.pdf", "isFamilyFriendly": true, "displayUrl": "https://cmr.berkeley.edu/assets/documents/pdf/2020-11-algorithmic-<b>bias</b>.pdf", "snippet": "Algorithmic <b>Bias</b>: Why Bother? b y . Damini Gupta and T S Krishnan. With the advent of AI, the impact of <b>bias</b> in algorithmic decisions will spread on an even wider scale. INSIGHT | FRONTIER 17 Nov 2020 T he hue and cry raised about <b>bias</b> in algorithmic decisions does not mean <b>humans</b> never. made biased decisions. If the biases always existed, then why bother now? Why should one. care about <b>bias</b> in AI decision-making? Because earlier the impact of the biased decisions made by <b>humans</b> was ...", "dateLastCrawled": "2021-12-22T21:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "AI Policy Making Part 4: A <b>Primer On Fair and Responsible ML and</b> AI ...", "url": "https://towardsdatascience.com/ai-policy-making-part-4-a-primer-on-fair-and-responsible-ml-and-ai-28f52b32190f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/ai-policy-making-part-4-a-primer-on-fair-and...", "snippet": "<b>Bias</b> in ML and AI could be due to bad design, biased source of information (data) or inadvertent human <b>bias</b> that gets encoded into our data, algorithms or both. <b>Bias</b> in Data. Data is the foundation for any ML and AI work and that is where many <b>bias</b> and fairness issues start. Data \u2192 Includes human <b>bias</b> from past \u2192 Feeds into Model", "dateLastCrawled": "2022-02-03T07:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Ethical Implications of Bias in Machine</b> Learning", "url": "https://www.researchgate.net/publication/323378868_Ethical_Implications_of_Bias_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/323378868_<b>Ethical_Implications_of_Bias_in</b>...", "snippet": "In some cases, the problem is <b>bias</b>-related, and if there is a feedback loop (i.e. the model learns based on its previous predictions), the <b>bias</b> is aggravated in each new prediction made (Mehrabi ...", "dateLastCrawled": "2022-01-16T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "seai/intro-<b>ethics-fairness</b>.md at S2021 \u00b7 ckaestne/seai \u00b7 GitHub", "url": "https://github.com/ckaestne/seai/blob/S2021/lectures/15_intro_ethics_fairness/intro-ethics-fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ckaestne/seai/blob/S2021/lectures/15_intro_<b>ethics_fairness</b>/intro...", "snippet": "In September 2015, Shkreli received widespread criticism when Turing obtained the manufacturing license for the antiparasitic drug Daraprim and raised its price by a factor of 56 (from USD 13.5 to 750 per pill), leading him to be referred to by the media as &quot;the most hated man in America&quot; and &quot;Pharma Bro&quot;.", "dateLastCrawled": "2021-08-29T22:43:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Ethical <b>Bias</b> Meaning and <b>Similar</b> Products and Services List ...", "url": "https://www.listalternatives.com/ethical-bias-meaning", "isFamilyFriendly": true, "displayUrl": "https://www.listalternatives.com/ethical-<b>bias</b>-meaning", "snippet": "Conflicts of Interest, <b>Bias</b>, and Ethics - Environmental ... top www.ncbi.nlm.nih.gov. Avoiding conflict of interest in order to achieve sound, unbiased science is in the vested interest of the scientific community as well as the general public.However, creating a system in which scientific decisions are made in an ethical manner while free of conflict and individual <b>bias</b> is a challenge.", "dateLastCrawled": "2022-01-27T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>ethics, fairness and accountability</b> of AI", "url": "https://www.cranfield.ac.uk/alumni/communications/alumni-news/2021/0322-potential-issues-around-ethics-fairness-and-accountability", "isFamilyFriendly": true, "displayUrl": "https://www.cranfield.ac.uk/alumni/communications/alumni-news/2021/0322-potential...", "snippet": "<b>Bias</b> and fairness have been extensively studied by AI ethics researchers, but it is still possible to unknowingly train AI to discriminate. For example, skewed data sets may actively discriminate against women or those from an ethnic minority background. There are many other prominent examples of <b>bias</b>, for example, speech and facial recognition software that fails to work appropriately for large parts of society, and recruitment AI that favours males over females in the shortlisting process.", "dateLastCrawled": "2021-12-29T04:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Is it really fair or did you automate it?", "url": "https://deep1401.github.io/data-ethics-2", "isFamilyFriendly": true, "displayUrl": "https://deep1401.github.io/data-ethics-2", "snippet": "Fairness and <b>Bias</b> Fairness has been talked about since a long time in the field of Computer Science or any other policy related fields where third party decisions influence the lives of other people. However, it has always been the case that fairness has always been described broadly as being impartial to people and in a real world scenario, such a vague definition wouldn\u2019t yield much results.", "dateLastCrawled": "2022-01-14T15:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Artificial intelligence researchers discuss bias</b> in algorithms at ...", "url": "https://news.northeastern.edu/2019/05/10/researchers-discuss-bias-in-algorithms-at-northeastern-university-for-new-england-machine-learning-day/", "isFamilyFriendly": true, "displayUrl": "https://news.northeastern.edu/2019/05/10/researchers-discuss-<b>bias</b>-in-algorithms-at...", "snippet": "\u201cIn particular, there are these issues around <b>ethics, fairness</b>, and <b>bias</b> that are really interesting.\u201d Panelists said that researchers need to work to eliminate <b>bias</b> in algorithms, more accurately communicate to the public the limitations of machine learning, and build systems that prioritize the health and wellness of <b>humans</b>.", "dateLastCrawled": "2022-01-14T12:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Algorithmic injustice: a relational ethics approach", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7892355/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7892355", "snippet": "When we see <b>bias</b> and discrimination, what we see is problems that have surfaced as a result of a field that has thoughtlessly inherited deeply rooted unjust, racist, and white supremacist histories and practices.56 As D&#39;Ignazio and Klein42 contend, \u201caddressing <b>bias</b> in a dataset is a tiny technological Band-Aid for a much larger problem.\u201d Furthermore, underlying the idea of \u201cfixing\u201d <b>bias</b> is the assumption that there exists a single correct description of reality where a deviation from ...", "dateLastCrawled": "2022-01-26T01:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Conservative AI</b> and social inequality: conceptualizing alternatives to ...", "url": "https://link.springer.com/article/10.1007/s00146-021-01153-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00146-021-01153-9", "snippet": "When authors specify forms of <b>bias</b>, these are typically differentiated by their source: human <b>bias</b>, machine <b>bias</b>, systemic <b>bias</b>, societal <b>bias</b>, historical <b>bias</b>, sampling <b>bias</b>, observation <b>bias</b>, and so on (for example, Shah et al. 2019a). There is considerable conceptual confusion and overlap with these terms, but they are used to distinguish, where the <b>bias</b> is supposedly \u2018coming from\u2019; whether an individual decision maker\u2019s unconscious <b>bias</b>, historically entrenched distributions, or ...", "dateLastCrawled": "2022-01-30T15:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Responsible AI practices \u2013 Google AI", "url": "https://ai.google/responsibilities/responsible-ai-practices/?category=fairness", "isFamilyFriendly": true, "displayUrl": "https://ai.google/responsibilities/responsible-ai-practices/?category=<b>fairness</b>", "snippet": "Data <b>bias</b> is another important consideration; learn more in practices on AI and <b>fairness</b>. Understand the limitations of your dataset and model . A model trained to detect correlations should not be used to make causal inferences, or imply that it can. E.g., your model may learn that people who buy basketball shoes are taller on average, but this does not mean that a user who buys basketball shoes will become taller as a result. Machine learning models today are largely a reflection of the ...", "dateLastCrawled": "2022-02-02T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine \u2026 Why ain\u2019t thee Fair</b>? | AI Strategy &amp; Policy", "url": "https://aistrategyblog.com/2018/10/11/machine-why-aint-thee-fair/", "isFamilyFriendly": true, "displayUrl": "https://aistrategyblog.com/2018/10/11/<b>machine-why-aint-thee-fair</b>", "snippet": "While there are 42 (i.e., many, but 42 is the answer to many things unknown and known) definitions out there defining fairness (or <b>bias</b>), I will define it as \u201ca systematic and significant difference in outcome of a given policy between distinct and statistically meaningful groups\u201d (note that in case of in-group systematic <b>bias</b> it often means that there actually are distinct sub-groups within that main group). So, yes this is a challenge.", "dateLastCrawled": "2022-01-18T05:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Create Unbiased Machine Learning Models - KDnuggets", "url": "https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2021/07/create-un<b>bias</b>ed-machine-learning-models.html", "snippet": "Where Does <b>Bias</b> Come From? &quot;<b>Humans</b> are the weakest link&quot; \u2014Bruce Schneier . In the field of Cybersecurity it is often said that \u201c<b>humans</b> are the weakest link\u201d (Schneier). This idea applies in our case as well. Biases are in fact introduced into ML models by <b>humans</b> unintentionally. Remember, an ML model can only be as good as the data it\u2019s trained on, and thus if the training data contains biases, we can expect our model to mimic those same biases. Some representative examples for this ...", "dateLastCrawled": "2022-02-02T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Cognitive Biases in Negotiation Processes", "url": "https://www.researchgate.net/publication/225210899_Cognitive_Biases_in_Negotiation_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/225210899_Cognitive_<b>Bias</b>es_in_Negotiation...", "snippet": "A. L. Woodward and E. M. Markman (see record 1991-30078-001) argue that the mutual exclusivity constraint is an inborn <b>bias</b>, rather than a fixed limitation, and that its expression during early ...", "dateLastCrawled": "2021-12-13T06:39:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Is it really fair or did you automate it?", "url": "https://deep1401.github.io/data-ethics-2", "isFamilyFriendly": true, "displayUrl": "https://deep1401.github.io/data-ethics-2", "snippet": "Fairness and <b>Bias</b> Fairness has been talked about since a long time in the field of Computer Science or any other policy related fields where third party decisions influence the lives of other people. However, it has always been the case that fairness has always been described broadly as being impartial to people and in a real world scenario, such a vague definition wouldn\u2019t yield much results.", "dateLastCrawled": "2022-01-14T15:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Ethical Implications of Bias in Machine</b> Learning", "url": "https://www.researchgate.net/publication/323378868_Ethical_Implications_of_Bias_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/323378868_<b>Ethical_Implications_of_Bias_in</b>...", "snippet": "Biases in AI and machine learning algorithms are. presented and analyzed through two issues. management frameworks with the aim of showing h ow. ethical problems a nd dilemmas <b>can</b> evolve. While ...", "dateLastCrawled": "2022-01-16T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Algorithmic injustice: a relational ethics approach", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7892355/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7892355", "snippet": "The common phrase \u201c<b>bias</b> in, <b>bias</b> out\u201d captures this deeply ingrained reductive thinking. Although datasets are often part of the problem, this commonly held belief relegates deeply rooted societal and historical injustices, nuanced power asymmetries, and structural inequalities to mere datasets. The implication is that if one <b>can</b> \u201cfix\u201d a certain dataset, the deeper problems disappear. When we see <b>bias</b> and discrimination, what we see is problems that have surfaced as a result of a ...", "dateLastCrawled": "2022-01-26T01:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>ethics, fairness and accountability</b> of AI", "url": "https://www.cranfield.ac.uk/alumni/communications/alumni-news/2021/0322-potential-issues-around-ethics-fairness-and-accountability", "isFamilyFriendly": true, "displayUrl": "https://www.cranfield.ac.uk/alumni/communications/alumni-news/2021/0322-potential...", "snippet": "By doing so, it <b>can</b> uncover blind spots and see what <b>humans</b> miss as well as improving situational awareness, avoiding \u2018alarm fatigue\u2019 and, perhaps more importantly, cutting the noise to identify the real risks to security. The UK\u2019s Government Communications Headquarters (GCHQ) recently published a paper on The Ethics of Artificial Intelligence which outlines several specific ways AI <b>can</b> be used. These include: Mapping international networks that enable human, drugs, and weapons ...", "dateLastCrawled": "2021-12-29T04:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "AI Policy Making Part 4: A <b>Primer On Fair and Responsible ML and</b> AI ...", "url": "https://towardsdatascience.com/ai-policy-making-part-4-a-primer-on-fair-and-responsible-ml-and-ai-28f52b32190f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/ai-policy-making-part-4-a-primer-on-fair-and...", "snippet": "However there is debate about shortcomings of these measures (e.g. these definitions are incompatible with each other) and some researchers believe that these definitions <b>can</b> actually exacerbate the problem of <b>bias</b>. Sam Corbett-Davies and Sharad Goel of Stanford University argue that all three of these fairness definitions suffer from significant statistical limitations. In many cases threshold based risk estimates are a better choice. This approach means treating \u201csimilarly risky people ...", "dateLastCrawled": "2022-02-03T07:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>The Coded Bias within Neurotechnology</b>", "url": "http://www.theneuroethicsblog.com/2021/04/the-coded-bias-within-neurotechnology.html", "isFamilyFriendly": true, "displayUrl": "www.theneuroethicsblog.com/2021/04/<b>the-coded-bias-within-neurotechnology</b>.html", "snippet": "Four years ago, the \u201cPoet of Code&quot; and founder of the Algorithmic Justice League, Joy Buolamwini, presented a TED Talk titled \u201cHow I\u2019m Fighting <b>Bias</b> in Algorithms.\u201d In this talk, Buolamwini dissects the chilling reality of algorithmic <b>bias</b>, a pervasive human-made virus for which she has coined the phrase \u201cthe coded gaze.\u201dAlgorithmic <b>bias</b> occurs when machines and models receive biased data and make biased, unfair decisions as a result.", "dateLastCrawled": "2021-12-13T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Can</b> Auditable AI Improve Fairness in Models? - Towards AI \u2014 The Best ...", "url": "https://towardsai.net/ai/fairness", "isFamilyFriendly": true, "displayUrl": "https://towardsai.net/ai/fairness", "snippet": "Category archive for Fairness. Author(s): Davor Petreski Recent events of racial discrimination in law enforcement and the healthcare industry have shown us how biased and racist <b>humans</b> are.", "dateLastCrawled": "2022-01-26T10:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How <b>Human-Centered AI</b> Could Empower <b>Humans</b>", "url": "https://issues.org/human-centered-ai/", "isFamilyFriendly": true, "displayUrl": "https://issues.org/<b>human-centered-ai</b>", "snippet": "But as flaws such as algorithmic <b>bias</b> in AI-driven systems emerged to shatter the belief in the perfectibility of these systems, HCAI researchers have begun to take on fairness, accountability, transparency, explainability, and other design features that give human developers, managers, users, and lawyers a better understanding of what is happening than in the previously closed black boxes. Moreover, many types of AI systems should include logging activity to support transparent and ...", "dateLastCrawled": "2022-01-28T15:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Justice and Fairness - Markkula Center for Applied Ethics", "url": "https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/justice-and-fairness/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.scu.edu</b>/ethics/ethics-resources/ethical-decision-making/justice-and-fairness", "snippet": "The foundations of justice <b>can</b> be traced to the notions of social stability, interdependence, and equal dignity. As the ethicist John Rawls has pointed out, the stability of a society\u2014or any group, for that matter\u2014depends upon the extent to which the members of that society feel that they are being treated justly. When some of society&#39;s members come to feel that they are subject to unequal treatment, the foundations have been laid for social unrest, disturbances, and strife. The members ...", "dateLastCrawled": "2022-02-03T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "As one of the shortlisted finalist, I <b>thought</b> it would be good to blog about the approach we are taking and why we believe it to be the right one. In the initial written proposal, rather than simply pitching our existing experience and expertise, I decided to write an aspirational proposal of what fairness in machine learning could look like, which I feel is very different from what it currently is in practice. I thank the MAS and organising committee for giving us the opportunity to shape ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>Tutorial to AI Ethics - Fairness, Bias &amp; Perception</b>", "url": "https://www.slideshare.net/KimKyllesbechLarsen/a-tutorial-to-ai-ethics-fairness-bias-perception", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/KimKyllesbechLarsen/a-<b>tutorial-to-ai-ethics-fairness</b>-<b>bias</b>...", "snippet": "Cognitive biases Statistical <b>bias</b> Contextual biases <b>Bias</b> is a disproportionate weight in favor of or against one thing, person, or group <b>compared</b> with another, usually in a way considered to be unfair. \u212cO MO = E MO \u2212 O O: Observation. M: Model estimator of observation O. E: Expected value (long-run average) B: <b>Bias</b> of model of O relative to O. Hundreds of cognitive biases, e.g., Anchoring, Aavailability, Confirmation <b>bias</b>, Belief <b>bias</b>, framing effect, etc\u2026 (See: https://en.wikipedia ...", "dateLastCrawled": "2022-02-02T01:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Sex and gender differences and biases in artificial intelligence for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7264169/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7264169", "snippet": "Since digital devices <b>can</b> acquire health related data in real-time, they <b>can</b> enable a continuous monitoring of an individual\u2019s health parameters in a cost-effective way that is more granular, ecological and objective than the currently clinically used self-reports, questionnaires or psychometric tests. Digital biomarkers are becoming especially relevant for those clinical conditions where small fluctuations in daily symptoms or performance are clinically meaningful. This is the case, for ...", "dateLastCrawled": "2022-01-30T00:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Ethics and Fairness in Assessing Learning Outcomes in Higher Education ...", "url": "https://link.springer.com/article/10.1057/s41307-019-00149-x", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1057/s41307-019-00149-x", "snippet": "Consequently, an issue concerning both test development and the use of tests in HE practice is how <b>bias</b> caused by non-construct-related factors <b>can</b> be reduced. Different approaches such as the imputation of missing values or the use of different test formats for various student subgroups are currently being tried and discussed in assessment research (e.g., Mutz et al., 2015 ; Orley, 2017 ).", "dateLastCrawled": "2022-01-27T07:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "3 Ways to Manage <b>Human Bias in the Analytics Process</b> - <b>KDnuggets</b>", "url": "https://www.kdnuggets.com/2019/09/manage-human-bias-analytics-process.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2019/09/manage-human-<b>bias</b>-analytics-process.html", "snippet": "Human <b>bias</b> <b>can</b> enter the analytics process every step of the way. As business decision-makers start looking to predictive analytics for specific advice on what action to take next, it\u2019s important that data and methods are leveraged as objectively as possible. The responsibility of monitoring human <b>bias</b> in analytics is a great one, and it all starts with the people building models from the ground up: data scientists.", "dateLastCrawled": "2022-01-22T18:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "A business problem <b>can</b> then be translated to a mathematical / statistical / computational problem and different methods <b>can</b> <b>be compared</b> against each other based on how far the predicted output differs from the actual output as measured by the objective function. If fairness could be distilled down to a single metric, a data scientist <b>can</b> include it as part of the objective function or as a constraint and find an ideal point that maximises overall business needs while satisfying fairness ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Responsible AI practices \u2013 Google AI", "url": "https://ai.google/responsibilities/responsible-ai-practices/?category=fairness", "isFamilyFriendly": true, "displayUrl": "https://ai.google/responsibilities/responsible-ai-practices/?category=<b>fairness</b>", "snippet": "Data <b>bias</b> is another important consideration; learn more in practices on AI and <b>fairness</b>. Understand the limitations of your dataset and model . A model trained to detect correlations should not be used to make causal inferences, or imply that it <b>can</b>. E.g., your model may learn that people who buy basketball shoes are taller on average, but this does not mean that a user who buys basketball shoes will become taller as a result. Machine learning models today are largely a reflection of the ...", "dateLastCrawled": "2022-02-02T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Ethical, Explainable Artificial Intelligence: Bias and Principles</b> ...", "url": "https://www.thefreelibrary.com/Ethical%2c+Explainable+Artificial+Intelligence%3a+Bias+and+Principles.-a0532528739", "isFamilyFriendly": true, "displayUrl": "https://www.thefreelibrary.com/Ethical,+Explainable+Artificial+Intelligence:+<b>Bias</b>+and...", "snippet": "However, many of these training sets <b>can</b> suffer from a variety of biases: Sets <b>can</b> be incomplete, skewed, non-representative, and rely on data improperly labeled by <b>humans</b> to embed biases and cultural assumptions. According to The AI Now Institute, the biases are &quot;difficult to find and understand, especially when systems are proprietary, treated as black boxes or taken at face value&quot; (&quot;AI Now 2017 Report,&quot; Campolo, Alex, Madelyn Sanfilippo, Meredith Whittaker, Kate Crawford, et al ...", "dateLastCrawled": "2021-03-25T07:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Public Artificial Intelligence</b>: A Crash Course for Politicians and ...", "url": "https://institute.global/policy/public-artificial-intelligence-crash-course-politicians-and-policy-professionals", "isFamilyFriendly": true, "displayUrl": "https://institute.global/policy/<b>public-artificial-intelligence</b>-crash-course...", "snippet": "Take responsibility for <b>ethics, fairness</b> and transparency. Algorithmic <b>bias</b> and fairness are serious concerns when it comes to deploying AI for social causes. Governments must provide further support for research in this area, help coordinate industry ethics codes, be transparent about how data are used so users understand these technologies ...", "dateLastCrawled": "2022-02-01T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Justice and Fairness - Markkula Center for Applied Ethics", "url": "https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/justice-and-fairness/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.scu.edu</b>/ethics/ethics-resources/ethical-decision-making/justice-and-fairness", "snippet": "The foundations of justice <b>can</b> be traced to the notions of social stability, interdependence, and equal dignity. As the ethicist John Rawls has pointed out, the stability of a society\u2014or any group, for that matter\u2014depends upon the extent to which the members of that society feel that they are being treated justly. When some of society&#39;s members come to feel that they are subject to unequal treatment, the foundations have been laid for social unrest, disturbances, and strife. The members ...", "dateLastCrawled": "2022-02-03T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How do we Humans feel about AI</b>? - SlideShare", "url": "https://www.slideshare.net/KimKyllesbechLarsen/how-do-we-humans-feel-about-ai-91783326", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/KimKyllesbechLarsen/<b>how-do-we-humans-feel-about-ai</b>-91783326", "snippet": "How important is human opinion <b>compared</b> to data-driven insights in your decision making? \u2248 20. 20Dr. Kim K. Larsen / <b>How do we Humans feel about AI</b>? Who do you trust the most with corporate decisions \u2026 Your fellow Human or your Corporate AI? 53% 17% AIs are hold to much much stricter standards than our fellow <b>humans</b>. 21. 21Dr. Kim K. Larsen ...", "dateLastCrawled": "2021-12-23T14:20:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Is Bias in Machine Learning all Bad</b>? - KDnuggets", "url": "https://www.kdnuggets.com/2019/07/bias-machine-learning-bad.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2019/07/<b>bias</b>-<b>machine</b>-<b>learning</b>-bad.html", "snippet": "<b>Analogy</b> with previously learned generalizations; If a system is <b>learning</b> a collection of related concepts, or generalizations, then a possible constraint on generalizing any one of them is to consider successful generalization of others. For example, consider a task of <b>learning</b> structural descriptions of blocks-world objects, such as \u201darch\u201d, \u201dtower\u201d, etc. After <b>learning</b> several concepts, the learned descriptions may reveal that certain features are more significant for describing ...", "dateLastCrawled": "2022-01-22T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is it really fair or did you automate it?", "url": "https://deep1401.github.io/data-ethics-2", "isFamilyFriendly": true, "displayUrl": "https://deep1401.github.io/data-ethics-2", "snippet": "<b>Machine</b> <b>learning</b> can amplify <b>bias</b>. As proposed by (De-Arteaga et al.,2019) ,it was observed that the algorithms were clearly amplifying <b>bias</b> in an already biased dataset. For example, the proportion of females in an occupation dataset who were surgeons was 14.6%.", "dateLastCrawled": "2022-01-14T15:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Responsible AI practices \u2013 Google AI", "url": "https://ai.google/responsibilities/responsible-ai-practices/?category=fairness", "isFamilyFriendly": true, "displayUrl": "https://ai.google/responsibilities/responsible-ai-practices/?category=<b>fairness</b>", "snippet": "Data <b>bias</b> is another important consideration; learn more in practices on AI and <b>fairness</b>. Understand the limitations of your dataset and model . A model trained to detect correlations should not be used to make causal inferences, or imply that it can. E.g., your model may learn that people who buy basketball shoes are taller on average, but this does not mean that a user who buys basketball shoes will become taller as a result. <b>Machine</b> <b>learning</b> models today are largely a reflection of the ...", "dateLastCrawled": "2022-02-02T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Algorithmic injustice: a relational ethics approach", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7892355/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7892355", "snippet": "<b>Machine</b> classification and prediction are practices that act directly upon the world and result in tangible impact.64 Various companies, institutes, and governments use <b>machine</b>-<b>learning</b> systems across a variety of areas. These systems process people&#39;s behaviors, actions, and the social world at large. The <b>machine</b>-detected patterns often provide \u201canswers\u201d to fuzzy, contingent, and open-ended questions. These \u201canswers\u201d neither reveal any causal relations nor provide explanation on why ...", "dateLastCrawled": "2022-01-26T01:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "A lot of what is discussed in the <b>machine</b> <b>learning</b> literature touches on fairness (or rather equivalence in certain outcomes) between groups, yet this narrowly constricts fairness to the notion of equality. Of course, we should think about fairness in the context of prejudiced groups, but we should also ask whether it is fair to an individual. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Algorithmic injustice: a relational ethics approach</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666389921000155", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666389921000155", "snippet": "<b>Machine</b> classification and prediction are practices that act directly upon the world and result in tangible impact. 64 Various companies, institutes, and governments use <b>machine</b>-<b>learning</b> systems across a variety of areas. These systems process people&#39;s behaviors, actions, and the social world at large. The <b>machine</b>-detected patterns often provide \u201canswers\u201d to fuzzy, contingent, and open-ended questions. These \u201canswers\u201d neither reveal any causal relations nor provide explanation on why ...", "dateLastCrawled": "2022-01-29T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) A review of some techniques for inclusion of domain-knowledge ...", "url": "https://www.researchgate.net/publication/357983755_A_review_of_some_techniques_for_inclusion_of_domain-knowledge_into_deep_neural_networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357983755_A_review_of_some_techniques_for...", "snippet": "The <b>machine</b> <b>learning</b> system then conveys its explanations to the biologist. ... <b>ethics, fairness</b>, and explainability o ... <b>Analogy</b> Model 75 RNN. Transf orming Model. KBANN 86 MLP. Cascade-ARTMAP ...", "dateLastCrawled": "2022-01-21T17:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Algorithmic injustice: <b>a relational ethics approach</b>: <b>Patterns</b>", "url": "https://www.cell.com/patterns/fulltext/S2666-3899(21)00015-5", "isFamilyFriendly": true, "displayUrl": "https://www.cell.com/<b>patterns</b>/fulltext/S2666-3899(21)00015-5", "snippet": "Data science and <b>machine</b>-<b>learning</b> systems sit firmly within the rationalist tradition. The core of what <b>machine</b>-<b>learning</b> systems do can be exemplified as clustering similarities and differences, abstracting commonalities, and detecting <b>patterns</b>. <b>Machine</b>-<b>learning</b> systems \u201cwork\u201d by identifying <b>patterns</b> in vast amounts of data. Given immense, messy, and complex data, a <b>machine</b>-<b>learning</b> system can sort, classify, and cluster similarities based on seemingly shared features. Feed a neural ...", "dateLastCrawled": "2022-01-31T12:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>MATHEMATICS FOR MACHINE LEARNING</b> | g t - Academia.edu", "url": "https://www.academia.edu/41334219/MATHEMATICS_FOR_MACHINE_LEARNING", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/41334219", "snippet": "<b>MATHEMATICS FOR MACHINE LEARNING</b>. g t. Kong Yao Chee. fabio baca. book P D F services. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package ...", "dateLastCrawled": "2022-02-02T23:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) Data Is the New What? Popular Metaphors &amp; Professional Ethics in ...", "url": "https://www.researchgate.net/publication/332828046_Data_Is_the_New_What_Popular_Metaphors_Professional_Ethics_in_Emerging_Data_Culture", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/332828046_Data_Is_the_New_What_Popular...", "snippet": "PDF | On Jan 1, 2019, Luke Stark and others published Data Is the New What? Popular Metaphors &amp; Professional Ethics in Emerging Data Culture | Find, read and cite all the research you need on ...", "dateLastCrawled": "2022-01-19T14:44:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bias (ethics/fairness))  is like +(bias in humans)", "+(bias (ethics/fairness)) is similar to +(bias in humans)", "+(bias (ethics/fairness)) can be thought of as +(bias in humans)", "+(bias (ethics/fairness)) can be compared to +(bias in humans)", "machine learning +(bias (ethics/fairness) AND analogy)", "machine learning +(\"bias (ethics/fairness) is like\")", "machine learning +(\"bias (ethics/fairness) is similar\")", "machine learning +(\"just as bias (ethics/fairness)\")", "machine learning +(\"bias (ethics/fairness) can be thought of as\")", "machine learning +(\"bias (ethics/fairness) can be compared to\")"]}