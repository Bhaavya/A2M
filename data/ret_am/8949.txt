{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to use <b>Learning</b> Curves to Diagnose <b>Machine Learning</b> Model Performance", "url": "https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>learning</b>-<b>curve</b>", "snippet": "This case can be identified by a <b>learning</b> <b>curve</b> for training <b>loss</b> that looks <b>like</b> a good fit (or other fits) and a <b>learning</b> <b>curve</b> for validation <b>loss</b> that shows noisy movements around the training <b>loss</b>. Example of Train and Validation <b>Learning</b> Curves Showing a Validation Dataset That May Be too Small Relative to the Training Dataset. It may also be identified by a validation <b>loss</b> that is lower than the training <b>loss</b>. In this case, it indicates that the validation dataset may be easier for ...", "dateLastCrawled": "2022-02-03T06:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Interpreting <b>Loss</b> Curves | Testing and Debugging in Machine <b>Learning</b> ...", "url": "https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-<b>learning</b>/testing-debugging/metrics/interpretic", "snippet": "Machine <b>learning</b> would be a breeze if all our <b>loss</b> curves looked <b>like</b> this the first time we ... Your friend Mel and you continue working on a unicorn appearance predictor. Here&#39;s your first <b>loss curve</b>. Describe the problem and how Mel could fix it: Click on the plus icon to expand the section and reveal the answer. Your model is not converging. Try these debugging steps: Check if your features can predict the labels by following the steps in Model Debugging. Check your data against a data ...", "dateLastCrawled": "2022-02-03T06:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning curve vs training (loss) curve</b>? - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/476339/learning-curve-vs-training-loss-curve", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/476339/<b>learning-curve-vs-training-loss-curve</b>", "snippet": "The <b>learning</b> <b>curve</b> gives you an idea of how the model benefits from being incrementally fed more and more data observations, therefore focusing on inputs external to the model, thereby quantifying the marginal benefit of each new data point.. The training <b>curve</b> gives you an idea of how the model benefits from having its bias-variance trade-off managed while cycling its algorithm back from start to finish repeatedly, therefore, focusing on processes or parameter calibration inputs internal to ...", "dateLastCrawled": "2022-02-03T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning</b> <b>Curve</b> Theory: Meaning, Formulas, Graphs", "url": "https://www.valamis.com/hub/learning-curve", "isFamilyFriendly": true, "displayUrl": "https://www.valamis.com/hub/<b>learning</b>-<b>curve</b>", "snippet": "A <b>learning</b> <b>curve</b> is a correlation between a learner&#39;s performance on a task and the number of attempts or time required to complete the task; this can be represented as a direct proportion on a graph. The <b>learning</b> <b>curve</b> theory proposes that a learner\u2019s efficiency in a task improves over time the more the learner performs the task. Graphical ...", "dateLastCrawled": "2022-02-03T03:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning</b> <b>Curve</b> to identify <b>Overfitting</b> and Underfitting in Machine ...", "url": "https://towardsdatascience.com/learning-curve-to-identify-overfitting-underfitting-problems-133177f38df5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>learning</b>-<b>curve</b>-to-identify-<b>overfitting</b>-underfitting...", "snippet": "Interpreting the validation <b>loss</b>. <b>Learning</b> <b>curve</b> of a good fit model has a high validation <b>loss</b> at the beginning which gradually decreases upon adding training examples and flattens gradually, indicating addition of more training examples doesn\u2019t improve the model performance on unseen data. We can also see that upon adding a reasonable number of training examples, both the training and validation <b>loss</b> moved close to each other. Typical features of the <b>learning</b> <b>curve</b> of a good fit model ...", "dateLastCrawled": "2022-02-03T01:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Tutorial: <b>Learning Curves for Machine Learning</b> in Python", "url": "https://www.dataquest.io/blog/learning-curves-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.dataquest.io/blog/<b>learning</b>-<b>curves</b>-machine-<b>learning</b>", "snippet": "The <b>learning</b>_<b>curve</b>() function from scikit-learn. We\u2019ll use the <b>learning</b>_<b>curve</b>() function from the scikit-learn library to generate a <b>learning</b> <b>curve</b> for a regression model. There\u2019s no need on our part to put aside a validation set because <b>learning</b>_<b>curve</b>() will take care of that. In the code cell below, we:", "dateLastCrawled": "2022-02-02T06:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Useful Plots to Diagnose your Neural Network | by George V Jose ...", "url": "https://towardsdatascience.com/useful-plots-to-diagnose-your-neural-network-521907fa2f45", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/useful-plots-to-diagnose-your-neural-network-521907fa2f45", "snippet": "One of the most used plots to debug a neural network is a <b>Loss</b> <b>curve</b> during training. It gives us a snapshot of the training process and the direction in which the network learns. An awesome explanation is from Andrej Karpathy at Stanford University at this link. And this section is heavily inspired by it. Effect of <b>Learning</b> rate on <b>Loss</b> (Source: CS231n Convolutional Neural Networks for Visual Recognition) The image is pretty much self-explanatory. You can log your <b>loss</b> in two periods: After ...", "dateLastCrawled": "2022-02-03T05:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Process Of <b>Learning</b>, The <b>Learning</b> <b>Curve</b> - Study material for education ...", "url": "https://www.yogiraj.co.in/process-of-learning-the-learning-curve", "isFamilyFriendly": true, "displayUrl": "https://www.yogiraj.co.in/process-of-<b>learning</b>-the-<b>learning</b>-<b>curve</b>", "snippet": "<b>Learning</b> <b>curve</b> is a graphic representation of how <b>learning</b> takes place in a particular situation. In all type of <b>learning</b> situations, the course of <b>learning</b> can be depicted and described graphically by drawing <b>learning</b> curves against x and y axis. The above figure shows a typical <b>learning</b> <b>curve</b> of many types of <b>learning</b>.", "dateLastCrawled": "2022-01-06T08:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How do I interpret my validation and training <b>loss</b> <b>curve</b> if there is a ...", "url": "https://stats.stackexchange.com/questions/335890/how-do-i-interpret-my-validation-and-training-loss-curve-if-there-is-a-large-dif", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/335890/how-do-i-interpret-my-validation-and...", "snippet": "What is causing the shape of my <b>loss</b> <b>curve</b> to be <b>like</b> that and how do I tweak it so validation <b>loss</b> drops quicker in sync? I don\u2019t understand why there is a large gap between the two which only closes in after a few epochs. I&#39;m reluctant to increase my data-set since I&#39;ve seen other similar projects being done with similar size data. Input image pixels have already been standardized to between 0-1 . Edit: I&#39;m posting my architecture below just for more info if it helps, hope someone takes ...", "dateLastCrawled": "2022-01-22T00:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Plotting <b>Learning</b> Curves \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_<b>learning</b>_<b>curve</b>.html", "snippet": "See :term:`Glossary &lt;n_jobs&gt;` for more details. train_sizes : array-<b>like</b> of shape (n_ticks,) Relative or absolute numbers of training examples that will be used to generate the <b>learning</b> <b>curve</b>. If the ``dtype`` is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1].", "dateLastCrawled": "2022-02-02T23:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to use <b>Learning</b> Curves to Diagnose <b>Machine Learning</b> Model Performance", "url": "https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>learning</b>-<b>curve</b>", "snippet": "This situation can be identified by a <b>learning</b> <b>curve</b> for training <b>loss</b> that shows improvement and similarly a <b>learning</b> <b>curve</b> for validation <b>loss</b> that shows improvement, but a large gap remains between both curves. Example of Train and Validation <b>Learning</b> Curves Showing a Training Dataset That May Be too Small Relative to the Validation Dataset . Unrepresentative Validation Dataset. An unrepresentative validation dataset means that the validation dataset does not provide sufficient ...", "dateLastCrawled": "2022-02-03T06:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Interpreting <b>Loss</b> Curves | Testing and Debugging in Machine <b>Learning</b> ...", "url": "https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-<b>learning</b>/testing-debugging/metrics/interpretic", "snippet": "Machine <b>learning</b> would be a breeze if all our <b>loss</b> curves looked like this the first time we trained our model: But in reality, <b>loss</b> curves can be quite challenging to interpret. Use your understanding of <b>loss</b> curves to answer the following questions. 1. My Model Won&#39;t Train! Your friend Mel and you continue working on a unicorn appearance predictor. Here&#39;s your first <b>loss curve</b>. Describe the problem and how Mel could fix it: Click on the plus icon to expand the section and reveal the answer ...", "dateLastCrawled": "2022-02-03T06:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "machine <b>learning</b> - Why training and validation <b>similar</b> <b>loss</b> curves lead ...", "url": "https://datascience.stackexchange.com/questions/31860/why-training-and-validation-similar-loss-curves-lead-to-poor-performance", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/31860", "snippet": "to prevent overfitting in a model the training <b>curve</b> in a <b>loss</b> graph should be <b>similar</b> to the validation <b>curve</b>. but in the current situation the third graph shows <b>curve</b> where validation <b>curve</b> <b>is similar</b> to training although the overall accuracy is low as compared to the <b>curve</b> where the two <b>curve</b> diverges in the above plot. WHy this is happening and what I am doing wrong in understanding these curves? machine-<b>learning</b> classification matlab. Share. Improve this question. Follow asked May 19 ...", "dateLastCrawled": "2022-01-20T17:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Diagnosing Model Performance with <b>Learning</b> Curves", "url": "https://rstudio-conf-2020.github.io/dl-keras-tf/notebooks/learning-curve-diagnostics.nb.html", "isFamilyFriendly": true, "displayUrl": "https://rstudio-conf-2020.github.io/dl-keras-tf/notebooks/<b>learning-curve</b>-diagnostics...", "snippet": "This situation can be identified by a <b>learning curve</b> for training <b>loss</b> that shows improvement and similarly a <b>learning curve</b> for validation <b>loss</b> that shows improvement, but a large gap remains between both curves. This can occur when. The training dataset has too few examples as compared to the validation dataset. Training dataset contains features with less variance than the validation dataset. Solution: Add more observations. You may not have enough data to capture patterns present in both ...", "dateLastCrawled": "2022-01-29T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning</b> <b>Curve</b> Theory: Meaning, Formulas, Graphs", "url": "https://www.valamis.com/hub/learning-curve", "isFamilyFriendly": true, "displayUrl": "https://www.valamis.com/hub/<b>learning</b>-<b>curve</b>", "snippet": "A <b>learning</b> <b>curve</b> is a correlation between a learner&#39;s performance on a task and the number of attempts or time required to complete the task; this can be represented as a direct proportion on a graph. The <b>learning</b> <b>curve</b> theory proposes that a learner\u2019s efficiency in a task improves over time the more the learner performs the task. Graphical ...", "dateLastCrawled": "2022-02-03T03:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How do I interpret my validation and training <b>loss</b> <b>curve</b> if there is a ...", "url": "https://stats.stackexchange.com/questions/335890/how-do-i-interpret-my-validation-and-training-loss-curve-if-there-is-a-large-dif", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/335890/how-do-i-interpret-my-validation-and...", "snippet": "I thought maybe its because my <b>learning</b> rate is too large, causing the training <b>loss</b> to plunge initially. I tried reducing <b>learning</b> rate. But doesn&#39;t change much. I thought maybe its due to initial overfitting and that I am \u201clucky\u201d to have the validation <b>loss</b> eventually find its way lower, so I tried adding dropouts to my model . I&#39;m still getting the same issue. I also tried changing the momentum from 0.5 to 0.9. What is causing the shape of my <b>loss</b> <b>curve</b> to be like that and how do I ...", "dateLastCrawled": "2022-01-22T00:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Limits of the <b>Learning Curve</b> - <b>Harvard Business Review</b>", "url": "https://hbr.org/1974/09/limits-of-the-learning-curve", "isFamilyFriendly": true, "displayUrl": "https://<b>hbr.org</b>/1974/09/limits-of-the-<b>learning-curve</b>", "snippet": "Proponents of the <b>learning curve</b> have developed the relationships between volume growth and cost reduction through the use of two distinct but related approaches: 1. The <b>learning curve</b> (also ...", "dateLastCrawled": "2022-01-31T00:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "sklearn.model_selection.<b>learning_curve</b> \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/.../modules/generated/sklearn.model_selection.<b>learning_curve</b>.html", "snippet": "Only used in conjunction with a \u201cGroup\u201d cv instance (e.g., GroupKFold ). train_sizesarray-like of shape (n_ticks,), default=np.linspace (0.1, 1.0, 5) Relative or absolute numbers of training examples that will be used to generate the <b>learning curve</b>. If the dtype is float, it is regarded as a fraction of the maximum size of the training set ...", "dateLastCrawled": "2022-02-02T22:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "machine <b>learning</b> - Overfit from the <b>loss</b> <b>curve</b> - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/43725/overfit-from-the-loss-curve", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/43725/overfit-from-the-<b>loss</b>-<b>curve</b>", "snippet": "I have a binary classification task. I have shown the <b>loss</b> <b>curve</b> here. I have decreased the <b>learning</b> rate by 1/10 every 15 epochs. There is also dropout put in the model. As you can see, I am tryin...", "dateLastCrawled": "2022-01-13T18:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Loss</b> and accuracy <b>curve</b> explanation : learnmachinelearning", "url": "https://www.reddit.com/r/learnmachinelearning/comments/sd4k3t/loss_and_accuracy_curve_explanation/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/learnmachine<b>learning</b>/comments/sd4k3t/<b>loss</b>_and_accuracy_<b>curve</b>...", "snippet": "26 votes, 15 comments. 256k members in the learnmachinelearning community. A subreddit dedicated <b>to learning</b> machine <b>learning</b>. Press J to jump to the feed. Press question mark to learn the rest of the keyboard shortcuts . Search within r/learnmachinelearning. r/learnmachinelearning. Log In Sign Up. User account menu. Found the internet! 26. <b>Loss</b> and accuracy <b>curve</b> explanation. Close. 26. Posted by 8 hours ago. <b>Loss</b> and accuracy <b>curve</b> explanation. 1/2. 15 comments. share. save. hide. report ...", "dateLastCrawled": "2022-01-26T19:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to use <b>Learning</b> Curves to Diagnose <b>Machine Learning</b> Model Performance", "url": "https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>learning</b>-<b>curve</b>", "snippet": "This case <b>can</b> be identified by a <b>learning</b> <b>curve</b> for training <b>loss</b> that looks like a good fit (or other fits) and a <b>learning</b> <b>curve</b> for validation <b>loss</b> that shows noisy movements around the training <b>loss</b>. Example of Train and Validation <b>Learning</b> Curves Showing a Validation Dataset That May Be too Small Relative to the Training Dataset. It may also be identified by a validation <b>loss</b> that is lower than the training <b>loss</b>. In this case, it indicates that the validation dataset may be easier for ...", "dateLastCrawled": "2022-02-03T06:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning</b> <b>Curve</b> Theory: Meaning, Formulas, Graphs", "url": "https://www.valamis.com/hub/learning-curve", "isFamilyFriendly": true, "displayUrl": "https://www.valamis.com/hub/<b>learning</b>-<b>curve</b>", "snippet": "Using the <b>learning</b> <b>curve</b> <b>can</b> provide additional insight for planning purposes. Business. <b>Learning</b> curves <b>can</b> also be applied to organizational performance using either the generalized approach or by conducting a measured analysis. Determining which approach to take depends on whether the desired performance <b>can</b> be directly measured. For example, employees <b>learning</b> a difficult task, such as <b>learning</b> to use a complex software program, may have poor performance at the beginning due to the ...", "dateLastCrawled": "2022-02-03T03:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> <b>curve</b> and short-term outcomes of modularized LADG for advanced ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6417604/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6417604", "snippet": "In terms of intraoperative blood <b>loss</b>, the <b>learning</b> <b>curve</b> trend was the same as the operative time ... Therefore, we <b>thought</b> that the <b>learning</b> <b>curve</b> for MLADG might be 20 cases in this study. Several LADG <b>learning</b> <b>curve</b> studies have reported that experience in managing 40 to 60 LADG cases are required to achieve proficiency and to reach a <b>learning</b> <b>curve</b> plateau. [31,34\u201336] The number of MLADG cases needed to overcome the <b>learning</b> <b>curve</b> in our study, was less than that for LADG. Compared ...", "dateLastCrawled": "2019-12-31T17:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning Structured Models with the</b> <b>AUC Loss and Its Generalizations</b> - TTIC", "url": "https://home.ttic.edu/~meshi/papers/structAUC_aistats14.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~meshi/papers/structAUC_aistats14.pdf", "snippet": "it <b>can</b> also <b>be thought</b> of as an accuracy measure for predictors of multi-labeled instances. In this paper we focus on the latter and apply an Empirical Risk Min- imization (ERM) methodology, where our goal is to minimize the AUC <b>loss</b> over a set of examples. Ideally, we would like to be able to take the same model that we developed for <b>learning</b> under a Ham-<b>Learning Structured Models with the</b> <b>AUC Loss and Its Generalizations</b> ming <b>loss</b> objective, replace the Hamming <b>loss</b> with AUC, then proceed ...", "dateLastCrawled": "2022-01-31T01:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How do I interpret my validation and training <b>loss</b> <b>curve</b> if there is a ...", "url": "https://stats.stackexchange.com/questions/335890/how-do-i-interpret-my-validation-and-training-loss-curve-if-there-is-a-large-dif", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/335890/how-do-i-interpret-my-validation-and...", "snippet": "I <b>thought</b> maybe its because my <b>learning</b> rate is too large, causing the training <b>loss</b> to plunge initially. I tried reducing <b>learning</b> rate. But doesn&#39;t change much. I <b>thought</b> maybe its due to initial overfitting and that I am \u201clucky\u201d to have the validation <b>loss</b> eventually find its way lower, so I tried adding dropouts to my model . I&#39;m still getting the same issue. I also tried changing the momentum from 0.5 to 0.9. What is causing the shape of my <b>loss</b> <b>curve</b> to be like that and how do I ...", "dateLastCrawled": "2022-01-22T00:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>learning curve</b> - Everything2.com", "url": "https://everything2.com/title/learning+curve", "isFamilyFriendly": true, "displayUrl": "https://everything2.com/title/<b>learning+curve</b>", "snippet": "A <b>learning curve</b> is a hypothetical graph of how much time and energy must be invested before a tool <b>can</b> be used productively by the average user. If a tool has a high <b>learning curve</b>, it requires substantial study and experimentation before it will actually be useful. If a tool has an extremely low <b>learning curve</b>, the average user will be able ...", "dateLastCrawled": "2022-01-17T21:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Managing Risk Along the <b>Loss</b> <b>Curve</b> | Insurance <b>Thought</b> Leadership", "url": "https://www.insurancethoughtleadership.com/risk-management/managing-risk-along-loss-curve", "isFamilyFriendly": true, "displayUrl": "https://www.insurance<b>thought</b>leadership.com/risk-management/managing-risk-along-<b>loss</b>-<b>curve</b>", "snippet": "The traditional approach, focusing on expected losses and relying on insurance, <b>can</b> leave important issues unexplored. ||There are many definitions of risk, with most coming pretty close to each other. Interestingly, most of these definitions put &quot;risk&quot; well beyond the point of &quot;expected losses&quot; (think of the high point on the actuarial <b>loss</b> <b>curve</b> that trails off into infinity as <b>loss</b> becomes less and less likely to occur but more and more severe; see figure 1 below).", "dateLastCrawled": "2022-01-31T13:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning</b> <b>Curve</b>", "url": "https://co-hv.org/deaf-plus/learning-curve/", "isFamilyFriendly": true, "displayUrl": "https://co-hv.org/deaf-plus/<b>learning</b>-<b>curve</b>", "snippet": "<b>Learning</b> <b>Curve</b>. By Laurie Pachl, Colorado H&amp;V Board member \u201cIf only I knew then what I know now\u2026\u201d Isn\u2019t that the catch phrase? Where do we look? Where do we find help, encouragement, support and acknowledgement on how hard our task is? Where do we find the strength in the moments when we feel alone? Lots of times we have family and friends who have said the wrong things \u2013who have made us feel bad instead of better. We have had people stare or question us in ways that seem ...", "dateLastCrawled": "2022-02-03T08:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] Why does L-BFGS have spikes in the <b>loss</b> <b>curve</b>? : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/hasc8r/d_why_does_lbfgs_have_spikes_in_the_loss_curve/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/hasc8r/d_why_does_lbfgs_have_spikes_in_the_<b>loss</b>_<b>curve</b>", "snippet": "The docs you linked to say that there is no line search unless you set line_search_fn=&quot;strong_wolfe&quot;.If you didn&#39;t do that, that would explain it. If you did set that, then it&#39;s possible that the line search is failing under some circumstances. The basics of writing a line search function for the strong Wolfe conditions aren&#39;t that difficult, but it&#39;s not straight forward.. In my experience, the harder part is detecting all the ways it <b>can</b> go wrong and what to do when it fails.", "dateLastCrawled": "2022-01-29T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine <b>learning</b> - <b>Advantages</b> of ROC curves - Cross Validated", "url": "https://stats.stackexchange.com/questions/28745/advantages-of-roc-curves", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/28745", "snippet": "After creating a ROC <b>curve</b>, the AUC (area under the <b>curve</b>) <b>can</b> be calculated. The AUC is accuracy of the test across many thresholds. AUC = 1 means the test is perfect. AUC = .5 means performs at chance for binary classification. If there are multiple models, AUC provides a single measurement to compare across different models.", "dateLastCrawled": "2022-02-03T03:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to use <b>Learning</b> Curves to Diagnose <b>Machine Learning</b> Model Performance", "url": "https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>learning</b>-<b>curve</b>", "snippet": "This may occur if the training dataset has too few examples as <b>compared</b> to the validation dataset. This situation <b>can</b> be identified by a <b>learning</b> <b>curve</b> for training <b>loss</b> that shows improvement and similarly a <b>learning</b> <b>curve</b> for validation <b>loss</b> that shows improvement, but a large gap remains between both curves. Example of Train and Validation <b>Learning</b> Curves Showing a Training Dataset That May Be too Small Relative to the Validation Dataset. Unrepresentative Validation Dataset. An ...", "dateLastCrawled": "2022-02-03T06:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to Interpret model <b>learning</b> <b>curve</b> (epoch wise accuracy &amp; <b>loss</b> <b>curve</b> ...", "url": "https://stackoverflow.com/questions/66411818/how-to-interpret-model-learning-curve-epoch-wise-accuracy-loss-curve-for-trai", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/66411818/how-to-interpret-model-<b>learning</b>-<b>curve</b>...", "snippet": "here I am working with 1D cnn based model,i am not understanding the model <b>learning</b> <b>curve</b>, as the test/validation accuracy <b>curve</b> are fluctuating, and overall model performance is about 70%, On the", "dateLastCrawled": "2022-01-20T03:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> <b>Curve</b> Theory: Meaning, Formulas, Graphs", "url": "https://www.valamis.com/hub/learning-curve", "isFamilyFriendly": true, "displayUrl": "https://www.valamis.com/hub/<b>learning</b>-<b>curve</b>", "snippet": "Using the <b>learning</b> <b>curve</b> <b>can</b> provide additional insight for planning purposes. Business. <b>Learning</b> curves <b>can</b> also be applied to organizational performance using either the generalized approach or by conducting a measured analysis. Determining which approach to take depends on whether the desired performance <b>can</b> be directly measured. For example, employees <b>learning</b> a difficult task, such as <b>learning</b> to use a complex software program, may have poor performance at the beginning due to the ...", "dateLastCrawled": "2022-02-03T03:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning</b> <b>Curve</b> to identify <b>Overfitting</b> and Underfitting in Machine ...", "url": "https://towardsdatascience.com/learning-curve-to-identify-overfitting-underfitting-problems-133177f38df5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>learning</b>-<b>curve</b>-to-identify-<b>overfitting</b>-underfitting...", "snippet": "The standard deviation of cross validation accuracies is low <b>compared</b> to overfit and good fit model. However, underfitting <b>can</b> be detected from the <b>learning</b> <b>curve</b>. Image by author Interpreting the training <b>loss</b> . <b>Learning</b> <b>curve</b> of an underfit model has a low training <b>loss</b> at the beginning which gradually increases upon adding training examples and suddenly falls to an arbitrary minimum point (minimum doesn\u2019t mean zero <b>loss</b>) at the end. This sudden fall at the end may not always happen. The ...", "dateLastCrawled": "2022-02-03T01:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Diagnosing Model Performance with <b>Learning</b> Curves", "url": "https://rstudio-conf-2020.github.io/dl-keras-tf/notebooks/learning-curve-diagnostics.nb.html", "isFamilyFriendly": true, "displayUrl": "https://rstudio-conf-2020.github.io/dl-keras-tf/notebooks/<b>learning-curve</b>-diagnostics...", "snippet": "This situation <b>can</b> be identified by a <b>learning curve</b> for training <b>loss</b> that shows improvement and similarly a <b>learning curve</b> for validation <b>loss</b> that shows improvement, but a large gap remains between both curves. This <b>can</b> occur when. The training dataset has too few examples as <b>compared</b> to the validation dataset.", "dateLastCrawled": "2022-01-29T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Tutorial: <b>Learning Curves for Machine Learning</b> in Python", "url": "https://www.dataquest.io/blog/learning-curves-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.dataquest.io/blog/<b>learning</b>-<b>curves</b>-machine-<b>learning</b>", "snippet": "By examining the gap between the validation <b>learning</b> <b>curve</b> and training <b>learning</b> <b>curve</b>. By examining the training error: its value and its evolution as the training set sizes increase. A narrow gap indicates low variance. Generally, the more narrow the gap, the lower the variance. The opposite is also true: the wider the gap, the greater the variance. Let\u2019s now explain why this is the case. As we\u2019ve discussed earlier, if the variance is high, then the model fits training data too well ...", "dateLastCrawled": "2022-02-02T06:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "machine <b>learning</b> - Why training and validation similar <b>loss</b> curves lead ...", "url": "https://datascience.stackexchange.com/questions/31860/why-training-and-validation-similar-loss-curves-lead-to-poor-performance", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/31860", "snippet": "to prevent overfitting in a model the training <b>curve</b> in a <b>loss</b> graph should be similar to the validation <b>curve</b>. but in the current situation the third graph shows <b>curve</b> where validation <b>curve</b> is similar to training although the overall accuracy is low as <b>compared</b> to the <b>curve</b> where the two <b>curve</b> diverges in the above plot. WHy this is happening and what I am doing wrong in understanding these curves? machine-<b>learning</b> classification matlab. Share. Improve this question. Follow asked May 19 ...", "dateLastCrawled": "2022-01-20T17:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How do I interpret my validation and training <b>loss</b> <b>curve</b> if there is a ...", "url": "https://stats.stackexchange.com/questions/335890/how-do-i-interpret-my-validation-and-training-loss-curve-if-there-is-a-large-dif", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/335890/how-do-i-interpret-my-validation-and...", "snippet": "I thought maybe its because my <b>learning</b> rate is too large, causing the training <b>loss</b> to plunge initially. I tried reducing <b>learning</b> rate. But doesn&#39;t change much. I thought maybe its due to initial overfitting and that I am \u201clucky\u201d to have the validation <b>loss</b> eventually find its way lower, so I tried adding dropouts to my model . I&#39;m still getting the same issue. I also tried changing the momentum from 0.5 to 0.9. What is causing the shape of my <b>loss</b> <b>curve</b> to be like that and how do I ...", "dateLastCrawled": "2022-01-22T00:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Problems with fixed and decaying learning rates</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/11/11/problems-with-fixed-and-decaying-learning-rates/", "isFamilyFriendly": true, "displayUrl": "https://www.machine<b>curve</b>.com/.../11/11/<b>problems-with-fixed-and-decaying-learning-rates</b>", "snippet": "It actually shows a very normal <b>learning</b> <b>curve</b>: a steep descent during the first few epochs, after which the model gets close to the minimum (whether local or global!) and <b>learning</b> stabilizes. The final <b>loss</b> value on the training data is approximately 0.01 whereas it\u2019s 0.05 on the validation data \u2013 that\u2019s pretty good. Test accuracy, in this case, confirmed the results: Test <b>loss</b>: 0.02863448634357819 / Test accuracy: 0.9919000267982483. Too large fixed <b>learning</b> rate: overshooting <b>loss</b> ...", "dateLastCrawled": "2022-01-30T18:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The accuracy <b>curve</b> and <b>loss</b> <b>curve</b> and confusion matrix of BHCNet-3 for ...", "url": "https://researchgate.net/figure/The-accuracy-curve-and-loss-curve-and-confusion-matrix-of-BHCNet-3-for-the-binary_fig4_332436202", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/The-accuracy-<b>curve</b>-and-<b>loss</b>-<b>curve</b>-and-confusion-matrix...", "snippet": "The accuracy <b>curve</b>, <b>loss</b> <b>curve</b> and confusion matrix of the experiment result for each magnification factor is shown in Fig 6. We <b>compared</b> our method with some existing approaches that were ...", "dateLastCrawled": "2021-05-09T11:33:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning</b> <b>Evaluation Metrics</b> - GitHub Pages", "url": "https://kevalnagda.github.io/evaluation-metrics", "isFamilyFriendly": true, "displayUrl": "https://kevalnagda.github.io/<b>evaluation-metrics</b>", "snippet": "Introduction Evaluating your <b>machine learning</b> model is a crucial part of any project. Your model may give satisfactory results when evaluated using metrics such as accuracy but may perform poorly when evaluated against other metrics such as <b>loss</b> or F1 score. In most cases, we use accuracy to measure the model performance, however, it is not enough to truly judge our model. Thus, let\u2019s take a look at different <b>evaluation metrics</b> available. Confusion Matrix Confusion Matrix is a performance ...", "dateLastCrawled": "2021-10-13T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning to Teach with Dynamic Loss Functions</b>", "url": "https://proceedings.neurips.cc/paper/2018/file/8051a3c40561002834e59d566b7430cf-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2018/file/8051a3c40561002834e59d566b7430cf-Paper.pdf", "snippet": "<b>loss</b> function of a <b>machine</b> <b>learning</b> model (we call it student) is de\ufb01ned by another <b>machine</b> <b>learning</b> model (we call it teacher). The ultimate goal of teacher model is cultivating the student to have better performance measured on development dataset. Towards that end, similar to human teaching, the teacher, a parametric model, dynamically outputs different <b>loss</b> functions that will be used and optimized by its student model at different training stages. We develop an ef\ufb01cient <b>learning</b> ...", "dateLastCrawled": "2022-01-26T01:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning</b> <b>Curve</b> Graphs Part 2: The Experiment - innotescus", "url": "https://innotescus.io/data-visualization/learning-curve-graphs-part-2-the-experiment/", "isFamilyFriendly": true, "displayUrl": "https://innotescus.io/data-visualization/<b>learning</b>-<b>curve</b>-graphs-part-2-the-experiment", "snippet": "In part 1, we discussed the data requirement curse for non-linear <b>machine</b> <b>learning</b> models. We saw that we can use <b>learning</b> <b>curve</b> graphs to estimate the dataset size for target performance. In part 2, we will design this experiment using a benchmark dataset: Fashion-MNIST. The Fashion-MNIST Each training and test example is assigned to one of the following labels: The figure below shows some examples of Fashion-MNIST data points we will use in our experiment. Our goal is to see if we can ...", "dateLastCrawled": "2022-01-24T09:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>learning</b> fundamentals (I): Cost functions and gradient descent ...", "url": "https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine</b>-<b>learning</b>-fundamentals-via-linear-regression-41a...", "snippet": "In this post I\u2019ll use a simple linear regression model to explain two <b>machine</b> <b>learning</b> (ML) fundamentals; (1) cost functions and; (2) gradient descent. The linear regression isn\u2019t the most powerful model in the ML tool kit, but due to its familiarity and interpretability, it is still in widespread use in research and industry. Simply, linear regression is used to estimate linear relationships between continuous or/and categorical data and a continuous output variable \u2014 you can see an ...", "dateLastCrawled": "2022-01-30T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "PREDICTION OF RESEARCH TOPICS USING COMBINATION OF <b>MACHINE</b> <b>LEARNING</b> AND ...", "url": "http://www.jatit.org/volumes/Vol49No3/14Vol49No3.pdf", "isFamilyFriendly": true, "displayUrl": "www.jatit.org/volumes/Vol49No3/14Vol49No3.pdf", "snippet": "Extreme <b>Learning</b> <b>Machine</b> and Support Vector <b>Machine</b>. The prediction result is then finally refined by logistic <b>curve</b>. The dataset used in this study is a research report on Bioinformatics from Microsoft Research and NCBI (National Center for Biotechnology Information), over the past 30 years. Experimental result indicates that the combination of <b>machine</b> <b>learning</b> approaches and logistic-<b>curve</b> may improve the prediction accuracy. In addition, the emerging topic of the same dataset can be ...", "dateLastCrawled": "2021-11-21T16:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Measuring model loss</b> | <b>Machine</b> <b>Learning</b> for Finance", "url": "https://subscription.packtpub.com/book/data/9781789136364/1/ch01lvl1sec21/measuring-model-loss", "isFamilyFriendly": true, "displayUrl": "https://subscription.packtpub.com/.../9781789136364/1/ch01lvl1sec21/<b>measuring-model-loss</b>", "snippet": "In <b>machine</b> <b>learning</b>, a <b>loss</b> function measures how bad the model performs. A high <b>loss</b> function goes hand in hand with low accuracy, whereas if the function is low, then the model is doing well. In this case, our issue is a binary classification problem. Because of that, we will be using the binary cross-entropy <b>loss</b>, as we can see in the ...", "dateLastCrawled": "2021-12-21T06:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "B. <b>Analogy</b> C. Deduction D. Memorization Answer : A Explanation: Different <b>learning</b> methods does not include the introduction. 8. The model will be trained with data in one single batch is known as ? A. Batch <b>learning</b> B. Offline <b>learning</b> C. Both A and B D. None of the above Ans : C Explanation: we have end-to-end <b>Machine</b> <b>Learning</b> systems in which we need to train the model in one go by using whole available training data. Such kind of <b>learning</b> method or algorithm is called Batch or Offline ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "deep <b>learning</b> - How can both generator and discriminator losses ...", "url": "https://datascience.stackexchange.com/questions/32699/how-can-both-generator-and-discriminator-losses-decrease", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/32699", "snippet": "In the subsection of the figure, they compute the average <b>loss</b> across 100 iterations, which is why the <b>loss</b> is monotonically decreasing because on average the <b>loss</b> does decrease with the training. You are correct in inferring that if this was reported on an iteration to iteration basis, the <b>loss</b> would be a zig zag <b>curve</b>, which is less nice to look at than a smooth <b>curve</b>.", "dateLastCrawled": "2022-01-28T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Has anyone experience val <b>loss</b> curves like this? : learnmachinelearning", "url": "https://www.reddit.com/r/learnmachinelearning/comments/s2h03q/has_anyone_experience_val_loss_curves_like_this/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/learn<b>machinelearning</b>/comments/s2h03q/has_anyone_experience...", "snippet": "\ud83c\udfc3 Although a relatively simple optimization algorithm, gradient descent (and its variants) has found an irreplaceable place in the heart of <b>machine</b> <b>learning</b>. This is majorly due to the fact that it has shown itself to be quite handy when optimizing deep neural networks and other models. The models behind the latest advances in ML and computer vision are majorly optimized using gradient descent and its variants like Adam and gradient descent with momentum.", "dateLastCrawled": "2022-01-13T05:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "PINN deep <b>learning</b> method for the Chen\u2013Lee\u2013Liu equation: Rogue wave on ...", "url": "https://www.sciencedirect.com/science/article/pii/S1007570421003798", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1007570421003798", "snippet": "<b>Machine</b> <b>learning</b> with the neural network method ... Interestingly, from Fig. 5(d), we can observe that the <b>loss curve is like</b> \u201cstair\u201d, which does not exist in that one of periodic wave solution. 4.3. The data-driven soliton wave solution. As shown in Ref. , the expression (59) of Ref. will be the bright soliton solution with taking a = c = 1, \u03b2 = 0. 5, and be the dark soliton solution with taking a = c = 1, \u03b2 = \u2212 0. 5. Let [x 0, x 1] and [t 0, t 1] in Eq. as [\u2212 6. 0, 6. 0] and [\u2212 ...", "dateLastCrawled": "2022-01-17T00:12:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(loss curve)  is like +(learning curve)", "+(loss curve) is similar to +(learning curve)", "+(loss curve) can be thought of as +(learning curve)", "+(loss curve) can be compared to +(learning curve)", "machine learning +(loss curve AND analogy)", "machine learning +(\"loss curve is like\")", "machine learning +(\"loss curve is similar\")", "machine learning +(\"just as loss curve\")", "machine learning +(\"loss curve can be thought of as\")", "machine learning +(\"loss curve can be compared to\")"]}