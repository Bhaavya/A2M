{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Fairness (<b>machine</b> learning) - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Fairness_(machine_learning)", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/Fairness_(<b>machine</b>_learning)", "snippet": "<b>Equalized</b> <b>odds</b>, also referred to as conditional procedure accuracy equality and disparate mistreatment, is a measure of fairness in <b>machine</b> learning. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal true positive rate and equal false positive rate, satisfying the formula:", "dateLastCrawled": "2021-09-03T00:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Ensuring Fairness in <b>Machine</b> Learning to Advance Health Equity ...", "url": "https://europepmc.org/article/MED/30508424", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/30508424", "snippet": "This impossibility also holds for <b>equalized</b> <b>odds</b> and equal allocation, and for equal allocation and equal positive and negative predictive value . <b>Machine</b>-learning fairness is not just for <b>machine</b>-learning specialists to understand; it requires clinical and ethical reasoning to determine which type of fairness is appropriate for a given application and what level of it is satisfactory. Although no cookie-cutter solution exists, the examples and recommendations provide a starting point for ...", "dateLastCrawled": "2022-01-26T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Explainable AI: A Review of <b>Machine</b> Learning Interpretability Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "<b>Machine</b> Learning Fairness is a sub-domain of <b>machine</b> learning interpretability that focuses solely on the social and ethical impact of <b>machine</b> learning algorithms by evaluating them in terms impartiality and discrimination. The study of fairness in <b>machine</b> learning is becoming more broad and diverse, and it is progressing rapidly. Traditionally, the fairness of a <b>machine</b> learning system has been evaluated by checking the models\u2019 predictions and errors across certain demographic segments ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "Certain definitions will be more applicable in attempting to achieve certain goals, and <b>equalized</b> <b>odds</b> may not be the best in each scenario, but the ideals of our justice system and the potential benefits of <b>equalized</b> <b>odds</b> for defendants suggest that equalizing <b>odds</b> is at least as valid as adhering to predictive parity\u2014especially when the unequal <b>odds</b> currently fall along racial lines. Because of this, it is worth examining in greater detail how the law might facilitate and implement risk ...", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Fairness metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "Whereas the previous two metrics look at the probability of getting the favorable label and do not consider potentially existing group differences (e.g., a different percentage of class 1 observations between the groups), the following concepts of equal opportunity and <b>equalized</b> <b>odds</b> consider additional metrics based on binary classification that take these potential differences into account. Specifically, they look at true positive rates (TPR) and false positive rates (FPR). Equal ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Dense neural <b>networks in knee osteoarthritis classification: a study on</b> ...", "url": "https://link.springer.com/article/10.1007/s00521-020-05459-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00521-020-05459-5", "snippet": "To accomplish this, a hybrid criterion including accuracies, confusion matrix and two fairness metrics (demographic parity (DP) and balanced <b>equalized</b> <b>odds</b> (BEO)) were employed to validate the performance of the proposed methodology. Different subgroups of control participants from self-reported clinical data were considered to prove the performance of the proposed methodology. The best performing DNN method is compared with some popular and well-known <b>machine</b> learning techniques for ...", "dateLastCrawled": "2022-01-28T17:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "FR-Train: A Mutual Information-Based Approach to Fair and Robust Training", "url": "http://proceedings.mlr.press/v119/roh20a/roh20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/roh20a/roh20a.pdf", "snippet": "For sensitive applications <b>like</b> healthcare, \ufb01nance, and self-driving cars, a trained model must not discriminate cus- ... pact (Feldman et al. ,2015), <b>equalized</b> <b>odds</b> (Hardt et al. 2016), and equal opportunity (Hardt et al., ), which cap-ture various notions of discrimination. More recently, there has been a surge in unfairness mitigation techniques (Bel- lamy et al.,2018b), which improve the model fairness by either \ufb01xing the training data, training process, or trained model. Unfairness ...", "dateLastCrawled": "2022-01-23T10:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 7, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Fairness (machine learning</b>) - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Fairness_(machine_learning)", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Fairness_(machine_learning</b>)", "snippet": "Context. Research about fairness in <b>machine</b> learning is a relatively recent topic. In 2018, a majority of papers on the topic had been published in the preceding three years. That same year, IBM introduced AI Fairness 360, a Python library with several algorithms to reduce software bias and increase its fairness and Facebook made public their use of a tool, Fairness Flow, to detect bias in their AI. However, the source code of the tool is not accessible. In 2019, Google published a set of ...", "dateLastCrawled": "2022-02-03T01:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Fair prediction with disparate impact</b>: A study of bias in ... - DeepAI", "url": "https://deepai.org/publication/fair-prediction-with-disparate-impact-a-study-of-bias-in-recidivism-prediction-instruments", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fair-prediction-with-disparate-impact</b>-a-study-of-bias...", "snippet": "Risk assessment instruments are gaining increasing popularity within the criminal justice system, with versions of such instruments <b>being</b> used or considered for use in pre-trial decision-making, parole decisions, and in some states even sentencing [1, 2, 3]. In each of these cases, a high-risk classification\u2014particularly a high-risk misclassification\u2014may have a direct adverse impact on a criminal defendant\u2019s outcome. If the use of RPI\u2019s is to become commonplace, it is especially ...", "dateLastCrawled": "2022-01-13T20:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Diving Deep Into <b>Fair Synthetic Data Generation (Fairness Series</b> Part 5 ...", "url": "https://mostly.ai/blog/diving-deep-into-fair-synthetic-data-generation-fairness-series-part-5/", "isFamilyFriendly": true, "displayUrl": "https://<b>mostly.ai</b>/blog/diving-deep-into-<b>fair-synthetic-data-generation-fairness-series</b>...", "snippet": "One of the first ideas to try when creating a fair data set for <b>machine</b> learning is to drop the sensitive column. In the presented case that\u2019s the \u201csex\u201d attribute. At first sight, this sounds <b>like</b> a good and easy-to-implement solution but, unfortunately, it can actually cause more harm than good. On one hand, what makes this approach fail ...", "dateLastCrawled": "2022-02-03T06:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Ensuring Fairness in <b>Machine</b> Learning to Advance Health Equity ...", "url": "https://europepmc.org/article/MED/30508424", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/30508424", "snippet": "This impossibility also holds for <b>equalized</b> <b>odds</b> and equal allocation, and for equal allocation and equal positive and negative predictive value . <b>Machine</b>-learning fairness is not just for <b>machine</b>-learning specialists to understand; it requires clinical and ethical reasoning to determine which type of fairness is appropriate for a given application and what level of it is satisfactory. Although no cookie-cutter solution exists, the examples and recommendations provide a starting point for ...", "dateLastCrawled": "2022-01-26T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Residual Unfairness in Fair <b>Machine</b> Learning from Prejudiced Data", "url": "http://proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "snippet": "or <b>equalized</b> <b>odds</b> introduced by Hardt et al. (2016). The adjustment determines a fair policy Y\u02c6 from a (possibly dis- criminatory) black-box binary predictor or score R\u02c6 without access to the original training data. They identify two par-ticular types of fairness, equal opportunity and <b>equalized</b> <b>odds</b>, which require that a fairness-<b>adjusted</b> policy Y\u02c6 be independent of class A given a positive label Y =1or given any label Y, respectively. For loan approval, equality of opportunity requires ...", "dateLastCrawled": "2022-01-31T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "In this scenario, for the definition of <b>equalized</b> <b>odds</b> to be satisfied, black and white defendants must have the same <b>odds</b> of <b>being</b> misclassified as High Risk, and the same <b>odds</b> of <b>being</b> misclassified as Low Risk. To make this more concrete, it is helpful to examine what group of defendants the definition of <b>equalized</b> <b>odds</b> most benefits.", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Explainable AI: A Review of <b>Machine</b> Learning Interpretability Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "Based on the above, interpretability is mostly connected with the intuition behind the outputs of a model ; with the idea <b>being</b> that the more interpretable a <b>machine</b> learning system is, the easier it is to identify cause-and-effect relationships within the system\u2019s inputs and outputs. For example, in image recognition tasks, part of the reason that led a system to decide that a specific object is part of an image (output) could be certain dominant patterns in the image (input ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "The <b>adjusted</b> predictions and probabilities should match the distribution of an observed set of labels. candidate generation. #recsystems . The initial set of recommendations chosen by a recommendation system. For example, consider a bookstore that offers 100,000 titles. The candidate generation phase creates a much smaller list of suitable books for a particular user, say 500. But even 500 books is way too many to recommend to a user. Subsequent, more expensive, phases of a recommendation ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Towards Equity and Algorithmic Fairness in Student Grade ...", "url": "https://www.academia.edu/69838840/Towards_Equity_and_Algorithmic_Fairness_in_Student_Grade_Prediction", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69838840/Towards_Equity_and_Algorithmic_Fairness_in_Student...", "snippet": "A survey on datasets for fairness-aware <b>machine</b> learning. By Tai Le Quy. Assessing the Fairness of Intelligent Systems. By In\u00eas Valentim. An Enhanced Evolutionary Student Performance Prediction Model Using Whale Optimization Algorithm Boosted with Sine-Cosine Mechanism. By Thaer Thaher. Spatially Localized Perturbation GAN (SLP-GAN) for Generating Invisible Adversarial Patches. By Harashta Tatimma Larasati. Guaranteeing Correctness of <b>Machine</b> Learning Based Decision Making at Higher ...", "dateLastCrawled": "2022-01-29T08:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fairness metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "<b>Similar</b> to the <b>machine</b> learning domain, the concern of fairness is highly relevant in recommender systems; for example, bias can exist against certain groups of users, categories of items, etc. A more specific example can be potential gender-based discrimination on STEM course recommendation Pollack, 2013) or a possible bias in item recommendation (Zhu, Wang, &amp; Caverlee, 2020) based on age/gender of the user or the demographics of the seller etc. Other examples found in literature are ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "When <b>machine</b> learning models are <b>being</b> used to make decisions, they cannot be separated from the social and ethical context in which they are applied, and those developing and deploying these models must take care to do so in a manner that accounts for both accuracy and fairness. In this chapter, we will discuss sources of potential bias in the modeling pipeline, as well as some of the ways that bias introduced by a model can be measured, with a particular focus on classification problems ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Diving Deep Into <b>Fair Synthetic Data Generation (Fairness Series</b> Part 5 ...", "url": "https://mostly.ai/blog/diving-deep-into-fair-synthetic-data-generation-fairness-series-part-5/", "isFamilyFriendly": true, "displayUrl": "https://<b>mostly.ai</b>/blog/diving-deep-into-<b>fair-synthetic-data-generation-fairness-series</b>...", "snippet": "Let\u2019s start with a quick reminder: a data set or algorithm <b>being</b> unfair usually refers to some kind of imbalance. A ... women on average are less likely to commit future violent crimes than men with <b>similar</b> criminal records. So, a gender-neutral assessment can overestimate a woman\u2019s recidivism risk. Our synthetic data platform&#39;s community version is free to use and leverages deep neural networks to produce synthetic data. In order to generate fair synthetic data, we add a fairness ...", "dateLastCrawled": "2022-02-03T06:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Ultimate Data Science Flashcards | Quizlet", "url": "https://quizlet.com/474731310/ultimate-data-science-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/474731310/ultimate-data-science-flash-cards", "snippet": "Contrast with <b>equalized</b> <b>odds</b> and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain specified ground-truth labels to depend on sensitive attributes. See &quot;Attacking discrimination with smarter <b>machine</b> learning&quot; for a visualization exploring the tradeoffs when optimizing for demographic parity.", "dateLastCrawled": "2021-06-24T10:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "Certain definitions will be more applicable in attempting to achieve certain goals, and <b>equalized</b> <b>odds</b> may not be the best in each scenario, but the ideals of our justice system and the potential benefits of <b>equalized</b> <b>odds</b> for defendants suggest that equalizing <b>odds</b> is at least as valid as adhering to predictive parity\u2014especially when the unequal <b>odds</b> currently fall along racial lines. Because of this, it is worth examining in greater detail how the law might facilitate and implement risk ...", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Inequality in Teaching and Schooling: How Opportunity Is Rationed to ...", "url": "https://www.ncbi.nlm.nih.gov/books/NBK223640/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/books/NBK223640", "snippet": "Despite the rhetoric of American equality, the school experiences of African-American and other \u201cminority\u201d students in the United States continue to be substantially separate and unequal. Few Americans realize that the U.S. educational system is one of the most unequal in the industrialized world, and that students routinely receive dramatically different learning opportunities based on their social status. In contrast to European and Asian nations that fund schools centrally and equally ...", "dateLastCrawled": "2022-02-02T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Equality of Opportunity</b> (<b>Stanford Encyclopedia of Philosophy</b>)", "url": "https://plato.stanford.edu/entries/equal-opportunity/", "isFamilyFriendly": true, "displayUrl": "https://<b>plato.stanford.edu</b>/entries/equal-opportunity", "snippet": "If the Kantian doctrine of right <b>can</b> be defended, then we have good grounds to uphold a version of formal equal opportunity without <b>being</b> under normative pressure of good reasons to go further and embrace any substantive equal opportunity doctrine. In a nutshell, the Kantian claim will be that embracing more expansive notions of equal opportunity, substantive equal opportunity or the yet more expansive luck egalitarian family of doctrines, will prove incompatible with maintaining formal ...", "dateLastCrawled": "2022-01-31T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Algorithmic Fairness \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2001.09784/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2001.09784", "snippet": "<b>Equalized</b> <b>odds</b> \u2013 This measure was designed by (Hardt et al., 2016) to overcome the disadvantages of measures such as disparate impact and demographic parity. The measure computes the difference between the false positive rates (FPR), and the difference between the true positive rates (TPR) of the two groups. Formally, this measure is computed as follows:", "dateLastCrawled": "2021-12-30T11:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Equality of Opportunity in Supervised Learning</b>", "url": "https://www.researchgate.net/publication/308980568_Equality_of_Opportunity_in_Supervised_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/308980568_Equality_of_Opportunity_in...", "snippet": "In <b>Equalized</b> <b>Odds</b> (a.k.a. Positive Rate Parity) (see [HPS16]), we say that a classifier h \u2208 H is fair with respect to the distribution P on X \u2212S \u00d7 S \u00d7 [K] if h(X) and S are independent ...", "dateLastCrawled": "2022-01-15T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "This article seeks to provide an overview of the different schools of <b>thought</b> and approaches to mitigating (social) biases and increase fairness in the <b>Machine</b> Learning literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems,", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "When <b>machine</b> learning models are <b>being</b> used to make decisions, they cannot be separated from the social and ethical context in which they are applied, and those developing and deploying these models must take care to do so in a manner that accounts for both accuracy and fairness. In this chapter, we will discuss sources of potential bias in the modeling pipeline, as well as some of the ways that bias introduced by a model <b>can</b> be measured, with a particular focus on classification problems ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Fairness metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "The first step in the process was to formulate a research question to define the aim of the study and map answers from the literature. The research question that this semi-systematic literature review address is the RQ1, presented in Section 1.It aims to determine what concepts from classification-based fair <b>machine</b> learning <b>can</b> be mapped to fairness metrics in rating-based recommender system settings.", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fighting Money Laundering with Statistics and <b>Machine</b> Learning: An ...", "url": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-machine-learning-an-introduction-and-review", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-<b>machine</b>...", "snippet": "Deep learning is increasingly <b>being</b> used in high-stake decision making a... Mengnan Du, et al. \u2219. share 0 research \u2219 06/11/2021. What <b>Can</b> Knowledge Bring to <b>Machine</b> Learning? \u2013 A Survey of Low-shot Learning for Structured Data Supervised <b>machine</b> learning has several drawbacks that make it difficult... Yang Hu, et al. \u2219. share 0 research \u2219 05/16/2016. Identification of promising research directions using <b>machine</b> learning aided medical literature analysis The rapidly expanding corpus ...", "dateLastCrawled": "2022-01-28T21:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Scandinavian Journal of Work, Environment &amp; Health - Loss of permanent ...", "url": "https://www.sjweh.fi/show_abstract.php?abstract_id=3646&fullText=1", "isFamilyFriendly": true, "displayUrl": "https://www.sjweh.fi/show_abstract.php?abstract_id=3646&amp;fullText=1", "snippet": "Objective Precarious employment is associated with worse mental health, but it is unclear whether changes in employment status are related to suicidal behaviors. This study examined the association between change in employment status and suicidal ideation among workers in South Korea. Methods To maximize power of the analysis, we combined data from the ongoing Korean Welfare Panel Study. We analyzed 3793 participants who were permanent workers at baseline (2011\u20132014) and who either: (i ...", "dateLastCrawled": "2022-01-31T11:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Ensuring Fairness in <b>Machine</b> Learning to Advance Health Equity ...", "url": "https://europepmc.org/article/MED/30508424", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/30508424", "snippet": "<b>Machine</b> learning <b>can</b> identify the statistical patterns of data generated by tens of thousands of physicians and billions of patients ... -positive rate for this group, manifesting as more false alarms and subsequent class-specific alert fatigue. Likewise, <b>equalized</b> <b>odds</b> <b>can</b> be achieved by lowering accuracy for the nonprotected group , which undermines the principle of beneficence. Equal Allocation. Predictions are often used to allocate resources, such as in case 2, in which some patients ...", "dateLastCrawled": "2022-01-26T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Explainable AI: A Review of <b>Machine</b> Learning Interpretability Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "<b>Machine</b> Learning Fairness is a sub-domain of <b>machine</b> learning interpretability that focuses solely on the social and ethical impact of <b>machine</b> learning algorithms by evaluating them in terms impartiality and discrimination. The study of fairness in <b>machine</b> learning is becoming more broad and diverse, and it is progressing rapidly. Traditionally, the fairness of a <b>machine</b> learning system has been evaluated by checking the models\u2019 predictions and errors across certain demographic segments ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "Certain definitions will be more applicable in attempting to achieve certain goals, and <b>equalized</b> <b>odds</b> may not be the best in each scenario, but the ideals of our justice system and the potential benefits of <b>equalized</b> <b>odds</b> for defendants suggest that equalizing <b>odds</b> is at least as valid as adhering to predictive parity\u2014especially when the unequal <b>odds</b> currently fall along racial lines. Because of this, it is worth examining in greater detail how the law might facilitate and implement risk ...", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "Equal Opportunity: As parity and disparate impact do not consider potential differences in groups that are <b>being</b> <b>compared</b>, [hardt2016, pleiss2017] consider additional metrics that make use of the fpr and tpr between groups. Specifically, an algorithm is considered to be fair under equal opportunity if its tpr is the same across different groups. Pr(^y=1 \u2014y=1 &amp; g_i) &amp;= Pr(^y=1 \u2014 y=1 &amp; g_j)", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Towards Equity and Algorithmic Fairness in Student Grade ...", "url": "https://www.academia.edu/69838840/Towards_Equity_and_Algorithmic_Fairness_in_Student_Grade_Prediction", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69838840/Towards_Equity_and_Algorithmic_Fairness_in_Student...", "snippet": "A survey on datasets for fairness-aware <b>machine</b> learning. By Tai Le Quy. Assessing the Fairness of Intelligent Systems. By In\u00eas Valentim. An Enhanced Evolutionary Student Performance Prediction Model Using Whale Optimization Algorithm Boosted with Sine-Cosine Mechanism. By Thaer Thaher. Spatially Localized Perturbation GAN (SLP-GAN) for Generating Invisible Adversarial Patches. By Harashta Tatimma Larasati. Guaranteeing Correctness of <b>Machine</b> Learning Based Decision Making at Higher ...", "dateLastCrawled": "2022-01-29T08:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Inequality in Teaching and Schooling: How Opportunity Is Rationed to ...", "url": "https://www.ncbi.nlm.nih.gov/books/NBK223640/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/books/NBK223640", "snippet": "In 1993, a recent school dropout who was black had only a one in four chance of <b>being</b> employed, whereas the <b>odds</b> for his or her white counterpart were about 50% (NCES, 1995, p. 88). Even recent graduates from high school struggle to find jobs. Among African-American high school graduates not enrolled in college, only 42% were employed in 1993, as <b>compared</b> with 72% of white graduates. Those who do not succeed in school are becoming part of a growing underclass, cut off from productive ...", "dateLastCrawled": "2022-02-02T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Bias Mitigation <b>Post-processing</b> for Individual and Group Fairness - DeepAI", "url": "https://deepai.org/publication/bias-mitigation-post-processing-for-individual-and-group-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/bias-mitigation-<b>post-processing</b>-for-individual-and...", "snippet": "EOP, <b>being</b> designed for <b>equalized</b> <b>odds</b> rather than disparate impact, cannot be optimized for ranges of disparate impact. In the subsections that follow, we first demonstrate the efficacy of the individual bias detector used in the proposed IGD algorithm and then compare the three algorithms for classification accuracy, disparate impact, and individual fairness.", "dateLastCrawled": "2022-01-08T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Chapter 11 Bias and Fairness</b> | Big Data and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "When <b>machine</b> learning models are <b>being</b> used to make decisions, they cannot be separated from the social and ethical context in which they are applied, and those developing and deploying these models must take care to do so in a manner that accounts for both accuracy and fairness. In this chapter, we will discuss sources of potential bias in the modeling pipeline, as well as some of the ways that bias introduced by a model <b>can</b> be measured, with a particular focus on classification problems ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Diving Deep Into <b>Fair Synthetic Data Generation (Fairness Series</b> Part 5 ...", "url": "https://mostly.ai/blog/diving-deep-into-fair-synthetic-data-generation-fairness-series-part-5/", "isFamilyFriendly": true, "displayUrl": "https://<b>mostly.ai</b>/blog/diving-deep-into-<b>fair-synthetic-data-generation-fairness-series</b>...", "snippet": "These predictive models output the probability of <b>being</b> high-income for any data point, so we <b>can</b> look at how these probabilities are distributed. Since there are more low-income samples, we expect these probabilities to be concentrated close to 0, both for females and males. However, for the model fitted on the original data, we see below that there is a much higher number of around-0 probabilities for females than males (Figure 11).", "dateLastCrawled": "2022-02-03T06:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Accuracy of the Sequential Organ Failure Assessment Score for In ...", "url": "https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2781190", "isFamilyFriendly": true, "displayUrl": "https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2781190", "snippet": "The <b>odds</b> of death for Black <b>compared</b> with White patients were lower in the highest priority tier (ie, priority 1) and lowest priority tier (ie, priority 3 for system B and priority 4 for systems A and C) of the 3 systems, regardless of the Sequential Organ Failure Assessment score threshold used (ie, for systems A, B, and C, scores of 6, 8, and 9, respectively, for highest priority; 12, 12, and 15, respectively, for lowest priority). Table 1. Number of Patient Encounters and Mortality by ...", "dateLastCrawled": "2022-02-02T13:05:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A <b>machine learning</b> technique that iteratively combines a set of simple and not very accurate classifiers ... Contrast with <b>equalized</b> <b>odds</b> and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain specified ground-truth labels to depend on sensitive attributes. See &quot;Attacking discrimination with smarter <b>machine learning</b>&quot; for a visualization exploring the tradeoffs when optimizing for ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Residual Unfairness in Fair <b>Machine</b> <b>Learning</b> from Prejudiced Data", "url": "http://proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "snippet": "chine <b>learning</b> has considered some subcomponents of the overall problem we study of <b>learning</b> fair policies from bi-ased datasets. Hardt et al. (2016) formalize the criteria of equal opportunity and <b>equalized</b> <b>odds</b>. Lum &amp; Isaac (2016) show that a predictive policing algorithm for drug enforce-ment in Oakland, trained on police records, will ...", "dateLastCrawled": "2022-01-31T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Survey on Bias and Fairness in <b>Machine</b> <b>Learning</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1908.09635/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.09635", "snippet": "With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in <b>machine</b> <b>learning</b>, natural language ...", "dateLastCrawled": "2021-11-15T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "We embed the evaluation of AI fairness within the best practices of <b>machine</b> <b>learning</b> development and operations such as version control, ... This includes measures such as Demographic Parity / Statistical Parity (Dwork et al., 2012), <b>Equalized</b> <b>Odds</b> Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Mitigating Unwanted Biases with Adversarial Learning</b> | DeepAI", "url": "https://deepai.org/publication/mitigating-unwanted-biases-with-adversarial-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>mitigating-unwanted-biases-with-adversarial-learning</b>", "snippet": "<b>Machine</b> <b>learning</b> leverages data to build models capable of assessing the labels and properties of novel data. Unfortunately, the available training data frequently contains biases with respect to things that we would rather not use for decision making. <b>Machine</b> <b>learning</b> builds models faithful to training data and can lead to perpetuating these undesirable biases. For example, systems designed to predict creditworthiness and systems designed to perform <b>analogy</b> completion have been demonstrated ...", "dateLastCrawled": "2021-12-10T20:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Mitigating Unwanted Biases with Adversarial Learning</b>", "url": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concern- ing demographic groups are in the training data, well-trained models will re\ufb02ect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an ad-versary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or in ...", "dateLastCrawled": "2021-12-17T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Papers on fairness in <b>machine</b> <b>learning</b>, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about fairness as it might apply to an ML process at hand, common language \u2013 and common sense \u2013 will do just fine. If, after analyzing your use case, you judge that the more technical results are relevant to the process in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On Predicting Recidivism: Epistemic Risk, Tradeoffs, and Values in ...", "url": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on-predicting-recidivism-epistemic-risk-tradeoffs-and-values-in-machine-learning/7E541FA03E78C3141A65EA99A0CA6E9A", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on...", "snippet": "This paper examines the role of value judgments in the design of <b>machine</b>-<b>learning</b> (ML) systems generally and in recidivism-prediction algorithms specifically. Drawing on work on inductive and epistemic risk, the paper argues that ML systems are value laden in ways similar to human decision making, because the development and design of ML systems requires human decisions that involve tradeoffs that reflect values. In many cases, these decisions have significant\u2014and, in some cases, disparate ...", "dateLastCrawled": "2022-01-26T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Measuring discrimination in algorithmic <b>decision making</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10618-017-0506-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10618-017-0506-1", "snippet": "A <b>machine</b> <b>learning</b> algorithm is a procedure used for producing a predictive model from historical data. A model is a collection of decision rules used for <b>decision making</b> for new incoming data. The model would take personal characteristics as inputs (for example, income, credit history, employment status), and produce a prediction (for example, credit risk level). Fig. 1. A typical <b>machine</b> <b>learning</b> setting. Full size image. <b>Learning</b> algorithms as such cannot discriminate, because they are ...", "dateLastCrawled": "2022-01-29T20:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(equalized odds)  is like +(machine that is being adjusted)", "+(equalized odds) is similar to +(machine that is being adjusted)", "+(equalized odds) can be thought of as +(machine that is being adjusted)", "+(equalized odds) can be compared to +(machine that is being adjusted)", "machine learning +(equalized odds AND analogy)", "machine learning +(\"equalized odds is like\")", "machine learning +(\"equalized odds is similar\")", "machine learning +(\"just as equalized odds\")", "machine learning +(\"equalized odds can be thought of as\")", "machine learning +(\"equalized odds can be compared to\")"]}