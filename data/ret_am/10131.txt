{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "regression - What does it mean <b>L1</b> <b>loss</b> is not differentiable? - Cross ...", "url": "https://stats.stackexchange.com/questions/429720/what-does-it-mean-l1-loss-is-not-differentiable", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/429720/what-does-it-mean-<b>l1</b>-<b>loss</b>-is-not...", "snippet": "<b>L 1</b> <b>loss</b> uses the absolute <b>value</b> of the <b>difference</b> <b>between</b> <b>the predicted</b> <b>and the actual</b> <b>value</b> to measure the <b>loss</b> (or the error) made by the model. The absolute <b>value</b> (or the modulus function), i.e. f ( x) = | x | is not differentiable is the way of saying that its derivative is not defined for its whole domain.", "dateLastCrawled": "2022-01-25T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss functions</b> - GitHub Pages", "url": "https://kharshit.github.io/blog/2018/08/24/loss-functions", "isFamilyFriendly": true, "displayUrl": "https://kharshit.github.io/blog/2018/08/24/<b>loss-functions</b>", "snippet": "It minimizes the squared <b>difference</b> <b>between</b> <b>the predicted</b> <b>value</b> <b>and the actual</b> <b>value</b>. The residual sum of squares is defined as: The residual sum of squares is defined as: \\[RSS = \\sum_{i=1}^n(y_{i} - \\hat{y_i})^2\\]", "dateLastCrawled": "2022-01-25T18:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "It represents the <b>difference</b> <b>between</b> the original and <b>predicted</b> values extracted by averaging the absolute <b>difference</b> over the data set. ... Cross-entropy <b>loss</b> measures the performance of a classification model whose output is a probability <b>value</b> <b>between</b> 0 and 1. Cross-entropy <b>loss</b> increases as <b>the predicted</b> probability diverge from the <b>actual</b> label. So predicting a probability of .012 when the <b>actual</b> observation label is 1 would be bad and result in a high <b>loss</b> <b>value</b>. A perfect model would ...", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How <b>To Build Custom Loss Functions In Keras</b> For Any Use Case | cnvrg.io", "url": "https://cnvrg.io/keras-custom-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://cnvrg.io/keras-custom-<b>loss</b>-functions", "snippet": "Mean Absolute error, also known as <b>L1</b> Error, is defined as the average of the absolute differences <b>between</b> the <b>actual</b> <b>value</b> and <b>the predicted</b> <b>value</b>. This is the average of the absolute <b>difference</b> <b>between</b> <b>the predicted</b> <b>and the actual</b> <b>value</b>. Mathematically, it can be shown as:", "dateLastCrawled": "2022-01-31T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Quickly Master <b>L1</b> vs L2 Regularization - ML Interview Q&amp;A", "url": "https://analyticsarora.com/quickly-master-l1-vs-l2-regularization-ml-interview-qa/", "isFamilyFriendly": true, "displayUrl": "https://analyticsarora.com/quickly-master-<b>l1</b>-vs-l2-regularization-ml-interview-qa", "snippet": "The <b>loss</b> function is used to measure the <b>difference</b> <b>between</b> the <b>actual</b> <b>value</b> and <b>the predicted</b> output. The coefficients are chosen in such a way that they minimize the <b>loss</b> function subject to the training data. Suppose there is noise in the training data. In that case, the coefficients will not be able to generalize well to the future data, meaning that the model cannot correctly predict the output or target column for test data. By noise, we mean irrelevant information or randomness in a ...", "dateLastCrawled": "2022-01-23T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Professionals Point: <b>Loss Functions in Machine Learning (MAE</b>, MSE ...", "url": "https://theprofessionalspoint.blogspot.com/2019/02/loss-functions-in-machine-learning-mae.html", "isFamilyFriendly": true, "displayUrl": "https://theprofessionalspoint.blogspot.com/2019/02/<b>loss-functions-in-machine-learning</b>...", "snippet": "<b>Loss</b> Function indicates the <b>difference</b> <b>between</b> the <b>actual</b> <b>value</b> and <b>the predicted</b> <b>value</b>. If the magnitude of the <b>loss</b> function is high, it means our algorithm is showing a lot of variance in the result and needs to be corrected. Lets look into the types of <b>loss functions in Machine Learning</b> in detail. There are broadly two types of losses based on the type of algorithm we are using: Types of Losses: 1. Regression Losses. 2. Classification Losses. Lets first discuss regression losses: 1. Mean ...", "dateLastCrawled": "2022-01-29T00:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>L1</b> Regularization or Lasso or <b>L1</b> norm - 911 WeKnow", "url": "https://911weknow.com/l1-regularization-or-lasso-or-l1-norm", "isFamilyFriendly": true, "displayUrl": "https://911weknow.com/<b>l1</b>-regularization-or-lasso-or-<b>l1</b>-norm", "snippet": "<b>Loss</b> function is the sum of squared <b>difference</b> <b>between</b> the <b>actual</b> <b>value</b> and <b>the predicted</b> <b>value</b>. <b>Loss</b> function for a linear regression with 4 input variables. In the equation i=4 . As the degree of the input features increases the model becomes complex and tries to fit all the data points as shown below. When we penalize the weights ?_3 and ?_4 and make them too small, very close to zero. It makes those terms negligible and helps simplify the model. Regularization works on assumption that ...", "dateLastCrawled": "2022-01-23T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Loss Function For Regression Problem in Machine</b> Learning \u2014 Python ...", "url": "https://laptrinhx.com/loss-function-for-regression-problem-in-machine-learning-python-implementation-using-numpy-and-2592355124/", "isFamilyFriendly": true, "displayUrl": "https://laptrinhx.com/<b>loss-function-for-regression-problem-in-machine</b>-learning-python...", "snippet": "This is typically expressed as a <b>difference</b> or distance <b>between</b> <b>the predicted</b> <b>value</b> <b>and the actual</b> <b>value</b>. There are many types of Cost Function area present in Machine Learning. These are the following some examples: Cross-entropy cost; Mean Squared Error; Root Mean Square Error; Hellinger Distance; Here are I am mentioned some <b>Loss</b> Function that is commonly used in Machine Learning for Regression Problems. Let\u2019s Get Start. Before I get started let\u2019s see some notation that is commonly ...", "dateLastCrawled": "2021-12-18T18:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Loss Function</b> - Pipline", "url": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/loss-function", "isFamilyFriendly": true, "displayUrl": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/<b>loss-function</b>", "snippet": "It behaves as <b>L1</b>-<b>loss</b> when the absolute <b>value</b> of the argument is high, and it behaves <b>like</b> L2-<b>loss</b> when the absolute <b>value</b> of the argument is close to zero. Smooth <b>L1</b>-<b>loss</b> combines the advantages of <b>L1</b>-<b>loss</b> (steady gradients for large values of \ud835\udc65) and L2-<b>loss</b> (less oscillations during updates when \ud835\udc65 is small).", "dateLastCrawled": "2022-01-24T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Visualization of Loss Functions for Deep Learning with Tensorflow</b> | by ...", "url": "https://medium.com/@risingdeveloper/visualization-of-some-loss-functions-for-deep-learning-with-tensorflow-9f60be9d09f9", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@risingdeveloper/visualization-of-some-<b>loss</b>-functions-for-deep...", "snippet": "This is j u st the square of the <b>difference</b>/distance <b>between</b> <b>the predicted</b> <b>value</b> and the true <b>value</b>. The L2 norm <b>loss</b> is good because it is curved or seem to converge near the target. Implementing ...", "dateLastCrawled": "2022-01-26T23:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Visualization of Loss Functions for Deep Learning with Tensorflow</b> | by ...", "url": "https://medium.com/@risingdeveloper/visualization-of-some-loss-functions-for-deep-learning-with-tensorflow-9f60be9d09f9", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@risingdeveloper/visualization-of-some-<b>loss</b>-functions-for-deep...", "snippet": "This is j u st the square of the <b>difference</b>/distance <b>between</b> <b>the predicted</b> <b>value</b> and the true <b>value</b>. The L2 norm <b>loss</b> is good because it is curved or seem to converge near the target. Implementing ...", "dateLastCrawled": "2022-01-26T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "The distinction is the <b>difference</b> <b>between</b> <b>predicted</b> and <b>actual</b> probability. This adds data about information <b>loss</b> in the model training. The farther away <b>the predicted</b> probability distribution is ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss Function</b> - Pipline", "url": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/loss-function", "isFamilyFriendly": true, "displayUrl": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/<b>loss-function</b>", "snippet": "The distinction is the <b>difference</b> <b>between</b> <b>predicted</b> and <b>actual</b> probability. This adds data about information <b>loss</b> in the model training. The farther away <b>the predicted</b> probability distribution is from the true probability distribution, greater is the <b>loss</b>. It does not penalize the model based on the confidence of prediction, as in cross entropy <b>loss</b>, but how different is the prediction from ground truth. It usually outperforms mean square error, especially when data is not normally ...", "dateLastCrawled": "2022-01-24T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "LossFunctions in Deep Learning-DeepVidhya", "url": "https://deepvidhya.com/blog/lossfunctions-in-deep-learning-1205", "isFamilyFriendly": true, "displayUrl": "https://deepvidhya.com/blog/<b>loss</b>functions-in-deep-learning-1205", "snippet": "It can be seen that the sigmoid function smooths <b>the predicted</b> <b>value</b> (like directly entering 0.1 and 0.01 and entering 0.1, 0.01 sigmoid and then entering, the last obviously has a lot smaller change <b>value</b>), which makes <b>the predicted</b> <b>value</b> of sigmoid far from the label <b>loss</b> growth is not so steep.", "dateLastCrawled": "2022-01-28T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "python - <b>difference</b> <b>between</b> <b>penalty</b> and <b>loss</b> parameters in Sklearn ...", "url": "https://stackoverflow.com/questions/25042909/difference-between-penalty-and-loss-parameters-in-sklearn-linearsvc-library", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/25042909", "snippet": "In machine learning, <b>loss</b> function measures the quality of your solution, while <b>penalty</b> function imposes some constraints on your solution. Specifically, Let X be your data, and y be labels of your data. Then <b>loss</b> function V(f(X),y) measures how well your model f maps your data to the labels. Here, f(X) is a vector of <b>predicted</b> labels. <b>L1</b> and L2 norms are commonly used and intuitively understood <b>loss</b> functions (see *", "dateLastCrawled": "2022-01-15T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fighting Overfitting With <b>L1</b> or L2 <b>Regularization</b>: Which One Is Better ...", "url": "https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/fighting-overfitting-with-<b>l1</b>-or-l2-<b>regularization</b>", "snippet": "Look at the alpha <b>value</b> of the ridge regression model \u2013 it\u2019s 100. The larger the hyperparameter <b>value</b> alpha, the closer the values will be to 0, without becoming 0. Which is better \u2013 <b>L1</b> or L2 <b>regularization</b>? Whether one <b>regularization</b> method is better than the other is a question for academics to debate. However, as a practitioner, there are some important factors to consider when you need to choose <b>between</b> <b>L1</b> and L2 <b>regularization</b>. I\u2019ve divided them into 6 categories, and will show ...", "dateLastCrawled": "2022-01-28T19:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Loss Functions and Optimization Algorithms</b> - XpertUp", "url": "https://www.xpertup.com/blog/machine-learning/loss-functions-and-optimization-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.xpertup.com/blog/machine-learning/<b>loss-functions-and-optimization-algorithms</b>", "snippet": "Similarly, a small <b>value</b> indicates a more certain distribution. We want to minimize the <b>value</b> of uncertainty. This is also called Log-<b>Loss</b>. For calculating the probability (P), we can make use of sigmoid function. Z is a function of our input features: 2. Hinge <b>Loss</b>. It is mostly used in SVM problems which have class labels as -1 and 1 instead ...", "dateLastCrawled": "2022-01-09T08:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What Is Regularization in Machine Learning? Techniques &amp; Methods", "url": "https://www.analytixlabs.co.in/blog/regularization-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.analytixlabs.co.in/blog/regularization-in-machine-learning", "snippet": "Remember, bias is the <b>difference</b> <b>between</b> <b>the predicted</b> <b>and the actual</b> values. With the increase in bias to the model, the variance (which is the <b>difference</b> <b>between</b> the predictions when the model fits different datasets.) decreases. And, by decreasing the variance, the overfitting gets reduced.", "dateLastCrawled": "2022-02-02T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Cost Functions In Machine Learning</b> - The Click Reader", "url": "https://www.theclickreader.com/cost-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.theclickreader.com/<b>cost-functions-in-machine-learning</b>", "snippet": "Therefore, the cost function gives the <b>value</b> of how far <b>the predicted</b> <b>value</b> is from the <b>actual</b> <b>value</b> of the model and tries to adjust the parameters so that the model becomes better. The goal of a machine learning or a deep learning model is hence to find the best set of parameters through an iterative process that minimizes the cost function until it cannot be minimized further.", "dateLastCrawled": "2022-02-02T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>ML - Different Regression types - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/ml-different-regression-types/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/ml-different-regression-types", "snippet": "Find the <b>difference</b> <b>between</b> the <b>actual</b> y and <b>predicted</b> y <b>value</b>(y = mx + c), for a given x. Square this <b>difference</b>. Find the mean of the squares for every <b>value</b> in X. Here y is the <b>actual</b> <b>value</b> and y\u2019 is <b>the predicted</b> <b>value</b>. Lets substitute the <b>value</b> of y\u2019:", "dateLastCrawled": "2022-01-30T14:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "We <b>can</b> achieve this using the Huber <b>Loss</b> (Smooth <b>L1</b> <b>Loss</b>), a combination of <b>L1</b> (MAE) and L2 (MSE) losses. <b>Can</b> be called Huber <b>Loss</b> or Smooth MAE Less sensitive to outliers in data than the squared ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "Mean Square Error/Quadratic <b>Loss</b>/L2 <b>Loss</b>: averages the squared <b>difference</b> <b>between</b> predictions and ground truth, with a focus on the average magnitudes of errors regardless of direction. Mean Absolute Error, <b>L1</b> <b>Loss</b> (used by PerceptiLabs\u2019 Regression component): sums the absolute differences <b>between</b> the predictions and ground truth, and finds the average.", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Cost Function and <b>Loss</b> Function in Machine Learning - Shishir Kant Singh", "url": "http://shishirkant.com/cost-function-and-loss-function-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "shishirkant.com/cost-function-and-<b>loss</b>-function-in-machine-learning", "snippet": "Huber <b>Loss</b>. A comparison <b>between</b> <b>L1</b> and L2 <b>loss</b> yields the following results: <b>L1</b> <b>loss</b> is more robust than its counterpart. On taking a closer look at the formulas, one <b>can</b> observe that if the <b>difference</b> <b>between</b> <b>the predicted</b> <b>and the actual</b> <b>value</b> is high, L2 <b>loss</b> magnifies the effect when compared to <b>L1</b>. Since L2 succumbs to outliers, <b>L1</b> <b>loss</b> ...", "dateLastCrawled": "2022-01-29T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> <b>Function</b> and Cost <b>Function</b> in Neural Networks | by Simran ...", "url": "https://medium.com/analytics-vidhya/loss-function-and-cost-function-in-neural-networks-4ade0c9ccb18", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>loss</b>-<b>function</b>-and-cost-<b>function</b>-in-neural-networks...", "snippet": "Now, it quite obvious that <b>the predicted</b> <b>value</b> would\u2019t be same as <b>actual</b> <b>value</b> only in one epoch, there would be some deviation. This is called <b>loss</b>, now this <b>loss</b> will be optimized by optimizer ...", "dateLastCrawled": "2022-01-30T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Choosing and Customizing Loss Functions for Image Processing</b>", "url": "https://blog.perceptilabs.com/choosing-and-customizing-loss-functions-for-image-processing/", "isFamilyFriendly": true, "displayUrl": "https://blog.perceptilabs.com/<b>choosing-and-customizing-loss-functions-for-image-processing</b>", "snippet": "Mean Square Error/Quadratic <b>Loss</b>/L2 <b>Loss</b>: averages the squared <b>difference</b> <b>between</b> predictions and ground truth, with a focus on the average magnitudes of errors regardless of direction. Mean Absolute Error, <b>L1</b> <b>Loss</b> (used by PerceptiLabs&#39; Regression component): sums the absolute differences <b>between</b> the predictions and ground truth, and finds the average.", "dateLastCrawled": "2022-01-28T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Cutting Your Losses: <b>Loss</b> Functions &amp; the Sum of <b>Squared</b> Errors <b>Loss</b> ...", "url": "https://dustinstansbury.github.io/theclevermachine/cutting-your-losses", "isFamilyFriendly": true, "displayUrl": "https://dustinstansbury.github.io/theclevermachine/cutting-your-<b>loss</b>es", "snippet": "The <b>difference</b> <b>between</b> <b>the predicted</b> and <b>actual</b> <b>value</b> is often referred to as the model \u201cerror\u201d or \u201cresidual\u201d \\(e_i\\) for the datapoint. The semantics here being that small errors correspond to small distances. Each of the \\(M\\) distances are then aggregated across the entire dataset through addition, giving a single number indicating how well (or badly) the current model <b>function</b> captures the structure of the entire dataset. The \u201cbest\u201d model will minimize the SSE and is called the", "dateLastCrawled": "2022-01-31T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fighting Overfitting With <b>L1</b> or L2 <b>Regularization</b>: Which One Is Better ...", "url": "https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/fighting-overfitting-with-<b>l1</b>-or-l2-<b>regularization</b>", "snippet": "Look at the alpha <b>value</b> of the ridge regression model \u2013 it\u2019s 100. The larger the hyperparameter <b>value</b> alpha, the closer the values will be to 0, without becoming 0. Which is better \u2013 <b>L1</b> or L2 <b>regularization</b>? Whether one <b>regularization</b> method is better than the other is a question for academics to debate. However, as a practitioner, there are some important factors to consider when you need to choose <b>between</b> <b>L1</b> and L2 <b>regularization</b>. I\u2019ve divided them into 6 categories, and will show ...", "dateLastCrawled": "2022-01-28T19:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How <b>To Build Custom Loss Functions In Keras</b> For Any Use Case | cnvrg.io", "url": "https://cnvrg.io/keras-custom-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://cnvrg.io/keras-custom-<b>loss</b>-functions", "snippet": "So our goal is to minimize the <b>difference</b> <b>between</b> <b>the predicted</b> <b>value</b> which is h ... This <b>loss</b> function has 2 parts. If our <b>actual</b> label is 1, the equation after \u2018+\u2019 becomes 0 because 1-1 = 0. So <b>loss</b> when our label is 1 is And when our label is 0, then the first part becomes 0. So our <b>loss</b> in that case would be. This <b>loss</b> function is also known as the Log <b>Loss</b> function, because of the logarithm of the <b>loss</b>. Standalone Implementation: You <b>can</b> create an object for Binary Cross Entropy ...", "dateLastCrawled": "2022-01-31T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Loss</b> Functions in Machine Learning: An Easy Overview(2021)", "url": "https://www.jigsawacademy.com/blogs/ai-ml/loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.<b>jigsawacademy</b>.com/blogs/ai-ml/<b>loss</b>-function", "snippet": "MSE <b>loss</b> performs as outlined because of the average of absolute variations <b>between</b> the particular and also the foretold <b>value</b>. It\u2019s the second most ordinarily used Regression <b>loss</b> function. The function <b>value</b> is the Mean of these Absolute Errors (MAE). The MAE <b>Loss</b> function is additional strong to outliers compared to the MSE <b>Loss</b> function. Therefore, it ought to be used if the information is liable to several outliers. The logistic <b>loss</b> function has nice mathematical properties, which ...", "dateLastCrawled": "2022-01-26T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Loss</b> function <b>autoencoder</b> vs variational-<b>autoencoder</b> or MSE-<b>loss</b> vs ...", "url": "https://stats.stackexchange.com/questions/350211/loss-function-autoencoder-vs-variational-autoencoder-or-mse-loss-vs-binary-cross", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/350211", "snippet": "Cross-entropy <b>loss</b> is assymetrical.. If your true intensity is high, e.g. 0.8, generating a pixel with the intensity of 0.9 is penalized more than generating a pixel with intensity of 0.7.. Conversely if it&#39;s low, e.g. 0.3, predicting an intensity of 0.4 is penalized less than a <b>predicted</b> intensity of 0.2.. You might have guessed by now - cross-entropy <b>loss</b> is biased towards 0.5 whenever the ground truth is not binary. For a ground truth of 0.5, the per-pixel zero-normalized <b>loss</b> is equal to ...", "dateLastCrawled": "2022-02-01T22:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "L2 vs <b>L1 Regularization in Machine Learning</b> | Ridge and Lasso ...", "url": "https://www.analyticssteps.com/blogs/l2-and-l1-regularization-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.analyticssteps.com/blogs/l2-and-<b>l1</b>-regularization-machine-learning", "snippet": "Where <b>L1</b> regularization attempts to estimate the median of data, L2 regularization makes estimation for the mean of the data in order to evade overfitting. Through including the absolute <b>value</b> of weight parameters, <b>L1</b> regularization <b>can</b> add the penalty term in cost function. On the other hand, L2 regularization appends the squared <b>value</b> of ...", "dateLastCrawled": "2022-02-02T11:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Understanding Loss Functions in Machine Learning</b> | Engineering ...", "url": "https://www.section.io/engineering-education/understanding-loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.section.io/engineering-education/<b>understanding-loss-functions-in-machine</b>...", "snippet": "Huber <b>Loss</b>. A comparison <b>between</b> <b>L1</b> and L2 <b>loss</b> yields the following results: <b>L1</b> <b>loss</b> is more robust than its counterpart. On taking a closer look at the formulas, one <b>can</b> observe that if the <b>difference</b> <b>between</b> <b>the predicted</b> <b>and the actual</b> <b>value</b> is high, L2 <b>loss</b> magnifies the effect when <b>compared</b> to <b>L1</b>. Since L2 succumbs to outliers, <b>L1</b> <b>loss</b> ...", "dateLastCrawled": "2022-02-01T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "1. Introduction to Time Series \u2014 PseudoLab Tutorial Book", "url": "https://pseudo-lab.github.io/Tutorial-Book-en/chapters/en/time-series/Ch1-Time-Series.html", "isFamilyFriendly": true, "displayUrl": "https://pseudo-lab.github.io/Tutorial-Book-en/chapters/en/time-series/Ch1-Time-Series.html", "snippet": "MAE, also known as <b>L1</b> <b>Loss</b>, <b>can</b> be calculated by dividing the sum of the absolute differences <b>between</b> <b>the predicted</b> values <b>and the actual</b> values by the number of samples(n). Since this is the process for calculating an average, from now on we will refer to this as \u2018calculating the mean\u2019. Since the scale of MAE is the same scale as the target variable being <b>predicted</b>, the meaning of the <b>value</b> <b>can</b> be understood intuitively. The implemented code looks like this:", "dateLastCrawled": "2022-01-30T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "Cross-Entropy <b>loss</b> is also called logarithmic <b>loss</b>, log <b>loss</b>, or logistic <b>loss</b>. Each <b>predicted</b> class probability is <b>compared</b> to the <b>actual</b> class desired output 0 or 1 and a score/<b>loss</b> is calculated that penalizes the probability based on how far it is from the <b>actual</b> expected <b>value</b>. The penalty is logarithmic in nature yielding a large score for large differences close to 1 and small score for small differences tending to 0.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss Function</b> - Pipline", "url": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/loss-function", "isFamilyFriendly": true, "displayUrl": "https://nuoxu2016.gitbook.io/pipline/preprocessing-1/<b>loss-function</b>", "snippet": "Smooth <b>L1</b>-<b>loss</b> <b>can</b> be interpreted as a combination of <b>L1</b>-<b>loss</b> and L2-<b>loss</b>. It behaves as <b>L1</b>-<b>loss</b> when the absolute <b>value</b> of the argument is high, and it behaves like L2-<b>loss</b> when the absolute <b>value</b> of the argument is close to zero. Smooth <b>L1</b>-<b>loss</b> combines the advantages of <b>L1</b>-<b>loss</b> (steady gradients for large values of \ud835\udc65) and L2-<b>loss</b> (less oscillations during updates when \ud835\udc65 is small). Mask RCNN applies Smooth <b>L1</b>- <b>loss</b> for training and regression. Huber <b>Loss</b>. The Huber <b>loss</b> acts like the ...", "dateLastCrawled": "2022-01-24T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "Cross-entropy <b>loss</b> increases as <b>the predicted</b> probability <b>value</b> deviate from the <b>actual</b> label. Hinge <b>loss</b>. Hinge <b>loss</b> <b>can</b> be used as an alternative to cross-entropy, which was initially developed to use with a support vector machine algorithm. Hinge <b>loss</b> works best with the classification problem because target values are in the set of {-1,1 ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "LossFunctions in Deep Learning-DeepVidhya", "url": "https://deepvidhya.com/blog/lossfunctions-in-deep-learning-1205", "isFamilyFriendly": true, "displayUrl": "https://deepvidhya.com/blog/<b>loss</b>functions-in-deep-learning-1205", "snippet": "It <b>can</b> be seen that the sigmoid function smooths <b>the predicted</b> <b>value</b> (like directly entering 0.1 and 0.01 and entering 0.1, 0.01 sigmoid and then entering, the last obviously has a lot smaller change <b>value</b>), which makes <b>the predicted</b> <b>value</b> of sigmoid far from the label <b>loss</b> growth is not so steep.", "dateLastCrawled": "2022-01-28T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Loss</b> Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>loss</b>-functions-and-optimization-algorithms...", "snippet": "One of the most widely used <b>loss function</b> is mean square error, which calculates the square of <b>difference</b> <b>between</b> <b>actual</b> <b>value</b> and <b>predicted</b> <b>value</b>. Different <b>loss</b> functions are used to deal with ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "If x &gt; 0 <b>loss</b> will be x itself (higher <b>value</b>), if 0&lt;x&lt;1 <b>loss</b> will be 1 \u2014 x (smaller <b>value</b>) and if x &lt; 0 <b>loss</b> will be 0 (minimum <b>value</b>). For y =1, the <b>loss</b> is as high as the <b>value</b> of x .", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Linear, Lasso, Ridge, and Elastic Net Regression</b> | Regression Models in ...", "url": "https://www.analyticssteps.com/blogs/linear-lasso-ridge-and-elastic-net-regression-overview", "isFamilyFriendly": true, "displayUrl": "https://www.analyticssteps.com/blogs/<b>linear-lasso-ridge-and-elastic-net-regression</b>...", "snippet": "The technology has been employed to precisely and accurately examine medical information and aid in predicting if a patient is vulnerable to a heart attack or stroke. You <b>can</b> gain further knowledge of AI\u2019s role in the medical sector through our blog on Artificial Intelligence in Healthcare. 2. Nest learning thermostat.", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Regularization \u2014 Understanding <b>L1</b> and L2 regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-<b>l1</b>-and-l2...", "snippet": "Understanding what regularization is and why it is required for <b>machine</b> <b>learning</b> and diving deep to clarify the importance of <b>L1</b> and L2 regularization in Deep <b>learning</b>.", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, the ... Mean Absolute Error, <b>L1</b> <b>Loss</b> (used by PerceptiLabs\u2019 Regression component): sums the absolute differences between the predictions and ground truth, and finds the average. <b>Loss</b> functions are used in a variety of use cases. The following table shows common image processing use cases where you might apply these, and other <b>loss</b> functions: Image Source: PerceptiLabs <b>Loss</b> in PL. Configuring a <b>loss</b> ...", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "is known as <b>L1</b>-norm, while the latter is known as the L2-norm. Keep in mind that L2-norm is more sensitive than <b>L1</b>-norm to large-valued outliers. Ridge and LASSO regularizations are based on L2-norm and <b>L1</b>-norm, respectively, while Elastic Net regularization is based on the mix of two. 2.6 What does a <b>machine</b> <b>learning</b> <b>learning</b>-curve measure ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "regression - Why <b>L1</b> norm for sparse models - Cross Validated", "url": "https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/45643", "snippet": "Show activity on this post. With a sparse model, we think of a model where many of the weights are 0. Let us therefore reason about how <b>L1</b>-regularization is more likely to create 0-weights. Consider a model consisting of the weights . With <b>L1</b> regularization, you penalize the model by a <b>loss</b> function = .", "dateLastCrawled": "2022-01-26T23:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - <b>L1</b>-norm vs l2-norm as cost function when ...", "url": "https://stackoverflow.com/questions/43301036/l1-norm-vs-l2-norm-as-cost-function-when-standardizing", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43301036", "snippet": "I feel that the l2-norm will penalize less the model than the <b>l1</b>-norm since squaring a number that is between 0 and 1 will always result in a lower number. So my question is, is it ok to use the l2-norm when both the input and the output are standardized? <b>machine</b>-<b>learning</b> statistics gradient-descent. Share. Follow edited Apr 10 &#39;17 at 12:08. jeremie. asked Apr 8 &#39;17 at 22:34. jeremie jeremie. 807 8 8 silver badges 15 15 bronze badges. Add a comment | 1 Answer Active Oldest Votes. 1 It does ...", "dateLastCrawled": "2022-01-24T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0888613X21000141", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0888613X21000141", "snippet": "The <b>machine</b> <b>learning</b> approaches outperform state of the art approaches on Google, BATS and DiffVec datasets. As far as we know, neither <b>analogy</b> classification nor <b>analogy</b> completion have been investigated in the same way as we have proposed in this paper, namely <b>learning</b> a model, instead of starting from the parallelogram model. The paper is structured as follows. Section 2 recalls the postulates characterizing analogical proportions and identifies a rigorous method for enlarging a set of ...", "dateLastCrawled": "2021-11-13T01:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Gentle Introduction to Pix2Pix Generative</b> Adversarial Network", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-pix2pix-generative</b>...", "snippet": "In <b>analogy</b> to automatic language translation, we define automatic image-to-image translation as the task of translating one possible representation of a scene into another, given sufficient training data. \u2014 Image-to-Image Translation with Conditional Adversarial Networks, 2016. It is a challenging problem that typically requires the development of a specialized model and hand-crafted <b>loss</b> function for the type of translation task being performed. Classical approaches use per-pixel ...", "dateLastCrawled": "2022-02-02T13:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Deep learning</b> - lectures.alex.balgavy.eu", "url": "https://lectures.alex.balgavy.eu/ml-notes/deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://lectures.alex.balgavy.eu/ml-notes/<b>deep-learning</b>", "snippet": "<b>Deep learning</b> <b>Deep learning</b> systems (autodiff engines) Tensors. To scale up backpropagation, want to move from operations on scalars to tensors. Tensor: generalisation of vectors/matrices to higher dimensions. e.g. a 2-tensor has two dimensions, a 4-tensor has 4 dimensions. You can represent data as a tensor. e.g. an RGB image is a 3-tensor of the red, green, and blue values for each pixel. Functions on tensors. Functions have inputs and outputs, all of which are tensors. They implement ...", "dateLastCrawled": "2021-12-15T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Denoising Seismic Records with Image Translation Networks</b> | CSEG RECORDER", "url": "https://csegrecorder.com/articles/view/denoising-seismic-records-with-image-translation-networks", "isFamilyFriendly": true, "displayUrl": "https://csegrecorder.com/articles/view/<b>denoising-seismic-records-with-image</b>...", "snippet": "The pix2pix network is a generative <b>machine</b> <b>learning</b> algorithm. Based on Alec Radford, et. al\u2019s DCGAN [6] architecture, ... the <b>L1 loss is similar</b> to the L2 loss: except the second-degree norm is replaced with the first-degree norm: The alternative denoising strategies tested against the image translation network included total-variation filtering, bilateral filtering, and wavelet transform filtering. Figure 3. Results of various image denoising techniques on synthetic data. Upper left ...", "dateLastCrawled": "2022-01-12T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A deep <b>learning</b> framework for constitutive modeling based on temporal ...", "url": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "snippet": "These two features meet the requirement for sequence modeling in <b>machine</b> <b>learning</b>. Therefore, the nonlinear constitutive models may be classified as sequence modeling from the viewpoint of <b>machine</b> <b>learning</b>. Concrete material and steel material both exhibit significant ultra-long-term memory effects and many model-driven constitutive relationships were developed to simulate stress-strain curves of materials , , , , with ultra-long-term memory effect. For steel material, the traditional ...", "dateLastCrawled": "2022-01-20T12:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(l1 loss)  is like +(difference between the predicted value and the actual value)", "+(l1 loss) is similar to +(difference between the predicted value and the actual value)", "+(l1 loss) can be thought of as +(difference between the predicted value and the actual value)", "+(l1 loss) can be compared to +(difference between the predicted value and the actual value)", "machine learning +(l1 loss AND analogy)", "machine learning +(\"l1 loss is like\")", "machine learning +(\"l1 loss is similar\")", "machine learning +(\"just as l1 loss\")", "machine learning +(\"l1 loss can be thought of as\")", "machine learning +(\"l1 loss can be compared to\")"]}