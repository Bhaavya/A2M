{"src_spec_res": [[], [], [], []], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Batch, <b>Mini Batch</b> &amp; <b>Stochastic</b> <b>Gradient Descent</b> | by Sushant Patrikar ...", "url": "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/batch-<b>mini-batch</b>-<b>stochastic</b>-<b>gradient-descent</b>-7a62ecba642a", "snippet": "<b>Mini Batch</b> <b>Gradient Descent</b>. We have seen the Batch <b>Gradient Descent</b>. We have also seen the <b>Stochastic</b> <b>Gradient Descent</b>. Batch <b>Gradient Descent</b> can be used for smoother curves. SGD can be used when the dataset is large. Batch <b>Gradient Descent</b> converges directly to minima. SGD converges faster for larger datasets. But, since in SGD we use only ...", "dateLastCrawled": "2022-02-02T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Batch vs <b>Mini-batch</b> vs <b>Stochastic Gradient Descent</b> with Code Examples ...", "url": "https://medium.datadriveninvestor.com/batch-vs-mini-batch-vs-stochastic-gradient-descent-with-code-examples-cd8232174e14", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/batch-vs-<b>mini-batch</b>-vs-<b>stochastic</b>-<b>gradient</b>...", "snippet": "It is possible to use only the <b>Mini-batch</b> <b>Gradient Descent</b> code to implement all versions of <b>Gradient Descent</b>, you just need to set the <b>mini_batch</b>_size equals one to <b>Stochastic</b> GD or the number of training examples to Batch GD. Thus, the main difference between Batch, <b>Mini-batch</b>, and <b>Stochastic Gradient Descent</b> is the number of examples used for each epoch and the time and effort necessary to reach the global minimum value of the Cost Function.", "dateLastCrawled": "2022-01-27T13:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Batch vs <b>Mini-batch vs Stochastic Gradient Descent</b> with Code Examples", "url": "https://edgeaiguru.com/Batch-vs-Mini-batch-vs-Stochastic-Gradient-Descent", "isFamilyFriendly": true, "displayUrl": "https://edgeaiguru.com/Batch-vs-<b>Mini-batch-vs-Stochastic-Gradient-Descent</b>", "snippet": "It is possible to use only the <b>Mini-batch</b> <b>Gradient</b> <b>Descent</b> code to implement all versions of <b>Gradient</b> <b>Descent</b>, you just need to set the <b>mini_batch</b>_size equals one to <b>Stochastic</b> GD or to the number of training examples to Batch GD. Thus, the main difference between Batch, <b>Mini-batch</b> and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> is the number of examples used for each epoch and the time and effort necessary to reach the global minimum value of the Cost Function.", "dateLastCrawled": "2022-01-27T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "13.6 <b>Stochastic and mini-batch gradient descent</b>", "url": "https://kenndanielso.github.io/mlrefined/blog_posts/13_Multilayer_perceptrons/13_6_Stochastic_and_minibatch_gradient_descent.html", "isFamilyFriendly": true, "displayUrl": "https://kenndanielso.github.io/.../13_6_<b>Stochastic_and_minibatch_gradient_descent</b>.html", "snippet": "It is noteworthy that because moderately accurate solutions (provided by a moderate amount of minimization of a cost function) tend to perform reasonably well in <b>machine</b> <b>learning</b> applications, and because with large datasets a random initialization will tend to lie far from a convergent point, in many cases even a single iteration of <b>stochastic</b>/<b>mini-batch</b> <b>gradient</b> <b>descent</b> can provide a good solution.", "dateLastCrawled": "2022-02-02T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient</b> <b>Descent</b>: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-<b>descent</b>", "snippet": "Types of <b>gradient</b> <b>descent</b>: batch, <b>stochastic</b>, <b>mini-batch</b>; Introduction to <b>Gradient</b> <b>Descent</b>. <b>Gradient</b> <b>descent</b> is an optimization algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum. What is <b>Gradient</b> <b>Descent</b>? <b>Gradient</b> <b>Descent</b> is an optimization algorithm for finding a local minimum of a differentiable function. <b>Gradient</b> <b>descent</b> is simply used in <b>machine</b> <b>learning</b> to ...", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Empirical Risk Minimization and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "models, <b>mini-batch</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> (SGD) can e\ufb03ciently solve the minimization problem (albeit, approximately). The ease of SGD comes from the de\ufb01- nition of the empirical risk as the expectation over a randomly subsampled example: the <b>gradient</b> of the loss on a randomly subsampled example is an unbiased es-timate of the <b>gradient</b> of the empirical risk. Combined with automatic di\ufb00erentiation, this provides a turnkey approach to \ufb01tting <b>machine</b>-<b>learning</b> models. Returning to ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! | by Aishwarya V ...", "url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>stochastic-gradient-descent</b>-clearly-explained-53d239905d31", "snippet": "<b>Stochastic gradient descent</b> is a very popular and common algorithm used in various <b>Machine</b> <b>Learning</b> algorithms, most importantly forms the basis of Neural Networks. In this article, I have tried my\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! Aishwarya V Srinivasan. Sep 7, 2019 \u00b7 4 min read. <b>Stochastic gradient descent</b> is a ...", "dateLastCrawled": "2022-02-02T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Understanding the 3 Primary <b>Types of Gradient Descent</b> | by ODSC - Open ...", "url": "https://medium.com/odscjournal/understanding-the-3-primary-types-of-gradient-descent-987590b2c36", "isFamilyFriendly": true, "displayUrl": "https://medium.com/odscjournal/understanding-the-3-primary-<b>types-of-gradient-descent</b>...", "snippet": "<b>Mini Batch</b> <b>Gradient Descent</b> is commonly used for deep <b>learning</b> problems. Conclusion This article should give you the basic motivation for the <b>gradient descent</b> process in <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2022-02-03T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Basics and Beyond: <b>Gradient Descent</b> | by Kumud Lakara | The Startup ...", "url": "https://medium.com/swlh/basics-and-beyond-gradient-descent-87fa964c31dd", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/basics-and-beyond-<b>gradient-descent</b>-87fa964c31dd", "snippet": "3. <b>Mini-batch Gradient Descent</b>. This is actually the best of both worlds. It accounts for the computational expenses in case of <b>batch gradient descent</b> and the high variance in case of SGD. Mini ...", "dateLastCrawled": "2021-05-02T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "11.5. <b>Minibatch</b> <b>Stochastic</b> <b>Gradient Descent</b> \u2014 Dive into Deep <b>Learning</b> 0 ...", "url": "http://d2l.ai/chapter_optimization/minibatch-sgd.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_optimization/<b>minibatch</b>-sgd.html", "snippet": "So far we encountered two extremes in the approach to <b>gradient</b> based <b>learning</b>: Section 11.3 uses the full dataset to compute gradients and to update parameters, one pass at a time. Conversely Section 11.4 processes one observation at a time to make progress. Each of them has its own drawbacks. <b>Gradient Descent</b> is not particularly data efficient whenever data is very similar. <b>Stochastic</b> <b>Gradient Descent</b> is not particularly computationally efficient since CPUs and GPUs cannot exploit the full ...", "dateLastCrawled": "2022-02-01T18:13:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(mini-batch stochastic gradient descent)  is like +(rowers in a boat)", "+(mini-batch stochastic gradient descent) is similar to +(rowers in a boat)", "+(mini-batch stochastic gradient descent) can be thought of as +(rowers in a boat)", "+(mini-batch stochastic gradient descent) can be compared to +(rowers in a boat)", "machine learning +(mini-batch stochastic gradient descent AND analogy)", "machine learning +(\"mini-batch stochastic gradient descent is like\")", "machine learning +(\"mini-batch stochastic gradient descent is similar\")", "machine learning +(\"just as mini-batch stochastic gradient descent\")", "machine learning +(\"mini-batch stochastic gradient descent can be thought of as\")", "machine learning +(\"mini-batch stochastic gradient descent can be compared to\")"]}