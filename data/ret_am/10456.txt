{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Mean Average Precision</b> (<b>mAP</b>) Explained | Paperspace Blog", "url": "https://blog.paperspace.com/mean-average-precision/", "isFamilyFriendly": true, "displayUrl": "https://blog.paperspace.com/<b>mean-average-precision</b>", "snippet": "Due to the importance of both precision and recall, there is a <b>precision-recall</b> <b>curve</b> the shows the tradeoff between the precision and recall values for different thresholds. This <b>curve</b> helps to select the best threshold to maximize both metrics. There are some inputs needed to create the <b>precision-recall</b> <b>curve</b>: The ground-truth labels.", "dateLastCrawled": "2022-02-03T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Precision-recall</b> curves \u2013 what are they and how are they used?", "url": "https://acutecaretesting.org/en/articles/precision-recall-curves-what-are-they-and-how-are-they-used", "isFamilyFriendly": true, "displayUrl": "https://acutecaretesting.org/en/articles/<b>precision-recall</b>-<b>curves</b>-what-are-they-and-how...", "snippet": "FIG. 7: <b>Precision-recall curve</b> for a test with complete overlap of results between persons with and without disease \u2013 imbalanced distribution Y:N equal to 1:9. A PCR plot for a test with complete overlap of results between persons with and without disease will be determined by the ratio between the two groups: \u2022 For the balanced data set with Y:N equal to 1:1, you will due to the complete overlap of data for each cut-off have the same number of persons with disease as persons without ...", "dateLastCrawled": "2022-02-02T06:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>What is Mean Average Precision</b> (<b>mAP</b>) in Object Detection?", "url": "https://blog.roboflow.com/mean-average-precision/", "isFamilyFriendly": true, "displayUrl": "https://blog.roboflow.com/<b>mean-average-precision</b>", "snippet": "In order to calculate <b>mAP</b>, we draw a series of <b>precision recall</b> curves with the IoU threshold set at varying levels of difficulty. A sketch of <b>mAP</b> <b>precision-recall</b> curves by yours truly. In my sketch, red is drawn with the highest requirement for IoU (perhaps 90 percent) and the orange line is drawn with the most lenient requirement for IoU (perhaps 10 percent).", "dateLastCrawled": "2022-02-02T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Precision-Recall Curve | ML - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/precision-recall-curve-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>precision-recall</b>-<b>curve</b>-ml", "snippet": "Let us briefly understand what is a <b>Precision-Recall</b> <b>curve</b>. <b>Precision-Recall</b> (PR) <b>Curve</b> \u2013 A PR <b>curve</b> is simply a graph with Precision values on the y-axis and Recall values on the x-axis. In other words, the PR <b>curve</b> contains TP/(TP+FN) on the y-axis and TP/(TP+FP) on the x-axis. It is important to note that Precision is also called the Positive Predictive Value (PPV). Recall is also called Sensitivity, Hit Rate or True Positive Rate (TPR). The figure below shows a juxtaposition of sample ...", "dateLastCrawled": "2022-02-02T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Breaking Down Mean <b>Average Precision</b> (<b>mAP</b>) | by Ren Jie Tan | Towards ...", "url": "https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/breaking-down-mean-<b>average-precision</b>-<b>map</b>-ae462f623a52", "snippet": "<b>Precision/Recall</b> <b>Curve</b> (PR <b>Curve</b>) With the TP, FP and FN formally defined, we can now calculate the precision and recall of our detection for a given class across the test set. Each BB would have its confidence level, usually given by its softmax layer, and would be used to rank the output. Note that this is very similar to the information ...", "dateLastCrawled": "2022-02-02T18:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How To <b>Calculate the mean Average Precision</b> (<b>mAP</b>) in object detection ...", "url": "https://hungsblog.de/en/technology/how-to-calculate-mean-average-precision-map/", "isFamilyFriendly": true, "displayUrl": "https://hungsblog.de/en/technology/how-to-calculate-mean-average-precision-<b>map</b>", "snippet": "The Average Precision (AP) is meant to summarize the <b>Precision-Recall</b> <b>Curve</b> by averaging the precision across all recall values between 0 and 1. Efffectively it is the area under the <b>Precision-Recall</b> <b>curve</b>. Because the <b>curve</b> is a characterized by zick zack lines it is best to approximate the area using interpolation. At this point i would again <b>like</b> to refer to the already comprehensive work of Padilla et al., 2020 and also EL Aidouni, 2019 on how to interpolate the precision from the recall ...", "dateLastCrawled": "2022-02-01T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding the <b>mAP</b> Evaluation Metric for Object <b>Detection</b> | by ...", "url": "https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@timothycarlen/understanding-the-<b>map</b>-evaluation-metric-for-object...", "snippet": "<b>Precision-Recall</b> <b>curve</b> for an example classifier. A point on the <b>precision-recall</b> <b>curve</b> is determined by considering all objects above a given model score threshold as a positive prediction, then ...", "dateLastCrawled": "2022-02-03T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-<b>precision-recall</b>-<b>curves</b>-for...", "snippet": "A <b>precision-recall</b> <b>curve</b> (or PR <b>Curve</b>) is a plot of the precision (y-axis) and the recall (x-axis) for different probability thresholds. PR <b>Curve</b>: Plot of Recall (x) vs Precision (y). A model with perfect skill is depicted as a point at a coordinate of (1,1). A skillful model is represented by a <b>curve</b> that bows towards a coordinate of (1,1). A no-skill classifier will be a horizontal line on the plot with a precision that is proportional to the number of positive examples in the dataset. For ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Segmentation And Recongnition Metrics</b> | Penguinflys\u2019 Log", "url": "https://penguinflys.github.io/penguinflys/fundamental/Segmentation-And-Recongnition-Metrics/", "isFamilyFriendly": true, "displayUrl": "https://penguinflys.github.io/penguinflys/fundamental/Segmentation-And-Recongnition...", "snippet": "<b>Precision Recall</b> <b>curve</b>(PR-<b>curve</b>) is used to measure the performance of a model by tuning confidence score, precision, and recall from a <b>curve</b> showing the compromise of precision and recall. The larger the area under that <b>curve</b> is, the better performance the model has. The area is what we call average precision. However, such a <b>curve</b> is not convenient to compare models, especially when the <b>curve</b> is noisy and intersects with the saw-tooth shape.", "dateLastCrawled": "2022-01-20T00:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "sklearn.metrics.<b>precision_recall</b>_<b>curve</b> \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/.../modules/generated/sklearn.metrics.<b>precision_recall</b>_<b>curve</b>.html", "snippet": "sklearn.metrics. <b>precision_recall</b>_<b>curve</b> (y_true, probas_pred, *, pos_label = None, sample_weight = None) [source] \u00b6 Compute <b>precision-recall</b> pairs for different probability thresholds. Note: this implementation is restricted to the binary classification task. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The ...", "dateLastCrawled": "2022-02-03T02:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Difference between <b>MAP</b>@K for recommendations, <b>MAP</b> from <b>Precision Recall</b> ...", "url": "https://datascience.stackexchange.com/questions/93011/difference-between-mapk-for-recommendations-map-from-precision-recall-curves-a", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/93011/difference-between-<b>map</b>k-for...", "snippet": "I have been using the 3 metrics independently for a while now, but trying to figure out if they are actually 3 separate things (with <b>similar</b>-looking definitions/names) or there is some underlying connection between them. <b>mAP</b> - Mean Average Precision is the average of the <b>Precision-Recall</b> <b>curve</b> over various thresholds. 1 2", "dateLastCrawled": "2022-01-14T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Breaking Down Mean <b>Average Precision</b> (<b>mAP</b>) | by Ren Jie Tan | Towards ...", "url": "https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/breaking-down-mean-<b>average-precision</b>-<b>map</b>-ae462f623a52", "snippet": "<b>Precision/Recall</b> <b>Curve</b> (PR <b>Curve</b>) With the TP, FP and FN formally defined, we can now calculate the precision and recall of our detection for a given class across the test set. Each BB would have its confidence level, usually given by its softmax layer, and would be used to rank the output. Note that this is very <b>similar</b> to the information ...", "dateLastCrawled": "2022-02-02T18:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "machine learning - Why does <b>precision_recall</b>_<b>curve</b>() return <b>similar</b> but ...", "url": "https://stats.stackexchange.com/questions/559203/why-does-precision-recall-curve-return-similar-but-not-equal-values-than-confu", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/559203/why-does-<b>precision-recall</b>-<b>curve</b>...", "snippet": "Why does <b>precision_recall</b>_<b>curve</b>() return <b>similar</b> but not equal values than confusion matrix? Ask Question Asked 29 days ago. Active 29 days ago. Viewed 63 times 2 $\\begingroup$ INTRO: I wrote a very simple machine learning project which classifies numbers based on the minst dataset: from sklearn.datasets import fetch_openml import numpy as np from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.linear_model import SGDClassifier from sklearn.metrics ...", "dateLastCrawled": "2022-02-02T17:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>MRR</b> vs <b>MAP</b> vs NDCG: Rank-Aware Evaluation Metrics And When To Use Them ...", "url": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "snippet": "We get the <b>precision-recall</b> <b>curve</b> by computing the precision as a function of recall values. In this ... The goal of the <b>MAP</b> measure <b>is similar</b> to the goal of the NDCG metric. They both value ...", "dateLastCrawled": "2022-01-31T10:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>information retrieval evaluation python precision, recall</b>, f score, AP,<b>MAP</b>", "url": "https://stackoverflow.com/questions/40457331/information-retrieval-evaluation-python-precision-recall-f-score-ap-map", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/40457331", "snippet": "I have a <b>similar</b> question to yours @M.R.. I&#39;ve been wondering how I plot a <b>Precision-Recall</b> <b>curve</b> that represents an entire set of queries, instead of a single query. Should I take, for example, the mean (between all queries) of the precision and recall values at different points, and plot that? \u2013", "dateLastCrawled": "2022-01-27T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Generate <b>Precision Recall</b> <b>Curve</b> with a custom IoU Threshold i.e., <b>mAP</b>@0 ...", "url": "https://github.com/ultralytics/yolov5/issues/6478", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ultralytics/yolov5/issues/6478", "snippet": "Search before asking. I have searched the YOLOv5 issues and discussions and found no <b>similar</b> questions.; Question. Hello, how do I gernerate a <b>precision recall</b> <b>curve</b> at a different <b>mAP</b> (i.e. <b>mAP</b>@75)? From what I understand it should be possible in this function, but I am not sure how exactly I would to it.. Thanks,", "dateLastCrawled": "2022-02-05T19:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>ROC and precision-recall curves</b> \u2013 way to be a data scientist", "url": "https://datascience103579984.wordpress.com/2019/04/30/roc-and-precision-recall-curves/", "isFamilyFriendly": true, "displayUrl": "https://datascience103579984.wordpress.com/2019/04/30/<b>roc-and-precision-recall-curves</b>", "snippet": "ROC curves are quite useful for comparing methods. However, they have one weakness, and it is that neither of the measures plotted depend on prevalence. In cases in which prevalence matters, we may instead. make a <b>precision recall</b> plot. The idea <b>is similar</b>, but we instead plot precision against recall. 1.", "dateLastCrawled": "2021-11-03T09:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "graph - <b>Precision Recall</b> Curves in R - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/64218496/precision-recall-curves-in-r", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/64218496", "snippet": "The gradient colour scale corresponds to the range of your x and y values (i.e. range(c(x, y))); in your case, the randomly generated values are somewhere between -4 and 3 (the exact values depend on the random number generator seed).Compare this with e.g. plot(pr.<b>curve</b>(1:10, 1:10, <b>curve</b> = TRUE)) which will produce a gradient colour scale between 1 and 10. Why do you think the numbers should be between 0 and 1? \u2013 Maurits Evers", "dateLastCrawled": "2022-01-25T08:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>PRECISION-RECALL</b> <b>CURVE</b> \u00b7 Issue #898 \u00b7 ultralytics/<b>yolov3</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/ultralytics/yolov3/issues/898", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ultralytics/<b>yolov3</b>/issues/898", "snippet": "<b>Precision Recall</b> curves may be plotted by uncommenting code here when running test.py: For <b>yolov3</b>-spp-ultralytics.pt on COCO, the curves for all 80 classes look like this: For a single class 0, or person, the <b>curve</b> looks like this. During testing we evaluate the area under the <b>curve</b> as average <b>precision</b>, AP.", "dateLastCrawled": "2022-01-26T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Simple bag-of-words loop closure for visual SLAM", "url": "https://nicolovaligi.com/articles/bag-of-words-loop-closure-visual-slam/", "isFamilyFriendly": true, "displayUrl": "https://nicolovaligi.com/articles/bag-of-words-loop-closure-visual-slam", "snippet": "Edit: I added some code and info on the <b>precision-recall</b> <b>curve</b> that is usually used to evaluate this sort of classification algorithms. This post goes through the theory and implementation of a simple algorithm for loop closure detection in visual SLAM. The code is available on my Github. As part of my research in using deep learning for SLAM, I found that loop closure detection would be a promising first application, due to its similarity to well-studied image classification problems ...", "dateLastCrawled": "2022-02-02T01:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>precision recall</b> <b>curve</b> in r", "url": "https://davenue.in/8kgelue/precision-recall-curve-in-r.html", "isFamilyFriendly": true, "displayUrl": "https://davenue.in/8kgelue/<b>precision-recall</b>-<b>curve</b>-in-r.html", "snippet": "The measurement and &quot;truth&quot; data must have the same two possible outcomes and one of the outcomes must <b>be thought</b> of as a &quot;relevant&quot; results. Non-linear interpolation. You will explore how the probabilities output by your classifier <b>can</b> be used to trade-off precision with recall, and dive into this spectrum, using <b>precision-recall</b> curves. The area under the <b>precision-recall</b> <b>curve</b> as a performance metric for rare binary events. These functions calculate the recall, precision or F values of a ...", "dateLastCrawled": "2022-01-23T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Precision-recall curves</b> - Andreas Beger", "url": "https://www.andybeger.com/2015/03/16/precision-recall-curves/", "isFamilyFriendly": true, "displayUrl": "https://www.andybeger.com/2015/03/16/<b>precision-recall-curves</b>", "snippet": "The plot below is a <b>precision-recall</b> <b>curve</b> that does this, for the same example as before. Instead of FPR we now have precision, and I&#39;ve also flipped the axes as it seems to be convention to plot recall on the x-axis. <b>Precision-recall</b> <b>curve</b> for the same example data with 0.4 positives. Simulations! Since the example I used had a positive rate of 0.4, the plot doesn&#39;t really make it obvious why one would want to look at <b>precision-recall curves</b> for sparse data. To illustrate that better ...", "dateLastCrawled": "2022-01-26T04:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How NOT to use ROC, <b>Precision-Recall curves &amp; MCC (Matthews Correlation</b> ...", "url": "https://towardsdatascience.com/how-not-to-use-roc-precision-recall-curves-mcc-matthews-correlation-coefficient-f68a33108f8b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-not-to-use-roc-<b>precision-recall</b>-<b>curves</b>-mcc-matthews...", "snippet": "ROC <b>curve</b>; <b>Precision-Recall</b> (PR) <b>curve</b>; Something else? <b>Thought</b> Process. First of all, let\u2019s eliminate ROC <b>curve</b> as it is not best suited for imbalanced class problems. Here\u2019s a great video to get one\u2019s foundation right for this. On to <b>Precision-Recall</b> <b>curve</b> now - this is what my colleague had chosen. It has Precision as y-axis and Recall as x-axis. If I use the above confusion matrix at various thresholds to plot this <b>curve</b>, do you see any problem in the approach? The <b>Precision-Recall</b> ...", "dateLastCrawled": "2022-01-20T17:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Precision recall</b> <b>curve</b> object detection \u2014 every step of object ...", "url": "https://nonethelesscompetitive.com/article/78165761385/c3zj2195zup1ek", "isFamilyFriendly": true, "displayUrl": "https://nonethelesscompetitive.com/article/78165761385/c3zj2195zup1ek", "snippet": "You <b>can</b> plot a <b>precision recall</b> <b>curve</b> for object detection by going to the Plot menu under Analysis and then selecting Plots under Plotting . +1 vote . answered Aug 4, 2020 by QOPMisty4602 (100 points) The best method for plotting a <b>precision recall</b> <b>curve</b> is to use the accuracy and precision <b>curve</b> provided by the software. If you have the accuracy <b>curve</b>, you <b>can</b> use it to plot the.", "dateLastCrawled": "2022-01-25T15:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Intro to Deep Learning \u2014 performance metrics(<b>Precision, Recall</b>, F1, ROC ...", "url": "https://hk3342.medium.com/intro-to-deep-learning-performance-metrics-precision-recall-f1-roc-pr-prg-87f5073f9354", "isFamilyFriendly": true, "displayUrl": "https://hk3342.medium.com/intro-to-deep-learning-performance-metrics-<b>precision-recall</b>...", "snippet": "It <b>can</b> <b>be thought</b> of as the fraction the model correctly predicted among the positive classes. Precision and recall don\u2019t consider the true negative. To get high precision, the model needs to reduce false positive(i.e. when the model incorrectly predicts as positive which was actually negative class). A good example application where precision could be an appropriate metric would be a spam email scanner. To get a high recall, the model needs to decrease false negative(i.e. when the model ...", "dateLastCrawled": "2022-01-30T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How To <b>Calculate the mean Average Precision</b> (<b>mAP</b>) in object detection ...", "url": "https://hungsblog.de/en/technology/how-to-calculate-mean-average-precision-map/", "isFamilyFriendly": true, "displayUrl": "https://hungsblog.de/en/technology/how-to-calculate-mean-average-precision-<b>map</b>", "snippet": "The Average Precision (AP) is meant to summarize the <b>Precision-Recall</b> <b>Curve</b> by averaging the precision across all recall values between 0 and 1. Efffectively it is the area under the <b>Precision-Recall</b> <b>curve</b>. Because the <b>curve</b> is a characterized by zick zack lines it is best to approximate the area using interpolation. At this point i would again like to refer to the already comprehensive work of Padilla et al., 2020", "dateLastCrawled": "2022-02-01T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "classification - &quot;Good&quot; <b>classifier destroyed my Precision-Recall</b> <b>curve</b> ...", "url": "https://stats.stackexchange.com/questions/201750/good-classifier-destroyed-my-precision-recall-curve-what-happened", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/201750/good-classifier-destroyed-my...", "snippet": "If a model shows good AUC, but still has poor early retrieval, the <b>Precision-Recall</b> <b>curve</b> will leave a lot to be desired. You <b>can</b> see a great example of this happening in this answer to a similar question. For this reason, Saito et al. recommend using area under the <b>Precision-Recall</b> <b>curve</b> rather than AUC when you have imbalanced classes.", "dateLastCrawled": "2022-01-25T11:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Evaluation Metrics for Object Detection</b> - DebuggerCafe", "url": "https://debuggercafe.com/evaluation-metrics-for-object-detection/", "isFamilyFriendly": true, "displayUrl": "https://debuggercafe.com/<b>evaluation-metrics-for-object-detection</b>", "snippet": "The intention in interpolating the <b>precision/recall</b> <b>curve</b> in this way is to reduce the impact of the \u201cwiggles\u201d in the <b>precision/recall</b> <b>curve</b>, caused by small variations in the ranking of examples. It should be noted that to obtain a high score, a method must have precision at all levels of recall\u2014 this penalises methods which retrieve only a subset of examples with high precision (e.g. side views of cars). The PASCAL Visual Object Classes (VOC) Challenge. I think that the above words ...", "dateLastCrawled": "2022-02-03T01:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>precision, recall and f-measure</b> in R - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/12572357/precision-recall-and-f-measure-in-r", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/12572357", "snippet": "&gt; Fmeasure &lt;- 2 * <b>precision * recall</b> / (<b>precision + recall</b>) Share. Follow answered Sep 24 &#39;12 at 22:40. JACKY Li JACKY Li. 3,111 5 5 gold badges 28 28 silver badges 44 44 bronze badges. 2. Thanks, it worked really well! (and was mush simpler than I <b>thought</b>, i guess I was overthinking again) \u2013 Fanny. Sep 25 &#39;12 at 18:21. 3. When predict is all 0 (the classifier predicted all samples as belonging to class 0), then retrieved = 0 and you divide by 0 \u2013 Omri374. Nov 28 &#39;13 at 12:45. Add a ...", "dateLastCrawled": "2022-01-26T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "computer vision - What does the notation <b>mAP</b>@[.5:.95] mean? - Data ...", "url": "https://datascience.stackexchange.com/questions/16797/what-does-the-notation-map-5-95-mean", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/16797", "snippet": "Commonly, IoU &gt; 0.5 means that it was a hit, otherwise it was a fail. For each class, one <b>can</b> calculate the. The <b>mAP</b> (mean average precision) = 1 | c l a s s e s | \u2211 c \u2208 c l a s s e s # T P ( c) # T P ( c) + # F P ( c) If one wants better proposals, one does increase the IoU from 0.5 to a higher value (up to 1.0 which would be perfect).", "dateLastCrawled": "2022-01-27T18:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Precision-recall</b> curves \u2013 what are they and how are they used?", "url": "https://acutecaretesting.org/en/articles/precision-recall-curves-what-are-they-and-how-are-they-used", "isFamilyFriendly": true, "displayUrl": "https://acutecaretesting.org/en/articles/<b>precision-recall</b>-<b>curves</b>-what-are-they-and-how...", "snippet": "A <b>precision-recall curve</b> shows the relationship between precision (= positive predictive value) and recall (= sensitivity) for every possible cut-off. The PRC is a graph with: \u2022 The x-axis showing recall (= sensitivity = TP / (TP + FN)) \u2022 The y-axis showing precision (= positive predictive value = TP / (TP + FP)) Thus every point on the PRC represents a chosen cut-off even though you cannot see this cut-off. What you <b>can</b> see is the precision and the recall that you will get when you ...", "dateLastCrawled": "2022-02-02T06:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Precision-Recall Curve | ML - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/precision-recall-curve-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>precision-recall</b>-<b>curve</b>-ml", "snippet": "Let us briefly understand what is a <b>Precision-Recall</b> <b>curve</b>. <b>Precision-Recall</b> (PR) <b>Curve</b> \u2013 A PR <b>curve</b> is simply a graph with Precision values on the y-axis and Recall values on the x-axis. In other words, the PR <b>curve</b> contains TP/(TP+FN) on the y-axis and TP/(TP+FP) on the x-axis. It is important to note that Precision is also called the Positive Predictive Value (PPV). Recall is also called Sensitivity, Hit Rate or True Positive Rate (TPR). The figure below shows a juxtaposition of sample ...", "dateLastCrawled": "2022-02-02T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to Use ROC Curves and <b>Precision-Recall Curves for Classification</b> in ...", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-<b>precision-recall</b>-<b>curves</b>-for...", "snippet": "The curves of different models <b>can</b> <b>be compared</b> directly in general or for different thresholds. The area under the <b>curve</b> (AUC) <b>can</b> be used as a summary of the model skill. The shape of the <b>curve</b> contains a lot of information, including what we might care about most for a problem, the expected false positive rate, and the false negative rate. To make this clear: Smaller values on the x-axis of the plot indicate lower false positives and higher true negatives. Larger values on the y-axis of ...", "dateLastCrawled": "2022-02-03T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-<b>precision-recall</b>-<b>curves</b>-for...", "snippet": "Now that we have seen the <b>Precision-Recall</b> <b>Curve</b>, let\u2019s take a closer look at the ROC area under <b>curve</b> score. <b>Precision-Recall</b> Area Under <b>Curve</b> (AUC) Score. The <b>Precision-Recall</b> AUC is just like the ROC AUC, in that it summarizes the <b>curve</b> with a range of threshold values as a single score. The score <b>can</b> then be used as a point of comparison between different models on a binary classification problem where a score of 1.0 represents a model with perfect skill. The <b>Precision-Recall</b> AUC score ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Precision Recall Curve Simplified</b> - ListenData", "url": "https://www.listendata.com/2019/07/precision-recall-curve-simplified.html", "isFamilyFriendly": true, "displayUrl": "https://www.listendata.com/2019/07/<b>precision-recall-curve-simplified</b>.html", "snippet": "This article outlines <b>precision recall</b> <b>curve</b> and how it is used in real-world data science application. It includes explanation of how it is different from ROC <b>curve</b>. It also highlights limitation of ROC <b>curve</b> and how it <b>can</b> be solved via area under <b>precision-recall</b> <b>curve</b>. This article also covers implementation of area under <b>precision recall</b> ...", "dateLastCrawled": "2022-01-30T14:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "computer vision - High <b>mAP</b>@50 with low precision and recall. What does ...", "url": "https://stackoverflow.com/questions/62973155/high-map50-with-low-precision-and-recall-what-does-it-mean-and-what-metric-sho", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/62973155", "snippet": "In your case faster rcnn results indicate that <b>precision-recall</b> <b>curve</b> metric is bad <b>compared</b> to that of Yolov3, which means that either faster rcnn has very bad recall at higher confidence thresholds or very bad precision at lower confidence threshold <b>compared</b> to that of Yolov3 (especially for small objects). <b>Precision, Recall</b> and F1 score are computed for given confidence threshold. I&#39;m assuming you&#39;re running the model with default confidence threshold (could be 0.25). So higher Precision ...", "dateLastCrawled": "2022-01-27T12:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to <b>plot a precision-recall curve in MATLAB</b> - Quora", "url": "https://www.quora.com/How-can-I-plot-a-precision-recall-curve-in-MATLAB", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-I-<b>plot-a-precision-recall-curve-in-MATLAB</b>", "snippet": "Answer (1 of 3): Using perfcurve() from the Statistics Toolbox: [code] scores = rand(1000, 1); targets = round(targets + 0.5*(rand(1000,1) - 0.5)); figure [Xpr,Ypr ...", "dateLastCrawled": "2022-01-30T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>MRR</b> vs <b>MAP</b> vs NDCG: Rank-Aware Evaluation Metrics And When To Use Them ...", "url": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "snippet": "We get the <b>precision-recall</b> <b>curve</b> by computing the precision as a function of recall values. In this ... <b>Compared</b> to the <b>MAP</b> metric it does a good job at evaluating the position of ranked items ...", "dateLastCrawled": "2022-01-31T10:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to <b>interpret the area under the precision-recall curve</b> - Quora", "url": "https://www.quora.com/How-do-you-interpret-the-area-under-the-precision-recall-curve", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-you-<b>interpret-the-area-under-the-precision-recall-curve</b>", "snippet": "Answer: There isn\u2019t much of a point in computing this, but it would correspond to the average precision you <b>can</b> expect when your recall varies from 0 to 1. Why\u2019s that? That\u2019s because the area under the <b>curve</b> is the integral of precision as a function of recall. And integrating a function over a ...", "dateLastCrawled": "2022-01-23T16:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Model Evaluation - Data Science in Python", "url": "https://mapattacker.github.io/datascience/model-evaluation/", "isFamilyFriendly": true, "displayUrl": "https://<b>map</b>attacker.github.io/datascience/model-evaluation", "snippet": "From sklearn, the <b>precision-recall</b> <b>curve</b> shows the tradeoff between precision and recall for different threshold. A high area under the <b>curve</b> represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. from sklearn.metrics import <b>precision_recall</b>_<b>curve</b> from sklearn.metrics import plot_<b>precision_recall</b>_<b>curve</b> import matplotlib.pyplot as plt average_precision = average_precision_score (y_test, y ...", "dateLastCrawled": "2022-01-24T20:08:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> <b>Evaluation Metrics</b> - GitHub Pages", "url": "https://kevalnagda.github.io/evaluation-metrics", "isFamilyFriendly": true, "displayUrl": "https://kevalnagda.github.io/<b>evaluation-metrics</b>", "snippet": "This is where Average Precision (AP), which is based on the <b>precision-recall</b> <b>curve</b>, comes into play. In essence, AP is the precision averaged across all unique recall levels. where, r1, r2, r3, \u2026, rn are the recall levels at which the precision is first interpolated. ROC <b>Curve</b> The Receiver Operating Characteristic <b>curve</b> is a plot that shows the performance of a binary classifier as a function of its cut-off threshold. It essentially shows the True Positive Rate (TPR) against the False ...", "dateLastCrawled": "2021-10-13T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>machine</b> <b>learning</b> - How to calculate precision and recall in a 3 x 3 ...", "url": "https://stats.stackexchange.com/questions/91044/how-to-calculate-precision-and-recall-in-a-3-x-3-confusion-matrix", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/91044/how-to-calculate-precision-and-recall...", "snippet": "<b>machine</b>-<b>learning</b> <b>precision-recall</b>. Share. Cite. Improve this question. Follow edited Mar 23 &#39;14 at 11:58. TooTone. 3,621 ... I already understand the <b>analogy</b> described in your solution. I will read paper. I will accept this as a answer. I don&#39;t understand PPV AND NPV.Please explain these concept as graphic as the Sens and Spec were explained and I will accept your answer. $\\endgroup$ \u2013 user22149. Mar 23 &#39;14 at 22:27. Add a comment | 3 $\\begingroup$ By reducing the data down to forced ...", "dateLastCrawled": "2022-01-30T05:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Learning</b> Curves in <b>Machine</b> <b>Learning</b> - ResearchGate", "url": "https://www.researchgate.net/publication/247934703_Learning_Curves_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/247934703_<b>Learning</b>_<b>Curves</b>_in_<b>Machine</b>_<b>Learning</b>", "snippet": "The area under the receiver operating characteristic (ROC) <b>curve</b> (AUC) was 0.62 (95% confidence interval [CI]: 0.57, 0.68) and the area under the <b>precision\u2010recall</b> <b>curve</b> was 0.58. <b>Learning</b> <b>curve</b> ...", "dateLastCrawled": "2021-12-15T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Interpret your Regression</b>. A walk through Logistic Regression | by ...", "url": "https://towardsdatascience.com/interpret-your-regression-d5f93908327b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpret-your-regression</b>-d5f93908327b", "snippet": "The <b>precision-recall</b> <b>curve</b> calls attention to the point that the model is just slightly above the no skill line for most thresholds. The no skill line is a line parallel to the x-axis with the value of the ratio of positive cases in the dataset, which is, in this case, 0.06. But this contradicts the high accuracy of 93%.", "dateLastCrawled": "2022-02-01T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>An Intuitive Explanation to Precision, Recall and</b> Accuracy", "url": "https://www.linkedin.com/pulse/intuitive-explanation-precision-recall-accuracy-daniel-d-souza/", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/intuitive-explanation-<b>precision-recall</b>-accuracy-daniel...", "snippet": "Earlier this year, at an interview in New York I was asked about the recall and precision of one of my <b>Machine</b> <b>Learning</b> Projects. For a couple of minutes following that, the interviewer sat back ...", "dateLastCrawled": "2021-10-21T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Bias -Variance &amp; <b>Precision-Recall</b> Trade-offs: How to aim for the sweet ...", "url": "https://towardsdatascience.com/tradeoffs-how-to-aim-for-the-sweet-spot-c20b40d5e6b6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/tradeoffs-how-to-aim-for-the-sweet-spot-c20b40d5e6b6", "snippet": "<b>Machine</b> <b>Learning</b> mostly have to deal with two Trade-offs, Bias-Variance Trade-offs; <b>Precision-Recall</b> Trade-offs; Part 1: Bias-Variance Trade-offs 1.1 First thing first, What is Bias, What is Variance? 1.1.1 Bias: To understand it, we must know its general meaning. Cambridge dictionary states as, The action of supporting or opposing a particular person or thing in an unfair way, because of allowing personal opinions to influence your judgment. \u2192 So in the world of stats, it is defined as ...", "dateLastCrawled": "2022-01-30T08:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Cohort-Derived <b>Machine Learning</b> Models for Individual Prediction of ...", "url": "https://academic.oup.com/jid/article/224/7/1198/5835004", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jid/article/224/7/1198/5835004", "snippet": "We used 64 static and 502 time-changing variables: Across prediction horizons and algorithms and in contrast to expert-based standard models, most <b>machine learning</b> models achieved state-of-the-art predictive performances with areas under the receiver operating characteristic <b>curve</b> and <b>precision recall</b> <b>curve</b> ranging from 0.926 to 0.996 and from 0.631 to 0.956, respectively.", "dateLastCrawled": "2021-12-15T15:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Differential and Integral Calculus - Differentiate with Respect to Anything", "url": "https://machinelearningmastery.com/differential-and-integral-calculus-differentiate-with-respect-to-anything/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/differential-and-integral-calculus-differentiate...", "snippet": "The Sweeping Area <b>Analogy</b>; The Fundamental Theorem of Calculus \u2013 Part 1; The Fundamental Theorem of Calculus \u2013 Part 2; Integration Example ; Application of Integration in <b>Machine</b> <b>Learning</b>; Differential and Integral Calculus \u2013 What is the Link? In our journey through calculus so far, we have learned that differential calculus is concerned with the measurement of the rate of change. We have also discovered differentiation, and applied it to different functions from first principles. We ...", "dateLastCrawled": "2022-01-28T21:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "6 Useful <b>Metrics to Evaluate Binary Classification Models</b> \u2013 The Digital ...", "url": "https://thedigitalskye.com/2021/04/19/6-useful-metrics-to-evaluate-binary-classification-models/", "isFamilyFriendly": true, "displayUrl": "https://thedigitalskye.com/2021/04/19/6-useful-metrics-to-evaluate-binary...", "snippet": "Accuracy, <b>precision, recall</b>, F1 Score; ROC <b>curve</b> and ROC AUC; Confusion matrix: The basis of all metrics. Image by Author . A confusion matrix just a way to record how many times the classification model correctly or incorrectly classify things into the corresponding buckets. For example, the model initially classified 10 eggs as hatchable. However, out of those 10 eggs, only 6 are hatchable while the remaining 4 are unhatchable. In this case, the True Positive (TP) is 6 while the False ...", "dateLastCrawled": "2022-01-24T20:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "More Performance Evaluation Metrics for Classification Problems You ...", "url": "https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2020/04/performance-evaluation-metrics-classification.html", "snippet": "Decision Thresholds and Receiver Operating Characteristic (ROC) <b>curve</b> . Warming up: The flow of <b>Machine</b> <b>Learning</b> model . In any binary classification task, we model can only achieve two results, either our model is correct or incorrect in the prediction where we only have two classes. Imagine we now have a classification task to predict if an image is a dog or cat. In supervised <b>learning</b>, we first fit/train a model on training data, then test the model on testing data. Once we have the model ...", "dateLastCrawled": "2022-01-26T05:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine learning - precision recall curve is like</b> stairs - Data Science ...", "url": "https://datascience.stackexchange.com/questions/86830/precision-recall-curve-is-like-stairs", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/86830/<b>precision-recall-curve-is-like</b>...", "snippet": "<b>precision recall curve is like</b> stairs [closed] Ask Question Asked 1 year ago. Active 1 year ago. Viewed 83 times 0 $\\begingroup$ Closed. This question needs details or clarity. It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post. Closed 1 year ago. Improve this question I am training an ensemble model using a 400 data set sample this led to a precision recall curve that looks like stairs ? what would be the reason ...", "dateLastCrawled": "2022-01-14T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Newest &#39;ensemble-modeling&#39; Questions</b> - <b>Data Science Stack Exchange</b>", "url": "https://datascience.stackexchange.com/questions/tagged/ensemble-modeling", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/tagged/ensemble-modeling", "snippet": "Q&amp;A for Data science professionals, <b>Machine</b> <b>Learning</b> specialists, and those interested in <b>learning</b> more about the field. Stack Exchange Network. Stack Exchange network consists of 178 Q&amp;A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange. Loading\u2026 0 +0; Tour Start here for a quick overview of the site Help Center Detailed answers to any questions you might have Meta ...", "dateLastCrawled": "2022-01-10T07:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Future Internet | Free Full-Text | <b>Machine</b> <b>Learning</b> in Detecting COVID ...", "url": "https://www.mdpi.com/1999-5903/13/10/244/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1999-5903/13/10/244/htm", "snippet": "Area under precision\u2013recall curve (PR-AUC): The <b>precision\u2013recall curve is similar</b> to the ROC curve, which is also a performance evaluation metric, especially when the supplied data are heavily imbalanced. PR-AUC is generally used to summarize the precision\u2013recall curve into a single value. If the value of PR-AUC is small, it indicates a bad classifier; a higher value such as 1 indicates an excellent classifier.", "dateLastCrawled": "2022-01-25T13:07:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(precision-recall curve)  is like +(map)", "+(precision-recall curve) is similar to +(map)", "+(precision-recall curve) can be thought of as +(map)", "+(precision-recall curve) can be compared to +(map)", "machine learning +(precision-recall curve AND analogy)", "machine learning +(\"precision-recall curve is like\")", "machine learning +(\"precision-recall curve is similar\")", "machine learning +(\"just as precision-recall curve\")", "machine learning +(\"precision-recall curve can be thought of as\")", "machine learning +(\"precision-recall curve can be compared to\")"]}