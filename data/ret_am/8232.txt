{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Solved: making <b>scale</b> <b>parameter</b> - <b>Autodesk</b> Community", "url": "https://forums.autodesk.com/t5/revit-architecture-forum/making-scale-parameter/td-p/4352162", "isFamilyFriendly": true, "displayUrl": "https://<b>forums.autodesk.com</b>/t5/revit-architecture-forum/making-<b>scale</b>-<b>parameter</b>/td-p/...", "snippet": "Report. 08-02-2013 09:46 AM. After the Revit Technology Conference (RTC) in Auckland, where Marcello Sgambelluri showed his method for scaling families, several bloggers posted their coments and own ideas about scaling families, too, motivated by that class. At this link you will find a list of those blog articles on the topic of scaling ...", "dateLastCrawled": "2022-01-30T10:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Optimization: <b>initialize and update</b> weights \u2014 mxnet documentation", "url": "https://mxnet.apache.org/versions/1.2.1/api/python/optimization/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mxnet.apache.org/versions/1.2.1/api/python/optimization/optimization.html", "snippet": "Modifies objective by <b>adding</b> a penalty for having large weights. sym (Symbol, optional) \u2013 The Symbol this optimizer is applying to. begin_num_<b>update</b> (int, optional) \u2013 The initial number of updates. multi_precision (bool, optional) \u2013 Flag to control the internal precision of the optimizer. False results in using the same precision as the weights (default), True makes internal 32-bit copy of the weights and applies gradients in 32-bit precision even if actual weights used in the model ...", "dateLastCrawled": "2022-01-26T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Weight</b> Decay in Neural Networks - Programmathically", "url": "https://programmathically.com/weight-decay-in-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://programmathically.com/<b>weight</b>-decay-in-neural-networks", "snippet": "<b>Weight</b> decay is a regularization technique in deep learning. <b>Weight</b> decay works by <b>adding</b> a penalty term to the cost function of a neural network which has the effect of shrinking the weights during backpropagation. This helps prevent the network from overfitting the training data as well as the exploding gradient problem.", "dateLastCrawled": "2022-01-30T08:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Raspberry Pi Smart Scale</b> : 10 Steps (with Pictures) - <b>Instructables</b>", "url": "https://www.instructables.com/Raspberry-Pi-Smart-Scale/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.instructables.com</b>/<b>Raspberry-Pi-Smart-Scale</b>", "snippet": "Go to your Initial State account and click on the new data bucket with the name corresponding to the BUCKET_NAME <b>parameter</b> (i.e. My <b>Weight</b> History). Click on Tiles to view your <b>weight</b> history dashboard. You should see three tiles the first time you view your data in Tiles - <b>Update</b>, <b>Weight</b> Date, and <b>Weight</b> (lb). You can customize your dashboard by resizing and moving tiles as well as changing view types and even <b>adding</b> tiles. This dashboard gives you the ability to see your <b>weight</b> history at ...", "dateLastCrawled": "2022-01-28T21:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "XGBoost Parameters \u2014 xgboost 1.6.0-dev documentation", "url": "http://xgboost.readthedocs.io/en/latest/parameter.html", "isFamilyFriendly": true, "displayUrl": "xgboost.readthedocs.io/en/latest/<b>parameter</b>.html", "snippet": "The larger min_child_<b>weight</b> is, the more conservative the algorithm will be. range: [0,\u221e] max_delta_step [default=0] Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the <b>update</b> step more conservative. Usually this <b>parameter</b> is ...", "dateLastCrawled": "2022-02-02T16:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "scikit learn - <b>XGboost</b> python - classifier class <b>weight</b> option? - Stack ...", "url": "https://stackoverflow.com/questions/42192227/xgboost-python-classifier-class-weight-option", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/42192227", "snippet": "This answer is useful. 10. This answer is not useful. Show activity on this post. when using the sklearn wrapper, there is a <b>parameter</b> for <b>weight</b>. example: import <b>xgboost</b> as xgb exgb_classifier = <b>xgboost</b>.XGBClassifier () exgb_classifier.fit (X, y, sample_<b>weight</b>=sample_weights_data) where the <b>parameter</b> shld be array <b>like</b>, length N, equal to the ...", "dateLastCrawled": "2022-01-28T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>SCALE</b> User Guide - Tachyon SDK", "url": "https://help.1e.com/display/TCNSDK/SCALE+User+Guide", "isFamilyFriendly": true, "displayUrl": "https://help.1e.com/display/TCNSDK/<b>SCALE</b>+User+Guide", "snippet": "This method takes a <b>parameter</b> named &quot;GroupName&quot; which is the name of the local group whose members should be queried. A <b>parameter</b> may be mandatory, in which case you must provide a value for that <b>parameter</b> when calling the method, or optional, in which case it can be omitted. A <b>parameter</b> has a corresponding data type, which determines the possible values that can be supplied for that <b>parameter</b>. For example, a <b>parameter</b> may be a &quot;string&quot;, or an &quot;integer&quot;, etc. Method parameters should not be ...", "dateLastCrawled": "2022-01-29T00:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Setting the <b>learning rate</b> of your neural network.", "url": "https://www.jeremyjordan.me/nn-learning-rate/", "isFamilyFriendly": true, "displayUrl": "https://www.jeremyjordan.me/nn-<b>learning-rate</b>", "snippet": "The intuition behind this approach is that we&#39;d <b>like</b> to traverse quickly from the initial parameters to a range of &quot;good&quot; <b>parameter</b> values but then we&#39;d <b>like</b> a <b>learning rate</b> small enough that we can explore the &quot;deeper, but narrower parts of the loss function&quot; (from Karparthy&#39;s CS231n notes). If you&#39;re having a hard time picturing what I just mentioned, recall that too high of a <b>learning rate</b> can cause the <b>parameter</b> <b>update</b> to &quot;jump over&quot; the ideal minima and subsequent updates will either ...", "dateLastCrawled": "2022-01-15T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Payoff Element Types", "url": "https://www.vensim.com/documentation/extended_payoffs.html", "isFamilyFriendly": true, "displayUrl": "https://www.vensim.com/documentation/extended_payoffs.html", "snippet": "These options are equivalent to <b>adding</b> a model variable with a timing switch, <b>like</b>: payoff elm = IF THEN ELSE(Time &gt; FINAL TIME-TIME STEP/2,model var,0) Note that if you are combining initial or final values with other values, you may need to adjust the weights to compensate for the fact that ordinary values are integrated over the course of the simulation.", "dateLastCrawled": "2022-01-18T21:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Solved: <b>Adding</b> <b>a &quot;Discipline</b>&quot; <b>Parameter</b> - <b>Autodesk</b> Community", "url": "https://forums.autodesk.com/t5/revit-architecture-forum/adding-a-quot-discipline-quot-parameter/td-p/7238891", "isFamilyFriendly": true, "displayUrl": "https://<b>forums.autodesk.com</b>/.../<b>adding</b>-a-quot-discipline-quot-<b>parameter</b>/td-p/7238891", "snippet": "<b>Adding</b> <b>a &quot;Discipline</b>&quot; <b>Parameter</b>. 8 REPLIES 8. SOLVED Back to Revit Products Category. Reply. Topic Options. Subscribe to RSS Feed; Mark Topic as New; Mark Topic as Read ; Float this Topic for Current User; Bookmark; Subscribe; Printer Friendly Page; Back to Topic Listing; Previous; Next; Filter by Lables. Categories &quot;Revit&quot; 1 (1; 1603 1; 2012 1; 2019 2; 2019 Revit Family 1; 2019 <b>Update</b> 2 1; 2021 2; 2022 2; 2022 version 1; 2d Drawings 1; 2d-3d lines issue 1; 33D model automatic segmentation 1 ...", "dateLastCrawled": "2022-01-19T16:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "When to Apply L1 or L2 Regularization to Neural Network Weights?", "url": "https://analyticsindiamag.com/when-to-apply-l1-or-l2-regularization-to-neural-network-weights/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/when-to-apply-l1-or-l2-regularization-to-neural-network...", "snippet": "Normally we don\u2019t add each <b>weight</b> of the penalty directly. Before <b>adding</b> them we optimize them using the alpha or lambda parameters. Using these hyperparameters we control the learning process to give attention to the penalty . The value of the alpha hyperparameter varies between zero to one and if the value is in zero we say it as the no penalty and the value is in one we can say it as the full penalty and using the values the hyperparameters controls the model to being biased form a low ...", "dateLastCrawled": "2022-01-28T09:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Options for <b>training</b> deep learning neural network - MATLAB ...", "url": "https://in.mathworks.com/help/deeplearning/ref/trainingoptions.html", "isFamilyFriendly": true, "displayUrl": "https://in.mathworks.com/help/deeplearning/ref/<b>training</b>options.html", "snippet": "Factor for L 2 regularization (<b>weight</b> decay), specified as the comma-separated pair ... <b>Adding</b> a momentum term to the <b>parameter</b> <b>update</b> is one way to reduce this oscillation . The stochastic gradient descent with momentum (SGDM) <b>update</b> is . \u03b8 \u2113 + 1 = \u03b8 \u2113 \u2212 \u03b1 \u2207 E (\u03b8 \u2113) + \u03b3 (\u03b8 \u2113 \u2212 \u03b8 \u2113 \u2212 1), where \u03b3 determines the contribution of the previous gradient step to the current iteration. You can specify this value using the &#39;Momentum&#39; name-value pair argument. To train a ...", "dateLastCrawled": "2022-02-02T16:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Optimization: <b>initialize and update</b> weights \u2014 mxnet documentation", "url": "https://mxnet.apache.org/versions/1.2.1/api/python/optimization/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mxnet.apache.org/versions/1.2.1/api/python/optimization/optimization.html", "snippet": "Modifies objective by <b>adding</b> a penalty for having large weights. sym (Symbol, optional) \u2013 The Symbol this optimizer is applying to. begin_num_<b>update</b> (int, optional) \u2013 The initial number of updates. multi_precision (bool, optional) \u2013 Flag to control the internal precision of the optimizer. False results in using the same precision as the weights (default), True makes internal 32-bit copy of the weights and applies gradients in 32-bit precision even if actual weights used in the model ...", "dateLastCrawled": "2022-01-26T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Setting the <b>learning rate</b> of your neural network.", "url": "https://www.jeremyjordan.me/nn-learning-rate/", "isFamilyFriendly": true, "displayUrl": "https://www.jeremyjordan.me/nn-<b>learning-rate</b>", "snippet": "In previous posts, I&#39;ve discussed how we can train neural networks using backpropagation with gradient descent.One of the key hyperparameters to set in order to train a neural network is the <b>learning rate</b> for gradient descent. As a reminder, this <b>parameter</b> scales the magnitude of our <b>weight</b> updates in order to minimize the network&#39;s loss function.", "dateLastCrawled": "2022-01-15T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Regularization in Neural Networks", "url": "https://cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.5-Regularization.pdf", "isFamilyFriendly": true, "displayUrl": "https://cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.5-Regularization.pdf", "snippet": "<b>Weight</b> Decay \u2022The name <b>weight</b> decay is due to the following \u2022To prevent overfitting, every time we <b>update</b> a <b>weight</b> wwith the gradient \u2207Jin respect to w, we also subtract from it \u03bbw. \u2022This gives the weights a tendency to decay towards zero, hence the name. 11", "dateLastCrawled": "2022-02-03T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Use <b>Weight Decay to Reduce Overfitting</b> of Neural Network in Keras", "url": "https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with...", "snippet": "<b>Update</b> the grid search example to grid search within the best-performing order of magnitude of <b>parameter</b> values. Repeated Regularization of Model . Create a new example to continue the training of a fit model with increasing levels of regularization (e.g. 1E-6, 1E-5, etc.) and see if it results in a better performing model on the test set.", "dateLastCrawled": "2022-01-27T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "scikit learn - <b>XGboost</b> python - classifier class <b>weight</b> option? - Stack ...", "url": "https://stackoverflow.com/questions/42192227/xgboost-python-classifier-class-weight-option", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/42192227", "snippet": "This answer is useful. 10. This answer is not useful. Show activity on this post. when using the sklearn wrapper, there is a <b>parameter</b> for <b>weight</b>. example: import <b>xgboost</b> as xgb exgb_classifier = <b>xgboost</b>.XGBClassifier () exgb_classifier.fit (X, y, sample_<b>weight</b>=sample_weights_data) where the <b>parameter</b> shld be array like, length N, equal to the ...", "dateLastCrawled": "2022-01-28T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>android</b> - How to set <b>layout_weight</b> attribute dynamically from code ...", "url": "https://stackoverflow.com/questions/4641072/how-to-set-layout-weight-attribute-dynamically-from-code", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/4641072", "snippet": "It&#39;ll be very <b>similar</b> to this question/answer: ... The last <b>parameter</b> is the <b>weight</b>. Share. Follow edited Feb 27 &#39;17 at 13:08. David. 3,282 3 3 gold badges 34 34 silver badges 46 46 bronze badges. answered Jan 9 &#39;11 at 19:05. Erich Douglass Erich Douglass. 50.7k 11 11 gold badges 74 74 silver badges 60 60 bronze badges. 9. 2. It should be param = new LayoutParams(LayoutParams.MATCH_PARENT, LayoutParams.MATCH_PARENT, (float) 1.0); \u2013 Mithun Sreedharan. Sep 1 &#39;11 at 12:55. 54. If you don&#39;t ...", "dateLastCrawled": "2022-01-27T16:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Solved: Multiple Scales on a Sheet: &quot;<b>As Indicated</b>&quot; - <b>Autodesk</b> Community", "url": "https://forums.autodesk.com/t5/revit-architecture-forum/multiple-scales-on-a-sheet-quot-as-indicated-quot/td-p/3081030", "isFamilyFriendly": true, "displayUrl": "https://<b>forums.autodesk.com</b>/t5/revit-architecture-forum/multiple-<b>scales</b>-on-a-sheet...", "snippet": "The &quot;<b>scale</b>&quot; <b>parameter</b> associated with sheets is something defined at the system level. If all views on a sheet are the same <b>scale</b> the value will resolve with the <b>scale</b> the view/views use. If the views have different values it changes to &quot;<b>as indicated</b>&quot;. By controling this at the system level it prevents coordination errors from occuring. If you need control over this value you just add a shared parmeter to the sheet and remove the &quot;<b>scale</b>&quot; label and replace it with a label that reads the ...", "dateLastCrawled": "2022-01-30T03:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>SCALE</b> User Guide - Tachyon SDK", "url": "https://help.1e.com/display/TCNSDK/SCALE+User+Guide", "isFamilyFriendly": true, "displayUrl": "https://help.1e.com/display/TCNSDK/<b>SCALE</b>+User+Guide", "snippet": "This method takes a <b>parameter</b> named &quot;GroupName&quot; which is the name of the local group whose members should be queried. A <b>parameter</b> may be mandatory, in which case you must provide a value for that <b>parameter</b> when calling the method, or optional, in which case it can be omitted. A <b>parameter</b> has a corresponding data type, which determines the possible values that can be supplied for that <b>parameter</b>. For example, a <b>parameter</b> may be a &quot;string&quot;, or an &quot;integer&quot;, etc. Method parameters should not be ...", "dateLastCrawled": "2022-01-29T00:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding the scaling of L\u00b2 regularization in the context of neural ...", "url": "https://towardsdatascience.com/understanding-the-scaling-of-l%C2%B2-regularization-in-the-context-of-neural-networks-e3d25f8b50db", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-scaling-of-l\u00b2-regularization-in-the...", "snippet": "Equation 3: <b>Weight</b> decay for neural networks. When looking at regularization from this angle, the common form starts to become clear. To get this term added in the <b>weight</b> <b>update</b>, we \u201chijack\u201d the cost function J, and add a term that, when derived, will yield this desired -\u03bb\u2219w; the term to add is, of course, -0.5 \u03bb\u2219w\u00b2.Taking the derivative of J -0.5 \u03bb\u2219w\u00b2 will thus yield \u2207J-\u03bb\u2219w, which is what we aimed for.", "dateLastCrawled": "2022-02-02T18:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Linear Classification and Perceptron</b>", "url": "https://cmci.colorado.edu/classes/INFO-4604/files/slides-3_perceptron.pdf", "isFamilyFriendly": true, "displayUrl": "https://cmci.colorado.edu/classes/INFO-4604/files/slides-3_perceptron.pdf", "snippet": "where the final feature <b>weight</b> is the value of the bias. \u2022Then we <b>can</b> <b>update</b> this <b>parameter</b> the same way as all the other weights. Learning the Weights The vector of wvalues is called the <b>weight</b> vector. Is the bias bcounted when we use this phrase? \u2022Usually\u2026 especially if you include it by using the trick of <b>adding</b> an extra feature with value 1 rather than treating it separately. \u2022Just be clear with your notation. Linear Separability The training instances are linearly separable if ...", "dateLastCrawled": "2022-02-02T03:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to <b>Update</b> LSTM Networks During Training <b>for Time Series Forecasting</b>", "url": "https://machinelearningmastery.com/update-lstm-networks-training-time-series-forecasting/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>update</b>-lstm-networks-training-<b>time-series-forecasting</b>", "snippet": "The results for experiments that <b>update</b> the model should be compared directly to experiments of a fixed model that uses the same number of overall epochs to see if <b>adding</b> the additional test patterns to the training dataset makes any noticeable difference. For example, 2 <b>update</b> epochs for each test pattern could be compared to a fixed model trained for 500 + (12-1) * 2) or 522 epochs, an <b>update</b> model 5 compared to a fixed model fit for 500 + (12-1) * 5) or 555 epochs, and so on.", "dateLastCrawled": "2022-02-03T09:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "scikit learn - <b>XGboost</b> python - classifier class <b>weight</b> option? - Stack ...", "url": "https://stackoverflow.com/questions/42192227/xgboost-python-classifier-class-weight-option", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/42192227", "snippet": "This answer is useful. 10. This answer is not useful. Show activity on this post. when using the sklearn wrapper, there is a <b>parameter</b> for <b>weight</b>. example: import <b>xgboost</b> as xgb exgb_classifier = <b>xgboost</b>.XGBClassifier () exgb_classifier.fit (X, y, sample_<b>weight</b>=sample_weights_data) where the <b>parameter</b> shld be array like, length N, equal to the ...", "dateLastCrawled": "2022-01-28T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[proposal] [discussion] Refactor pruning/<b>weight</b>_norm/spectral_norm ...", "url": "https://github.com/pytorch/pytorch/issues/7313", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/pytorch/pytorch/issues/7313", "snippet": "Currently the <b>weight</b>_norm and spectral_norm are patching a passed module + implement special functions for <b>adding</b>/removing these from a module.. Some ideas for refactoring to make it less tricky: provide a stable signature for getting <b>weight</b>, then they <b>can</b> be cleanly used with methods such as torch.matmul and F.conv2d; if module patching (<b>adding</b> some new buffers as parameters and registering a hook) is needed and is a reasonable pattern, provide a user-facing stable abstraction for it ...", "dateLastCrawled": "2022-02-02T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Is it the same <b>adding</b> <b>weight decay</b> to all the layers (including input ...", "url": "https://github.com/keras-team/keras/issues/2717", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/keras-team/keras/issues/2717", "snippet": "@apatsekin good point! I have not <b>thought</b> about it. Let me paste here part of bbabenko article you mention:. In frameworks that implement L2 regularization, the gradients the solver is using are for \u201ctotal loss\u201d, which has L2 regularization baked into it (this fact is typically abstracted away from the solver \u2014 it doesn\u2019t \u201cknow\u201d anything about whether a regularization term was included in the loss or not).", "dateLastCrawled": "2022-01-29T12:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>MariaDB MaxScale Configuration Guide</b> - MariaDB Knowledge Base", "url": "https://mariadb.com/kb/en/mariadb-maxscale-24-mariadb-maxscale-configuration-guide/", "isFamilyFriendly": true, "displayUrl": "https://mariadb.com/kb/en/mariadb-max<b>scale</b>-24-<b>mariadb-maxscale-configuration-guide</b>", "snippet": "Here is an excerpt from an example configuration with the serv_<b>weight</b> <b>parameter</b> used as the weighting <b>parameter</b>. [server1] type=server address=127.0.0.1 port=3000 protocol=MariaDBBackend serv_<b>weight</b>=3 [server2] type=server address=127.0.0.1 port=3001 protocol=MariaDBBackend serv_<b>weight</b>=1 [Read-Service] type=service router=readconnroute servers=server1,server2 weightby=serv_<b>weight</b> With this configuration and a heavy query load, the server server1 will get most of the connections and about a ...", "dateLastCrawled": "2022-01-29T16:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Solved: how to creat a new <b>Scale</b> (ex: 1=150 &amp; 1=250) - <b>Autodesk</b> Community", "url": "https://forums.autodesk.com/t5/revit-architecture-forum/how-to-creat-a-new-scale-ex-1-150-1-250/td-p/2047383", "isFamilyFriendly": true, "displayUrl": "https://<b>forums.autodesk.com</b>/t5/<b>revit</b>-architecture-forum/how-to-creat-a-new-<b>scale</b>-ex-1...", "snippet": "02-03-2017 06:20 AM. When you are in a plan view, click on your View <b>Scale</b> Dropdown. After you clicked it, then you add a custom <b>Scale</b> Value 1: 250 (for example). Hope this helps! LinkedIn | Twitter | Blog (Coming Soon!) If this post resolved your issue, kindly Accept as the Solution below. Kudos are always welcome \u21d8.", "dateLastCrawled": "2022-01-31T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Solved: View <b>filter</b> based on shared <b>parameter</b> - <b>Autodesk</b> Community", "url": "https://forums.autodesk.com/t5/revit-architecture-forum/view-filter-based-on-shared-parameter/td-p/8439821", "isFamilyFriendly": true, "displayUrl": "https://<b>forums.autodesk.com</b>/.../view-<b>filter</b>-based-on-shared-<b>parameter</b>/td-p/8439821", "snippet": "The <b>parameter</b> is not available in the <b>filter</b> rules dropdown. I then tried making that <b>parameter</b> instance. Same result. I tried once again, using a shared <b>parameter</b>, as instance. Once again, not available in the <b>filter</b> rules dropdown, and I realize now that none of my custom parameters are visible in this dialog. Every option is a stock <b>parameter</b>.", "dateLastCrawled": "2022-01-27T23:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Working with legends in sheets of diffrent <b>scale</b>? : Revit", "url": "https://www.reddit.com/r/Revit/comments/pvi74o/working_with_legends_in_sheets_of_diffrent_scale/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../pvi74o/working_with_legends_in_sheets_of_diffrent_<b>scale</b>", "snippet": "How do you go about <b>adding</b> lengends to sheets with diffrent <b>scale</b>? For this use-case this is a system drawing in 1:1 <b>scale</b>. If i create a legend in 1:1 i <b>can</b> draw it to fit in the &quot;sidebar&quot; of the sheet in say size A0. But if the titleblok is a A3, this area is smaler and the legend will be to big.", "dateLastCrawled": "2022-01-19T00:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Setting the <b>learning rate</b> of your neural network.", "url": "https://www.jeremyjordan.me/nn-learning-rate/", "isFamilyFriendly": true, "displayUrl": "https://www.jeremyjordan.me/nn-<b>learning-rate</b>", "snippet": "In previous posts, I&#39;ve discussed how we <b>can</b> train neural networks using backpropagation with gradient descent.One of the key hyperparameters to set in order to train a neural network is the <b>learning rate</b> for gradient descent. As a reminder, this <b>parameter</b> scales the magnitude of our <b>weight</b> updates in order to minimize the network&#39;s loss function.", "dateLastCrawled": "2022-01-15T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to <b>Update</b> LSTM Networks During Training <b>for Time Series Forecasting</b>", "url": "https://machinelearningmastery.com/update-lstm-networks-training-time-series-forecasting/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>update</b>-lstm-networks-training-<b>time-series-forecasting</b>", "snippet": "The results for experiments that <b>update</b> the model should <b>be compared</b> directly to experiments of a fixed model that uses the same number of overall epochs to see if <b>adding</b> the additional test patterns to the training dataset makes any noticeable difference. For example, 2 <b>update</b> epochs for each test pattern could <b>be compared</b> to a fixed model trained for 500 + (12-1) * 2) or 522 epochs, an <b>update</b> model 5 <b>compared</b> to a fixed model fit for 500 + (12-1) * 5) or 555 epochs, and so on.", "dateLastCrawled": "2022-02-03T09:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to Use <b>Weight Decay to Reduce Overfitting</b> of Neural Network in Keras", "url": "https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with...", "snippet": "MLP Model With <b>Weight</b> Regularization. We <b>can</b> add <b>weight</b> regularization to the hidden layer to reduce the overfitting of the model to the training dataset and improve the performance on the holdout set. We will use the L2 vector norm also called <b>weight</b> decay with a regularization <b>parameter</b> (called alpha or lambda) of 0.001, chosen arbitrarily.", "dateLastCrawled": "2022-01-27T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Optimization: <b>initialize and update</b> weights \u2014 mxnet documentation", "url": "https://mxnet.apache.org/versions/1.5.0/api/python/optimization/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mxnet.apache.org/versions/1.5.0/api/python/optimization/optimization.html", "snippet": "Modifies objective by <b>adding</b> a penalty for having large weights. sym (Symbol, optional, default None) \u2013 The Symbol this optimizer is applying to. begin_num_<b>update</b> (int, optional, default 0 ) \u2013 The initial number of updates. multi_precision (bool, optional, default False) \u2013 Flag to control the internal precision of the optimizer. False: results in using the same precision as the weights (default), True: makes internal 32-bit copy of the weights and applies gradients in 32-bit precision ...", "dateLastCrawled": "2022-01-18T11:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "python - Use both sample_<b>weight and class_weight simultaneously</b> - Stack ...", "url": "https://stackoverflow.com/questions/48173168/use-both-sample-weight-and-class-weight-simultaneously", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/48173168", "snippet": "The sample_<b>weight</b> is more when you want to define a <b>weight</b> or importance for each data element. For example if you pass: class_<b>weight</b> = {0 : 1. , 1: 50.} you will be saying that every sample from class 1 would count as 50 samples from class 0, therefore giving more &quot;importance&quot; to your elements from class 1 (as you have less of those samples ...", "dateLastCrawled": "2022-02-01T23:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Optimization: <b>initialize and update</b> weights \u2014 mxnet documentation", "url": "https://mxnet.apache.org/versions/1.2.1/api/python/optimization/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mxnet.apache.org/versions/1.2.1/api/python/optimization/optimization.html", "snippet": "Modifies objective by <b>adding</b> a penalty for having large weights. sym (Symbol, optional) \u2013 The Symbol this optimizer is applying to. begin_num_<b>update</b> (int, optional) \u2013 The initial number of updates. multi_precision (bool, optional) \u2013 Flag to control the internal precision of the optimizer. False results in using the same precision as the weights (default), True makes internal 32-bit copy of the weights and applies gradients in 32-bit precision even if actual weights used in the model ...", "dateLastCrawled": "2022-01-26T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "DeepSpeed: <b>Extreme-scale model training for everyone</b> - <b>Microsoft Research</b>", "url": "https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.microsoft.com</b>/en-us/research/blog/deepspeed-extreme-<b>scale</b>-model-training...", "snippet": "As a result, DeepSpeed <b>can</b> <b>scale</b> to fit the most massive models in memory without sacrificing speed. Learn the challenges of obtaining memory and compute efficiency for gigantic models . Memory Efficiency: The memory requirements to train a trillion-<b>parameter</b> model are far beyond what is available in a single GPU device. Training using the Adam optimizer in mixed precision requires approximately 16 terabytes (TB) of memory just to store the model states (parameters, gradients, and optimizer ...", "dateLastCrawled": "2022-01-27T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "{PDF} Tablet Evaluation \u2013 Pharmaceutics Pharmaceutical Apparatus ...", "url": "https://pharmawiki.in/pdf-tablet-evaluation-pharmaceutics-pharmaceutical-apparatus-material-ppt/", "isFamilyFriendly": true, "displayUrl": "https://pharmawiki.in/pdf-tablet-evaluation-pharmaceutics-pharmaceutical-apparatus...", "snippet": "6) <b>WEIGHT</b> VARIATION: <b>Weight</b> variation test is performed to check that the manufactured tablets have an uniform <b>weight</b>. As per USP twenty tablets are weighed individually and an compendia <b>weight</b> is taken, the average <b>weight</b> is obtained by dividing the compendia <b>weight</b> by 20, now the average <b>weight</b> is <b>compared</b> to the individual <b>weight</b> of the tablet,", "dateLastCrawled": "2022-01-31T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>React</b> API: Best Practices for Building Large-<b>Scale</b> Applications | ButterCMS", "url": "https://buttercms.com/blog/best-practices-for-building-a-large-scale-react-application", "isFamilyFriendly": true, "displayUrl": "https://buttercms.com/blog/best-practices-for-building-a-large-<b>scale</b>-<b>react</b>-application", "snippet": "Learn how your Marketing team <b>can</b> <b>update</b> your <b>React</b> app with ButterCMS. API calls and In-app actions. While working with redux, one thing that stands out is the usage of predefined actions. It makes the changes in data throughout the app, much more predictable. Even though it may seem like a lot of work - to define a bunch of constants in a large app, Step 2 of the planning phase makes it a whole lot easier. export const BOOK_ACTIONS = { GET:&#39;GET_BOOK&#39;, LIST:&#39;GET_BOOKS&#39;, POST:&#39;POST_BOOK ...", "dateLastCrawled": "2022-02-02T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "ml-agents/<b>Training-ML-Agents</b>.md at main \u00b7 Unity-Technologies ... - <b>GitHub</b>", "url": "https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-ML-Agents.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>Unity-Technologies/ml-agents</b>/blob/main/docs/<b>Training-ML-Agents</b>.md", "snippet": "Behaviors found in the environment that aren&#39;t specified in the YAML will now use the default_settings, and unspecified settings in behavior configurations will default to the values in default_settings if specified there.. Environment Parameters. In order to control the EnvironmentParameters in the Unity simulation during training, you need to add a section called environment_parameters.For example you <b>can</b> set the value of an EnvironmentParameter called my_environment_<b>parameter</b> to 3.0 with ...", "dateLastCrawled": "2022-01-31T08:39:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> fundamentals I: An <b>analogy</b> | Finn Rietz.dev", "url": "http://www.finnrietz.dev/machine%20learning/part-1-analogy/", "isFamilyFriendly": true, "displayUrl": "www.finnrietz.dev/<b>machine</b> <b>learning</b>/part-1-<b>analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> fundamentals I: An <b>analogy</b> Tags: <b>Machine</b> <b>Learning</b> fundamentals. Categories: <b>Machine</b> <b>Learning</b>. Updated: March 3, 2020. 11 minute read On this page. Introduction and motivation; Setting the scene for supervised <b>learning</b>; Telling right from wrong ; About parameters and artificial <b>learning</b>; Summary; Introduction and motivation. I remember the first weeks of attending Professor Uc-Cetina\u2019s lecture on <b>Machine</b> <b>Learning</b>, which was my first \u201creal\u201d academic encounter with the ...", "dateLastCrawled": "2022-01-16T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b>. November 2017 ; Authors: Colleen Farrelly. Jenzabar; Download file PDF Read file. Download file PDF. Read file. Download citation. Copy link Link copied. Read file ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Hitchhiker\u2019s Guide to Optimization in <b>Machine Learning</b> | by Aman ...", "url": "https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-machine-learning-edcf5a104210", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-<b>machine</b>...", "snippet": "<b>Machine Learning</b> is the ideal culmination of Applied Mathematics and Computer Science, where we train and use data-driven applications to run inferences on the available data. Generally speaking, for an ML task, the type of inference (i.e., the prediction that the model makes) varies on the basis of the problem statement and the type of data one is dealing with for the task at hand. However, in contrast to these dissimilarities, these algorithms tend to share some similarities as well ...", "dateLastCrawled": "2022-02-02T19:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "Optimization methods are applied to minimize the loss function by changing the <b>parameter</b> values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one loss is L0-1 = 1 (m &lt;= 0); in zero-one loss, value of loss is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this loss is it is not differentiable, non-convex, and also NP-hard. Hence, in order to make optimization feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Artificial Intelligence Open Elective Module</b> 5: <b>Learning</b> CH15", "url": "https://hemanthrajhemu.github.io/FutureVisionBIE/DOWNLOAD/AI/AI_Module5_Learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://hemanthrajhemu.github.io/FutureVisionBIE/DOWNLOAD/AI/AI_Module5_<b>Learning</b>.pdf", "snippet": "\u2022 <b>Machine</b> <b>learning</b> systems perform the following iteratively: Produce a result Evaluate it against expected result Tweak a system \u2022 <b>Machine</b> <b>learning</b> systems also discover patterns without prior expected results \u2022 Open box: changes are clearly visible in the knowledge base and clearly interpretable by the human users. \u2022 Black box: changes done to the system are not readily visible or understandable. 5 Learner Architecture \u2022 <b>Machine</b> <b>learning</b> systems has the four main components ...", "dateLastCrawled": "2022-01-06T03:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b> 101: An Intuitive Introduction to <b>Gradient</b> Descent ...", "url": "https://towardsdatascience.com/machine-learning-101-an-intuitive-introduction-to-gradient-descent-366b77b52645", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine</b>-<b>learning</b>-101-an-intuitive-introduction-to...", "snippet": "To build a <b>Machine</b> <b>Learning</b> model, we often need at least 3 things. A problem T, a performance measure P, and an experience E, ... In <b>analogy</b>, we can think of <b>Gradient</b> Descent as being a ball rolling down on a valley. The deepest valley is the optimal global minimum and that is the place we aim for. Depending on where the ball starts rolling, it may rest in the bottom of a valley. But not in the lowest one. This is called a local minimum and in the context of our model, the valley is the ...", "dateLastCrawled": "2022-01-30T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - How does Gradient Descent work? - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/102509/how-does-gradient-descent-work", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/102509/how-does-gradient-descent-work", "snippet": "I know the calculus and the famous hill and valley <b>analogy</b> (so to say) of gradient descent. However, I find the <b>update</b> rule of the weights and biases quite terrible. Let&#39;s say we have a couple of parameters, one weight &#39;w&#39; and one bias &#39;b&#39;. Using SGD, we can <b>update</b> both w and b after the evaluation of each mini-batch. If the size of the mini ...", "dateLastCrawled": "2022-01-16T12:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent in Logistic Regression [Explained</b> for ... - upGrad blog", "url": "https://www.upgrad.com/blog/gradient-descent-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/<b>gradient-descent-in-logistic-regression</b>", "snippet": "The reason is simple: it needs to compute the gradient, and <b>update</b> values simultaneously for every <b>parameter</b>,and that too for every training example. So think about all those calculations! It\u2019s massive, and hence there was a need for a slightly modified Gradient Descent Algorithm, namely \u2013 Stochastic Gradient Descent Algorithm (SGD). The only difference SGD has with Normal Gradient Descent is that, in SGD, we don\u2019t deal with the entire training instance at a single time. In SGD, we ...", "dateLastCrawled": "2022-01-28T16:49:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(parameter update)  is like +(adding weight to a scale)", "+(parameter update) is similar to +(adding weight to a scale)", "+(parameter update) can be thought of as +(adding weight to a scale)", "+(parameter update) can be compared to +(adding weight to a scale)", "machine learning +(parameter update AND analogy)", "machine learning +(\"parameter update is like\")", "machine learning +(\"parameter update is similar\")", "machine learning +(\"just as parameter update\")", "machine learning +(\"parameter update can be thought of as\")", "machine learning +(\"parameter update can be compared to\")"]}