{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>DQN</b> <b>Algorithm</b>: A father-son tale. The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b> ...", "url": "https://medium.com/analytics-vidhya/dqn-algorithm-a-father-son-tale-b4bf6ff1ae2f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>dqn</b>-<b>algorithm</b>-a-father-son-tale-b4bf6ff1ae2f", "snippet": "The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) Reinforcement <b>learning</b> <b>algorithm</b> has a surprisingly simple and real life analogy with which it can be explained. It helps understand the sequence of operations involved by ...", "dateLastCrawled": "2022-01-13T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep</b> <b>Q Network</b>(<b>DQN</b>)- Applying Neural Network as a functional ...", "url": "https://medium.com/intro-to-artificial-intelligence/deep-q-network-dqn-applying-neural-network-as-a-functional-approximation-in-q-learning-6ffe3b0a9062", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/intro-to-artificial-intelligence/<b>deep</b>-<b>q-network</b>-<b>dqn</b>-applying-neural...", "snippet": "<b>DQN</b> architecture. Source:[1] In <b>DQN</b>, we make use of two separate networks with the same architecture to estimate the target and prediction Q values for the stability of the Q-<b>learning</b> <b>algorithm</b>.", "dateLastCrawled": "2022-02-03T05:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep Q-Learning Tutorial: minDQN</b>. A Practical Guide to <b>Deep</b> Q-Networks ...", "url": "https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>deep-q-learning-tutorial-mindqn</b>-2a4c855abffc", "snippet": "One of the core concepts in Reinforcement <b>Learning</b> is the <b>Deep</b> Q-<b>Learning</b> <b>algorithm</b>. Naturally, a lot of us want to learn more about the algorithms behind these impressive accomplishments. In this tutorial, we\u2019ll be sharing a minimal <b>Deep</b> <b>Q-Network</b> implementation (minDQN) meant as a practical guide to help new learners code their own <b>Deep</b> Q-Networks.", "dateLastCrawled": "2022-02-03T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Deep</b> <b>Q-network</b> - msg <b>Machine Learning Catalogue</b>", "url": "https://machinelearningcatalogue.com/algorithm/alg_deep-q-network.html", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearningcatalogue</b>.com/<b>algorithm</b>/alg_<b>deep</b>-<b>q-network</b>.html", "snippet": "A <b>deep</b> <b>Q-network</b> (<b>DQN</b>) is a neural network used to learn a Q-function. As most reinforcement <b>learning</b> is associated with complex (typically visual) inputs, the initial layers of a <b>DQN</b> are normally convolutional. There are two ways of using a neural network to calculate expected rewards for actions: the network accepts the environment state and a possible action as input and outputs the expected reward; the network accepts the environment state as input and outputs a vector of possible ...", "dateLastCrawled": "2022-01-11T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Deep</b> Q <b>Learning</b> and <b>Deep</b> Q Networks (<b>DQN</b>) Intro ... - Python Programming", "url": "https://pythonprogramming.net/deep-q-learning-dqn-reinforcement-learning-python-tutorial/", "isFamilyFriendly": true, "displayUrl": "https://pythonprogramming.net/<b>deep</b>-q-<b>learning</b>-<b>dqn</b>-reinforcement-<b>learning</b>-python-tutorial", "snippet": "A typical <b>DQN</b> model might look something <b>like</b>: The <b>DQN</b> neural network model is a regression model, which typically will output values for each of our possible actions. These values will be continuous float values, and they are directly our Q values. As we enage in the environment, we will do a .predict() to figure out our next move (or move randomly). When we do a .predict(), we will get the 3 float values, which are our Q values that map to actions. We will then do an argmax on these, <b>like</b> ...", "dateLastCrawled": "2022-01-30T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Introduction to Various <b>Reinforcement Learning</b> Algorithms. Part I (Q ...", "url": "https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-various-<b>reinforcement-learning</b>...", "snippet": "2.3 <b>Deep</b> <b>Q Network</b> (<b>DQN</b>) Although Q-<b>learning</b> is a very powerful <b>algorithm</b>, its main weakness is lack of generality. If you view Q-<b>learning</b> as updating numbers in a two-dimensional array (Action Space * State Space), it, in fact, resembles dynamic programming. This indicates that for states that the Q-<b>learning</b> agent has not seen before, it has ...", "dateLastCrawled": "2022-02-03T03:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "GitHub - JohDonald/<b>Deep</b>-Q-<b>Learning</b>-<b>Deep</b>-SARSA-LunarLander-v2: Applying ...", "url": "https://github.com/JohDonald/Deep-Q-Learning-Deep-SARSA-LunarLander-v2", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/JohDonald/<b>Deep</b>-Q-<b>Learning</b>-<b>Deep</b>-SARSA-LunarLander-v2", "snippet": "<b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) A basic Q <b>Learning</b> <b>algorithm</b> is implemented to train the agent. Much <b>like</b> with the <b>Deep</b> SARSA I use a multi-layer neural network to estimate the Q table and a replay buffer to sample the information of what happened in the episode and to update/train the neural network.", "dateLastCrawled": "2021-11-25T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Improvements in <b>Deep</b> Q <b>Learning</b>: Dueling Double <b>DQN</b>, Prioritized ...", "url": "http://www.sefidian.com/2021/04/20/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-q-targets/", "isFamilyFriendly": true, "displayUrl": "www.sefidian.com/2021/04/20/improvements-in-<b>deep</b>-q-<b>learning</b>-dueling-double-<b>dqn</b>...", "snippet": "Inspired by Double Q-<b>Learning</b>, Double <b>DQN</b> uses two different <b>Deep</b> Neural Networks, <b>Deep</b> <b>Q Network</b> (<b>DQN</b>) and Target Network. Note that there is no <b>learning</b> rate \u03b1 when updating the Q-values since it will be used in the optimization stage of updating the parameters of <b>Deep</b> <b>Q Network</b>. <b>Deep</b> <b>Q Network</b> \u2014 selecting the best action a with maximum Q ...", "dateLastCrawled": "2022-01-23T04:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Q-Learning with Keras and Gym</b> \u00b7 Keon&#39;s Blog", "url": "https://keon.github.io/deep-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://keon.github.io/<b>deep</b>-q-<b>learning</b>", "snippet": "Google\u2019s DeepMind published its famous paper Playing Atari with <b>Deep</b> Reinforcement <b>Learning</b>, in which they introduced a new <b>algorithm</b> called <b>Deep</b> <b>Q Network</b> (<b>DQN</b> for short) in 2013. It demonstrated how an AI agent can learn to play games by just observing the screen without any prior information about those games. The result turned out to be pretty impressive. This paper opened the era of what is called \u2018<b>deep</b> reinforcement <b>learning</b>\u2019, a mix of <b>deep</b> <b>learning</b> and reinforcement <b>learning</b>.", "dateLastCrawled": "2022-02-01T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Deep Q-Learning - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/deep-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>deep</b>-q-<b>learning</b>", "snippet": "<b>Like</b> Article. <b>Deep</b> Q-<b>Learning</b>. Difficulty Level : Easy; Last Updated : 18 Jun, 2019. Prerequisites: Q-<b>Learning</b>. The process of Q-<b>Learning</b> creates an exact matrix for the working agent which it can \u201crefer to\u201d to maximize its reward in the long run. Although this approach is not wrong in itself, this is only practical for very small environments and quickly loses it\u2019s feasibility when the number of states and actions in the environment increases. The solution for the above problem comes ...", "dateLastCrawled": "2022-02-03T18:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Reinforcement Learning Explained Visually (Part</b> 5): <b>Deep</b> Q Networks ...", "url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning-explained-visually-part</b>-5-<b>deep</b>-q...", "snippet": "The underlying principle of a <b>Deep</b> <b>Q Network</b> is very <b>similar</b> to the Q <b>Learning</b> <b>algorithm</b>. It starts with arbitrary Q-value estimates and explores the environment using the \u03b5-greedy policy. And at its core, it uses the same notion of dual actions, a current action with a current Q-value and a target action with a target Q-value, for its update logic to improve its Q-value estimates.", "dateLastCrawled": "2022-01-31T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Introduction to Various <b>Reinforcement Learning</b> Algorithms. Part I (Q ...", "url": "https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-various-<b>reinforcement-learning</b>...", "snippet": "2.3 <b>Deep</b> <b>Q Network</b> (<b>DQN</b>) Although Q-<b>learning</b> is a very powerful <b>algorithm</b>, its main weakness is lack of generality. If you view Q-<b>learning</b> as updating numbers in a two-dimensional array (Action Space * State Space), it, in fact, resembles dynamic programming. This indicates that for states that the Q-<b>learning</b> agent has not seen before, it has ...", "dateLastCrawled": "2022-02-03T03:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep</b> <b>Q\u2010network</b> application for optimal energy management in a grid\u2010tied ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/tje2.12128?af=R", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/tje2.12128?af=R", "snippet": "A <b>deep</b> <b>Q-network</b> (<b>DQN</b>), a <b>deep</b> reinforcement <b>learning</b> <b>algorithm</b>, can tackle the problems outlined in Section 2.1 by combining supervised <b>learning</b> and RL . <b>DQN</b> incorporates <b>deep</b> <b>learning</b> techniques into Q-<b>learning</b> utilizing the experience replay method borrowed from the batch reinforcement <b>learning</b> technique . In place of a lookup table, a <b>deep</b> ...", "dateLastCrawled": "2022-02-07T16:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Episodic Memory <b>and Deep Q-Networks - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/episodic-memory-and-deep-q-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/episodic-memory-and-<b>deep</b>-q-networks", "snippet": "<b>Deep</b> Q-Networks (<b>DQN</b>): A well-established technique to perform the above task is Q-<b>learning</b>, where we decide on a function called Q-function which is important for the success of the <b>algorithm</b>. <b>DQN</b> uses the neural networks as Q-function to approximate the action values Q(s, a, \\theta) where the parameter of network and (s,a) represents the state-action pair .", "dateLastCrawled": "2022-01-16T05:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Minibatch Recursive Least Squares Q-<b>Learning</b>", "url": "https://www.hindawi.com/journals/cin/2021/5370281/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/cin/2021/5370281", "snippet": "The <b>deep</b> <b>Q-network</b> (<b>DQN</b>) is one of the most successful reinforcement <b>learning</b> algorithms, but it has some drawbacks such as slow convergence and instability. In contrast, the traditional reinforcement <b>learning</b> algorithms with linear function approximation usually have faster convergence and better stability, although they easily suffer from the curse of dimensionality. In recent years, many improvements to <b>DQN</b> have been made, but they seldom make use of the advantage of traditional ...", "dateLastCrawled": "2022-02-02T07:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Going Deeper Into Reinforcement Learning: Understanding Deep</b>-Q-Networks", "url": "https://danieltakeshi.github.io/2016/12/01/going-deeper-into-reinforcement-learning-understanding-dqn/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/2016/12/01/<b>going-deeper-into-reinforcement-learning</b>...", "snippet": "The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) <b>algorithm</b>, as introduced by DeepMind in a NIPS 2013 workshop paper, and later published in Nature 2015 can be credited with revolutionizing reinforcement <b>learning</b>. In this post, therefore, I would like to give a guide to a subset of the <b>DQN</b> <b>algorithm</b>. This is a continuation of an earlier reinforcement <b>learning</b> article about linear function approximators. My contribution here will be orthogonal to my previous post about the preprocessing steps for game frames. Before ...", "dateLastCrawled": "2022-02-03T15:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Deep</b> <b>Q-network</b>-based traffic signal control models", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256405", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256405", "snippet": "The <b>DQN</b> was selected as a reinforcement <b>learning</b> <b>algorithm</b> because it is widely used in other fields and is considered suitable for developing traffic signal control models. To implement the <b>DQN</b>, Python ver. 3.7, and Pytorch, which is a <b>deep</b> <b>learning</b> library, were used. The <b>DQN</b> implementation was performed using the experience replay memory, <b>deep</b> neural network, <b>learning</b> module, and action selection. The experience replay memory used in this study is one of the functions of a <b>DQN</b> that can ...", "dateLastCrawled": "2021-09-03T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Using <b>Keras and Deep Q-Network to Play FlappyBird</b> | Ben Lau", "url": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "isFamilyFriendly": true, "displayUrl": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "snippet": "<b>Deep</b> <b>Q-Network</b> is a <b>learning</b> <b>algorithm</b> developed by Google DeepMind to play Atari games. They demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable because it demonstrates the <b>algorithm</b> is generic enough to play various games. The following post is a must-read for those who are interested in <b>deep</b> reinforcement <b>learning</b>. Demystifying <b>Deep</b> Reinforcement <b>Learning</b> ...", "dateLastCrawled": "2022-01-30T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "My Journey Into <b>Deep</b> Q-<b>Learning</b> with <b>Keras</b> and Gym | by Gaetan ... - Medium", "url": "https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@gtnjuvin/my-journey-into-<b>deep</b>-q-<b>learning</b>-with-<b>keras</b>-and-gym-3e779...", "snippet": "At the end of 2013, Google introduced a new <b>algorithm</b> called <b>Deep</b> <b>Q Network</b> (<b>DQN</b>). It demonstrated how an AI agent can learn to play games by just observing the screen. The AI agent can do so ...", "dateLastCrawled": "2022-01-30T04:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Short-Term Load Forecasting <b>Algorithm</b> Using a <b>Similar</b> Day Selection ...", "url": "https://www.mdpi.com/1996-1073/13/10/2640/htm", "isFamilyFriendly": true, "displayUrl": "https://www.<b>mdpi</b>.com/1996-1073/13/10/2640/htm", "snippet": "The reinforcement <b>learning</b> <b>algorithm</b> is one of the most representative <b>machine</b> <b>learning</b> techniques along with supervised <b>learning</b> and unsupervised <b>learning</b>. After 2013, the <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) method was proposed by DeepMind. The <b>DQN</b> method has been used in numerous studies in a variety of fields. Reinforcement <b>learning</b> is mainly applied to areas such as robot control, stock trading, resource allocation, recommendation systems, and natural language processing. Additionally, reinforcement ...", "dateLastCrawled": "2022-01-23T16:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Going Deeper Into Reinforcement Learning: Understanding Deep</b>-Q-Networks", "url": "https://danieltakeshi.github.io/2016/12/01/going-deeper-into-reinforcement-learning-understanding-dqn/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/2016/12/01/<b>going-deeper-into-reinforcement-learning</b>...", "snippet": "The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) <b>algorithm</b>, as introduced by DeepMind in a NIPS 2013 workshop paper, and later published in Nature 2015 <b>can</b> be credited with revolutionizing reinforcement <b>learning</b>. In this post, therefore, I would like to give a guide to a subset of the <b>DQN</b> <b>algorithm</b>. This is a continuation of an earlier reinforcement <b>learning</b> article about linear function approximators. My contribution here will be orthogonal to my previous post about the preprocessing steps for game frames. Before ...", "dateLastCrawled": "2022-02-03T15:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Episodic Memory <b>and Deep Q-Networks - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/episodic-memory-and-deep-q-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/episodic-memory-and-<b>deep</b>-q-networks", "snippet": "<b>Deep</b> Q-Networks (<b>DQN</b>): A well-established technique to perform the above task is Q-<b>learning</b>, where we decide on a function called Q-function which is important for the success of the <b>algorithm</b>. <b>DQN</b> uses the neural networks as Q-function to approximate the action values Q(s, a, \\theta) where the parameter of network and (s,a) represents the state-action pair .", "dateLastCrawled": "2022-01-16T05:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "4. <b>Deep</b> Q-Networks - <b>Reinforcement Learning</b> [Book]", "url": "https://www.oreilly.com/library/view/reinforcement-learning/9781492072386/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/<b>reinforcement-learning</b>/9781492072386/ch04.html", "snippet": "<b>Deep</b> Q-networks was the breakthrough paper, but neural networks have been used in RL for a long time. 22 Given the flexibility of neural networks, you <b>can</b> find as many improvements to <b>DQN</b> as the number of papers on <b>deep</b> <b>learning</b>. The key insight is that although nonlinear function approximators are unruly and may not converge, they have the incredible ability to approximate any function. This opens the door to applications that were previously deemed too complex.", "dateLastCrawled": "2022-01-29T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Can</b> <b>Deep</b> Reinforcement <b>Learning</b> Solve Chess? | by Victor Sim | Towards ...", "url": "https://towardsdatascience.com/can-deep-reinforcement-learning-solve-chess-b9f52855cd1e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>can</b>-<b>deep</b>-reinforcement-<b>learning</b>-solve-chess-b9f52855cd1e", "snippet": "The hyperparameter gamma <b>can</b> <b>be thought</b> of a measure of the importance of future rewards to the implemented environment. Model: The gist on the left describes the Q_model class. It contains 3 functions that allow for it to interact with the environment. The model created for the <b>DQN</b> is a simple convolutional network with 3 convolutional layers with the relu activation function. The final layer contains 4096 neurons, representing the 4096 possible moves that <b>can</b> be played in any given ...", "dateLastCrawled": "2022-02-02T21:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Deep</b> Reinforcement <b>Learning</b> Based Searching Method for Source ...", "url": "https://www.researchgate.net/publication/357276566_A_Deep_Reinforcement_Learning_Based_Searching_Method_for_Source_Localization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357276566_A_<b>Deep</b>_Reinforcement_<b>Learning</b>_Based...", "snippet": "PC-<b>DQN</b> leverages the density-based spatial clustering of applications with noise (DBSCAN) <b>algorithm</b> to extract the feature of belief state, and employ the <b>deep</b> <b>Q-network</b> (<b>DQN</b>) <b>algorithm</b> to find ...", "dateLastCrawled": "2022-01-21T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>What is the difference between Q learning, deep</b> Q <b>learning</b> and <b>deep</b> Q ...", "url": "https://www.quora.com/What-is-the-difference-between-Q-learning-deep-Q-learning-and-deep-Q-network", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-the-difference-between-Q-learning-deep</b>-Q-<b>learning</b>-and...", "snippet": "Answer (1 of 2): Q: <b>What is the difference between Q learning, deep</b> Q <b>learning</b> and <b>deep</b> <b>Q network</b>? It is a very slight distinction only. Q-<b>Learning</b> [1] is a reinforcement <b>learning</b> <b>algorithm</b> that helps to solve sequential tasks. It does not need to know how the world works (it\u2019s model-free) and ...", "dateLastCrawled": "2022-01-21T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "GitHub - rishavb123/MineRL: Applies the <b>Deep</b> Q <b>Learning</b> <b>algorithm</b> using ...", "url": "https://github.com/rishavb123/MineRL", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rishavb123/MineRL", "snippet": "Applies the <b>Deep</b> Q <b>Learning</b> <b>algorithm</b> using a convolutional neural network to have an agent learn to fight zombies in a closed minecraft environment. This is done using Microsoft&#39;s Project Malmo (to create the environment) and tensorflow/keras to structure the network. - GitHub - rishavb123/MineRL: Applies the <b>Deep</b> Q <b>Learning</b> <b>algorithm</b> using a convolutional neural network to have an agent learn to fight zombies in a closed minecraft environment. This is done using Microsoft&#39;s Project Malmo ...", "dateLastCrawled": "2022-01-23T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - AmitBaanerjee/<b>Deep</b>-Reinforcement-<b>Learning</b>-and-<b>DQN</b>-on-GYM-env ...", "url": "https://github.com/AmitBaanerjee/Deep-Reinforcement-Learning-and-DQN-on-GYM-env", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/AmitBaanerjee/<b>Deep</b>-Reinforcement-<b>Learning</b>-and-<b>DQN</b>-on-GYM-env", "snippet": "<b>Deep</b> <b>Q Network</b> Using OpenAI\u2019s Gym <b>Deep</b> Q <b>Learning</b> <b>can</b> be defined as a modified method of implementing Q <b>Learning</b> by combining the independent nature of Reinforcement <b>Learning</b> with the efficiency of <b>Deep</b> <b>Learning</b>. Neural Networks <b>can</b> be added as an agent that learns to environment state-action pairs to rewards. Neural Network now works as a function approximator to relate the input values to the outputs. Generally, in order for the Neural Network to train, we allow the Network itself to ...", "dateLastCrawled": "2021-09-13T11:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Can</b> <b>deep</b> Q-<b>learning</b> be used for solving mazes (e.g. 100x100 size) then ...", "url": "https://www.quora.com/Can-deep-Q-learning-be-used-for-solving-mazes-e-g-100x100-size-then-apply-the-learned-to-new-mazes-variable-sizes-which-werent-used-for-learning-or-is-there-a-better-algorithm-from-the-machine-learning-family-equipped-to-deal-with-mazes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Can</b>-<b>deep</b>-Q-<b>learning</b>-be-used-for-solving-mazes-e-g-100x100-size...", "snippet": "Answer (1 of 2): Solving mazes with the heavy artillery, aren\u2019t we? No need to bring forth the big guns: there are a number of very good and efficient Maze solving algorithms that require no <b>learning</b> or training whatsoever. Peruse the article to familiarize yourself with the more common and well...", "dateLastCrawled": "2022-01-18T18:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to perform <b>deep</b> Q-<b>learning</b> batch update step on a neural network ...", "url": "https://stats.stackexchange.com/questions/336347/how-to-perform-deep-q-learning-batch-update-step-on-a-neural-network-with-multip", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/336347/how-to-perform-<b>deep</b>-q-<b>learning</b>-batch...", "snippet": "I am taking on <b>deep</b> Q-<b>learning</b> and I am stuck at understanding one particular thing. I have googled multiple <b>deep</b> Q-<b>learning</b> examples, but literally everyone posting tutorials uses a cart-pole game to present the <b>algorithm</b> and this game does not encounter similar issues to my problem.", "dateLastCrawled": "2022-01-28T20:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>A Theoretical Analysis of Deep Q</b>-<b>Learning</b> - Proceedings of <b>Machine</b> ...", "url": "http://proceedings.mlr.press/v120/yang20a/yang20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v120/yang20a/yang20a.pdf", "snippet": "proposed <b>algorithm</b>, named Minimax-<b>DQN</b>, <b>can</b> be viewed as a combination of the Minimax-Q <b>learning</b> <b>algorithm</b> for tabular zero-sum Markov games (Littman,1994) and <b>deep</b> neural networks for function approximation. <b>Compared</b> with <b>DQN</b>, the main difference lies in the approaches to compute the target values. In <b>DQN</b>, the target is computed via ...", "dateLastCrawled": "2022-02-01T14:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep</b> <b>Q-Network</b> based resource allocation for UAV-assisted Ultra-Dense ...", "url": "https://www.sciencedirect.com/science/article/pii/S138912862100284X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S138912862100284X", "snippet": "<b>Compared</b> with the traditional <b>machine</b> <b>learning</b> <b>algorithm</b>, DRL <b>can</b> process the massive system data generated under the UDN environment, which is more consistent with the scene investigated in this study. To the best of our knowledge, the resource allocation problem of the UAV-assisted UDN emergency communication system has not been fully studied in the previous work. The main contributions of this paper are summarized as follows: \u2022 We thoroughly investigate the resource allocation issue of ...", "dateLastCrawled": "2022-01-20T16:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep</b> <b>Q-network</b>-based traffic signal control models", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256405", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256405", "snippet": "Especially, a <b>deep</b> <b>Q-network</b> (<b>DQN</b>) is one of the predominant reinforcement <b>learning</b> algorithms designed to overcome the limitations of existing Q-<b>learning</b> algorithms . Q-<b>learning</b> has a Q function that estimates the Q value for each state\u2013action pair and determines whether to perform a specific action in a specific state based on this. The Q function is called an action-value function and <b>can</b> directly estimate the optimal action-value function", "dateLastCrawled": "2021-09-03T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A New Feature Selection <b>Algorithm</b> Based on <b>Deep</b> <b>Q-Network</b> | IEEE ...", "url": "https://ieeexplore.ieee.org/document/9550745/", "isFamilyFriendly": true, "displayUrl": "https://ieeexplore.ieee.org/document/9550745", "snippet": "In this paper, a new automatic feature selection method based on <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>-FS) is proposed to solve the shortcomings of traditional feature selection. Firstly, the feature selection problem is formulated as a Markov decision process. Subsequently, the <b>Q network</b> is used to optimize the search strategy of the optimal feature subset. Finally, the parameters of the <b>Q network</b> are updated by stochastic gradient descent. This paper also considers an extensive study of the feature sequence ...", "dateLastCrawled": "2021-12-28T13:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Implementation of Q <b>learning and deep Q network</b> for controlling a self ...", "url": "https://jrobio.springeropen.com/articles/10.1186/s40638-018-0091-9", "isFamilyFriendly": true, "displayUrl": "https://jrobio.springeropen.com/articles/10.1186/s40638-018-0091-9", "snippet": "In this paper, the implementations of two reinforcement learnings namely, Q <b>learning and deep Q network</b> (<b>DQN</b>) on the Gazebo model of a self balancing robot have been discussed. The goal of the experiments is to make the robot model learn the best actions for staying balanced in an environment. The more time it <b>can</b> remain within a specified limit, the more reward it accumulates and hence more balanced it is. We did various tests with many hyperparameters and demonstrated the performance curves.", "dateLastCrawled": "2022-01-01T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "4. <b>Deep</b> Q-Networks - <b>Reinforcement Learning</b> [Book]", "url": "https://www.oreilly.com/library/view/reinforcement-learning/9781492072386/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/<b>reinforcement-learning</b>/9781492072386/ch04.html", "snippet": "<b>Deep</b> Q-networks was the breakthrough paper, but neural networks have been used in RL for a long time. 22 Given the flexibility of neural networks, you <b>can</b> find as many improvements to <b>DQN</b> as the number of papers on <b>deep</b> <b>learning</b>. The key insight is that although nonlinear function approximators are unruly and may not converge, they have the incredible ability to approximate any function. This opens the door to applications that were previously deemed too complex.", "dateLastCrawled": "2022-01-29T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Double <b>deep</b> <b>Q-network</b> (<b>DQN</b>) based reinforcement <b>learning</b> (DARLING) for ...", "url": "https://www.researchgate.net/figure/Double-deep-Q-network-DQN-based-reinforcement-learning-DARLING-for-stochastic_fig2_325194373", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/Double-<b>deep</b>-<b>Q-network</b>-<b>DQN</b>-based-reinforcement...", "snippet": "Double <b>deep</b> <b>Q-network</b> (<b>DQN</b>) based reinforcement <b>learning</b> (DARLING) for stochastic computation offloading in a mobile-edge computing system.", "dateLastCrawled": "2022-01-30T19:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Auto-CASH: Autonomous Classification Algorithm Selection with</b> <b>Deep</b> Q ...", "url": "https://deepai.org/publication/auto-cash-autonomous-classification-algorithm-selection-with-deep-q-network", "isFamilyFriendly": true, "displayUrl": "https://<b>deep</b>ai.org/publication/<b>auto-cash-autonomous-classification-algorithm-selection</b>...", "snippet": "However, with the problem getting complicated, it is difficult to describe the environment by an acceptable amount of states an agent could possibly enter. If we still use Q-table, there should be heavy space cost. Searching in such a complex table also needs a lot of time and computing resources. <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>)(Mnih et al., 2013)", "dateLastCrawled": "2021-12-18T23:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Forecasting the Market with <b>Machine</b> <b>Learning</b> Algorithms: An Application ...", "url": "https://dl.acm.org/doi/10.1145/3488378", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3488378", "snippet": "Finally, the optimum parameters of the proposed <b>algorithm</b> were calculated using a reinforced <b>learning</b>-based <b>deep</b> <b>Q-Network</b>. <b>Compared</b> to existing forecasting methods, the proposed <b>algorithm</b> achieves better results with a forecasting accuracy of 61.77%, annualized return of 29.25%, and maximum losses of \u22128.29%. Furthermore, the proposed model ...", "dateLastCrawled": "2022-01-17T04:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to use a different model to <b>deep</b> neural network with reinforcement ...", "url": "https://datascience.stackexchange.com/questions/37643/how-to-use-a-different-model-to-deep-neural-network-with-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/37643/how-to-use-a-different-model-to...", "snippet": "Is it possible to implement a reinforcement <b>learning</b> <b>algorithm</b> without using a <b>deep</b> neural network (DNN) as used in <b>deep</b> reinforcement <b>learning</b> e.g. <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>)? How <b>can</b> I replace the DNN in the <b>DQN</b> <b>algorithm</b> with another <b>algorithm</b>? Should it be supervised or unsupervised, and what is this called - is it &quot;un/supervised reinforcement ...", "dateLastCrawled": "2022-01-18T01:41:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>DQN</b> Algorithm: A father-son tale. The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b> ...", "url": "https://medium.com/analytics-vidhya/dqn-algorithm-a-father-son-tale-b4bf6ff1ae2f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>dqn</b>-algorithm-a-father-son-tale-b4bf6ff1ae2f", "snippet": "The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) Reinforcement <b>learning</b> algorithm has a surprisingly simple and real life <b>analogy</b> with which it can be explained. It helps understand the sequence of operations involved by ...", "dateLastCrawled": "2022-01-13T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep Q-Learning with Keras and Gym</b> \u00b7 Keon&#39;s Blog", "url": "https://keon.github.io/deep-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://keon.github.io/<b>deep</b>-q-<b>learning</b>", "snippet": "If we use the <b>analogy</b> of the bicycle, we can define reward as the distance from the original starting point. ## <b>Deep</b> Reinforcement <b>Learning</b> Google\u2019s DeepMind published its famous paper Playing Atari with <b>Deep</b> Reinforcement <b>Learning</b>, in which they introduced a new algorithm called <b>Deep</b> <b>Q Network</b> (<b>DQN</b> for short) in 2013. It demonstrated how an ...", "dateLastCrawled": "2022-02-01T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Graying the Black Box: Understanding DQNs</b> | the morning paper", "url": "https://blog.acolyer.org/2016/03/02/graying-the-black-box-understanding-dqns/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2016/03/02/<b>graying-the-black-box-understanding-dqns</b>", "snippet": "<b>Deep</b> Reinforcement <b>Learning</b> (DRL) applies <b>Deep</b> Neural Networks to reinforcement <b>learning</b>. The <b>Deep</b> Mind team used a DRL algorithm called <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) to learn how to play the Atari games. In \u2018Graying the Black Box,\u2019 Zahavy et al. look at three of those games \u2013 Breakout, Pacman, and Seaquest \u2013 and develop a new visualization and interaction approach that helps to shed insight on what it is that <b>DQN</b> is actually <b>learning</b>.", "dateLastCrawled": "2022-01-20T19:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Using <b>Keras and Deep Q-Network to Play FlappyBird</b> | Ben Lau", "url": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "isFamilyFriendly": true, "displayUrl": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "snippet": "What is <b>Deep</b> <b>Q-Network</b>? <b>Deep</b> <b>Q-Network</b> is a <b>learning</b> algorithm developed by Google DeepMind to play Atari games. They demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable because it demonstrates the algorithm is generic enough to play various games. The following post is a must-read for those who are interested in <b>deep</b> reinforcement <b>learning</b>. Demystifying <b>Deep</b> ...", "dateLastCrawled": "2022-01-30T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Reinforcement Learning for On-Demand Logistics</b> - <b>DoorDash Engineering Blog</b>", "url": "https://doordash.engineering/2018/09/10/reinforcement-learning-for-on-demand-logistics/", "isFamilyFriendly": true, "displayUrl": "https://doordash.engineering/2018/09/10/<b>reinforcement-learning-for-on-demand-logistics</b>", "snippet": "This approach is known as <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) and is very useful when feature dimensionality is high and data volume is also high. Reinforcement learned assignment . Now we will discuss how we applied reinforcement <b>learning</b> to the DoorDash assignment problem. To formulate the assignment problem in a way that\u2019s suitable for reinforcement <b>learning</b>, we made the following definitions. State: The outstanding deliveries and working Dashers, since they represent the current status of the world ...", "dateLastCrawled": "2022-01-18T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Guide to Reinforcement <b>Learning with Python and TensorFlow</b>", "url": "https://rubikscode.net/2021/07/13/deep-q-learning-with-python-and-tensorflow-2-0/", "isFamilyFriendly": true, "displayUrl": "https://rubikscode.net/2021/07/13/<b>deep</b>-q-<b>learning-with-python-and-tensorflow</b>-2-0", "snippet": "In the previous two articles we started exploring the interesting universe of reinforcement <b>learning</b>.First we went through the basics of third paradigm within <b>machine</b> <b>learning</b> \u2013 reinforcement <b>learning</b>.Just to freshen up our memory, we saw that approach of this type of <b>learning</b> is unlike the previously explored supervised and unsupervised <b>learning</b>. In reinforcement <b>learning</b>, self-<b>learning</b> agent learns how to interact with the environment and solve a problem within it. In this article, we ...", "dateLastCrawled": "2022-02-03T13:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Solving Combinatorial Problems with Machine Learning Methods</b> | Request PDF", "url": "https://www.researchgate.net/publication/333525406_Solving_Combinatorial_Problems_with_Machine_Learning_Methods", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333525406_Solving_Combinatorial_Problems_with...", "snippet": "Next we discuss <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised <b>learning</b>, and ...", "dateLastCrawled": "2022-01-23T14:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep</b> Reinforcement <b>Learning</b> for Crowdsourced Urban Delivery - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0191261521001636", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0191261521001636", "snippet": "We propose a new <b>deep</b> reinforcement <b>learning</b> (DRL)-based approach to tackling this assignment problem. A <b>deep</b> <b>Q network</b> (<b>DQN</b>) algorithm is trained which entails two salient features of experience replay and target network that enhance the efficiency, convergence, and stability of DRL training. More importantly, this paper makes three methodological contributions: 1) presenting a comprehensive and novel characterization of crowdshipping system states that encompasses spatial-temporal and ...", "dateLastCrawled": "2022-01-19T19:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Reinforcement Learning for Formula 1</b> Race Strategy | by Ashref Maiza ...", "url": "https://towardsdatascience.com/reinforcement-learning-for-formula-1-race-strategy-7f29c966472a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning-for-formula-1</b>-race-strategy-7f29...", "snippet": "Reinforcement <b>Learning</b> (RL) is an advanced <b>machine</b> <b>learning</b> (ML) technique which takes a very different approach to training models than other <b>machine</b> <b>learning</b> methods. Its super power is that it learns very complex behaviors without requiring any labeled training data, and can make short term decisions while optimizing for a longer term goal. RL in the context of Formula 1 racing. In RL, an agent learns the optimal behavior to perform a certain task by interacting directly with the ...", "dateLastCrawled": "2022-02-02T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Applications of <b>Reinforcement Learning</b> in Real World | by garychl ...", "url": "https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/applications-of-<b>reinforcement-learning</b>-in-real-world-1a...", "snippet": "RL, known as a semi-supervised <b>learning</b> model in <b>machine</b> <b>learning</b>, is a technique to allow an agent to take actions and interact with an environment so as to maximize the total rewards. RL is usually modeled as a Markov Decision Process (MDP). Source: <b>Reinforcement Learning</b>:An Introduction. Imagine a baby is given a TV remote control at your home (environment). In simple terms, the baby (agent) will first observe and construct his/her own representation of the environment (state). Then the ...", "dateLastCrawled": "2022-02-02T20:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(deep q-network (dqn))  is like +(machine learning algorithm)", "+(deep q-network (dqn)) is similar to +(machine learning algorithm)", "+(deep q-network (dqn)) can be thought of as +(machine learning algorithm)", "+(deep q-network (dqn)) can be compared to +(machine learning algorithm)", "machine learning +(deep q-network (dqn) AND analogy)", "machine learning +(\"deep q-network (dqn) is like\")", "machine learning +(\"deep q-network (dqn) is similar\")", "machine learning +(\"just as deep q-network (dqn)\")", "machine learning +(\"deep q-network (dqn) can be thought of as\")", "machine learning +(\"deep q-network (dqn) can be compared to\")"]}