{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The Complete <b>Reinforcement Learning</b> Dictionary | by Shaked Zychlinski ...", "url": "https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-complete-<b>reinforcement-learning</b>-dictionary-e16230b7d24e", "snippet": "<b>Greedy</b> <b>Policy</b>, \u03b5-<b>Greedy</b> <b>Policy</b>: A ... <b>Reinforcement Learning</b> (RL): <b>Reinforcement Learning</b> <b>is, like</b> Supervised <b>Learning</b> and Unsupervised <b>Learning</b>, one the main areas of <b>Machine</b> <b>Learning</b> and Artificial Intelligence. It is concerned with the <b>learning</b> process of an arbitrary being, formally known as an Agent, in the world surrounding it, known as the Environment. The Agent seeks to maximize the rewards it receives from the Environment, and performs different actions in order to learn how the ...", "dateLastCrawled": "2022-01-31T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning Algorithms</b> | Know Top 8 <b>Machine</b> <b>Learning</b> ... - EDUCBA", "url": "https://www.educba.com/machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>machine-learning-algorithms</b>", "snippet": "<b>Machine Learning Algorithms</b> are defined as the algorithms that are used for training the models, in <b>machine</b> <b>learning</b> it is divide into three different types, i.e., Supervised <b>Learning</b>( in this dataset are labeled and Regression and Classification techniques are used), Unsupervised <b>Learning</b>(in this dataset are not labeled and techniques <b>like</b> Dimensionality reduction and Clustering are used) and Reinforcement <b>Learning</b>(<b>algorithm</b> in which model learn from its every action) for the development of ...", "dateLastCrawled": "2022-02-02T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Epsilon-Greedy Algorithm in Reinforcement Learning</b> - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>epsilon-greedy-algorithm-in-reinforcement-learning</b>", "snippet": "Epsilon-<b>Greedy</b> Action Selection. Epsilon-<b>Greedy</b> is a simple method to balance exploration and exploitation by choosing between exploration and exploitation randomly. The epsilon-<b>greedy</b>, where epsilon refers to the probability of choosing to explore, exploits most of the time with a small chance of exploring.", "dateLastCrawled": "2022-01-31T10:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Greedy algorithm</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Greedy_algorithm", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Greedy_algorithm</b>", "snippet": "A <b>greedy algorithm</b> is any <b>algorithm</b> that follows the problem-solving heuristic of making the locally optimal choice at each stage. ... In decision tree <b>learning</b>, <b>greedy</b> algorithms are commonly used, however they are not guaranteed to find the optimal solution. One popular such <b>algorithm</b> is the ID3 <b>algorithm</b> for decision tree construction. Dijkstra&#39;s <b>algorithm</b> and the related A* search <b>algorithm</b> are verifiably optimal <b>greedy</b> algorithms for graph search and shortest path finding. A* search is ...", "dateLastCrawled": "2022-02-03T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Epsilon-Greedy Q-learning</b> | Baeldung on Computer Science", "url": "https://www.baeldung.com/cs/epsilon-greedy-q-learning", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>epsilon-greedy-q-learning</b>", "snippet": "We\u2019ll also mention some basic reinforcement <b>learning</b> concepts <b>like</b> temporal difference and off-<b>policy</b> <b>learning</b> on the way. Then we\u2019ll inspect exploration vs. exploitation tradeoff and epsilon-<b>greedy</b> action selection. Finally, we\u2019ll discuss the <b>learning</b> parameters and how to tune them. 2. Q-<b>Learning</b> <b>Algorithm</b>. Reinforcement <b>learning</b> (RL) is a branch of <b>machine</b> <b>learning</b>, where the system learns from the results of actions. In this tutorial, we\u2019ll focus on Q-<b>learning</b>, which is said to ...", "dateLastCrawled": "2022-01-30T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "On-<b>Policy VS Off-Policy Reinforcement Learning</b>: The Differences", "url": "https://analyticsindiamag.com/reinforcement-learning-policy/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/reinforcement-<b>learning</b>-<b>policy</b>", "snippet": "In Q-<b>Learning</b>, the agent learns optimal <b>policy</b> with the help of a <b>greedy</b> <b>policy</b> and behaves using policies of other agents. Q-<b>learning</b> is called off-<b>policy</b> because the updated <b>policy</b> is different from the behavior <b>policy</b>, so Q-<b>Learning</b> is off-<b>policy</b>. In other words, it estimates the reward for future actions and appends a value to the new state without actually following any <b>greedy</b> <b>policy</b>.", "dateLastCrawled": "2022-02-03T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Off-<b>policy</b> vs On-<b>Policy</b> vs Offline <b>Reinforcement Learning</b> Demystified ...", "url": "https://kowshikchilamkurthy.medium.com/off-policy-vs-on-policy-vs-offline-reinforcement-learning-demystified-f7f87e275b48", "isFamilyFriendly": true, "displayUrl": "https://kowshikchilamkurthy.medium.com/off-<b>policy</b>-vs-on-<b>policy</b>-vs-offline...", "snippet": "<b>Reinforcement Learning</b> is a subfield of <b>machine</b> <b>learning</b> that teaches an agent how to choose an action from its action space. It interacts with an environment, in order to maximize rewards over time. Complex enough? let\u2019s break this definition for better understanding. Agent: The program y ou train, with the aim of doing a job you specify. Environment: The world in which the agent performs actions. Action: A move made by the agent, which causes a change in the environment. Rewards: The ...", "dateLastCrawled": "2022-01-31T17:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - what is epsilon/k how did that come in epsilon ...", "url": "https://stackoverflow.com/questions/50423955/what-is-epsilon-k-how-did-that-come-in-epsilon-greedy-algorithm", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/50423955", "snippet": "Epsilon-<b>greedy</b> is almost too simple. As you play the machines, you keep track of the average payout of each <b>machine</b>. Then, you select the <b>machine</b> with the highest current average payout with probability = (1 \u2013 epsilon) + (epsilon / k) where epsilon is a small value <b>like</b> 0.10. And you select machines that don\u2019t have the highest current ...", "dateLastCrawled": "2022-01-27T18:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>RL Tutorial Part 1: Monte Carlo Methods</b> \u2013 [+] Reinforcement", "url": "https://plusreinforcement.com/2018/07/05/rl-tutorial-part-1-monte-carlo-methods/", "isFamilyFriendly": true, "displayUrl": "https://plusreinforcement.com/2018/07/05/<b>rl-tutorial-part-1-monte-carlo-methods</b>", "snippet": "The reason why this <b>algorithm</b> is known as an -<b>greedy</b> <b>algorithm</b> is due to its approach in tackling the classic exploration-exploitation trade-off. This problem arises from the conflicting goals of RL, which are to both sufficiently explore the state space and behave optimally in all states. -<b>greedy</b> Monte Carlo algorithms approach this issue by employing a adjustable parameter, to balance these two requirements. This results in this <b>algorithm</b> picking a specific non-<b>greedy</b> action, with a ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>On-Policy VS Off-Policy</b> in <b>Reinforcement Learning</b> - Lei Mao&#39;s Log Book", "url": "https://leimao.github.io/blog/RL-On-Policy-VS-Off-Policy/", "isFamilyFriendly": true, "displayUrl": "https://leimao.github.io/blog/RL-<b>On-Policy-VS-Off-Policy</b>", "snippet": "Let us review a typical off-<b>policy</b> <b>algorithm</b> Q-<b>Learning</b> and a typical on-<b>policy</b> <b>algorithm</b> SARSA first. Q-<b>Learning</b> SARSA Off-<b>Policy</b> VS On-<b>Policy</b> . We can see that the major difference is that Q-<b>Learning</b> is using $$ Q(S,A) \\leftarrow Q(S,A) + \\alpha [R + \\gamma \\text{max}_{a} Q(S\u2019,a) - Q(S,A)] $$ to update Q value. While SARSA is using $$ Q(S,A) \\leftarrow Q(S,A) + \\alpha [R + \\gamma Q(S\u2019,A\u2019) - Q(S,A)] $$ to update Q value. Let us introduce two concepts first: update <b>policy</b> and behavior ...", "dateLastCrawled": "2022-01-22T03:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Greedy Algorithms</b> - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/greedy-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>greedy-algorithms</b>", "snippet": "<b>Greedy</b> is an algorithmic paradigm that builds up a solution piece by piece, always choosing the next piece that offers the most obvious and immediate benefit. So the problems where choosing locally optimal also leads to global solution are best fit for <b>Greedy</b>. For example consider the Fractional Knapsack Problem.", "dateLastCrawled": "2022-02-02T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Reinforcement <b>Learning</b>: Introduction to <b>Policy</b> Gradients | by Cheng Xi ...", "url": "https://medium.com/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/reinforcement-<b>learning</b>-introduction-to-<b>policy</b>...", "snippet": "Unlike an epsilon <b>greedy</b> <b>algorithm</b> that chooses the max value action with some noise, we are selecting an action based on the current <b>policy</b>. \u03c0(a | s, \u03b8) = Pr{A\u209c = a | S\u209c = s, \u03b8\u209c = \u03b8 ...", "dateLastCrawled": "2022-01-28T08:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Epsilon-Greedy Q-learning</b> | Baeldung on Computer Science", "url": "https://www.baeldung.com/cs/epsilon-greedy-q-learning", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>epsilon-greedy-q-learning</b>", "snippet": "Q-<b>learning</b> is an off-<b>policy</b> <b>algorithm</b>. It estimates the reward for state-action pairs based on the optimal (<b>greedy</b>) <b>policy</b>, independent of the agent\u2019s actions. An off-<b>policy</b> <b>algorithm</b> approximates the optimal action-value function, independent of the <b>policy</b>. Besides, off-<b>policy</b> algorithms can update the estimated values using made up actions. In this case, the Q-<b>learning</b> <b>algorithm</b> can explore and benefit from actions that did not happen during the <b>learning</b> phase. As a result, Q-<b>learning</b> is ...", "dateLastCrawled": "2022-01-30T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Greedy algorithm</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Greedy_algorithm", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Greedy_algorithm</b>", "snippet": "A <b>greedy algorithm</b> is any <b>algorithm</b> that follows the problem-solving heuristic of making the locally optimal choice at each stage. ... in a fashion <b>similar</b> to the travelling salesman problem. The game has a demo mode, where the game uses a <b>greedy algorithm</b> to go to every crystal. The artificial intelligence does not account for obstacles, so the demo mode often ends quickly. The matching pursuit is an example of a <b>greedy algorithm</b> applied on signal approximation. A <b>greedy algorithm</b> finds the ...", "dateLastCrawled": "2022-02-07T09:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Introduction to Various <b>Reinforcement Learning</b> Algorithms. Part I (Q ...", "url": "https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-various-<b>reinforcement-learning</b>...", "snippet": "An on-<b>policy</b> agent learns the value based on its current action a derived from the current <b>policy</b>, whereas its off-<b>policy</b> counter part learns it based on the action a* obtained from another <b>policy</b>. In Q-<b>learning</b>, such <b>policy</b> is the <b>greedy</b> <b>policy</b>. (We will talk more on that in Q-<b>learning</b> and SARSA) 2. Illustration of Various Algorithms 2.1 Q ...", "dateLastCrawled": "2022-02-03T03:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning Combinatorial Optimization Algorithms over Graphs</b>", "url": "https://proceedings.neurips.cc/paper/2017/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2017/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Paper.pdf", "snippet": "The learned <b>greedy</b> <b>policy</b> behaves like a meta-<b>algorithm</b> that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems. 1 Introduction Combinatorial optimization problems over graphs arising from ...", "dateLastCrawled": "2022-02-02T15:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Reinforcement <b>Learning</b>: Sample-Averages w/ \u03b5-<b>greedy</b> selection | by 0D0A ...", "url": "https://medium.com/swlh/reinforcement-learning-foundations-sample-averages-w-%CE%B5-greedy-selection-627f0291cf5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/reinforcement-<b>learning</b>-foundations-sample-averages-w-\u03b5-<b>greedy</b>...", "snippet": "Reinforcement <b>Learning</b>: Sample-Averages w/ \u03b5-<b>greedy</b> selection. The variety of methods and approaches that are wrapped up in the <b>Machine</b> <b>Learning</b> family is pretty impressive. Reinforcement ...", "dateLastCrawled": "2022-02-03T11:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>RL Tutorial Part 1: Monte Carlo Methods</b> \u2013 [+] Reinforcement", "url": "https://plusreinforcement.com/2018/07/05/rl-tutorial-part-1-monte-carlo-methods/", "isFamilyFriendly": true, "displayUrl": "https://plusreinforcement.com/2018/07/05/<b>rl-tutorial-part-1-monte-carlo-methods</b>", "snippet": "Both these approaches have very <b>similar</b> theoretical convergence properties but perform differently in practice. An implementation of the discussed <b>algorithm</b> in python is provided below with a line-by-line explanation following: def mc_control_epsilon_<b>greedy</b>(env, num_episodes, discount_factor=1.0, epsilon=0.1): &quot;&quot;&quot; Monte Carlo Control using Epsilon-<b>Greedy</b> policies. Finds an optimal epsilon-<b>greedy</b> <b>policy</b>. Args: env: OpenAI gym environment. num_episodes: Number of episodes to sample. discount ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Recommendation System in Python - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/recommendation-system-in-python/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/recommendation-system-in-python", "snippet": "The basic assumption behind the <b>algorithm</b> is that users with <b>similar</b> interests have common preferences. Content-Based Recommendation: It is supervised <b>machine</b> <b>learning</b> used to induce a classifier to discriminate between interesting and uninteresting items for the user. Content-Based Recommendation System: Content-Based systems recommends items to the customer <b>similar</b> to previously high-rated items by the customer. It uses the features and properties of the item. From these properties, it can ...", "dateLastCrawled": "2022-02-02T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Reinforcement <b>Learning</b> Explained Visually (Part 4): <b>Q Learning</b>, step-by ...", "url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-4-q-learning-step-by-step-b65efb731d3e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/reinforcement-<b>learning</b>-explained-visually-part-4-q...", "snippet": "This flow is very <b>similar</b> to the flow that we covered in the last article. So we will not repeat the explanation for all the steps again. (Image by author) The difference, which is the key hallmark of the <b>Q Learning</b> <b>algorithm</b>, is how it updates its estimates. The equation used to make the update in the fourth step is based on the Bellman equation, but if you examine it carefully it uses a slight variation of the formula we had studied earlier. Let\u2019s zoom in on the flow and examine this in ...", "dateLastCrawled": "2022-02-03T10:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement <b>Learning</b>: Introduction to <b>Policy</b> Gradients | by Cheng Xi ...", "url": "https://medium.com/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/reinforcement-<b>learning</b>-introduction-to-<b>policy</b>...", "snippet": "REINFORCE <b>algorithm</b>. Now with the <b>policy</b> gradient theorem, we <b>can</b> come up with a naive <b>algorithm</b> that makes use of gradient ascent to update our <b>policy</b> parameters. The theorem gives a sum over all ...", "dateLastCrawled": "2022-01-28T08:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The Complete <b>Reinforcement Learning</b> Dictionary | by Shaked Zychlinski ...", "url": "https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-complete-<b>reinforcement-learning</b>-dictionary-e16230b7d24e", "snippet": "<b>Greedy</b> <b>Policy</b>, \u03b5-<b>Greedy</b> <b>Policy</b>: A ... A well known off-<b>policy</b> <b>algorithm</b> is Q-<b>Learning</b>, as its update rule uses the action which will yield the highest Q-Value, while the actual <b>policy</b> used might restrict that action or choose another. The on-<b>policy</b> variation of Q-<b>Learning</b> is known as Sarsa, where the update rule uses the action chosen by the followed <b>policy</b>. One-Armed Bandits: See Bandits. One-Step TD: See Temporal Difference. <b>Policy</b> (\u03c0): The <b>policy</b>, denoted as \u03c0 (or sometimes \u03c0(a|s ...", "dateLastCrawled": "2022-01-31T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning Combinatorial Optimization Algorithms over Graphs</b>", "url": "https://proceedings.neurips.cc/paper/2017/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2017/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Paper.pdf", "snippet": "The learned <b>greedy</b> <b>policy</b> behaves like a meta-<b>algorithm</b> that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework <b>can</b> be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems. 1 Introduction Combinatorial optimization problems over graphs arising from ...", "dateLastCrawled": "2022-02-02T15:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Greedy</b> Algorithms Explained with Examples", "url": "https://www.freecodecamp.org/news/what-is-a-greedy-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://www.freecodecamp.org/news/what-is-a-<b>greedy</b>-<b>algorithm</b>", "snippet": "It is quite easy to come up with a <b>greedy</b> <b>algorithm</b> (or even multiple <b>greedy</b> algorithms) for a problem. Analyzing the run time for <b>greedy</b> algorithms will generally be much easier than for other techniques (like Divide and conquer). For the Divide and conquer technique, it is not clear whether the technique is fast or slow. This is because at each level of recursion the size of gets smaller and the number of sub-problems increases.", "dateLastCrawled": "2022-02-03T03:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>RL Tutorial Part 1: Monte Carlo Methods</b> \u2013 [+] Reinforcement", "url": "https://plusreinforcement.com/2018/07/05/rl-tutorial-part-1-monte-carlo-methods/", "isFamilyFriendly": true, "displayUrl": "https://plusreinforcement.com/2018/07/05/<b>rl-tutorial-part-1-monte-carlo-methods</b>", "snippet": "This is a powerful advantage for any <b>machine</b> <b>learning</b> <b>algorithm</b> that is required to be deployed into a production environment. Off-<b>policy</b> Monte Carlo algorithms also rely on a simple statistical technique known as importance sampling. This technique involves estimating expected values of one distribution given samples from another. Using this technique, it is easy to use episodes from <b>policy</b> to estimate the expected return under <b>policy</b> given that has sufficient coverage of . That is, every ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Department of Computer Science and Engineering Indian Institute of ...", "url": "https://cse.iitkgp.ac.in/~aritrah/course/theory/ML/Spring2021/scribes/2021-04-01_Thu_20CS60R53+20CS60R57.pdf", "isFamilyFriendly": true, "displayUrl": "https://cse.iitkgp.ac.in/~aritrah/course/theory/ML/Spring2021/scribes/2021-04-01_Thu...", "snippet": "<b>Machine</b> <b>Learning</b> (CS60050) Instructor : Aritra Hazra Vaibhav Saxena : 20CS60R57 jjSuprajit Sardar : 20CS60R53 01-April-2021 Drawbacks of existing Q <b>learning</b> <b>algorithm</b>: The existing Q <b>learning</b> <b>algorithm</b> discussed so far has some drawbacks, In this section we will discuss the solutions to these drawbacks. Exploration vs. Exploitation There are two competing objectives in Q <b>learning</b> model of reinforcement <b>learn-ing</b> which <b>can</b> <b>be thought</b> of as below : \u2022 Always try to nd a new action to do or an ...", "dateLastCrawled": "2022-01-13T08:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>Learning</b> 99+ Most Important MCQ Part-2 | <b>Machine</b> <b>Learning</b> MCQ ...", "url": "https://www.jobsaarnee.com/2021/06/machine-learning-100-most-important-mcq-part2.html", "isFamilyFriendly": true, "displayUrl": "https://www.jobsaarnee.com/2021/06/<b>machine</b>-<b>learning</b>-100-most-important-mcq-part2.html", "snippet": "Q74 Which of the following <b>machine</b> <b>learning</b> <b>algorithm</b> <b>can</b> be used for imputing missing values of both categorical and continuous variables? (a) K-NN (b) Linear Regression (c) Logistic Regression (d) None of the Above. Sol. (a) K-NN. Q75 Typically, value of k in k-nearest neighbors lie between-(a) 0 to1 (b) 1 to 20 (c) 20 to 50 (d) -1 to 1. Sol ...", "dateLastCrawled": "2022-02-03T12:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - DDPG question about exploration and \u03b5-<b>greedy</b> - Stack ...", "url": "https://stackoverflow.com/questions/70651954/ddpg-question-about-exploration-and-%CE%B5-greedy", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/70651954/ddpg-question-about-exploration-and-\u03b5-<b>greedy</b>", "snippet": "This may be a rookie question but as I&#39;m new to the field I would like to understand deeper something I got confused with. I have worked with DQN Networks before and I understand how \u03b5-<b>greedy</b> explo...", "dateLastCrawled": "2022-01-10T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "how to train to <b>think about counter examples for greedy algorithms</b> ...", "url": "https://www.reddit.com/r/algorithms/comments/fil5bn/how_to_train_to_think_about_counter_examples_for/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>algorithms</b>/comments/fil5bn/how_to_train_to_think_about...", "snippet": "You <b>can</b> expand this notion by modelling this such that player A <b>can</b> make up details about the program as the game goes. Each part of the problem is only fixed when player B asks about it; player B asks about parts of the problem state in the same manner the <b>greedy</b> <b>algorithm</b> would (i.e. view the <b>algorithm</b> as B&#39;s strategy). B&#39;s goal is again to find an optimal solution, but now A&#39;s goal seems a lot easier: A merely needs to be able to fix the rest of the problem description such that B&#39;s ...", "dateLastCrawled": "2020-07-01T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Intro to Reinforcement <b>Learning</b>: The <b>Explore</b>-Exploit Dilemma | by ...", "url": "https://towardsdatascience.com/intro-to-reinforcement-learning-the-explore-exploit-dilemma-463ceb004989", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/intro-to-reinforcement-<b>learning</b>-the-<b>explore</b>-exploit...", "snippet": "Then, by the \u03b5-<b>greedy</b> <b>algorithm</b>, (1- \u03b5) percent time we greedily exploit the best option k among the n options, and the remaining e percent time other options are randomly explored for a better decision than the previously best option k. The value of \u03b5 is set to 10%. Epsilon Decreasing Method. Epsilon deceasing is similar to the \u03b5-<b>greedy</b> ...", "dateLastCrawled": "2022-01-25T00:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A comparison of various reinforcement <b>learning</b> algorithms to solve ...", "url": "https://www.cs.jhu.edu/~vmohan3/document/ai_rl.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.jhu.edu/~vmohan3/document/ai_rl.pdf", "snippet": "are wide varieties of optimization problems in <b>machine</b> <b>learning</b> domain, all of which cannot be solved using one technique[11]. Therefore, for proving that the results which we are getting from one kind of technique is good enough for us makes it indispensable that we compare the results with other techniques for the given problem. Whilst doing this, we come across the various performance aspects of the <b>algorithm</b> i.e. where it would fail and where it <b>can</b> do remarkably well in solving the ...", "dateLastCrawled": "2022-02-02T05:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Reinforcement <b>Learning</b>: Introduction to <b>Policy</b> Gradients | by Cheng Xi ...", "url": "https://medium.com/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/reinforcement-<b>learning</b>-introduction-to-<b>policy</b>...", "snippet": "REINFORCE <b>algorithm</b>. Now with the <b>policy</b> gradient theorem, we <b>can</b> come up with a naive <b>algorithm</b> that makes use of gradient ascent to update our <b>policy</b> parameters. The theorem gives a sum over all ...", "dateLastCrawled": "2022-01-28T08:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Greedy Algorithms</b> - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/greedy-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>greedy-algorithms</b>", "snippet": "<b>Greedy</b> is an algorithmic paradigm that builds up a solution piece by piece, always choosing the next piece that offers the most obvious and immediate benefit. So the problems where choosing locally optimal also leads to global solution are best fit for <b>Greedy</b>. For example consider the Fractional Knapsack Problem.", "dateLastCrawled": "2022-02-02T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>A Comparison of Bandit Algorithms</b> | by Steve Roberts | Towards Data Science", "url": "https://towardsdatascience.com/a-comparison-of-bandit-algorithms-24b4adfcabb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>a-comparison-of-bandit-algorithms</b>-24b4adfcabb", "snippet": "Figure 6.4: <b>A comparison of bandit algorithms</b> on the 10-socket power problem, with a spread of 0.2 seconds of charge. Now we <b>can</b> see some separation in the performance of the algorithms: As before, the <b>Greedy</b> <b>algorithm</b> performs much worse than all the others. Epsilon <b>Greedy</b>, while being much better than the simple <b>Greedy</b> <b>algorithm</b>, is still ...", "dateLastCrawled": "2022-01-30T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Intro to reinforcement <b>learning</b>: temporal difference <b>learning</b>, SARSA vs ...", "url": "https://towardsdatascience.com/intro-to-reinforcement-learning-temporal-difference-learning-sarsa-vs-q-learning-8b4184bb4978", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/intro-to-reinforcement-<b>learning</b>-temporal-difference...", "snippet": "The difference is very subtle: For QL, which is the off-<b>policy</b> <b>algorithm</b>, when passing the reward from the next state (s_, a_) to the current state, it takes the maximum possible reward of the new state (s_) and ignores whatever <b>policy</b> we are using. For SARSA, which is on-<b>policy</b>, we still follow the <b>policy</b> (e-<b>greedy</b>), compute the next state (a_), and pass the reward corresponding to that exact a_ back the previous step.", "dateLastCrawled": "2022-02-03T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Exploration vs. Exploitation in Reinforcement Learning</b>", "url": "https://www.manifold.ai/exploration-vs-exploitation-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.manifold.ai/<b>exploration-vs-exploitation-in-reinforcement-learning</b>", "snippet": "As we <b>can</b> see, the <b>greedy</b> <b>policy</b> explored very little, and settled on choosing action 5 very quickly. The epsilon-<b>greedy</b> and decaying-epsilon-<b>greedy</b> algorithms found the optimal action (action 7, in this case) early, but continued to explore. However, the decaying-epsilon-<b>greedy</b> <b>algorithm</b> explores less and less over time. By time step 3000, it is choosing the optimal action almost all the time.", "dateLastCrawled": "2022-02-02T12:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie Mellon University", "url": "https://blog.ml.cmu.edu/2022/01/07/loop/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2022/01/07/loop", "snippet": "A model-free off-<b>policy</b> reinforcement <b>learning</b> <b>algorithm</b> typically consists of a parameterized actor and a value function (see Figure 2). The actor interacts with the environment collecting the transitions in the replay buffer. The value function is trained using the transitions from the replay buffer to predict the cumulative return of the actor, and the actor is updated by maximizing the action-values at the states visited in the replay buffer. This framework suffers from the following issues:", "dateLastCrawled": "2022-01-30T16:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reinforcement <b>learning</b> applications <b>to machine</b> scheduling problems: a ...", "url": "https://link.springer.com/article/10.1007/s10845-021-01847-3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10845-021-01847-3", "snippet": "They applied the approximate Q-<b>learning</b> <b>algorithm</b> and <b>compared</b> the performance of the <b>algorithm</b> with the FIFO method and heuristic <b>greedy</b> method. The study results verified that approximate Q-<b>learning</b> outperforms other methods and converges to optimal schedule faster. Qu et al. studied the problem of \\(HF/{S}_{jk} block/TC+\\sum {U}_{i}\\) with a multi-skill workforce and multiple <b>machine</b> types. Different from the previous study, they combined <b>machine</b> scheduling with workforce scheduling. The ...", "dateLastCrawled": "2022-02-01T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Community Detection</b> Algorithms \u2013 <b>Machine</b> <b>Learning</b> Blog", "url": "https://machinelearningnow.wordpress.com/2014/08/22/community-detection/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>now.wordpress.com/2014/08/22/<b>community-detection</b>", "snippet": "This is an advantage <b>compared</b> with community finding algorithms based on modularity measures. Training the SOM <b>algorithm</b> over the adjacency matrix of the network it isn\u2019t good enough, hence I\u2019ve done a <b>greedy</b> fine- tuning <b>algorithm</b> for editing the SOM\u2019s assignments. Whereby, only the best topology of the community is preserved and it\u2019s ...", "dateLastCrawled": "2022-01-22T07:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "DeepMind Wants To Change How Reinforcement <b>Learning</b> \u2018Collect &amp; Infer\u2019", "url": "https://analyticsindiamag.com/deepmind-wants-to-change-how-reinforcement-learning-collect-infer/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/deepmind-wants-to-change-how-reinforcement-<b>learning</b>...", "snippet": "Reinforcement <b>learning</b> (RL) is the most widely used <b>machine</b> <b>learning</b> <b>algorithm</b>, besides supervised and unsupervised <b>learning</b> and the less common self-supervised and semi-supervised <b>learning</b>. RL focuses on the controlled <b>learning</b> process, where a <b>machine</b> <b>learning</b> <b>algorithm</b> is provided with a set of actions, parameters, and end values. It teaches ...", "dateLastCrawled": "2022-01-24T04:47:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why <b>Machine Learning Is A Metaphor For</b> Life \u2013 Adit Deshpande ...", "url": "https://adeshpande3.github.io/Why-Machine-Learning-is-a-Metaphor-For-Life", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/Why-<b>Machine-Learning-is-a-Metaphor-For</b>-Life", "snippet": "Another cool <b>analogy</b> is that of the epsilon <b>greedy</b> <b>policy</b>. This is a term used in reinforcement <b>learning</b> to fight the problem of exploration vs exploitation. The basic idea is that the RL agent will take a random action (instead of the optimal action according to its current <b>policy</b>) with probability \u03b5, in hope of searching a larger area of the state space, and eventually getting a better reward.", "dateLastCrawled": "2022-01-31T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Decision Trees <b>in Machine Learning: Functions, Classification, Pros</b> ...", "url": "https://www.upgrad.com/blog/decision-trees-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/decision-trees-in-<b>machine</b>-<b>learning</b>", "snippet": "This <b>analogy</b> is to introduce you to the concept of a decision tree in <b>machine</b> <b>learning</b>. ... The <b>greedy</b> algorithm that reduces the cost function is used to choose the input variables as well as the points of division. The constriction of the tree is terminated with the help of the stopping criterion, which is defined in advance. The stopping criterion could mention anything, such as how many training instances are assigned to the tree\u2019s leaf nodes. 1. <b>Greedy</b> algorithm: The input space has ...", "dateLastCrawled": "2022-02-01T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "AI and Reinforcement <b>Learning</b> \u2014 Machines that Learn through Experience ...", "url": "https://www.cantorsparadise.com/ai-and-reinforcement-learning-machines-that-learn-through-experience-e7eea7bb6765", "isFamilyFriendly": true, "displayUrl": "https://www.cantorsparadise.com/ai-and-reinforcement-<b>learning</b>-<b>machines</b>-that-learn...", "snippet": "This tactic, where two simultaneous, interacting processes, one making the value function consistent with the current <b>policy</b> (<b>policy</b> evaluation), and the other making the <b>policy</b> <b>greedy</b> with respect to the current value function (<b>policy</b> improvement) is known as general <b>policy</b> iteration (GPI) and is not exclusive for Monte Carlo methods. In fact, almost all reinforcement <b>learning</b> methods are well described as GPI.", "dateLastCrawled": "2022-01-25T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Artificial Intelligence</b> and <b>Machine Learning</b>", "url": "https://content.kopykitab.com/ebooks/2016/06/7780/sample/sample_7780.pdf", "isFamilyFriendly": true, "displayUrl": "https://content.kopykitab.com/ebooks/2016/06/7780/sample/sample_7780.pdf", "snippet": "10. REINFORCEMENT <b>LEARNING</b> 186\u2013200 10.1 Markov Decision Problem188 10.2 Q-<b>learning</b> 191 10.2.1 Q-<b>Learning</b> Algorithm191 10.3 Temporal Difference Learning194 10.3.1 On-<b>policy</b> and Off-<b>policy</b> Learning195 10.3.2 Advantages of TD Prediction Methods195 10.4 <b>Learning</b> Automata196 10.5 Case Studies198 10.5.1 Super Mario: Reinforced Learning198 10.6 ...", "dateLastCrawled": "2022-02-02T20:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[1912.10329] Can Agents Learn by <b>Analogy</b>? An Inferable Model for PAC ...", "url": "https://arxiv.org/abs/1912.10329", "isFamilyFriendly": true, "displayUrl": "https://<b>arxiv</b>.org/abs/1912.10329", "snippet": "Model-based reinforcement <b>learning</b> algorithms make decisions by building and utilizing a model of the environment. However, none of the existing algorithms attempts to infer the dynamics of any state-action pair from known state-action pairs before meeting it for sufficient times. We propose a new model-based method called <b>Greedy</b> Inference Model (GIM) that infers the unknown dynamics from known dynamics based on the internal spectral properties of the environment. In other words, GIM can ...", "dateLastCrawled": "2021-10-26T15:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Finding the Word <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-word-<b>analogy</b>-from-given-words-using-word2vec...", "snippet": "In the word <b>analogy</b> task, we complete the sentence ... What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the word <b>analogy</b>. In this tutorial, we will be using Word2Vec model and a pre-trained model named \u2018GoogleNews-vectors-negative300.bin\u2018 which is trained on over 50 Billion words by Google. Each word inside the pre-trained dataset is embedded in a 300-dimensional space and the words which are similar in context/meaning are placed closer to each other in ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Modern Artificial Intelligence via Deep <b>Learning</b>", "url": "https://www.doc.ic.ac.uk/~mpd37/teaching/ml_tutorials/2016-10-19-Eslami-Modern_AI_via_Deep_Learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.doc.ic.ac.uk/.../2016-10-19-Eslami-Modern_AI_via_Deep_<b>Learning</b>.pdf", "snippet": "<b>Greedy</b>: Follow the max \u03b5-<b>greedy</b>: Follow the max with (1-\u03b5) probability and random otherwise Human-level control in ATARI The action-value function Maximizing Q\u03c0(s,a) over possible policies gives the optimal action-value function and the Bellman equation: Value iteration Basic idea: Approximate Apply the Bellman Equation as an iterative update: Human-level control in ATARI. Human-level control in ATARI End-to-end reinforcement <b>learning</b> Mnih et al. (2015) We need a loss function to minimize ...", "dateLastCrawled": "2021-09-02T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>All Unit MCQ questions of ML</b> \u2013 TheCodingShef", "url": "https://thecodingshef.com/all-unit-mcq-questions-of-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thecodingshef.com/<b>all-unit-mcq-questions-of</b>-<b>machine</b>-<b>learning</b>", "snippet": "Correct option is C. Choose the correct option regarding <b>machine</b> <b>learning</b> (ML) and artificial intelligence (AI) ML is a set of techniques that turns a dataset into a software. AI is a software that can emulate the human mind. ML is an alternate way of programming intelligent machines. All of the above. Correct option is D.", "dateLastCrawled": "2022-01-30T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Instance-based learning - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/instance-based-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/instance-based-<b>learning</b>", "snippet": "The <b>Machine</b> <b>Learning</b> systems which are categorized as instance-based <b>learning</b> are the systems that learn the training examples by heart and then generalizes to new instances based on some similarity measure. It is called instance-based because it builds the hypotheses from the training instances. It is also known as memory-based <b>learning</b> or lazy-<b>learning</b>.The time complexity of this algorithm depends upon the size of training data.", "dateLastCrawled": "2022-02-03T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Reinforcement Learning</b> For Mice. An Anology Between Animals And\u2026 | by ...", "url": "https://towardsdatascience.com/reinforcement-learning-3f87a0290ba2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning</b>-3f87a0290ba2", "snippet": "RL and Animal <b>Learning</b>. Below is an <b>analogy</b> between the mouse-maze experiment and RL concepts. Image by Author. Agent: The component that makes the decision of what action to take. Our agent is the mouse in this case. Environment: Physical world in which the agent operates. The maze is the environment. Actions: The agent\u2019s methods that allow it to interact and change its environment, and thus transfer between states. In this case, the mouse\u2019s motions to the right, left, forward, and ...", "dateLastCrawled": "2022-01-31T10:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement <b>Learning</b> | Ioannis Anifantakis | Analytics Vidhya", "url": "https://medium.com/analytics-vidhya/reinforcement-learning-basic-understanding-4fcb91ba4e4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/reinforcement-<b>learning</b>-basic-understanding-4fcb91ba4e4", "snippet": "Reinforcement <b>Learning</b> (RL) is a <b>Machine</b> <b>Learning</b> field which gained much attention since 2015 after Google\u2019s Deep Mind team demonstrated self-taught DQN agents <b>learning</b> to walk, mastering Atari ...", "dateLastCrawled": "2021-08-02T19:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Traveling Salesman Problem Theory and Applications</b> | Mikhil Raj ...", "url": "https://www.academia.edu/4409399/Traveling_Salesman_Problem_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/4409399/<b>Traveling_Salesman_Problem_Theory_and_Applications</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-30T12:57:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(greedy policy)  is like +(machine learning algorithm)", "+(greedy policy) is similar to +(machine learning algorithm)", "+(greedy policy) can be thought of as +(machine learning algorithm)", "+(greedy policy) can be compared to +(machine learning algorithm)", "machine learning +(greedy policy AND analogy)", "machine learning +(\"greedy policy is like\")", "machine learning +(\"greedy policy is similar\")", "machine learning +(\"just as greedy policy\")", "machine learning +(\"greedy policy can be thought of as\")", "machine learning +(\"greedy policy can be compared to\")"]}