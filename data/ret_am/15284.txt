{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>BERT</b> \u2014 <b>Bidirectional</b> <b>Encoder</b> Representation <b>from Transformers</b>", "url": "https://naokishibuya.medium.com/bert-bidirectional-encoder-representation-from-transformers-525ca78e1896", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/<b>bert</b>-<b>bidirectional</b>-<b>encoder</b>-representation-from...", "snippet": "<b>BERT</b> stands for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. As the name suggests, it generates <b>representations</b> using an <b>encoder</b> from Vaswani et al.\u2019s <b>Transformer</b> architecture. However, there are notable differences between <b>BERT</b> and the original <b>Transformer</b>, especially in how they train those models. This article discusses the ...", "dateLastCrawled": "2022-02-07T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How Does <b>BERT</b> Answer Questions?: A Layer-Wise Analysis of <b>Transformer</b> ...", "url": "https://www.researchgate.net/publication/337015027_How_Does_BERT_Answer_Questions_A_Layer-Wise_Analysis_of_Transformer_Representations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337015027_How_Does_<b>BERT</b>_Answer_Questions_A...", "snippet": "<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) (Devlin et al., 2018) is <b>one</b> of the key innovations in language representation learning (Howard &amp; Ruder, 2018;Peters et al., 2018).", "dateLastCrawled": "2022-01-30T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) A Text Abstraction Summary Model Based on <b>BERT</b> Word Embedding and ...", "url": "https://www.researchgate.net/publication/337014455_A_Text_Abstraction_Summary_Model_Based_on_BERT_Word_Embedding_and_Reinforcement_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337014455_A_Text_Abstraction_Summary_Model...", "snippet": "The sentence <b>encoder</b> adopts <b>BERT</b> as the <b>encoder</b>, a <b>bidirectional</b> GRU with self- attention is used to encode doc ument, a unidire ctional GRU is used to compute the sum mary represen tation.", "dateLastCrawled": "2022-01-15T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Clinical concept extraction using <b>transformers</b> | Request PDF", "url": "https://www.researchgate.net/publication/346562673_Clinical_concept_extraction_using_transformers", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../346562673_Clinical_concept_extraction_using_<b>transformers</b>", "snippet": "<b>Current</b> models do not <b>take</b> full advantage of cancer domain-specific corpus, whether pre-training <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> from <b>Transformer</b> model on cancer-specific corpus could ...", "dateLastCrawled": "2021-11-23T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "VideoBERT: A Joint Model <b>for Video and Language Representation Learning</b> ...", "url": "https://www.researchgate.net/publication/339561791_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339561791_Video<b>BERT</b>_A_Joint_Model_for_Video...", "snippet": "It is a breakthrough in NLP and is the base for many language models including <b>bidirectional</b> <b>encoder</b> <b>representations</b> <b>from transformers</b>, <b>BERT</b>, (Devlin et al., 2019), generative pretrained ...", "dateLastCrawled": "2021-11-14T10:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "New submissions for Tue, 8 Jun 21 \u00b7 Issue #129 \u00b7 dajinstory/daily-arxiv ...", "url": "https://github.com/dajinstory/daily-arxiv-noti/issues/129", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dajinstory/daily-arxiv-noti/issues/129", "snippet": "<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) has recently gained its popularity by establishing the state-of-the-art scores in several NLP benchmarks. A Lite <b>BERT</b> (ALBERT) is literally characterized as a lightweight version of <b>BERT</b>, in which the number of <b>BERT</b> parameters is reduced by repeatedly applying the same neural network called <b>Transformer&#39;s</b> <b>encoder</b> layer. By pre-training the parameters with a massive amount of natural language data, ALBERT <b>can</b> convert input ...", "dateLastCrawled": "2021-11-26T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Clinical <b>concept extraction: A</b> methodology review - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1532046420301544", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046420301544", "snippet": "The contextual pre-trained language models, such as <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>), leveraged the <b>bidirectional</b> training of <b>transformers</b>, an attention mechanism that learns contextual relations between words, resulting in state-of-the-art results in six <b>concept extraction</b> tasks. Hybrid and traditional machine learning achieved the best performance on <b>two</b> tasks. In", "dateLastCrawled": "2022-01-26T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A survey on <b>the techniques, applications, and performance of</b> short text ...", "url": "https://www.researchgate.net/publication/343829204_A_survey_on_the_techniques_applications_and_performance_of_short_text_semantic_similarity", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/343829204_A_survey_on_the_techniques...", "snippet": "Especially, <b>bidirectional</b> <b>encoder</b> <b>representations</b> from <b>transformer</b> model <b>can</b> fully employ scarce information of short texts and semantic information and obtain higher accuracy and F1 value. We ...", "dateLastCrawled": "2022-01-20T13:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Artificial Intelligence in the Battle against Coronavirus (COVID</b>-19): A ...", "url": "https://europepmc.org/article/PPR/PPR314276", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PPR/PPR314276", "snippet": "These tools may include Embeddings from Language Models (ELMo) , Universal Language Model Fine-Tuning (ULMFiT) , <b>Transformer</b> , Google\u2019s <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) , <b>Transformer</b>-XL , XLNet , Enhanced Representation through kNowledge IntEgration (ERNIE) , Text-to-Text Transfer <b>Transformer</b> (T5) , Binary-Partitioning <b>Transformer</b> (BPT) and OpenAI\u2019s Generative Pretrained <b>Transformer</b> 2 (GPT-2) . The core components of these tools are deep learning and ...", "dateLastCrawled": "2021-08-29T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Arabic question answering system: a survey | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10462-021-10031-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10462-021-10031-1", "snippet": "This module is composed of <b>two</b> parts\u2013QANet and <b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>). QANet (Yu et al. 2018) is a convolution and self-attention-based neural network, while <b>BERT</b> (Devlin et al. 2019) is a <b>transformer</b>-based pre-trained model. The third component is the ranking module, which re-ranks candidate answers ...", "dateLastCrawled": "2021-12-31T11:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>BERT</b> \u2014 <b>Bidirectional</b> <b>Encoder</b> Representation <b>from Transformers</b>", "url": "https://naokishibuya.medium.com/bert-bidirectional-encoder-representation-from-transformers-525ca78e1896", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/<b>bert</b>-<b>bidirectional</b>-<b>encoder</b>-representation-from...", "snippet": "<b>BERT</b> stands for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. As the name suggests, it generates <b>representations</b> using an <b>encoder</b> from Vaswani et al.\u2019s <b>Transformer</b> architecture. However, there are notable differences between <b>BERT</b> and the original <b>Transformer</b>, especially in how they train those models. This article discusses the ...", "dateLastCrawled": "2022-02-07T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How Does <b>BERT</b> Answer Questions?: A Layer-Wise Analysis of <b>Transformer</b> ...", "url": "https://www.researchgate.net/publication/337015027_How_Does_BERT_Answer_Questions_A_Layer-Wise_Analysis_of_Transformer_Representations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337015027_How_Does_<b>BERT</b>_Answer_Questions_A...", "snippet": "<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) (Devlin et al., 2018) is <b>one</b> of the key innovations in language representation learning (Howard &amp; Ruder, 2018;Peters et al., 2018).", "dateLastCrawled": "2022-01-30T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "New submissions for Tue, 8 Jun 21 \u00b7 Issue #129 \u00b7 dajinstory/daily-arxiv ...", "url": "https://github.com/dajinstory/daily-arxiv-noti/issues/129", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dajinstory/daily-arxiv-noti/issues/129", "snippet": "<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) has recently gained its popularity by establishing the state-of-the-art scores in several NLP benchmarks. A Lite <b>BERT</b> (ALBERT) is literally characterized as a lightweight version of <b>BERT</b>, in which the number of <b>BERT</b> parameters is reduced by repeatedly applying the same neural network called <b>Transformer&#39;s</b> <b>encoder</b> layer. By pre-training the parameters with a massive amount of natural language data, ALBERT <b>can</b> convert input ...", "dateLastCrawled": "2021-11-26T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Clinical concept extraction using <b>transformers</b> | Request PDF", "url": "https://www.researchgate.net/publication/346562673_Clinical_concept_extraction_using_transformers", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../346562673_Clinical_concept_extraction_using_<b>transformers</b>", "snippet": "<b>Current</b> models do not <b>take</b> full advantage of cancer domain-specific corpus, whether pre-training <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> from <b>Transformer</b> model on cancer-specific corpus could ...", "dateLastCrawled": "2021-11-23T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Clinical <b>concept extraction: A</b> methodology review - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1532046420301544", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046420301544", "snippet": "The contextual pre-trained language models, such as <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>), leveraged the <b>bidirectional</b> training of <b>transformers</b>, an attention mechanism that learns contextual relations between words, resulting in state-of-the-art results in six <b>concept extraction</b> tasks. Hybrid and traditional machine learning achieved the best performance on <b>two</b> tasks. In", "dateLastCrawled": "2022-01-26T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>comparative study of effective approaches for Arabic sentiment</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457320309316", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457320309316", "snippet": "The introduction of <b>Bidirectional</b> <b>Encoder</b> Representation <b>from Transformers</b> (<b>BERT</b>) (Devlin et al., 2019), led to a revolution in the NLP world. The proposed architecture is also reliant on transfer learning and fine-tuning. In the experiments, we use <b>two</b> variants of <b>BERT</b>-based models. The first is the multilingual <b>BERT</b> which was trained on 104 languages and relies on 110K shared WordPiece vocabulary. <b>BERT</b> consists of 12 layers with 768 hidden units in each of them, and 12 attention heads. The ...", "dateLastCrawled": "2021-12-09T08:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Artificial Intelligence in the Battle against Coronavirus (COVID</b>-19): A ...", "url": "https://europepmc.org/article/PPR/PPR314276", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PPR/PPR314276", "snippet": "These tools may include Embeddings from Language Models (ELMo) , Universal Language Model Fine-Tuning (ULMFiT) , <b>Transformer</b> , Google\u2019s <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) , <b>Transformer</b>-XL , XLNet , Enhanced Representation through kNowledge IntEgration (ERNIE) , Text-to-Text Transfer <b>Transformer</b> (T5) , Binary-Partitioning <b>Transformer</b> (BPT) and OpenAI\u2019s Generative Pretrained <b>Transformer</b> 2 (GPT-2) . The core components of these tools are deep learning and ...", "dateLastCrawled": "2021-08-29T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "TabReformer: Unsupervised Representation Learning for Erroneous Data ...", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3447541", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3447541", "snippet": "Moreover, most of these detection models either require user-defined rules or ample hand-labeled training examples. Therefore, in this article, we present TabReformer, a model that learns <b>bidirectional</b> <b>encoder</b> <b>representations</b> for tabular data. The proposed model consists of <b>two</b> main phases. In the first phase, TabReformer follows <b>encoder</b> ...", "dateLastCrawled": "2021-12-31T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Summarization BOOKSUM: A Collection of Datasets for Long-form Narrative", "url": "https://www.readkong.com/page/summarization-booksum-a-collection-of-datasets-for-9708861", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/summarization-booksum-a-collection-of-datasets-for-9708861", "snippet": "<b>BERT</b>: pre-training of deep <b>bidirectional</b> <b>transformers</b> for language under- Faisal Ladhak, Bryan Li, Yaser Al-Onaizan, and Kath- standing. In Proceedings of the 2019 Conference leen R. McKeown. 2020. Exploring content selec- of the North American Chapter of the Association tion in summarization of novel chapters. In Proceed- for Computational Linguistics: Human Language ings of the 58th Annual Meeting of the Association Technologies, NAACL-HLT 2019, Minneapolis, MN, for Computational ...", "dateLastCrawled": "2022-01-23T22:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Recent Advances and Applications of Deep Learning Methods in Materials ...", "url": "https://www.arxiv-vanity.com/papers/2110.14820/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2110.14820", "snippet": "Deep learning (DL) is <b>one</b> of the fastest growing topics in materials data science, with rapidly emerging applications spanning atomistic, image-based, spectral, and textual data modalities. DL allows analysis of unstructured data and automated identification of features. Recent development of large materials databases has fueled the application of DL methods in atomistic prediction in particular. In contrast, advances in image and spectral data have largely leveraged synthetic data enabled ...", "dateLastCrawled": "2022-02-03T11:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How Does <b>BERT</b> Answer Questions?: A Layer-Wise Analysis of <b>Transformer</b> ...", "url": "https://www.researchgate.net/publication/337015027_How_Does_BERT_Answer_Questions_A_Layer-Wise_Analysis_of_Transformer_Representations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337015027_How_Does_<b>BERT</b>_Answer_Questions_A...", "snippet": "<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) (Devlin et al., 2018) is <b>one</b> of the key innovations in language representation learning (Howard &amp; Ruder, 2018;Peters et al., 2018).", "dateLastCrawled": "2022-01-30T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "VideoBERT: A Joint Model <b>for Video and Language Representation Learning</b> ...", "url": "https://www.researchgate.net/publication/339561791_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339561791_Video<b>BERT</b>_A_Joint_Model_for_Video...", "snippet": "It is a breakthrough in NLP and is the base for many language models including <b>bidirectional</b> <b>encoder</b> <b>representations</b> <b>from transformers</b>, <b>BERT</b>, (Devlin et al., 2019), generative pretrained ...", "dateLastCrawled": "2021-11-14T10:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Sensors | Free Full-Text | Subsentence Extraction from Text Using ...", "url": "https://www.mdpi.com/1424-8220/21/8/2712/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/21/8/2712/htm", "snippet": "An overview of <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) (left) and task-driven fine-tuning models (right). Input sentence is split into multiple tokens (T o k N) and fed to a <b>BERT</b> model, which outputs embedded <b>output</b> feature vectors, O N, for each token. By attaching <b>different</b> head layers on top, it transforms <b>BERT</b> into a ...", "dateLastCrawled": "2021-12-02T14:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Clinical <b>concept extraction: A</b> methodology review - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1532046420301544", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046420301544", "snippet": "The contextual pre-trained language models, such as <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>), leveraged the <b>bidirectional</b> training of <b>transformers</b>, an attention mechanism that learns contextual relations between words, resulting in state-of-the-art results in six <b>concept extraction</b> tasks. Hybrid and traditional machine learning achieved the best performance on <b>two</b> tasks. In", "dateLastCrawled": "2022-01-26T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) VisQA: X-raying Vision and Language Reasoning in <b>Transformers</b>", "url": "https://www.researchgate.net/publication/354939528_VisQA_X-raying_Vision_and_Language_Reasoning_in_Transformers", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354939528_VisQA_X-raying_Vision_and_Language...", "snippet": "2.2 Vision-Language (VL)-<b>Transf ormers</b>. <b>Transformer</b> have been e xtended to reasoning on multiple modalities, in particular vision and language, through <b>different</b> types of layers: Language-only and ...", "dateLastCrawled": "2022-01-04T16:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>SPECTER: Document-level Representation Learning using Citation-informed</b> ...", "url": "https://www.researchgate.net/publication/343302079_SPECTER_Document-level_Representation_Learning_using_Citation-informed_Transformers", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/343302079_SPECTER_Document-level...", "snippet": "SPECTER is a novel method to generate document-level embeddings of scientific documents based on a <b>transformer</b> language model and the network of citations [Cohan et al., 2020]. SPECTER does not ...", "dateLastCrawled": "2022-01-17T15:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bert</b> Classification Text Tutorial [U43VJ9]", "url": "https://masserielussosalento.le.it/Bert_Text_Classification_Tutorial.html", "isFamilyFriendly": true, "displayUrl": "https://masserielussosalento.le.it/<b>Bert</b>_Text_Classification_Tutorial.html", "snippet": "<b>BERT</b> stands for <b>Bidirectional</b> Representation for <b>Transformers</b>, was proposed by researchers at Google AI language in 2018. <b>BERT</b>; Tutorial; Word embeddings; 2020-07-06 About [1909. Fine-Tune <b>BERT</b> for Spam Classification. This tutorial shows you how to train the <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) model on Cloud TPU. All we did was apply a <b>BERT</b>-style data transformation to pre-process the data, automatically download the pre-trained model, and feed the transformed ...", "dateLastCrawled": "2021-12-22T16:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "PEGASUS: Pre-training with <b>Extracted Gap-sentences for Abstractive</b> ...", "url": "https://deepai.org/publication/pegasus-pre-training-with-extracted-gap-sentences-for-abstractive-summarization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/pegasus-pre-training-with-extracted-gap-sentences-for...", "snippet": "In this work, we propose pre-training large <b>Transformer</b>-based <b>encoder</b>-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as <b>one</b> <b>output</b> sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills ...", "dateLastCrawled": "2022-01-25T12:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "lens, <b>align</b>. - goo", "url": "https://blog.goo.ne.jp/razoralign/rss2.xml", "isFamilyFriendly": true, "displayUrl": "https://blog.goo.ne.jp/razor<b>align</b>/rss2.xml", "snippet": "DNABERT: pre-trained <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> model for DNA-language in genome ... DNABERT adapts the idea of <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) model to DNA setting and developed a first-of-its-kind deep learning method in genomics. DNABERT resolves the challenges by developing general and transferable understandings of DNA from the purely unlabeled human genome, and utilizing them to generically solve various sequence-related tasks ...", "dateLastCrawled": "2020-10-21T06:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>BERT</b>: Pre-training of Deep <b>Bidirectional</b> <b>Transformers</b> for Language ...", "url": "https://cs330.stanford.edu/presentations/presentation-10.23-1.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs330.stanford.edu/presentations/presentation-10.23-1.pdf", "snippet": "Unidirectional LMs have limited expressive <b>power</b> <b>Can</b> only see left context or right context Solution: <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> <b>Bidirectional</b>: the word <b>can</b> see both side at the same time Empirically, improved the fine-tuning based approaches. Method Overview <b>BERT</b> = <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> <b>Two</b> steps: Pre-training on unlabeled text corpus Masked LM Next sentence prediction Fine-tuning on specific task Plug in the task specific ...", "dateLastCrawled": "2022-02-05T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How Does <b>BERT</b> Answer Questions?: A Layer-Wise Analysis of <b>Transformer</b> ...", "url": "https://www.researchgate.net/publication/337015027_How_Does_BERT_Answer_Questions_A_Layer-Wise_Analysis_of_Transformer_Representations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337015027_How_Does_<b>BERT</b>_Answer_Questions_A...", "snippet": "<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) (Devlin et al., 2018) is <b>one</b> of the key innovations in language representation learning (Howard &amp; Ruder, 2018;Peters et al., 2018).", "dateLastCrawled": "2022-01-30T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Clinical concept extraction using <b>transformers</b> | Request PDF", "url": "https://www.researchgate.net/publication/346562673_Clinical_concept_extraction_using_transformers", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../346562673_Clinical_concept_extraction_using_<b>transformers</b>", "snippet": "<b>Current</b> models do not <b>take</b> full advantage of cancer domain-specific corpus, whether pre-training <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> from <b>Transformer</b> model on cancer-specific corpus could ...", "dateLastCrawled": "2021-11-23T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "New submissions for Tue, 8 Jun 21 \u00b7 Issue #129 \u00b7 dajinstory/daily-arxiv ...", "url": "https://github.com/dajinstory/daily-arxiv-noti/issues/129", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dajinstory/daily-arxiv-noti/issues/129", "snippet": "<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) has recently gained its popularity by establishing the state-of-the-art scores in several NLP benchmarks. A Lite <b>BERT</b> (ALBERT) is literally characterized as a lightweight version of <b>BERT</b>, in which the number of <b>BERT</b> parameters is reduced by repeatedly applying the same neural network called <b>Transformer&#39;s</b> <b>encoder</b> layer. By pre-training the parameters with a massive amount of natural language data, ALBERT <b>can</b> convert input ...", "dateLastCrawled": "2021-11-26T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Clinical <b>concept extraction: A</b> methodology review - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1532046420301544", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046420301544", "snippet": "The contextual pre-trained language models, such as <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>), leveraged the <b>bidirectional</b> training of <b>transformers</b>, an attention mechanism that learns contextual relations between words, resulting in state-of-the-art results in six <b>concept extraction</b> tasks. Hybrid and traditional machine learning achieved the best performance on <b>two</b> tasks. In", "dateLastCrawled": "2022-01-26T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>comparative study of effective approaches for Arabic sentiment</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457320309316", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457320309316", "snippet": "The introduction of <b>Bidirectional</b> <b>Encoder</b> Representation <b>from Transformers</b> (<b>BERT</b>) (Devlin et al., 2019), led to a revolution in the NLP world. The proposed architecture is also reliant on transfer learning and fine-tuning. In the experiments, we use <b>two</b> variants of <b>BERT</b>-based models. The first is the multilingual <b>BERT</b> which was trained on 104 languages and relies on 110K shared WordPiece vocabulary. <b>BERT</b> consists of 12 layers with 768 hidden units in each of them, and 12 attention heads. The ...", "dateLastCrawled": "2021-12-09T08:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Artificial Intelligence in the Battle against Coronavirus (COVID</b>-19): A ...", "url": "https://europepmc.org/article/PPR/PPR314276", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PPR/PPR314276", "snippet": "These tools may include Embeddings from Language Models (ELMo) , Universal Language Model Fine-Tuning (ULMFiT) , <b>Transformer</b> , Google\u2019s <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) , <b>Transformer</b>-XL , XLNet , Enhanced Representation through kNowledge IntEgration (ERNIE) , Text-to-Text Transfer <b>Transformer</b> (T5) , Binary-Partitioning <b>Transformer</b> (BPT) and OpenAI\u2019s Generative Pretrained <b>Transformer</b> 2 (GPT-2) . The core components of these tools are deep learning and ...", "dateLastCrawled": "2021-08-29T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Proceedings of the 15th International Workshop on Semantic Evaluation ...", "url": "https://aclanthology.org/volumes/2021.semeval-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2021.semeval-1", "snippet": "It consists of three parts: a <b>transformer</b>-based model <b>that can</b> obtain the token representation, an auxiliary information module that combines features from <b>different</b> layers, and an <b>output</b> layer used for the classification. Various <b>BERT</b>-based models, such as <b>BERT</b>, ALBERT, RoBERTa, and XLNET, were used to learn contextual <b>representations</b>. The predictions of these models were assembled to improve the sequence labeling tasks by using a voting strategy. Experimental results showed that the ...", "dateLastCrawled": "2022-02-02T18:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Summarization BOOKSUM: A Collection of Datasets for Long-form Narrative", "url": "https://www.readkong.com/page/summarization-booksum-a-collection-of-datasets-for-9708861", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/summarization-booksum-a-collection-of-datasets-for-9708861", "snippet": "<b>BERT</b>: pre-training of deep <b>bidirectional</b> <b>transformers</b> for language under- Faisal Ladhak, Bryan Li, Yaser Al-Onaizan, and Kath- standing. In Proceedings of the 2019 Conference leen R. McKeown. 2020. Exploring content selec- of the North American Chapter of the Association tion in summarization of novel chapters. In Proceed- for Computational Linguistics: Human Language ings of the 58th Annual Meeting of the Association Technologies, NAACL-HLT 2019, Minneapolis, MN, for Computational ...", "dateLastCrawled": "2022-01-23T22:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Recent Advances and Applications of Deep Learning Methods in Materials ...", "url": "https://www.arxiv-vanity.com/papers/2110.14820/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2110.14820", "snippet": "Deep learning (DL) is <b>one</b> of the fastest growing topics in materials data science, with rapidly emerging applications spanning atomistic, image-based, spectral, and textual data modalities. DL allows analysis of unstructured data and automated identification of features. Recent development of large materials databases has fueled the application of DL methods in atomistic prediction in particular. In contrast, advances in image and spectral data have largely leveraged synthetic data enabled ...", "dateLastCrawled": "2022-02-03T11:10:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "14.8. <b>Bidirectional Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b> ...", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_natural-language-processing-pretraining/<b>bert</b>.html", "snippet": "Combining the best of both worlds, <b>BERT</b> (<b>Bidirectional Encoder</b> <b>Representations</b> <b>from Transformers</b>) encodes context bidirectionally and requires minimal architecture changes for a wide range of natural language processing tasks [Devlin et al., 2018]. Using a pretrained transformer <b>encoder</b>, <b>BERT</b> is able to represent any token based on its <b>bidirectional</b> context. During supervised <b>learning</b> of downstream tasks, <b>BERT</b> is similar to GPT in two aspects. First, <b>BERT</b> <b>representations</b> will be fed into an ...", "dateLastCrawled": "2022-02-02T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "This hampers <b>learning</b> unnecessarily, they argue, and they proposed a <b>bidirectional</b> variant instead: <b>BERT</b>, or <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. It is covered in this article. Firstly, we\u2019ll briefly take a look at finetuning-based approaches in NLP, which is followed by <b>BERT</b> as well.", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>BERT Word Embeddings Tutorial</b> \u00b7 Chris McCormick", "url": "http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/", "isFamilyFriendly": true, "displayUrl": "mccormickml.com/2019/05/14/<b>BERT-word-embeddings-tutorial</b>", "snippet": "<b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>), released in late 2018, is the model we will use in this tutorial to provide readers with a better understanding of and practical guidance for using transfer <b>learning</b> models in NLP. <b>BERT</b> is a method of pretraining language <b>representations</b> that was used to create models that NLP practicioners can then download and use for free. You can either use these models to extract high quality language features from your text data, or you ...", "dateLastCrawled": "2022-01-30T05:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Med-BERT: pretrained contextualized embeddings on large</b>-scale ...", "url": "https://www.nature.com/articles/s41746-021-00455-y", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41746-021-00455-y", "snippet": "Recently, <b>bidirectional</b> <b>encoder</b> <b>representations</b> <b>from transformers</b> (<b>BERT</b>) and related models have achieved tremendous successes in the natural language processing domain. The pretraining of <b>BERT</b> on ...", "dateLastCrawled": "2022-01-28T20:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "DNABERT: <b>pre-trained Bidirectional Encoder Representations from</b> ...", "url": "https://www.researchgate.net/publication/349060790_DNABERT_pre-trained_Bidirectional_Encoder_Representations_from_Transformers_model_for_DNA-language_in_genome", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349060790_DNA<b>BERT</b>_pre-trained_<b>Bidirectional</b>...", "snippet": "<b>Bidirectional</b> <b>encoder</b> <b>representations</b> from Transformer (<b>BERT</b>) is a language-based deep <b>learning</b> model that is highly interpretable. Therefore, a model based on <b>BERT</b> architecture can potentially ...", "dateLastCrawled": "2022-01-29T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Ultimate Guide To Different Word Embedding Techniques In NLP ...", "url": "https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html", "snippet": "<b>BERT</b> \u2014 <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. Introduced by Google in 2019, <b>BERT</b> belongs to a class of NLP-based language algorithms known as <b>transformers</b>. <b>BERT</b> is a massive pre-trained deeply <b>bidirectional</b> <b>encoder</b>-based transformer model that comes in two variants. <b>BERT</b>-Base has 110 million parameters, and <b>BERT</b>-Large has ...", "dateLastCrawled": "2022-02-03T07:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "LawBERT: Towards a Legal Domain-Specific <b>BERT</b>? | by Erin Yijie Zhang ...", "url": "https://towardsdatascience.com/lawbert-towards-a-legal-domain-specific-bert-716886522b49", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/law<b>bert</b>-towards-a-legal-domain-specific-<b>bert</b>-716886522b49", "snippet": "Google\u2019s <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) is a large-scale pre-trained autoencoding language model developed in 2018. Its development has been described as the NLP community\u2019s \u201cImageNet moment\u201d, largely because of how adept <b>BERT</b> is at performing downstream NLP language understanding tasks with very little backpropagation and fine-tuning needed (usually only 2\u20134 epochs).", "dateLastCrawled": "2022-01-27T06:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to perform Text Summarization with Python, HuggingFace <b>Transformers</b> ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2020/12/21/easy-text-summarization-with-hugging...", "snippet": "The <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> by Devlin et al. (2018) takes the <b>encoder</b> segment from the classic (or vanilla) Transformer, slightly changes how the inputs are generated (by means of WordPiece rather than learned embeddings) and changes the <b>learning</b> task into a Masked Language Model plus Next Sentence Prediction (NSP) rather than training a simple language model. They also follow the argument for pretraining and subsequent fine-tuning: by taking the <b>encoder</b> ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Introduction to Text <b>Representations</b> for Language Processing \u2014 Part 2 ...", "url": "https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-text-<b>representations</b>-for-language...", "snippet": "<b>BERT</b>. <b>BERT</b> is a paper from the Google AI team in the name of <b>BERT</b>: Pre-training of Deep <b>Bidirectional</b> <b>Transformers</b> for Language Understanding which came out of May 2019. It is a new self-supervised <b>learning</b> task for pre-training <b>transformers</b> in order to fine-tune them for downstream tasks", "dateLastCrawled": "2022-01-31T04:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Ultimate Guide To Different Word Embedding Techniques In NLP ...", "url": "https://machinelearningmastery.in/2021/11/10/the-ultimate-guide-to-different-word-embedding-techniques-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.in/2021/11/10/the-ultimate-guide-to-different-word...", "snippet": "Let\u2019s have a look at some of the most promising word embedding techniques in NLP. 1. TF-IDF \u2014 Term Frequency-Inverse Document Frequency. TF-IDF is a <b>machine</b> <b>learning</b> (ML) algorithm based on a statistical measure of finding the relevance of words in the text.", "dateLastCrawled": "2022-01-09T14:18:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bert (bidirectional encoder representations from transformers))  is like +(transformer that can take in electrical power from two different sources and output it as one unified current)", "+(bert (bidirectional encoder representations from transformers)) is similar to +(transformer that can take in electrical power from two different sources and output it as one unified current)", "+(bert (bidirectional encoder representations from transformers)) can be thought of as +(transformer that can take in electrical power from two different sources and output it as one unified current)", "+(bert (bidirectional encoder representations from transformers)) can be compared to +(transformer that can take in electrical power from two different sources and output it as one unified current)", "machine learning +(bert (bidirectional encoder representations from transformers) AND analogy)", "machine learning +(\"bert (bidirectional encoder representations from transformers) is like\")", "machine learning +(\"bert (bidirectional encoder representations from transformers) is similar\")", "machine learning +(\"just as bert (bidirectional encoder representations from transformers)\")", "machine learning +(\"bert (bidirectional encoder representations from transformers) can be thought of as\")", "machine learning +(\"bert (bidirectional encoder representations from transformers) can be compared to\")"]}