{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Interpreting machine learning models | by Lars Hulstaert | Towards Data ...", "url": "https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpretability</b>-in-machine-learning-70c30694a05f", "snippet": "With the rise of data and privacy protection regulation <b>like</b> GDPR, <b>interpretability</b> becomes even more essential. In addition, in medical applications or self-driving cars, a single incorrect prediction can have a significant impact and <b>being</b> <b>able</b> to \u2018verify\u2019 the model is critical. Therefore the system should be <b>able</b> to explain how it reached a given recommendation. Interpreting your models. A common quote on model <b>interpretability</b> is that with an increase in model complexity, model ...", "dateLastCrawled": "2022-02-03T07:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "6 \u2013 <b>Interpretability</b> \u2013 Machine Learning Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "As such, we believe that the true benefit of an <b>interpretability</b> framework is <b>being</b> <b>able</b> to formalize these different aspects of <b>interpretability</b>, then pick the most relevant method. The claim \u201cyou should use SHAP because it makes your model more interpretable\u201d is largely useless. However, the claim \u201cyou should use SHAP because the decomposability it offers is worth the risk of using a post-hoc explanation\u201d makes clear both the benefits and risks of SHAP.", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What everyone <b>needs to know about interpretability in machine learning</b> ...", "url": "https://dallascard.medium.com/what-everyone-needs-to-know-about-interpretability-in-machine-learning-d5ce16730407", "isFamilyFriendly": true, "displayUrl": "https://dallascard.medium.com/what-everyone-<b>needs-to-know-about-interpretability</b>-in...", "snippet": "The whole idea of supervised learning is automatically discovering patterns in large amounts of data that allow us to <b>map</b> from a given input to a predicted label or outcome. This does not, however, imply that there is any direct causal connection between the outcomes and the patterns that have been found. For example, think of the famous \u201cmarshmallow experiment\u201d. Give a child a marshmallow and tell them that if they don\u2019t eat it right away, they can have two later. In addition to the ...", "dateLastCrawled": "2022-01-12T04:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Interpretability vs Explainability: The Black</b> Box of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-<b>interpretability</b>-vs-explainability", "snippet": "High model <b>interpretability</b> wins arguments. Risk and responsibility. A model with high <b>interpretability</b> is desirable on a high-risk stakes game. High interpretable models equate to <b>being</b> <b>able</b> to hold another party liable. And when models are predicting whether a person has cancer, people need to be held accountable for the decision that was ...", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Building Blocks of <b>Interpretability</b> - Distill", "url": "https://distill.pub/2018/building-blocks/", "isFamilyFriendly": true, "displayUrl": "https://distill.pub/2018/building-blocks", "snippet": "The most common interface for attribution is called a saliency <b>map</b> \u2014 a simple heatmap that highlights pixels of the input image that most caused the output classification. We see two weaknesses with this current approach. First, it is not clear that individual pixels should be the primary unit of attribution. The meaning of each pixel is extremely entangled with other pixels, is not robust to simple visual transforms (e.g., brightness, contrast, etc.), and is far-removed from high-level ...", "dateLastCrawled": "2022-02-02T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Interpretable deep learning in computer vision | The Startup", "url": "https://medium.com/swlh/deep-learning-and-medical-imaging-interpret-what-the-model-sees-3a1a35b2a323", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/deep-learning-and-medical-imaging-interpret-what-the-model...", "snippet": "The goal of this tutorial is to get you familiar with a popular <b>interpretability</b> technique called Class Activation <b>Map</b>, that you can adapt to your projects. This will hopefully give you more ...", "dateLastCrawled": "2022-01-29T07:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Interpretability</b> of time-series deep learning models: A study in ...", "url": "https://www.sciencedirect.com/science/article/pii/S1532046421002057", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046421002057", "snippet": "Based on clinical and instrumental parameters, physicians must be <b>able</b> to timely recognize a worsening of the disease and act consequently, modifying patients\u2019 treatment, in order to avoid an adverse outcome. This evaluation could be supported by an algorithm that integrate vast amount of information more efficiently and yield more precise predictions. The way has been paved by the recent awareness of the potential enclosed in large amounts of raw data and the consequent effort to make ...", "dateLastCrawled": "2021-12-28T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Interpretability in ML: A Broad</b> Overview - LessWrong 2.0 viewer", "url": "https://www.greaterwrong.com/posts/57fTWCpsAyjeAimTp/interpretability-in-ml-a-broad-overview-2", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/57fTWCpsAyjeAimTp/<b>interpretability-in-ml-a-broad</b>...", "snippet": "This has already led to major criticism of proprietary recidivism predictors <b>like</b> COMPAS. Approaches to <b>interpretability</b> which focus on decomposing the model into sub-models or explicate a chain of reasoning could help with such appeals. Defining <b>Interpretability</b>. Lipton\u2019s paper breaks <b>interpretability</b> down into two types, transparency and post-hoc. Transparency <b>Interpretability</b>. These three questions are from Lipton\u2019s section on transparency as <b>interpretability</b>, where he features on ...", "dateLastCrawled": "2021-12-27T15:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Against <b>Interpretability</b>: a Critical Examination of the ...", "url": "https://link.springer.com/article/10.1007/s13347-019-00372-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13347-019-00372-9", "snippet": "One <b>interpretability</b>-<b>like</b> word is defined in terms of another <b>interpretability</b>-<b>like</b> word. These definitions move the bump under the rug. This need not be a problem for these definitions in all possible scenarios. It is not necessarily required that all definitions be reductive, or that they omit any reference to terms cognate with the term to be defined. Furthermore, when there is tacit agreement about what would constitute an interpretation or explanation in some particular context (e.g., a ...", "dateLastCrawled": "2021-12-28T15:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>ML Interpretability: LIME and SHAP in</b> prose and code - <b>Cloudera Blog</b>", "url": "https://blog.cloudera.com/ml-interpretability-lime-and-shap-in-prose-and-code/", "isFamilyFriendly": true, "displayUrl": "https://blog.cloudera.com/<b>ml-interpretability-lime-and-shap-in</b>-prose-and-code", "snippet": "At Cloudera Fast Forward, we see model <b>interpretability</b> as an important step in the data science workflow.<b>Being</b> <b>able</b> to explain how a model works serves many purposes, including building trust in the model\u2019s output, satisfying regulatory requirements, model debugging, and verifying model safety, amongst other things.", "dateLastCrawled": "2022-01-31T21:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretability vs Explainability: The Black</b> Box of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-<b>interpretability</b>-vs-explainability", "snippet": "High model <b>interpretability</b> wins arguments. Risk and responsibility. A model with high <b>interpretability</b> is desirable on a high-risk stakes game. High interpretable models equate <b>to being</b> <b>able</b> to hold another party liable. And when models are predicting whether a person has cancer, people need to be held accountable for the decision that was ...", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "6 \u2013 <b>Interpretability</b> \u2013 Machine Learning Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "As such, we believe that the true benefit of an <b>interpretability</b> framework is <b>being</b> <b>able</b> to formalize these different aspects of <b>interpretability</b>, then pick the most relevant method. The claim \u201cyou should use SHAP because it makes your model more interpretable\u201d is largely useless. However, the claim \u201cyou should use SHAP because the decomposability it offers is worth the risk of using a post-hoc explanation\u201d makes clear both the benefits and risks of SHAP.", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interpretability</b> of time-series deep learning models: A study in ...", "url": "https://www.sciencedirect.com/science/article/pii/S1532046421002057", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046421002057", "snippet": "<b>Interpretability</b> is fundamental in healthcare problems and the lack of it in deep learning models is currently the major barrier in the usage of such powerful algorithms in the field. The study describes the implementation of an attention layer for Long Short-Term Memory (LSTM) neural network that provides a useful picture on the influence of the several input variables included in the model. A cohort of 10,616 patients with cardiovascular diseases is selected from the MIMIC III dataset, an ...", "dateLastCrawled": "2021-12-28T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Towards the <b>Interpretability</b> of Machine Learning Predictions for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8122817/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8122817", "snippet": "Tree-based methods try to mimic the thinking of an expert clinician looking at a set of images of a new patient, identify a <b>similar</b> past patient with the most <b>similar</b> images and <b>map</b> the dose distribution administered to the former patient in order to assess the optimal treatment to be applied with the new patient. To do this, a collection of features is extracted from the images to build a dataset of structured data that can be handled by most ML algorithms. This approach reached 78.68% and ...", "dateLastCrawled": "2022-01-27T15:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interpretability</b> and Analysis of Models for NLP @ ACL 2020 | by Carolin ...", "url": "https://medium.com/@lawrence.carolin/interpretability-and-analysis-of-models-for-nlp-e6b977ac1dc6", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@lawrence.carolin/<b>interpretability</b>-and-analysis-of-models-for-nlp-e...", "snippet": "The first category is about the paper that in my personal opinion is a must-<b>read</b> for everyone planning to research <b>interpretability</b>: it asks crucial questions which we should all consider when ...", "dateLastCrawled": "2022-01-23T11:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Interpretability</b> in the medical field: A systematic mapping and review ...", "url": "https://www.sciencedirect.com/science/article/pii/S1568494621011522", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1568494621011522", "snippet": "Methods: This review was carried out according to the well-known systematic <b>map</b> and review process to analyze the literature on <b>interpretability</b> techniques when applied in the medical field with regard to different aspects: publication venues and publication year, contribution and empirical types, medical and ML disciplines and objectives, ML black-box techniques interpreted, <b>interpretability</b> techniques investigated, their performance and the best performing techniques, and lastly, the ...", "dateLastCrawled": "2022-01-23T08:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Interpretability in ML: A Broad</b> Overview - LessWrong 2.0 viewer", "url": "https://www.greaterwrong.com/posts/57fTWCpsAyjeAimTp/interpretability-in-ml-a-broad-overview-2", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/57fTWCpsAyjeAimTp/<b>interpretability-in-ml-a-broad</b>...", "snippet": "<b>Similar</b> to how humans often give post-hoc justifications for their actions, it could be informative to have models which can also give explanations, perhaps in text. Naive methods of pairing text with decisions, however, are likely going to optimize for something like \u201chow credible the explanation sounds to a human\u201d rather than \u201chow accurate the explanation is at summarizing the internal steps taken\u201d. While this seems clearly desirable, I think research in this area is hard to come ...", "dateLastCrawled": "2021-12-27T15:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Building Blocks of <b>Interpretability</b> - Distill", "url": "https://distill.pub/2018/building-blocks/", "isFamilyFriendly": true, "displayUrl": "https://distill.pub/2018/building-blocks", "snippet": "Activations now <b>map</b> to iconic representations, instead of abstract indices, with many appearing to be <b>similar</b> to salient human ideas, such as \u201cfloppy ear,\u201d \u201cdog snout,\u201d or \u201cfur.\u201d We use optimization-based feature visualization to avoid spurious correlation, but one could use other methods.", "dateLastCrawled": "2022-02-02T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Interpreting CNN Models</b> | by Sanjeev Suresh | Towards Data Science", "url": "https://towardsdatascience.com/interpreting-cnn-models-a11b1f720097", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpreting-cnn-models</b>-a11b1f720097", "snippet": "Nowadays machine learning models are <b>being</b> extensively used for aut o mation and decision making. Model <b>interpretability</b> is crucial in both debugging these models as well as building trust in the system. When we use traditional machine learning solutions like tree-based models there is extensive tooling available for model <b>interpretability</b>. We can visualize the individual trees, plot partial dependence plots to understand the effect of different features on the target variable, use ...", "dateLastCrawled": "2022-01-12T11:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Against <b>Interpretability</b>: a Critical Examination of the ...", "url": "https://link.springer.com/article/10.1007/s13347-019-00372-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13347-019-00372-9", "snippet": "The first section argued that the concepts of <b>interpretability</b> and cognates lack the kind of definition that would render them adequate to the kind of work that their proponents want them to do, and also suggested that <b>interpretability</b> may end up <b>being</b> of more limited use than is often thought. The second section argued that since <b>interpretability</b> is most often proposed as a means to further ends rather than an end in itself, it would be more perspicuous to organize discussion around the ...", "dateLastCrawled": "2021-12-28T15:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "6 \u2013 <b>Interpretability</b> \u2013 Machine Learning Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "These explanations help us and others determine whether a decision or <b>thought</b> was fair, logical, or well-<b>thought</b>-out. This form of post-hoc explanation applies to model <b>interpretability</b> as well. Post-hoc explanations are methods by which what models have learned <b>can</b> be visualized. Common examples are: Text explanations: Similar to how humans justify decisions verbally. An explanation- or text-generating model is trained in tandem with the prediction model. Visualization: Qualitative ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interpretability vs Explainability: The Black</b> Box of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-<b>interpretability</b>-vs-explainability", "snippet": "High interpretable models equate to <b>being</b> <b>able</b> to hold another party liable. And when models are predicting whether a person has cancer, people need to be held accountable for the decision that was made. Highly interpretable models, and maintaining high <b>interpretability</b> as a design standard, <b>can</b> help build trust between engineers and users. It\u2019s bad enough when the chain of command prevents a person from <b>being</b> <b>able</b> to speak to the party responsible for making the decision. It is much worse ...", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interpretability</b> - AI Alignment Forum", "url": "https://www.alignmentforum.org/posts/CzZ6Fch4JSpwCpu6C/interpretability", "isFamilyFriendly": true, "displayUrl": "https://www.alignmentforum.org/posts/CzZ6Fch4JSpwCpu6C/<b>interpretability</b>", "snippet": "A major risk of <b>interpretability</b> research seems to be becoming untethered and describing things that don\u2019t really <b>map</b> to what\u2019s going on in the model. Ultimately, <b>being</b> <b>able</b> to describe other research in terms of circuits is a very helpful epistemic check, and getting practice working with circuits is helpful for working up to this.", "dateLastCrawled": "2021-12-13T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What everyone <b>needs to know about interpretability in machine learning</b> ...", "url": "https://dallascard.medium.com/what-everyone-needs-to-know-about-interpretability-in-machine-learning-d5ce16730407", "isFamilyFriendly": true, "displayUrl": "https://dallascard.medium.com/what-everyone-<b>needs-to-know-about-interpretability</b>-in...", "snippet": "To some extent choices <b>can</b> be partially justified (typically because one model worked better than another on some held-out data), but this is not the same as <b>being</b> <b>able</b> to claim that one has discovered the \u201ctrue\u201d model in any meaningful sense. Nor does accuracy necessarily tell the full story. Even models that obtain relatively high accuracy on held out data <b>can</b> still be", "dateLastCrawled": "2022-01-12T04:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Building Blocks of <b>Interpretability</b> - Distill", "url": "https://distill.pub/2018/building-blocks/", "isFamilyFriendly": true, "displayUrl": "https://distill.pub/2018/building-blocks", "snippet": "Activations now <b>map</b> to iconic representations, instead of abstract indices, with many appearing to be similar to salient human ideas, such as \u201cfloppy ear,\u201d \u201cdog snout,\u201d or \u201cfur.\u201d We use optimization-based feature visualization to avoid spurious correlation, but one could use other methods. Semantic dictionaries are powerful not just because they move away from meaningless indices, but because they express a neural network\u2019s learned abstractions with canonical examples. With ...", "dateLastCrawled": "2022-02-02T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Interpretability in ML: A Broad</b> Overview - LessWrong 2.0 viewer", "url": "https://www.greaterwrong.com/posts/57fTWCpsAyjeAimTp/interpretability-in-ml-a-broad-overview-2", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/57fTWCpsAyjeAimTp/<b>interpretability-in-ml-a-broad</b>...", "snippet": "In the safety context, we <b>can</b> probably less concerned with <b>interpretability</b> that is useful for laypeople, and focus on <b>interpretability</b> that is useful for the people doing the technical work. To that end, I think that \u201cjust\u201d <b>being</b> careful about what the <b>interpretability</b> analysis means <b>can</b> help, like how good statisticians <b>can</b> avoid misuse of statistical testing, even though many practitioners get it wrong.", "dateLastCrawled": "2021-12-27T15:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Against <b>Interpretability</b>: a Critical Examination of the ...", "url": "https://link.springer.com/article/10.1007/s13347-019-00372-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13347-019-00372-9", "snippet": "<b>Interpretability</b> is often <b>thought</b> to play an important role in justification in an ML context. It <b>can</b> seem outright irresponsible to believe algorithmic outputs regarding unseen real-world data in the absence of detailed knowledge of the algorithm\u2019s inner workings. However, in some contexts, there are ways of achieving the desired assurance in the absence of knowledge about inner workings of tools. People were both responsible and justified in relying on the deliverances of their eyes ...", "dateLastCrawled": "2021-12-28T15:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Visual <b>Interpretability</b> for <b>Convolutional</b> Neural Networks | by Himanshu ...", "url": "https://towardsdatascience.com/visual-interpretability-for-convolutional-neural-networks-2453856210ce", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/visual-<b>interpretability</b>-for-<b>convolutional</b>-neural...", "snippet": "A deconvnet <b>can</b> <b>be thought</b> of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features, deconvnets projects the feature activations (convolution outputs) back to the input pixel space. A deconvnet layer (left) attached to a convnet layer (right). [1] To visualize a convnet, a deconvnet is attached to each of its layers, providing a continuous path back to image pixels. To start, an input image is presented to the convnet ...", "dateLastCrawled": "2022-01-30T19:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Interpretability</b> 2020", "url": "https://ff06-2020.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://ff06-2020.fastforwardlabs.com", "snippet": "Of all the platforms evaluated for this report, H2O\u2019s developers have <b>thought</b> the most extensively about <b>interpretability</b> and how to best explain complex, nonlinear relationships between inputs (i.e., features) and outputs (i.e., labels). In addition, they are actively working on novel solutions to aid data exploration and feature engineering, including visualization tools to help humans, who are adapted to perceive a three-dimensional world, understand relationships in higher-dimensional ...", "dateLastCrawled": "2022-01-16T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Interpretability</b>, validity, and the minimum important difference ...", "url": "https://link.springer.com/article/10.1007/s11017-011-9186-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11017-011-9186-9", "snippet": "Patient-reported outcomes are increasingly used as dependent variables in studies regarding the effectiveness of clinical interventions. But patient-reported outcome measures (PROMs) do not provide intuitively meaningful data. For instance, it is not clear what a five point increase or decrease on a particular scale signifies. Establishing \u2018<b>interpretability</b>\u2019 involves making changes in outcomes meaningful. Attempts to interpret PROMs have led to the development of methods for identifying ...", "dateLastCrawled": "2022-01-19T05:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretability vs Explainability: The Black</b> Box of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-<b>interpretability</b>-vs-explainability", "snippet": "High interpretable models equate <b>to being</b> <b>able</b> to hold another party liable. And when models are predicting whether a person has cancer, people need to be held accountable for the decision that was made. Highly interpretable models, and maintaining high <b>interpretability</b> as a design standard, <b>can</b> help build trust between engineers and users. It\u2019s bad enough when the chain of command prevents a person from <b>being</b> <b>able</b> to speak to the party responsible for making the decision. It is much worse ...", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "6 \u2013 <b>Interpretability</b> \u2013 Machine Learning Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "<b>Interpretability</b> takes many forms and <b>can</b> be difficult to define; we first explore general frameworks and sets of definitions in which model <b>interpretability</b> <b>can</b> be evaluated and <b>compared</b> (Lipton 2016, Doshi-Velez &amp; Kim 2017). Next, we analyze several well-known examples of <b>interpretability</b> methods\u2013LIME (Ribeiro et al. 2016), SHAP (Lundberg &amp; Lee 2017), and convolutional neural network visualization (Olah et al. 2018)\u2013in the context of this framework. Model <b>interpretability</b> has no ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interpretability</b> in the medical field: A systematic mapping and review ...", "url": "https://www.sciencedirect.com/science/article/pii/S1568494621011522", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1568494621011522", "snippet": "<b>Interpretability</b> <b>can</b> be defined as the degree to which a human <b>can</b> understand the cause of a decision ... which <b>can</b> be explained by the fact that the results <b>can</b> <b>be compared</b> with other techniques evaluated on the same datasets and the availability of public datasets in the medical domain. Additionally, it is preferred to evaluate and interpret ML models on historical data before using real-time evaluation. 23% of the qualified studies were empirically evaluated using case study methods that ...", "dateLastCrawled": "2022-01-23T08:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Towards the <b>Interpretability</b> of Machine Learning Predictions for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8122817/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8122817", "snippet": "In this sense, learning tools are becoming a commodity but, to be <b>able</b> to assist doctors on a daily basis, it is essential to fully understand how models <b>can</b> be interpreted. In this survey, we analyse current machine learning models and other in-silico tools as applied to medicine\u2014specifically, to cancer research\u2014and we discuss their <b>interpretability</b>, performance and the input data they are fed with. Artificial neural networks (ANN), logistic regression (LR) and support vector machines ...", "dateLastCrawled": "2022-01-27T15:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Explainable AI: A Review of Machine Learning <b>Interpretability</b> Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "Taxonomy mind-<b>map</b> of Machine Learning <b>Interpretability</b> Techniques. This taxonomy focuses on the purpose that these methods were created to serve and the ways through which they accomplish this purpose. As a result, according to the presented taxonomy, four major categories for <b>interpretability</b> methods are identified: methods for explaining complex black-box models, methods for creating white-box models, methods that promote fairness and restrict the existence of discrimination, and, lastly ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Interpretability</b> of time-series deep learning models: A study in ...", "url": "https://www.sciencedirect.com/science/article/pii/S1532046421002057", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046421002057", "snippet": "The second reason is linked to the main purpose of this study: having restricted our analysis on a group of quite homogeneous patients, we <b>can</b> more easily compare the finding of our <b>interpretability</b> investigation (in term of impact of input features) with the clinical knowledge in disease managing. Our research group <b>can</b> count on a long-standing collaboration with expert cardiologists. In line with the setting of standard prognostic scoring systems, that are typically measured in the first ...", "dateLastCrawled": "2021-12-28T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Visual <b>Interpretability</b> for <b>Convolutional</b> Neural Networks | by Himanshu ...", "url": "https://towardsdatascience.com/visual-interpretability-for-convolutional-neural-networks-2453856210ce", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/visual-<b>interpretability</b>-for-<b>convolutional</b>-neural...", "snippet": "The general category of techniques is called \u201cClass Activation <b>Map</b>\u201d (CAM) visualization and consists in producing heatmaps of \u201cclass activation\u201d over input images. A \u201cclass activation\u201d heatmap is a 2D grid of scores associated with a specific output class, computed for every location in an input image, indicating how important each location is with respect to the class considered. This helps us in debugging the decision process of a <b>convolutional</b> neural network. We <b>can</b> also use ...", "dateLastCrawled": "2022-01-30T19:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Explain Your Model with the <b>SHAP</b> Values | by Dr. Dataman | Towards Data ...", "url": "https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/explain-your-model-with-the-<b>shap</b>-values-bc36aac4de3d", "snippet": "Inspired by several methods (1,2,3,4,5,6,7) on model <b>interpretability</b>, Lundberg and Lee (2016) proposed the <b>SHAP</b> value as a united approach to explaining the output of any machine learning model. Three benefits worth mentioning here. The first one is global <b>interpretability</b> \u2014 the collective <b>SHAP</b> values <b>can</b> show how much each predictor contributes, either positively or negatively, to the target variable. This is like the variable importance plot but it is <b>able</b> to show the positive or ...", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>ML Interpretability: LIME and SHAP in</b> prose and code - <b>Cloudera Blog</b>", "url": "https://blog.cloudera.com/ml-interpretability-lime-and-shap-in-prose-and-code/", "isFamilyFriendly": true, "displayUrl": "https://blog.cloudera.com/<b>ml-interpretability-lime-and-shap-in</b>-prose-and-code", "snippet": "<b>Being</b> <b>able</b> to explain how a model works serves many purposes, including building trust in the model\u2019s output, satisfying regulatory requirements, model debugging, and verifying model safety, amongst other things. We have written a research report (which you <b>can</b> access here) that discusses this topic in detail.", "dateLastCrawled": "2022-01-31T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Local computational methods to improve the interpretability</b> and ...", "url": "https://www.nature.com/articles/s41467-021-21509-5", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-021-21509-5", "snippet": "<b>A Map</b> obtained by LocSpiral approach (left) <b>compared</b> with the <b>map</b> as deposited in EMDB for EMD-21375. B B -factor maps to be used for sharpening (slope of the local Guinier plot multiplied by 4 ...", "dateLastCrawled": "2022-01-26T08:54:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretability</b> in <b>Machine</b> <b>Learning</b>: An Overview", "url": "https://thegradient.pub/interpretability-in-ml-a-broad-overview/", "isFamilyFriendly": true, "displayUrl": "https://thegradient.pub/<b>interpretability</b>-in-ml-a-broad-overview", "snippet": "First, <b>interpretability</b> in <b>machine</b> <b>learning</b> is useful because it can aid in trust. As humans, we may be reluctant to rely on <b>machine</b> <b>learning</b> models for certain critical tasks, e.g., medical diagnosis, unless we know &quot;how they work.&quot; There&#39;s often a fear of the unknown when trusting in something opaque, which we see when people confront new technology, and this can slow down adoption. Approaches to <b>interpretability</b> that focus on transparency could help mitigate some of these fears.", "dateLastCrawled": "2022-02-01T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interpretability</b> in <b>Machine</b> <b>Learning</b>", "url": "https://www.cl.cam.ac.uk/teaching/1819/P230/IWML-Lecture-4.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cl.cam.ac.uk/teaching/1819/P230/IWML-Lecture-4.pdf", "snippet": "This provides a novel <b>analogy</b> between data compression and regularization. Qualitative and quantitative state-of-the-art results on three datasets. 20 / 33. Interpretable Lens Variable Model (ILVM) 21 / 33. Interactive <b>Interpretability</b> via Active <b>Learning</b> Interactive \u2018human-in-the-loop\u2019 <b>interpretability</b> Choose the point with index j that maximizes : ^j = argmax jI(s ; ) = H(s ) E q \u02da(z js)[H(s jz j)] = Z p(s j)log p(s j)ds + E q \u02da(z js) Z p (s jjz)log p (s jjz)ds : (5) Choose the point ...", "dateLastCrawled": "2022-01-19T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "6 \u2013 <b>Interpretability</b> \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "Figure 1: <b>Interpretability</b> for <b>machine</b> <b>learning</b> models bridges the concrete objectives models optimize for and the real-world (and less easy to define) desiderata that ML applications aim to achieve. Introduction . The objectives <b>machine</b> <b>learning</b> models optimize for do not always reflect the actual desiderata of the task at hand. <b>Interpretability</b> in models allows us to evaluate their decisions and obtain information that the objective alone cannot confer. <b>Interpretability</b> takes many forms ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Towards <b>Analogy</b>-Based Explanations in <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/chapter/10.1007/978-3-030-57524-3_17", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-57524-3_17", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-16T03:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Economic Methodology Meets Interpretable Machine Learning</b> - Part I ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes", "snippet": "In this series of posts, we will develop an <b>analogy</b> between the realistic assumptions debate in economic methodology and the current discussion over <b>interpretability</b> when using <b>machine</b> <b>learning</b> models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for <b>machine</b> <b>learning</b> theorists and practitioners to avoid analysis paralysis and make progress on the <b>interpretability</b> issue - one way or the other. Intro - Part ...", "dateLastCrawled": "2022-01-22T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Towards <b>Analogy</b>-Based Explanations in <b>Machine</b> <b>Learning</b>", "url": "https://arxiv.org/abs/2005.12800", "isFamilyFriendly": true, "displayUrl": "https://<b>arxiv</b>.org/abs/2005.12800", "snippet": "Principles of analogical reasoning have recently been applied in the context of <b>machine</b> <b>learning</b>, for example to develop new methods for classification and preference <b>learning</b>. In this paper, we argue that, while analogical reasoning is certainly useful for constructing new <b>learning</b> algorithms with high predictive accuracy, is is arguably not less interesting from an <b>interpretability</b> and explainability point of view. More specifically, we take the view that an <b>analogy</b>-based approach is a ...", "dateLastCrawled": "2021-10-24T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Economic Methodology Meets Interpretable <b>Machine</b> <b>Learning</b> ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_intro/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_intro", "snippet": "In this series of posts, we will develop an <b>analogy</b> between the realistic assumptions debate in economic methodology and the current discussion over <b>interpretability</b> when using <b>machine</b> <b>learning</b> models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for <b>machine</b> <b>learning</b> theorists and practitioners to avoid analysis paralysis and make progress on the <b>interpretability</b> issue - one way or the other. But first ...", "dateLastCrawled": "2022-01-05T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>SHAP</b>: A reliable way to analyze model <b>interpretability</b> | by Sharayu ...", "url": "https://towardsdatascience.com/shap-a-reliable-way-to-analyze-your-model-interpretability-874294d30af6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>shap</b>-a-reliable-way-to-analyze-your-model...", "snippet": "The balance: Accuracy vs. <b>Interpretability</b>. 2. How to interpret <b>machine</b> <b>learning</b> models? 3. LIME: Explaining predictions of <b>machine</b> <b>learning</b> models. In this blog, I wil l be talking about one of the most popular model agnostic technique that is used to explain predictions. <b>SHAP</b> stands for SHapley Additive exPlanations. Shapely values are obtained by incorporating concepts from Cooperative Game Theory and local explanations. Given a set of palyers, Cooperative Game Theory defines how well and ...", "dateLastCrawled": "2022-01-31T17:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Analogies between Biology and Deep <b>Learning</b> [rough note] -- colah&#39;s blog", "url": "http://colah.github.io/notes/bio-analogies/", "isFamilyFriendly": true, "displayUrl": "colah.github.io/notes/bio-analogies", "snippet": "Neuroscience \u2194 <b>Interpretability</b>. <b>Analogy</b>: model=brain. Artificial neural networks are historically inspired by neuroscience, but I used to be pretty skeptical that the connection was anything more than superficial. I&#39;ve since come around: I now think this is a very deep connection. The thing that personally persuaded me was that, in my own investigations of what goes on inside neural networks, we kept finding things that were previously discovered by neuroscientists. The most recent ...", "dateLastCrawled": "2022-01-30T16:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Chris Olah on what the hell is going on inside neural networks - 80,000 ...", "url": "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/", "isFamilyFriendly": true, "displayUrl": "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research", "snippet": "Chris is a <b>machine</b> <b>learning</b> researcher currently focused on neural network interpretability. Until last December he led OpenAI\u2019s interpretability team but along with some colleagues he recently moved on to help start a new AI lab focussed on large models and safety called Anthropic. Rob Wiblin: Before OpenAI he spent 4 years at Google Brain developing tools to visualize what\u2019s going on in neural networks. Chris was hugely impactful at Google Brain. He was second author on the launch of ...", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Marketing AI: Interpretability and Explainability</b> - Christopher S. Penn ...", "url": "https://www.christopherspenn.com/2021/03/marketing-ai-interpretability-and-explainability/", "isFamilyFriendly": true, "displayUrl": "https://www.christopherspenn.com/2021/03/<b>marketing-ai-interpretability-and-explainability</b>", "snippet": "<b>Interpretability is like</b> inspecting the baker\u2019s recipe for the cake. We look at the list of ingredients and the steps taken to bake the cake, and we verify that the recipe makes sense and the ingredients were good. This is a much more rigorous way of validating our results, but it\u2019s the most complete \u2013 and if we\u2019re in a high-stakes situation where we need to remove all doubt, this is the approach we take. Interpretability in AI is like that \u2013 we step through the code itself that ...", "dateLastCrawled": "2022-01-29T12:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Causal <b>Learning</b> From Predictive Modeling for Observational Data", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7931928/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7931928", "snippet": "Given the recent success of <b>machine</b> <b>learning</b>, specifically deep <b>learning</b>, in several applications (Goodfellow et al., ... This statistical <b>interpretability is similar</b> in spirit to traditional interpretability. This allows to answer questions, such as \u201cdoes BMI influence susceptibility to Covid?\u201d Moreover, it has been argued that developing an effective CBN for practical applications requires expert knowledge when data collection is cumbersome (Fenton and Neil, 2012). This applies to ...", "dateLastCrawled": "2021-12-09T23:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Optimal <b>Predictive Clustering</b> - Dimitris Bertsimas", "url": "https://dbertsim.mit.edu/pdfs/papers/2020-sobiesk-optimal-predictive-clustering.pdf", "isFamilyFriendly": true, "displayUrl": "https://dbertsim.mit.edu/pdfs/papers/2020-sobiesk-optimal-<b>predictive-clustering</b>.pdf", "snippet": "Table 1 Comparison of major <b>machine</b> <b>learning</b> methods relative to each other across the metrics of performance (out-of-sample R2), scalability and interpretability. 1 is the best, while 5 is the worst. Optimal <b>Predictive Clustering</b> 3 From Table 1, we observe all existing methods have weakness in at least one category. We therefore seek to design a method that has strong performance in all three categories at the same time. Optimal <b>Predictive Clustering</b> (OPC) is an algorithm that uses mixed ...", "dateLastCrawled": "2021-11-24T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reviewing Challenges of Predicting Protein Melting Temperature Change ...", "url": "https://link.springer.com/article/10.1007/s12033-021-00349-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12033-021-00349-0", "snippet": "Predicting the effects of mutations on protein stability is a key problem in fundamental and applied biology, still unsolved even for the relatively simple case of small, soluble, globular, monomeric, two-state-folder proteins. Many articles discuss the limitations of prediction methods and of the datasets used to train them, which result in low reliability for actual applications despite globally capturing trends. Here, we review these and other issues by analyzing one of the most detailed ...", "dateLastCrawled": "2022-02-03T02:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretable Machine Learning: Advantages and Disadvantages</b> | by ...", "url": "https://towardsdatascience.com/interpretable-machine-learning-advantages-and-disadvantages-901769f48c43", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpretable-machine-learning-advantages-and</b>...", "snippet": "In my view, a shor t coming of interpretable <b>machine</b> <b>learning</b> is that it assumes to a degree that the data being fed into the model is always going to be suitable for human interpretation. This is not necessarily the case. For instance, let\u2019s say that a company is trying to implement interpretable <b>machine</b> <b>learning</b> to devise a credit scoring model, whereby prospective credit card applications are classified as approved or rejected based on numerous features. It is often the case that such ...", "dateLastCrawled": "2022-01-19T03:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Breaking the interpretability barrier - a</b> method for interpreting deep ...", "url": "http://www.di.uniba.it/~loglisci/NFMCP2019/NFMCP/nfMCP2019_paper_17.pdf", "isFamilyFriendly": true, "displayUrl": "www.di.uniba.it/~loglisci/NFMCP2019/NFMCP/nfMCP2019_paper_17.pdf", "snippet": "Last, but not least, <b>interpretability can be thought of as</b> a useful tool for understanding and correcting model errors. In general, we are faced with a trade-o between performance and inter-pretability. Graph classi cation is normally a domain which requires the ap-plication of complex <b>learning</b> models, such as deep neural networks, which are not interpretable by nature. Several relevant attempts have been made to in-terpret complex models post-hoc (brie y reviewed in section2). However, most ...", "dateLastCrawled": "2021-09-22T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Discovering Discriminative Nodes for Classi\ufb01cation with Deep Graph ...", "url": "http://muresanlab.tins.ro/publications/preprints/Palcu_et_al_LNAI_2020.pdf", "isFamilyFriendly": true, "displayUrl": "muresanlab.tins.ro/publications/preprints/Palcu_et_al_LNAI_2020.pdf", "snippet": "Last, but not least, <b>interpretability can be thought of as</b> a useful tool for understanding and correcting model errors. In general, we are faced with a trade-o\ufb00 between performance and inter-pretability. Graph classi\ufb01cation is normally a domain which requires the appli-cation of complex <b>learning</b> models, such as deep neural networks, which are not interpretable by nature. Several relevant attempts have been made to interpret complex models post-hoc (brie\ufb02y reviewed in Sect.2). However ...", "dateLastCrawled": "2021-09-02T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Confronting Abusive Language Online: A Survey from the Ethical and ...", "url": "https://www.arxiv-vanity.com/papers/2012.12305/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2012.12305", "snippet": "The pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. Significant effort in Natural Language Processing (NLP) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. Although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this ...", "dateLastCrawled": "2021-10-13T19:21:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(interpretability)  is like +(being able to read a map)", "+(interpretability) is similar to +(being able to read a map)", "+(interpretability) can be thought of as +(being able to read a map)", "+(interpretability) can be compared to +(being able to read a map)", "machine learning +(interpretability AND analogy)", "machine learning +(\"interpretability is like\")", "machine learning +(\"interpretability is similar\")", "machine learning +(\"just as interpretability\")", "machine learning +(\"interpretability can be thought of as\")", "machine learning +(\"interpretability can be compared to\")"]}