{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A review on the attention mechanism of <b>deep learning</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "snippet": "applied the <b>self-attention</b> mechanism to the computer vision task to solve this problem, <b>called</b> non-local attention, as shown in Fig. 13. They proposed the non-local module that got attention masks by calculating the correlation matrix between each spatial point in the feature map, then the attention guided dense contextual information to aggregate. However, this method <b>also</b> has the following problems: 1) Only the positional attention module is involved, not the commonly used channel ...", "dateLastCrawled": "2022-02-02T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How Transformers Work. Transformers are a type of neural\u2026 | by Giuliano ...", "url": "https://towardsdatascience.com/transformers-141e32e69591", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-141e32e69591", "snippet": "The encoder\u2019s inputs first flow through a <b>self-attention</b> <b>layer</b>. It helps the encoder look at other words in the input sentence as it encodes a <b>specific</b> word. The decoder has both those layers, but between them is an attention <b>layer</b> that helps the decoder focus on relevant <b>parts</b> of the input sentence.", "dateLastCrawled": "2022-02-02T11:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Review on the Attention Mechanism of Deep Learning</b>", "url": "https://www.researchgate.net/publication/350565955_A_Review_on_the_Attention_Mechanism_of_Deep_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350565955_A_<b>Review_on_the_Attention_Mechanism</b>...", "snippet": "The transformer model which has introduced the <b>self-attention</b> mechanism is a classic case. Inspired by NLP, models in the field of CV have <b>also</b> continuously introduced the attention mechanism [10 ...", "dateLastCrawled": "2022-01-30T19:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Attention, please! A <b>survey of Neural Attention Models in Deep Learning</b> ...", "url": "https://deepai.org/publication/attention-please-a-survey-of-neural-attention-models-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/attention-please-a-<b>survey-of-neural-attention-models</b>-in...", "snippet": "These architectures and all that use <b>self-attention</b> belong to a new category of neural networks, <b>called</b> Self-Attentive Neural Networks. They aim to explore <b>self-attention</b> in various tasks and improve the following drawbacks: 1) a Large number of parameters and training iterations to converge; 2) High memory cost per <b>layer</b> and quadratic growth of memory according to sequence length; 3) Auto-regressive model; 4) Low parallelization in the decoder layers. Specifically, Weighted Transformer", "dateLastCrawled": "2022-01-21T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Frontiers | <b>Attention in Psychology, Neuroscience, and Machine Learning</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029", "snippet": "In this way, <b>self-attention</b> is more about creating a good encoding than performing a task-<b>specific</b> attention-<b>like</b> selection based on limited resources. In the context of a temporal task, its closest analogue in psychology may be priming because priming alters the encoding of subsequent stimuli based on those that came before. It is of course not the direct goal of machine learning engineers to replicate the brain, but rather to create networks that can be easily trained to perform well on ...", "dateLastCrawled": "2022-02-03T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Survey on Visual <b>Transformer</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2012.12556/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2012.12556", "snippet": "The <b>self-attention</b> <b>layer</b> is further improved by adding a mechanism <b>called</b> multi-head attention in order to boost the performance of the vanilla <b>self-attention</b> <b>layer</b>. Note that for a given reference word, we often want to focus on several other words when going through the sentence. Thus, a single-head <b>self-attention</b> <b>layer</b> limits the ability of <b>focusing</b> on a <b>specific</b> position (or several <b>specific</b> positions) while does not influence the attention on other positions that is equally important at ...", "dateLastCrawled": "2021-12-16T05:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Graph neural networks: A review of methods and applications</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000012", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000012", "snippet": "There have <b>also</b> been several surveys <b>focusing</b> on some <b>specific</b> graph learning fields. Sun et al. (2018) and Chen et al. ... following a <b>self-attention</b> strategy. The hidden state of node v can be obtained by: (12) h v t + 1 = \u03c1 (\u2211 u \u2208 N v \u03b1 v u W h u t), \u03b1 v u = exp (LeakyReLU (a T [W h v \u2225 W h u])) \u2211 k \u2208 N v exp (LeakyReLU (a T [W h v \u2225 W h k])), where W is the weight matrix associated with the linear transformation which is applied to each node, and a is the weight vector of ...", "dateLastCrawled": "2022-02-02T10:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Attention in Multi-Modal Machine Learning Problems | by Sumanth S Rao ...", "url": "https://sraosumanth.medium.com/attention-in-multi-modal-machine-learning-problems-67cd0aeb3964", "isFamilyFriendly": true, "displayUrl": "https://sraosumanth.medium.com/attention-in-multi-modal-machine-learning-problems-67cd...", "snippet": "This was observed to happen on investigating into incorporating this deep learning technique <b>called</b> <b>self-attention</b> while generating the feature vectors. Image and text representation models . The baseline of the model consists of the dataset of images and captions. We have applied the image and caption feature extraction models on top of the data to obtain the corresponding image representation and word embedding. Fig 2. High-level architecture of the solution. Image Representation Model. a ...", "dateLastCrawled": "2021-11-24T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "CVPR 2021: An Overview | Yassine", "url": "https://yassouali.github.io/ml-blog/cvpr2021/", "isFamilyFriendly": true, "displayUrl": "https://yassouali.github.io/ml-blog/cvpr2021", "snippet": "Thus, <b>self-attention</b> based models <b>like</b> SASA use a local form of <b>self-attention</b> that aggregates the information around each pixel similar to convolutions. However, to do this, we need to extract local 2D grids around each pixel, and such an operation can be quite expensive both computationally and memory wise, since for each pixel, we need to fetch \\(k^2\\) pixels, and this operation contains a lot of duplicates since two neighboring pixel share most of their neighbors (\\(k \\times (k-1)\\) out ...", "dateLastCrawled": "2022-01-31T06:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "NeurIPS 2021 papers", "url": "https://tanelp.github.io/neurips2021/", "isFamilyFriendly": true, "displayUrl": "https://tanelp.github.io/neurips2021", "snippet": "Our method relies on learned object-centric representations, <b>self-attention</b> and self-supervised dynamics learning, and all three elements together are required for strong performance to emerge. The success of this combination suggests that there may be no need to trade off flexibility for performance on problems involving spatio-temporal or causal-style reasoning. With the right soft biases and learning objectives in a neural network we may be able to attain the best of both worlds.", "dateLastCrawled": "2022-02-03T00:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A review on the attention mechanism of <b>deep learning</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "snippet": "applied the <b>self-attention</b> mechanism to the computer vision task to solve this problem, <b>called</b> non-local attention, as shown in Fig. 13. They proposed the non-local module that got attention masks by calculating the correlation matrix between each spatial point in the feature map, then the attention guided dense contextual information to aggregate. However, this method <b>also</b> has the following problems: 1) Only the positional attention module is involved, not the commonly used channel ...", "dateLastCrawled": "2022-02-02T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How Transformers Work. Transformers are a type of neural\u2026 | by Giuliano ...", "url": "https://towardsdatascience.com/transformers-141e32e69591", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-141e32e69591", "snippet": "The green step is <b>called</b> the encoding stage and the purple step is the ... Decoders share the same property, i.e. they are <b>also</b> very <b>similar</b> to each other. Each encoder consists of two layers: <b>Self-attention</b> and a feed Forward Neural Network. Image from 4. The encoder\u2019s inputs first flow through a <b>self-attention</b> <b>layer</b>. It helps the encoder look at other words in the input sentence as it encodes a <b>specific</b> word. The decoder has both those layers, but between them is an attention <b>layer</b> that ...", "dateLastCrawled": "2022-02-02T11:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Frontiers | <b>Attention in Psychology, Neuroscience, and Machine Learning</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029", "snippet": "In this way, <b>self-attention</b> is more about creating a good encoding than performing a task-<b>specific</b> attention-like selection based on limited resources. In the context of a temporal task, its closest analogue in psychology may be priming because priming alters the encoding of subsequent stimuli based on those that came before. It is of course not the direct goal of machine learning engineers to replicate the brain, but rather to create networks that can be easily trained to perform well on ...", "dateLastCrawled": "2022-02-03T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Survey on Visual <b>Transformer</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2012.12556/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2012.12556", "snippet": "The <b>self-attention</b> <b>layer</b> is further improved by adding a mechanism <b>called</b> multi-head attention in order to boost the performance of the vanilla <b>self-attention</b> <b>layer</b>. Note that for a given reference word, we often want to focus on several other words when going through the sentence. Thus, a single-head <b>self-attention</b> <b>layer</b> limits the ability of <b>focusing</b> on a <b>specific</b> position (or several <b>specific</b> positions) while does not influence the attention on other positions that is equally important at ...", "dateLastCrawled": "2021-12-16T05:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Review on the Attention Mechanism of Deep Learning</b>", "url": "https://www.researchgate.net/publication/350565955_A_Review_on_the_Attention_Mechanism_of_Deep_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350565955_A_<b>Review_on_the_Attention_Mechanism</b>...", "snippet": "Attention mechanism is a target feature enhancement method that is widely studied and applied at present. It can be used as a module of the deep learning model to focus attention on objects of ...", "dateLastCrawled": "2022-01-30T19:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "CVPR 2021: An Overview | Yassine", "url": "https://yassouali.github.io/ml-blog/cvpr2021/", "isFamilyFriendly": true, "displayUrl": "https://yassouali.github.io/ml-blog/cvpr2021", "snippet": "Thus, <b>self-attention</b> based models like SASA use a local form of <b>self-attention</b> that aggregates the information around each pixel <b>similar</b> to convolutions. However, to do this, we need to extract local 2D grids around each pixel, and such an operation can be quite expensive both computationally and memory wise, since for each pixel, we need to fetch \\(k^2\\) pixels, and this operation contains a lot of duplicates since two neighboring pixel share most of their neighbors (\\(k \\times (k-1)\\) out ...", "dateLastCrawled": "2022-01-31T06:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Graph neural networks: A review of methods and applications</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000012", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000012", "snippet": "There have <b>also</b> been several surveys <b>focusing</b> on some <b>specific</b> graph learning fields. Sun et al. (2018) and Chen et al. ... following a <b>self-attention</b> strategy. The hidden state of node v can be obtained by: (12) h v t + 1 = \u03c1 (\u2211 u \u2208 N v \u03b1 v u W h u t), \u03b1 v u = exp (LeakyReLU (a T [W h v \u2225 W h u])) \u2211 k \u2208 N v exp (LeakyReLU (a T [W h v \u2225 W h k])), where W is the weight matrix associated with the linear transformation which is applied to each node, and a is the weight vector of ...", "dateLastCrawled": "2022-02-02T10:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "NeurIPS 2021 papers", "url": "https://tanelp.github.io/neurips2021/", "isFamilyFriendly": true, "displayUrl": "https://tanelp.github.io/neurips2021", "snippet": "Our method relies on learned object-centric representations, <b>self-attention</b> and self-supervised dynamics learning, and all three elements together are required for strong performance to emerge. The success of this combination suggests that there may be no need to trade off flexibility for performance on problems involving spatio-temporal or causal-style reasoning. With the right soft biases and learning objectives in a neural network we may be able to attain the best of both worlds. 14 ...", "dateLastCrawled": "2022-02-03T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Document <b>Embedding</b> Techniques. A review of notable literature on the ...", "url": "https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/document-<b>embedding</b>-techniques-fed3e7a6a25d", "snippet": "<b>Similar</b> to paragraph vectors, Doc2VecC (an acronym of document vector through corruption) consists of an input <b>layer</b>, a projection <b>layer</b> and an output <b>layer</b> to predict the target word (\u201cceremony\u201d in the above example). The embeddings of neighboring words (e.g. \u201copening\u201d, \u201cfor\u201d, \u201cthe\u201d) provide local context while the vector representation of the entire document (shown in grey) serves as the global context. In contrast to", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Query2Label: A Simple Transformer Way to Multi-Label Classification ...", "url": "https://deepai.org/publication/query2label-a-simple-transformer-way-to-multi-label-classification", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/query2label-a-simple-transformer-way-to-multi-label...", "snippet": "Using cross attention for adaptively feature pooling through <b>focusing</b> on different <b>parts</b> (best view in colors). Motivated by the success of Transformer used in computer vision tasks [carion2020end, dosovitskiy2020image], we present in this paper a simple yet effective solution using Transformer decoder to query the existence of a class label. We show that, without bells and whistles, the proposed solution leads to new state-of-the-art results and establishes strong baselines for its simple ...", "dateLastCrawled": "2022-01-30T00:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention in Psychology, Neuroscience, and Machine Learning", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7177153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7177153", "snippet": "These weightings scale the word encodings themselves to create the next <b>layer</b> in the model, a process known as \u201c<b>self-attention</b>.\u201d This process repeats, and eventually interacts with the autoregressive decoder which <b>also</b> has attention mechanisms that allow it to flexibly focus on the encoded input (as in the standard form of attention) and on the previously generated output. The Transformer\u2014the name given to this new attention architecture\u2014outperformed many previous models and quickly ...", "dateLastCrawled": "2021-12-27T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Frontiers | <b>Attention in Psychology, Neuroscience, and Machine Learning</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029", "snippet": "These weightings scale the word encodings themselves to create the next <b>layer</b> in the model, a process known as \u201c<b>self-attention</b>.\u201d This process repeats, and eventually interacts with the autoregressive decoder which <b>also</b> has attention mechanisms that allow it to flexibly focus on the encoded input (as in the standard form of attention) and on the previously generated output. The Transformer\u2014the name given to this new attention architecture\u2014outperformed many previous models and quickly ...", "dateLastCrawled": "2022-02-03T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How Transformers Work. Transformers are a type of neural\u2026 | by Giuliano ...", "url": "https://towardsdatascience.com/transformers-141e32e69591", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-141e32e69591", "snippet": "The encoder\u2019s inputs first flow through a <b>self-attention</b> <b>layer</b>. It helps the encoder look at other words in the input sentence as it encodes a <b>specific</b> word. The decoder has both those layers, but between them is an attention <b>layer</b> that helps the decoder focus on relevant <b>parts</b> of the input sentence.", "dateLastCrawled": "2022-02-02T11:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Attention in Multi-Modal Machine Learning Problems | by Sumanth S Rao ...", "url": "https://sraosumanth.medium.com/attention-in-multi-modal-machine-learning-problems-67cd0aeb3964", "isFamilyFriendly": true, "displayUrl": "https://sraosumanth.medium.com/attention-in-multi-modal-machine-learning-problems-67cd...", "snippet": "Applying the same concept in our machine learning problems, We narrow down our image area by <b>focusing</b> only on the pixels corresponding to the object of interest and ignore the rest to produce a more meaningful output. This mechanism is <b>called</b> attention. Fig. 1. \u201cA woman is throwing a frisbee in a park.\u201d (Image source: Fig. 6(b) in Xu et al. 2015). The pixels focused during the prediction of each word is highlighted in white <b>Self Attention</b>. <b>Self-attention</b> is a relatively new concept that ...", "dateLastCrawled": "2021-11-24T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "IJGI | Free Full-Text | Dual Path Attention Net for Remote Sensing ...", "url": "https://www.mdpi.com/2220-9964/9/10/571/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2220-9964/9/10/571/htm", "snippet": "Another effective way of tackling the issues described above is to use <b>self-attention</b> mechanisms. These are popular and simple to adapt to semantic segmentation tasks because of their varied and flexible structure 17,18,19,20,21,22]. <b>Self-attention</b> mechanisms focus on local features by generating weight feature maps and fusing downstream feature maps. This may involve having one or more modules built upon a basic backbone, with each module <b>focusing</b> on things such as the channel or spatial ...", "dateLastCrawled": "2022-01-02T18:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Beyond Word Embedding: Key Ideas in Document Embedding</b> - KDnuggets", "url": "https://www.kdnuggets.com/2019/10/beyond-word-embedding-document-embedding.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2019/10/beyond-word-embedding-document-embedding.html", "snippet": "These word embedding techniques are sometimes <b>also</b> <b>called</b> ... The skip-<b>thought</b> encoder uses a word embedding <b>layer</b> that converts each word in the input sentence to its corresponding word embedding, effectively converting the input sentence into a sequence of word embeddings. This embedding <b>layer</b> is <b>also</b> shared with both of the decoders. Figure 11: In the skip-thoughts model, sentence s\u1d62 is encoded by the encoder; the two decoders condition on the hidden representation of the encoder\u2019s ...", "dateLastCrawled": "2022-01-06T22:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and ...", "url": "https://aclanthology.org/volumes/W18-54/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/W18-54", "snippet": "We assess the representations of the encoder by extracting dependency relations based on <b>self-attention</b> weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we <b>also</b> test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that <b>specific</b> attention heads mark syntactic dependency relations and we <b>can</b> <b>also</b> confirm that lower layers ...", "dateLastCrawled": "2022-01-31T13:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "NeurIPS 2021 papers", "url": "https://tanelp.github.io/neurips2021/", "isFamilyFriendly": true, "displayUrl": "https://tanelp.github.io/neurips2021", "snippet": "Our approach uses <b>self-attention</b> to reason about relationships between datapoints explicitly, which <b>can</b> be seen as realizing non-parametric models using parametric attention mechanisms. However, unlike conventional non-parametric models, we let the model learn end-to-end from the data how to make use of other datapoints for prediction. Empirically, our models solve cross-datapoint lookup and complex reasoning tasks unsolvable by traditional deep learning models. We show highly competitive ...", "dateLastCrawled": "2022-02-03T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is &#39;attention&#39; in the context of deep learning? - Quora", "url": "https://www.quora.com/What-is-attention-in-the-context-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-attention-in-the-context-of-deep-learning", "snippet": "Answer (1 of 5): In feed-forward deep networks, the entire input is presented to the network, which computes an output in one pass. In recurrent networks, new inputs <b>can</b> be presented at each time step, and the output of the previous time step <b>can</b> be used as an input to the network. This <b>can</b> be ...", "dateLastCrawled": "2022-01-15T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Generative adversarial network: An overview</b> of theory and applications ...", "url": "https://www.sciencedirect.com/science/article/pii/S2667096820300045", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2667096820300045", "snippet": "It <b>can</b> <b>also</b> be considered as a secondary field of ML algorithms inspired by the brain structure and functionality. In the applications of image identification, speech synthesis, text mining applications by receiving a distinct kind of data that hierarchical models <b>can</b> be built by representing probability distributions. Deep learning dependant on an end to end wireless communication system with conditional GANs using Deep Neural Networks (DNNs) do function of message passing like encoding ...", "dateLastCrawled": "2022-01-29T21:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention in Psychology, Neuroscience, and Machine Learning", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7177153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7177153", "snippet": "In this way, <b>self-attention</b> is more about creating a good encoding than performing a task-<b>specific</b> attention-like selection based on limited resources. In the context of a temporal task, its closest analogue in psychology may be priming because priming alters the encoding of subsequent stimuli based on those that came before. It is of course not the direct goal of machine learning engineers to replicate the brain, but rather to create networks that <b>can</b> be easily trained to perform well on ...", "dateLastCrawled": "2021-12-27T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Self-Attention-Based Deep Feature Fusion</b> for Remote Sensing <b>Scene</b> ...", "url": "https://www.researchgate.net/publication/339036826_Self-Attention-Based_Deep_Feature_Fusion_for_Remote_Sensing_Scene_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339036826_<b>Self-Attention</b>-Based_Deep_Feature...", "snippet": "<b>Remote sensing scene classification</b> aims to assign automatically each aerial image a <b>specific</b> sematic label. In this letter, we propose a new method, <b>called</b> <b>self-attention</b>-based deep feature ...", "dateLastCrawled": "2022-01-18T04:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Survey on Visual <b>Transformer</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2012.12556/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2012.12556", "snippet": "The <b>self-attention</b> <b>layer</b> is further improved by adding a mechanism <b>called</b> multi-head attention in order to boost the performance of the vanilla <b>self-attention</b> <b>layer</b>. Note that for a given reference word, we often want to focus on several other words when going through the sentence. Thus, a single-head <b>self-attention</b> <b>layer</b> limits the ability of <b>focusing</b> on a <b>specific</b> position (or several <b>specific</b> positions) while does not influence the attention on other positions that is equally important at ...", "dateLastCrawled": "2021-12-16T05:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A review on the attention mechanism of <b>deep learning</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "snippet": "In some cases, q <b>can</b> <b>also</b> be in the form of a matrix or two vectors according to <b>specific</b> tasks. Then the neural network computes the correlation between queries and keys through the score function f (<b>also</b> <b>called</b> energy function [61] or compatibility function [62] ) to obtain the energy score e that reflects the importance of queries with respect to keys in deciding the next output: (6) e = f ( q , K ) .", "dateLastCrawled": "2022-02-02T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Bidirectional LSTM with <b>self-attention</b> mechanism and multi-channel ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231220300254", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231220300254", "snippet": "The <b>self-attention</b> we used in this paper is a special case of <b>self-attention</b>. Different from the traditional attention mechanism, the <b>self-attention</b> mechanism <b>can</b> reduce the dependence on external information, it <b>can</b> directly calculate the dependence relationship without paying attention to the distance between words, learn the weight distribution of each word on the sentiment tendency of sentences, and focus on strengthening the sentiment features in the sentence, so that the model <b>can</b> ...", "dateLastCrawled": "2022-02-02T14:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "CVPR 2021: An Overview | Yassine", "url": "https://yassouali.github.io/ml-blog/cvpr2021/", "isFamilyFriendly": true, "displayUrl": "https://yassouali.github.io/ml-blog/cvpr2021", "snippet": "While <b>self-attention</b> models have recently been shown to provide notable improvements on accuracy-parameter trade-offs <b>compared</b> to baseline convolutional models such as ResNet-50, they are still not on par with the high-performing convolutional models such as EfficientNet . In order to close this gap, this paper proposes a new <b>self-attention</b> model family <b>called</b> HaloNets, which is based on a more efficient implementation of <b>self-attention</b>, that improves the speed, memory usage, and accuracy of ...", "dateLastCrawled": "2022-01-31T06:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A <b>self-attention</b> <b>layer</b> starts with a sequence of input representations, one for each word. The input representation for a word <b>can</b> be a simple embedding. For each word in an input sequence, the network scores the relevance of the word to every element in the whole sequence of words. The relevance scores determine how much the word&#39;s final representation incorporates the representations of other words.", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A^3: Accelerating Attention Mechanisms in Neural Networks with ...", "url": "https://www.researchgate.net/publication/340690399_A3_Accelerating_Attention_Mechanisms_in_Neural_Networks_with_Approximation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340690399_A3_Accelerating_Attention...", "snippet": "We <b>also</b> propose a <b>self-attention</b> mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what <b>specific</b> <b>parts</b> of the ...", "dateLastCrawled": "2022-01-31T19:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "NeurIPS 2021 papers", "url": "https://tanelp.github.io/neurips2021/", "isFamilyFriendly": true, "displayUrl": "https://tanelp.github.io/neurips2021", "snippet": "Our approach uses <b>self-attention</b> to reason about relationships between datapoints explicitly, which <b>can</b> be seen as realizing non-parametric models using parametric attention mechanisms. However, unlike conventional non-parametric models, we let the model learn end-to-end from the data how to make use of other datapoints for prediction. Empirically, our models solve cross-datapoint lookup and complex reasoning tasks unsolvable by traditional deep learning models. We show highly competitive ...", "dateLastCrawled": "2022-02-03T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning</b> 2020 summary: 84 interesting papers/articles | by ...", "url": "https://towardsdatascience.com/machine-learning-2020-summary-84-interesting-papers-articles-45bd45c0d35b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-2020-summary-84-interesting-papers...", "snippet": "This corresponds to the principal component of the <b>layer</b>-<b>specific</b> representation. These <b>can</b> be used for pruning with minimal impact on accuracy. In addition, there was a tendency for the wide model to be strong in <b>scene</b> discrimination and the deep model to be strong in consumer goods discrimination.", "dateLastCrawled": "2022-01-29T20:04:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention in Psychology, Neuroscience, and <b>Machine</b> <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7177153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7177153", "snippet": "These weightings scale the word encodings themselves to create the next <b>layer</b> in the model, a process known as \u201c<b>self-attention</b>.\u201d This process repeats, and eventually interacts with the autoregressive decoder which <b>also</b> has attention mechanisms that allow it to flexibly focus on the encoded input (as in the standard form of attention) and on the previously generated output. The Transformer\u2014the name given to this new attention architecture\u2014outperformed many previous models and quickly ...", "dateLastCrawled": "2021-12-27T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training ...", "url": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_Self_attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_<b>Self_attention</b>_and_Statef...", "snippet": "<b>Self-attention</b>, <b>also</b> known as intra-attention, is an attention mechanism relat- ing di\ufb00erent positions of a sequence in order to model dependencies between dif- ferent parts of the sequence. This di\ufb00ers from general attention in that instead of seeking to discover the \u201cimportant\u201d parts of the sequence relating to the net- work output, <b>self-attention</b> seeks to \ufb01nd the \u201cimportant\u201d portions of the sequence that relate to each other. This is done in order to leverage those intra ...", "dateLastCrawled": "2022-02-03T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine Learning Papers: Molecules</b> - <b>Machine Learning</b> Applied", "url": "https://machinelearningapplied.com/machine-learning-papers-molecules/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>applied.com/<b>machine-learning-papers-molecules</b>", "snippet": "A <b>self-attention</b> based message passing neural network for predicting molecular lipophilicity and aqueous solubility - Tang et al 2020 . Efficient and accurate prediction of molecular properties, such as lipophilicity and solubility, is highly desirable for rational compound design in chemical and pharmaceutical industries. To this end, we build and apply a graph-neural-network framework <b>called</b> <b>self-attention</b>-based message-passing neural network (SAMPN) to study the relationship between ...", "dateLastCrawled": "2021-12-22T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Lecture 7: Transformers</b> - Deep <b>Learning</b>", "url": "https://chinmayhegde.github.io/dl-notes/notes/lecture07/", "isFamilyFriendly": true, "displayUrl": "https://chinmayhegde.github.io/dl-notes/notes/lecture07", "snippet": "<b>Self-Attention</b>. This is the point where papers-blogs-tweets-slides etc start talking about keys/values and attention mechanisms and everything goes a bit haywire. Let\u2019s just ignore all that for now, and instead talk about something <b>called</b> <b>self-attention</b>. The use of the \u201cself-\u201c prefix will become clear later on. Here is how it is defined.", "dateLastCrawled": "2022-02-03T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training", "url": "https://www.researchgate.net/publication/353690477_Enhancing_LSTM_Models_with_Self-attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353690477_Enhancing_LSTM_Models_with_Self...", "snippet": "<b>Self-attention</b>, <b>also</b> known as in tra-attention, is an attention mec hanism re- lating di\ufb00erent positions of a sequence in order to model dependencies b etween di\ufb00erent parts of the sequence.", "dateLastCrawled": "2022-01-13T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Seven Myths in Machine Learning Research</b> | DeepAI", "url": "https://deepai.org/publication/seven-myths-in-machine-learning-research", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>seven-myths-in-machine-learning-research</b>", "snippet": "Importantly, Vaswani et al. noted that \u201dthe computational cost of a separable convolution is equal to the combination of a <b>self-attention</b> <b>layer</b> and a point-wise feed-forward <b>layer</b>.\u201d Even state-of-the-art GANS find <b>self-attention</b> superior to standard convolutions in its ability to model long-range, multi-scale dependencies [Zhang et al., 2018 ] .", "dateLastCrawled": "2022-01-12T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The encoder is composed of a stack of N = 6 identical layers. Each <b>layer</b> has two sub-layers. The first is a multi-head <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by <b>layer</b> normalization. That is, the output ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "<b>Self attention</b>, sometimes <b>called</b> intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. In simpler terms, <b>self attention</b> helps us create similar connections but within the same sentence. Look at the following example: \u201cI poured water from the bottle into the cup until it was full.\u201d it =&gt; cup \u201cI poured water from the bottle into the cup until it was empty.\u201d it=&gt; bottle. By changing one word ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "<b>Machine</b> <b>Learning</b> in Natural Language Processing has traditionally been performed with recurrent neural networks. Recurrent, here, means that when a sequence is processed, the hidden state (or \u2018memory\u2019) that is used for generating a prediction for a token is <b>also</b> passed on, so that it can be used when generating the subsequent prediction. A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Journal of Physics: Conference Series PAPER OPEN ACCESS You may <b>also</b> ...", "url": "https://iopscience.iop.org/article/10.1088/1742-6596/1651/1/012019/pdf", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1742-6596/1651/1/012019/pdf", "snippet": "Different <b>machine</b> <b>learning</b> techniques have been used in this field for many years. But recently, deep <b>learning</b> has caused more and more attention in the field of education. Deep <b>learning</b> is a <b>machine</b> <b>learning</b> method based on neural network structure of multi-<b>layer</b> processing units, and it has been successfully applied to a series of problems in the field of image recognition and natural language processing[2]. With the diversified cultivation of traditional universities and the development ...", "dateLastCrawled": "2021-12-29T04:07:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(self-attention (also called self-attention layer))  is like +(focusing on specific parts of a scene)", "+(self-attention (also called self-attention layer)) is similar to +(focusing on specific parts of a scene)", "+(self-attention (also called self-attention layer)) can be thought of as +(focusing on specific parts of a scene)", "+(self-attention (also called self-attention layer)) can be compared to +(focusing on specific parts of a scene)", "machine learning +(self-attention (also called self-attention layer) AND analogy)", "machine learning +(\"self-attention (also called self-attention layer) is like\")", "machine learning +(\"self-attention (also called self-attention layer) is similar\")", "machine learning +(\"just as self-attention (also called self-attention layer)\")", "machine learning +(\"self-attention (also called self-attention layer) can be thought of as\")", "machine learning +(\"self-attention (also called self-attention layer) can be compared to\")"]}