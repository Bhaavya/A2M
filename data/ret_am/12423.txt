{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Develop <b>Word</b> Embeddings in Python with Gensim", "url": "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/", "isFamilyFriendly": true, "displayUrl": "https://<b>machine</b>learningmastery.com/develop-<b>word</b>-<b>embeddings</b>-python-gensim", "snippet": "<b>Word</b> embeddings are a modern approach for representing text in natural language processing. <b>Word</b> <b>embedding</b> algorithms <b>like</b> word2vec and GloVe are key to the state-of-the-art results achieved by neural network models on natural language processing problems <b>like</b> <b>machine</b> <b>translation</b>. In this tutorial, you will discover how to train and load <b>word</b> <b>embedding</b> models for natural language processing applications in Python using Gensim. After", "dateLastCrawled": "2022-02-02T04:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Introduction To Machine Translation</b> | by Kurtis Pykes | Towards Data ...", "url": "https://towardsdatascience.com/introduction-to-machine-translation-5613f834e0be", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>introduction-to-machine-translation</b>-5613f834e0be", "snippet": "<b>Machine</b> <b>translation</b> is the task of starting with a <b>word</b> (sentence or document) ... we first calculate the <b>word</b> embeddings associated with each of the languages then retrieve a particular <b>word</b> <b>embedding</b> of one of the language&#39;s words, for instance, we may retrieve the English <b>word</b> cat then come up with a way of transforming the English <b>word</b> <b>embedding</b> into a <b>word</b> <b>embedding</b> that is meaningful in the French <b>word</b> vector space. With this <b>word</b> vector in the translated language (French vector space ...", "dateLastCrawled": "2022-01-27T10:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Translation</b> in NLP | Chandan&#39;s Blog", "url": "https://chandan5362.github.io/blog/machine-translation/", "isFamilyFriendly": true, "displayUrl": "https://chandan5362.github.io/blog/<b>machine</b>-<b>translation</b>", "snippet": "In eng-french context, <b>Machine</b> <b>translation</b> finds a similar french <b>word</b> <b>embedding</b> corresponding to each <b>word</b> <b>embedding</b> in English language. It means that to find a french <b>translation</b> of <b>word</b> cat in English, If you are able to find a vector ( among French <b>word</b> embeddings) similar to cat\u2019s embeddings, then that <b>embedding</b> would correspond to <b>word</b> chat in French.", "dateLastCrawled": "2022-02-01T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Na\u00efve <b>Machine</b> <b>Translation</b> in NLP. Na\u00efve <b>Machine</b> <b>Translation</b>: | by ...", "url": "https://medium.com/mlearning-ai/na%C3%AFve-machine-translation-in-nlp-13cf02b9400", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mlearning-ai/na\u00efve-<b>machine</b>-<b>translation</b>-in-nlp-13cf02b9400", "snippet": "Na\u00efve <b>Machine</b> <b>Translation</b>: ... Gensim is a topic modelling and and similarity retrieval library in Python which provides access to <b>word</b> <b>embedding</b> algorithms <b>like</b> Word2Vec and others for training ...", "dateLastCrawled": "2022-01-29T20:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> Learning Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machine</b>learningmastery.com/<b>what-are-word-embeddings</b>", "snippet": "<b>Word</b> embeddings are a type of <b>word</b> representation that allows words with similar meaning to have a similar representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems. In this post, you will discover the <b>word</b> <b>embedding</b> approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Word</b> <b>Embeddings</b> for NLP. Understanding <b>word</b> <b>embeddings</b> and their\u2026 | by ...", "url": "https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>word</b>-<b>embeddings</b>-for-nlp-5b72991e01d4", "snippet": "Human language, <b>like</b> English or Hindi consists of words and sentences, and NLP attempts to extract information from these sentences. A few of the tasks that NLP is used for. Text summarization: extractive or abstractive text summarization; Sentiment Analysis; Translating from one language to another: neural <b>machine</b> <b>translation</b>; Chatbots; <b>Machine</b> learning and deep learning algorithms only take numeric input so how do we convert text to numbers? Bag of words(BOW) Bag of words i s a simple and ...", "dateLastCrawled": "2022-02-02T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>word</b> <b>embeddings and applications to machine translation and</b> sentiment\u2026", "url": "https://www.slideshare.net/mostafabenhenda/word-embeddings-and-applications-to-machine-translation-and-sentiment-analysis", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/mostafabenhenda/<b>word</b>-<b>embeddings-and-applications-to-machine</b>...", "snippet": "Overview 1 Introduction 2 Applications <b>Machine</b> <b>translation</b> Sentiment analysis of movie reviews Vector averaging (Kaggle tutorial) Convolutional Neural Networks (deep learning) Other applications of <b>word</b> embeddings 3 Example of <b>word</b> <b>embedding</b> algorithm: GloVe Build the co-occurrence matrix X Matrix factorization 4 Future work 5 References Mostapha Benhenda <b>word</b> embeddings February 19, 2015 2 / 28", "dateLastCrawled": "2022-01-15T05:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>About machine translation</b>. Developer&#39;s guide", "url": "https://yandex.com/dev/translate/doc/dg/concepts/how-works-machine-translation.html", "isFamilyFriendly": true, "displayUrl": "https://yandex.com/dev/translate/doc/dg/concepts/how-works-<b>machine</b>-<b>translation</b>.html", "snippet": "However, instead of using simple identifiers <b>like</b> the statistical approach, neural <b>machine</b> <b>translation</b> uses what is called <b>word</b> <b>embedding</b>: a vector representation is formed for each <b>word</b>, consisting of numbers that identify its lexical and semantic features. The neural network translates each source sentence as a whole, instead of breaking it down into words and phrases for separate <b>translation</b>. Each <b>word</b> in the sentence is mapped to a vector that is several hundred numbers long. As a result ...", "dateLastCrawled": "2022-01-29T13:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Translation with Cross-lingual Word Embeddings</b>", "url": "https://www.researchgate.net/publication/338138114_Machine_Translation_with_Cross-lingual_Word_Embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338138114_<b>Machine</b>_<b>Translation</b>_with_Cross...", "snippet": "1 Introduction. Methods for <b>machine</b> translations have been stud-. ied for years, and at the same time algorithms. to generate <b>word</b> embeddings are becoming more. and more accurate. Still, there is ...", "dateLastCrawled": "2022-01-30T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Introduction to <b>Word</b> <b>Embedding</b>. Here is a <b>word</b> representation using a ...", "url": "https://medium.com/@dhartidhami/nlp-applications-of-rnn-54e38327ee2e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@dhartidhami/nlp-applications-of-rnn-54e38327ee2e", "snippet": "<b>Word</b> <b>Embedding</b> has been less useful for language modeling, <b>machine</b> <b>translation</b>, especially if you\u2019re accessing a language modeling or <b>machine</b> <b>translation</b> task for which you have a lot of data ...", "dateLastCrawled": "2021-12-16T10:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word Embedding Tutorial | Word2vec</b> Model Gensim Example", "url": "https://www.guru99.com/word-embedding-word2vec.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/<b>word</b>-<b>embedding</b>-<b>word</b>2vec.html", "snippet": "<b>Word</b> <b>Embedding</b> is a <b>word</b> representation type that allows <b>machine</b> learning algorithms to understand words with <b>similar</b> meanings. It is a language modeling and feature learning technique to map words into vectors of real numbers using neural networks, probabilistic models, or dimension reduction on the <b>word</b> co-occurrence matrix. Some <b>word</b> <b>embedding</b> models are Word2vec (Google), Glove (Stanford), and fastest (Facebook). <b>Word</b> <b>Embedding</b> is also called as distributed semantic model or distributed ...", "dateLastCrawled": "2022-02-02T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Shared-Private Bilingual <b>Word</b> Embeddings for Neural <b>Machine</b> <b>Translation</b>", "url": "https://aclanthology.org/P19-1352.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/P19-1352.pdf", "snippet": "<b>Word</b> <b>embedding</b> is central to neural <b>machine</b> <b>translation</b> (NMT), which has attracted inten-sive research interest in recent years. In NMT, the source <b>embedding</b> plays the role of the entrance while the target <b>embedding</b> acts as the terminal. These layers occupy most of the model parameters for representation learn-ing. Furthermore, they indirectly ...", "dateLastCrawled": "2022-01-31T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> Learning Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machine</b>learningmastery.com/<b>what-are-word-embeddings</b>", "snippet": "<b>Word</b> embeddings are a type of <b>word</b> representation that allows words with <b>similar</b> meaning to have a <b>similar</b> representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems. In this post, you will discover the <b>word</b> <b>embedding</b> approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Translation with Cross-lingual Word Embeddings</b>", "url": "https://www.researchgate.net/publication/338138114_Machine_Translation_with_Cross-lingual_Word_Embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338138114_<b>Machine</b>_<b>Translation</b>_with_Cross...", "snippet": "1 Introduction. Methods for <b>machine</b> translations have been stud-. ied for years, and at the same time algorithms. to generate <b>word</b> embeddings are becoming more. and more accurate. Still, there is ...", "dateLastCrawled": "2022-01-30T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Introduction <b>to Machine</b> <b>Translation</b> in NLP | Chandan&#39;s Blog", "url": "https://chandan5362.github.io/blog/machine-translation/", "isFamilyFriendly": true, "displayUrl": "https://chandan5362.github.io/blog/<b>machine</b>-<b>translation</b>", "snippet": "In eng-french context, <b>Machine</b> <b>translation</b> finds a <b>similar</b> french <b>word</b> <b>embedding</b> corresponding to each <b>word</b> <b>embedding</b> in English language. It means that to find a french <b>translation</b> of <b>word</b> cat in English, If you are able to find a vector ( among French <b>word</b> embeddings) <b>similar</b> to cat\u2019s embeddings, then that <b>embedding</b> would correspond to <b>word</b> chat in French. But the question arises, how to find <b>similar</b> french embeddings for english words? Well, from here, we will proceed mathematically ...", "dateLastCrawled": "2022-02-01T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Shared-Private Bilingual <b>Word</b> Embeddings for Neural <b>Machine</b> <b>Translation</b>", "url": "https://arxiv.org/abs/1906.03100", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1906.03100", "snippet": "<b>Word</b> <b>embedding</b> is central to neural <b>machine</b> <b>translation</b> (NMT), which has attracted intensive research interest in recent years. In NMT, the source <b>embedding</b> plays the role of the entrance while the target <b>embedding</b> acts as the terminal. These layers occupy most of the model parameters for representation learning. Furthermore, they indirectly interface via a soft-attention mechanism, which makes them comparatively isolated. In this paper, we propose shared-private bilingual <b>word</b> embeddings ...", "dateLastCrawled": "2021-07-04T19:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>Translation</b> <b>Word</b> Embeddings/Vectors", "url": "https://lms.sulb.uni-saarland.de/moodle/mod/resource/view.php?id=127156", "isFamilyFriendly": true, "displayUrl": "https://lms.sulb.uni-saarland.de/moodle/mod/resource/view.php?id=127156", "snippet": "Intuitively cat is more <b>similar</b> (closer) to dog than to mat, and dog is more <b>similar</b> (closer) to cat than to mat etc.: cat \u2248 dog, cat \u2249mat, dog \u2249mat \u2026. Josef.van_Genabith@dfki.de <b>Translation</b> Technology WS2020/21: <b>Machine</b> <b>Translation</b> 5. One-hot encodings Josef.van_Genabith@dfki.de <b>Translation</b> Technology WS2020/21: <b>Machine</b> <b>Translation</b> 6 cat=&lt;1,0,0&gt; dog=&lt;0,1,0&gt; mat=&lt;0,0,1&gt; V = &lt;cat, dog, mat&gt; cat = &lt;1,0,0&gt; dog = &lt;0,1,0&gt; mat = &lt;0,0,1&gt; 1 1 1. Frequency-based <b>word</b> vectors One-hot encoding ...", "dateLastCrawled": "2022-01-21T18:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Why do we use <b>word</b> embeddings in NLP? | by Natasha Latysheva | Towards ...", "url": "https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-do-we-use-<b>embeddings</b>-in-nlp-2f20e1b632d2", "snippet": "Whether you\u2019re starting a project in text classification, sentiment analysis, or <b>machine</b> <b>translation</b>, odds are that you\u2019ll start by either downloading pre-calculated embeddings (if your problem is relatively standard) or thinking about which method to use to calculate your own <b>word</b> embeddings from your dataset. But why exactly do we use embeddings in NLP? Without talking about any specific algorithms for calculating embeddings (pretend you\u2019ve never heard of word2vec or FastText or ELMo", "dateLastCrawled": "2022-01-29T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Na\u00efve <b>Machine</b> <b>Translation</b> in NLP. Na\u00efve <b>Machine</b> <b>Translation</b>: | by ...", "url": "https://medium.com/mlearning-ai/na%C3%AFve-machine-translation-in-nlp-13cf02b9400", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mlearning-ai/na\u00efve-<b>machine</b>-<b>translation</b>-in-nlp-13cf02b9400", "snippet": "Na\u00efve <b>Machine</b> <b>Translation</b>: The aim of this project is to translate English words to French using <b>word</b> <b>embedding</b> and vector space models.", "dateLastCrawled": "2022-01-29T20:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Translation Weekly 51: Machine Translation without Embeddings</b> ...", "url": "https://jlibovicky.github.io/2020/09/03/MT-Weekly-Machine-Translation-without-Embeddings.html", "isFamilyFriendly": true, "displayUrl": "https://jlibovicky.github.io/.../03/MT-Weekly-<b>Machine-Translation</b>-without-<b>Embeddings</b>.html", "snippet": "It is used as a sort of working memory by the decoder that typically has a <b>similar</b> architecture as the decoder that generates the output left-to-right. In most cases, the input and output vocabularies are the same, so the same <b>embedding</b> matrix can be used both in the encoder, in the decoder, and also as an output projection giving a probability distribution over the output words. Jind\u0159ich&#39;s blog About. <b>Machine Translation Weekly 51: Machine Translation without Embeddings</b>. Sep 3, 2020 mt ...", "dateLastCrawled": "2021-12-27T15:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>An Introduction to Word Embeddings</b> | Springboard Blog", "url": "https://www.springboard.com/blog/data-science/introduction-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.springboard.com/blog/data-science/introduction-<b>word</b>-<b>embeddings</b>", "snippet": "The goal of <b>word</b>-<b>embedding</b> algorithms is, ... Google is <b>thought</b> to use word2vec in RankBrain as part of their search algorithm. Other researchers are using word2vec for sentiment analysis, which attempts to identify the emotionality behind the words people use to communicate. For example, one Stanford research group looked at how the same words in different Reddit communities take on different connotations. Here\u2019s an example with the <b>word</b> soft: As you <b>can</b> see, the <b>word</b> \u201csoft\u201d has a ...", "dateLastCrawled": "2021-12-20T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> Learning Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machine</b>learningmastery.com/<b>what-are-word-embeddings</b>", "snippet": "This section reviews three techniques that <b>can</b> be used to learn a <b>word</b> <b>embedding</b> from text data. 1. <b>Embedding</b> Layer. An <b>embedding</b> layer, for lack of a better name, is a <b>word</b> <b>embedding</b> that is learned jointly with a neural network model on a specific natural language processing task, such as language modeling or document classification. It requires that document text be cleaned and prepared such that each <b>word</b> is one-hot encoded. The size of the vector space is specified as part of the model ...", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why do we use <b>word</b> embeddings in NLP? | by Natasha Latysheva | Towards ...", "url": "https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-do-we-use-<b>embeddings</b>-in-nlp-2f20e1b632d2", "snippet": "Whether you\u2019re starting a project in text classification, sentiment analysis, or <b>machine</b> <b>translation</b>, odds are that you\u2019ll start by either downloading pre-calculated embeddings (if your problem is relatively standard) or thinking about which method to use to calculate your own <b>word</b> embeddings from your dataset. But why exactly do we use embeddings in NLP? Without talking about any specific algorithms for calculating embeddings (pretend you\u2019ve never heard of word2vec or FastText or ELMo", "dateLastCrawled": "2022-01-29T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Positional Encoding. How Does It Know <b>Word</b> Positions Without\u2026 | by ...", "url": "https://naokishibuya.medium.com/positional-encoding-286800cce437", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/positional-encoding-286800cce437", "snippet": "The positional encoding happens after input <b>word</b> <b>embedding</b> and before the encoder. The author explains further: The positional encodings have the same dimension d_model as the embeddings, so that the two <b>can</b> be summed. The base transformer uses <b>word</b> embeddings of 512 dimensions (elements). Therefore, the positional encoding also has 512 ...", "dateLastCrawled": "2022-01-30T21:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Word</b> embeddings for Indian Languages \u2014 AI4Bharat", "url": "https://ai4bharat.squarespace.com/articles/word-embedding", "isFamilyFriendly": true, "displayUrl": "https://ai4bharat.squarespace.com/articles/<b>word</b>-<b>embedding</b>", "snippet": "Learning <b>word</b> embeddings <b>can</b> <b>be thought</b> of as unsupervised feature extraction, reducing the need for building linguistic resources for feature extraction and hand-coding feature extractors . India has 22 constitutionally recognised languages with a combined speaker base of over 1 billion people. Though India is rich in languages, it is poor in resources on these languages. This severely limits our ability to build Natural language tools for Indian languages. The demand for such tools for ...", "dateLastCrawled": "2022-02-01T03:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Word</b> Level English <b>to Marathi Neural Machine Translation using</b> Encoder ...", "url": "https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>word</b>-level-english-to-marathi-neural-<b>machine</b>...", "snippet": "There are various <b>word</b> <b>embedding</b> techniques which map (embed) a <b>word</b> into a fixed length vector. I will assume the reader to be familiar with the concept of <b>word</b> embeddings and won\u2019t cover this topic in detail. However, we will use the built-in <b>Embedding</b> Layer of the Keras API to map each <b>word</b> into a fixed length vector.", "dateLastCrawled": "2022-01-30T19:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "\ud83d\udcdaThe Current Best of Universal <b>Word</b> <b>Embeddings</b> and <b>Sentence</b> <b>Embeddings</b> ...", "url": "https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/huggingface/universal-<b>word</b>-<b>sentence</b>-<b>embeddings</b>-ce48ddc8fc3a", "snippet": "The 6 tasks chosen (Skip-thoughts prediction of the next/previous <b>sentence</b>, neural <b>machine</b> <b>translation</b>, constituency parsing and natural language inference) share the same <b>sentence</b> <b>embedding</b> ...", "dateLastCrawled": "2022-02-03T11:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Neural <b>Machine Translation for Hindi-English: Sequence</b> to sequence ...", "url": "https://medium.com/analytics-vidhya/neural-machine-translation-for-hindi-english-sequence-to-sequence-learning-1298655e334a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/neural-<b>machine-translation-for-hindi-english</b>...", "snippet": "The result is a 50-dimensional vector representation of \u2019Man\u2019. This is called the \u2018<b>embedding</b>\u2019 for the <b>word</b> \u2018Man\u2019. Similarly, all the words in this corpus are represented as 50 ...", "dateLastCrawled": "2022-01-28T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GitHub</b> - brightmart/<b>machine_translation</b>: <b>Machine translation</b> using deep ...", "url": "https://github.com/brightmart/machine_translation", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/brightmart/<b>machine_translation</b>", "snippet": "when it is training, another RNN will be used to try to get a <b>word</b> by using this &quot;<b>thought</b> vector&quot; as init state, and take input from decoder input at each timestamp. decoder start from special token &quot;_GO&quot;. after one step is performanced, new hidden state will be get and together with new input, we <b>can</b> continue this process until we reach to a special token &quot;_END&quot;. we <b>can</b> calculate loss by compute cross entropy loss of logits and target label. logits is get through a projection layer for the ...", "dateLastCrawled": "2021-12-07T17:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Neural machine translation of Hindi and English</b> - IOS Press", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs179873", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs179873", "snippet": "One of the ways of looking at <b>translation</b> <b>can</b> be to think of it as a combination of the three subtasks, <b>word</b> imputation i.e. inserting appropriate words in blanks in a given sentence, sentence reconstruction and similar sentence searches; where from the perspective of one language, the other languages are fuzzy inputs which need to be reconstructed. The goal of this paper is to develop an autoencoding technique that is able to map and learn the latent space distributions of sentences in ...", "dateLastCrawled": "2021-12-30T00:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding <b>Word Embeddings</b>: From Word2Vec to Count Vectors", "url": "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "isFamilyFriendly": true, "displayUrl": "https://www.analyticsvidhya.com/blog/2017/06/<b>word</b>-embeddi", "snippet": "It <b>can</b> be used to perform <b>Machine</b> <b>Translation</b>. The above graph is a bilingual <b>embedding</b> with chinese in green and english in yellow. If we know the words having similar meanings in chinese and english, the above bilingual <b>embedding</b> <b>can</b> be used to translate one language into the other. 4. Using pre-trained <b>word</b> vectors. We are going to use google\u2019s pre-trained model. It contains <b>word</b> vectors for a vocabulary of 3 million words trained on around 100 billion words from the google news dataset ...", "dateLastCrawled": "2022-01-28T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bilingual Word Embeddings for Phrase-Based Machine Translation</b>", "url": "https://ai.stanford.edu/~wzou/emnlp2013_ZouSocherCerManning.pdf", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/~wzou/emnlp2013_ZouSocherCerManning.pdf", "snippet": "<b>Bilingual Word Embeddings for Phrase-Based Machine Translation</b> ... We evaluate these <b>embedding</b> on Chinese <b>word</b> semantic similarity from SemEval-2012 (Jin and Wu, 2012). The embeddings sig-ni\ufb01cantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity Recognition on the OntoNotes dataset (Hovy et al., 2006) with a neural network model. We apply the bilingual embeddings in an end-to-end phrase-based MT ...", "dateLastCrawled": "2022-01-31T08:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine Translation with Cross-lingual Word Embeddings</b>", "url": "https://www.researchgate.net/publication/338138114_Machine_Translation_with_Cross-lingual_Word_Embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338138114_<b>Machine</b>_<b>Translation</b>_with_Cross...", "snippet": "1 Introduction. Methods for <b>machine</b> translations have been stud-. ied for years, and at the same time algorithms. to generate <b>word</b> embeddings are becoming more. and more accurate. Still, there is ...", "dateLastCrawled": "2022-01-30T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "When and Why Are Pre-Trained <b>Word</b> Embeddings Useful for Neural <b>Machine</b> ...", "url": "https://aclanthology.org/N18-2084.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/N18-2084.pdf", "snippet": "<b>can</b> be highly effective. 5 Q3: Effect of Language Similarity The main intuitive hypothesis as to why pre-training works is that the <b>embedding</b> space be-comes more consistent, with semantically simi-lar words closer together. We <b>can</b> also make an additional hypothesis: if the two languages in the <b>translation</b> pair are more linguistically simi-", "dateLastCrawled": "2022-01-30T21:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Introduction <b>to Machine</b> <b>Translation</b> | by Cyprien NIELLY | Towards Data ...", "url": "https://towardsdatascience.com/introduction-to-machine-translation-9cb0e93e7cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-<b>to-machine</b>-<b>translation</b>-9cb0e93e7cb", "snippet": "It compares the <b>machine</b> <b>translation</b> to one or several human-written <b>translation</b>(s), and it computes a similarity score between 0 and 1. A perfect match results in a score of 1, whereas a perfect mismatch results in a score of 0. BLEU score is today the benchmark metric most commonly used. Although this metric is not perfect, it has the advantage of being interpretable and easy to compute.", "dateLastCrawled": "2022-01-31T02:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Word2Vec vs GloVe - A Comparative Guide to <b>Word</b> <b>Embedding</b> Techniques", "url": "https://analyticsindiamag.com/word2vec-vs-glove-a-comparative-guide-to-word-embedding-techniques/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>word</b>2vec-vs-glove-a-comparative-guide-to-<b>word</b>-<b>embedding</b>...", "snippet": "The one noticeable thing about the word2vec generated <b>word</b> <b>embedding</b> is that it <b>can</b> hold the <b>word</b> vectors such as \u201cking\u201d \u2013 \u201cman\u201d + \u201cwoman\u201d -&gt; \u201cqueen\u201d or \u201cbetter\u201d \u2013 \u201cgood\u201d + \u201cbad\u201d -&gt; \u201cworse\u201d together or close in the vector space where the GloVe <b>can</b> not understand such linear relationship between the words in the vector space. Somehow now we are able to make GloVe understand such linear relationships. The gloVe <b>can</b> observe the weightage of <b>word</b>-<b>word</b> co ...", "dateLastCrawled": "2022-01-29T03:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Word</b> <b>embedding</b>. What are <b>word embeddings</b>? Why we use\u2026 | by Manjeet ...", "url": "https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>word</b>-<b>embedding</b>-2d05d270b285", "snippet": "Now, a column <b>can</b> also be understood as <b>word</b> vector for the corresponding <b>word</b> in the matrix M. For example, the <b>word</b> vector for \u2018cat\u2019 in the above matrix is [1,1] and so on.Here, the rows ...", "dateLastCrawled": "2022-01-29T19:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Translation</b>. In this blog, I build a model that\u2026 | by Nupur ...", "url": "https://medium.com/@nupur94/machine-translation-715d1f460c07", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@nupur94/<b>machine-translation</b>-715d1f460c07", "snippet": "An <b>embedding</b> is a vector representation of the <b>word</b> that is close to similar words in an n-dimensional space, where the n represents the size of the <b>embedding</b> vectors. <b>Word</b> embeddings provide a ...", "dateLastCrawled": "2022-01-10T00:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Bias in word embeddings</b> | the morning paper", "url": "https://blog.acolyer.org/2020/12/08/bias-in-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2020/12/08/<b>bias-in-word-embeddings</b>", "snippet": "The inputs to a <b>machine</b> learning model are a description of what is. ... Turkish, Polish, Italian, Greek, French, US American, Russian, and Arabic. The authors also <b>compared</b> male and female names. The study illustrated the use of biased <b>word</b> embeddings results in the creation of biased <b>machine</b> learning classifiers. Models trained on the embeddings replicate the preexisting bias. Bias diffusion was proved both for sexism and xenophobia, with sentiment classifiers assigning positive sentiments ...", "dateLastCrawled": "2022-01-30T00:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - MarcoBerlot/<b>Languages_for_Machine_Translation</b>: Learning <b>word</b> ...", "url": "https://github.com/MarcoBerlot/Languages_for_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/MarcoBerlot/<b>Languages_for_Machine_Translation</b>", "snippet": "This subset consists of slightly more common words because the more common words should have more accurate <b>word</b> embeddings.The English <b>word</b> <b>embedding</b> was multiplied by the transformation matrix to create the predicted translated <b>word</b> <b>embedding</b>. The cosine similarity was then generated with each translated <b>word</b> <b>embedding</b> stored in the corpus. The 20 words with the highest cosine similarity were then outputted. Then the English <b>word</b> was translated through Google Translate and <b>compared</b> to the ...", "dateLastCrawled": "2021-08-12T05:29:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C16-1332.pdf", "snippet": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man + Woman = Queen Aleksandr Drozd y, Anna Gladkova z, Satoshi Matsuoka y yTokyo Institute of Technology, Meguro-ku, Tokyo 152-8550, Japan alex@smg.is.titech.ac.jp, matsu@is.titech.ac.jp z The University of Tokyo, Meguro-ku, Tokyo 153-8902 Japan gladkova@phiz.c.u-tokyo.ac.jp Abstract Solving <b>word</b> analogies became one of the most popular benchmarks for <b>word</b> embeddings on the assumption that linear relations between <b>word</b> pairs ...", "dateLastCrawled": "2022-01-20T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332/", "isFamilyFriendly": true, "displayUrl": "https://acl<b>anthology</b>.org/C16-1332", "snippet": "\ufeff%0 Conference Proceedings %T <b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man + woman = queen %A Drozd, Aleksandr %A Gladkova, Anna %A Matsuoka, Satoshi %S Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers %D 2016 %8 dec %I The COLING 2016 Organizing Committee %C Osaka, Japan %F drozd-etal-2016-<b>word</b> %X Solving <b>word</b> analogies became one of the most popular benchmarks for <b>word</b> embeddings on the assumption that ...", "dateLastCrawled": "2022-01-17T09:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King ...", "url": "https://www.researchgate.net/publication/311843169_Word_Embeddings_Analogies_and_Machine_Learning_Beyond_King_-_Man_Woman_Queen", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/311843169_<b>Word</b>_<b>Embeddings</b>_Analogies_and...", "snippet": "<b>Word</b> Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man+ Woman= Queen December 2016 Conference: Proceedings of COLING 2016, the 26th International Conference on Computational ...", "dateLastCrawled": "2021-11-25T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "<b>Word</b> embeddings are a type of <b>word</b> representation that allows words with similar meaning to have a similar representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep <b>learning</b> methods on challenging natural language processing problems. In this post, you will discover the <b>word</b> <b>embedding</b> approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Finding the <b>Word</b> <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-<b>word</b>-<b>analogy</b>-from-given-<b>words</b>-using-<b>word</b>2vec...", "snippet": "What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the <b>word</b> <b>analogy</b>. In ... Overview of <b>Word</b> <b>Embedding</b> using Embeddings from Language Models (ELMo) 16, Mar 21. <b>Word</b> Embeddings in NLP. 11, Oct 20. Implement your own word2vec(skip-gram) model in Python. 18, Jan 19. Scraping And Finding Ordered Words In A Dictionary using Python. 23, Jul 17 . Python - Replace all words except the given <b>word</b>. 25, Sep 20. Python | Finding &#39;n&#39; Character Words in a Text File. 15, Oct ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-<b>word</b>2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, <b>word</b> <b>embedding</b> is used to map words into vectors of real numbers. There are various <b>word</b> <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce <b>word</b> embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector space, with each unique <b>word</b> in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "GitHub - jungsoh/<b>word</b>-embeddings-<b>word</b>-<b>analogy</b>-by-document-similarity ...", "url": "https://github.com/jungsoh/word-embeddings-word-analogy-by-document-similarity", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jungsoh/<b>word</b>-<b>embeddings</b>-<b>word</b>-<b>analogy</b>-by-document-similarity", "snippet": "An example of a <b>word</b> <b>analogy</b> problem is to fill in the blank: Man is to Woman as King is to _____`. Because <b>word</b> embeddings are very computationally expensive to train, most <b>machine</b> <b>learning</b> practitioners will load a pre-trained set of embeddings. We will load a collection of pre-trained embeddings and measure similarity between <b>word</b> embeddings ...", "dateLastCrawled": "2022-01-25T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "DeepLearning <b>series: Natural Language Processing and Word Embeddings</b> ...", "url": "https://medium.com/machine-learning-bites/deeplearning-series-natural-language-processing-and-word-embeddings-70599080efc9", "isFamilyFriendly": true, "displayUrl": "https://medium.com/<b>machine</b>-<b>learning</b>-bites/deep<b>learning</b>-series-natural-language...", "snippet": "<b>Learning</b> <b>word</b> embeddings: When we implement an algorithm to learn <b>word</b> embeddings, what we end up <b>learning</b> is an <b>embedding</b> matrix. For a 300-feature <b>embedding</b> and a 10,000-<b>word</b> vocabulary, the ...", "dateLastCrawled": "2021-10-27T13:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Word2Vec in Gensim Explained for Creating <b>Word</b> <b>Embedding</b> Models ...", "url": "https://machinelearningknowledge.ai/word2vec-in-gensim-explained-for-creating-word-embedding-models-pretrained-and-custom/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>knowledge.ai/<b>word</b>2vec-in-gensim-explained-for-creating-<b>word</b>...", "snippet": "What is <b>Word</b> Embeddings? <b>Machine</b> <b>learning</b> and ... This is another way putting that word2vec can draw the <b>analogy</b> that if Man is to Woman then Kind is to Queen! The publicly released model of word2vec by Google consists of 300 features and the model is trained in the Google news dataset. The vocabulary size of the model is around 1.6 billion words. However, this might have taken a huge time for the model to be trained on but they have applied a method of simple subsampling approach to ...", "dateLastCrawled": "2022-02-02T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Introduction to <b>Word Embeddings</b>. What is a <b>word</b> <b>embedding</b>? | by Hunter ...", "url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-<b>word-embeddings</b>-4cf857b12edc", "snippet": "A very basic definition of a <b>word</b> <b>embedding</b> is a real number, vector representation of a <b>word</b>. Typically, these days, words with similar meaning will have vector representations that are close together in the <b>embedding</b> space (though this hasn\u2019t always been the case). When constructing a <b>word</b> <b>embedding</b> space, typically the goal is to capture some sort of relationship in that space, be it meaning, morphology, context, or some other kind of relationship. By encoding <b>word embeddings</b> in a ...", "dateLastCrawled": "2022-01-30T04:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Word Embeddings Explained. What is <b>Word Embedding</b> ? | by Ashwin Prasad ...", "url": "https://medium.com/analytics-vidhya/word-embeddings-explained-62c046f7c79e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/word-<b>embedding</b>s-explained-62c046f7c79e", "snippet": "<b>Word Embedding</b> is a technique in Natural Language Processing which is used to represent words in a Deep <b>Learning</b> environment. The main advantage of using <b>word embedding</b> is that it allows words of\u2026", "dateLastCrawled": "2022-01-24T11:15:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Survey and challenges of story generation models - A multimodal ...", "url": "https://www.sciencedirect.com/science/article/pii/S156625352030378X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S156625352030378X", "snippet": "Pang et al. used the deep Bolzmann <b>machine</b> (DBM), which is a joint density model for the visual, auditory, and textual modalities, for <b>learning</b> highly non-linear relations between low-level features across different modalities for emotional prediction. It is trained using joint representation over multimodal inputs; thus, it can handle training samples, which is absent from certain modality. It can be used for emotional prediction and retrieval on any combination of modalities.", "dateLastCrawled": "2022-01-24T04:42:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(word embedding)  is like +(machine translation)", "+(word embedding) is similar to +(machine translation)", "+(word embedding) can be thought of as +(machine translation)", "+(word embedding) can be compared to +(machine translation)", "machine learning +(word embedding AND analogy)", "machine learning +(\"word embedding is like\")", "machine learning +(\"word embedding is similar\")", "machine learning +(\"just as word embedding\")", "machine learning +(\"word embedding can be thought of as\")", "machine learning +(\"word embedding can be compared to\")"]}