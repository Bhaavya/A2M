{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Disparate</b> Impact in <b>Machine</b> <b>Learning</b> \u00bb Dome | Blog Archive | Boston ...", "url": "https://sites.bu.edu/dome/2020/06/08/disparate-impact-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://sites.bu.edu/dome/2020/06/08/<b>disparate</b>-impact-in-<b>machine</b>-<b>learning</b>", "snippet": "Thus, instead of relying on either <b>disparate</b> impact or <b>disparate</b> <b>treatment</b> theory, perhaps legal analysis of discrimination in <b>machine</b> <b>learning</b> should be entirely outcomes-driven. If, in fact, an <b>algorithm</b> wrongly predicts the likelihood of an event occurring, and that <b>algorithm</b> is less accurate for protected class members than unprotected class members, the <b>algorithm</b> should be considered prima facie discriminatory. Such a solution is viable for examining recidivism, interest rates and loan ...", "dateLastCrawled": "2021-12-09T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Disparate</b> <b>Treatment</b> \u2014 Involves classifying someone in an impermissible way. It involves the intent to discriminate, evidenced by explicit reference to group membership. <b>Disparate</b> Impact \u2014 Looks at the consequences of classification/decision making on certain groups. No intent is required and it is facially neutral. <b>Disparate</b> impact is often referred to as unintentional <b>discrimination</b>, whereas <b>disparate</b> <b>treatment</b> is intentional. Practices with a disproportionate impact on a particular ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Discrimination</b> in the Age of Algorithms | Journal of Legal Analysis ...", "url": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "snippet": "We focus on one kind of <b>machine</b>-<b>learning</b> <b>algorithm</b> often applied to such problems, ... With this in mind, consider what <b>disparate</b> <b>treatment</b> looks <b>like</b>. For hiring, an HR manager might do something <b>like</b> rank-order a less-productive white applicant over a more-productive minority applicant. But with a training and a screening <b>algorithm</b>, if we have accounted for the possible sources of <b>discrimination</b> in the human\u2019s choices for the training <b>algorithm</b> (outcome, candidate predictors, and ...", "dateLastCrawled": "2022-01-28T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Does mitigating ML&#39;s <b>disparate impact require disparate treatment</b>?", "url": "https://arxiv.org/abs/1711.07076v1", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1711.07076v1", "snippet": "The natural way to reduce <b>disparate</b> impact would be to apply <b>disparate</b> <b>treatment</b> in favor of the disadvantaged group, i.e. to apply affirmative action. However, owing to the practice&#39;s contested legal status, several papers have proposed trying to eliminate both forms of unfairness simultaneously, introducing a family of algorithms that we denote <b>disparate</b> <b>learning</b> processes (DLPs). These processes incorporate the protected characteristic as an input to the <b>learning</b> <b>algorithm</b> (e.g.~via a ...", "dateLastCrawled": "2021-12-18T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> in Drug Discovery: A Review", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8356896/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8356896", "snippet": "<b>Machine</b> <b>learning</b> techniques improve the decision-making in pharmaceutical data across various applications <b>like</b> QSAR analysis, hit discoveries, de novo drug architectures to retrieve accurate outcomes. Target validation, prognostic biomarkers, digital pathology are considered under problem statements in this review. ML challenges must be applicable for the main cause of inadequacy in interpretability outcomes that may restrict the applications in drug discovery. In clinical trials, absolute ...", "dateLastCrawled": "2022-01-27T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Differentially Private Fair <b>Learning</b> - Proceedings of <b>Machine</b> <b>Learning</b> ...", "url": "http://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "snippet": "can be viewed as a form of \u201c<b>disparate</b> <b>treatment</b>\u201d. Our second <b>algorithm</b> is a differentially private version of the oracle-ef\ufb01cient in-processing ap-proach of (Agarwal et al.,2018) which is more complex but need not have access to protected group membership at test time. We identify new tradeoffs between fairness, accuracy, and privacy that emerge only when requiring all three proper-ties, and show that these tradeoffs can be milder if group membership may be used at test time. We ...", "dateLastCrawled": "2022-01-22T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Disparate</b> impact in a <b>machine</b> <b>learning</b> model originates from bias in either the data or the algorithms. A popular example is the prejudicially biased data used for recidivism prediction. Due to <b>disparate</b> socioeconomic factors and systemic racism in the United States, blacks have historically been (and continue to be) incarcerated at higher rates than whites . Not coincidentally, blacks are also exonerated due to wrongful accusation at a considerably higher rate than whites . A recidivism ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Fairness in Machine Learning is Tricky</b> | by John Dickerson | Arthur AI ...", "url": "https://medium.com/arthur-ai/fairness-in-machine-learning-is-tricky-1ea47111b847", "isFamilyFriendly": true, "displayUrl": "https://medium.com/arthur-ai/<b>fairness-in-machine-learning-is-tricky</b>-1ea47111b847", "snippet": "Then, one goal that a <b>fairness in machine learning</b> practitioner might have is to mathematically certify that an <b>algorithm</b> does not suffer from <b>disparate</b> <b>treatment</b> or <b>disparate</b> impact, perhaps ...", "dateLastCrawled": "2021-04-29T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b> by Runshan Fu, Manmohan Aseri ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "snippet": "<b>Machine</b> <b>Learning</b> algorithms are becoming widely deployed in real world decision-making. Ensuring fairness in algorithmic decision-making is a crucial policy issue. Current legislation ensures fairness by barring <b>algorithm</b> designers from using demographic information in their decision-making. As a result, the algorithms need to ensure equal <b>treatment</b> to be legally compliant. However, in many cases, ensuring equal <b>treatment</b> leads to <b>disparate</b> impact particularly when there are differences ...", "dateLastCrawled": "2022-01-29T16:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "AI Fairness \u2014 Explanation of <b>Disparate Impact</b> Remover | by Stacey ...", "url": "https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/ai-fairness-explanation-of-<b>disparate-impact</b>-remover-ce0...", "snippet": "The <b>algorithm</b> requires the user to specify a repair_level, this indicates how much you wish for the distributions of the groups to overlap. Let\u2019s explore the impact of two different repair levels, 1.0 and 0.8. Repair value = 1.0. This diagram shows the repaired values for Feature for the unprivileged group Blue and privileged group Orange after using DisparateImpactRemover with a repair level of 1.0. You are no longer able to select a point and infer which group it belongs to. This would ...", "dateLastCrawled": "2022-01-29T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness in Machine Learning is Tricky</b> - Arthur AI", "url": "https://www.arthur.ai/blog/fairness-in-ml", "isFamilyFriendly": true, "displayUrl": "https://www.arthur.ai/blog/fairness-in-ml", "snippet": "Similarly, that <b>algorithm</b> is said to result in <b>disparate</b> <b>treatment</b> if its decisioning is performed in part based on membership in a group. Then, one goal that a <b>fairness in machine learning</b> practitioner might have is to mathematically certify that an <b>algorithm</b> does not suffer from <b>disparate</b> <b>treatment</b> or <b>disparate</b> impact, perhaps given some expected use case or input distribution.", "dateLastCrawled": "2022-02-03T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine Learning and Discrimination</b> - GitHub Pages", "url": "https://harvard-ml-courses.github.io/cs181-web-2020/files/lecture12.pdf", "isFamilyFriendly": true, "displayUrl": "https://harvard-ml-courses.github.io/cs181-web-2020/files/lecture12.pdf", "snippet": "<b>Machine Learning and Discrimination</b> Diana Acosta-Navas PhD candidate, Harvard Philosophy Department Adjunct Lecturer in Ethics and Public Policy, Harvard Kennedy School . For Today\u2026 \u2022Discrimination/ wrongful discrimination \u2022Case Study: PredPol \u2022<b>Disparate</b> <b>treatment</b> vs. <b>Disparate</b> impact \u2022How predictive policing could wrongfully discriminate \u2022What contextual considerations are important to determine whether an <b>algorithm</b> wrongfully discriminates? For Today\u2026 \u2022Content warning ...", "dateLastCrawled": "2021-09-15T23:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>AI Fairness \u2014 Explanation of Disparate Impact Remover</b> - Adolfo Eliaz\u00e0t ...", "url": "https://adolfoeliazat.com/2021/05/06/ai-fairness-explanation-of-disparate-impact-remover/", "isFamilyFriendly": true, "displayUrl": "https://adolfoeliazat.com/2021/05/06/<b>ai-fairness-explanation-of-disparate-impact-remover</b>", "snippet": "<b>Disparate</b> Impact Remover preserves rank-ordering within groups; if an individual has the highest score for group Blue, it will still have the highest score among Blues after repair. Building <b>Machine</b> <b>Learning</b> Models. Once <b>Disparate</b> Impact Remover has been implemented, a <b>machine</b> <b>learning</b> model can be built using the repaired data. The <b>Disparate</b> ...", "dateLastCrawled": "2022-01-22T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Disparate</b> impact in a <b>machine</b> <b>learning</b> model originates from bias in either the data or the algorithms. A popular example is the prejudicially biased data used for recidivism prediction. Due to <b>disparate</b> socioeconomic factors and systemic racism in the United States, blacks have historically been (and continue to be) incarcerated at higher rates than whites . Not coincidentally, blacks are also exonerated due to wrongful accusation at a considerably higher rate than whites . A recidivism ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> in Drug Discovery: A Review", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8356896/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8356896", "snippet": "<b>Machine</b> <b>learning</b> techniques improve the decision-making in pharmaceutical data across various applications like QSAR analysis, hit discoveries, de novo drug architectures to retrieve accurate outcomes. Target validation, prognostic biomarkers, digital pathology are considered under problem statements in this review. ML challenges must be applicable for the main cause of inadequacy in interpretability outcomes that may restrict the applications in drug discovery. In clinical trials, absolute ...", "dateLastCrawled": "2022-01-27T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Disparate</b> Impact Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-impact-analysis", "snippet": "<b>Disparate</b> Impact Analysis (DIA) Sensitivity Analysis(SA) As a matter of speaking, the above two features provide a solution to a common problem in ML: the multiplicity of good models. It is well understood that for the same set of input features and prediction targets, complex <b>machine</b> <b>learning</b> algorithms can produce multiple accurate models with very <b>similar</b>, but not the same, internal architectures: the multiplicity of good models [1]. This alone is an obstacle to interpretation, but when ...", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Beyond Bias Audits: Bringing Equity to the <b>Machine</b> <b>Learning</b> Pipeline", "url": "https://responsibleml.iaa.ncsu.edu/wp-content/uploads/2021/06/20210603_WIDS_RML.pdf", "isFamilyFriendly": true, "displayUrl": "https://responsibleml.iaa.ncsu.edu/wp-content/uploads/2021/06/20210603_WIDS_RML.pdf", "snippet": "Percentile of <b>Algorithm</b> Risk Score s [1] Adamson and Smith, \u201c<b>Machine</b> <b>Learning</b> and Health Care Disparities in Dermatology,\u201d JAMA Dermatology 2018. [2] Obermeyer et al, \u201cDissecting racial bias in <b>algorithm</b> used to manage the health of populations\u201c, Science 2019.", "dateLastCrawled": "2022-01-25T02:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2,", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The <b>Racist Algorithm</b>? - University of Michigan", "url": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1657&context=mlr", "isFamilyFriendly": true, "displayUrl": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1657&amp;context=mlr", "snippet": "he calls the \u201csweet mystery of <b>machine</b> <b>learning</b>.\u201d Frank Pasquale, Bittersweet Mysteries of <b>Ma-chine</b> <b>Learning</b> (A Provocation) ... redesign the <b>algorithm</b> or to distrust its results. The distinction <b>is similar</b> to the evidentiary difference between demonstrating <b>disparate</b> <b>treatment</b> and demonstrating <b>disparate</b> impact. 10. My central claim is this: if we believe that the real-world facts, on which algorithms are trained and operate, are deeply suffused with invidious discrimination, then our ...", "dateLastCrawled": "2022-01-30T00:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Disparate</b> <b>Treatment</b> \u2014 Involves classifying someone in an impermissible way. It involves the intent to discriminate, evidenced by explicit reference to group membership. <b>Disparate</b> Impact \u2014 Looks at the consequences of classification/decision making on certain groups. No intent is required and it is facially neutral. <b>Disparate</b> impact is often referred to as unintentional <b>discrimination</b>, whereas <b>disparate</b> <b>treatment</b> is intentional. Practices with a disproportionate impact on a particular ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A primer on AI <b>fairness</b>. What it is and the tradeoffs to be made | by ...", "url": "https://towardsdatascience.com/artificial-intelligence-fairness-and-tradeoffs-ce11ac284b63", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/artificial-intelligence-<b>fairness</b>-and-tradeoffs-ce11ac284b63", "snippet": "Discrimination Law: <b>Disparate</b> <b>Treatment</b> (formal vs intentional) vs <b>Disparate</b> Impact (20% rule, legal rule of thumb). <b>Disparate</b> <b>treatment</b> <b>can</b> <b>be thought</b> of as procedural <b>fairness</b>. The underlying philosophy is equality of opportunity. <b>Disparate</b> impact is distributive justice. There is tension between these two goals.", "dateLastCrawled": "2022-02-02T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> (CS 181) - 2018 Spring | Embedded EthiCS", "url": "https://embeddedethics.seas.harvard.edu/classes/cs-181-2018-spring", "isFamilyFriendly": true, "displayUrl": "https://embeddedethics.seas.harvard.edu/classes/cs-181-2018-spring", "snippet": "<b>machine</b> <b>learning</b> CS Module Overview: In this module, we probe the ways that <b>machine</b> <b>learning</b> models <b>can</b> be discriminatory and examine different methods for preventing discriminatory outcomes. We begin by introducing two concepts of discrimination: <b>disparate</b> <b>treatment</b> and <b>disparate</b> impact. We then use those concepts to argue that there are at ...", "dateLastCrawled": "2022-01-06T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Review on Fairness in <b>Machine</b> <b>Learning</b> | ACM Computing Surveys", "url": "https://dl.acm.org/doi/10.1145/3494672", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3494672", "snippet": "The legal domain has introduced two main definitions of discrimination: (i) <b>disparate</b> <b>treatment</b> [15, 224]: ... However, such mechanisms are tightly coupled with the <b>machine</b> <b>algorithm</b> itself. Hence, we see that the selection of the method depends on the availability of the ground truth, the availability of the sensitive attributes at test time, and on the desired definition of fairness, which <b>can</b> also vary from one application to another. Several preliminary attempts were made to understand ...", "dateLastCrawled": "2022-02-07T09:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Algorithmic bias detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/<b>algorithm</b>ic-bias-detection-and-mitigation-best-", "snippet": "For example, the demonstration of <b>disparate</b> <b>treatment</b> does not describe the ways in which an <b>algorithm</b> <b>can</b> learn to treat similarly situated groups differently, as will be discussed later in the ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Bias and Fairness in <b>Machine</b> <b>Learning</b>, Part 2: building a baseline ...", "url": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-2-building-a-baseline-model-and-features/", "isFamilyFriendly": true, "displayUrl": "https://freecontent.manning.com/bias-and-fairness-in-<b>machine</b>-<b>learning</b>-part-2-building...", "snippet": "This question goes hand in hand with our model\u2019s <b>disparate</b> <b>treatment</b>. Dalex has a very handy plot that <b>can</b> be used with tree-based models and linear models to help visualize the features our model is <b>learning</b> the most from. exp_tree.model_parts().plot() Figure 4. Feature importance of our bias-unaware model as reported by dalex. This visualization is taking feature importances directly from the Random Forest\u2019s feature importance attribute and is showing that priors_count and age are our ...", "dateLastCrawled": "2022-02-03T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The promise of <b>machine learning in predicting treatment outcomes</b> in ...", "url": "https://onlinelibrary.wiley.com/doi/10.1002/wps.20882", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1002/wps.20882", "snippet": "Predicting <b>treatment</b> response is just one relatively narrow use case where <b>machine</b> <b>learning</b> <b>can</b> add value and improve mental health care. Prediction <b>can</b> help with so many more clinical decisions and clinical processes. We could predict barriers that prevent an individual from engaging in care initially, or non-adherence or dropout from care after initiation. We could streamline patients to the appropriate level of care, such as self-guided programs vs. outpatient care, or intensive ...", "dateLastCrawled": "2022-01-22T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Algorithms as discrimination detectors | <b>PNAS</b>", "url": "https://www.pnas.org/content/117/48/30096", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/48/30096", "snippet": "Such algorithms are generally developed using the methods of <b>machine</b> <b>learning</b>; beyond this level of generality there is nothing specific about our argument that hinges on any particular formalism within <b>machine</b> <b>learning</b>\u2014for example, whether we are working with, say, a neural network or some other <b>machine</b>-<b>learning</b> technique. Regardless of the complexity of the function class that the <b>learning</b> procedure allows, all standard techniques share the property that the <b>algorithm</b> builder will need ...", "dateLastCrawled": "2022-01-28T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fall 2020 Journal | Algorithms and Child Welfare: The <b>Disparate</b> Impact ...", "url": "https://bppj.berkeley.edu/2021/02/02/algorithms-and-child-welfare-the-disparate-impact-of-family-surveillance-in-risk-assessment-technologies/", "isFamilyFriendly": true, "displayUrl": "https://bppj.berkeley.edu/2021/02/02/<b>algorithms</b>-and-child-welfare-the-<b>disparate</b>-impact...", "snippet": "We believe that if <b>machine</b> <b>learning</b> is to continue to be used in social services, the history of the data must be considered [34]. Through our literature review, we did not find evidence of regulation over the child welfare data used in <b>machine</b> <b>learning</b> technologies. At the time of writing, Pennsylvania\u2019s statutes on Child Protective Services did not include any guidance on the use of <b>machine</b> <b>learning</b> or artificial intelligence. Searches for the words \u201cautomated\u201d and \u201c<b>algorithm</b> ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "TOP: A Compiler-Based Framework for Optimizing <b>Machine</b> <b>Learning</b> ...", "url": "https://sites.cs.ucsb.edu/~yufeiding/publication/SysML18.pdf", "isFamilyFriendly": true, "displayUrl": "https://sites.cs.ucsb.edu/~yufeiding/publication/SysML18.pdf", "snippet": "<b>treatment</b> to various ML algorithms, and TOP, a compiler-based optimizer for e ectively applying TI to optimize <b>ma-chine</b> <b>learning</b> algorithms. Experiments show that TOP is able to automatically produce optimized algorithms that ei-ther matches or outperforms manually designed algorithms, giving up to 237x speedups and 2.5X on average1. 1. INTRODUCTION Vector dot products and point-to-point distance calcula-tions are essential to many important algorithms across var-ious domains. Vector dot ...", "dateLastCrawled": "2021-09-18T13:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Disparate</b> <b>Treatment</b> \u2014 Involves classifying someone in an impermissible way. It involves the intent to discriminate, evidenced by explicit reference to group membership. <b>Disparate</b> Impact \u2014 Looks at the consequences of classification/decision making on certain groups. No intent is required and it is facially neutral. <b>Disparate</b> impact is often referred to as unintentional <b>discrimination</b>, whereas <b>disparate</b> <b>treatment</b> is intentional. Practices with a disproportionate impact on a particular ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b>", "url": "https://www.ftc.gov/system/files/documents/public_events/1567421/fuaserisinghsrinivasan_updated2.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ftc.gov</b>/system/files/documents/public_events/1567421/fuaserisinghsriniva...", "snippet": "<b>Compared</b> to the current law, which requires <b>treatment</b> parity, these \u201cfair\u201d algorithms, which require impact parity, limit the bene\ufb01ts of a more accurate <b>algorithm</b> for a \ufb01rm. As a result, pro\ufb01t maximizing \ufb01rms could under-invest in <b>learning</b>, i.e., improving the accuracy of their <b>machine</b> <b>learning</b> algorithms. We show that the investment in <b>learning</b> decreases when misclassi\ufb01cation is costly, which is exactly the case when greater accuracy is otherwise desired. Our paper highlights ...", "dateLastCrawled": "2022-02-03T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding and Reducing Bias in <b>Machine Learning</b> | by Jaspreet ...", "url": "https://towardsdatascience.com/understanding-and-reducing-bias-in-machine-learning-6565e23900ac", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-and-reducing-bias-in-<b>machine-learning</b>...", "snippet": "Even if we remove <b>disparate</b> <b>treatment</b> by removing the sensitive feature, discrimination <b>can</b> still happen through other correlated features such as zip code. Measuring and correcting <b>disparate</b> impact makes sure that this is corrected. This requirement should be used when the training dataset is biased. While being considered a controversial measure by many, notably by critics who hold that some scenarios cannot be freed from disproportionate outcomes.", "dateLastCrawled": "2022-01-28T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "\u201cUn\u201dFair <b>Machine</b> <b>Learning</b> Algorithms | Management Science", "url": "https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4065", "isFamilyFriendly": true, "displayUrl": "https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4065", "snippet": "However, in many cases, ensuring equal <b>treatment</b> leads to <b>disparate</b> impact particularly when there are differences among groups based on demographic classes. In response, several \u201cfair\u201d <b>machine</b> <b>learning</b> (ML) algorithms that require impact parity (e.g., equal opportunity) at the cost of equal <b>treatment</b> have recently been proposed to adjust for the societal inequalities. Advocates of fair ML propose changing the law to allow the use of protected class-specific decision rules. We show that ...", "dateLastCrawled": "2022-02-01T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> in Drug Discovery: A Review", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8356896/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8356896", "snippet": "In <b>machine</b> <b>learning</b>, mapping ability features <b>can</b> yield great accomplishment to extract physical, geometric, and chemical features (Khamis et al. ) to retrieve scores. Based on scores, data-driven black box models which are considered to predict interactions in binding affinities and furthermore avoiding few concepts in docking like physical function are very hard to study (Ain et al. 2015 ).", "dateLastCrawled": "2022-01-27T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b> by Runshan Fu, Manmohan Aseri ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "snippet": "<b>Machine</b> <b>Learning</b> algorithms are becoming widely deployed in real world decision-making. Ensuring fairness in algorithmic decision-making is a crucial policy issue. Current legislation ensures fairness by barring <b>algorithm</b> designers from using demographic information in their decision-making. As a result, the algorithms need to ensure equal <b>treatment</b> to be legally compliant. However, in many cases, ensuring equal <b>treatment</b> leads to <b>disparate</b> impact particularly when there are differences ...", "dateLastCrawled": "2022-01-29T16:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The promise of <b>machine learning in predicting treatment outcomes</b> in ...", "url": "https://onlinelibrary.wiley.com/doi/10.1002/wps.20882", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1002/wps.20882", "snippet": "Predicting <b>treatment</b> response is just one relatively narrow use case where <b>machine</b> <b>learning</b> <b>can</b> add value and improve mental health care. Prediction <b>can</b> help with so many more clinical decisions and clinical processes. We could predict barriers that prevent an individual from engaging in care initially, or non-adherence or dropout from care after initiation. We could streamline patients to the appropriate level of care, such as self-guided programs vs. outpatient care, or intensive ...", "dateLastCrawled": "2022-01-22T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Two-stage <b>Algorithm</b> for <b>Fairness-aware</b> <b>Machine</b> <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/two-stage-algorithm-for-fairness-aware-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/two-stage-<b>algorithm</b>-for-<b>fairness-aware</b>-<b>machine</b>-<b>learning</b>", "snippet": "In other words, a <b>machine</b> <b>learning</b> <b>algorithm</b> that utilizes sensitive attributes is subject to biases in the existing data. This could be viewed as an algorithmic version of <b>disparate</b> <b>treatment</b> , where decisions are made on the basis of these sensitive attributes. However, removing sensitive attributes from the dataset is not sufficient solution as it has a <b>disparate</b> impact. <b>Disparate</b> impact is a notion that was born in the 1970s. The U.S. Supreme Court ruled that the hiring decision at the ...", "dateLastCrawled": "2021-12-10T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Disparate</b> Impact Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-impact-analysis", "snippet": "<b>Machine</b> <b>learning</b> models <b>can</b> make drastically differing predictions for only minor changes in input variable values. For example, when looking at predictions that determine financial decisions, SA <b>can</b> be used to help you understand the impact of changing the most important input variables and the impact of changing socially sensitive variables (such as Sex, Age, Race, etc.) in the model. If the model changes in reasonable and expected ways when important variable values are changed, this <b>can</b> ...", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Machine</b> <b>learning</b> models <b>can</b> also be a source of <b>disparate</b> impact in their implementation, through unconscious human biases that affect the fair interpretation or use of the model&#39;s results. This reference does not cover measurement of fairness at implementation. However, if you are interested in fair implementation, we recommend looking at Google&#39;s Fairness Indicators. Harms. In evaluating the potential impact of an ML model, it <b>can</b> be helpful to first clarify what specific harm(s) <b>can</b> be ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>learning</b> and applications in microbiology", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8498514/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8498514", "snippet": "<b>Machine</b> <b>learning</b> has two main <b>learning</b> modes: supervised (also known as predictive) to make future predictions from training data, and unsupervised (descriptive), which is exploratory in nature without training data, defined target or output (Mitchell 1997). Training data are the initial information used to teach supervised ML algorithms in the process of developing a model, from which the model creates and refines its rules required for prediction. Typically, training data comprises a set ...", "dateLastCrawled": "2021-12-06T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Realistically Integrating <b>Machine</b> <b>Learning</b> Into Clinical Pra ...", "url": "https://journals.lww.com/anesthesia-analgesia/fulltext/2020/05000/realistically_integrating_machine_learning_into.4.aspx", "isFamilyFriendly": true, "displayUrl": "https://<b>journals.lww.com</b>/.../05000/realistically_integrating_<b>machine</b>_<b>learning</b>_into.4.aspx", "snippet": "<b>Machine</b>-<b>learning</b> models have been created to predict an increasing number of clinical outcomes, such as diagnoses and mortality, with applications including C. difficile infection in the inpatient hospital setting, 2 identifying molecular markers for cancer treatments, 3 and postoperative surgical outcomes. 4 Examples of <b>machine</b> <b>learning</b> include a cardiologist using an automated interpretation of an ECG and a radiologist using an automated detection of a lung nodule in a chest x-ray. In both ...", "dateLastCrawled": "2021-11-22T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine Learning: A maturing field</b> - Springer", "url": "https://link.springer.com/content/pdf/10.1007/BF00993251.pdf", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/content/pdf/10.1007/BF00993251.pdf", "snippet": "<b>Machine</b> <b>learning</b> conferences in the late &#39;80s drew ten times as many participants as that first CMU workshop: IJCAI and AAAI have very well represented <b>machine</b> <b>learning</b> tracks; this journal was established (with several publishers bidding against each other) and ex- panded from four to six and then to eight issues per year; books on the subject are no longer a rarity; and even undergraduate artificial intelligence curricula routinely include <b>machine</b> <b>learning</b>. What do the steady-as-you-go ...", "dateLastCrawled": "2022-01-22T12:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> Solution Manual Tom M Mitchell", "url": "https://aiai.icaboston.org/d/download/F2T4O6/machine-learning-solution-manual-tom-m-mitchell_pdf", "isFamilyFriendly": true, "displayUrl": "https://aiai.icaboston.org/d/download/F2T4O6/<b>machine</b>-<b>learning</b>-solution-manual-tom-m...", "snippet": "offers expanded <b>treatment</b> of off-policy <b>learning</b> and policy-gradient methods. Part III has new chapters on reinforcement <b>learning</b>&#39;s relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson&#39;s wagering strategy. The final chapter discusses the future societal impacts of reinforcement <b>learning</b>. <b>Machine</b> <b>Learning</b>-Tom M. Mitchell 2012-12-06 One of the currently most active research areas within ...", "dateLastCrawled": "2022-01-28T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Immune System Computes the State of the Body: Crowd Wisdom, <b>Machine</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fimmu.2019.00010/full", "isFamilyFriendly": true, "displayUrl": "https://www.<b>frontiers</b>in.org/articles/10.3389/fimmu.2019.00010", "snippet": "The correlations between the components comprising a set of data can be very subtle and obscure to the human observer, yet such correlations are detectable by <b>machine</b> <b>learning</b> algorithms, and, by <b>analogy</b>, by networks of cells and antibodies in the immune system. As a consequence of exposure to training sets of input, the computer algorithm\u2014and the immune system\u2014can accumulate a bank of learned correlations. These formative correlations can then be used by the computer or the repertoire ...", "dateLastCrawled": "2022-01-30T04:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Protein function in precision medicine: deep understanding with <b>machine</b> ...", "url": "https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.12307", "isFamilyFriendly": true, "displayUrl": "https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.12307", "snippet": "<b>disparate</b> parts may ultimately lead to the same observable effect. In this <b>analogy</b>, we might argue that medicine has so far been often investing into mitigat- ing the inconvenience with lemons and much less into improving and augmenting the protocols for \ufb01nding the individual causes of problems. In his recent State-of-the-Union address, the US Pres-ident Barack Obama announced the Precision Medicine Initiative, making this challenge a national and interna-tional priority. Precision ...", "dateLastCrawled": "2021-12-12T19:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Recurrence in biological and artificial neural <b>networks</b> | by Matthew ...", "url": "https://towardsdatascience.com/recurrence-in-biological-and-artificial-neural-networks-e8a6d5639781", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/recurrence-in-biological-and-artificial-neural-<b>networks</b>...", "snippet": "Recurrence is an overloaded term in the context of neural <b>networks</b>, with <b>disparate</b> colloquial meanings in the <b>machine</b> <b>learning</b> and the neuroscience communities. The difference is narrowing, however, as the artificial neural <b>networks</b> (ANNs) used for practical applications are increasingly sophisticated and more like biological neural <b>networks</b> (BNNs) in some ways (yet still vastly different on the whole).In this post we\u2019ll discuss the historic di f ferences in the use of term recurrence ...", "dateLastCrawled": "2022-01-14T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Latent bias and the implementation of <b>artificial intelligence</b> in ...", "url": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "snippet": "<b>Artificial intelligence</b> (AI) in general, and <b>machine</b> <b>learning</b> in particular, by all accounts, appear poised to revolutionize medicine. 1\u20133 With a wide spectrum of potential uses across translational research (from bench to bedside to health policy), clinical medicine (including diagnosis, <b>treatment</b>, prediction, and healthcare resource allocation), and public health, every area of medicine will be affected.", "dateLastCrawled": "2022-01-28T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "47 <b>Analogy</b> Examples To Make You As Sharp As A Tack (and then some)", "url": "https://www.greetingcardpoet.com/good-analogy-examples-and-definition/", "isFamilyFriendly": true, "displayUrl": "https://www.greetingcardpoet.com/<b>good-analogy-examples-and-definition</b>", "snippet": "The essence of this literary device is to set up a comparison that highlights similarities between two seemingly <b>disparate</b> items. It is by examining how the two items are alike in some way that leads to a clear understanding. Differences between Similes, Metaphors, and Analogies . While similes, metaphors, and analogies are similar in that they all compare two different things, similes and metaphors are figures of speech. In contrast, an <b>analogy</b> is more akin to a logical argument. A writer ...", "dateLastCrawled": "2022-02-02T03:57:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A primer on AI <b>fairness</b>. What it is and the tradeoffs to be made | by ...", "url": "https://towardsdatascience.com/artificial-intelligence-fairness-and-tradeoffs-ce11ac284b63", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/artificial-intelligence-<b>fairness</b>-and-tradeoffs-ce11ac284b63", "snippet": "A <b>machine</b> <b>learning</b> algorithms value is being able to increase the number of true positives and true negatives, which each have a value attached. Each false positive and false negative is costly. The value assigned to each depends on each context. A false negative is more costly in medical situations while a false positive is costlier in death penalty decisions. Expected value is profits that businesses can expect from using the algorithm. The more accurate the model, the higher the profits.", "dateLastCrawled": "2022-02-02T02:42:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(disparate treatment)  is like +(machine learning algorithm)", "+(disparate treatment) is similar to +(machine learning algorithm)", "+(disparate treatment) can be thought of as +(machine learning algorithm)", "+(disparate treatment) can be compared to +(machine learning algorithm)", "machine learning +(disparate treatment AND analogy)", "machine learning +(\"disparate treatment is like\")", "machine learning +(\"disparate treatment is similar\")", "machine learning +(\"just as disparate treatment\")", "machine learning +(\"disparate treatment can be thought of as\")", "machine learning +(\"disparate treatment can be compared to\")"]}