{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep Learning Book: Chapter 8\u2014 <b>Optimization For Training Deep Models</b> ...", "url": "https://medium.com/inveterate-learner/deep-learning-book-chapter-8-optimization-for-training-deep-models-part-i-20ae75984cb2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/inveterate-learner/deep-learning-book-chapter-8-optimization-for...", "snippet": "This is called <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>): Notice the change in distribution over which the expectation is taken. Although this might look relatively similar to optimization, there are two ...", "dateLastCrawled": "2022-02-03T00:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Best</b> k-Layer <b>Neural</b> <b>Network</b> Approximations", "url": "https://www.researchgate.net/publication/348184566_Best_k-Layer_Neural_Network_Approximations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348184566_<b>Best</b>_k-Layer_<b>Neural</b>_<b>Network</b>...", "snippet": "<b>neural</b> <b>network</b> but a parameter estimation problem called the <b>empirical</b> <b>risk</b> <b>minimization</b> probl em . Let s 1 , . . . , s n \u2208 R p be a sample of n independent, iden-", "dateLastCrawled": "2021-12-21T08:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Statistical learning theory for location fingerprinting</b> in wireless ...", "url": "https://www.sciencedirect.com/science/article/pii/S1389128604002610", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1389128604002610", "snippet": "The classical learning method is based on the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) ... The SRM principle chooses the subset H n for which minimizing the <b>empirical</b> <b>risk</b> yields <b>the best</b> bound on the actual <b>risk</b>. Disregarding logarithmic factors, the following problem must be solved: (4) min H n R emp (\u03b8) + h (n) \u2113. The SVM algorithm described in the following section is based on the SRM principle, by minimizing a bound on the VC-dimension and the number of training errors at the same time. 5 ...", "dateLastCrawled": "2022-02-02T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Advances in Empirical Risk Minimization for Image</b> Analysis and Pattern ...", "url": "https://www.researchgate.net/publication/282055557_Advances_in_Empirical_Risk_Minimization_for_Image_Analysis_and_Pattern_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/282055557_Advances_in_<b>Empirical</b>_<b>Risk</b>...", "snippet": "The neuromorphic <b>neural</b> <b>network</b> is proposed to the brain imaging analytic, based on the visual cortex-inspired deep <b>neural</b> <b>network</b> developed for 3 dimensional tooth segmentation and robust visual ...", "dateLastCrawled": "2021-10-22T09:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Support-vector machine</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Support-vector_machine", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Support-vector_machine</b>", "snippet": "The soft-margin <b>support vector machine</b> described above is an example of an <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.", "dateLastCrawled": "2022-02-02T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>flexible algorithm for fault diagnosis</b> in a ... - ScienceDirect.com", "url": "https://www.sciencedirect.com/science/article/pii/S1568494612003006", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1568494612003006", "snippet": "However, ANN, which uses <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) principle, suffers from local minimum traps and the difficulty of determining the hidden layer size and learning rate , . Despite the above-mentioned problems, ANN is still considered as one of the precise methods for classifying problems when the practitioners have enough knowledge and experience. Due to their interpretability and transparency, fuzzy rule-base systems (FRBSs) are recommended by some fault diagnosis studies", "dateLastCrawled": "2022-01-19T21:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning Fundamentals</b> - matthewmcateer.me", "url": "https://matthewmcateer.me/blog/ml-research-interview-ml-basics/", "isFamilyFriendly": true, "displayUrl": "https://matthewmcateer.me/blog/ml-research-interview-ml-basics", "snippet": "We are interested in <b>finding</b> the optimal predictor f ... Therefore, because we don\u2019t have all the data, <b>the best</b> we can do is to minimize the <b>empirical</b> <b>risk</b>, from data that we do have (our training data), and use regularization techniques to generalize (i.e. avoid overfitting). This is why minimizing loss and minimizing <b>empirical</b> <b>risk</b> are roughly the same thing. State the uniform convergence theorem. A sequence {f n} n = 1 \u221e \\{f_n\\}_{n=1}^{\\infty} {f n } n = 1 \u221e of real-valued ...", "dateLastCrawled": "2021-06-14T16:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Notes from MIT Deep Learning Theory and Non-Convex Optimization ...", "url": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019/", "isFamilyFriendly": true, "displayUrl": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019", "snippet": "We\u2019ll think of a <b>neural</b> <b>network</b> as a directed acyclic graph. We have activation fixed, graph fixed, etc. The thing to learn is the weights of the units. This defines a hypothesis class \u2014 inductive bias is <b>given</b> by functions which can be captured by this hypothesis class. This is a flat inductive bias. If you\u2019re also investigating different architectures, you can think of it as a complexity measure over the architectures.", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Machine Learning Google Course Flashcards | Quizlet", "url": "https://quizlet.com/300254930/machine-learning-google-course-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/300254930/machine-learning-google-course-flash-cards", "snippet": "A node <b>in a neural</b> <b>network</b>, typically taking in multiple input values and generating one output value. The <b>neuron</b> calculates the output value by applying an activation function (nonlinear transformation) to a weighted sum of input values.", "dateLastCrawled": "2022-01-22T15:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What&#39;s the <b>difference between perceptron and expectation</b> ... - Quora", "url": "https://www.quora.com/Whats-the-difference-between-perceptron-and-expectation-maximization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Whats-the-<b>difference-between-perceptron-and-expectation-maximization</b>", "snippet": "Answer: This is kind of an apples vs. oranges question. The perceptron algorithm is one of the first algorithms developed for training a single layer <b>neural</b> <b>network</b>. This is very similar to a logistic regression except that the objective function is not the same. The result in both cases is a ...", "dateLastCrawled": "2022-01-21T20:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep Learning Book: Chapter 8\u2014 <b>Optimization For Training Deep Models</b> ...", "url": "https://medium.com/inveterate-learner/deep-learning-book-chapter-8-optimization-for-training-deep-models-part-i-20ae75984cb2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/inveterate-learner/deep-learning-book-chapter-8-optimization-for...", "snippet": "This is called <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>): Notice the change in distribution over which the expectation is taken. Although this might look relatively <b>similar</b> to optimization, there are two ...", "dateLastCrawled": "2022-02-03T00:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Advances in Empirical Risk Minimization for Image</b> Analysis and Pattern ...", "url": "https://www.researchgate.net/publication/282055557_Advances_in_Empirical_Risk_Minimization_for_Image_Analysis_and_Pattern_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/282055557_Advances_in_<b>Empirical</b>_<b>Risk</b>...", "snippet": "The neuromorphic <b>neural</b> <b>network</b> is proposed to the brain imaging analytic, based on the visual cortex-inspired deep <b>neural</b> <b>network</b> developed for 3 dimensional tooth segmentation and robust visual ...", "dateLastCrawled": "2021-10-22T09:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Statistical learning theory for location fingerprinting</b> in wireless ...", "url": "https://www.sciencedirect.com/science/article/pii/S1389128604002610", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1389128604002610", "snippet": "The classical learning method is based on the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) ... The SRM principle chooses the subset H n for which minimizing the <b>empirical</b> <b>risk</b> yields <b>the best</b> bound on the actual <b>risk</b>. Disregarding logarithmic factors, the following problem must be solved: (4) min H n R emp (\u03b8) + h (n) \u2113. The SVM algorithm described in the following section is based on the SRM principle, by minimizing a bound on the VC-dimension and the number of training errors at the same time. 5 ...", "dateLastCrawled": "2022-02-02T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Support-vector machine</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Support-vector_machine", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Support-vector_machine</b>", "snippet": "The soft-margin <b>support vector machine</b> described above is an example of an <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.", "dateLastCrawled": "2022-02-02T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Best</b> k-Layer <b>Neural</b> <b>Network</b> Approximations", "url": "https://www.researchgate.net/publication/348184566_Best_k-Layer_Neural_Network_Approximations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348184566_<b>Best</b>_k-Layer_<b>Neural</b>_<b>Network</b>...", "snippet": "<b>neural</b> <b>network</b> but a parameter estimation problem called the <b>empirical</b> <b>risk</b> <b>minimization</b> probl em . Let s 1 , . . . , s n \u2208 R p be a sample of n independent, iden-", "dateLastCrawled": "2021-12-21T08:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Over-parameterization: Pitfalls and Opportunities", "url": "https://icml.cc/virtual/2021/workshop/8357", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2021/workshop/8357", "snippet": "Specifically, we consider the following popular training algorithms on separable data generated from Gaussian mixtures: (i) <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) with cross-entropy loss, which converges to the multiclass support vector machine (SVM) solution; (ii) <b>ERM</b> with least-squares loss, which converges to the min-norm interpolating (MNI) solution; and, (iii) the one-vs-all SVM classifier. Our first key <b>finding</b> is that under a simple sufficient condition, all three algorithms lead to ...", "dateLastCrawled": "2022-02-02T15:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "New submissions for Tue, 20 Jul 21 \u00b7 Issue #105 \u00b7 zoq/arxiv-updates ...", "url": "https://github.com/zoq/arxiv-updates/issues/105", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zoq/arxiv-updates/issues/105", "snippet": "In this paper, we investigate the excess <b>risk</b> performance and towards improved learning rates for two popular approaches of stochastic optimization: <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and stochastic gradient descent (SGD). Although there exists plentiful generalization analysis of <b>ERM</b> and SGD for supervised learning, current theoretical understandings of <b>ERM</b> and SGD are either have stronger assumptions in convex learning, e.g., strong convexity condition, or show slow rates and less studied ...", "dateLastCrawled": "2021-09-08T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Timeliness online regularized extreme learning machine</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s13042-016-0544-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13042-016-0544-9", "snippet": "Hence, a <b>weight</b> factor for <b>empirical</b> <b>risk</b> is introduced to regularize the proportion of the structural <b>risk</b> <b>minimization</b> and <b>empirical</b> <b>risk</b> <b>minimization</b> to achieve better generalization performance . Motivated by it, a novel algorithm named timeless online regularized ELM (TORELM) is proposed in this paper. This approach makes full use of newly incremental training data in the prediction model. Meanwhile, the timeliness management is employed to maximize the contribution of the new training ...", "dateLastCrawled": "2021-12-03T21:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On Testing Machine Learning Programs | DeepAI", "url": "https://deepai.org/publication/on-testing-machine-learning-programs", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-testing-machine-learning-programs", "snippet": "Following the concept of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), the optimizer allows <b>finding</b> the fitted model that minimizes the <b>empirical</b> <b>risk</b>; which is the loss computed over the training data assuming that it is a representative sample of the target distribution. The <b>empirical</b> <b>risk</b> can correctly approximates the true <b>risk</b> only if the training data distribution is a good approximation of the true data distribution (which is often out of reach in real-world scenarios). The size of the training ...", "dateLastCrawled": "2021-12-16T16:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Machine learning-based methods in structural reliability analysis: A ...", "url": "https://www.sciencedirect.com/science/article/pii/S0951832021007018", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0951832021007018", "snippet": "In addition, ANNs are developed based on <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), resulting in some drawbacks : First, the optimization problem to be solved during the training process may result in a local minimum. Second, the generalization performance highly depends on the <b>network</b> architecture and the training sample size and quality. Eliminating these drawbacks has been the subject of many studies, including ANN applications in SRA. Hurtado in", "dateLastCrawled": "2022-01-20T12:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Best</b> k-Layer <b>Neural</b> <b>Network</b> Approximations", "url": "https://www.researchgate.net/publication/348184566_Best_k-Layer_Neural_Network_Approximations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348184566_<b>Best</b>_k-Layer_<b>Neural</b>_<b>Network</b>...", "snippet": "We show that the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) problem for <b>neural</b> networks has no solution in general. <b>Given</b> a training set \\(s_1, \\ldots , s_n \\in {\\mathbb {R}}^p\\) with corresponding ...", "dateLastCrawled": "2021-12-21T08:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Deep learning - <b>mlstory</b>.org", "url": "https://www.mlstory.org/deep.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>mlstory</b>.org/deep.html", "snippet": "At an abstract level, <b>empirical</b> <b>risk</b> <b>minimization</b> amounts to minimizing R_S[w] = \\frac{1}{n}\\sum_{i=1}^n \\mathit{loss}(f(x_i;w),y_i)\\,. This is a nonconvex optimization problem, but we <b>can</b> still run gradient methods to try to find minimizers. Our main concerns from an optimization perspective are whether we run into local optima and how <b>can</b> we compute gradient steps efficiently. We will address gradient computation through a discussion of automatic differentiation. With regards to global ...", "dateLastCrawled": "2022-01-26T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "NIPS2017abs.md \u00b7 GitHub", "url": "https://gist.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "isFamilyFriendly": true, "displayUrl": "https://<b>gist</b>.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is ubiquitous in machine learning and underlies most supervised learning methods. While there is a large body of work on algorithms for various <b>ERM</b> problems, the exact computational complexity of <b>ERM</b> is still not understood. We address this issue for multiple popular <b>ERM</b> problems including kernel SVMs, kernel ridge regression, and training the final layer of a <b>neural</b> <b>network</b>. In particular, we give conditional hardness results for these problems based on ...", "dateLastCrawled": "2022-01-31T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Neural</b> <b>Network</b> FAQ, part 4 of 7: Books, data, etc.", "url": "https://people.minesparis.psl.eu/fabien.moutarde/FAQs/Neuron-faq/FAQ4.html", "isFamilyFriendly": true, "displayUrl": "https://people.minesparis.psl.eu/fabien.moutarde/FAQs/<b>Neuron</b>-faq/FAQ4.html", "snippet": "Conditions for Consistency of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> Principle; 4. Bounds on the <b>Risk</b> for Indicator Loss Functions; Appendix: Lower Bounds on the <b>Risk</b> of the <b>ERM</b> Principle; 5. Bounds on the <b>Risk</b> for Real-Valued Loss Functions; 6. The Structural <b>Risk</b> <b>Minimization</b> Principle; Appendix: Estimating Functions on the Basis of Indirect Measurements; 7. Stochastic Ill-Posed Problems; 8. Estimating the Values of Functions at <b>Given</b> Points; 9. Perceptrons and Their Generalizations; 10. The Support ...", "dateLastCrawled": "2021-10-29T19:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "Recall that, <b>given</b> a hypothesis class, H, the <b>ERM</b> learning paradigm works as follows: Upon receiving a training sample, S, the learner evaluates the <b>risk</b> (or error) of each h in H on the <b>given</b> sample and outputs a member of H that minimizes this <b>empirical</b> <b>risk</b>. The hope is that an h that minimizes the <b>empirical</b> <b>risk</b> with respect to S is a <b>risk</b> minimizer (or has <b>risk</b> close to the minimum) with respect to the true data probability distribution as well. For that, it suffices to ensure that the ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Notes from MIT Deep Learning Theory and Non-Convex Optimization ...", "url": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019/", "isFamilyFriendly": true, "displayUrl": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019", "snippet": "It <b>can</b> only capture reality with huge capacity, which is useless for us. We want to be able to capture reality well with small capacity. There\u2019s many explanations of what you <b>can</b> capture with a small <b>neural</b> <b>network</b>. You <b>can</b> talk about all kinds of features (formulas, disjunctions and conjunctions, etc.). But all of this is unnecessary ...", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Understanding Machine Learning: From Theory</b> to Algorithms | Keep ...", "url": "https://www.academia.edu/40679311/Understanding_Machine_Learning_From_Theory_to_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40679311/<b>Understanding_Machine_Learning_From_Theory</b>_to_Algorithms", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-23T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Zeyuan</b> Allen-Zhu&#39;s Publications - People | MIT CSAIL", "url": "https://people.csail.mit.edu/zeyuan/publications.htm", "isFamilyFriendly": true, "displayUrl": "https://people.csail.mit.edu/<b>zeyuan</b>/publications.htm", "snippet": "We design a stochastic algorithm to train any smooth <b>neural</b> <b>network</b> to \\(\\varepsilon\\)-approximate local minima, using \\(O(\\varepsilon^{-3.25})\\) backpropagations. <b>The best</b> result was essentially \\(O(\\varepsilon^{-4})\\) by SGD. More broadly, it finds \\(\\varepsilon\\)-approximate local minima of any smooth nonconvex function in rate \\(O(\\varepsilon^{-3.25})\\), with only oracle access to stochastic gradients. &quot;How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD ...", "dateLastCrawled": "2022-01-30T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) 246616207-Understanding-Machine-Learning.pdf | moch chamadani ...", "url": "https://www.academia.edu/31654804/246616207_Understanding_Machine_Learning_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/31654804/246616207_Understanding_Machine_Learning_pdf", "snippet": "246616207-Understanding-Machine-Learning.pdf", "dateLastCrawled": "2022-01-29T09:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What&#39;s the <b>difference between perceptron and expectation</b> ... - Quora", "url": "https://www.quora.com/Whats-the-difference-between-perceptron-and-expectation-maximization", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Whats-the-<b>difference-between-perceptron-and-expectation-maximization</b>", "snippet": "Answer: This is kind of an apples vs. oranges question. The perceptron algorithm is one of the first algorithms developed for training a single layer <b>neural</b> <b>network</b>. This is very similar to a logistic regression except that the objective function is not the same. The result in both cases is a ...", "dateLastCrawled": "2022-01-21T20:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Best</b> k-Layer <b>Neural</b> <b>Network</b> Approximations", "url": "https://www.researchgate.net/publication/348184566_Best_k-Layer_Neural_Network_Approximations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348184566_<b>Best</b>_k-Layer_<b>Neural</b>_<b>Network</b>...", "snippet": "We show that the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) problem for <b>neural</b> networks has no solution in general. <b>Given</b> a training set \\(s_1, \\ldots , s_n \\in {\\mathbb {R}}^p\\) with corresponding ...", "dateLastCrawled": "2021-12-21T08:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) Choosing the function that minimizes loss on the training set. Contrast with structural <b>risk</b> <b>minimization</b>. encoder. #language . In general, any ML system that converts from a raw, sparse, or external representation into a more processed, denser, or more internal representation. Encoders are often a component of a larger model, where they are frequently paired with a decoder. Some Transformers pair encoders with decoders, though other Transformers use only ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Statistical learning theory for location fingerprinting</b> in wireless ...", "url": "https://www.sciencedirect.com/science/article/pii/S1389128604002610", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1389128604002610", "snippet": "The classical learning method is based on the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) ... The SRM principle chooses the subset H n for which minimizing the <b>empirical</b> <b>risk</b> yields <b>the best</b> bound on the actual <b>risk</b>. Disregarding logarithmic factors, the following problem must be solved: (4) min H n R emp (\u03b8) + h (n) \u2113. The SVM algorithm described in the following section is based on the SRM principle, by minimizing a bound on the VC-dimension and the number of training errors at the same time. 5 ...", "dateLastCrawled": "2022-02-02T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Advances in Empirical Risk Minimization for Image</b> Analysis and Pattern ...", "url": "https://www.researchgate.net/publication/282055557_Advances_in_Empirical_Risk_Minimization_for_Image_Analysis_and_Pattern_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/282055557_Advances_in_<b>Empirical</b>_<b>Risk</b>...", "snippet": "The neuromorphic <b>neural</b> <b>network</b> is proposed to the brain imaging analytic, based on the visual cortex-inspired deep <b>neural</b> <b>network</b> developed for 3 dimensional tooth segmentation and robust visual ...", "dateLastCrawled": "2021-10-22T09:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Over-parameterization: Pitfalls and Opportunities", "url": "https://icml.cc/virtual/2021/workshop/8357", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2021/workshop/8357", "snippet": "Specifically, we consider the following popular training algorithms on separable data generated from Gaussian mixtures: (i) <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) with cross-entropy loss, which converges to the multiclass support vector machine (SVM) solution; (ii) <b>ERM</b> with least-squares loss, which converges to the min-norm interpolating (MNI) solution; and, (iii) the one-vs-all SVM classifier. Our first key <b>finding</b> is that under a simple sufficient condition, all three algorithms lead to ...", "dateLastCrawled": "2022-02-02T15:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Block Layer Decomposition schemes for training Deep</b> <b>Neural</b> Networks ...", "url": "https://deepai.org/publication/block-layer-decomposition-schemes-for-training-deep-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../<b>block-layer-decomposition-schemes-for-training-deep</b>-<b>neural</b>-<b>networks</b>", "snippet": "According to the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle, training a DFNN <b>can</b> be formulated as an unconstrained non convex optimization problem. min w \u2208 I R n f (w) = 1 P P \u2211 p = 1 E p (w) + g (w) (1) where the first term represents the average loss, being E p a measure of the loss on the single sample p, and the second term is a regularization term added to improve generalization properties which <b>can</b> also help from an optimization viewpoint by inducing a convexification of the ...", "dateLastCrawled": "2021-12-25T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "On Testing Machine Learning Programs | DeepAI", "url": "https://deepai.org/publication/on-testing-machine-learning-programs", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-testing-machine-learning-programs", "snippet": "Following the concept of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), the optimizer allows <b>finding</b> the fitted model that minimizes the <b>empirical</b> <b>risk</b>; which is the loss computed over the training data assuming that it is a representative sample of the target distribution. The <b>empirical</b> <b>risk</b> <b>can</b> correctly approximates the true <b>risk</b> only if the training data distribution is a good approximation of the true data distribution (which is often out of reach in real-world scenarios). The size of the training ...", "dateLastCrawled": "2021-12-16T16:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>flexible algorithm for fault diagnosis</b> in a ... - ScienceDirect.com", "url": "https://www.sciencedirect.com/science/article/pii/S1568494612003006", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1568494612003006", "snippet": "However, ANN, which uses <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) principle, suffers from local minimum traps and the difficulty of determining the hidden layer size and learning rate , . Despite the above-mentioned problems, ANN is still considered as one of the precise methods for classifying problems when the practitioners have enough knowledge and experience. Due to their interpretability and transparency, fuzzy rule-base systems (FRBSs) are recommended by some fault diagnosis studies", "dateLastCrawled": "2022-01-19T21:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning Fundamentals</b> - matthewmcateer.me", "url": "https://matthewmcateer.me/blog/ml-research-interview-ml-basics/", "isFamilyFriendly": true, "displayUrl": "https://matthewmcateer.me/blog/ml-research-interview-ml-basics", "snippet": "We are interested in <b>finding</b> the optimal predictor f ... Therefore, because we don\u2019t have all the data, <b>the best</b> we <b>can</b> do is to minimize the <b>empirical</b> <b>risk</b>, from data that we do have (our training data), and use regularization techniques to generalize (i.e. avoid overfitting). This is why minimizing loss and minimizing <b>empirical</b> <b>risk</b> are roughly the same thing. State the uniform convergence theorem. A sequence {f n} n = 1 \u221e \\{f_n\\}_{n=1}^{\\infty} {f n } n = 1 \u221e of real-valued ...", "dateLastCrawled": "2021-06-14T16:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Notes from MIT Deep Learning Theory and Non-Convex Optimization ...", "url": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019/", "isFamilyFriendly": true, "displayUrl": "https://kiranvodrahalli.github.io/notes/liveTeX/MITdeep2019", "snippet": "So not only <b>can</b> you not solve <b>ERM</b>, just because your data is representible by a small <b>network</b>, this is not a sufficient inductive bias to ensure learning. Nevertheless, people are using deep learning, and lo and behold, it actually works! There\u2019s some magic property about reality, such that under this property, <b>neural</b> net training actually works. I would claim that we don\u2019t have the faintest idea (hopefully this week maybe some people will have a faint idea) what it is. Well, it is NOT ...", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> and Stochastic Gradient Descent for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "where F(Sn) is the <b>empirical</b> distribution.2 The <b>ERM</b> dogma is to select the predictor \u03c0\u02c6\u03b8 n given by \u02c6\u03b8 n = argmin\u03b8 R\u02c6(\u03b8,Sn). That is, the objective function that de\ufb01nes <b>learning</b> is the <b>empirical</b> <b>risk</b>. <b>ERM</b> has two useful properties. (1) It provides a prin-cipled framework for de\ufb01ning new <b>machine</b> <b>learning</b> methods. In particular, when ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Statistical <b>Learning</b> Theory and the C-Loss cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the <b>Risk</b> functional as L(.) is called the Loss function, and minimize it w.r.t. w achieving the best possible loss. But we can not do this ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Computational and Statistical <b>Learning</b> Theory", "url": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) / Sample Average Approximation (SAA): Collect sample z1UYU zm ... SGD for <b>Machine</b> <b>Learning</b> Initialize S 4 L r At iteration t: Draw T \u00e7\u00e1U \u00e71\u00de If U \u00e7 S \u00e7 \u00e1\u00f6 T \u00e7 O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00e7\u00f6 T \u00e7 else: S \u00e7 &gt; 5 Z S \u00e7 Return S % \u00cd L 5 \u00cd \u00c3 \u00cd S \u00e7 \u00e7 @ 5 Draw T 5\u00e1U 5 \u00e1\u00e5\u00e1 T \u00e0 \u00e1U \u00e0 1\u00de Initialize S 4 L r At iteration t: Pick E \u00d0 s\u00e5I at random If U \u00dc S \u00e7 \u00e1\u00f6 T \u00dc O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00dc\u00f6 T \u00dc else: S \u00e7 &gt; 5 Z S \u00e7 S \u00e7 &gt; 5 Z ...", "dateLastCrawled": "2022-01-26T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Basics of <b>Machine</b> <b>Learning</b>", "url": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_<b>learning</b>.pdf", "snippet": "This is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) COMPSCI 527 \u2014 Computer Vision Basics of <b>Machine</b> <b>Learning</b> 15/26. Loss and <b>Risk</b> <b>Machine</b> <b>Learning</b> and the Statistical <b>Risk</b> <b>ERM</b>: w^ 2argmin w2R m L T(w) In <b>machine</b> <b>learning</b>, we go much farther: We also want h to do well on previously unseen inputs To relate past and future data, assume that all data comes from the same joint probability distribution p(x;y) p is called the generative data model or just model The goal of <b>machine</b> <b>learning</b> is to ...", "dateLastCrawled": "2021-11-06T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, <b>empirical</b> <b>risk</b>, motivation for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Further Reading, Supplementary: Jan 12: Consistency of <b>ERM</b>, Sufficient condition for <b>ERM</b> as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture 1: Reinforcement <b>Learning</b>: What and Why?", "url": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "snippet": "<b>machine</b> <b>learning</b> and is referred to as <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). 3 Challenges of reinforcement <b>learning</b> Consider the cart pole balancing problem, where a cart carrying an unactuated pole \ufb02oats on a straight horizontal track. The cart is actuated by a torque applied either to the right or the left direction. Seeherefor a real cart ...", "dateLastCrawled": "2021-09-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Stratified <b>Sampling Meets Machine Learning</b>", "url": "http://proceedings.mlr.press/v48/liberty16.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v48/liberty16.pdf", "snippet": "3. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard ap-proach in <b>machine</b> <b>learning</b> in which the chosen model is the minimizer of the <b>empirical</b> <b>risk</b>. The <b>empirical</b> <b>risk</b> R emp(p) is de\ufb01ned as an average loss of the model over the training set Q. Here Qis a query log containing a ran-", "dateLastCrawled": "2021-10-13T10:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Liu Liu - GitHub Pages", "url": "https://liuliuforph.github.io/", "isFamilyFriendly": true, "displayUrl": "https://liuliuforph.github.io", "snippet": "In <b>analogy</b> to classical compressed sensing, here we assume a generative model as a prior, that is, we assume the vector is represented by a deep generative model G: R^k \u2013&gt; R^n. Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the ...", "dateLastCrawled": "2022-02-02T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[2006.09461] Robust <b>Compressed Sensing using Generative Models</b> - arXiv", "url": "https://arxiv.org/abs/2006.09461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2006.09461", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-06-27T11:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ToyotaTechnologicalInstituteatChicago UniversityofTexasatAustin surbhi ...", "url": "https://arxiv.org/pdf/2005.07652", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2005.07652", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, \u02c6h \u2208 RERM U(S) ,argmin h\u2208H 1 m Xm i=1 sup z\u2208U(x) 1 [h(z) 6= y]. In this paper, we provide necessary and su\ufb03cient conditions on perturbation sets U, under which the robust empirical risk minimization (RERM) problem is e\ufb03ciently solvable in the realizable setting. We show that an e\ufb03cient ...", "dateLastCrawled": "2021-10-06T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Efficiently Learning Adversarially Robust Halfspaces with</b> Noise | DeepAI", "url": "https://deepai.org/publication/efficiently-learning-adversarially-robust-halfspaces-with-noise", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>efficiently-learning-adversarially-robust-halfspaces</b>...", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, ^ h \u2208 R E R M U ( S ) \u225c argmin h \u2208 H 1 m m \u2211 i = 1 sup z \u2208 U ( x ) 1 [ h ( z ) \u2260 y ] .", "dateLastCrawled": "2021-12-05T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficiently <b>Learning</b> Adversarially Robust Halfspaces with Noise", "url": "http://proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "snippet": "remains a major challenge in <b>machine</b> <b>learning</b>. A line of work has shown that predictors learned by deep neural networks are not robust to adversarial examples (Szegedy et al.,2014;Biggio et al.,2013;Goodfellow et al.,2015). This has led to a long line of research studying different aspects of robustness to adversarial examples. In this paper, we consider the problem of distribution-independent <b>learning</b> of halfspaces that are robust to ad-versarial examples at test time, also referred to as ...", "dateLastCrawled": "2021-11-21T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(empirical risk minimization (erm))  is like +(finding the best weight for a given neuron in a neural network)", "+(empirical risk minimization (erm)) is similar to +(finding the best weight for a given neuron in a neural network)", "+(empirical risk minimization (erm)) can be thought of as +(finding the best weight for a given neuron in a neural network)", "+(empirical risk minimization (erm)) can be compared to +(finding the best weight for a given neuron in a neural network)", "machine learning +(empirical risk minimization (erm) AND analogy)", "machine learning +(\"empirical risk minimization (erm) is like\")", "machine learning +(\"empirical risk minimization (erm) is similar\")", "machine learning +(\"just as empirical risk minimization (erm)\")", "machine learning +(\"empirical risk minimization (erm) can be thought of as\")", "machine learning +(\"empirical risk minimization (erm) can be compared to\")"]}