{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Definitions, methods, and applications in interpretable machine learning", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6825274/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6825274", "snippet": "The information produced by an interpretation method should be faithful to <b>the underlying</b> process the practitioner is <b>trying</b> <b>to understand</b>. In the context of ML, there are 2 areas where errors can arise: when approximating <b>the underlying</b> data relationships with a model (predictive accuracy) and when approximating what the model <b>has</b> learned using an interpretation method (descriptive accuracy). For an interpretation to be trustworthy, one should try to maximize both of the accuracies. In ...", "dateLastCrawled": "2022-01-26T13:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is <b>Interpretability</b>? | SpringerLink", "url": "https://link.springer.com/article/10.1007/s13347-020-00435-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13347-020-00435-2", "snippet": "Conveniently, a plausible solution to this problem <b>has</b> already <b>been</b> proposed: <b>interpretability</b>. A further problem, however, is that there is little agreement over exactly what <b>interpretability</b> is, an issue we deal with in Section 4. Medical AI Systems Are Explainable. It is possible to explain MAIS using each of the four models outlined above.", "dateLastCrawled": "2022-01-26T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) What is <b>Interpretability</b>?", "url": "https://www.researchgate.net/publication/345820069_What_is_Interpretability", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/345820069_What_is_<b>Interpretability</b>", "snippet": "defining a geometric measure of distance between input values and building a model. (e.g., a sparse linear model or shallow tree, Section 4.4) that approximates the more. global model in the ...", "dateLastCrawled": "2022-01-05T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Interpretable machine learning: definitions, methods, and applications</b> ...", "url": "https://www.arxiv-vanity.com/papers/1901.04592/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1901.04592", "snippet": "The information produced by an interpretation method should be faithful to <b>the underlying</b> process the practitioner is <b>trying</b> <b>to understand</b>. In the context of ML, there are two areas where errors can arise: when approximating <b>the underlying</b> data relationships with a model (predictive accuracy) and when approximating what the model <b>has</b> learned using an interpretation method (descriptive accuracy). For an interpretation to be trustworthy, one should try to maximize both of the accuracies. In ...", "dateLastCrawled": "2022-01-11T04:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Explainable AI: A Review of Machine Learning <b>Interpretability</b> Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "That <b>said</b>, there <b>has</b> not <b>been</b> a best-in-class method developed to address every need, as most methods focus on either a specific type of model, or a specific type of data, or their scope is either local or global, but not both. Of the methods presented, SHAP is the most complete method, providing explanations for any model and any type of data, doing so at both a global and local scope. However, SHAP is not without shortcomings: The kernel version of SHAP, KernelSHAP, <b>like</b> most permutation ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>Survey of Methods for Explaining Black Box Models</b>", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3236009", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3236009", "snippet": "To interpret means to give or provide the <b>meaning</b> or to explain and present in understandable terms some concepts. 4 Therefore, in data mining and machine learning, <b>interpretability</b> is defined as the ability to explain or to provide the <b>meaning</b> in understandable terms to a human . These definitions assume implicitly that the concepts expressed in the understandable terms composing an explanation are self-contained and do not need further explanations. Essentially, an explanation is an ...", "dateLastCrawled": "2022-01-18T12:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evaluate Topic Models: Latent Dirichlet Allocation (LDA) | by Shashank ...", "url": "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet...", "snippet": "In the previous article, I introduced the concept of topic modeling and walked through the code for developing your first topic model using Latent Dirichlet Allocation (LDA) method in the python using Gensim implementation.. Pursuing on that <b>understand</b> i ng, in this article, we\u2019ll go a few steps deeper by outlining the framework to quantitatively evaluate topic models through the measure of topic <b>coherence</b> and share the code template in python using Gensim implementation to allow for end ...", "dateLastCrawled": "2022-02-03T00:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Research on quantum cognition in autonomous driving | Scientific Reports", "url": "https://www.nature.com/articles/s41598-021-04239-y", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-04239-y", "snippet": "Quantum-<b>like</b> Bayes (QLB) method <b>has</b> a good effect on pedestrian prediction, but it <b>has</b> some limitations. For example, further optimization is needed in terms of interpretation, and a more perfect ...", "dateLastCrawled": "2022-01-29T22:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>the interpretation of RMSE, MAE, MPE</b>, MAPE, and MASE in testing ...", "url": "https://www.quora.com/What-is-the-interpretation-of-RMSE-MAE-MPE-MAPE-and-MASE-in-testing-forecasting-accuracy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-interpretation-of-RMSE-MAE-MPE</b>-MAPE-and-MASE-in...", "snippet": "Answer: It is important to evaluate forecast accuracy using genuine forecasts. ... Because the test data is not used in determining the forecasts, it should ... error: MAE= mean(|et|),Root mean squared error: RMSE=\u221amean(e2t). ... the RMSE is also widely used, despite being more difficult to inte...", "dateLastCrawled": "2022-01-26T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why is machine learning \u2018hard\u2019? | Hacker News", "url": "https://news.ycombinator.com/item?id=12936891", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=12936891", "snippet": "Its <b>like</b> we don&#39;t yet <b>understand</b> ML properly yet to definitively say, for a given data set, what sort of model we&#39;ll need. This can lead us down the debugging black-hole TFA talks about since we appear to have zero-clue about why we chose something, so debugging something ultimately might just be &quot;opps - we chose 3 layers of 10, 15, and 11 nodes, instead of 3 layers of 10, 15 and 12 nodes!", "dateLastCrawled": "2020-12-08T16:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>Interpretability</b>? | SpringerLink", "url": "https://link.springer.com/article/10.1007/s13347-020-00435-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13347-020-00435-2", "snippet": "Conveniently, a plausible solution to this problem <b>has</b> already <b>been</b> proposed: <b>interpretability</b>. A further problem, however, is that there is little agreement over exactly what <b>interpretability</b> is, an issue we deal with in Section 4. Medical AI Systems Are Explainable. It is possible to explain MAIS using each of the four models outlined above.", "dateLastCrawled": "2022-01-26T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Explainable AI: A Review of Machine Learning <b>Interpretability</b> Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "That <b>said</b>, there <b>has</b> not <b>been</b> a best-in-class method developed to address every need, as most methods focus on either a specific type of model, or a specific type of data, or their scope is either local or global, but not both. Of the methods presented, SHAP is the most complete method, providing explanations for any model and any type of data, doing so at both a global and local scope. However, SHAP is not without shortcomings: The kernel version of SHAP, KernelSHAP, like most permutation ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Definitions, methods, and applications in interpretable machine ... - <b>PNAS</b>", "url": "https://www.pnas.org/content/116/44/22071", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/116/44/22071", "snippet": "The information produced by an interpretation method should be faithful to <b>the underlying</b> process the practitioner is <b>trying</b> <b>to understand</b>. In the context of ML, there are 2 areas where errors can arise: when approximating <b>the underlying</b> data relationships with a model (predictive accuracy) and when approximating what the model <b>has</b> learned using an interpretation method (descriptive accuracy). For an interpretation to be trustworthy, one should try to maximize both of the accuracies. In ...", "dateLastCrawled": "2022-02-02T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Interpreting Interpretability: Understanding Data Scientists</b>&#39; Use of ...", "url": "https://www.researchgate.net/publication/341690531_Interpreting_Interpretability_Understanding_Data_Scientists'_Use_of_Interpretability_Tools_for_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/341690531_Interpreting_<b>Interpretability</b>...", "snippet": "As the discipline <b>has</b> evolved, research in machine learning <b>has</b> <b>been</b> focused more and more on creating more powerful neural networks, without regard for the <b>interpretability</b> of these networks ...", "dateLastCrawled": "2022-01-20T05:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Explainable AI: A Review of Machine Learning <b>Interpretability</b> Methods", "url": "https://www.researchgate.net/publication/348032815_Explainable_AI_A_Review_of_Machine_Learning_Interpretability_Methods", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348032815_Explainable_AI_A_Review_of_Machine...", "snippet": "That <b>said</b>, there <b>has</b> not <b>been</b> a best-in-class method developed to address every Entropy 2021 , 23 , 18 17 of 45 need, as most methods focus on either a speci\ufb01c type of model, or a speci\ufb01c type ...", "dateLastCrawled": "2022-01-25T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Explainable AI: A Review of <b>Machine Learning</b> <b>Interpretability</b> ... - MDPI", "url": "https://www.mdpi.com/1099-4300/23/1/18/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/23/1/18/htm", "snippet": "That <b>said</b>, there <b>has</b> not <b>been</b> a best-in-class method developed to address every need, as most methods focus on either a specific type of model, or a specific type of data, or their scope is either local or global, but not both. Of the methods presented, SHAP is the most complete method, providing explanations for any model and any type of data, doing so at both a global and local scope. However, SHAP is not without shortcomings: The kernel version of SHAP, KernelSHAP, like most permutation ...", "dateLastCrawled": "2022-02-01T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A <b>Survey of Methods for Explaining Black Box Models</b>", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3236009", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3236009", "snippet": "To interpret means to give or provide the <b>meaning</b> or to explain and present in understandable terms some concepts. 4 Therefore, in data mining and machine learning, <b>interpretability</b> is defined as the ability to explain or to provide the <b>meaning</b> in understandable terms to a human . These definitions assume implicitly that the concepts expressed in the understandable terms composing an explanation are self-contained and do not need further explanations. Essentially, an explanation is an ...", "dateLastCrawled": "2022-01-18T12:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is <b>the interpretation of RMSE, MAE, MPE</b>, MAPE, and MASE in testing ...", "url": "https://www.quora.com/What-is-the-interpretation-of-RMSE-MAE-MPE-MAPE-and-MASE-in-testing-forecasting-accuracy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-interpretation-of-RMSE-MAE-MPE</b>-MAPE-and-MASE-in...", "snippet": "Answer: It is important to evaluate forecast accuracy using genuine forecasts. ... Because the test data is not used in determining the forecasts, it should ... error: MAE= mean(|et|),Root mean squared error: RMSE=\u221amean(e2t). ... the RMSE is also widely used, despite being more difficult to inte...", "dateLastCrawled": "2022-01-26T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Practical thoughts on <b>explanatory</b> vs. predictive modeling - Cross Validated", "url": "https://stats.stackexchange.com/questions/1194/practical-thoughts-on-explanatory-vs-predictive-modeling", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/1194", "snippet": "<b>Explanatory</b> model <b>has</b> also <b>been</b> used in medicine and the health area as well, with a very different <b>meaning</b>. Basically what people have as internal beliefs or meanings can be quite different from accepted explanations. For example a religious person may have an <b>explanatory</b> model that an illness was due to punishment or karma for a past behaviour along with accepting th biological reasons as well.", "dateLastCrawled": "2022-01-23T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Evaluate Topic Models: Latent Dirichlet Allocation (LDA) | by Shashank ...", "url": "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet...", "snippet": "Before we <b>understand</b> topic <b>coherence</b>, let\u2019s briefly look at the perplexity measure. Perplexity as well is one of the intrinsic evaluation metric, and is widely used for language model evaluation. It captures how surprised a model is of new data it <b>has</b> not seen before, and is measured as the normalized log-likelihood of a held-out test set.", "dateLastCrawled": "2022-02-03T00:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>Interpretability</b>? | SpringerLink", "url": "https://link.springer.com/article/10.1007/s13347-020-00435-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13347-020-00435-2", "snippet": "Conveniently, a plausible solution to this problem <b>has</b> already <b>been</b> proposed: <b>interpretability</b>. A further problem, however, is that there is little agreement over exactly what <b>interpretability</b> is, an issue we deal with in Section 4. Medical AI Systems Are Explainable. It is possible to explain MAIS using each of the four models outlined above.", "dateLastCrawled": "2022-01-26T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Definitions, methods, and applications in interpretable machine ... - <b>PNAS</b>", "url": "https://www.pnas.org/content/116/44/22071", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/116/44/22071", "snippet": "The information produced by an interpretation method should be faithful to <b>the underlying</b> process the practitioner is <b>trying</b> <b>to understand</b>. In the context of ML, there are 2 areas where errors <b>can</b> arise: when approximating <b>the underlying</b> data relationships with a model (predictive accuracy) and when approximating what the model <b>has</b> learned using an interpretation method (descriptive accuracy). For an interpretation to be trustworthy, one should try to maximize both of the accuracies. In ...", "dateLastCrawled": "2022-02-02T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Foundations of Symbolic Languages for Model <b>Interpretability</b>", "url": "https://www.researchgate.net/publication/355111182_Foundations_of_Symbolic_Languages_for_Model_Interpretability", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355111182_Foundations_of_Symbolic_Languages...", "snippet": "PDF | Several queries and scores have recently <b>been</b> proposed to explain individual predictions over ML models. Given the need for flexible, reliable,... | Find, read and cite all the research you ...", "dateLastCrawled": "2021-11-03T00:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Intuitively Assessing ML Model Reliability through Example-Based ...", "url": "https://www.arxiv-vanity.com/papers/2102.08540/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2102.08540", "snippet": "<b>Interpretability</b> methods aim to help users build trust in and <b>understand</b> the capabilities of machine learning models. However, existing approaches often rely on abstract, complex visualizations that poorly map to the task at hand or require non-trivial ML expertise to interpret. Here, we present two visual analytics modules that facilitate an intuitive assessment of model reliability. To help users better characterize and reason about a model\u2019s uncertainty, we visualize raw and aggregate ...", "dateLastCrawled": "2021-11-26T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Why ANOVA and Linear Regression are the Same Analysis</b>", "url": "https://www.theanalysisfactor.com/why-anova-and-linear-regression-are-the-same-analysis/", "isFamilyFriendly": true, "displayUrl": "https://www.theanalysisfactor.com/<b>why-anova-and-linear-regression-are-the-same-analysis</b>", "snippet": "I believe that understanding this little concept <b>has</b> <b>been</b> key to my understanding the general linear model as a whole\u2013its applications are far reaching. Use a model with a single categorical independent variable, employment category, with 3 categories: managerial, clerical, and custodial. The dependent variable is Previous Experience in months. (This data set is employment.sav, and it is one of the data sets that comes free with SPSS). We <b>can</b> run this as either an ANOVA or a regression. In ...", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Limitations of <b>Machine Learning</b> | by Matthew Stewart, PhD ...", "url": "https://towardsdatascience.com/the-limitations-of-machine-learning-a00e0c3040c6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-limitations-of-<b>machine-learning</b>-a00e0c3040c6", "snippet": "A neural network <b>can</b> never tell us how to be a good person, and, at least for now, do not <b>understand</b> Newton\u2019s laws of motion or Einstein\u2019s theory of relativity. There are also fundamental limitations grounded in <b>the underlying</b> theory of <b>machine learning</b>, called computational learning theory, which are primarily statistical limitations. We have also discussed issues associated with the scope of the analysis and the dangers of p-hacking, which <b>can</b> lead to spurious conclusions. There are ...", "dateLastCrawled": "2022-02-02T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is <b>the interpretation of RMSE, MAE, MPE</b>, MAPE, and MASE in testing ...", "url": "https://www.quora.com/What-is-the-interpretation-of-RMSE-MAE-MPE-MAPE-and-MASE-in-testing-forecasting-accuracy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-interpretation-of-RMSE-MAE-MPE</b>-MAPE-and-MASE-in...", "snippet": "Answer: It is important to evaluate forecast accuracy using genuine forecasts. ... Because the test data is not used in determining the forecasts, it should ... error: MAE= mean(|et|),Root mean squared error: RMSE=\u221amean(e2t). ... the RMSE is also widely used, despite being more difficult to inte...", "dateLastCrawled": "2022-01-26T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "From replication to substantiation: A complexity theory perspective ...", "url": "https://www.cambridge.org/core/journals/language-teaching/article/from-replication-to-substantiation-a-complexity-theory-perspective/5E07688E857C74E635AB15F54E17F852", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/language-teaching/article/from-replication-to...", "snippet": "It <b>has</b> <b>been</b> proposed that theories start as a conjecture that needs to be falsified (Popper, ... The better researchers come <b>to understand</b> (complex) <b>underlying</b> causal relations, the more accurate their expectations should be. This representational philosophy of science (Freese &amp; Peterson, Reference Freese and Peterson 2017; Hacking, Reference Hacking 1983; Pickering, Reference Pickering 1995) devalues exploratory and pre-theoretical observation and experimentation, consequently impeding the ...", "dateLastCrawled": "2022-01-29T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Underlying Structure of Ruminative Thinking</b>: Factor Analysis of the ...", "url": "https://link.springer.com/article/10.1007%2Fs10608-012-9492-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10608-012-9492-1", "snippet": "<b>The Ruminative Thought Style Questionnaire</b> (RTSQ) is a 20-item measure assessing a single dimension of rumination over and above valence, temporal orientation of <b>thought</b> content, and the cognitive-affective context in which it occurs. The current study examined the factor structure of rumination as measured by the RTSQ, and whether findings of its initial validation study could be replicated within an adolescent sample (N = 2,362). An exploratory factor analysis and a subsequent confirmatory ...", "dateLastCrawled": "2022-01-29T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Evaluate Topic Models: Latent Dirichlet Allocation (LDA) | by Shashank ...", "url": "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet...", "snippet": "In the previous article, I introduced the concept of topic modeling and walked through the code for developing your first topic model using Latent Dirichlet Allocation (LDA) method in the python using Gensim implementation.. Pursuing on that <b>understand</b> i ng, in this article, we\u2019ll go a few steps deeper by outlining the framework to quantitatively evaluate topic models through the measure of topic <b>coherence</b> and share the code template in python using Gensim implementation to allow for end ...", "dateLastCrawled": "2022-02-03T00:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Definitions, methods, and applications in interpretable machine ... - <b>PNAS</b>", "url": "https://www.pnas.org/content/116/44/22071", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/116/44/22071", "snippet": "The information produced by an interpretation method should be faithful to <b>the underlying</b> process the practitioner is <b>trying</b> <b>to understand</b>. In the context of ML, there are 2 areas where errors <b>can</b> arise: when approximating <b>the underlying</b> data relationships with a model (predictive accuracy) and when approximating what the model <b>has</b> learned using an interpretation method (descriptive accuracy). For an interpretation to be trustworthy, one should try to maximize both of the accuracies. In ...", "dateLastCrawled": "2022-02-02T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Explainable AI: A Review of Machine Learning <b>Interpretability</b> Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "That <b>said</b>, there <b>has</b> not <b>been</b> a best-in-class method developed to address every need, as most methods focus on either a specific type of model, or a specific type of data, or their scope is either local or global, but not both. Of the methods presented, SHAP is the most complete method, providing explanations for any model and any type of data, doing so at both a global and local scope. However, SHAP is not without shortcomings: The kernel version of SHAP, KernelSHAP, like most permutation ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>Interpretability</b>? | SpringerLink", "url": "https://link.springer.com/article/10.1007/s13347-020-00435-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13347-020-00435-2", "snippet": "Conveniently, a plausible solution to this problem <b>has</b> already <b>been</b> proposed: <b>interpretability</b>. A further problem, however, is that there is little agreement over exactly what <b>interpretability</b> is, an issue we deal with in Section 4. Medical AI Systems Are Explainable. It is possible to explain MAIS using each of the four models outlined above.", "dateLastCrawled": "2022-01-26T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Interpretable machine learning: definitions, methods, and applications</b> ...", "url": "https://www.arxiv-vanity.com/papers/1901.04592/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1901.04592", "snippet": "The information produced by an interpretation method should be faithful to <b>the underlying</b> process the practitioner is <b>trying</b> <b>to understand</b>. In the context of ML, there are two areas where errors <b>can</b> arise: when approximating <b>the underlying</b> data relationships with a model (predictive accuracy) and when approximating what the model <b>has</b> learned using an interpretation method (descriptive accuracy). For an interpretation to be trustworthy, one should try to maximize both of the accuracies. In ...", "dateLastCrawled": "2022-01-11T04:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Explaining the decisions of XGBoost models using counterfactual ...", "url": "https://towardsdatascience.com/explaining-the-decisions-of-xgboost-models-using-counterfactual-examples-fd9c57c83062", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/explaining-the-decisions-of-xgboost-models-using...", "snippet": "Quite different from the industrial fault detection scenario I advertised at the beginning of the blog post, but CF <b>interpretability</b> approaches <b>can</b> be widened to any classification scenario which is composed of a normal class (credit approval), and an \u201cabnormal\u201d one (credit denial). Also, small datasets are nice to showcase something, and the academic world <b>has</b> <b>been</b> using them for decades now (is it a solid argument ?). The dataset allows to learn a mapping between 20 input features such ...", "dateLastCrawled": "2022-01-19T08:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>the interpretation of RMSE, MAE, MPE</b>, MAPE, and MASE in testing ...", "url": "https://www.quora.com/What-is-the-interpretation-of-RMSE-MAE-MPE-MAPE-and-MASE-in-testing-forecasting-accuracy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-interpretation-of-RMSE-MAE-MPE</b>-MAPE-and-MASE-in...", "snippet": "Answer: It is important to evaluate forecast accuracy using genuine forecasts. ... Because the test data is not used in determining the forecasts, it should ... error: MAE= mean(|et|),Root mean squared error: RMSE=\u221amean(e2t). ... the RMSE is also widely used, despite being more difficult to inte...", "dateLastCrawled": "2022-01-26T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A <b>Survey of Methods for Explaining Black Box Models</b>", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3236009", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3236009", "snippet": "In the analysis of the <b>interpretability</b> of predictive models, we <b>can</b> identify a set of dimensions to be taken into consideration, and that characterize the <b>interpretability</b> of the model . Global and Local <b>Interpretability</b> : A model may be completely interpretable, i.e., we are able <b>to understand</b> the whole logic of a model and follow the entire reasoning leading to all the different possible outcomes.", "dateLastCrawled": "2022-01-18T12:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Limitations of <b>Machine Learning</b> | by Matthew Stewart, PhD ...", "url": "https://towardsdatascience.com/the-limitations-of-machine-learning-a00e0c3040c6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-limitations-of-<b>machine-learning</b>-a00e0c3040c6", "snippet": "A neural network <b>can</b> never tell us how to be a good person, and, at least for now, do not <b>understand</b> Newton\u2019s laws of motion or Einstein\u2019s theory of relativity. There are also fundamental limitations grounded in <b>the underlying</b> theory of <b>machine learning</b>, called computational learning theory, which are primarily statistical limitations. We have also discussed issues associated with the scope of the analysis and the dangers of p-hacking, which <b>can</b> lead to spurious conclusions. There are ...", "dateLastCrawled": "2022-02-02T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Practical thoughts on <b>explanatory</b> vs. predictive modeling - Cross Validated", "url": "https://stats.stackexchange.com/questions/1194/practical-thoughts-on-explanatory-vs-predictive-modeling", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/1194", "snippet": "<b>Explanatory</b> model <b>has</b> also <b>been</b> used in medicine and the health area as well, with a very different <b>meaning</b>. Basically what people have as internal beliefs or meanings <b>can</b> be quite different from accepted explanations. For example a religious person may have an <b>explanatory</b> model that an illness was due to punishment or karma for a past behaviour along with accepting th biological reasons as well.", "dateLastCrawled": "2022-01-23T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are the pros and cons of <b>the ARIMA model over regression ... - Quora</b>", "url": "https://www.quora.com/What-are-the-pros-and-cons-of-the-ARIMA-model-over-regression", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-the-pros-and-cons-of-<b>the-ARIMA-model-over-regression</b>", "snippet": "Answer (1 of 4): I am assuming that you didn\u2019t mean \u2018regression\u2019 as [dependent variable] ~ [time] but you meant regression as [dependent variable] ~ [some predictor variables]. If this is not the case then the below answer is not valid. Regression is much more widely used than ARIMA in practical...", "dateLastCrawled": "2022-02-03T06:45:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretability</b> in <b>Machine</b> <b>Learning</b>: An Overview", "url": "https://thegradient.pub/interpretability-in-ml-a-broad-overview/", "isFamilyFriendly": true, "displayUrl": "https://thegradient.pub/<b>interpretability</b>-in-ml-a-broad-overview", "snippet": "First, <b>interpretability</b> in <b>machine</b> <b>learning</b> is useful because it can aid in trust. As humans, we may be reluctant to rely on <b>machine</b> <b>learning</b> models for certain critical tasks, e.g., medical diagnosis, unless we know &quot;how they work.&quot; There&#39;s often a fear of the unknown when trusting in something opaque, which we see when people confront new technology, and this can slow down adoption. Approaches to <b>interpretability</b> that focus on transparency could help mitigate some of these fears.", "dateLastCrawled": "2022-02-01T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "6 \u2013 <b>Interpretability</b> \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "Figure 1: <b>Interpretability</b> for <b>machine</b> <b>learning</b> models bridges the concrete objectives models optimize for and the real-world (and less easy to define) desiderata that ML applications aim to achieve. Introduction . The objectives <b>machine</b> <b>learning</b> models optimize for do not always reflect the actual desiderata of the task at hand. <b>Interpretability</b> in models allows us to evaluate their decisions and obtain information that the objective alone cannot confer. <b>Interpretability</b> takes many forms ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interpretability</b> in <b>Machine</b> <b>Learning</b>", "url": "https://www.cl.cam.ac.uk/teaching/1819/P230/IWML-Lecture-4.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cl.cam.ac.uk/teaching/1819/P230/IWML-Lecture-4.pdf", "snippet": "This provides a novel <b>analogy</b> between data compression and regularization. Qualitative and quantitative state-of-the-art results on three datasets. 20 / 33. Interpretable Lens Variable Model (ILVM) 21 / 33. Interactive <b>Interpretability</b> via Active <b>Learning</b> Interactive \u2018human-in-the-loop\u2019 <b>interpretability</b> Choose the point with index j that maximizes : ^j = argmax jI(s ; ) = H(s ) E q \u02da(z js)[H(s jz j)] = Z p(s j)log p(s j)ds + E q \u02da(z js) Z p (s jjz)log p (s jjz)ds : (5) Choose the point ...", "dateLastCrawled": "2022-01-19T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Towards <b>Analogy</b>-Based Explanations in <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/chapter/10.1007/978-3-030-57524-3_17", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-57524-3_17", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-16T03:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Towards <b>Analogy</b>-Based Explanations in <b>Machine</b> <b>Learning</b>", "url": "https://arxiv.org/abs/2005.12800", "isFamilyFriendly": true, "displayUrl": "https://<b>arxiv</b>.org/abs/2005.12800", "snippet": "Principles of analogical reasoning have recently been applied in the context of <b>machine</b> <b>learning</b>, for example to develop new methods for classification and preference <b>learning</b>. In this paper, we argue that, while analogical reasoning is certainly useful for constructing new <b>learning</b> algorithms with high predictive accuracy, is is arguably not less interesting from an <b>interpretability</b> and explainability point of view. More specifically, we take the view that an <b>analogy</b>-based approach is a ...", "dateLastCrawled": "2021-10-24T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Confusion Matrices</b> &amp; <b>Interpretable ML</b> | by andrea b | high stakes ...", "url": "https://medium.com/high-stakes-design/interpretability-techniques-explained-in-simple-terms-f5e1573674f3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/high-stakes-design/<b>interpretability</b>-techniques-explained-in-simple...", "snippet": "The best [<b>analogy</b>] I can think of is an indicator light in your car \u2014 [and the] <b>machine</b> that you plug in to tell you more about the readout. ANDREA: Do you see <b>interpretability</b>, primarily, as ...", "dateLastCrawled": "2021-03-22T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Economic Methodology Meets Interpretable Machine Learning</b> - Part I ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes", "snippet": "In this series of posts, we will develop an <b>analogy</b> between the realistic assumptions debate in economic methodology and the current discussion over <b>interpretability</b> when using <b>machine</b> <b>learning</b> models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for <b>machine</b> <b>learning</b> theorists and practitioners to avoid analysis paralysis and make progress on the <b>interpretability</b> issue - one way or the other. Intro - Part ...", "dateLastCrawled": "2022-01-22T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Economic Methodology Meets Interpretable <b>Machine</b> <b>Learning</b> ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_intro/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_intro", "snippet": "In this series of posts, we will develop an <b>analogy</b> between the realistic assumptions debate in economic methodology and the current discussion over <b>interpretability</b> when using <b>machine</b> <b>learning</b> models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for <b>machine</b> <b>learning</b> theorists and practitioners to avoid analysis paralysis and make progress on the <b>interpretability</b> issue - one way or the other. But first ...", "dateLastCrawled": "2022-01-05T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Analogies between Biology and Deep <b>Learning</b> [rough note] -- colah&#39;s blog", "url": "http://colah.github.io/notes/bio-analogies/", "isFamilyFriendly": true, "displayUrl": "colah.github.io/notes/bio-analogies", "snippet": "Neuroscience \u2194 <b>Interpretability</b>. <b>Analogy</b>: model=brain. Artificial neural networks are historically inspired by neuroscience, but I used to be pretty skeptical that the connection was anything more than superficial. I&#39;ve since come around: I now think this is a very deep connection. The thing that personally persuaded me was that, in my own investigations of what goes on inside neural networks, we kept finding things that were previously discovered by neuroscientists. The most recent ...", "dateLastCrawled": "2022-01-30T16:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Chris Olah on what the hell is going on inside neural networks - 80,000 ...", "url": "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/", "isFamilyFriendly": true, "displayUrl": "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research", "snippet": "Chris is a <b>machine</b> <b>learning</b> researcher currently focused on neural network interpretability. Until last December he led OpenAI\u2019s interpretability team but along with some colleagues he recently moved on to help start a new AI lab focussed on large models and safety called Anthropic. Rob Wiblin: Before OpenAI he spent 4 years at Google Brain developing tools to visualize what\u2019s going on in neural networks. Chris was hugely impactful at Google Brain. He was second author on the launch of ...", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Marketing AI: Interpretability and Explainability</b> - Christopher S. Penn ...", "url": "https://www.christopherspenn.com/2021/03/marketing-ai-interpretability-and-explainability/", "isFamilyFriendly": true, "displayUrl": "https://www.christopherspenn.com/2021/03/<b>marketing-ai-interpretability-and-explainability</b>", "snippet": "<b>Interpretability is like</b> inspecting the baker\u2019s recipe for the cake. We look at the list of ingredients and the steps taken to bake the cake, and we verify that the recipe makes sense and the ingredients were good. This is a much more rigorous way of validating our results, but it\u2019s the most complete \u2013 and if we\u2019re in a high-stakes situation where we need to remove all doubt, this is the approach we take. Interpretability in AI is like that \u2013 we step through the code itself that ...", "dateLastCrawled": "2022-01-29T12:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Causal <b>Learning</b> From Predictive Modeling for Observational Data", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7931928/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7931928", "snippet": "Given the recent success of <b>machine</b> <b>learning</b>, specifically deep <b>learning</b>, in several applications (Goodfellow et al., ... This statistical <b>interpretability is similar</b> in spirit to traditional interpretability. This allows to answer questions, such as \u201cdoes BMI influence susceptibility to Covid?\u201d Moreover, it has been argued that developing an effective CBN for practical applications requires expert knowledge when data collection is cumbersome (Fenton and Neil, 2012). This applies to ...", "dateLastCrawled": "2021-12-09T23:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Optimal <b>Predictive Clustering</b> - Dimitris Bertsimas", "url": "https://dbertsim.mit.edu/pdfs/papers/2020-sobiesk-optimal-predictive-clustering.pdf", "isFamilyFriendly": true, "displayUrl": "https://dbertsim.mit.edu/pdfs/papers/2020-sobiesk-optimal-<b>predictive-clustering</b>.pdf", "snippet": "Table 1 Comparison of major <b>machine</b> <b>learning</b> methods relative to each other across the metrics of performance (out-of-sample R2), scalability and interpretability. 1 is the best, while 5 is the worst. Optimal <b>Predictive Clustering</b> 3 From Table 1, we observe all existing methods have weakness in at least one category. We therefore seek to design a method that has strong performance in all three categories at the same time. Optimal <b>Predictive Clustering</b> (OPC) is an algorithm that uses mixed ...", "dateLastCrawled": "2021-11-24T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reviewing Challenges of Predicting Protein Melting Temperature Change ...", "url": "https://link.springer.com/article/10.1007/s12033-021-00349-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12033-021-00349-0", "snippet": "Predicting the effects of mutations on protein stability is a key problem in fundamental and applied biology, still unsolved even for the relatively simple case of small, soluble, globular, monomeric, two-state-folder proteins. Many articles discuss the limitations of prediction methods and of the datasets used to train them, which result in low reliability for actual applications despite globally capturing trends. Here, we review these and other issues by analyzing one of the most detailed ...", "dateLastCrawled": "2022-02-03T02:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretable Machine Learning: Advantages and Disadvantages</b> | by ...", "url": "https://towardsdatascience.com/interpretable-machine-learning-advantages-and-disadvantages-901769f48c43", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpretable-machine-learning-advantages-and</b>...", "snippet": "In my view, a shor t coming of interpretable <b>machine</b> <b>learning</b> is that it assumes to a degree that the data being fed into the model is always going to be suitable for human interpretation. This is not necessarily the case. For instance, let\u2019s say that a company is trying to implement interpretable <b>machine</b> <b>learning</b> to devise a credit scoring model, whereby prospective credit card applications are classified as approved or rejected based on numerous features. It is often the case that such ...", "dateLastCrawled": "2022-01-19T03:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Breaking the interpretability barrier - a</b> method for interpreting deep ...", "url": "http://www.di.uniba.it/~loglisci/NFMCP2019/NFMCP/nfMCP2019_paper_17.pdf", "isFamilyFriendly": true, "displayUrl": "www.di.uniba.it/~loglisci/NFMCP2019/NFMCP/nfMCP2019_paper_17.pdf", "snippet": "Last, but not least, <b>interpretability can be thought of as</b> a useful tool for understanding and correcting model errors. In general, we are faced with a trade-o between performance and inter-pretability. Graph classi cation is normally a domain which requires the ap-plication of complex <b>learning</b> models, such as deep neural networks, which are not interpretable by nature. Several relevant attempts have been made to in-terpret complex models post-hoc (brie y reviewed in section2). However, most ...", "dateLastCrawled": "2021-09-22T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Discovering Discriminative Nodes for Classi\ufb01cation with Deep Graph ...", "url": "http://muresanlab.tins.ro/publications/preprints/Palcu_et_al_LNAI_2020.pdf", "isFamilyFriendly": true, "displayUrl": "muresanlab.tins.ro/publications/preprints/Palcu_et_al_LNAI_2020.pdf", "snippet": "Last, but not least, <b>interpretability can be thought of as</b> a useful tool for understanding and correcting model errors. In general, we are faced with a trade-o\ufb00 between performance and inter-pretability. Graph classi\ufb01cation is normally a domain which requires the appli-cation of complex <b>learning</b> models, such as deep neural networks, which are not interpretable by nature. Several relevant attempts have been made to interpret complex models post-hoc (brie\ufb02y reviewed in Sect.2). However ...", "dateLastCrawled": "2021-09-02T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Confronting Abusive Language Online: A Survey from the Ethical and ...", "url": "https://www.arxiv-vanity.com/papers/2012.12305/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2012.12305", "snippet": "The pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. Significant effort in Natural Language Processing (NLP) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. Although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this ...", "dateLastCrawled": "2021-10-13T19:21:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(interpretability)  is like +(trying to understand the underlying meaning or purpose of what has been said)", "+(interpretability) is similar to +(trying to understand the underlying meaning or purpose of what has been said)", "+(interpretability) can be thought of as +(trying to understand the underlying meaning or purpose of what has been said)", "+(interpretability) can be compared to +(trying to understand the underlying meaning or purpose of what has been said)", "machine learning +(interpretability AND analogy)", "machine learning +(\"interpretability is like\")", "machine learning +(\"interpretability is similar\")", "machine learning +(\"just as interpretability\")", "machine learning +(\"interpretability can be thought of as\")", "machine learning +(\"interpretability can be compared to\")"]}