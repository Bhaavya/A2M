{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least Squares Regression</b> - <b>mathsisfun.com</b>", "url": "https://www.mathsisfun.com/data/least-squares-regression.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mathsisfun.com</b>/<b>data</b>/<b>least-squares-regression</b>.html", "snippet": "<b>Least Squares Regression</b> <b>Line</b> of <b>Best</b> Fit. Imagine you have some <b>points</b>, and want to have a <b>line</b> <b>that best</b> <b>fits</b> them <b>like</b> this: We can place the <b>line</b> &quot;by eye&quot;: try to have the <b>line</b> as close as possible to all <b>points</b>, and a similar number of <b>points</b> above and below the <b>line</b>. But for better accuracy let&#39;s see how to calculate the <b>line</b> using <b>Least Squares Regression</b>. The <b>Line</b>. Our aim is to calculate the values m (slope) and b (y-intercept) in <b>the equation</b> <b>of a line</b>: y = mx + b. Where: y = how ...", "dateLastCrawled": "2022-02-03T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>The Least Squares Regression Method</b> \u2013 How to Find the <b>Line</b> of <b>Best</b> Fit", "url": "https://www.freecodecamp.org/news/the-least-squares-regression-method-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.freecodecamp.org/news/<b>the-least-squares-regression-method</b>-explained", "snippet": "In a graph these <b>points</b> look <b>like</b> this: Each point is a student (X, Y) and how long it took that specific student to complete a certain number of topics . Disclaimer: This <b>data</b> is fictional and was made by hitting random keys. I have no idea of the actual values. The formula Y = a + bX. The formula, for those unfamiliar with it, probably looks underwhelming \u2013 even more so given the fact that we already have the values for Y and X in our example. Having said that, and now that we&#39;re not ...", "dateLastCrawled": "2022-02-03T15:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Calculating a <b>Least</b> <b>Squares</b> <b>Regression</b> <b>Line</b>: <b>Equation</b>, Example ...", "url": "https://technologynetworks.com/informatics/articles/calculating-a-least-squares-regression-line-equation-example-explanation-310265", "isFamilyFriendly": true, "displayUrl": "https://technologynetworks.com/informatics/articles/calculating-a-<b>least</b>-<b>squares</b>...", "snippet": "<b>Least</b> <b>squares</b> <b>regression</b> <b>line</b> example Suppose we wanted to estimate a score for someone who had spent exactly 2.3 hours on an essay. I\u2019m sure most of us have experience in drawing lines of <b>best</b> fit , where we <b>line</b> up a ruler, think \u201cthis seems about right\u201d, and draw some lines from the X to the Y axis.", "dateLastCrawled": "2022-01-28T04:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>What Is the Least Squares</b> <b>Regression</b> <b>Line</b>?", "url": "https://www.thoughtco.com/what-is-a-least-squares-line-3126250", "isFamilyFriendly": true, "displayUrl": "https://www.thoughtco.com/what-is-a-<b>least-squares-line</b>-3126250", "snippet": "Since the <b>least squares line</b> minimizes the squared distances between the <b>line</b> and our <b>points</b>, we can think of this <b>line</b> as the one <b>that best</b> <b>fits</b> our <b>data</b>. This is why the <b>least squares line</b> is also known as the <b>line</b> of <b>best</b> fit. Of all of the possible lines that could be drawn, the <b>least squares line</b> is closest to the <b>set</b> <b>of data</b> as a whole.", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Calculating the equation of the least-squares</b> <b>line</b> (practice) | Khan ...", "url": "https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/least-squares-regression/e/calculating-equation-least-squares", "isFamilyFriendly": true, "displayUrl": "https://<b>www.khanacademy.org</b>/math/ap-statistics/bivariate-<b>data</b>-ap/<b>least</b>-<b>squares</b>...", "snippet": "Calculating <b>the equation</b> of a <b>regression</b> <b>line</b>. Practice: <b>Calculating the equation of the least-squares</b> <b>line</b>. This is the currently selected item. Interpreting slope of <b>regression</b> <b>line</b>. Interpreting y-intercept in <b>regression</b> model. Practice: Interpreting slope and y-intercept for linear models. Using <b>least</b> <b>squares</b> <b>regression</b> output.", "dateLastCrawled": "2022-02-02T13:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Least Squares Calculator</b>", "url": "https://www.mathsisfun.com/data/least-squares-calculator.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mathsisfun.com</b>/<b>data</b>/<b>least-squares-calculator</b>.html", "snippet": "<b>Least Squares Calculator</b>. <b>Least</b> <b>Squares</b> <b>Regression</b> is a way of <b>finding</b> a straight <b>line</b> <b>that best</b> <b>fits</b> the <b>data</b>, called the &quot;<b>Line</b> of <b>Best</b> Fit&quot;.. Enter your <b>data</b> as (x, y) pairs, and find <b>the equation</b> <b>of a line</b> <b>that best</b> <b>fits</b> the <b>data</b>.", "dateLastCrawled": "2022-02-02T08:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Scatterplots and regression lines</b> \u2014 Krista King Math | Online math tutor", "url": "https://www.kristakingmath.com/blog/scatterplots-regression-lines", "isFamilyFriendly": true, "displayUrl": "https://www.kristakingmath.com/blog/scatterplots-<b>regression</b>-<b>lines</b>", "snippet": "It\u2019s the <b>line</b> <b>that best</b> shows the trend in the <b>data</b> given in a scatterplot. A <b>regression</b> <b>line</b> is also called the <b>best</b>-fit <b>line</b>, <b>line</b> of <b>best</b> fit, or <b>least</b>-<b>squares</b> <b>line</b>. The <b>regression</b> <b>line</b> is a trend <b>line</b> we use to model a linear trend that we see in a scatterplot, but realize that some <b>data</b> will show a relationship that isn\u2019t necessarily ...", "dateLastCrawled": "2022-02-03T06:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is \u201c<b>Line</b> of <b>Best fit\u201d in linear regression</b>?", "url": "https://www.numpyninja.com/post/what-is-line-of-best-fit-in-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://www.numpyninja.com/post/what-is-<b>line</b>-of-<b>best-fit-in-linear-regression</b>", "snippet": "Simple linear <b>regression</b> is a statistical method that allows us to summarize and study relationships between two variables: One variable is the predictor, explanatory, or independent variable and the other one is the dependent variable. Linear <b>Regression</b> is the process of <b>finding</b> a <b>line</b> <b>that best</b> <b>fits</b> the <b>data</b> <b>points</b> available on the plot, so that we can use it to predict output values for given inputs. So, what is \u201c<b>Best</b> fitting <b>line</b>\u201d? A <b>Line</b> of <b>best</b> fit is a straight <b>line</b> that ...", "dateLastCrawled": "2022-02-03T00:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Linear Regression - Problems with Solutions</b>", "url": "https://www.analyzemath.com/statistics/linear_regression.html", "isFamilyFriendly": true, "displayUrl": "https://www.analyzemath.com/statistics/<b>line</b>ar_<b>regression</b>.html", "snippet": "The <b>least</b> <b>squares</b> <b>regression</b> <b>line</b> is the <b>line</b> that minimizes the sum of the <b>squares</b> (d1 + d2 + d3 + d4) of the vertical deviation from each <b>data</b> point to the <b>line</b> (see figure below as an example of 4 <b>points</b>). Figure 1. Linear <b>regression</b> where the sum of vertical distances d1 + d2 + d3 + d4 between observed and predicted (<b>line</b> and its <b>equation</b>) values is minimized. The <b>least</b> square <b>regression</b> <b>line</b> for the <b>set</b> of n <b>data</b> <b>points</b> is given by <b>the equation</b> <b>of a line</b> in slope intercept form: y = a x ...", "dateLastCrawled": "2022-02-02T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Line of Best Fit</b> in Linear <b>Regression</b> | by ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/line-of-best-fit-in-linear-regression-13658266fbc8", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>line-of-best-fit</b>-in-<b>line</b>ar-<b>regression</b>-13658266fbc8", "snippet": "The <b>Best Fit</b> <b>Line</b>. After <b>finding</b> the correlation between the variables[independent variable and target variable], and if the variables are linearly correlated, we can proceed with the Linear <b>Regression</b> model. The Linear <b>Regression</b> model will find out the <b>best fit</b> <b>line</b> for the <b>data</b> <b>points</b> in the scatter cloud. Let\u2019s learn how to find the <b>best fit</b> <b>line</b>. <b>Equation</b> of Straight <b>Line</b> y=mx+c. m \u2192slope c \u2192intercept. y=x [Slope=1, Intercept=0] -Image by Author Model Coefficient. Slope m and ...", "dateLastCrawled": "2022-02-03T00:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least Square Method</b> - Definition, Graph and Formula", "url": "https://byjus.com/maths/least-square-method/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>least-square-method</b>", "snippet": "The <b>least square method</b> is the process of <b>finding</b> the <b>best</b>-fitting curve or <b>line</b> of <b>best</b> fit for <b>a set</b> <b>of data</b> <b>points</b> by reducing the sum of the <b>squares</b> of the offsets (residual part) of the <b>points</b> from the curve. During the process of <b>finding</b> the relation between two variables, the trend of outcomes are estimated quantitatively. This process is termed as <b>regression</b> analysis.The method of curve fitting is an approach to <b>regression</b> analysis.", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>The Least Squares Regression Method</b> \u2013 How to Find the <b>Line</b> of <b>Best</b> Fit", "url": "https://www.freecodecamp.org/news/the-least-squares-regression-method-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.freecodecamp.org/news/<b>the-least-squares-regression-method</b>-explained", "snippet": "This will help us more easily visualize the formula in action using Chart.js to represent the <b>data</b>. What is <b>the Least Squares Regression method</b> and why use it? <b>Least</b> <b>squares</b> is a method to apply linear <b>regression</b>. It helps us predict results based on an existing <b>set</b> <b>of data</b> as well as clear anomalies in our <b>data</b>. Anomalies are values that are too good, or bad, to be true or that represent rare cases. For example, say we have a list of how many topics future engineers here at freeCodeCamp can ...", "dateLastCrawled": "2022-02-03T15:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Least Squares Regression</b> - <b>mathsisfun.com</b>", "url": "https://www.mathsisfun.com/data/least-squares-regression.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mathsisfun.com</b>/<b>data</b>/<b>least-squares-regression</b>.html", "snippet": "<b>Least Squares Regression</b> <b>Line</b> of <b>Best</b> Fit. Imagine you have some <b>points</b>, and want to have a <b>line</b> <b>that best</b> <b>fits</b> them like this: We can place the <b>line</b> &quot;by eye&quot;: try to have the <b>line</b> as close as possible to all <b>points</b>, and a <b>similar</b> number of <b>points</b> above and below the <b>line</b>. But for better accuracy let&#39;s see how to calculate the <b>line</b> using <b>Least Squares Regression</b>. The <b>Line</b>. Our aim is to calculate the values m (slope) and b (y-intercept) in <b>the equation</b> <b>of a line</b>: y = mx + b. Where: y = how ...", "dateLastCrawled": "2022-02-03T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "Method of <b>least</b> <b>squares</b>. The <b>regression</b> <b>line</b> is obtained using the method of <b>least</b> <b>squares</b>. Any <b>line</b> y = a + bx that we draw through the <b>points</b> gives a predicted or fitted value of y for each value of x in the <b>data</b> <b>set</b>. For a particular value of x the vertical difference between the observed and fitted value of y is known as the deviation, or residual (Fig. (Fig.8). 8). The method of <b>least</b> <b>squares</b> finds the values of a and b that minimise the sum of the <b>squares</b> of all the deviations. This ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Linear Regression</b>-<b>Equation</b>, Formula and Properties", "url": "https://byjus.com/maths/linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>linear-regression</b>", "snippet": "<b>Linear regression</b> determines the straight <b>line</b>, called the <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> or LSRL, <b>that best</b> expresses observations in a bivariate analysis <b>of data</b> <b>set</b>. Suppose Y is a dependent variable, and X is an independent variable, then the population <b>regression</b> <b>line</b> is given by; Y = B 0 +B 1 X. Where. B 0 is a constant. B 1 is the ...", "dateLastCrawled": "2022-02-02T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Regression</b>: <b>Finding</b> <b>the equation</b> of the <b>line</b> of <b>best</b> fit", "url": "http://www.ams.sunysb.edu/~zhu/ams571/Regression.pdf", "isFamilyFriendly": true, "displayUrl": "www.ams.sunysb.edu/~zhu/ams571/<b>Regression</b>.pdf", "snippet": "<b>Finding</b> <b>the equation</b> of the <b>line</b> of <b>best</b> fit Objectives: To find <b>the equation</b> of the <b>least</b> <b>squares</b> <b>regression</b> <b>line</b> of y on x. Background and general principle The aim of <b>regression</b> is to find the linear relationship between two variables. This is in turn translated into a mathematical problem of <b>finding</b> <b>the equation</b> of the <b>line</b> that is closest to all <b>points</b> observed. Consider the scatter plot on the right. One possible <b>line</b> of <b>best</b> fit has been drawn on the diagram. Some of the <b>points</b> lie ...", "dateLastCrawled": "2022-02-02T13:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Section 4.2: <b>Least</b>-<b>Squares</b> <b>Regression</b> - Elgin", "url": "https://faculty.elgin.edu/dkernler/statistics/ch04/4-2.html", "isFamilyFriendly": true, "displayUrl": "https://faculty.elgin.edu/dkernler/statistics/ch04/4-2.html", "snippet": "<b>Finding</b> <b>the Equation</b> for a <b>Line</b>. Another very important skill is <b>finding</b> <b>the equation</b> for a <b>line</b>. In particular, it&#39;s important for us to know how to find <b>the equation</b> when we&#39;re given two <b>points</b>. A very useful <b>equation</b> to know is the point-slope form for a <b>line</b>. The point-slope form <b>of a line</b> is. y - y 1 = m(x - x 1) where m is the slope of the <b>line</b> and (x 1, y 1) is a point on the <b>line</b>. Let&#39;s practice using this form to find an <b>equation</b> for the <b>line</b>. Example 2. In Example 1 from section 4 ...", "dateLastCrawled": "2022-01-31T10:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Linear Regression</b> - Examples, <b>Equation</b>, Formula and Properties", "url": "https://www.vedantu.com/maths/linear-regression", "isFamilyFriendly": true, "displayUrl": "https://www.vedantu.com/maths/<b>linear-regression</b>", "snippet": "The most popular method to fit a <b>regression</b> <b>line</b> in the XY plot is found by using <b>least</b>-<b>squares</b>. This process is used to determine the <b>best</b>-fitting <b>line</b> for the given <b>data</b> by reducing the sum of the <b>squares</b> of the vertical deviations from each <b>data</b> point to the <b>line</b>. If a point rests on the fitted <b>line</b> accurately, then the value of its perpendicular deviation is 0. It is 0 because the variations are first squared, then added, so their positive and negative values will not be cancelled.", "dateLastCrawled": "2022-02-02T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is \u201c<b>Line</b> of <b>Best fit\u201d in linear regression</b>?", "url": "https://www.numpyninja.com/post/what-is-line-of-best-fit-in-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://www.numpyninja.com/post/what-is-<b>line</b>-of-<b>best-fit-in-linear-regression</b>", "snippet": "Simple linear <b>regression</b> is a statistical method that allows us to summarize and study relationships between two variables: One variable is the predictor, explanatory, or independent variable and the other one is the dependent variable. Linear <b>Regression</b> is the process of <b>finding</b> a <b>line</b> <b>that best</b> <b>fits</b> the <b>data</b> <b>points</b> available on the plot, so that we can use it to predict output values for given inputs. So, what is \u201c<b>Best</b> fitting <b>line</b>\u201d? A <b>Line</b> of <b>best</b> fit is a straight <b>line</b> that ...", "dateLastCrawled": "2022-02-03T00:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Topic 13. Analysis of Covariance (ANCOVA, 13. 1. Introduction", "url": "https://psfaculty.plantsciences.ucdavis.edu/agr205/Lectures/2011_Lectures/L13_ANCOVA.pdf", "isFamilyFriendly": true, "displayUrl": "https://psfaculty.plantsciences.ucdavis.edu/agr205/Lectures/2011_Lectures/L13_ANCOVA.pdf", "snippet": "To find <b>the equation</b> of the straight <b>line</b> <b>that best</b> <b>fits</b> a dataset consisting of (X,Y) pairs, we use a strategy which relies on the concept of <b>least</b> <b>squares</b>. For each point in the dataset, we find its vertical distance from the putative <b>best</b>-fit straight <b>line</b>, square this distance, and then add together all the squared distances (i.e. vertical deviations). Of all the lines that could possibly be drawn through the scatter <b>of data</b>, the <b>line</b> of <b>best</b> fit is the one that minimizes this sum ...", "dateLastCrawled": "2022-02-02T02:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>What Is the Least Squares</b> <b>Regression</b> <b>Line</b>?", "url": "https://www.thoughtco.com/what-is-a-least-squares-line-3126250", "isFamilyFriendly": true, "displayUrl": "https://www.<b>thought</b>co.com/what-is-a-<b>least-squares-line</b>-3126250", "snippet": "Since the <b>least squares line</b> minimizes the squared distances between the <b>line</b> and our <b>points</b>, we <b>can</b> think of this <b>line</b> as the one <b>that best</b> <b>fits</b> our <b>data</b>. This is why the <b>least squares line</b> is also known as the <b>line</b> of <b>best</b> fit. Of all of the possible lines that could be drawn, the <b>least squares line</b> is closest to the <b>set</b> <b>of data</b> as a whole.", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Least Squares Regression Line</b> - GitHub Pages", "url": "https://saylordotorg.github.io/text_introductory-statistics/s14-04-the-least-squares-regression-l.html", "isFamilyFriendly": true, "displayUrl": "https://saylordotorg.github.io/.../s14-04-the-<b>least-squares-regression</b>-l.html", "snippet": "It is called the <b>least squares regression line</b> The <b>line</b> <b>that best</b> <b>fits</b> <b>a set</b> of sample <b>data</b> in the sense of minimizing the sum of the squared errors.. Its slope \u03b2 ^ 1 and y-intercept \u03b2 ^ 0 are computed using the formulas. \u03b2 ^ 1 = S S x y S S x x a n d \u03b2 ^ 0 = y-\u2212 \u03b2 ^ 1 x-where. S S x x = \u03a3 x 2 \u2212 1 n (\u03a3 x) 2, S S x y = \u03a3 x y \u2212 1 n (\u03a3 x) (\u03a3 y) x-is the mean of all the x-values, y-is the mean of all the y-values, and n is the number of pairs in the <b>data</b> <b>set</b>. <b>The equation</b> y ...", "dateLastCrawled": "2022-02-02T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How To <b>Run Linear Regressions In Python Scikit-learn</b> - ActiveState", "url": "https://www.activestate.com/resources/quick-reads/how-to-run-linear-regressions-in-python-scikit-learn/", "isFamilyFriendly": true, "displayUrl": "https://www.activestate.com/resources/quick-reads/how-to-run-<b>line</b>ar-<b>regressions</b>-in...", "snippet": "Linear <b>regression</b> <b>can</b> <b>be thought</b> <b>of as finding</b> the straight <b>line</b> <b>that best</b> <b>fits</b> <b>a set</b> of scattered <b>data</b> <b>points</b>: ... Intercept \u2013 the location where the Slope intercepts the Y-axis denoted b in the slope <b>equation</b> y=ax+b. <b>Least</b> <b>Squares</b> \u2013 a method of estimating a <b>Best</b> Fit to <b>data</b>, by minimizing the sum of the <b>squares</b> of the differences between observed and estimated values. Mean \u2013 an av erage of <b>a set</b> of numbers, but in linear <b>regression</b>, Mean is modeled by a linear function. Ordinary ...", "dateLastCrawled": "2022-01-27T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "<b>Regression</b> <b>can</b> be used to find <b>the equation</b> of this <b>line</b>. This <b>line</b> is usually referred to as the <b>regression</b> <b>line</b>. Note that in a scatter diagram the response variable is always plotted on the vertical (y) axis. <b>Equation</b> of a straight <b>line</b> . <b>The equation</b> of a straight <b>line</b> is given by y = a + bx, where the coefficients a and b are the intercept of the <b>line</b> on the y axis and the gradient, respectively. <b>The equation</b> of the <b>regression</b> <b>line</b> for the A&amp;E <b>data</b> (Fig. (Fig.7) 7) is as follows: ln ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Regularization</b> in Machine Learning | by Prashant Gupta | Towards <b>Data</b> ...", "url": "https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>regularization</b>-in-machine-learning-76441ddcf99a", "snippet": "The ridge <b>regression</b> <b>can</b> <b>be thought</b> of as solving an <b>equation</b>, where summation of <b>squares</b> of coefficients is less than or equal to s. And the Lasso <b>can</b> <b>be thought</b> of as an <b>equation</b> where summation of modulus of coefficients is less than or equal to s. Here, s is a constant that exists for each value of shrinkage factor \u03bb.", "dateLastCrawled": "2022-02-02T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Geodesic <b>Regression</b> and the Theory of <b>Least</b> <b>Squares</b> on Riemannian ...", "url": "https://link.springer.com/article/10.1007/s11263-012-0591-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11263-012-0591-y", "snippet": "where \\(d\\) is the geodesic distance between <b>points</b> on \\(M.\\) This <b>equation</b> generalizes the principle of <b>least</b> <b>squares</b> to the metric space setting. Karcher provided conditions guaranteeing the existence and uniqueness of the Fr\u00e9chet mean, which were later improved by Kendall ().Second-order statistics on manifolds, such as principal geodesic analysis (Fletcher et al. 2003), a generalization of principal components analysis to manifolds, and Gaussian covariance estimation (Pennec 2006), have ...", "dateLastCrawled": "2022-01-12T13:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Chapter 3 Correlation and <b>regression</b> | Montana State Introductory ...", "url": "https://mtstateintrostats.github.io/IntroStatTextbook/cor-reg.html", "isFamilyFriendly": true, "displayUrl": "https://mtstateintrostats.github.io/IntroStatTextbook/cor-reg.html", "snippet": "<b>Points</b> that fall horizontally far from the <b>line</b> are <b>points</b> of high leverage; these <b>points</b> <b>can</b> strongly influence the slope of the <b>least</b> <b>squares</b> <b>line</b>. If one of these high leverage <b>points</b> does appear to actually invoke its influence on the slope of the <b>line</b> \u2013 as in Plots C, D, and E of Figures 3.16 and 3.17 \u2013 then we call it an influential point .", "dateLastCrawled": "2022-01-31T04:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is the intuitive meaning of the solution to the problem of <b>finding</b> ...", "url": "https://www.quora.com/What-is-the-intuitive-meaning-of-the-solution-to-the-problem-of-finding-least-squares-linear-regression-coefficients", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-intuitive-meaning-of-the-solution-to-the-problem-of...", "snippet": "Answer (1 of 5): I don&#39;t know if this is a helpful way to look at it for you, but it certainly is different. In Y ~= X * B, we think of B as the operator, sending X to Y. But you could think of X as a transformation too, transforming B into Y. Viewed that way, solving this is like figuring out t...", "dateLastCrawled": "2022-01-25T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What Are Residuals</b>?", "url": "https://www.thoughtco.com/what-are-residuals-3126253", "isFamilyFriendly": true, "displayUrl": "https://www.<b>thought</b>co.com/<b>what-are-residuals</b>-3126253", "snippet": "Linear <b>regression</b> is a statistical tool that determines how well a straight <b>line</b> <b>fits</b> <b>a set</b> of paired <b>data</b>. The straight <b>line</b> <b>that best</b> <b>fits</b> that <b>data</b> is called the <b>least</b> <b>squares</b> <b>regression</b> <b>line</b>. This <b>line</b> <b>can</b> be used in a number of ways. One of these uses is to estimate the value of a response variable for a given value of an explanatory variable. Related to this idea is that of a residual. Residuals are obtained by performing subtraction. All that we must do is to subtract the predicted ...", "dateLastCrawled": "2022-02-02T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Regressions</b> Flashcards | Quizlet", "url": "https://quizlet.com/105311159/regressions-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/105311159/<b>regressions</b>-flash-cards", "snippet": "-<b>Regression</b> is a technique used to make sense of scatterplot <b>data</b> by <b>finding</b> the <b>line</b> <b>that best</b> <b>fits</b> the <b>data</b>.-The <b>regression</b> <b>line</b> tells us the relationship between two variables (x and y). -Are X and Y correlated?-<b>Regression</b> allows us to estimate the coefficients for these lines. Inference and <b>Regressions</b>. Inference helps us determine whether our <b>regression</b> findings (i.e., <b>the equation</b> of the <b>regression</b> <b>line</b>) are statistically valid. Randomized experiments versus observational studies-We ...", "dateLastCrawled": "2022-01-09T15:03:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least Square Method</b> - Definition, Graph and Formula", "url": "https://byjus.com/maths/least-square-method/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>least-square-method</b>", "snippet": "The <b>least square method</b> is the process of <b>finding</b> the <b>best</b>-fitting curve or <b>line</b> of <b>best</b> fit for <b>a set</b> <b>of data</b> <b>points</b> by reducing the sum of the <b>squares</b> of the offsets (residual part) of the <b>points</b> from the curve. During the process of <b>finding</b> the relation between two variables, the trend of outcomes are estimated quantitatively. This process is termed as <b>regression</b> analysis.The method of curve fitting is an approach to <b>regression</b> analysis.", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Least Squares Regression</b> - How to Create <b>Line</b> of <b>Best</b> Fit?", "url": "https://www.wallstreetmojo.com/least-squares-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.wallstreetmojo.com/<b>least-squares-regression</b>", "snippet": "The <b>line</b> of <b>best</b> fit is a straight <b>line</b> drawn through a scatter <b>of data</b> <b>points</b> <b>that best</b> represents the relationship between them. Let us consider the following graph wherein <b>a set</b> <b>of data</b> is plotted along the x and y-axis. These <b>data</b> <b>points</b> are represented using the blue dots. Three lines are drawn through these <b>points</b> \u2013 a green, a red, and a blue <b>line</b>. The green <b>line</b> passes through a single point, and the red <b>line</b> passes through three <b>data</b> <b>points</b>. However, the blue <b>line</b> passes through ...", "dateLastCrawled": "2022-02-03T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "<b>Regression</b> <b>can</b> be used to find <b>the equation</b> of this <b>line</b>. This <b>line</b> is usually referred to as the <b>regression</b> <b>line</b>. Note that in a scatter diagram the response variable is always plotted on the vertical (y) axis. <b>Equation</b> of a straight <b>line</b> . <b>The equation</b> of a straight <b>line</b> is given by y = a + bx, where the coefficients a and b are the intercept of the <b>line</b> on the y axis and the gradient, respectively. <b>The equation</b> of the <b>regression</b> <b>line</b> for the A&amp;E <b>data</b> (Fig. (Fig.7) 7) is as follows: ln ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Ordinary Least Squares Linear Regression: Flaws, Problems and Pitfalls</b> ...", "url": "http://www.clockbackward.com/2009/06/18/ordinary-least-squares-linear-regression-flaws-problems-and-pitfalls/", "isFamilyFriendly": true, "displayUrl": "www.clockbackward.com/2009/06/18/<b>ordinary-least-squares-linear-regression</b>-flaws...", "snippet": "<b>Least</b> <b>squares</b> <b>regression</b> <b>can</b> perform very badly when some <b>points</b> in the training <b>data</b> have excessively large or small values for the dependent variable <b>compared</b> to the rest of the training <b>data</b>. The reason for this is that since the <b>least</b> <b>squares</b> method is concerned with minimizing the sum of the squared error, any training point that has a dependent value that differs a lot from the rest of the <b>data</b> will have a disproportionately large effect on the resulting constants that are being solved ...", "dateLastCrawled": "2022-01-28T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>12.1 Ordinary least squares regression</b> | An Introduction to <b>Data</b> Analysis", "url": "https://michael-franke.github.io/intro-data-analysis/ordinary-least-squares-regression.html", "isFamilyFriendly": true, "displayUrl": "https://michael-franke.github.io/intro-<b>data</b>-analysis/<b>ordinary-least-squares-regression</b>...", "snippet": "<b>12.1 Ordinary least squares regression</b>. This section introduces ordinary <b>least</b> <b>squares</b> (OLS) linear <b>regression</b>. The main idea is that we look for the <b>best</b>-fitting <b>line</b> in a (multi-dimensional) cloud of <b>points</b>, where \u201c<b>best</b>-fitting\u201d is defined in terms of a geometrical measure of distance (squared prediction error). 12.1.1 Prediction without any further information. We are interested in explaining or predicting the murder rates in a city using the murder <b>data</b> <b>set</b>. Concretely, we are ...", "dateLastCrawled": "2022-01-29T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Linear Regression</b> - Examples, <b>Equation</b>, Formula and Properties", "url": "https://www.vedantu.com/maths/linear-regression", "isFamilyFriendly": true, "displayUrl": "https://www.vedantu.com/maths/<b>linear-regression</b>", "snippet": "The most popular method to fit a <b>regression</b> <b>line</b> in the XY plot is found by using <b>least</b>-<b>squares</b>. This process is used to determine the <b>best</b>-fitting <b>line</b> for the given <b>data</b> by reducing the sum of the <b>squares</b> of the vertical deviations from each <b>data</b> point to the <b>line</b>. If a point rests on the fitted <b>line</b> accurately, then the value of its perpendicular deviation is 0. It is 0 because the variations are first squared, then added, so their positive and negative values will not be cancelled.", "dateLastCrawled": "2022-02-02T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>The Regression Equation</b> \u2013 Introductory Business Statistics", "url": "https://opentextbc.ca/introbusinessstatopenstax/chapter/the-regression-equation/", "isFamilyFriendly": true, "displayUrl": "https://opentextbc.ca/introbusinessstatopenstax/chapter/<b>the-regression-equation</b>", "snippet": "<b>Regression</b> analysis is sometimes called \u201c<b>least</b> <b>squares</b>\u201d analysis because the method of determining which <b>line</b> <b>best</b> \u201c<b>fits</b>\u201d the <b>data</b> is to minimize the sum of the squared residuals <b>of a line</b> put through the <b>data</b>. Population <b>Equation</b>: C = \u03b2 0 + \u03b2 1 Income + \u03b5. Estimated <b>Equation</b>: C = b 0 + b 1 Income + e.", "dateLastCrawled": "2022-02-03T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Scatterplots and regression lines</b> \u2014 Krista King Math | Online math tutor", "url": "https://www.kristakingmath.com/blog/scatterplots-regression-lines", "isFamilyFriendly": true, "displayUrl": "https://www.kristakingmath.com/blog/scatterplots-<b>regression</b>-<b>lines</b>", "snippet": "It\u2019s the <b>line</b> <b>that best</b> shows the trend in the <b>data</b> given in a scatterplot. A <b>regression</b> <b>line</b> is also called the <b>best</b>-fit <b>line</b>, <b>line</b> of <b>best</b> fit, or <b>least</b>-<b>squares</b> <b>line</b>. The <b>regression</b> <b>line</b> is a trend <b>line</b> we use to model a linear trend that we see in a scatterplot, but realize that some <b>data</b> will show a relationship that isn\u2019t necessarily ...", "dateLastCrawled": "2022-02-03T06:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Z-12: <b>Correlation and Simple Least Squares Regression</b>", "url": "https://www.westgard.com/lesson42.htm", "isFamilyFriendly": true, "displayUrl": "https://www.westgard.com/lesson42.htm", "snippet": "The objective in simple <b>regression</b> is to generate the <b>best</b> <b>line</b> between the two variables (the tabled values of X and Y), i.e., the <b>best</b> <b>line</b> that <b>fits</b> the <b>data</b> <b>points</b>. <b>Regression</b> uses a formula to calculate the slope, then another formula to calculate the y-intercept, assuming there is a straight <b>line</b> relationship. The <b>best</b> <b>line</b>, or fitted <b>line</b>, is the one that minimizes the distances of the <b>points</b> from the <b>line</b>, as shown in the accompanying figure. Since some of the distances are positive ...", "dateLastCrawled": "2022-01-30T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>The Method of Least Squares</b> | Introduction to Statistics | JMP", "url": "https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-regression/the-method-of-least-squares.html", "isFamilyFriendly": true, "displayUrl": "https://www.jmp.com/.../what-is-<b>regression</b>/<b>the-method-of-least-squares</b>.html", "snippet": "<b>The Method of Least Squares</b>. When we fit a <b>regression</b> <b>line</b> to <b>set</b> of <b>points</b>, we assume that there is some unknown linear relationship between Y and X, and that for every one-unit increase in X, Y increases by some <b>set</b> amount on average. Our fitted <b>regression</b> <b>line</b> enables us to predict the response, Y, for a given value of X. \u03bcY |X = \u03b20 + \u03b21X ...", "dateLastCrawled": "2022-02-03T05:25:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS 189/289A: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189s21/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189s21", "snippet": "LDA vs. logistic <b>regression</b>: advantages and disadvantages. ROC curves. Weighted <b>least</b>-<b>squares</b> <b>regression</b>. <b>Least</b>-<b>squares</b> polynomial <b>regression</b>. Read ISL, Sections 4.4.3, 7.1, 9.3.3; ESL, Section 4.4.1. Optional: here is a fine short discussion of ROC curves\u2014but skip the incoherent question at the top and jump straight to the answer.", "dateLastCrawled": "2022-01-31T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>CS 189/289A</b>: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189", "snippet": "<b>regression</b>: <b>least</b>-<b>squares</b> linear <b>regression</b>, logistic <b>regression</b>, polynomial <b>regression</b>, ridge <b>regression</b>, Lasso; density estimation: maximum likelihood estimation (MLE); dimensionality reduction: principal components analysis (PCA), random projection; and clustering: k-means clustering, hierarchical clustering, spectral graph clustering. Useful Links. Access the <b>CS 189/289A</b> Piazza discussion group. If you want an instructional account, you can get one online. Go to the same link if you ...", "dateLastCrawled": "2022-02-02T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "A difficult <b>regression</b> parameter estimation problem is posed when the data sample is hypothesized to have been generated by more than a single <b>regression</b> model. To find the best-fitting number and ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "LSEbA: <b>least squares regression and estimation by analogy</b> in a semi ...", "url": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "snippet": "In this study, we indicatively applied the ordinary <b>least</b> <b>squares</b> <b>regression</b> and the estimation by <b>analogy</b> technique for the computation of the parametric and non-parametric part, respectively. However, there are lots of other well-known methods that can substitute the abovementioned methods and can be used for evaluation of these components. For example, practitioners may use a robust <b>regression</b> in the computation of the parametric portion of the proposed model in order to have a model less ...", "dateLastCrawled": "2021-12-03T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Big Problem with Linear <b>Regression</b> and How to Solve It | Towards Data ...", "url": "https://towardsdatascience.com/robust-regression-23b633e5d6a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/robust-<b>regression</b>-23b633e5d6a5", "snippet": "Introduction to Robust <b>Regression</b> in <b>Machine</b> <b>Learning</b>. Hussein Abdulrahman . Just now \u00b7 7 min read. The idea behind classic linear <b>regression</b> is simple: draw a \u201cbest-fit\u201d line across the data points that minimizes the mean squared errors: Classic linear <b>regression</b> with ordinary <b>least</b> <b>squares</b>. (Image by author) Looks good. But we don\u2019t always get such clean, well behaved data in real life. Instead, we may get something like this: Same algorithm as above, but now performing poorly due ...", "dateLastCrawled": "2022-02-01T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Machine</b> <b>learning</b> terminology for model building and validation There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear <b>regression</b> with two independent variables is trying to fit the best plane with\u2026", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Trends <b>in artificial intelligence, machine learning, and chemometrics</b> ...", "url": "https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ansa.202000162", "isFamilyFriendly": true, "displayUrl": "https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ansa.202000162", "snippet": "The derived spectra were analyzed for classification and quantification purposes using soft independent modeling of class <b>analogy</b> (SIMCA), artificial neural network (ANN), and partial <b>least</b> <b>squares</b> <b>regression</b> (PLSR). A good classification of tomatoes based on their carotenoid profile of 93% and 100% is shown using SIMCA and ANN, respectively. Besides this result, PLSR and ANN were able to achieve a good quantification of all-", "dateLastCrawled": "2022-02-01T19:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "econometrics - Principle of <b>Analogy</b> and Method of Moments - Cross Validated", "url": "https://stats.stackexchange.com/questions/272803/principle-of-analogy-and-method-of-moments", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/272803/principle-of-<b>analogy</b>-and-method-of...", "snippet": "<b>Least</b> <b>squares</b> estimator in the classical linear <b>regression</b> model is a Method of Moments estimator. The model is. y = X \u03b2 + u. Instead of minimizing the sum of squared residuals, we can obtain the OLS estimator by noting that under the assumptions of the specific model, it holds that (&quot;orhtogonality condition&quot;) E ( X \u2032 u) = 0.", "dateLastCrawled": "2022-01-25T20:40:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bayesian <b>Learning</b> - Rebellion Research", "url": "https://www.rebellionresearch.com/bayesian-learning", "isFamilyFriendly": true, "displayUrl": "https://www.rebellionresearch.com/bayesian-<b>learning</b>", "snippet": "Linear Regression example of <b>machine learning Least Squares Regression can be thought of as</b> a very limited <b>learning</b> algorithm, where the training set consists of a number of x and y data pairs. The task would be trying to predict the y value, and the performance measure would be the sum of the squared differences between the predicted and actual y\u2019s.", "dateLastCrawled": "2022-01-19T02:15:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(least squares regression)  is like +(finding the equation of a line that best fits a set of data points)", "+(least squares regression) is similar to +(finding the equation of a line that best fits a set of data points)", "+(least squares regression) can be thought of as +(finding the equation of a line that best fits a set of data points)", "+(least squares regression) can be compared to +(finding the equation of a line that best fits a set of data points)", "machine learning +(least squares regression AND analogy)", "machine learning +(\"least squares regression is like\")", "machine learning +(\"least squares regression is similar\")", "machine learning +(\"just as least squares regression\")", "machine learning +(\"least squares regression can be thought of as\")", "machine learning +(\"least squares regression can be compared to\")"]}