{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>How to Descend a Hill</b> via <b>Gradient</b> Descent. \u2013 Alan Wise", "url": "https://alanwiseblog.wordpress.com/2018/02/04/how-to-descend-a-hill/", "isFamilyFriendly": true, "displayUrl": "https://alanwiseblog.wordpress.com/2018/02/04/<b>how-to-descend-a-hill</b>", "snippet": "<b>Gradient</b> descent does this by taking small steps <b>down</b> this path until we reached the bottom, e.g. in the example above it took 22498 steps to reach the bottom of the <b>hill</b>. These small steps take a while to compute as we have to calculate the <b>gradient</b> for every direction \u2013 this is fine if we have only one or two directions to choose from but in practice, we could be working in a high dimensional space. In this high dimensional space, the number of directions we\u2019d need to calculate is huge ...", "dateLastCrawled": "2022-01-17T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Walking on an Incline</b>: Benefits, Drawbacks, and How to Start", "url": "https://www.healthline.com/nutrition/walking-on-incline", "isFamilyFriendly": true, "displayUrl": "https://<b>www.healthline.com</b>/nutrition/<b>walking</b>-on-incline", "snippet": "<b>Like</b> any other type of exercise, <b>walking on an incline</b> has some benefits and downsides. This article covers common incline gradients, benefits, downsides, calories burned, and how incline <b>walking</b> ...", "dateLastCrawled": "2022-02-02T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "7 Reasons Why It Is Harder To Cycle Uphill Than Walk \u2013 Bicycles In Motion", "url": "https://bicyclesinmotion.com/why-is-it-harder-to-cycle-uphill-than-walk/", "isFamilyFriendly": true, "displayUrl": "https://bicyclesinmotion.com/why-is-it-harder-to-cycle-up<b>hill</b>-than-walk", "snippet": "The <b>gradient</b> will naturally slow the wheels <b>down</b> and will slow your <b>walking</b> pace <b>down</b> too. <b>A hill</b> with a <b>gradient</b> of 4% will slow a <b>person</b> <b>walking</b> by 38% but slows a cyclist by 75%. So, more momentum is needed to keep moving forward. You will cycle harder. When <b>walking</b>, you swing your arms to generate and keep momentum. This will not require as much energy as you need to propel a bicycle up the <b>hill</b>. As a result, you work harder on a bike, which makes cycling uphill more challenging. Tire ...", "dateLastCrawled": "2022-01-30T18:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>Key to Helping a Person Who Is Depressed</b> - <b>Tiny Buddha</b>", "url": "https://tinybuddha.com/blog/the-key-to-helping-a-person-who-is-depressed/", "isFamilyFriendly": true, "displayUrl": "https://<b>tinybuddha.com</b>/blog/the-<b>key-to-helping-a-person-who-is-depressed</b>", "snippet": "Depression for me <b>is like</b> constantly <b>walking</b> up <b>a hill</b>. Most of the time the <b>hill</b> has only a one percent <b>gradient</b>. You can hardly even tell it\u2019s <b>a hill</b>. I walk, run, jump, skip along, doing cartwheels and stopping to smell pretty flowers and listen to bird-calls; it\u2019s sunny and warm, with clear blue skies. Even though I have to put in a little bit of effort to walk up, times are good. And then something happens in my life, <b>like</b> I lose my job, I have to move, or I\u2019m having ongoing ...", "dateLastCrawled": "2022-01-21T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradients and cycling: how much harder are steeper climbs</b>? - The ...", "url": "https://theclimbingcyclist.com/gradients-and-cycling-how-much-harder-are-steeper-climbs/", "isFamilyFriendly": true, "displayUrl": "https://theclimbingcyclist.com/<b>gradients-and-cycling-how-much-harder-are-steeper-climbs</b>", "snippet": "That is, a <b>gradient</b> of 5% can be plugged in as 0.05. Technically, the value should be the sine of the angle of the slope in degrees. For example, a 5% <b>hill</b> has an angle of 2.86\u00ba, and sin(2.86\u00ba) = 0.049 which is close enough to 0.05. Even at very steep gradients, the percentage <b>gradient</b>/100 approximation is fine (e.g. 20% = 11.31\u00ba. sin(11 ...", "dateLastCrawled": "2022-01-28T12:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Potential uses and <b>benefits of hillwalking as cardiovascular exercise</b> ...", "url": "https://ysjournal.com/potential-uses-and-benefits-of-hillwalking-as-cardiovascular-exercise/", "isFamilyFriendly": true, "displayUrl": "https://ysjournal.com/potential-uses-and-<b>benefits-of-hillwalking-as-cardiovascular</b>...", "snippet": "Potential uses and <b>benefits of hillwalking as cardiovascular exercise</b>. Andrew Wang, 2017. Abstract. The hills are often frequented by all athletes alike, as it is believed that <b>walking</b> up a steeper <b>gradient</b> makes your heart beat faster \u2013 a 24% <b>gradient</b> can increase your heart rate by 55% at a modest speed compared with <b>walking</b> on flat terrain.", "dateLastCrawled": "2022-01-03T06:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Walking</b> Vs Biking <b>Uphill</b> | Pedal Chile", "url": "https://pedalchile.com/blog/uphill", "isFamilyFriendly": true, "displayUrl": "https://pedalchile.com/blog/<b>uphill</b>", "snippet": "<b>A hill</b> with only a 4% <b>gradient</b> will slow a cyclist <b>down</b> by 75%, while the same <b>hill</b> will slow a walker <b>down</b> by 38% at the same power output. Why is cycling <b>uphill</b> harder than <b>walking</b> <b>uphill</b>? Even a small <b>hill</b> can feel <b>like</b> a mountain. (Riding with my friend Josh in DuPont\u2026or in this situation\u2026.<b>walking</b>) \u201c Cycling <b>uphill</b> is merciless, and it immediately has an enormous impact on your body. Your breathing and heart rhythm react to the smallest slope percentage. \u201d \u2014 Paul Van Den Bosch ...", "dateLastCrawled": "2022-01-30T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What Does <b>Walking</b> on an Incline Do for the Body? | <b>Livestrong.com</b>", "url": "https://www.livestrong.com/article/455041-walking-incline-body/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.livestrong.com</b>/article/455041-<b>walking</b>-incline-body", "snippet": "A 205-pound <b>person</b> burns 558 calories per hour <b>walking</b> uphill at a 15 degree angle. Advertisement When <b>walking</b> uphill on a treadmill, avoid supporting your body weight with your arms, as this can counteract the advantage of incline. The American Council on Exercise states that incline <b>walking</b> is one of the best activities for burning fat and shaping muscles. ...", "dateLastCrawled": "2022-02-03T03:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Effect of uphill and <b>downhill walking on walking performance</b> in ...", "url": "https://www.researchgate.net/publication/310745335_Effect_of_uphill_and_downhill_walking_on_walking_performance_in_geriatric_patients_using_a_wheeled_walker", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/310745335_Effect_of_up<b>hill</b>_and_<b>downhill</b>...", "snippet": "of <b>walking</b> with calming <b>down</b> <b>walking</b>. In our study this is supported by the . decrease in gai t speed and it m ight be. supported by low er electromyogr aphic. activity of lower limb muscles ...", "dateLastCrawled": "2022-01-13T19:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Top ten <b>toughest cycling climbs in the Kent</b> Alps - <b>Broleur</b>", "url": "https://www.broleur.com/top-10-climbs-in-the-kent-alps/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>broleur</b>.com/top-10-climbs-in-the-kent-alps", "snippet": "The first part is fine and dandy but then the <b>gradient</b> starts to wear you <b>down</b> after you take a left, about two-thirds of the way up. By the time you reach the sign for the village, you think you&#39;ve conquered it, but it keeps going and going. Plus there always seems to be an audience of old ladies outside the bus stop at the Village Hall to witness your expletives combined with the inevitable glob of spittle on your chin. OUT Rowdow <b>Hill</b>. OUT Exedown <b>Hill</b>. OUT Old Polhill. If I thought Yorks ...", "dateLastCrawled": "2022-01-30T08:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Kinematic and kinetic comparison of downhill and level <b>walking</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/026800339592043L", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/026800339592043L", "snippet": "A ramp of 6 m length and a <b>gradient</b> of \u221219% was used for downhill <b>walking</b> and this incorporated the same force platform that was used for level <b>walking</b>. Planar net joint moments and mechanical power at the ankle, knee, and hip joints were calculated for the sagittal view using force platform and video records based on standard inverse dynamics procedures. On the basis of differences in ankle, knee, and hip joint kinematics the ankle joint was seen to compensate for the <b>gradient</b> at push off ...", "dateLastCrawled": "2021-11-19T21:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Energy cost of <b>walking</b> and running at extreme uphill and downhill ...", "url": "https://journals.physiology.org/doi/full/10.1152/japplphysiol.01177.2001", "isFamilyFriendly": true, "displayUrl": "https://journals.physiology.org/doi/full/10.1152/japplphysiol.01177.2001", "snippet": "The results show that1) the minimum in energy cost <b>is similar</b> in <b>walking</b> and running at \u223c0.10\u20130.20 downhill <b>gradient</b>; 2) the optimum <b>gradient</b> for mountain paths is close to 0.20\u20130.30, both uphill and downhill, for the two gaits; 3) a better progression economy is expected in mountain-running athletes in the downhill range; and 4) the running speeds adopted in downhill competition are far lower than metabolically feasible, mainly because of safety reasons. If athletes wish to improve ...", "dateLastCrawled": "2022-01-25T14:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The influence of slope on <b>walking activity and the pedestrian modal</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2352146517309924", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2352146517309924", "snippet": "This simplification is not entirely correct, but the potential positive utility of <b>walking</b> downhill is certainly lower than the disutility of <b>walking</b> uphill on a <b>similar</b> slope. Depending on the type of <b>person</b>, <b>walking</b> on a flat surface might actually be more comfortable than experiencing a negative slope despite the theoretical energetic advantage in the latter case. Furthermore, at a certain point going downhill ceases to be energetically advantageous since increasing one\u00e2\u20ac\u2122s speed ...", "dateLastCrawled": "2022-01-26T19:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradients and cycling: how much harder are steeper climbs</b>? - The ...", "url": "https://theclimbingcyclist.com/gradients-and-cycling-how-much-harder-are-steeper-climbs/", "isFamilyFriendly": true, "displayUrl": "https://theclimbingcyclist.com/<b>gradients-and-cycling-how-much-harder-are-steeper-climbs</b>", "snippet": "That is, a <b>gradient</b> of 5% can be plugged in as 0.05. Technically, the value should be the sine of the angle of the slope in degrees. For example, a 5% <b>hill</b> has an angle of 2.86\u00ba, and sin(2.86\u00ba) = 0.049 which is close enough to 0.05. Even at very steep gradients, the percentage <b>gradient</b>/100 approximation is fine (e.g. 20% = 11.31\u00ba. sin(11 ...", "dateLastCrawled": "2022-01-28T12:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Walking</b> a Mile at a Full <b>Incline</b> Vs. Jogging Flat on the Treadmill ...", "url": "https://healthyliving.azcentral.com/walking-mile-full-incline-vs-jogging-flat-treadmill-9765.html", "isFamilyFriendly": true, "displayUrl": "https://<b>healthyliving.azcentral.com</b>/<b>walking</b>-mile-full-<b>incline</b>-vs-jogging-flat...", "snippet": "<b>Walking</b> one mile on a treadmill that is set to its maximum <b>incline</b> is a more rigorous exercise than jogging one mile on a flat treadmill. <b>Walking</b> uphill is so difficult that <b>walking</b> up <b>a hill</b> with a 15 percent <b>incline</b> burns 67 percent more calories than <b>walking</b> on a level surface, although <b>walking</b> uphill reduces speed and faster people burn more calories, according to \u201cThe Complete Guide to <b>Walking</b> for Health, Weight Loss, and Fitness.\u201d", "dateLastCrawled": "2022-02-02T13:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Regents Earth ScienceMr. ParisPace High School - Class Notes", "url": "http://mrparispace.weebly.com/uploads/2/1/3/2/21329598/2016_contour_map_answer.pdf", "isFamilyFriendly": true, "displayUrl": "mrparispace.weebly.com/uploads/2/1/3/2/21329598/2016_contour_map_answer.pdf", "snippet": "<b>Hill</b> Darry Lin Lake 4 miles Contour interval = 10 feet Nasus Hil Black Creek 25 Key Paved roads Unpaved roads swamp 4. What is a possible elevation for the surface of Darry Lin Lake? A) 228 feet B) 242 feet C) 255 feet D) 268 feet 5. <b>What is the</b> approximate <b>gradient</b> from point A to point B on the map? A) 25 feet per mile C) 75 feet per mile B) 50 feet per mile D) 100 feet per mile D) northwest 6. In which general direction does Red Creek flow? A) northeast B) southeast C) southwest . 23 ...", "dateLastCrawled": "2022-01-29T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Incline Sprinting \u2013 Using Hills, Ramps, Slopes, and Treadmills</b> for ...", "url": "https://simplifaster.com/articles/incline-sprinting-hills-ramps-treadmills/", "isFamilyFriendly": true, "displayUrl": "https://simplifaster.com/articles/incline-sprinting-<b>hills</b>-ramps-treadmills", "snippet": "The run is the horizontal component, and the rise is the vertical distance up from the start to end. For geometry fans, the hypotenuse is the slope length, or the actual running distance of the incline. I tend to use the term \u201cslope\u201d for naturally existing grass hills, and \u201cramps\u201d and \u201cinclines\u201d for human-made structures.", "dateLastCrawled": "2022-01-25T20:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Walk Up HIlls More</b> Easily... - Outdoors Magic", "url": "https://outdoorsmagic.com/article/walk-hills-easily/", "isFamilyFriendly": true, "displayUrl": "https://outdoorsmagic.com/article/walk-<b>hills</b>-easily", "snippet": "You\u2019re using the same muscle groups in a very <b>similar</b> way and the intensity of <b>hill</b> running will actually improve your ability to operate at a lower level too. Don\u2019t fret if you don\u2019t have many hills to run up, even running on the flat will help improve your general <b>hill</b> fitness as you\u2019ll be working significantly harder than you would be <b>walking</b> over the same terrain. If you do take up running, buy proper running shoes from a specialist running shop and don\u2019t increase your mileage ...", "dateLastCrawled": "2022-01-27T01:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>10 Magnetic Hills, Gravity Roads, and Mystery Spots</b> | <b>Mental Floss</b>", "url": "https://www.mentalfloss.com/article/61636/10-magnetic-hills-gravity-roads-and-mystery-spots", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mentalfloss.com</b>/article/61636/<b>10-magnetic-hills-gravity-roads-and</b>-mystery...", "snippet": "Spook <b>Hill</b> of Lake Wales, Florida, claims, according to local legend, that its gravity <b>hill</b> (an exterior optical illusion created by the land surrounding a road) was created by the ghost of either ...", "dateLastCrawled": "2022-02-02T01:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why does going up the <b>slope take more effort than going down</b>? - Quora", "url": "https://www.quora.com/Why-does-going-up-the-slope-take-more-effort-than-going-down", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-does-going-up-the-<b>slope-take-more-effort-than-going-down</b>", "snippet": "Answer (1 of 2): It\u2019s true that going up a slope takes <b>more effort than going down</b>. The plot below is taken from this excellent article on the subject. If the human body were 100% efficient, it would store energy while going downhill and release that energy to go uphill. The plot above would be ...", "dateLastCrawled": "2022-01-17T01:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>How to Descend a Hill</b> via <b>Gradient</b> Descent. \u2013 Alan Wise", "url": "https://alanwiseblog.wordpress.com/2018/02/04/how-to-descend-a-hill/", "isFamilyFriendly": true, "displayUrl": "https://alanwiseblog.wordpress.com/2018/02/04/<b>how-to-descend-a-hill</b>", "snippet": "Stochastic <b>gradient</b> descent <b>can</b> <b>be thought</b> of a drunk <b>person</b> <b>walking</b> <b>down</b> <b>a hill</b>. Imagine at the top of the <b>hill</b> there were the STOR-i arms from my post about Bandits. After a good night, we try to descend <b>down</b> the <b>hill</b>. A sober <b>person</b>, i.e. the full <b>gradient</b> descent method, will try to carefully descend the <b>hill</b> very slowly by taking calculated steps. Our wasted <b>person</b>, i.e. the stochastic <b>gradient</b> descent method, will descend the <b>hill</b> much faster due to not calculating every step they need ...", "dateLastCrawled": "2022-01-17T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Applying the Bradford <b>Hill</b> criteria in the 21st century: how data ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4589117/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4589117", "snippet": "Biological <b>gradient</b> is an example of how data integration <b>can</b> complicate causal inference. New tools and technical capabilities have allowed researchers to characterize a variety of low-level molecular endpoints that may not lead to disease or observable adverse outcomes on a larger scale. For example, innate responses <b>can</b> repair, eliminate, or reverse molecular changes caused by low levels of exposure. Thus, molecular changes within the no-observable-adverse-effect level (NOAEL) may not ...", "dateLastCrawled": "2022-02-03T03:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>the easiest way to estimate a hill gradient on</b> roads? for <b>hill</b> ...", "url": "http://www.attackpoint.org/discussionthread.jsp/message_217066", "isFamilyFriendly": true, "displayUrl": "www.attackpoint.org/discussionthread.jsp/message_217066", "snippet": "You could take multiple values of H for different spots in the <b>hill</b> (say <b>walking</b> 20 paces uphill) so you get a clear picture of the <b>gradient</b> variation. The wider the bottle, less precision is lost at higher gradients. So you <b>can</b> also take your bottle to your favourite/known <b>gradient</b> <b>hill</b> and mark the <b>gradient</b> with a red pen, so whenever you take your bottle to an unknown <b>hill</b> you <b>can</b> assess wether the new <b>hill</b> is steeper/less steep than the one you know. Nov 11, 2008 1:56 AM # Ricka: I&#39;ve ...", "dateLastCrawled": "2021-12-28T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradients and cycling: how much harder are steeper climbs</b>? - The ...", "url": "https://theclimbingcyclist.com/gradients-and-cycling-how-much-harder-are-steeper-climbs/", "isFamilyFriendly": true, "displayUrl": "https://theclimbingcyclist.com/<b>gradients-and-cycling-how-much-harder-are-steeper-climbs</b>", "snippet": "That is, a <b>gradient</b> of 5% <b>can</b> be plugged in as 0.05. Technically, the value should be the sine of the angle of the slope in degrees. For example, a 5% <b>hill</b> has an angle of 2.86\u00ba, and sin(2.86\u00ba) = 0.049 which is close enough to 0.05. Even at very steep gradients, the percentage <b>gradient</b>/100 approximation is fine (e.g. 20% = 11.31\u00ba. sin(11 ...", "dateLastCrawled": "2022-01-28T12:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "A <b>person</b> is stuck in the mountains and is trying to get <b>down</b> (i.e., trying to find the global minimum). There is heavy fog such that visibility is extremely low. Therefore, the path <b>down</b> the mountain is not visible, so they must use local information to find the minimum. They <b>can</b> use the method of <b>gradient descent</b>, which involves looking at the steepness of the <b>hill</b> at their current position, then proceeding in the direction with the steepest descent (i.e., downhill). If they were trying to ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Character move on slope problem</b> - Unity Forum", "url": "https://forum.unity.com/threads/character-move-on-slope-problem.311112/", "isFamilyFriendly": true, "displayUrl": "https://forum.unity.com/threads/<b>character-move-on-slope-problem</b>.311112", "snippet": "(because addforce will stuck when <b>walking</b> on slope) if it comes to end of up-slope or to <b>down</b>-slope in a little bit high speed, it will fly off ground, how <b>can</b> I keep my character always on ground, I had think if I <b>can</b> not using rigidbody things, but the game will have some moving platforms, so, just use raycast behind character to make character position to hitpoint, should not work.(because it will not move with platform) so it must have rigidbody physics, and if I use strongly gravity ...", "dateLastCrawled": "2022-02-02T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Regents Earth ScienceMr. ParisPace High School - Class Notes", "url": "http://mrparispace.weebly.com/uploads/2/1/3/2/21329598/2016_contour_map_answer.pdf", "isFamilyFriendly": true, "displayUrl": "mrparispace.weebly.com/uploads/2/1/3/2/21329598/2016_contour_map_answer.pdf", "snippet": "<b>Hill</b> Darry Lin Lake 4 miles Contour interval = 10 feet Nasus Hil Black Creek 25 Key Paved roads Unpaved roads swamp 4. What is a possible elevation for the surface of Darry Lin Lake? A) 228 feet B) 242 feet C) 255 feet D) 268 feet 5. <b>What is the</b> approximate <b>gradient</b> from point A to point B on the map? A) 25 feet per mile C) 75 feet per mile B) 50 feet per mile D) 100 feet per mile D) northwest 6. In which general direction does Red Creek flow? A) northeast B) southeast C) southwest . 23 ...", "dateLastCrawled": "2022-01-29T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Psychology Unit 4: Sensation and Perception - <b>Quizlet</b>", "url": "https://quizlet.com/ca/512346422/psychology-unit-4-sensation-and-perception-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/ca/512346422/psychology-unit-4-sensation-and-perception-flash-cards", "snippet": "Sensory adaptation <b>can</b> explain all of the following EXCEPT: a.getting used to the touch of your clothes on your skin. b.getting used to the smell of the perfume you are wearing. c.hearing your name spoken in a noisy room. d.feeling comfortable in a cold swimming pool after being in for a few minutes. c. The purity of a wavelength of light corresponds to the perception of: a.hue. b.color constancies. c.saturation. d.brightness. c. Courtney wears glasses to correct the far-sightedness in her ...", "dateLastCrawled": "2022-02-03T06:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Incline Sprinting \u2013 Using Hills, Ramps, Slopes, and Treadmills</b> for ...", "url": "https://simplifaster.com/articles/incline-sprinting-hills-ramps-treadmills/", "isFamilyFriendly": true, "displayUrl": "https://simplifaster.com/articles/incline-sprinting-<b>hills</b>-ramps-treadmills", "snippet": "<b>Incline Sprinting \u2013 Using Hills, Ramps, Slopes, and Treadmills</b> for Speed. For some reason, the good old-fashioned <b>hill</b> is slowly dying out from many training programs. Perhaps the advent of sleds has turned a traditional staple into a less relevant option, or the difficulty in evaluating training sessions is the cause.", "dateLastCrawled": "2022-01-25T20:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "ELI5: What&#39;s the maximum <b>gradient</b> a road <b>can</b> be before a car&#39;s tyres ...", "url": "https://www.reddit.com/r/explainlikeimfive/comments/3dfnt0/eli5_whats_the_maximum_gradient_a_road_can_be/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/3dfnt0/eli5_whats_the_maximum_<b>gradient</b>_a_road_<b>can</b>_be", "snippet": "There is a certain slope where the car will no longer be able to maintain grip on the road and begin to slide <b>down</b> the <b>hill</b>. This also is dramatically altered if the road surface is wet or icy. A lot of how you work out what that balance point is relies on how those specific tires interact with that specific stretch of road. But for this reason, there are standards for how much grip a tire must have, and because that amount of grip relies on the condition of the road, the roads also must be ...", "dateLastCrawled": "2021-08-15T12:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Kinematic and kinetic comparison of downhill and level <b>walking</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/026800339592043L", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/026800339592043L", "snippet": "Indeed, the work by the knee extensors of a 70 kg <b>person</b> <b>walking</b> downhill in the present study was calculated to be 78 21 J <b>compared</b> to 29 16 J for level <b>walking</b>. The muscle power absorption requirement of the knee extensors observed in the present study for downhill <b>walking</b> is comparable to that previously reported for running on a level surface3. Furthermore, electromyographic (EMG) analyses of <b>walking</b> consistently demonstrate a degree of coactivity of the hamstring and quadriceps muscle ...", "dateLastCrawled": "2021-11-19T21:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Walking</b> a Mile at a Full <b>Incline</b> Vs. Jogging Flat on the Treadmill ...", "url": "https://healthyliving.azcentral.com/walking-mile-full-incline-vs-jogging-flat-treadmill-9765.html", "isFamilyFriendly": true, "displayUrl": "https://<b>healthyliving.azcentral.com</b>/<b>walking</b>-mile-full-<b>incline</b>-vs-jogging-flat...", "snippet": "<b>Walking</b> one mile on a treadmill that is set to its maximum <b>incline</b> is a more rigorous exercise than jogging one mile on a flat treadmill. <b>Walking</b> uphill is so difficult that <b>walking</b> up <b>a hill</b> with a 15 percent <b>incline</b> burns 67 percent more calories than <b>walking</b> on a level surface, although <b>walking</b> uphill reduces speed and faster people burn more calories, according to \u201cThe Complete Guide to <b>Walking</b> for Health, Weight Loss, and Fitness.\u201d", "dateLastCrawled": "2022-02-02T13:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Gradients and cycling: how much harder are steeper climbs</b>? - The ...", "url": "https://theclimbingcyclist.com/gradients-and-cycling-how-much-harder-are-steeper-climbs/", "isFamilyFriendly": true, "displayUrl": "https://theclimbingcyclist.com/<b>gradients-and-cycling-how-much-harder-are-steeper-climbs</b>", "snippet": "That is, a <b>gradient</b> of 5% <b>can</b> be plugged in as 0.05. Technically, the value should be the sine of the angle of the slope in degrees. For example, a 5% <b>hill</b> has an angle of 2.86\u00ba, and sin(2.86\u00ba) = 0.049 which is close enough to 0.05. Even at very steep gradients, the percentage <b>gradient</b>/100 approximation is fine (e.g. 20% = 11.31\u00ba. sin(11 ...", "dateLastCrawled": "2022-01-28T12:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is <b>the easiest way to estimate a hill gradient on</b> roads? for <b>hill</b> ...", "url": "http://www.attackpoint.org/discussionthread.jsp/message_217066", "isFamilyFriendly": true, "displayUrl": "www.attackpoint.org/discussionthread.jsp/message_217066", "snippet": "You could take multiple values of H for different spots in the <b>hill</b> (say <b>walking</b> 20 paces uphill) so you get a clear picture of the <b>gradient</b> variation. The wider the bottle, less precision is lost at higher gradients. So you <b>can</b> also take your bottle to your favourite/known <b>gradient</b> <b>hill</b> and mark the <b>gradient</b> with a red pen, so whenever you take your bottle to an unknown <b>hill</b> you <b>can</b> assess wether the new <b>hill</b> is steeper/less steep than the one you know. Nov 11, 2008 1:56 AM # Ricka: I&#39;ve ...", "dateLastCrawled": "2021-12-28T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Tips for Hiking Downhill</b> | The Hiking Life", "url": "https://www.thehikinglife.com/2020/05/tips-for-hiking-downhill/", "isFamilyFriendly": true, "displayUrl": "https://www.thehikinglife.com/2020/05/<b>tips-for-hiking-downhill</b>", "snippet": "3. Minimize Stress Due to the force of gravity, hiking downhill takes significantly more toll on our bodies than other types of <b>walking</b>. According to a study by pub-med.gov, the \u201cincreased moment and knee flexion angle yield a 3 to 4 times bigger femoropatellar joint compressive force for downhill <b>walking</b> <b>compared</b> to level <b>walking</b>.\u201d. That being the case, how do we minimize weight-bearing impact while descending?", "dateLastCrawled": "2022-01-29T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What Does <b>Walking</b> on an Incline Do for the Body? | <b>Livestrong.com</b>", "url": "https://www.livestrong.com/article/455041-walking-incline-body/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.livestrong.com</b>/article/455041-<b>walking</b>-incline-body", "snippet": "When <b>compared</b> to running, <b>walking</b> on a flat terrain is a lower intensity activity. Unlike running, <b>walking</b> places a lower level of strain on your lower body joints. Video of the Day Tip. <b>Walking</b> uphill increases the intensity of your workout and helps tone your leg muscles. Uphill-<b>Walking</b> Muscles The movement of <b>walking</b> utilizes many muscle groups. As your leg extends forward, your quadriceps, which are the front of your thighs, contract to extend, or straighten your leg at the knee joint ...", "dateLastCrawled": "2022-02-03T03:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Do You Burn More Calories <b>Walking</b> Uphill Rather Than on Flat Ground ...", "url": "https://healthyliving.azcentral.com/burn-calories-walking-uphill-rather-flat-ground-5420.html", "isFamilyFriendly": true, "displayUrl": "https://<b>healthyliving.azcentral.com</b>/burn-calories-<b>walking</b>-up<b>hill</b>-rather-flat-ground...", "snippet": "<b>Walking</b> uphill causes you to breathe harder, upping your heart rate. Not only will you burn more calories <b>walking</b> uphill instead of on a flat surface, you&#39;ll also tone your legs and buttocks. Since the added incline is more stressful on your body, warm up and cool <b>down</b>, and wear supportive shoes to minimize risk of injury.", "dateLastCrawled": "2022-01-25T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 7, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Naismith&#39;s rule</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Naismith%27s_rule", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Naismith&#39;s_rule</b>", "snippet": "<b>Naismith&#39;s rule</b> helps with the planning of a <b>walking</b> or hiking expedition by calculating how long it will take to travel the intended route, including any extra time taken when <b>walking</b> uphill. This rule of thumb was devised by William W. Naismith, a Scottish mountaineer, in 1892. A modern version <b>can</b> be formulated as follows: Allow one hour for every 3 miles (5 km) forward, plus an additional hour for every 2,000 feet (600 m) of ascent.", "dateLastCrawled": "2022-02-03T06:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The influence of slope on <b>walking activity and the pedestrian modal</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2352146517309924", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2352146517309924", "snippet": "In addition, occasionally pedestrians are omitted when they are covered by another <b>person</b> <b>walking</b> in the same direction. The data are corrected for these occurrences by applying a fixed correction factor, which was determined by comparing sensor data with manual counts. The sensor is located on a sloped alleyway called \u00e2\u20ac\u0153Weinbergfussweg\u00e2\u20ac , which is located between tram stops \u00e2\u20ac\u0153Central\u00e2\u20ac and \u00e2\u20ac\u0153Haldenegg\u00e2\u20ac , roughly halfway on the main important pedestrian connection ...", "dateLastCrawled": "2022-01-26T19:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why does going up the <b>slope take more effort than going down</b>? - Quora", "url": "https://www.quora.com/Why-does-going-up-the-slope-take-more-effort-than-going-down", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-does-going-up-the-<b>slope-take-more-effort-than-going-down</b>", "snippet": "Answer (1 of 2): It\u2019s true that going up a slope takes <b>more effort than going down</b>. The plot below is taken from this excellent article on the subject. If the human body were 100% efficient, it would store energy while going downhill and release that energy to go uphill. The plot above would be ...", "dateLastCrawled": "2022-01-17T01:47:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Gradient</b> Descent for <b>Machine</b> <b>Learning</b> <b>Gradient</b> Descent for <b>Machine</b> <b>Learning</b>", "url": "http://www.bel.utcluj.ro/dce/didactic/eai/04_GradientDescent_ML.pdf", "isFamilyFriendly": true, "displayUrl": "www.bel.utcluj.ro/dce/didactic/eai/04_<b>Gradient</b>Descent_ML.pdf", "snippet": "<b>Gradient</b> Descent for <b>Machine</b> <b>Learning</b> Elements of Artificial Intelligence G. Oltean 3 / 28 <b>Gradient</b> Descent Algorithm (GDA) - <b>Analogy</b> A person is stuck in the mountains and is trying to get down (i.e. trying to find the global minimum). There is heavy fog such that visibility is extremely low. Therefore, the path down the mountain is not", "dateLastCrawled": "2022-02-03T10:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> 101: An Intuitive Introduction to <b>Gradient</b> Descent ...", "url": "https://towardsdatascience.com/machine-learning-101-an-intuitive-introduction-to-gradient-descent-366b77b52645", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine</b>-<b>learning</b>-101-an-intuitive-introduction-to...", "snippet": "<b>Gradient</b> descent is, with no doubt, the heart and soul of most <b>Machine</b> <b>Learning</b> (ML) algorithms. I definitely believe that you should take the time to understanding it. Because once you do, for starters, you will better comprehend how most ML algorithms work. Besides, understanding basic concepts is key for developing intuition about more complicated subjects.", "dateLastCrawled": "2022-01-30T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> with Spreadsheets! Part 1: <b>Gradient</b> Descent and ...", "url": "https://medium.com/excel-with-ml/machine-learning-with-spreadsheets-part-1-gradient-descent-f9316676db9b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/excel-with-ml/<b>machine</b>-<b>learning</b>-with-spreadsheets-part-1-<b>gradient</b>...", "snippet": "<b>Gradient</b> descent: Step-by-step spreadsheets show you how machines learn without the code. Go under the hood with backprop, partial derivatives, and <b>gradient</b> descent.", "dateLastCrawled": "2022-01-29T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Understanding Machine Learning by Analogy</b> with a Simple Contour Map ...", "url": "https://contemplations.blog/machine-learning-analogy-countour-map/", "isFamilyFriendly": true, "displayUrl": "https://<b>contemplations</b>.blog/<b>machine</b>-<b>learning</b>-<b>analogy</b>-countour-map", "snippet": "The Basis for <b>Machine</b> <b>Learning</b> by <b>Analogy</b>, Using a Contour Map. In this post, we will take a closer look at <b>Machine</b> <b>Learning</b> and its nephew, Deep <b>Learning</b>. There is no \u201c<b>Learning</b>\u201d (in the human sense) in either <b>Machine</b> <b>learning</b> or Deep <b>Learning</b>, there are only quite simple and readily available mathematical procedures which allow us to adapt parameters of many kinds of parameterized systems (or networks), such as a neural network, in such a way that the system (or network), together with ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Gradient</b> Descent for <b>machine</b> <b>learning</b> \u2013 Dhruv Sharma", "url": "https://dhruvsharma196205867.wordpress.com/2020/09/20/gradient-descent-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://dhruvsharma196205867.wordpress.com/.../09/20/<b>gradient</b>-descent-for-<b>machine</b>-<b>learning</b>", "snippet": "<b>Gradient</b> Descent for <b>machine</b> <b>learning</b>. Posted by dhruvsharma845 September 20, 2020 September 28, 2020 Posted in Uncategorized Tags: gd, gradientdescent. Where does <b>gradient</b> descent fit into the picture? Most <b>machine</b> <b>learning</b> problems involve the training data and the task of optimising some function f(x). If it is a supervised <b>learning</b> task, optimisation refers to approximating a function which maps training features to target labels as closely as possible or in other words, finding the ...", "dateLastCrawled": "2021-12-31T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A <b>beautiful Analogy : Understanding gradient descent algorithm for</b> ...", "url": "https://www.linkedin.com/pulse/beautiful-analogy-understanding-gradient-descent-algorithm-jain", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/<b>beautiful-analogy-understanding-gradient-descent</b>...", "snippet": "<b>Gradient</b> descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the <b>gradient</b>. In <b>machine</b> ...", "dateLastCrawled": "2021-08-10T13:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - How does <b>Gradient</b> Descent work? - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/102509/how-does-gradient-descent-work", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/102509/how-does-<b>gradient</b>-descent-work", "snippet": "I know the calculus and the famous hill and valley <b>analogy</b> (so to say) of <b>gradient</b> descent. However, I find the update rule of the weights and biases quite terrible. Let&#39;s say we have a couple of parameters, one weight &#39;w&#39; and one bias &#39;b&#39;. Using SGD, we can update both w and b after the evaluation of each mini-batch. If the size of the mini-batch is 1, we give way to online <b>learning</b>.", "dateLastCrawled": "2022-01-16T12:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What <b>does gradient feature extraction techniques mean</b> in <b>Machine</b> <b>Learning</b>?", "url": "https://www.quora.com/What-does-gradient-feature-extraction-techniques-mean-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>does-gradient-feature-extraction-techniques-mean</b>-in-<b>Machine</b>...", "snippet": "Answer: Two of the obstacles to building a \u201cgood\u201d <b>Machine</b> <b>Learning</b> (ML) model is: 1. Too little labeled data 2. Too many features, some of which may be \u201credundant\u201d or \u201cuseless\u201d Think of features as an N-dimensional space. Think of data as points sparsely population the space. In the case of clas...", "dateLastCrawled": "2021-12-31T11:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> \u2014 Programming Differential Privacy", "url": "https://programming-dp.com/notebooks/ch12.html", "isFamilyFriendly": true, "displayUrl": "https://programming-dp.com/notebooks/ch12.html", "snippet": "The <b>gradient is like</b> a multi-dimensional derivative: ... In differentially private <b>machine</b> <b>learning</b>, it\u2019s important (and sometimes, very challenging) to strike the right balance between the number of iterations used and the scale of the noise added. Let\u2019s do a small experiment to see how the setting of \\(\\epsilon\\) effects the accuracy of our model. We\u2019ll train a model for several values of \\(\\epsilon\\), using 20 iterations each time, and graph the accuracy of each model against the ...", "dateLastCrawled": "2022-02-01T06:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Notes On Support Vector <b>Machine</b>", "url": "https://wuciawe.github.io/machine%20learning/math/2016/06/02/notes-on-support-vector-machine.html", "isFamilyFriendly": true, "displayUrl": "https://wuciawe.github.io/<b>machine</b> <b>learning</b>/math/2016/06/02/notes-on-support-vector...", "snippet": "And the sub-<b>gradient is like</b>. And the objective function is to minimize the total loss. which is a convex linear problem, thus can be easily solved by SGD or L-BFGS. 02 June 2016 Categories: 28 <b>machine</b> <b>learning</b> 75 math Tags: 29 <b>machine</b> <b>learning</b> 75 math 1 quadratic programming 2 classification 3 loss function 1 svm Prev; Archive; Next ; 2014-2020, \u80e1\u5609\u5049 (wuciawe@ ...", "dateLastCrawled": "2021-12-26T02:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Optimization techniques in Deep <b>learning</b> | by sumanth donapati | CodeX ...", "url": "https://medium.com/codex/optimization-techniques-in-deep-learning-5ac07a6e552b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/codex/optimization-techniques-in-deep-<b>learning</b>-5ac07a6e552b", "snippet": "7 stages of <b>machine</b> <b>learning</b> The goal of the 7 Stages framework is to break down all necessary tasks in <b>Machine</b> <b>Learning</b> and organize them in a logical way. Get started", "dateLastCrawled": "2022-01-26T13:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is <b>PyTorch</b>?. Think about Numpy, but with strong GPU\u2026 | by Khuyen ...", "url": "https://towardsdatascience.com/what-is-pytorch-a84e4559f0e3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/what-is-<b>pytorch</b>-a84e4559f0e3", "snippet": "The <b>gradient is like</b> derivative but in vector form. It is important to calculate the loss function in neural networks. But it impractical to calculate gradients of such large composite functions by solving mathematical equations because of the high number of dimensions. Luckily, <b>PyTorch</b> can find this gradient numerically in a matter of seconds! Let\u2019s say we want to find the gradient of the vector below. We expect the gradient of y to be x. Use tensor to find the gradient and check whether ...", "dateLastCrawled": "2022-01-29T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Artificial Intelligence</b> Tutorials with Examples - <b>Tutorial And Example</b>", "url": "https://www.tutorialandexample.com/artificial-intelligence/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>tutorialandexample</b>.com/<b>artificial-intelligence</b>", "snippet": "Neural Networks are one of the most popular techniques and tools in <b>Machine</b> <b>learning</b>. Neural Networks were inspired by the human brain as early as in the 1940s. Researchers studied the neuroscience and researched about the working of the human brain i.e. how the human... Gradient Descent. by admin | Nov 29, 2020 | <b>Artificial Intelligence</b>. Gradient Descent When training a neural network, an algorithm is used to minimize the loss. This algorithm is called as Gradient Descent. And loss refers ...", "dateLastCrawled": "2022-01-24T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "CSE 234 Data Systems for <b>Machine</b> <b>Learning</b>", "url": "https://cseweb.ucsd.edu/classes/fa21/cse234-a/slides/Topic1-ClassicalMLScale.pdf", "isFamilyFriendly": true, "displayUrl": "https://cseweb.ucsd.edu/classes/fa21/cse234-a/slides/Topic1-ClassicalMLScale.pdf", "snippet": "Data Systems for <b>Machine</b> <b>Learning</b> 1 Topic 1: Classical ML Training at Scale Chapters 2, 5, and 6 of MLSys book Arun Kumar. 2 Academic ML 101 Generalized Linear Models (GLMs); from statistics Bayesian Networks; inspired by causal reasoning Decision Tree-based: CART, Random Forest, Gradient-Boosted Trees (GBT), etc.; inspired by symbolic logic Support Vector Machines (SVMs); inspired by psychology Artificial Neural Networks (ANNs): Multi-Layer Perceptrons (MLPs), Convolutional NNs (CNNs ...", "dateLastCrawled": "2021-12-29T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Overview of <b>Reinforcement Learning</b> Algorithms | Towards Data Science", "url": "https://towardsdatascience.com/an-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/an-overview-of-classic-<b>reinforcement-learning</b>...", "snippet": "Q-<b>learning</b>. Q-<b>learning</b> is another type of TD method. The difference between SARSA and Q-<b>learning</b> is that SARSA is an on-policy model while Q-<b>learning</b> is off-policy. In SARSA, our return at state st is rt + \u03b3Q(st+1, at+1), where Q(st+1, at+1) is calculated from the state-action pair (st, at, rt, st+1, at+1) that was obtained by following policy \u03c0. However, in Q-<b>learning</b>, Q(st+1, at+1) is obtained by taking the optimal action, which might not necessarily be the same as our policy. In general ...", "dateLastCrawled": "2022-02-02T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to calculate and measure slope - EngineerSupply", "url": "https://www.engineersupply.com/Understanding-Slope-and-How-it-is-Measured.aspx", "isFamilyFriendly": true, "displayUrl": "https://www.engineersupply.com/<b>Understanding-Slope-and-How-it</b>-is-Measured.aspx", "snippet": "The two terms are similar to each other, but slope refers to a connection between two coordinate values. <b>Gradient is like</b> slope, except it refers to a single vector. This difference is important, because each part of the slope gradient indicates the rate of change with regard to that particular dimension. Why is it called &quot;rise over run?&quot; If you want to know how to calculate slope, you find the ratio of the \u201cvertical change\u201d to the \u201chorizontal change\u201d between two points on a line ...", "dateLastCrawled": "2022-02-03T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "USC Researchers Present 30 Papers at NeurIPS 2021 - USC Viterbi ...", "url": "https://viterbischool.usc.edu/news/2021/12/usc-researchers-present-30-papers-at-neurips-2021/", "isFamilyFriendly": true, "displayUrl": "https://viterbischool.usc.edu/news/2021/12/usc-researchers-present-30-papers-at...", "snippet": "With innovations in <b>machine</b> <b>learning</b> and AI occurring at faster speeds than ever before, the annual Conference on Neural Information Processing Systems (NeurIPS) brings together researchers and engineers to share new discoveries and collaborate on ideas to propel artificial intelligence into the future.. In total, 30 papers co-authored by USC-affiliated researchers have been selected for presentation at this week\u2019s 2021 event (Dec. 6-14), showcasing novel work that could ultimately ...", "dateLastCrawled": "2022-02-03T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) Introduction to Deep <b>Learning</b> - From Logical Calculus to ...", "url": "https://www.academia.edu/42933956/Introduction_to_Deep_Learning_From_Logical_Calculus_to_Artificial_Intelligence", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/42933956/Introduction_to_Deep_<b>Learning</b>_From_Logical_Calculus...", "snippet": "Introduction to Deep <b>Learning</b> - From <b>Logical Calculus to Artificial Intelligence</b>. 2018. Nicko V. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 36 Full PDFs related to this paper. Read Paper. Introduction to Deep <b>Learning</b> - From <b>Logical Calculus to Artificial Intelligence</b>. Download ...", "dateLastCrawled": "2022-01-23T08:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Validating analytic gradient for a Neural</b> Network | by Shiva Verma - Medium", "url": "https://shiva-verma.medium.com/how-to-validate-your-gradient-expression-for-a-neural-network-8284ede6272", "isFamilyFriendly": true, "displayUrl": "https://shiva-verma.medium.com/how-to-validate-your-gradient-expression-for-a-neural...", "snippet": "Analytic gradient on weight w1. This is all the code you have to write to calculate the gradient. First, we initialize weights matrices. Second, we calculate all activations and last we backpropagate and calculate the gradient of loss w.r.t. our weights using the chain rule. The Gradient calculated by this method is called the analytic gradient. This code is self-explanatory.", "dateLastCrawled": "2022-01-11T05:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Recurrent Neural Network</b> &amp; LSTM with Practical Implementation | by Amir ...", "url": "https://medium.com/machine-learning-researcher/recurrent-neural-network-rnn-e6f69db16eba", "isFamilyFriendly": true, "displayUrl": "https://medium.com/<b>machine</b>-<b>learning</b>-researcher/<b>recurrent-neural-network</b>-rnn-e6f69db16eba", "snippet": "The working of the exploding <b>gradient is similar</b> but the weights here change drastically instead of negligible change. Notice the small change in the diagram below: We need to overcome both of ...", "dateLastCrawled": "2022-02-03T05:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Working of RNN in TensorFlow</b> - Javatpoint", "url": "https://www.javatpoint.com/working-of-rnn-in-tensorflow", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>working-of-rnn-in-tensorflow</b>", "snippet": "The working of the collapse <b>gradient is similar</b>, but the weights here change extremely instead of negligible change. Notice the small here: We have to overcome both of these, and it is some challenge at first. Exploding gradients Vanishing gradients ; Truncated BTT Instead of starting backpropagation at the last timestamp, we can choose a smaller timestamp like 10; ReLU activation function We can use activation like ReLU, which gives output one while calculating the gradient; Clip gradients ...", "dateLastCrawled": "2022-01-27T11:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction to Deterministic Policy Gradient (DPG) | by Cheng Xi Tsou ...", "url": "https://medium.com/geekculture/introduction-to-deterministic-policy-gradient-dpg-e7229d5248e2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/introduction-to-deterministic-policy-gradient-dpg-e7229...", "snippet": "The majority of model-free <b>learning</b> algorithms are ... The proof for this deterministic policy <b>gradient is similar</b> in structure to the proof for the policy gradient theorem detailed in (Sutton et ...", "dateLastCrawled": "2022-01-29T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Recurrent Neural Networks</b> (RNN) Tutorial Using TensorFlow In ... - Edureka", "url": "https://www.edureka.co/blog/recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.edureka.co/blog/<b>recurrent-neural-networks</b>", "snippet": "The working of the exploding <b>gradient is similar</b> but the weights here change drastically instead of negligible change. Notice the small change in the diagram below: We need to overcome both of these and it is a bit of a challenge at first. Consider the following chart: Continuing this blog on <b>Recurrent Neural Networks</b>, we will be discussing further on LSTM networks. Long Short-Term Memory Networks. Long Short-Term Memory networks are usually just called \u201cLSTMs\u201d. They are a special kind ...", "dateLastCrawled": "2022-01-29T12:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "t-SNE - MATLAB &amp; Simulink - MathWorks", "url": "https://www.mathworks.com/help/stats/t-sne.html", "isFamilyFriendly": true, "displayUrl": "https://www.mathworks.com/help/stats/t-sne.html", "snippet": "The idea, originally used in astrophysics, is that the <b>gradient is similar</b> for nearby points, so the computations can be simplified. See van der Maaten . Characteristics of t-SNE. Cannot Use Embedding to Classify New Data. Performance Depends on Data Sizes and Algorithm. Helpful Nonlinear Distortion. Cannot Use Embedding to Classify New Data. Because t-SNE often separates data clusters well, it can seem that t-SNE can classify new data points. However, t-SNE cannot classify new points. The t ...", "dateLastCrawled": "2022-02-02T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep associative <b>learning</b> <b>for neural networks</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221003623", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221003623", "snippet": "In <b>machine</b> <b>learning</b>, artificial neural networks (ANNs) are one type of popular approaches, especially deep ones . ANNs are inspired from the information processing mechanism of neural systems in brain and are composed of inter-connected processing units. Many neural <b>learning</b> models have been proposed according to different mechanisms and problems. For instance, self-organizing feature map was inspired from the competitive mechanism of neurons and the neurons are organized according to the ...", "dateLastCrawled": "2022-01-07T08:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "GitHub - suneelpatel/Deep-<b>Learning</b>-with-TensorFlow: Learn Deep <b>Learning</b> ...", "url": "https://github.com/suneelpatel/Deep-Learning-with-TensorFlow", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/suneelpatel/Deep-<b>Learning</b>-with-TensorFlow", "snippet": "Deep <b>Learning</b> is a branch of <b>Machine</b> <b>Learning</b> based on a set of algorithms that attempt to model high-level abstraction in the data by using a deep graph with multiple processing layers. It is composed of multiple linear and non-linear transformations. Deep <b>learning</b> mimics the way our brain functions i.e. it learns from experience. A collection of statistical <b>machine</b> <b>learning</b> techniques used to learn feature hierarchies often based on artificial neural networks. Deep <b>learning</b> is a specific ...", "dateLastCrawled": "2022-01-22T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Pushparaja Murugan and Shanmugasundaram Durairaj School of Mechanical ...", "url": "https://arxiv.org/pdf/1712.04711.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1712.04711.pdf", "snippet": "plex <b>machine</b> <b>learning</b> tasks. The architecture of ConvNets demands the huge and rich amount of data and involves with a vast number of parameters that leads the <b>learning</b> takes to be com-putationally expensive, slow convergence towards the global minima, trap in local minima with poor predictions. In some cases, architecture over ts the data and make the architecture di cult to generalise for new samples that were not in the training set samples. To address these limita-tions, many ...", "dateLastCrawled": "2020-10-06T08:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Segmentation and graph-based techniques", "url": "https://www.cs.cmu.edu/~16385/lectures/lecture27.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~16385/lectures/lecture27.pdf", "snippet": "British <b>Machine</b> Vision Conference (BMVC), September, 2007. Multiple segmentations: Example \u2022 Task: Regions \u2192Features \u2192Labels (horizontal, vertical, sky, etc.) \u2022 Chicken and egg problem: \u2013 If we knew the regions, we could compute the features and label the right regions \u2013 But to know the right regions we need to know the labels! \u2022 Solution: \u2013 Generate lots of segmentations \u2013 Combine the classifications to get consensus 50x50 Patch 50x50 Patch Example from D. Hoiem Recovering ...", "dateLastCrawled": "2022-01-28T19:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>learning</b> for KPIs prediction: a case study of the overall ...", "url": "https://link.springer.com/article/10.1007/s00500-020-05348-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00500-020-05348-y", "snippet": "<b>Machine</b> <b>learning</b> algorithms are divided into three categories, namely supervised <b>learning</b> (Smola and Vishwanathan 2008), ... XG-Boost is an ensemble tree-based model, which flows the principle of gradient boosting <b>just as gradient</b> boosting <b>machine</b> (GBM) and Adaboost. However, XG-Boost has more customizable parameters that allow it a better flexibility. Additionally, XG-Boost uses more regularized model formalization to control over-fitting, which gives it better performance. All of the above ...", "dateLastCrawled": "2021-12-28T17:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning Playing an Important Role in Data Management</b>", "url": "https://www.analyticsinsight.net/machine-learning-playing-an-important-role-in-data-management/", "isFamilyFriendly": true, "displayUrl": "https://www.analyticsinsight.net/<b>machine-learning-playing-an-important-role</b>-in-data...", "snippet": "Luckily, <b>machine</b> <b>learning</b> can help. A variety of <b>machine</b> <b>learning</b> and deep <b>learning</b> strategies might be utilized to achieve this. Comprehensively, <b>machine</b>/deep <b>learning</b> methods might be named either unsupervised <b>learning</b>, supervised <b>learning</b>, or reinforcement <b>learning</b> . The decision of which strategy will be driven by what issue is being fathomed. For instance, supervised <b>learning</b> mechanisms, for example, random forest might be utilized to build up a gauge, or what comprises \u201ctypical ...", "dateLastCrawled": "2022-02-02T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting Point Spread in NFL Games - CS229: <b>Machine</b> <b>Learning</b>", "url": "http://cs229.stanford.edu/proj2016/report/WadsworthVera-PredictingPointSpreadinNFLGames-report.pdf", "isFamilyFriendly": true, "displayUrl": "cs229.stanford.edu/proj2016/report/WadsworthVera-PredictingPointSpreadinNFLGames...", "snippet": "Though there may be some <b>machine</b> <b>learning</b> involved, it usually stays hidden and so is not a useful reference for this project other than looking at what features sports writers focus on. A popular publication that is more transparent about how it numerically calculates point spread is FiveThirtyEight, which uses \u201cElo Ratings\u201d - a metric FiveThirtyEight founder Nate Silver is famous for. After obtaining the team\u2019s ratings, a simple equation is used: P(team A wins) = , 1+ 10 400 \u2212 ...", "dateLastCrawled": "2022-02-02T20:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "CNN-boosted full-waveform inversion | SEG Technical Program Expanded ...", "url": "https://library.seg.org/doi/10.1190/segam2020-3420598.1", "isFamilyFriendly": true, "displayUrl": "https://library.seg.org/doi/10.1190/segam2020-3420598.1", "snippet": "In addition to finding the optimal step length, <b>just as gradient</b>-descent FWI does, CNN-boosted FWI fixes this optimal step length and optimizes the CNN, which is originally trained to approximate the negative gradients at each iteration, to update the velocity model. Synthetic examples using the modified Marmousi2 P wave model show that CNN-boosted FWI, as well as a hybrid, of CNN-boosted FWI and gradient-descent FWI, inverts for the velocity model with lower model and data errors than the ...", "dateLastCrawled": "2022-01-07T11:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Beyond log-concave sampling \u2013 <b>Off the convex path</b>", "url": "http://www.offconvex.org/2020/09/19/beyondlogconvavesampling/", "isFamilyFriendly": true, "displayUrl": "www.offconvex.org/2020/09/19/beyondlogconvavesampling", "snippet": "However, optimization is only one of the basic algorithmic primitives in <b>machine</b> <b>learning</b> \u2014 it\u2019s used by most forms of risk minimization and model fitting. Another important primitive is sampling, which is used by most forms of inference (i.e. answering probabilistic queries of a learned model). It turns out that there is a natural analogue of convexity for sampling \u2014 log-concavity. Paralleling the state of affairs in optimization, we have a variety of (provably efficient) algorithms ...", "dateLastCrawled": "2022-02-01T21:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Top 2019 predictions for deep <b>learning</b> in XNUMX-artificial intelligence ...", "url": "https://easyai.tech/en/blog/10-deep-learning-trends-and-predictions-for-2019/?variant=zh-hant", "isFamilyFriendly": true, "displayUrl": "https://easyai.tech/en/blog/10-deep-<b>learning</b>-trends-and-predictions-for-2019/?variant=...", "snippet": "Suggested Search: \u4eba\u5de5\u667a\u80fd, <b>Machine</b> <b>learning</b>, Deep <b>learning</b>, NLP. Home; Blog; Top 2019 predictions for deep <b>learning</b> in XNUMX. 2019/2/1 by Unbeatable Xiaoqiang. AI News; 0 comments; This article is reproduced from the public artificial intelligence scientist,Original address. 2018 is over and it is time to start predicting deep <b>learning</b> in 2019. Here are my previous forecasts and reviews for 2017 and 2018: About 2017 forecast and review. The 2017 forecast covers hardware acceleration ...", "dateLastCrawled": "2022-01-23T04:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "1 <b>Cooperative Multi-Agent Reinforcement Learning</b> for Low-Level Wireless ...", "url": "https://arxiv.org/pdf/1801.04541.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1801.04541.pdf", "snippet": "<b>machine</b> <b>learning</b>, wireless communication can also be improved by utilizing similar techniques to increase the \ufb02exibility of wireless networks. In this work, we pose the problem of discovering low-level wireless communication schemes ex-nihilo between two agents in a fully decentralized fashion as a reinforcement <b>learning</b> problem. Our proposed approach uses policy gradients to learn an optimal bi-directional communication scheme and shows surprisingly sophisticated and intelligent <b>learning</b> ...", "dateLastCrawled": "2021-10-25T13:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Simulated tempering Langevin Monte Carlo", "url": "http://holdenlee.github.io/Simulated%20tempering%20Langevin%20Monte%20Carlo.html", "isFamilyFriendly": true, "displayUrl": "holdenlee.github.io/Simulated tempering Langevin Monte Carlo.html", "snippet": "We care about this difficult case because modern sampling problems (such as those arising in Bayesian <b>machine</b> <b>learning</b>) are often non-log-concave. Like in nonconvex optimization, we must go beyond worst case analysis, and find what kind of structure in non-log-concave distributions allows us to sample efficiently. Note that log-concavity makes sense for sampling problems on \\(\\R^d\\), but there are other conditions that similarly give guarantees for mixing, such as correlation decay for ...", "dateLastCrawled": "2022-01-30T19:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A arXiv:1611.02639v2 [cs.LG] 15 Nov 2016", "url": "https://arxiv.org/pdf/1611.02639.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1611.02639.pdf", "snippet": "Practitioners of <b>machine</b> <b>learning</b> regularly inspect the coef\ufb01cients of linear models as a measure of feature importance. This process allows them to understand and debug these models. The natural analog of these coef\ufb01cients for deep models are the gradients of the prediction score with respect to the input. For linear models, the gradient of an input feature is equal to its coef\ufb01cient. For deep nonlinear models, the <b>gradient can be thought of as</b> a local linear approximation (Simonyan ...", "dateLastCrawled": "2021-09-16T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>GRADIENTS, BATCH NORMALIZATION AND LAYER NORMALIZATION</b> - Abracadabra", "url": "https://tomaxent.com/2017/05/09/GRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION/", "isFamilyFriendly": true, "displayUrl": "https://tomaxent.com/2017/05/09/<b>GRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION</b>", "snippet": "The <b>gradient can be thought of as</b> several things. One is that the magnitude of the gradient represents the sensitivity or impact this weight has on determining y which determines our loss. This can be seen below: CS231n. What the gradients (dfdx, dfdy, dfdz, dfdq, dfdz) tell us is the sensitivity of each variable on our result f. In an MLP, we will produce a result (logits) and compare it with our targets to determine the deviance in what we got and what we should have gotten. From this we ...", "dateLastCrawled": "2022-01-31T10:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Recommending Movies with <b>Machine</b> <b>Learning</b> - Home", "url": "https://andrewlim1990.github.io/machine-learning/simple-movie-recommender/", "isFamilyFriendly": true, "displayUrl": "https://andrewlim1990.github.io/<b>machine</b>-<b>learning</b>/simple-movie-recommender", "snippet": "X_beta_<b>gradient can be thought of as</b> the derivative of the cost function. For those who are interested in this, please click here. Inputs of compute_error: X_beta value is the genre-score and user preference arrays unrolled into a single vector array. This will be made more clear later. y is matrix containing the ratings of each movie from each user. rated is a boolean form of y showing whether or not a user has provided a rating for a specific movie. reg_coeff is the regularization constant ...", "dateLastCrawled": "2021-12-15T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>GRADIENTS OF COUNTERFACTUALS</b>", "url": "https://openreview.net/pdf?id=rJzaDdYxx", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=rJzaDdYxx", "snippet": "Practitioners of <b>machine</b> <b>learning</b> regularly inspect the coef\ufb01cients of linear models as a measure of feature importance. This process allows them to understand and debug these models. The natural analog of these coef\ufb01cients for deep models are the gradients of the prediction score with respect to the input. For linear models, the gradient of an input feature is equal to its coef\ufb01cient. For deep nonlinear models, the <b>gradient can be thought of as</b> a local linear approximation (Simonyan ...", "dateLastCrawled": "2021-12-01T19:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Look Into Neural Networks and Deep Reinforcement <b>Learning</b> | by Chloe ...", "url": "https://chloeewang.medium.com/a-look-into-neural-networks-and-deep-reinforcement-learning-2d5a9baef3e3", "isFamilyFriendly": true, "displayUrl": "https://chloeewang.medium.com/a-look-into-neural-networks-and-deep-reinforcement...", "snippet": "<b>Machine</b> <b>learning</b> (ML), which provides computers the ability to learn automatically and improve from experience without being explicitly programmed to do so, is the largest and most popular subset of AI. However, a standard ML model cannot handle high-dimensional data found in realistic problems, and struggles to extract relevant features from a dataset. Deep <b>learning</b> (DL) is defined as a collection of statistical ML techniques that are used to learn feature hierarchies based on neural ...", "dateLastCrawled": "2022-01-24T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Interview questions on Data Science", "url": "https://iq.opengenus.org/interview-questions-on-data-science/", "isFamilyFriendly": true, "displayUrl": "https://iq.opengenus.org/interview-questions-on-data-science", "snippet": "Overfitting is when a <b>machine</b> <b>learning</b> model is too closely fit over a certain dataset and tries to go through more data points in the dataset than required and looses its ability to generalize and adapt over any given dataset to produce result. Underfitting is when the model fails to catch the underlying trend in the dataset i.e when it fails to learn properly from the training data. This reduces the accuracy of the prediction. 7. What is a confusion matrix? Confusion matrix is a table that ...", "dateLastCrawled": "2022-02-02T19:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What Types of <b>Generative Models</b> Are There? | Text <b>Machine</b> Blog", "url": "https://text-machine-lab.github.io/blog/2020/generative-models/", "isFamilyFriendly": true, "displayUrl": "https://text-<b>machine</b>-lab.github.io/blog/2020/<b>generative-models</b>", "snippet": "Recently, the field of <b>machine</b> <b>learning</b> has seen a surge in generative modeling - the ability to learn from data to generate complex outputs such as images or natural language. The best models have synthesized photo-realistic images of people who have never existed, Google Translate outputs impressive generative translations between hundreds of languages, and new waveform models are responding to your voice commands with voices of their own. Style transfer models answer the question of how ...", "dateLastCrawled": "2022-02-01T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Numerical <b>gradient</b> - MATLAB <b>gradient</b> - MathWorks", "url": "https://www.mathworks.com/help/matlab/ref/gradient.html", "isFamilyFriendly": true, "displayUrl": "https://www.mathworks.com/help/matlab/ref/<b>gradient</b>.html", "snippet": "Numerical gradients, returned as arrays of the same size as F.The first output FX is always the <b>gradient</b> along the 2nd dimension of F, going across columns.The second output FY is always the <b>gradient</b> along the 1st dimension of F, going across rows.For the third output FZ and the outputs that follow, the Nth output is the <b>gradient</b> along the Nth dimension of F.", "dateLastCrawled": "2022-02-03T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Optimisation Techniques I \u00b7 <b>Deep Learning</b>", "url": "https://atcold.github.io/pytorch-Deep-Learning/en/week05/05-1/", "isFamilyFriendly": true, "displayUrl": "https://atcold.github.io/pytorch-<b>Deep-Learning</b>/en/week05/05-1", "snippet": "If the <b>learning</b> rate is too low, then we would make steady progress towards the minimum. However, this might take more time than what is ideal. It is generally very difficult (or impossible) to get a step-size that would directly take us to the minimum. What we would ideally want is to have a step-size a little larger than the optimal. In practice, this gives the quickest convergence. However, if we use too large a <b>learning</b> rate, then the iterates get further and further away from the minima ...", "dateLastCrawled": "2022-01-29T13:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Gradient Descent Multivariate Matlab [TPA0GF]", "url": "https://reset.tn.it/Multivariate_Gradient_Descent_Matlab.html", "isFamilyFriendly": true, "displayUrl": "https://reset.tn.it/Multivariate_Gradient_Descent_Matlab.html", "snippet": "The <b>gradient can be thought of as</b> a collection of vectors pointing in the direction of increasing values of F. MATLAB Release Compatibility. Gradient Descent Matlab Code. When you fit a <b>machine</b> <b>learning</b> method to a training Multivariate Linear Regression <b>Machine</b> <b>Learning</b> - Stanford University | Coursera by Andrew Ng Please visit Coursera site.", "dateLastCrawled": "2022-01-15T10:48:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(gradient)  is like +(person walking down a hill)", "+(gradient) is similar to +(person walking down a hill)", "+(gradient) can be thought of as +(person walking down a hill)", "+(gradient) can be compared to +(person walking down a hill)", "machine learning +(gradient AND analogy)", "machine learning +(\"gradient is like\")", "machine learning +(\"gradient is similar\")", "machine learning +(\"just as gradient\")", "machine learning +(\"gradient can be thought of as\")", "machine learning +(\"gradient can be compared to\")"]}