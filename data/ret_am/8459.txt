{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Persagen Consulting | Specializing in molecular genomics, precision ...", "url": "https://persagen.com/resources/glossary.html", "isFamilyFriendly": true, "displayUrl": "https://persagen.com/resources/<b>glossary</b>.html", "snippet": "<b>Self-attention</b>. Attention mechanisms, <b>like</b> Google\u2019s Transformer, have become an integral part of sequence modeling and transduction models in various tasks, <b>allowing</b> modeling of dependencies without regard to their distance in the <b>input</b> or output sequences. <b>Self-attention</b> is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or ...", "dateLastCrawled": "2022-01-17T05:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "MM &#39;20: Proceedings of the 28th ACM International Conference on ...", "url": "http://sigmm.org/opentoc/MM2020-TOC-2", "isFamilyFriendly": true, "displayUrl": "sigmm.org/opentoc/MM2020-TOC-2", "snippet": "Simultaneously, <b>Self-Attention</b> module is introduced to match or outperform their convolutional counterparts, which allows the feature aggregation to adapt to <b>each</b> channel. Furthermore, to improve the basic convolutional feature transformation process of Convolutional Neural Networks (CNNs), Self-Calibrated convolution is applied to build long-range spatial and inter-channel dependencies around <b>each</b> spatial location that explicitly expand fields-of-view of <b>each</b> convolutional <b>layer</b> through ...", "dateLastCrawled": "2022-01-29T07:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Hands-On Machine Learning with Scikit-<b>Learn</b> &amp; TensorFlow CONCEPTS ...", "url": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow_CONCEPTS_TOOLS_AND_TECHNIQUES_TO_BUILD_INTELLIGENT_SYSTEMS", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39825063/Hands_On_Machine_<b>Learn</b>ing_with_Scikit_<b>Learn</b>_and...", "snippet": "Hands-On Machine Learning with <b>Scikit-Learn &amp; TensorFlow CONCEPTS, TOOLS, AND TECHNIQUES TO BUILD INTELLIGENT SYSTEMS</b> . \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. <b>Remember</b> me on this computer. or reset password. Enter the email address you signed up with and we&#39;ll email you a reset link. ...", "dateLastCrawled": "2022-01-30T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "notes-1/Deep Learning.md at master \u00b7 kirk86/notes-1 \u00b7 <b>GitHub</b>", "url": "https://github.com/kirk86/notes-1/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kirk86/notes-1/blob/master/Deep <b>Learn</b>ing.md", "snippet": "During this stage, mutual <b>information</b> between <b>each</b> <b>layer</b>&#39;s representation and the <b>input</b>/output increases to the point that the network&#39;s representation in the <b>information</b> plane is more or less linear. All this means is that <b>information</b> loss/gain from <b>layer</b> to <b>layer</b> is approximately constant, so in a sense no <b>layer</b> is doing more work than others. The second phase consists of continuing to maximizing the mutual <b>information</b> between <b>each</b> <b>layer</b> and the output, but now at the expense of the mutual ...", "dateLastCrawled": "2021-12-24T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>ICML 2018 Abstracts</b> \u00b7 GitHub", "url": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "snippet": "<b>Each</b> <b>layer</b> is <b>also</b> augmented with latent random variables, which are sampled from a prior distribution during the training of that <b>layer</b>. The maximum entropy objective causes these latent variables to be incorporated into the <b>layer</b>&#39;s policy, and the higher level <b>layer</b> can directly control the behavior of the lower <b>layer</b> through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the ...", "dateLastCrawled": "2022-01-19T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The prospects for a scientific understanding of ... - SelfAwarePatterns", "url": "https://selfawarepatterns.com/2018/07/02/the-prospects-for-a-scientific-understanding-of-consciousness/", "isFamilyFriendly": true, "displayUrl": "https://selfawarepatterns.com/2018/07/02/the-prospects-for-a-scientific-understanding...", "snippet": "Furthermore I believe that such an organism should tend to develop <b>its</b> <b>own</b> conscious form of memory as another <b>input</b> from which to work, and interpret inputs and construct scenarios about what might complete a given quest. I consider this computer to mark the rise of the teleological, or purpose driven, form of computer. And apparently to support it a computer must exist that does not function alone on the basis of chemical dynamics (1), or <b>neuron</b> dynamics (2), or electrical dynamics (4 ...", "dateLastCrawled": "2022-01-08T19:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Social Psychology</b> Flashcards | Quizlet", "url": "https://quizlet.com/156414423/social-psychology-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/156414423/<b>social-psychology</b>-flash-cards", "snippet": "contains a special type of brain cells or <b>neuron</b>&#39;s <b>called</b> spindle <b>neuron</b>, which are much larger than other neutrons in the brain. spindle neurons . spindle neurons collect waves and neural signals from one region of the brain and send them to other regions it appears the anterior cingulate cortex with <b>its</b> spindle neurons act as an executive attention system that facilitates self awareness humans are one of only a few species who possess spindle <b>neuron</b> research indicate when people are trying ...", "dateLastCrawled": "2018-11-29T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Deep Learning for NLP and Speech Recognition [1st ed.] 978-3-030-14595 ...", "url": "https://dokumen.pub/deep-learning-for-nlp-and-speech-recognition-1st-ed-978-3-030-14595-8978-3-030-14596-5.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/deep-<b>learn</b>ing-for-nlp-and-speech-recognition-1st-ed-978-3-030...", "snippet": "Note that X \u2286 X. An <b>individual</b> data point in the set X drawn from the <b>input</b> space X, <b>also</b> referred to as an instance or an example, is normally represented in vector form as xi of d dimensions. The elements of a vector xi are <b>also</b> referred to as features or attributes. For example, apples and oranges can be defined in terms of {shape, size ...", "dateLastCrawled": "2021-12-29T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Robots and AI: Our Immortality or Extinction - page 21 - The rest ...", "url": "https://forum.arctic-sea-ice.net/index.php?topic=1392.1000", "isFamilyFriendly": true, "displayUrl": "https://forum.arctic-sea-ice.net/index.php?topic=1392.1000", "snippet": "The experts in <b>each</b> <b>layer</b> are controlled by a gating network that activates experts based on the <b>input</b> data. For <b>each</b> token (generally a word or part of a word), the gating network selects the two most appropriate experts to process the data. The full version of GLaM has 1.2 Trillion total parameters across 64 experts per MoE <b>layer</b> with 32 MoE layers in total, but only activates a subnetwork of 97 Billion (8% of 1.2 Trillion) parameters per token prediction during inference.", "dateLastCrawled": "2022-01-30T05:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Stand-Alone <b>Self-Attention</b> in Vision Models | papers_we_read", "url": "https://vlgiitr.github.io/papers_we_read/summaries/vision_attention.html", "isFamilyFriendly": true, "displayUrl": "https://vlgiitr.github.io/papers_we_read/summaries/vision_attention.html", "snippet": "<b>Self attention</b> <b>is similar</b> to transformer, consisting of key,value,query pairs. Query of a pixel is dot-producted with keys of all local pixels, and softmax is applied to get weights, which are used to find a weighted sum of the values to get the output. The <b>input</b> pixel feature is of din dimension which is transformed to dout for <b>each</b> of key ...", "dateLastCrawled": "2022-02-04T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training", "url": "https://www.researchgate.net/publication/353690477_Enhancing_LSTM_Models_with_Self-attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353690477_Enhancing_LSTM_Models_with_Self...", "snippet": "<b>Self-attention</b>, <b>also</b> known as in tra -attention, is an attention mec hanism re-lating di\ufb00erent positions of a sequence in order to model dependencies b etween. di\ufb00erent parts of the sequence ...", "dateLastCrawled": "2022-01-13T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "MM &#39;20: Proceedings of the 28th ACM International Conference on ...", "url": "http://sigmm.org/opentoc/MM2020-TOC-2", "isFamilyFriendly": true, "displayUrl": "sigmm.org/opentoc/MM2020-TOC-2", "snippet": "Simultaneously, <b>Self-Attention</b> module is introduced to match or outperform their convolutional counterparts, which allows the feature aggregation to adapt to <b>each</b> channel. Furthermore, to improve the basic convolutional feature transformation process of Convolutional Neural Networks (CNNs), Self-Calibrated convolution is applied to build long-range spatial and inter-channel dependencies around <b>each</b> spatial location that explicitly expand fields-of-view of <b>each</b> convolutional <b>layer</b> through ...", "dateLastCrawled": "2022-01-29T07:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Hands-On Machine Learning with Scikit-<b>Learn</b> &amp; TensorFlow CONCEPTS ...", "url": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow_CONCEPTS_TOOLS_AND_TECHNIQUES_TO_BUILD_INTELLIGENT_SYSTEMS", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39825063/Hands_On_Machine_<b>Learn</b>ing_with_Scikit_<b>Learn</b>_and...", "snippet": "Hands-On Machine Learning with <b>Scikit-Learn &amp; TensorFlow CONCEPTS, TOOLS, AND TECHNIQUES TO BUILD INTELLIGENT SYSTEMS</b> . \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. <b>Remember</b> me on this computer. or reset password. Enter the email address you signed up with and we&#39;ll email you a reset link. ...", "dateLastCrawled": "2022-01-30T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "notes-1/Deep Learning.md at master \u00b7 kirk86/notes-1 \u00b7 <b>GitHub</b>", "url": "https://github.com/kirk86/notes-1/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kirk86/notes-1/blob/master/Deep <b>Learn</b>ing.md", "snippet": "During this stage, mutual <b>information</b> between <b>each</b> <b>layer</b>&#39;s representation and the <b>input</b>/output increases to the point that the network&#39;s representation in the <b>information</b> plane is more or less linear. All this means is that <b>information</b> loss/gain from <b>layer</b> to <b>layer</b> is approximately constant, so in a sense no <b>layer</b> is doing more work than others. The second phase consists of continuing to maximizing the mutual <b>information</b> between <b>each</b> <b>layer</b> and the output, but now at the expense of the mutual ...", "dateLastCrawled": "2021-12-24T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Autodidactic <b>Universe</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2104.03902/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2104.03902", "snippet": "We present an approach to cosmology in which the <b>Universe</b> learns <b>its</b> <b>own</b> physical laws. It does so by exploring a landscape of possible laws, which we express as a certain class of matrix models. We discover maps that put <b>each</b> of these matrix models in correspondence with both a gauge/gravity theory and a mathematical model of a learning machine, such as a deep recurrent, cyclic neural network. This establishes a correspondence between <b>each</b> solution of the physical theory and a run of a ...", "dateLastCrawled": "2022-01-25T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The prospects for a scientific understanding of ... - SelfAwarePatterns", "url": "https://selfawarepatterns.com/2018/07/02/the-prospects-for-a-scientific-understanding-of-consciousness/", "isFamilyFriendly": true, "displayUrl": "https://selfawarepatterns.com/2018/07/02/the-prospects-for-a-scientific-understanding...", "snippet": "<b>Each</b> <b>individual</b> light <b>is similar</b>, but the mechanism of reading the board to produce distinct outputs can differentiate <b>each</b> <b>individual</b> light from the others. Imagine there is a camera looking at a room and wired such that when a red light is on in the room, a particular but arbitrary light on the board lights up. When a blue light is on, a different light on the board is turned on. As far as the mechanism is concerned, there is only inputs and associated outputs. But the mechanism is not ...", "dateLastCrawled": "2022-01-08T19:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "iclr2017-submission-papers-index/openreview.txt at master - <b>github.com</b>", "url": "https://github.com/rickiepark/iclr2017-submission-papers-index/blob/master/openreview.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rickiepark/iclr2017-submission-papers-index/blob/master/openreview.txt", "snippet": "<b>Learn</b> and contribute. Topics \u2192 Collections \u2192 Trending \u2192 Learning Lab \u2192 Open source guides \u2192 Connect with others. The ReadME Project \u2192 Events \u2192 Community forum \u2192 GitHub Education \u2192 GitHub Stars program \u2192 Marketplace; Pricing Plans \u2192 Compare plans \u2192 Contact Sales \u2192 Education \u2192 In this repository All GitHub \u21b5 Jump to \u21b5 No suggested jump to results; In this repository All GitHub \u21b5 Jump to \u21b5 In this user All GitHub \u21b5 Jump to \u21b5 In this repository All GitHub ...", "dateLastCrawled": "2021-11-05T19:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Social Psychology</b> Flashcards | Quizlet", "url": "https://quizlet.com/156414423/social-psychology-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/156414423/<b>social-psychology</b>-flash-cards", "snippet": "contains a special type of brain cells or <b>neuron</b>&#39;s <b>called</b> spindle <b>neuron</b>, which are much larger than other neutrons in the brain. spindle neurons . spindle neurons collect waves and neural signals from one region of the brain and send them to other regions it appears the anterior cingulate cortex with <b>its</b> spindle neurons act as an executive attention system that facilitates self awareness humans are one of only a few species who possess spindle <b>neuron</b> research indicate when people are trying ...", "dateLastCrawled": "2018-11-29T23:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Hands-On Machine Learning with Scikit-<b>Learn</b> &amp; TensorFlow CONCEPTS ...", "url": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow_CONCEPTS_TOOLS_AND_TECHNIQUES_TO_BUILD_INTELLIGENT_SYSTEMS", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39825063/Hands_On_Machine_<b>Learn</b>ing_with_Scikit_<b>Learn</b>_and...", "snippet": "Hands-On Machine Learning with <b>Scikit-Learn &amp; TensorFlow CONCEPTS, TOOLS, AND TECHNIQUES TO BUILD INTELLIGENT SYSTEMS</b> . \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. <b>Remember</b> me on this computer. or reset password. Enter the email address you signed up with and we&#39;ll email you a reset link. ...", "dateLastCrawled": "2022-01-30T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "notes-1/Deep Learning.md at master \u00b7 kirk86/notes-1 \u00b7 <b>GitHub</b>", "url": "https://github.com/kirk86/notes-1/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kirk86/notes-1/blob/master/Deep <b>Learn</b>ing.md", "snippet": "During this stage, mutual <b>information</b> between <b>each</b> <b>layer</b>&#39;s representation and the <b>input</b>/output increases to the point that the network&#39;s representation in the <b>information</b> plane is more or less linear. All this means is that <b>information</b> loss/gain from <b>layer</b> to <b>layer</b> is approximately constant, so in a sense no <b>layer</b> is doing more work than others. The second phase consists of continuing to maximizing the mutual <b>information</b> between <b>each</b> <b>layer</b> and the output, but now at the expense of the mutual ...", "dateLastCrawled": "2021-12-24T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Neural network</b> applications in fault diagnosis and detection: an ...", "url": "https://link.springer.com/article/10.1007/s00521-018-3911-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00521-018-3911-5", "snippet": "The <b>input</b> data set is <b>also</b> varied in a <b>way</b> such that not only does it has a specific fault target, but there are <b>also</b> specific severities of <b>each</b> fault target. The inputs are <b>also</b> normalized, and the discretization of the fault severities was made between the range of [0, 1] for <b>input</b> deviations of \u00b1 5%, \u00b1 12.5%, and \u00b1 20% from nominal value. This means the target values are equivalent to 0.25, 0.625, or 1.0 for the three different severities, respectively. The objective of the ...", "dateLastCrawled": "2022-01-10T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The Autodidactic <b>Universe</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2104.03902/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2104.03902", "snippet": "We present an approach to cosmology in which the <b>Universe</b> learns <b>its</b> <b>own</b> physical laws. It does so by exploring a landscape of possible laws, which we express as a certain class of matrix models. We discover maps that put <b>each</b> of these matrix models in correspondence with both a gauge/gravity theory and a mathematical model of a learning machine, such as a deep recurrent, cyclic neural network. This establishes a correspondence between <b>each</b> solution of the physical theory and a run of a ...", "dateLastCrawled": "2022-01-25T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation", "url": "https://www.researchgate.net/publication/308760773_Bi-directional_LSTM_Recurrent_Neural_Network_for_Chinese_Word_Segmentation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/308760773_Bi-directional_LSTM_Recurrent...", "snippet": "The whole system achieved the F1score of 47.23% on test data and obtained the first place in the NLPCC KBQA evaluation task. Yao and Huang (2016) held that the bidirectional LSTM network (Bi-LSTM ...", "dateLastCrawled": "2021-12-16T14:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The prospects for a scientific understanding of ... - SelfAwarePatterns", "url": "https://selfawarepatterns.com/2018/07/02/the-prospects-for-a-scientific-understanding-of-consciousness/", "isFamilyFriendly": true, "displayUrl": "https://selfawarepatterns.com/2018/07/02/the-prospects-for-a-scientific-understanding...", "snippet": "<b>Each</b> <b>individual</b> light is similar, but the mechanism of reading the board to produce distinct outputs <b>can</b> differentiate <b>each</b> <b>individual</b> light from the others. Imagine there is a camera looking at a room and wired such that when a red light is on in the room, a particular but arbitrary light on the board lights up. When a blue light is on, a different light on the board is turned on. As far as the mechanism is concerned, there is only inputs and associated outputs. But the mechanism is not ...", "dateLastCrawled": "2022-01-08T19:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "/docs/ai/ <b>Directory</b> Listing \u00b7 Gwern.net", "url": "https://www.gwern.net/docs/ai/index", "isFamilyFriendly": true, "displayUrl": "https://www.gwern.net/docs/ai", "snippet": "We bypass the quadratic cost by considering <b>self-attention</b> as a sum of <b>individual</b> tokens associated with Bernoulli random variables that <b>can</b>, in principle, be sampled at once by a single hash (although in practice, this number may be a small constant). This leads to an efficient sampling scheme to estimate <b>self-attention</b> which relies on specific modifications of LSH (to enable deployment on GPU architectures). We evaluate our algorithm on the GLUE benchmark with standard 512 sequence length ...", "dateLastCrawled": "2022-02-02T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Social Psychology</b> Flashcards | Quizlet", "url": "https://quizlet.com/156414423/social-psychology-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/156414423/<b>social-psychology</b>-flash-cards", "snippet": "contains a special type of brain cells or <b>neuron</b>&#39;s <b>called</b> spindle <b>neuron</b>, which are much larger than other neutrons in the brain. spindle neurons . spindle neurons collect waves and neural signals from one region of the brain and send them to other regions it appears the anterior cingulate cortex with <b>its</b> spindle neurons act as an executive attention system that facilitates self awareness humans are one of only a few species who possess spindle <b>neuron</b> research indicate when people are trying ...", "dateLastCrawled": "2018-11-29T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "MM &#39;20: Proceedings of the 28th ACM International Conference on ...", "url": "http://sigmm.org/opentoc/MM2020-TOC-2", "isFamilyFriendly": true, "displayUrl": "sigmm.org/opentoc/MM2020-TOC-2", "snippet": "To <b>learn</b> more discriminative representations for videos, we not only <b>learn</b> the video dynamic <b>information</b> but <b>also</b> focus on the static postures of the detected athletes in specific frames, which represent the action quality at certain moments, along with the help of the proposed hybrid dynamic-static architecture. Moreover, we leverage a context-aware attention module consisting of a temporal instance-wise graph convolutional network unit and an attention unit for both streams to extract more ...", "dateLastCrawled": "2022-01-29T07:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "iclr2017-submission-papers-index/openreview.txt at master - <b>github.com</b>", "url": "https://github.com/rickiepark/iclr2017-submission-papers-index/blob/master/openreview.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rickiepark/iclr2017-submission-papers-index/blob/master/openreview.txt", "snippet": "<b>Learn</b> and contribute. Topics \u2192 Collections \u2192 Trending \u2192 Learning Lab \u2192 Open source guides \u2192 Connect with others. The ReadME Project \u2192 Events \u2192 Community forum \u2192 GitHub Education \u2192 GitHub Stars program \u2192 Marketplace; Pricing Plans \u2192 Compare plans \u2192 Contact Sales \u2192 Education \u2192 In this repository All GitHub \u21b5 Jump to \u21b5 No suggested jump to results; In this repository All GitHub \u21b5 Jump to \u21b5 In this user All GitHub \u21b5 Jump to \u21b5 In this repository All GitHub ...", "dateLastCrawled": "2021-11-05T19:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training", "url": "https://www.researchgate.net/publication/353690477_Enhancing_LSTM_Models_with_Self-attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353690477_Enhancing_LSTM_Models_with_Self...", "snippet": "<b>Self-attention</b>, <b>also</b> known as in tra -attention, is an attention mec hanism re-lating di\ufb00erent positions of a sequence in order to model dependencies b etween. di\ufb00erent parts of the sequence ...", "dateLastCrawled": "2022-01-13T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Hands-On Machine Learning with Scikit-<b>Learn</b> &amp; TensorFlow CONCEPTS ...", "url": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow_CONCEPTS_TOOLS_AND_TECHNIQUES_TO_BUILD_INTELLIGENT_SYSTEMS", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39825063/Hands_On_Machine_<b>Learn</b>ing_with_Scikit_<b>Learn</b>_and...", "snippet": "Hands-On Machine Learning with <b>Scikit-Learn &amp; TensorFlow CONCEPTS, TOOLS, AND TECHNIQUES TO BUILD INTELLIGENT SYSTEMS</b> . \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. <b>Remember</b> me on this computer. or reset password. Enter the email address you signed up with and we&#39;ll email you a reset link. ...", "dateLastCrawled": "2022-01-30T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "notes-1/Deep Learning.md at master \u00b7 kirk86/notes-1 \u00b7 <b>GitHub</b>", "url": "https://github.com/kirk86/notes-1/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kirk86/notes-1/blob/master/Deep <b>Learn</b>ing.md", "snippet": "During this stage, mutual <b>information</b> between <b>each</b> <b>layer</b>&#39;s representation and the <b>input</b>/output increases to the point that the network&#39;s representation in the <b>information</b> plane is more or less linear. All this means is that <b>information</b> loss/gain from <b>layer</b> to <b>layer</b> is approximately constant, so in a sense no <b>layer</b> is doing more work than others. The second phase consists of continuing to maximizing the mutual <b>information</b> between <b>each</b> <b>layer</b> and the output, but now at the expense of the mutual ...", "dateLastCrawled": "2021-12-24T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation", "url": "https://www.researchgate.net/publication/308760773_Bi-directional_LSTM_Recurrent_Neural_Network_for_Chinese_Word_Segmentation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/308760773_Bi-directional_LSTM_Recurrent...", "snippet": "The difference between RNN and ordinary neural network is that the <b>neuron</b> will not only receive the <b>input</b> of the current time point, but <b>also</b> receive the output of the previous <b>neuron</b>, which ...", "dateLastCrawled": "2021-12-16T14:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The prospects for a scientific understanding of ... - SelfAwarePatterns", "url": "https://selfawarepatterns.com/2018/07/02/the-prospects-for-a-scientific-understanding-of-consciousness/", "isFamilyFriendly": true, "displayUrl": "https://selfawarepatterns.com/2018/07/02/the-prospects-for-a-scientific-understanding...", "snippet": "<b>Each</b> <b>individual</b> light is similar, but the mechanism of reading the board to produce distinct outputs <b>can</b> differentiate <b>each</b> <b>individual</b> light from the others. Imagine there is a camera looking at a room and wired such that when a red light is on in the room, a particular but arbitrary light on the board lights up. When a blue light is on, a different light on the board is turned on. As far as the mechanism is concerned, there is only inputs and associated outputs. But the mechanism is not ...", "dateLastCrawled": "2022-01-08T19:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Data Scientist Pocket Guide: Over 600 Concepts, Terminologies, and ...", "url": "https://dokumen.pub/data-scientist-pocket-guide-over-600-concepts-terminologies-and-processes-of-machine-learning-and-deep-learning-assembled-together-english-edition-9390684978-9789390684977.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/data-scientist-pocket-guide-over-600-concepts-terminologies-and...", "snippet": "Hidden layers are layers between the <b>input</b> <b>layer</b> and the output <b>layer</b> in a neural network, <b>its</b> role is to <b>learn</b> features from <b>input</b> <b>layer</b>. While increasing the number of hidden layers, it helps the neural network to <b>learn</b> more complex features from the <b>input</b> data. Building a state-of-the-art deep neural network \ufb01rst depends on the type of problem that has to be solved. A problem in image classi\ufb01cation doesn\u2019t require the same architecture as a problem in anomaly detection or ...", "dateLastCrawled": "2022-01-31T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Artificial Intelligence with Python: Your complete guide to building ...", "url": "https://dokumen.pub/artificial-intelligence-with-python-your-complete-guide-to-building-intelligent-apps-using-python-3x-and-tensorflow-2-2nbsped-183921953x-9781839219535.html", "isFamilyFriendly": true, "displayUrl": "https://<b>dokumen.pub</b>/artificial-intelligence-with-python-your-complete-guide-to...", "snippet": "Insurance pricing Machine learning algorithms <b>can</b> be used to <b>better</b> price insurance by more accurately predicting how much will be spent on a patient, how good a driver an <b>individual</b> is, or how long a person will live. As an example, the young.ai project from Insilico Medicine <b>can</b> predict with some accuracy how long someone will live from a blood sample and a photograph. The blood sample provides 21 biomarkers such as cholesterol level, inflammation markers, hemoglobin counts and albumin ...", "dateLastCrawled": "2022-01-22T13:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "iclr2017-submission-papers-index/openreview.txt at master - <b>github.com</b>", "url": "https://github.com/rickiepark/iclr2017-submission-papers-index/blob/master/openreview.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rickiepark/iclr2017-submission-papers-index/blob/master/openreview.txt", "snippet": "<b>Learn</b> and contribute. Topics \u2192 Collections \u2192 Trending \u2192 Learning Lab \u2192 Open source guides \u2192 Connect with others. The ReadME Project \u2192 Events \u2192 Community forum \u2192 GitHub Education \u2192 GitHub Stars program \u2192 Marketplace; Pricing Plans \u2192 Compare plans \u2192 Contact Sales \u2192 Education \u2192 In this repository All GitHub \u21b5 Jump to \u21b5 No suggested jump to results; In this repository All GitHub \u21b5 Jump to \u21b5 In this user All GitHub \u21b5 Jump to \u21b5 In this repository All GitHub ...", "dateLastCrawled": "2021-11-05T19:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Robots and AI: Our Immortality or Extinction - page 21 - The rest ...", "url": "https://forum.arctic-sea-ice.net/index.php?topic=1392.1000", "isFamilyFriendly": true, "displayUrl": "https://forum.arctic-sea-ice.net/index.php?topic=1392.1000", "snippet": "The experts in <b>each</b> <b>layer</b> are controlled by a gating network that activates experts based on the <b>input</b> data. For <b>each</b> token (generally a word or part of a word), the gating network selects the two most appropriate experts to process the data. The full version of GLaM has 1.2 Trillion total parameters across 64 experts per MoE <b>layer</b> with 32 MoE layers in total, but only activates a subnetwork of 97 Billion (8% of 1.2 Trillion) parameters per token prediction during inference.", "dateLastCrawled": "2022-01-30T05:37:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention in Psychology, Neuroscience, and <b>Machine</b> <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7177153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7177153", "snippet": "These weightings scale the word encodings themselves to create the next <b>layer</b> in the model, a process known as \u201c<b>self-attention</b>.\u201d This process repeats, and eventually interacts with the autoregressive decoder which <b>also</b> has attention mechanisms that allow it to flexibly focus on the encoded input (as in the standard form of attention) and on the previously generated output. The Transformer\u2014the name given to this new attention architecture\u2014outperformed many previous models and quickly ...", "dateLastCrawled": "2021-12-27T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>learning</b> glossary - DataTime", "url": "https://www.dtalg.com/article-1/", "isFamilyFriendly": true, "displayUrl": "https://www.dtalg.com/article-1", "snippet": "<b>self-attention</b> (<b>also</b> <b>called</b> <b>self-attention</b> <b>layer</b>) #language. A neural network <b>layer</b> that transforms a sequence of embeddings (for instance, token embeddings) into another sequence of embeddings. Each embedding in the output sequence is constructed by integrating information from the elements of the input sequence through an attention mechanism.", "dateLastCrawled": "2022-01-25T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "10.6. <b>Self-Attention</b> and <b>Positional Encoding</b> \u2014 Dive into Deep <b>Learning</b> ...", "url": "http://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_attention-mechanisms/<b>self-attention</b>-and-<b>positional-encoding</b>.html", "snippet": "In deep <b>learning</b>, we often use CNNs or RNNs to encode a sequence. Now with attention mechanisms, imagine that we feed a sequence of tokens into attention pooling so that the same set of tokens act as queries, keys, and values. Specifically, each query attends to all the key-value pairs and generates one attention output. Since the queries, keys, and values come from the same place, this performs <b>self-attention</b> [Lin et al., 2017b] [Vaswani et al., 2017], which is <b>also</b> <b>called</b> intra-attention ...", "dateLastCrawled": "2022-02-02T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Lecture 7: Transformers</b> - Deep <b>Learning</b>", "url": "https://chinmayhegde.github.io/dl-notes/notes/lecture07/", "isFamilyFriendly": true, "displayUrl": "https://chinmayhegde.github.io/dl-notes/notes/lecture07", "snippet": "<b>Self-Attention</b>. This is the point where papers-blogs-tweets-slides etc start talking about keys/values and attention mechanisms and everything goes a bit haywire. Let\u2019s just ignore all that for now, and instead talk about something <b>called</b> <b>self-attention</b>. The use of the \u201cself-\u201c prefix will become clear later on. Here is how it is defined.", "dateLastCrawled": "2022-02-03T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training ...", "url": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_Self_attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_<b>Self_attention</b>_and_Statef...", "snippet": "<b>Self-attention</b>, <b>also</b> known as intra-attention, is an attention mechanism relat- ing di\ufb00erent positions of a sequence in order to model dependencies between dif- ferent parts of the sequence. This di\ufb00ers from general attention in that instead of seeking to discover the \u201cimportant\u201d parts of the sequence relating to the net- work output, <b>self-attention</b> seeks to \ufb01nd the \u201cimportant\u201d portions of the sequence that relate to each other. This is done in order to leverage those intra ...", "dateLastCrawled": "2022-02-03T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "<b>Self attention</b>, sometimes <b>called</b> intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. In simpler terms, <b>self attention</b> helps us create similar connections but within the same sentence. Look at the following example: \u201cI poured water from the bottle into the cup until it was full.\u201d it =&gt; cup \u201cI poured water from the bottle into the cup until it was empty.\u201d it=&gt; bottle. By changing one word ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning Papers: Molecules</b> - <b>Machine Learning</b> Applied", "url": "https://machinelearningapplied.com/machine-learning-papers-molecules/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>applied.com/<b>machine-learning-papers-molecules</b>", "snippet": "A <b>self-attention</b> based message passing neural network for predicting molecular lipophilicity and aqueous solubility - Tang et al 2020 . Efficient and accurate prediction of molecular properties, such as lipophilicity and solubility, is highly desirable for rational compound design in chemical and pharmaceutical industries. To this end, we build and apply a graph-neural-network framework <b>called</b> <b>self-attention</b>-based message-passing neural network (SAMPN) to study the relationship between ...", "dateLastCrawled": "2021-12-22T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The encoder is composed of a stack of N = 6 identical layers. Each <b>layer</b> has two sub-layers. The first is a multi-head <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by <b>layer</b> normalization. That is, the output ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Journal of Physics: Conference Series PAPER OPEN ACCESS You may <b>also</b> ...", "url": "https://iopscience.iop.org/article/10.1088/1742-6596/1651/1/012019/pdf", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1742-6596/1651/1/012019/pdf", "snippet": "Different <b>machine</b> <b>learning</b> techniques have been used in this field for many years. But recently, deep <b>learning</b> has caused more and more attention in the field of education. Deep <b>learning</b> is a <b>machine</b> <b>learning</b> method based on neural network structure of multi-<b>layer</b> processing units, and it has been successfully applied to a series of problems in the field of image recognition and natural language processing[2]. With the diversified cultivation of traditional universities and the development ...", "dateLastCrawled": "2021-12-29T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is &#39;attention&#39; in the context of deep <b>learning</b>? - Quora", "url": "https://www.quora.com/What-is-attention-in-the-context-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-attention-in-the-context-of-deep-<b>learning</b>", "snippet": "Answer (1 of 5): In feed-forward deep networks, the entire input is presented to the network, which computes an output in one pass. In recurrent networks, new inputs can be presented at each time step, and the output of the previous time step can be used as an input to the network. This can be ...", "dateLastCrawled": "2022-01-15T04:26:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(self-attention (also called self-attention layer))  is like +(a way of giving each individual neuron its own input, allowing it to better learn and remember information)", "+(self-attention (also called self-attention layer)) is similar to +(a way of giving each individual neuron its own input, allowing it to better learn and remember information)", "+(self-attention (also called self-attention layer)) can be thought of as +(a way of giving each individual neuron its own input, allowing it to better learn and remember information)", "+(self-attention (also called self-attention layer)) can be compared to +(a way of giving each individual neuron its own input, allowing it to better learn and remember information)", "machine learning +(self-attention (also called self-attention layer) AND analogy)", "machine learning +(\"self-attention (also called self-attention layer) is like\")", "machine learning +(\"self-attention (also called self-attention layer) is similar\")", "machine learning +(\"just as self-attention (also called self-attention layer)\")", "machine learning +(\"self-attention (also called self-attention layer) can be thought of as\")", "machine learning +(\"self-attention (also called self-attention layer) can be compared to\")"]}