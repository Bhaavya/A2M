{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Target output <b>distribution</b> and <b>distribution</b> of <b>bias</b> for statistical ...", "url": "https://link.springer.com/article/10.1007/s00158-019-02338-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00158-019-02338-z", "snippet": "Second, even with limited data, the proposed target output <b>distribution</b> can closely approximate the <b>true</b> output <b>distribution</b> and thus can be effectively used to measure the <b>distribution</b> of <b>bias</b>. Third, the proposed target output <b>distribution</b> is robust under possible different sets of test data. In addition, since only limited data is available, a certain dataset could have large gaps <b>between</b> data (i.e., sparse data) or outliers, which may lead to an overestimation of the output variance. To ...", "dateLastCrawled": "2022-01-15T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Fairness metrics and <b>bias</b> mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "<b>Distribution</b>-based fairness metrics - inequality index . Whereas the previous metrics consider fairness from a group perspective, Speicher et al. (2018) consider the general <b>distribution</b> of fairness for classification using an inequality index. Specifically, they define a Generalized Entropy Index (GEI) that measures the inequality <b>between</b> all users with respect to how fair they are treated by the algorithm. Entropy based metrics such as Generalized Entropy Index (GEI) are a family of ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Bunching up the background betters <b>bias</b> in species <b>distribution</b> models ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/ecog.04503", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/ecog.04503", "snippet": "Relative presence <b>probability</b> of Sitka spruce, <b>predicted</b> by models using CHELSA climate data and four different sampling <b>bias</b> approaches. ... because the <b>true</b> sampling <b>probability</b> <b>distribution</b> is unknown. Without added, reliable information about the <b>true</b> sampling or presence <b>probability</b> distributions, the extent to which a correction reduces existing <b>bias</b> or introduces new <b>bias</b> always remains ambiguous (Yackulic et al. 2013). Target group background selection and formal methods <b>like</b> Maxent ...", "dateLastCrawled": "2022-02-01T16:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Lars&#39; Blog - <b>Metrics for uncertainty estimation</b>", "url": "https://lars76.github.io/2020/08/07/metrics-for-uncertainty-estimation.html", "isFamilyFriendly": true, "displayUrl": "https://lars76.github.io/2020/08/07/<b>metrics-for-uncertainty-estimation</b>.html", "snippet": "Calibration metrics measure the difference <b>between</b> \u201c<b>true</b> confidence\u201d and \u201c<b>predicted</b> confidence\u201d. If \\(\\hat{p}\\) equals \\(0.6\\), then it should mean that the neural network is 60% sure. A model is calibrated if \\(\\mathbf{P}\\left(\\hat{Y} = y \\mid \\hat{P} = p\\right) = p\\). Then the difference is \\(\\left\\lvert \\mathbf{P}\\left(\\hat{Y} = y \\mid \\hat{P} = p\\right) - p\\right\\rvert\\). The <b>predicted</b> confidence is the output <b>probability</b> of the neural network, while the <b>true</b> confidence is ...", "dateLastCrawled": "2022-01-28T21:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Performance and Prediction \u2014 <b>H2O</b> 3.36.0.2 documentation", "url": "https://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html", "isFamilyFriendly": true, "displayUrl": "https://docs.<b>h2o</b>.ai/<b>h2o</b>/latest-stable/<b>h2o</b>-docs/performance-and-prediction.html", "snippet": "The Kolmogorov-Smirnov (KS) <b>metric</b> represents the degree of separation <b>between</b> the positive (1) and negative (0) cumulative <b>distribution</b> functions for a binomial model. It is a nonparametric test that compares the cumulative distributions of two unmatched data sets and does not assume that data are sampled from any defined distributions. The KS <b>metric</b> has more power to detect changes in the shape of the <b>distribution</b> and less to detect a shift in the median because it tests for more ...", "dateLastCrawled": "2022-02-03T03:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Model Validation, KS Test and Lorenz</b> Curve \u2013 KDAG", "url": "https://kgpdag.wordpress.com/2016/03/14/model-validation-ks-test-and-lorenz-curve/", "isFamilyFriendly": true, "displayUrl": "https://kgpdag.wordpress.com/2016/03/14/<b>model-validation-ks-test-and-lorenz</b>-curve", "snippet": "It is a <b>distance</b> measure <b>between</b> the <b>predicted</b> numeric target and the actual numeric answer (ground truth). The smaller the value of the RMSE, the better is the predictive accuracy of the model. A model with perfectly correct predictions would have an RMSE of 0. Mathematically, it is defined as: R M S E = \u2211 i (y i \u2212 y i ^) 2 n \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u221a. Cross Validation. Cross validation is a model validation technique for assessing the fit of a model on a new unseen data set ...", "dateLastCrawled": "2022-02-02T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>The reasonable ineffectiveness of pixel metrics</b> for future prediction ...", "url": "https://medium.com/@olegrybkin_20684/the-reasonable-ineffectiveness-of-mse-pixel-loss-for-future-prediction-and-what-to-do-about-it-4dca8152355d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@olegrybkin_20684/the-reasonable-ineffectiveness-of-mse-pixel-loss...", "snippet": "The main difference is that L2 satisfies the triangle inequality and thus is a <b>true</b> <b>metric</b>, whereas MSE is not. Using L2, it makes sense to talk about <b>distance</b> <b>between</b> two images.", "dateLastCrawled": "2022-01-25T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Optimization metrics: DataRobot docs", "url": "https://docs.datarobot.com/en/docs/modeling/reference/model-detail/opt-metric.html", "isFamilyFriendly": true, "displayUrl": "https://docs.datarobot.com/en/docs/modeling/reference/model-detail/opt-<b>metric</b>.html", "snippet": "Where parameter p is an index value that differentiates <b>between</b> the <b>distribution</b> family. For example, 0 is Normal, 1 is Poisson, 1.5 is Tweedie, and 2 is Gamma. Interpreting these <b>metric</b> scores is not particularly intuitive. y and pred values are in the unit of target (e.g., dollars), but as can be seen above, log functions and scaling complicates it. You can transform this to a weighted deviance function simply by introducing a weights multiplier, for example for Poisson: Note. Because of ...", "dateLastCrawled": "2022-01-31T16:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "Binomial <b>distribution</b> is a <b>probability</b> with only two possible outcomes, the prefix \u2018bi\u2019 means two or twice. An example of this would be a coin toss. The outcome will either be heads or tails. Normal <b>distribution</b> describes how the values of a variable are distributed. It is typically a symmetric <b>distribution</b> where most of the observations cluster around the central peak. The values further away from the mean taper off equally in both directions. An example would be the height of students ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Top 75 Statistics Interview Questions</b> &amp; Answers 2022 - Intellipaat", "url": "https://intellipaat.com/blog/interview-question/statistics-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/blog/interview-question/statistics", "snippet": "A long-tailed <b>distribution</b> is a type of <b>distribution</b> where the tail drops off gradually toward the end of the curve. The Pareto principle and the product sales <b>distribution</b> are good examples to denote the use of long-tailed distributions. Also, it is widely used in classification and regression problems. 3. What is the central limit theorem? The central limit theorem states that the normal <b>distribution</b> is arrived at when the sample size varies without having an effect on the shape of the ...", "dateLastCrawled": "2022-02-03T06:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) A <b>distance</b>-based sample selection <b>bias</b> <b>metric</b>", "url": "https://www.researchgate.net/publication/329538893_A_distance-based_sample_selection_bias_metric", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../329538893_A_<b>distance</b>-based_sample_selection_<b>bias</b>_<b>metric</b>", "snippet": "5.1 Comparing the <b>bias</b> <b>distance</b> <b>metric</b> to Pearson\u2019s \u03c7 2 test. F or the purp ose of the \ufb01rst part, consider the AGE v ariable in the dataset - a histogram plot, where the data range is split ...", "dateLastCrawled": "2022-01-03T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Fairness metrics and <b>bias</b> mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "The adjustment of the <b>predicted</b> scores and thus the specific type of <b>bias</b> mitigation depends on the targeted fairness <b>metric</b>, <b>similar</b> to the mitigation strategies developed for machine learning classification. The algorithm can be applied both as an in-processing approach by learning the adjustments during training procedure and applying them on the separate test data, or as post-processing approach which directly calculates the necessary adjustments on the test data itself (<b>similar</b> to post ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Assessing the performance of prediction models: a framework for some ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3575184", "snippet": "Overall performance measures. The <b>distance</b> <b>between</b> the <b>predicted</b> outcome and actual outcome is central to quantify overall model performance from a statistical modeler\u2019s perspective 32 p, and for survival outcomes it is the <b>predicted</b> event <b>probability</b> at a given time (or as a function of time). These distances <b>between</b> observed and <b>predicted</b> outcomes are related to the concept of \u2018goodness-of-fit\u2019 of a model, with better models having smaller distances <b>between</b> <b>predicted</b> and observed ...", "dateLastCrawled": "2022-02-03T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "fairMLHealth/Measures_QuickReference.md at integration \u00b7 KenSciResearch ...", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Measures_QuickReference.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "for a set of applicants V , a <b>distance</b> <b>metric</b> <b>between</b> applicants k : V \u00c5~V \u2192 R, a mapping from a set of applicants to <b>probability</b> distributions over outcomes M : V \u2192 \u03b4A, and a <b>distance</b> D <b>metric</b> <b>between</b> <b>distribution</b> of outputs, fairness is achieved iff : <b>Similar</b> individuals (as defined by some <b>distance</b> <b>metric</b>) should have <b>similar</b> ...", "dateLastCrawled": "2022-01-22T08:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Field validation shows bias\u2010corrected pseudo\u2010absence selection</b> is the ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/ddi.12249", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/ddi.12249", "snippet": "In turn, the <b>distribution</b> models are then calibrated on biased environmental data (Phillips et al., 2009), and if this <b>bias</b> is not handled with ad hoc procedures, the fitted models will more closely reflect the <b>distribution</b> of the observers\u2019 most frequent prospecting \u2018habitats\u2019 than the <b>true</b> species <b>distribution</b> (Phillips et al., 2009). Careful selection of the method for handling absence data in <b>distribution</b> modelling is thus a key step in the modelling process, which may have a ...", "dateLastCrawled": "2022-01-03T19:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>The reasonable ineffectiveness of pixel metrics</b> for future prediction ...", "url": "https://medium.com/@olegrybkin_20684/the-reasonable-ineffectiveness-of-mse-pixel-loss-for-future-prediction-and-what-to-do-about-it-4dca8152355d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@olegrybkin_20684/the-reasonable-ineffectiveness-of-mse-pixel-loss...", "snippet": "The main difference is that L2 satisfies the triangle inequality and thus is a <b>true</b> <b>metric</b>, whereas MSE is not. Using L2, it makes sense to talk about <b>distance</b> <b>between</b> two images.", "dateLastCrawled": "2022-01-25T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "fairMLHealth/Evaluating_Fairness.md at integration \u00b7 KenSciResearch ...", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "for a set of applicants V , a <b>distance</b> <b>metric</b> <b>between</b> applicants k : V \u00c5~V \u2192 R, a mapping from a set of applicants to <b>probability</b> distributions over outcomes M : V \u2192 \u03b4A, and a <b>distance</b> D <b>metric</b> <b>between</b> <b>distribution</b> of outputs, fairness is achieved iff : <b>Similar</b> individuals (as defined by some <b>distance</b> <b>metric</b>) should have <b>similar</b> ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Optimization metrics: DataRobot docs", "url": "https://docs.datarobot.com/en/docs/modeling/reference/model-detail/opt-metric.html", "isFamilyFriendly": true, "displayUrl": "https://docs.datarobot.com/en/docs/modeling/reference/model-detail/opt-<b>metric</b>.html", "snippet": "Where parameter p is an index value that differentiates <b>between</b> the <b>distribution</b> family. For example, 0 is Normal, 1 is Poisson, 1.5 is Tweedie, and 2 is Gamma. Interpreting these <b>metric</b> scores is not particularly intuitive. y and pred values are in the unit of target (e.g., dollars), but as can be seen above, log functions and scaling complicates it. You can transform this to a weighted deviance function simply by introducing a weights multiplier, for example for Poisson: Note. Because of ...", "dateLastCrawled": "2022-01-31T16:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "Binomial <b>distribution</b> is a <b>probability</b> with only two possible outcomes, the prefix \u2018bi\u2019 means two or twice. An example of this would be a coin toss. The outcome will either be heads or tails. Normal <b>distribution</b> describes how the values of a variable are distributed. It is typically a symmetric <b>distribution</b> where most of the observations cluster around the central peak. The values further away from the mean taper off equally in both directions. An example would be the height of students ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Top 75 Statistics Interview Questions</b> &amp; Answers 2022 - Intellipaat", "url": "https://intellipaat.com/blog/interview-question/statistics-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/blog/interview-question/statistics", "snippet": "A long-tailed <b>distribution</b> is a type of <b>distribution</b> where the tail drops off gradually toward the end of the curve. The Pareto principle and the product sales <b>distribution</b> are good examples to denote the use of long-tailed distributions. Also, it is widely used in classification and regression problems. 3. What is the central limit theorem? The central limit theorem states that the normal <b>distribution</b> is arrived at when the sample size varies without having an effect on the shape of the ...", "dateLastCrawled": "2022-02-03T06:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Fairness metrics and <b>bias</b> mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "the overall difference \u03b4 <b>between</b> <b>predicted</b> ratings for two groups g 1 and g 2 is learned on the training set: (12) \u03b4 t r a i n = y \u0304 \u02c6 g 1, t r a i n \u2212 y \u0304 \u02c6 g 2, t r a i n Then, it is used to adjust the test set predictions to reduce non-parity. (13) y \u02c6 i, j, g 1, t e s t = y \u02c6 i, j, g 1, t e s t + \u03b4 t r a i n. Download : Download high-res image (517KB) Download : Download full-size image; The pseudocode of the <b>bias</b> mitigation strategy is described in Algorithm 1. The rationale ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Introduction to Regression with SPSS Lesson 2: SPSS Regression Diagnostics", "url": "https://stats.oarc.ucla.edu/spss/seminars/introduction-to-regression-with-spss/introreg-lesson2/", "isFamilyFriendly": true, "displayUrl": "https://stats.oarc.ucla.edu/spss/seminars/introduction-to-regression-with-spss/introre...", "snippet": "The lowest value that Cook\u2019s D <b>can</b> assume is zero, and the higher the Cook\u2019s D is, the more influential the point is. The conventional cut-off point is 4/n, or in this case 4/400 or .01. School 2910 is the top influential point. DFBETA: Cook\u2019s <b>Distance</b> <b>can</b> <b>be thought</b> of as a general measure of influence. You <b>can</b> also consider more ...", "dateLastCrawled": "2022-02-02T18:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What does RMSE really mean?. Root <b>Mean Square</b> Error (RMSE) is a\u2026 | by ...", "url": "https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e", "snippet": "These errors, <b>thought</b> of as random variables, might have Gaussian <b>distribution</b> with mean \u03bc and standard deviation \u03c3, but any other <b>distribution</b> with a square-integrable PDF (<b>probability</b> density function) would also work. We want to think of \u0177\u1d62 as an underlying physical quantity, such as the exact <b>distance</b> from Mars to the Sun at a particular point in time. Our observed quantity y\u1d62 would then be the <b>distance</b> from Mars to the Sun", "dateLastCrawled": "2022-02-02T23:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Model selection and psychological theory: A discussion of the ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3366160/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3366160", "snippet": "Here we see that the posterior <b>probability</b> <b>distribution</b> of the parameters on the left-hand side is proportional to the <b>probability</b> <b>distribution</b> of the data given the parameters and model, as well as the prior <b>probability</b> <b>distribution</b> of the parameters given the model in the first place, before any data were observed. p(\u03c4 \u2113 |M \u2113), the prior <b>probability</b> <b>distribution</b> of the parameters given the model, must be specified a priori by the researcher. Equation (3) determines how prior beliefs ...", "dateLastCrawled": "2022-01-29T09:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: Data ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "It <b>can</b> be equivalently <b>thought</b> of as the <b>probability</b> of accepting the alternative hypothesis (H1) when it is <b>true</b>\u2014that is, the ability of a test to detect an effect, if the effect actually exists. To put in another way, Statistical power is the likelihood that a study will detect an effect when the effect is present.", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "On the Importance of Firth <b>Bias</b> Reduction in Few-Shot Classification ...", "url": "https://deepai.org/publication/on-the-importance-of-firth-bias-reduction-in-few-shot-classification", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-the-importance-of-firth-<b>bias</b>-reduction-in-few-shot...", "snippet": "In this paper, we show that using Firth <b>bias</b> reduction produces reliable improvements in a wide range of circumstancesWe achieve this by deriving a simplified yet effective Firth formulation that penalizes the KL-divergence <b>between</b> the <b>predicted</b> <b>distribution</b> and the uniform <b>distribution</b> of classes, for both . multinomial logistic regression models and cosine classifiers. Note that common regularization techniques (such as L2-regularization and label smoothing [szegedy2016rethinking]) cannot ...", "dateLastCrawled": "2022-01-19T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Test Your Skills: 26 <b>Data Science</b> Interview Questions &amp; Answers | by ...", "url": "https://medium.com/analytics-vidhya/test-your-skills-26-data-science-interview-questions-answers-69cb2b223e57", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/test-your-skills-26-<b>data-science</b>-interview...", "snippet": "A normal <b>distribution</b>, also known as a Bell Curve, <b>can</b> be described as a <b>distribution</b> with the majority of instances clustered at the center, and the number of instances decreasing as <b>distance</b> ...", "dateLastCrawled": "2022-01-28T14:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Data and Sampling Distributions</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/data-and-sampling-distributions/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/<b>data-and-sampling-distributions</b>", "snippet": "For events that occur at a constant rate, the number of events per unit of time or space <b>can</b> be modeled as a Poisson <b>distribution</b>. In this scenario, you <b>can</b> also model the time or <b>distance</b> <b>between</b> one event and the next as an exponential <b>distribution</b>. A changing event rate over time (e.g., an increasing <b>probability</b> of device failure) <b>can</b> be ...", "dateLastCrawled": "2022-02-02T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Contrastive Representation Learning", "url": "https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html", "isFamilyFriendly": true, "displayUrl": "https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html", "snippet": "Chuang et al. (2020) studied the sampling <b>bias</b> in <b>contrastive learning</b> and proposed debiased loss. In the unsupervised setting, since we do not know the ground truth labels, we may accidentally sample false negative samples. Sampling <b>bias</b> <b>can</b> lead to significant performance drop. Fig. 3. Sampling <b>bias</b> which refers to false negative samples in ...", "dateLastCrawled": "2022-02-02T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Optimization metrics: DataRobot docs", "url": "https://docs.datarobot.com/en/docs/modeling/reference/model-detail/opt-metric.html", "isFamilyFriendly": true, "displayUrl": "https://docs.datarobot.com/en/docs/modeling/reference/model-detail/opt-<b>metric</b>.html", "snippet": "In this paper, in binary classification problems, it has been used as dissimilarity <b>metric</b> for assessing the classifier\u2019s discriminant power measuring the <b>distance</b> that its score produces <b>between</b> the cumulative <b>distribution</b> functions (CDFs) of the two data classes, known as KS2 for this purpose (two samples). The usual <b>metric</b> for both purposes is the maximum vertical difference (MVD) <b>between</b> the CDFs (the Max_KS), which is invariant to score range and scale making it suitable for ...", "dateLastCrawled": "2022-01-31T16:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) A <b>distance</b>-based sample selection <b>bias</b> <b>metric</b>", "url": "https://www.researchgate.net/publication/329538893_A_distance-based_sample_selection_bias_metric", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../329538893_A_<b>distance</b>-based_sample_selection_<b>bias</b>_<b>metric</b>", "snippet": "5.1 Comparing the <b>bias</b> <b>distance</b> <b>metric</b> to Pearson\u2019s \u03c7 2 test. F or the purp ose of the \ufb01rst part, consider the AGE v ariable in the dataset - a histogram plot, where the data range is split ...", "dateLastCrawled": "2022-01-03T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Fairness metrics and <b>bias</b> mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "<b>Distribution</b>-based fairness metrics - inequality index . Whereas the previous metrics consider fairness from a group perspective, Speicher et al. (2018) consider the general <b>distribution</b> of fairness for classification using an inequality index. Specifically, they define a Generalized Entropy Index (GEI) that measures the inequality <b>between</b> all users with respect to how fair they are treated by the algorithm. Entropy based metrics such as Generalized Entropy Index (GEI) are a family of ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Assessing the performance of prediction models: a framework for some ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3575184", "snippet": "Overall performance measures. The <b>distance</b> <b>between</b> the <b>predicted</b> outcome and actual outcome is central to quantify overall model performance from a statistical modeler\u2019s perspective 32 p, and for survival outcomes it is the <b>predicted</b> event <b>probability</b> at a given time (or as a function of time). These distances <b>between</b> observed and <b>predicted</b> outcomes are related to the concept of \u2018goodness-of-fit\u2019 of a model, with better models having smaller distances <b>between</b> <b>predicted</b> and observed ...", "dateLastCrawled": "2022-02-03T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Target output <b>distribution</b> and <b>distribution</b> of <b>bias</b> for statistical ...", "url": "https://link.springer.com/article/10.1007/s00158-019-02338-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00158-019-02338-z", "snippet": "Second, even with limited data, the proposed target output <b>distribution</b> <b>can</b> closely approximate the <b>true</b> output <b>distribution</b> and thus <b>can</b> be effectively used to measure the <b>distribution</b> of <b>bias</b>. Third, the proposed target output <b>distribution</b> is robust under possible different sets of test data. In addition, since only limited data is available, a certain dataset could have large gaps <b>between</b> data (i.e., sparse data) or outliers, which may lead to an overestimation of the output variance. To ...", "dateLastCrawled": "2022-01-15T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Top 75 Statistics Interview Questions</b> &amp; Answers 2022 - Intellipaat", "url": "https://intellipaat.com/blog/interview-question/statistics-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/blog/interview-question/statistics", "snippet": "What is the <b>probability</b> of throwing two fair dice when the sum is 5 and 8? There are 4 ways of rolling a 5 (1+4, 4+1, 2+3, 3+2): P(Getting a 5) = 4/36 = 1/9. Now, there are 7 ways of rolling an 8 (1+7, 7+1, 2+6, 6+2, 3+5, 5+3, 4+4) P(Getting an 8) = 7/36 = 0.194. 13. State the case where the median is a better measure when <b>compared</b> to the mean. In the case where there are a lot of outliers that <b>can</b> positively or negatively skew data, the median is preferred as it provides an accurate measure ...", "dateLastCrawled": "2022-02-03T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Estimators, Loss Functions, Optimizers \u2014Core of ML Algorithms | by ...", "url": "https://towardsdatascience.com/estimators-loss-functions-optimizers-core-of-ml-algorithms-d603f6b0161a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/estimators-loss-functions-<b>optimizer</b>s-core-of-ml...", "snippet": "where P is the <b>distribution</b> of the <b>true</b> labels, ... In case of a binary classification each <b>predicted</b> <b>probability</b> is <b>compared</b> to the actual class output value (0 or 1) and a score is calculated that penalizes the <b>probability</b> based on the <b>distance</b> from the expected value. Visualization. The graph below shows the range of possible log loss values given a <b>true</b> observation (y= 1). As the <b>predicted</b> <b>probability</b> approaches 1, log loss slowly decreases. As the <b>predicted</b> <b>probability</b> decreases ...", "dateLastCrawled": "2022-02-02T04:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "3.3. Metrics and scoring: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "The sum <b>can</b> be truncated after the first \\(K\\) results, in which case we call it DCG@K. NDCG, or NDCG@K is DCG divided by the DCG obtained by a perfect prediction, so that it is always <b>between</b> 0 and 1. Usually, NDCG is preferred to DCG. <b>Compared</b> with the ranking loss, NDCG <b>can</b> take into account relevance scores, rather than a ground-truth ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "Binomial <b>distribution</b> is a <b>probability</b> with only two possible outcomes, the prefix \u2018bi\u2019 means two or twice. An example of this would be a coin toss. The outcome will either be heads or tails. Normal <b>distribution</b> describes how the values of a variable are distributed. It is typically a symmetric <b>distribution</b> where most of the observations cluster around the central peak. The values further away from the mean taper off equally in both directions. An example would be the height of students ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Cross-Entropy Loss</b> Function. A loss function used in most\u2026 | by Kiprono ...", "url": "https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>cross-entropy-loss</b>-function-f38c4ec8643e", "snippet": "Each <b>predicted</b> class <b>probability</b> is <b>compared</b> to the actual class desired output 0 or 1 and a score/loss is calculated that penalizes the <b>probability</b> based on how far it is from the actual expected value. The penalty is logarithmic in nature yielding a large score for large differences close to 1 and small score for small differences tending to 0.", "dateLastCrawled": "2022-02-02T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Definition of <b>Difference Between The Actual Value And Predicted Value</b> ...", "url": "https://www.chegg.com/homework-help/definitions/difference-between-the-actual-value-and-predicted-value-31", "isFamilyFriendly": true, "displayUrl": "https://www.chegg.com/.../<b>difference-between-the-actual-value-and-predicted-value</b>-31", "snippet": "Question 3 &gt; 2C For a group of four 50-year old men, the <b>probability</b> <b>distribution</b> for the number 2 who live through the next year is as given in the table below. P(2) 0 0.0+ 1 0.0001 2 0.0051 3 0.1095 0.8853 Verify...", "dateLastCrawled": "2022-02-03T06:41:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias</b>, Variance, and <b>Overfitting</b> Explained, Step by Step", "url": "https://machinelearningcompass.com/model_optimization/bias_and_variance/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>compass.com/model_optimization/<b>bias</b>_and_variance", "snippet": "The <b>bias</b> of a specific <b>machine learning</b> model trained on a specific dataset describes how well this <b>machine learning</b> model can capture the relationship between the features and the targets. So for our example, the <b>bias</b> of any one model would tell us how well this particular model can predict the exam points received for any number of hours studied in our specific dataset. That seems like a reasonable definition. Practical Examples (<b>Bias</b>) Let\u2019s take a look at three of the above models and ...", "dateLastCrawled": "2022-01-31T18:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Short Discussion On <b>Bias</b> In <b>Machine</b> <b>Learning</b>", "url": "https://www.encora.com/insights/a-short-discussion-on-bias-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.encora.com/insights/a-short-discussion-on-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "A typical <b>machine</b> <b>learning</b> lifecycle might start with a Scoping stage. At this point, an important decision to be made by the analysts regards the level of performance the <b>machine</b> <b>learning</b> system should have. The <b>machine</b> <b>learning</b> team, along with the stakeholders involved, should decide on a <b>metric</b> to be used as a measure of success. This ...", "dateLastCrawled": "2022-02-03T02:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Short Discussion on <b>Bias</b> in <b>Machine</b> <b>Learning</b> | by Daitan | Daitan ...", "url": "https://medium.com/daitan-tech/a-short-discussion-on-bias-in-machine-learning-5bb2066afabc", "isFamilyFriendly": true, "displayUrl": "https://medium.com/daitan-tech/a-short-discussion-on-<b>bias</b>-in-<b>machine</b>-<b>learning</b>-5bb2066afabc", "snippet": "The problem of <b>bias</b> in <b>machine</b> <b>learning</b> is very serious. Moreover, though it seems to be a \u201cdata related\u201d problem, one might think that it can be solved by simply curating datasets so that ...", "dateLastCrawled": "2021-08-05T05:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Short Discussion on <b>Bias</b> in <b>Machine</b> <b>Learning</b> - Adolfo Eliaz\u00e0t ...", "url": "https://adolfoeliazat.com/2021/06/16/a-short-discussion-on-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://adolfoeliazat.com/2021/06/16/a-short-discussion-on-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "The problem of <b>bias</b> in <b>machine</b> <b>learning</b> is very serious. Moreover, though it seems to be a \u201cdata related\u201d problem, one might think that it can be solved by simply curating datasets so that classes and ethical groups are well represented. This line of thinking is a trap and must be avoided. Overall, <b>bias</b> in technology can happen anywhere or anytime a decision must be taken by a human. In such situations, it is very common to consider aspects that make sense from a marketing or profit ...", "dateLastCrawled": "2022-01-16T17:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "CS 540 Lecture Notes: <b>Machine</b> <b>Learning</b>", "url": "https://pages.cs.wisc.edu/~dyer/cs540/notes/learning.html", "isFamilyFriendly": true, "displayUrl": "https://pages.cs.wisc.edu/~dyer/cs540/notes/<b>learning</b>.html", "snippet": "Inductive <b>Bias</b>. Inductive <b>learning</b> is an inherently conjectural process because any knowledge created by generalization from specific facts cannot be proven true; it can only be proven false. Hence, inductive inference is falsity preserving, not truth preserving. To generalize beyond the specific training examples, we need constraints or biases on what f is best. That is, <b>learning</b> can be viewed as searching the Hypothesis Space H of possible f functions. A <b>bias</b> allows us to choose one f over ...", "dateLastCrawled": "2022-02-03T15:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A New <b>Metric</b> for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare ...", "url": "https://towardsdatascience.com/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare-closedloop-ai-fc07b9c83487", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-new-<b>metric</b>-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "The <b>analogy</b> would be the difference between a Pearson correlation or a residual sum of squared errors in regression. While both quantify the models performance, the former is significantly easier to understand and explain; unsurprisingly, it is the <b>metric</b> most individuals use to describe a regression model. In order to achieve this effect, many fairness metrics are presented as the quotient of a protected subgroup to a base subgroup[2]. As the goal of healthcare is to deliver interventions ...", "dateLastCrawled": "2022-01-17T14:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bias</b> -Variance &amp; <b>Precision</b>-Recall Trade-offs: How to aim for the sweet ...", "url": "https://towardsdatascience.com/tradeoffs-how-to-aim-for-the-sweet-spot-c20b40d5e6b6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/tradeoffs-how-to-aim-for-the-sweet-spot-c20b40d5e6b6", "snippet": "Enoug h with the \u2018Bookish\u2019 definition, let us understand it by more relatable <b>analogy</b> with the real world. \u2192 In simple English, \u201cThe inability of <b>machine</b> <b>learning</b> techniques to capture the true relationship is <b>Bias</b>\u201d. Low <b>Bias</b>: Predicted data points are close to the target. Also, the model suggests less assumptions about the form of the target function. High-<b>Bias</b>: Predicted data points are far from the target. Also, the model suggests more assumptions about the form of the target ...", "dateLastCrawled": "2022-01-30T08:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Fairness Metrics - Data Analytics - Data Science, <b>Machine</b> <b>Learning</b>, AI", "url": "https://vitalflux.com/fairness-metrics-ml-model-sensitivity-bias-detection/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/<b>fairness-metrics-ml-model-sensitivity</b>-<b>bias</b>-detection", "snippet": "There are many different ways in which <b>machine</b> <b>learning</b> (ML) models\u2019 fairness could be determined. Some of them are statistical parity, the relative significance of features, model sensitivity etc. In this post, you would learn about how model sensitivity could be used to determine model fairness or <b>bias</b> of model towards the privileged or unprivileged group.The following are some of the topics covered in this post:", "dateLastCrawled": "2022-01-20T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How <b>to Split Your Dataset</b> the Right Way - <b>Machine</b> <b>Learning</b> Compass", "url": "https://machinelearningcompass.com/dataset_optimization/split_data_the_right_way/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>compass.com/dataset_optimization/split_data_the_right_way", "snippet": "You have likely heard about <b>bias</b> and variance before. They are two fundamental terms in <b>machine</b> <b>learning</b> and often used to explain overfitting and underfitting. If you&#39;re working with <b>machine</b> <b>learning</b> methods, it&#39;s crucial to understand these concepts well so that you can make optimal decisions in your own projects. In this article, you&#39;ll ...", "dateLastCrawled": "2022-01-31T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning Methods for Planning</b> | ScienceDirect", "url": "https://www.sciencedirect.com/book/9781483207742/machine-learning-methods-for-planning", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/book/9781483207742", "snippet": "<b>Machine Learning Methods for Planning</b> provides information pertinent to <b>learning</b> methods for planning and scheduling. This book covers a wide variety of <b>learning</b> methods and <b>learning</b> architectures, including analogical, case-based, decision-tree, explanation-based, and reinforcement <b>learning</b>. Organized into 15 chapters, this book begins with an overview of planning and scheduling and describes some representative <b>learning</b> systems that have been developed for these tasks. This text then ...", "dateLastCrawled": "2022-01-10T04:15:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bias metric)  is like +(distance between predicted probability distribution and true distribution)", "+(bias metric) is similar to +(distance between predicted probability distribution and true distribution)", "+(bias metric) can be thought of as +(distance between predicted probability distribution and true distribution)", "+(bias metric) can be compared to +(distance between predicted probability distribution and true distribution)", "machine learning +(bias metric AND analogy)", "machine learning +(\"bias metric is like\")", "machine learning +(\"bias metric is similar\")", "machine learning +(\"just as bias metric\")", "machine learning +(\"bias metric can be thought of as\")", "machine learning +(\"bias metric can be compared to\")"]}