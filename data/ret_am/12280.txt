{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Types of Loss functions in ML</b> - Studytonight", "url": "https://www.studytonight.com/post/types-of-loss-functions-in-ml", "isFamilyFriendly": true, "displayUrl": "https://www.studytonight.com/post/<b>types-of-loss-functions-in-ml</b>", "snippet": "Regression <b>Loss</b>. In this article, most of the <b>loss</b> functions that are going to be discussed, fall into the category of Regression <b>Loss</b>. So let&#39;s study each one of them, one at a time. 1. Mean <b>Squared</b> <b>Loss</b>/Error: It is defined as the sum of the <b>squared</b> distances between the actual values and the predicted values. Formula:", "dateLastCrawled": "2022-01-06T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "regression - Why is using <b>squared error</b> the standard when <b>absolute</b> ...", "url": "https://stats.stackexchange.com/questions/470626/why-is-using-squared-error-the-standard-when-absolute-error-is-more-relevant-to", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/470626/why-is-using-<b>squared-error</b>-the...", "snippet": "Hence, I&#39;d argue that <b>absolute</b> <b>loss</b>, which is symmetric and has linear losses on forecasting error, is not realistic in most business situations. Also, although symmetric, the <b>squared</b> <b>loss</b> is at least non linear. Yet the differences between <b>absolute</b> and <b>squared</b> <b>loss</b> functions don&#39;t end here. For instance, it can be shown that the optimal point ...", "dateLastCrawled": "2022-02-02T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss Functions in Deep Learning: An Overview</b>", "url": "https://analyticsindiamag.com/loss-functions-in-deep-learning-an-overview/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>loss-functions-in-deep-learning-an-overview</b>", "snippet": "If the <b>difference</b> is large the model will penalize it as we are computing the <b>squared</b> <b>difference</b>. Practical Implementation . from sklearn.datasets import make_regression from sklearn.preprocessing import StandardScaler from keras.models import Sequential from keras.layers import Dense from keras.optimizers import SGD from matplotlib import pyplot # generate regression dataset X, y = make_regression(n_samples=5000, n_features=20, noise=0.1, random_state=1) # standardize dataset X ...", "dateLastCrawled": "2022-02-02T02:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Ultimate Guide To <b>Loss functions</b> In PyTorch With Python Implementation", "url": "https://analyticsindiamag.com/all-pytorch-loss-function/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/all-pytorch-<b>loss</b>-function", "snippet": "<b>Like</b>, Mean <b>absolute</b> error(MAE), Mean <b>squared</b> error(MSE) sums the <b>squared</b> paired differences between ground truth and prediction divided by the number of such pairs. MSE <b>loss</b> function is generally used when larger errors are well-noted, But there are some cons <b>like</b> it also squares up the units of data.", "dateLastCrawled": "2022-02-02T14:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Comprehensive Guide To <b>Loss</b> Functions \u2014 Part 1 : Regression | by ...", "url": "https://medium.com/analytics-vidhya/a-comprehensive-guide-to-loss-functions-part-1-regression-ff8b847675d6", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-comprehensive-guide-to-<b>loss</b>-functions-part-1...", "snippet": "<b>Loss</b> cannot be calculated for these, as division by zero is not defined. Again, division operation means that even for the same error, the magnitude of actual value can cause a <b>difference</b> in <b>loss</b> ...", "dateLastCrawled": "2022-01-30T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding the 3 most common <b>loss</b> functions for Machine Learning ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "To calculate the MAE, you take the <b>difference</b> between your model\u2019s predictions and the ground truth, apply the <b>absolute</b> value to that <b>difference</b>, and then average it out across the whole dataset. The MAE, <b>like</b> the MSE, will never be negative since in this case we are always taking the <b>absolute</b> value of the errors.", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Professionals Point: <b>Loss Functions in Machine Learning (MAE</b>, MSE ...", "url": "https://theprofessionalspoint.blogspot.com/2019/02/loss-functions-in-machine-learning-mae.html", "isFamilyFriendly": true, "displayUrl": "https://theprofessionalspoint.blogspot.com/2019/02/<b>loss-functions-in-machine-learning</b>...", "snippet": "<b>Loss</b> Function indicates the <b>difference</b> between the actual value and the predicted value. If the magnitude of the <b>loss</b> function is high, it means our algorithm is showing a lot of variance in the result and needs to be corrected. Lets look into the types of <b>loss functions in Machine Learning</b> in detail. There are broadly two types of losses based on the type of algorithm we are using: Types of Losses: 1. Regression Losses. 2. Classification Losses. Lets first discuss regression losses: 1. Mean ...", "dateLastCrawled": "2022-01-29T00:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "A <b>loss</b> function is a measure of how good a prediction model does in terms of being able to predict the expected outcome. A most commonly used method of finding the minimum point of function is \u201cgradient descent\u201d. Think of <b>loss</b> function <b>like</b> undulating mountain and gradient descent <b>is like</b> sliding down the mountain to reach the bottommost point.", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why does <b>regression</b> use <b>least &quot;squares</b>&quot; instead of <b>least</b> &quot;<b>absolute</b> values&quot;?", "url": "https://math.stackexchange.com/questions/3580109/why-does-regression-use-least-squares-instead-of-least-absolute-values", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/3580109/why-does-<b>regression</b>-use-<b>least-squares</b>...", "snippet": "the mean of response values, in the case of <b>squared</b> differences; the median of the response values, in the case of of <b>absolute</b> differences. By replacing the <b>absolute</b> value with a tilted <b>absolute</b> value <b>loss</b> function, we obtain quantile <b>regression</b>. The figures below exemplify the differences in solutions for the two methods (these images were ...", "dateLastCrawled": "2022-01-20T02:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "definition - Why square the <b>difference</b> instead of taking the <b>absolute</b> ...", "url": "https://stats.stackexchange.com/questions/118/why-square-the-difference-instead-of-taking-the-absolute-value-in-standard-devia", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/118", "snippet": "I suppose you could say that <b>absolute</b> <b>difference</b> assigns equal weight to the spread of data whereas squaring emphasises the extremes. Technically though, as others have pointed out, squaring makes the algebra much easier to work with and offers properties that the <b>absolute</b> method does not (for example, the variance is equal to the expected value of the square of the distribution minus the square of the mean of the distribution) It is important to note however that there&#39;s no reason you ...", "dateLastCrawled": "2022-01-29T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss Functions in Deep Learning: An Overview</b>", "url": "https://analyticsindiamag.com/loss-functions-in-deep-learning-an-overview/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>loss-functions-in-deep-learning-an-overview</b>", "snippet": "If the <b>difference</b> is large the model will penalize it as we are computing the <b>squared</b> <b>difference</b>. Practical Implementation . from sklearn.datasets import make_regression from sklearn.preprocessing import StandardScaler from keras.models import Sequential from keras.layers import Dense from keras.optimizers import SGD from matplotlib import pyplot # generate regression dataset X, y = make_regression(n_samples=5000, n_features=20, noise=0.1, random_state=1) # standardize dataset X ...", "dateLastCrawled": "2022-02-02T02:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>the difference between the different orders</b> of <b>loss</b> function ...", "url": "https://www.quora.com/What-is-the-difference-between-the-different-orders-of-loss-function-square-loss-absolute-loss-etc", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-difference-between-the-different-orders</b>-of-<b>loss</b>...", "snippet": "Answer: I only have a superficial answer for order 1 and 2. If you choose Least Square (order 2), you have the average (expected value); when you choose Distance (order 1), you have the median. Imagine there are three aliens with height 1,2, 6. We are meeting a new alien. What is our best guess...", "dateLastCrawled": "2022-01-21T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Comprehensive Guide To <b>Loss</b> Functions \u2014 Part 1 : Regression | by ...", "url": "https://medium.com/analytics-vidhya/a-comprehensive-guide-to-loss-functions-part-1-regression-ff8b847675d6", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-comprehensive-guide-to-<b>loss</b>-functions-part-1...", "snippet": "<b>Loss</b> cannot be calculated for these, as division by zero is not defined. Again, division operation means that even for the same error, the magnitude of actual value can cause a <b>difference</b> in <b>loss</b> ...", "dateLastCrawled": "2022-01-30T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "regression - Why <b>squared</b> <b>residuals</b> instead of <b>absolute</b> <b>residuals</b> in OLS ...", "url": "https://stats.stackexchange.com/questions/46019/why-squared-residuals-instead-of-absolute-residuals-in-ols-estimation", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/46019/why-<b>squared</b>-<b>residuals</b>-instead-", "snippet": "Both are done. Least squares is easier, and the fact that for independent random variables &quot;variances add&quot; means that it&#39;s considerably more convenient; for examples, the ability to partition variances is particularly handy for comparing nested models. It&#39;s somewhat more efficient at the normal (least squares is maximum likelihood), which might seem to be a good justification -- however, some robust estimators with high breakdown can have surprisingly high efficiency at the normal.", "dateLastCrawled": "2022-01-29T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "The Mean <b>Absolute</b> Error, or MAE, <b>loss</b> is an appropriate <b>loss</b> function in this case as it is more robust to outliers. It is calculated as the average of the <b>absolute</b> <b>difference</b> between the actual and predicted values. The model can be updated to use the \u2018mean_<b>absolute</b>_error\u2018 <b>loss</b> function and keep the same configuration for the output layer.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "<b>Loss</b> functions can be broadly categorized into 2 types: Classification and Regression <b>Loss</b>. In this post, I\u2019m focussing on regression <b>loss</b>. In future posts I cover <b>loss</b> functions in other categories. Please let me know in comments if I miss something. Also, all the codes and plots shown in this blog can be found in this notebook.", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "The main <b>difference</b> between the hinge <b>loss</b> and the cross entropy <b>loss</b> is that the former arises from trying to maximize the margin between our decision boundary and data points - thus attempting to ensure that each point is correctly and confidently classified*, while the latter comes from a maximum likelihood estimate of our model\u2019s parameters. The softmax function, whose scores are used by the cross entropy <b>loss</b>, allows us to interpret our model\u2019s scores as relative probabilities ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "In <b>statistics, what is the difference between loss and risk</b>? - Quora", "url": "https://www.quora.com/In-statistics-what-is-the-difference-between-loss-and-risk", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-<b>statistics-what-is-the-difference-between-loss-and-risk</b>", "snippet": "Answer: <b>Loss</b> is a function that measures the badness of some particular guess, d, that you made for some value that was actually y. Some common <b>loss</b> functions may be <b>absolute</b> <b>loss</b> (L(y,d) = |d-y|) or <b>squared</b> <b>loss</b> (L(y,d) = (d-y)^2). Risk is the expected value of your <b>loss</b> over all of the guesses...", "dateLastCrawled": "2022-01-11T11:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - keras validation <b>mean squared error</b> always <b>similar</b> ...", "url": "https://datascience.stackexchange.com/questions/14688/keras-validation-mean-squared-error-always-similar-to-1", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/14688", "snippet": "It could be very close to 1 (or for that matter, changing very slowly) for many reasons: Maybe your learning rate is very small. Maybe the network has reached its learning capacity.", "dateLastCrawled": "2022-01-28T19:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "machine learning - Mean <b>absolute</b> <b>relative error</b> as <b>loss</b> function in ...", "url": "https://stackoverflow.com/questions/52680886/mean-absolute-relative-error-as-loss-function-in-tensorflow-regression-problem", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/52680886", "snippet": "Almost in all tensorflow tutorials they use custom functions. For example in the very beginning tutorial they write a custom function, which sums the squares of the deltas between the current model and the provided data. <b>squared</b>_deltas = tf.square(linear_model - y) <b>loss</b> = tf.reduce_sum(<b>squared</b>_deltas)", "dateLastCrawled": "2022-01-09T19:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "We <b>can</b> achieve this using the Huber <b>Loss</b> (Smooth L1 <b>Loss</b>), a combination of L1 (MAE) and L2 (MSE) losses. <b>Can</b> be called Huber <b>Loss</b> or Smooth MAE Less sensitive to outliers in data than the <b>squared</b> ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Cutting Your Losses: <b>Loss</b> Functions &amp; the Sum of <b>Squared</b> Errors <b>Loss</b> ...", "url": "https://dustinstansbury.github.io/theclevermachine/cutting-your-losses", "isFamilyFriendly": true, "displayUrl": "https://dustinstansbury.github.io/theclevermachine/cutting-your-<b>loss</b>es", "snippet": "A helpful interpretation of the SSE <b>loss function</b> is demonstrated in Figure 2.The area of each red square is a literal geometric interpretation of each observation\u2019s contribution to the overall <b>loss</b>. We see that no matter if the errors are positive or negative (i.e. actual \\(y_i\\) are located above or below the black line), the contribution to the <b>loss</b> is always an area, and therefore positive.", "dateLastCrawled": "2022-01-31T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> <b>Function</b> and Cost <b>Function</b> in Neural Networks | by Simran ...", "url": "https://medium.com/analytics-vidhya/loss-function-and-cost-function-in-neural-networks-4ade0c9ccb18", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>loss</b>-<b>function</b>-and-cost-<b>function</b>-in-neural-networks...", "snippet": "Regression is a supervised machine learning problem, where output is a continuous value. The <b>loss</b> functions that we will study, in this article are: L1 <b>Loss</b> (Least <b>Absolute</b> Deviation (LAD)/ Mean ...", "dateLastCrawled": "2022-01-30T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions | <b>Loss</b> Functions in <b>Machine Learning</b> - 360DigiTMG", "url": "https://360digitmg.com/loss-functions-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://360digitmg.com/<b>loss</b>-functions-in-<b>machine-learning</b>", "snippet": "<b>Squared</b> Hinge <b>Loss</b>: This is an extension of the hinge <b>loss</b> and it is quite simply the square of the hinge <b>loss</b> function. Since this is a square of the original <b>loss</b>, it has some mathematical properties that make it easier to calculate the gradients. This is perfectly suitable for Yes or No type of questions where the deviation in probability is ...", "dateLastCrawled": "2022-01-30T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "definition - Why square the <b>difference</b> instead of taking the <b>absolute</b> ...", "url": "https://stats.stackexchange.com/questions/118/why-square-the-difference-instead-of-taking-the-absolute-value-in-standard-devia", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/118", "snippet": "I suppose you could say that <b>absolute</b> <b>difference</b> assigns equal weight to the spread of data whereas squaring emphasises the extremes. Technically though, as others have pointed out, squaring makes the algebra much easier to work with and offers properties that the <b>absolute</b> method does not (for example, the variance is equal to the expected value of the square of the distribution minus the square of the mean of the distribution) It is important to note however that there&#39;s no reason you ...", "dateLastCrawled": "2022-01-29T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "Mean Square Error/Quadratic <b>Loss</b>/L2 <b>Loss</b>: averages the <b>squared</b> <b>difference</b> between predictions and ground truth, with a focus on the average magnitudes of errors regardless of direction. Mean <b>Absolute</b> Error, L1 <b>Loss</b> (used by PerceptiLabs\u2019 Regression component): sums the <b>absolute</b> differences between the predictions and ground truth, and finds the average.", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Choosing and Customizing Loss Functions for Image Processing</b>", "url": "https://blog.perceptilabs.com/choosing-and-customizing-loss-functions-for-image-processing/", "isFamilyFriendly": true, "displayUrl": "https://blog.perceptilabs.com/<b>choosing-and-customizing-loss-functions-for-image-processing</b>", "snippet": "Mean Square Error/Quadratic <b>Loss</b>/L2 <b>Loss</b>: averages the <b>squared</b> <b>difference</b> between predictions and ground truth, with a focus on the average magnitudes of errors regardless of direction. Mean <b>Absolute</b> Error, L1 <b>Loss</b> (used by PerceptiLabs&#39; Regression component): sums the <b>absolute</b> differences between the predictions and ground truth, and finds the average.", "dateLastCrawled": "2022-01-28T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "keras - Confused between optimizer and <b>loss</b> function - Data Science ...", "url": "https://datascience.stackexchange.com/questions/85579/confused-between-optimizer-and-loss-function", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/.../confused-between-optimizer-and-<b>loss</b>-function", "snippet": "The <b>loss</b> is a way of measuring the <b>difference</b> between your target label (s) and your prediction label (s). There are many ways of doing this, for example mean <b>squared</b> error, squares the <b>difference</b> between target and prediction. Cross entropy is a more complex <b>loss</b> formula related to information theory. Gradient descent algorithms like batch ...", "dateLastCrawled": "2022-02-02T17:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Can</b> you describe the Mean <b>Squared</b> Error function in layperson&#39;s terms ...", "url": "https://www.quora.com/Can-you-describe-the-Mean-Squared-Error-function-in-laypersons-terms", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Can</b>-you-describe-the-Mean-<b>Squared</b>-Error-function-in-laypersons-terms", "snippet": "Answer (1 of 19): Suppose there is a group of people. You don\u2019t know who they are , how they behave , from where they have come. your job is to make some food let\u2019s say spicy soup for them. So your concern will be what should be the amount of spice so that everyone will enjoy the food. 1. Assu...", "dateLastCrawled": "2022-01-08T03:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Is my comparison of <b>squared</b> <b>loss</b>, <b>absolute</b> <b>loss</b>, and huber <b>loss</b> correct ...", "url": "https://stats.stackexchange.com/questions/306778/is-my-comparison-of-squared-loss-absolute-loss-and-huber-loss-correct", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/306778/is-my-comparison-of-<b>squared</b>-<b>loss</b>...", "snippet": "According to the definitions of the Huber <b>loss</b>, <b>squared</b> <b>loss</b> ($\\sum(y^{(i)}-\\hat y^{(i)})^2$), and <b>absolute</b> <b>loss</b> ($\\sum|y^{(i)}-\\hat y^{(i)}|$), I have the following interpretation.Is there anything wrong? Here, by robust to outliers I mean the samples that are too far from the best linear estimation have a low effect on the estimation. On the other hand, by insensitive to noise I mean the deviations of the samples that are very close to the best linear function have a low effect on the ...", "dateLastCrawled": "2022-01-19T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "regression - Why is using <b>squared error</b> the standard when <b>absolute</b> ...", "url": "https://stats.stackexchange.com/questions/470626/why-is-using-squared-error-the-standard-when-absolute-error-is-more-relevant-to", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/470626/why-is-using-<b>squared-error</b>-the...", "snippet": "Hence, I&#39;d argue that <b>absolute</b> <b>loss</b>, which is symmetric and has linear losses on forecasting error, is not realistic in most business situations. Also, although symmetric, the <b>squared</b> <b>loss</b> is at least non linear. Yet the differences between <b>absolute</b> and <b>squared</b> <b>loss</b> functions don&#39;t end here. For instance, it <b>can</b> be shown that the optimal point ...", "dateLastCrawled": "2022-02-02T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction of Different types of <b>Loss</b> Functions in Machine <b>learning</b> ...", "url": "https://medium.com/analytics-vidhya/introduction-of-different-types-of-loss-functions-in-machine-learning-and-deep-learning-66ef7804668b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/introduction-of-different-types-of-<b>loss</b>-functions...", "snippet": "It is the <b>absolute</b> <b>difference</b> between the actual and predicted value. MAE is not sensitive towards outliers and given several examples with the same input feature values, and the optimal ...", "dateLastCrawled": "2022-02-02T22:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Is it possible for the <b>squared</b> <b>loss</b> to see how the variance depends on ...", "url": "https://www.quora.com/Is-it-possible-for-the-squared-loss-to-see-how-the-variance-depends-on-both-the-number-of-data-set-points-and-the-complexity-of-the-model", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-it-possible-for-the-<b>squared</b>-<b>loss</b>-to-see-how-the-variance...", "snippet": "Answer (1 of 2): You did not specify what variance you are talking about. Given that you mention <b>squared</b> <b>loss</b> - you are talking about the variance of the residual? This is pretty much equivalent to the R-<b>squared</b>. You also did not specify whether you are looking at <b>squared</b> <b>loss</b> in or out of sample...", "dateLastCrawled": "2022-01-06T05:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>Mean Squared Error, Mean Absolute Error</b>, Root Mean <b>Squared</b> ...", "url": "https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared", "isFamilyFriendly": true, "displayUrl": "https://www.studytonight.com/post/what-is-<b>mean-squared-error-mean-absolute-error</b>-root...", "snippet": "4. R <b>Squared</b>. It is also known as the coefficient of determination.This metric gives an indication of how good a model fits a given dataset. It indicates how close the regression line (i.e the predicted values plotted) is to the actual data values. The R <b>squared</b> value lies between 0 and 1 where 0 indicates that this model doesn&#39;t fit the given data and 1 indicates that the model fits perfectly to the dataset provided.. import numpy as np X = np.random.randn(100) y = np.random.randn(60) # y ...", "dateLastCrawled": "2022-02-03T03:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Cutting Your Losses: <b>Loss</b> Functions &amp; the Sum of <b>Squared</b> Errors <b>Loss</b> ...", "url": "https://dustinstansbury.github.io/theclevermachine/cutting-your-losses", "isFamilyFriendly": true, "displayUrl": "https://dustinstansbury.github.io/theclevermachine/cutting-your-<b>loss</b>es", "snippet": "A helpful interpretation of the SSE <b>loss function</b> is demonstrated in Figure 2.The area of each red square is a literal geometric interpretation of each observation\u2019s contribution to the overall <b>loss</b>. We see that no matter if the errors are positive or negative (i.e. actual \\(y_i\\) are located above or below the black line), the contribution to the <b>loss</b> is always an area, and therefore positive.", "dateLastCrawled": "2022-01-31T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "machine learning - Mean <b>absolute</b> <b>relative error</b> as <b>loss</b> function in ...", "url": "https://stackoverflow.com/questions/52680886/mean-absolute-relative-error-as-loss-function-in-tensorflow-regression-problem", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/52680886", "snippet": "Almost in all tensorflow tutorials they use custom functions. For example in the very beginning tutorial they write a custom function, which sums the squares of the deltas between the current model and the provided data. <b>squared</b>_deltas = tf.square(linear_model - y) <b>loss</b> = tf.reduce_sum(<b>squared</b>_deltas)", "dateLastCrawled": "2022-01-09T19:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why does <b>regression</b> use <b>least &quot;squares</b>&quot; instead of <b>least</b> &quot;<b>absolute</b> values&quot;?", "url": "https://math.stackexchange.com/questions/3580109/why-does-regression-use-least-squares-instead-of-least-absolute-values", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/3580109/why-does-<b>regression</b>-use-<b>least-squares</b>...", "snippet": "the mean of response values, in the case of <b>squared</b> differences; the median of the response values, in the case of of <b>absolute</b> differences. By replacing the <b>absolute</b> value with a tilted <b>absolute</b> value <b>loss</b> function, we obtain quantile <b>regression</b>. The figures below exemplify the differences in solutions for the two methods (these images were ...", "dateLastCrawled": "2022-01-20T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "regression - MAD vs RMSE vs MAE vs MSLE vs R\u00b2: When to use which ...", "url": "https://datascience.stackexchange.com/questions/42760/mad-vs-rmse-vs-mae-vs-msle-vs-r%C2%B2-when-to-use-which", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/42760", "snippet": "I have very rough ideas for some: MAD if a deviation of 2 is &quot;double as bad&quot; than having a deviation of 1. RMSE if the value deteriorates more quickly - punishes outliers hard!", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> fundamentals I: An <b>analogy</b> | Finn Rietz.dev", "url": "http://www.finnrietz.dev/machine%20learning/part-1-analogy/", "isFamilyFriendly": true, "displayUrl": "www.finnrietz.dev/<b>machine</b> <b>learning</b>/part-1-<b>analogy</b>", "snippet": "And this is what the <b>loss</b> function does, so the <b>loss</b> function for a <b>Machine</b> <b>learning</b> algorithm is like the teacher for the real-world dermatologist in-training. In mathematical terms, the <b>loss</b> function could look something like this: \\(L = (y_i - \\hat{y_i})^2\\), where \\(y_i\\) is the actual output value (the one that the teacher has written down) and \\(\\hat{y_i}\\) is the one our <b>learning</b> algorithm produced.", "dateLastCrawled": "2022-01-16T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "However, in <b>machine</b> <b>learning</b> methodology, <b>squared</b> <b>loss</b> will be minimized with respect to ... If one of the supports doesn\u2019t exist, the beam will eventually fall down by moving out of balance. A similar <b>analogy</b> is applied for comparing statistical modeling and <b>machine</b> <b>learning</b> methodologies here. The two-point validation is performed on the statistical modeling methodology on training data using overall model accuracy and individual parameters significance test. Due to the fact that either ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bias-Variance Decomposition</b> - mlxtend", "url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/", "isFamilyFriendly": true, "displayUrl": "rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp", "snippet": "We can decompose a <b>loss</b> function such as the <b>squared</b> <b>loss</b> into three terms, a variance, bias, and a noise term (and the same is true for the decomposition of the 0-1 <b>loss</b> later). However, for simplicity, we will ignore the noise term. Before we introduce the <b>bias-variance decomposition</b> of the 0-1 <b>loss</b> for classification, let us start with the decomposition of the <b>squared</b> <b>loss</b> as an easy warm-up exercise to get familiar with the overall concept. The previous section already listed the common ...", "dateLastCrawled": "2022-01-31T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machined Learnings: ML and OR: An <b>analogy</b> with cost-sensitive ...", "url": "http://www.machinedlearnings.com/2010/07/ml-and-or.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2010/07/ml-and-or.html", "snippet": "Nonetheless I&#39;ve been amusing myself by thinking about it, in particular trying to think about it from a <b>machine</b> <b>learning</b> reduction standpoint. The simplest well-understood reduction that I can think of which is analogous to supplying estimates to a linear program is the reduction of cost-sensitive multiclass classification (CSMC) to regression.", "dateLastCrawled": "2021-12-25T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "Optimization methods are applied to minimize the <b>loss</b> function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one <b>loss</b> is L0-1 = 1 (m &lt;= 0); in zero-one <b>loss</b>, value of <b>loss</b> is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this <b>loss</b> is it is not differentiable, non-convex, and also NP-hard. Hence, in order to make optimization feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Mean squared error</b> in <b>machine</b> <b>learning</b> | by Aaron Li | MLearning.ai ...", "url": "https://medium.com/mlearning-ai/gradient-descent-4fda4e3fbdc0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/gradient-descent-4fda4e3fbdc0", "snippet": "For the ones <b>learning</b> <b>machine</b> <b>learning</b> in a bottom up approach, I suggest you try to train some models in Google Co-lab and get an idea of how <b>machine</b> <b>learning</b> works.", "dateLastCrawled": "2022-01-30T03:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Stratified <b>Sampling Meets Machine Learning</b>", "url": "http://proceedings.mlr.press/v48/liberty16.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v48/liberty16.pdf", "snippet": "model in this <b>analogy</b> is the vector of probabilities p. A standard objective in regression problems is to mini-mize the <b>squared</b> <b>loss</b>, L(~y;y) = (~y 2y) . In our set- ting, however, the prediction ~y is itself a random vari-able. By taking the expectation over the random bits of the sampling algorithm and by overloading the <b>loss</b> func-tion L(p;q) := P i q 2(1=p i 1), our goal is modi\ufb01ed to minimize E q[E y~(~y y)2] = E q X i q2 i (1=p i 1) = E qL(p;q) : Strati\ufb01ed Sampling Meets <b>Machine</b> ...", "dateLastCrawled": "2021-10-13T10:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "<b>Machine</b> <b>Learning</b> A Quantitative Approach Henry H. Liu P PerfMath. ... Bayesian, (4) <b>Analogy</b>, and (5) Unsupervised <b>learning</b>. Pedro Domingos proposed these five ML paradigms, and \u00a71.3 explains briefly what each of these five ML paradigms is about. <b>MACHINE</b> <b>LEARNING</b>: A QUANTITATIVE APPROACH 5 2 <b>Machine</b> <b>Learning</b> Fundamentals Illustrated with Regression 2.1 Try to find a publicly available <b>machine</b> <b>learning</b> dataset and apply an end-to-end procedure similar to the one we used with the fuel economy ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A novel <b>semi-supervised support vector machine with asymmetric</b> squared ...", "url": "https://link.springer.com/article/10.1007/s11634-020-00390-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11634-020-00390-y", "snippet": "In the field of <b>machine</b> <b>learning</b>, loss function is usually one of the key issues in designing <b>learning</b> algorithms since most problems require it to describe the cost of the discrepancy between the prediction and the observation. In fact, the use of the loss function can be traced back to a long time ago. For example, the least-square loss function for regression was already employed by Legendre, Gauss, and Adrain in the early 19th century (Steinwart and Christmann 2008). At present, various ...", "dateLastCrawled": "2021-11-13T10:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(squared loss)  is like +(absolute difference)", "+(squared loss) is similar to +(absolute difference)", "+(squared loss) can be thought of as +(absolute difference)", "+(squared loss) can be compared to +(absolute difference)", "machine learning +(squared loss AND analogy)", "machine learning +(\"squared loss is like\")", "machine learning +(\"squared loss is similar\")", "machine learning +(\"just as squared loss\")", "machine learning +(\"squared loss can be thought of as\")", "machine learning +(\"squared loss can be compared to\")"]}