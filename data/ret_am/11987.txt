{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) D3-SACNN: DGA Domain Detection with <b>Self-Attention</b> Convolutional ...", "url": "https://www.researchgate.net/publication/356205339_D3-SACNN_DGA_Domain_Detection_with_Self-Attention_Convolutional_Network", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356205339_D3-SACNN_DGA_Domain_Detection_with...", "snippet": "The <b>multi-head</b> <b>self-attention</b> mechanism with different input values is used to effectively obtain the relationship between the character s and the extracted implicit featur es, which will help", "dateLastCrawled": "2021-12-21T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "AMMU: A survey of transformer-based biomedical pretrained language ...", "url": "https://www.sciencedirect.com/science/article/pii/S1532046421003117", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046421003117", "snippet": "<b>Multi-Head</b> <b>Self Attention</b> (MHSA) ... Structured data includes laboratory test results, various medical codes, etc. Unstructured data include clinical notes <b>like</b> medication <b>instructions</b>, progress notes, discharge summaries, etc. Clinical notes include the most valuable patient information which is difficult and expensive to extract manually. So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications ...", "dateLastCrawled": "2022-01-23T23:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Bayesian and Attentive Aggregation for Multi-Agent Deep Reinforcement ...", "url": "https://phiresky.github.io/masters-thesis/", "isFamilyFriendly": true, "displayUrl": "https://phiresky.github.io/masters-thesis", "snippet": "For <b>self-attention</b>, the query \\(Q\\), the key \\(K\\) and the value \\(V\\) are all <b>set</b> to the same input value. In [ 9 ] , the authors combine the <b>multi-head</b> attention with a mean aggregation. Note that they only use the attention mechanism to individually transform the information from each separate observable into a new feature <b>set</b> instead of directly using it as a weighing function for the aggregation.", "dateLastCrawled": "2022-01-28T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Perceptual Reasoning and Interaction Research - Publications", "url": "https://prior.allenai.org/publications", "isFamilyFriendly": true, "displayUrl": "https://prior.allenai.org/publications", "snippet": "Further, each head in our <b>multi-head</b> <b>self-attention</b> layer focuses on a different subset of relations. Our approach has two advantages: (1) each head considers local context instead of dispersing the attention amongst all visual entities; (2) we avoid learning redundant features. We show that our model improves the absolute accuracy of current state-of-the-art methods on TextVQA by 2.2% overall over an improved baseline, and 4.62% on questions that involve spatial reasoning and can be ...", "dateLastCrawled": "2022-02-02T00:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Temporal Fusion Transformers for Interpretable Multi-horizon Time ...", "url": "https://www.readkong.com/page/temporal-fusion-transformers-for-interpretable-3335678", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/temporal-fusion-transformers-for-interpretable-3335678", "snippet": "As such, using the attention weights present in the <b>self-attention</b> layer of the temporal fusion decoder, we present a method to identify similar persistent patterns \u2013 by measuring the contributions of features at fixed lags in the past on forecasts at various horizons. Combining Eq. (14) and (19), we see that the <b>self-attention</b> layer contains a matrix of attention weights at each forecast time t \u2013 i.e. \u00c3(\u03c6(t), \u03c6(t)). <b>Multi-head</b> attention outputs at each forecast horizon \u03c4 19", "dateLastCrawled": "2021-12-26T12:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Uncategorized \u2013 prettyandnerdy", "url": "https://pretteyandnerdy.wordpress.com/category/uncategorized/", "isFamilyFriendly": true, "displayUrl": "https://pretteyandnerdy.wordpress.com/category/uncategorized", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism in the encoder, and the second is the original encoder-decoder attention in the decoder, which performs <b>multi-head</b> attention over the output of the encoder stack The decoder stack also has a <b>self-attention</b> layer but it is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions by setting them to negative infinity before the Softmax step. Residual Connection. Six layers is pretty deep. When ...", "dateLastCrawled": "2021-12-20T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>New submissions for Wed, 12 May</b> 21 \u00b7 Issue #102 \u00b7 dajinstory/daily ...", "url": "https://github.com/dajinstory/daily-arxiv-noti/issues/102", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dajinstory/daily-arxiv-noti/issues/102", "snippet": "The input to our model is <b>a set</b> of text segments associated with the nodes and edges of the graph, which are then processed with a transformer encoder-decoder model, equipped with a <b>self-attention</b> mechanism that is aware of the graphical relations between the nodes containing the segments. This also allows us to use BERT-<b>like</b> models that are already trained on large amounts of text. While the proposed model has wide applications, we demonstrate its capabilities on data-to-text generation ...", "dateLastCrawled": "2021-09-02T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Joint entity recognition and relation extraction as</b> a <b>multi-head</b> ...", "url": "https://www.researchgate.net/publication/324717352_Joint_entity_recognition_and_relation_extraction_as_a_multi-head_selection_problem", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324717352_Joint_entity_recognition_and...", "snippet": "Instructive text (iText) consists of <b>a set</b> <b>of instructions</b> to accomplish a task or operation. Hence, iText includes a <b>group</b> of texts having a title or name of the task or operation and step-by ...", "dateLastCrawled": "2022-01-30T15:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Transformer demo - this is a demonstration of the operation of a ...", "url": "https://otabudete.com/wiki/index0bn996-83up.php?title=Transformer_Demo", "isFamilyFriendly": true, "displayUrl": "https://otabudete.com/wiki/index0bn996-83up.php?title=Transformer_Demo", "snippet": "FastFormers provides <b>a set</b> of recipes and methods to achieve highly efficient inference of Transformer models for Natural Language Understanding (NLU) including the demo models showing 233.87x speed-up (Yes, 233x on CPU with the <b>multi-head</b> self-attentive Transformer architecture. This is not an LSTM or an RNN). The details of the methods and analyses are described in the paper.", "dateLastCrawled": "2022-01-05T22:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Learning Structural Representations for Recipe Generation and Food ...", "url": "https://www.readkong.com/page/learning-structural-representations-for-recipe-generation-1247095", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/learning-structural-representations-for-recipe...", "snippet": "Page topic: &quot;Learning Structural Representations for Recipe Generation and Food Retrieval&quot;. Created by: Cecil Santos. Language: english.", "dateLastCrawled": "2022-01-18T08:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>multi-head</b> <b>self-attention</b>. #language. An extension of <b>self-attention</b> that applies the <b>self-attention</b> mechanism multiple times for each position in the input sequence. Transformers introduced <b>multi-head</b> <b>self-attention</b>. multimodal model. #language. A model whose inputs and/or outputs include more than one modality. For example, consider a model that takes both an image and a text caption (two modalities) as features, and outputs a score indicating how appropriate the text caption is for the ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) D3-SACNN: DGA Domain Detection with <b>Self-Attention</b> Convolutional ...", "url": "https://www.researchgate.net/publication/356205339_D3-SACNN_DGA_Domain_Detection_with_Self-Attention_Convolutional_Network", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356205339_D3-SACNN_DGA_Domain_Detection_with...", "snippet": "The <b>multi-head</b> <b>self-attention</b> mechanism with different input values is used to effectively obtain the relationship between the characters and the extracted implicit features, which will help us ...", "dateLastCrawled": "2021-12-21T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Bayesian and Attentive Aggregation for Multi-Agent Deep Reinforcement ...", "url": "https://phiresky.github.io/masters-thesis/", "isFamilyFriendly": true, "displayUrl": "https://phiresky.github.io/masters-thesis", "snippet": "For <b>self-attention</b>, the query \\(Q\\), the key \\(K\\) and the value \\(V\\) are all <b>set</b> to the same input value. In [ 9 ] , the authors combine the <b>multi-head</b> attention with a mean aggregation. Note that they only use the attention mechanism to individually transform the information from each separate observable into a new feature <b>set</b> instead of directly using it as a weighing function for the aggregation.", "dateLastCrawled": "2022-01-28T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Perceptual Reasoning and Interaction Research - Publications", "url": "https://prior.allenai.org/publications", "isFamilyFriendly": true, "displayUrl": "https://prior.allenai.org/publications", "snippet": "Further, each head in our <b>multi-head</b> <b>self-attention</b> layer focuses on a different subset of relations. Our approach has two advantages: (1) each head considers local context instead of dispersing the attention amongst all visual entities; (2) we avoid learning redundant features. We show that our model improves the absolute accuracy of current state-of-the-art methods on TextVQA by 2.2% overall over an improved baseline, and 4.62% on questions that involve spatial reasoning and can be ...", "dateLastCrawled": "2022-02-02T00:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Temporal Fusion Transformers for Interpretable Multi-horizon Time ...", "url": "https://www.readkong.com/page/temporal-fusion-transformers-for-interpretable-3335678", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/temporal-fusion-transformers-for-interpretable-3335678", "snippet": "As such, using the attention weights present in the <b>self-attention</b> layer of the temporal fusion decoder, we present a method to identify <b>similar</b> persistent patterns \u2013 by measuring the contributions of features at fixed lags in the past on forecasts at various horizons. Combining Eq. (14) and (19), we see that the <b>self-attention</b> layer contains a matrix of attention weights at each forecast time t \u2013 i.e. \u00c3(\u03c6(t), \u03c6(t)). <b>Multi-head</b> attention outputs at each forecast horizon \u03c4 19", "dateLastCrawled": "2021-12-26T12:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Joint entity recognition and relation extraction as</b> a <b>multi-head</b> ...", "url": "https://www.researchgate.net/publication/324717352_Joint_entity_recognition_and_relation_extraction_as_a_multi-head_selection_problem", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324717352_Joint_entity_recognition_and...", "snippet": "Instructive text (iText) consists of <b>a set</b> <b>of instructions</b> to accomplish a task or operation. Hence, iText includes a <b>group</b> of texts having a title or name of the task or operation and step-by ...", "dateLastCrawled": "2022-01-30T15:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Uncategorized \u2013 prettyandnerdy", "url": "https://pretteyandnerdy.wordpress.com/category/uncategorized/", "isFamilyFriendly": true, "displayUrl": "https://pretteyandnerdy.wordpress.com/category/uncategorized", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism in the encoder, and the second is the original encoder-decoder attention in the decoder, which performs <b>multi-head</b> attention over the output of the encoder stack The decoder stack also has a <b>self-attention</b> layer but it is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions by setting them to negative infinity before the Softmax step. Residual Connection. Six layers is pretty deep. When ...", "dateLastCrawled": "2021-12-20T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "NeurIPS 2021 papers", "url": "https://tanelp.github.io/neurips2021/?s=09", "isFamilyFriendly": true, "displayUrl": "https://tanelp.github.io/neurips2021/?s=09", "snippet": "The key idea is to treat the <b>self-attention</b> mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are ...", "dateLastCrawled": "2022-02-03T07:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Transformer demo - this is a demonstration of the operation of a ...", "url": "https://otabudete.com/wiki/index0bn996-83up.php?title=Transformer_Demo", "isFamilyFriendly": true, "displayUrl": "https://otabudete.com/wiki/index0bn996-83up.php?title=Transformer_Demo", "snippet": "FastFormers provides <b>a set</b> of recipes and methods to achieve highly efficient inference of Transformer models for Natural Language Understanding (NLU) including the demo models showing 233.87x speed-up (Yes, 233x on CPU with the <b>multi-head</b> self-attentive Transformer architecture. This is not an LSTM or an RNN). The details of the methods and analyses are described in the paper.", "dateLastCrawled": "2022-01-05T22:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Learning Structural Representations for Recipe Generation and Food ...", "url": "https://www.readkong.com/page/learning-structural-representations-for-recipe-generation-1247095", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/learning-structural-representations-for-recipe...", "snippet": "Page topic: &quot;Learning Structural Representations for Recipe Generation and Food Retrieval&quot;. Created by: Cecil Santos. Language: english.", "dateLastCrawled": "2022-01-18T08:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Enhancing brain decoding using attention augmented deep neural ...", "url": "https://www.researchgate.net/publication/355251522_Enhancing_brain_decoding_using_attention_augmented_deep_neural_networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355251522_Enhancing_brain_decoding_using...", "snippet": "This mechanism is introduced in [1], and combines con volutions and <b>multi-head</b> <b>self-attention</b> [10]. The key (K), value (V) and query (Q) are extracted from the", "dateLastCrawled": "2022-01-31T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Proceedings of the 2021 Conference on Empirical Methods in Natural ...", "url": "https://aclanthology.org/volumes/2021.emnlp-main/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2021.emnlp-main", "snippet": "Recent efforts to improve the efficiency of <b>self-attention</b> have led to a proliferation of long-range Transformer language models, which <b>can</b> process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM ...", "dateLastCrawled": "2022-01-24T04:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "NeurIPS 2021 papers", "url": "https://tanelp.github.io/neurips2021/?s=09", "isFamilyFriendly": true, "displayUrl": "https://tanelp.github.io/neurips2021/?s=09", "snippet": "Our approach uses <b>self-attention</b> to reason about relationships between datapoints explicitly, which <b>can</b> be seen as realizing non-parametric models using parametric attention mechanisms. However, unlike conventional non-parametric models, we let the model learn end-to-end from the data how to make use of other datapoints for prediction. Empirically, our models solve cross-datapoint lookup and complex reasoning tasks unsolvable by traditional deep learning models. We show highly competitive ...", "dateLastCrawled": "2022-02-03T07:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Uncategorized \u2013 prettyandnerdy", "url": "https://pretteyandnerdy.wordpress.com/category/uncategorized/", "isFamilyFriendly": true, "displayUrl": "https://pretteyandnerdy.wordpress.com/category/uncategorized", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism in the encoder, and the second is the original encoder-decoder attention in the decoder, which performs <b>multi-head</b> attention over the output of the encoder stack The decoder stack also has a <b>self-attention</b> layer but it is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions by setting them to negative infinity before the Softmax step. Residual Connection. Six layers is pretty deep. When ...", "dateLastCrawled": "2021-12-20T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Accepted Papers: Main Conference</b> | COLING\u20192020", "url": "https://coling2020.org/pages/accepted_papers_main_conference.html", "isFamilyFriendly": true, "displayUrl": "https://coling2020.org/pages/<b>accepted_papers_main_conference</b>.html", "snippet": "The triple-level <b>self-attention</b> treats head entity, relation, and tail entity as a sequence and captures the dependency within a triple. At the same time the pseudo residual connection retains primitive semantic features. Furthermore, to deal with symmetric and antisymmetric relations, two schemas of score function are designed via a position-adaptive mechanism. Experimental results on public datasets demonstrate that our model <b>can</b> produce expressive knowledge embedding and significantly ...", "dateLastCrawled": "2022-01-30T19:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Dancing the Self Personhood and Performance in the Pandav Lila of Garhwal", "url": "https://blank-manual.co/97", "isFamilyFriendly": true, "displayUrl": "https://blank-manual.co/97", "snippet": "Mhsa-net contains two main novel components: <b>multi-head</b> <b>self-attention</b> branch (mhsab) and attention competition mechanism (acm). The mhsam adaptively captures key local person information, and then . Melt method is the new art of self care based on sue hitzmann&#39;s bestseller. Get all your old clothes ready for the charity shop (when they reopen). The stone is a forum for contemporary philosophers and other thinkers on issues both timely and timeless. Mark pierpont used to be an important ...", "dateLastCrawled": "2022-01-25T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Master&#39;s theses</b> \u2013 Seminar for Statistics | ETH Zurich", "url": "https://math.ethz.ch/sfs/research/master-theses.html", "isFamilyFriendly": true, "displayUrl": "https://math.ethz.ch/sfs/research/master-theses.html", "snippet": "Abstract: When <b>giving</b> the guarantees on test <b>set</b> performance of interpolating neural networks, machine learning researchers would traditionally use uniform convergence framework type of bounds. Since these bounds give information on what is the worst performance neural network that you could get, when fitting neural network on a given training data, it has been shown to fail (look into Nagarajan and Kolter (2019)) when applied to complex hypothesis space such as neural networks. This is ...", "dateLastCrawled": "2022-02-03T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Python Machine Learning: Machine Learning and Deep Learning with Python ...", "url": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with-python-scikit-learn-and-tensorflow-2-3rd-edition-3nbsped-9781789955750-1789955750.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with...", "snippet": "By formulating our arithmetic operations as a sequence <b>of instructions</b> on an array, rather than performing <b>a set</b> of operations for each element at a time, we <b>can</b> make better use of our modern central processing unit (CPU) architectures with single instruction, multiple data (SIMD) support. Furthermore, NumPy uses highly optimized linear algebra libraries, such as Basic Linear Algebra Subprograms (BLAS) and Linear Algebra Package (LAPACK), that have been written in C or Fortran. Lastly, NumPy ...", "dateLastCrawled": "2022-01-31T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "AI academic express [12.7]", "url": "https://cdmana.com/2022/01/202201080055497567.html", "isFamilyFriendly": true, "displayUrl": "https://cdmana.com/2022/01/202201080055497567.html", "snippet": "We found that , The proposed external attention mechanism <b>can</b> significantly improve the performance of existing artificial intelligence systems , Allow practitioners to easily integrate the foundation AI The model is customized to many different downstream applications . especially , We focus on the task of common sense reasoning , It is proved that the proposed external attention mechanism <b>can</b> enhance the existing Transformer Model , And significantly improve the reasoning ability of the ...", "dateLastCrawled": "2022-01-29T18:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Gpt 2 download - humans find gpt-2 outputs convincing", "url": "https://takze-semmire.biz/tag/gpt-2/c-d40508mdtq", "isFamilyFriendly": true, "displayUrl": "https://takze-semmire.biz/tag/gpt-2/c-d40508mdtq", "snippet": "The large-scale unsupervised language model was kept under lock and key for this long as it was deemed too dangerous\u2014a controversial decision that led to backlash from the open source community GPT-2 <b>can</b> generate thematically-appropriate text for a range of scenarios, even surreal ones like a CNN article about Donald Trump <b>giving</b> a speech praising the anime character Asuka Langley Soryu. Here, the tendency to generate nonsensical and repetitive text with increasing output length (even in ...", "dateLastCrawled": "2021-12-31T03:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "AMMU: A survey of transformer-based biomedical pretrained language ...", "url": "https://www.sciencedirect.com/science/article/pii/S1532046421003117", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046421003117", "snippet": "<b>Multi-Head</b> <b>Self Attention</b> (MHSA) ... T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular <b>group</b> <b>of people</b> <b>compared</b> to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained , , . It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any <b>group</b>. Zhang et al. further pretrained SciBERT on MIMIC-III clinical notes and showed ...", "dateLastCrawled": "2022-01-23T23:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Bayesian and Attentive Aggregation for Multi-Agent Deep Reinforcement ...", "url": "https://phiresky.github.io/masters-thesis/", "isFamilyFriendly": true, "displayUrl": "https://phiresky.github.io/masters-thesis", "snippet": "For <b>self-attention</b>, the query \\(Q\\), the key \\(K\\) and the value \\(V\\) are all <b>set</b> to the same input value. In [ 9 ] , the authors combine the <b>multi-head</b> attention with a mean aggregation. Note that they only use the attention mechanism to individually transform the information from each separate observable into a new feature <b>set</b> instead of directly using it as a weighing function for the aggregation.", "dateLastCrawled": "2022-01-28T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Perceptual Reasoning and Interaction Research - Publications", "url": "https://prior.allenai.org/publications", "isFamilyFriendly": true, "displayUrl": "https://prior.allenai.org/publications", "snippet": "Further, each head in our <b>multi-head</b> <b>self-attention</b> layer focuses on a different subset of relations. Our approach has two advantages: (1) each head considers local context instead of dispersing the attention amongst all visual entities; (2) we avoid learning redundant features. We show that our model improves the absolute accuracy of current state-of-the-art methods on TextVQA by 2.2% overall over an improved baseline, and 4.62% on questions that involve spatial reasoning and <b>can</b> be ...", "dateLastCrawled": "2022-02-02T00:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Joint entity recognition and relation extraction as</b> a <b>multi-head</b> ...", "url": "https://www.researchgate.net/publication/324717352_Joint_entity_recognition_and_relation_extraction_as_a_multi-head_selection_problem", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324717352_Joint_entity_recognition_and...", "snippet": "Instructive text (iText) consists of <b>a set</b> <b>of instructions</b> to accomplish a task or operation. Hence, iText includes a <b>group</b> of texts having a title or name of the task or operation and step-by ...", "dateLastCrawled": "2022-01-30T15:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is Semantic Communication? A View on Conveying Meaning in the Era ...", "url": "https://deepai.org/publication/what-is-semantic-communication-a-view-on-conveying-meaning-in-the-era-of-machine-intelligence", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/what-is-semantic-communication-a-view-on-conveying...", "snippet": "An encoder cascades one <b>self-attention</b> layer with a feed forward neural network. The former performs feature extraction to find the relation of words in the input sentence; the latter is trained with a suitable objective, such as language translation. A decoder has a similar structure as the encoder except for having an extra encoder-decoder attention layer inserted between the <b>self-attention</b> layer and the feed forward neural network. The additional layer helps the decoder focus on a ...", "dateLastCrawled": "2022-02-01T07:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "feature <b>set</b>. The <b>group</b> of features your <b>machine learning</b> model trains on. For example, postal code, property size, and property condition might comprise a simple feature <b>set</b> for a model that predicts housing prices. feature spec. #TensorFlow. Describes the information required to extract features data from the tf.Example protocol buffer. Because the tf.Example protocol buffer is just a container for data, you must specify the following: the data to extract (that is, the keys for the features ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Master thesis by asck - Issuu", "url": "https://issuu.com/asck/docs/multilingual_detection_ofoffensive_speech_in_socia", "isFamilyFriendly": true, "displayUrl": "https://issuu.com/asck/docs/multilingual_detection_ofoffensive_speech_in_socia", "snippet": "Both the Scaled Dot-Product Attention and the <b>Multi-Head</b> Attention <b>can</b> be seen in figure 10. Figure 10: To the left the <b>self-attention</b> module is depicted, and to the right the <b>Multi-Head</b> Attention ...", "dateLastCrawled": "2022-02-02T05:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Proceedings of the 2021 Conference on Empirical Methods in Natural ...", "url": "https://aclanthology.org/volumes/2021.emnlp-main/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2021.emnlp-main", "snippet": "Recent efforts to improve the efficiency of <b>self-attention</b> have led to a proliferation of long-range Transformer language models, which <b>can</b> process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM ...", "dateLastCrawled": "2022-01-24T04:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "nlp21/acl.dev at main \u00b7 dbamman/nlp21 \u00b7 GitHub", "url": "https://github.com/dbamman/nlp21/blob/main/HW3/acl.dev", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dbamman/nlp21/blob/main/HW3/acl.dev", "snippet": "It turns out that we <b>can</b> get by without input-wide <b>self-attention</b> at all layers, especially in the lower layers. We introduce DeFormer, a decomposed transformer, which substitutes the full <b>self-attention</b> with question-wide and passage-wide self-attentions in the lower layers. This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically. Furthermore, because DeFormer is ...", "dateLastCrawled": "2021-12-24T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Transformer demo - this is a demonstration of the operation of a ...", "url": "https://otabudete.com/wiki/index0bn996-83up.php?title=Transformer_Demo", "isFamilyFriendly": true, "displayUrl": "https://otabudete.com/wiki/index0bn996-83up.php?title=Transformer_Demo", "snippet": "FastFormers provides <b>a set</b> of recipes and methods to achieve highly efficient inference of Transformer models for Natural Language Understanding (NLU) including the demo models showing 233.87x speed-up (Yes, 233x on CPU with the <b>multi-head</b> self-attentive Transformer architecture. This is not an LSTM or an RNN). The details of the methods and analyses are described in the paper.", "dateLastCrawled": "2022-01-05T22:43:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "10.5. <b>Multi-Head Attention</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_attention-mechanisms/multihead-attention.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_attention-mechanisms/<b>multihead-attention</b>.html", "snippet": "This design is called <b>multi-head attention</b>, where each of the \\(h\\) attention pooling outputs is a head [Vaswani et al., 2017]. Using fully-connected layers to perform learnable linear transformations, Fig. 10.5.1 describes <b>multi-head attention</b>.", "dateLastCrawled": "2022-02-02T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "The masked <b>multi-head</b> attention segment, which performs <b>multi-head</b> <b>self-attention</b> on the outputs, but does so in a masked way, so that positions depend on the past only. The <b>multi-head</b> attention segment , which performs <b>multi-head</b> <b>self-attention</b> on a combination of the ( encoded ) inputs and the outputs, so that the model learns to correlate encoded inputs with desired outputs.", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "Refer also to <b>self-attention</b> and <b>multi-head</b> <b>self-attention</b>, which are the building blocks of Transformers. B. bag of words. #language. A representation of the words in a phrase or passage, irrespective of order. For example, bag of words represents the following three phrases identically: the dog jumps; jumps the dog; dog jumps the; Each word is mapped to an index in a sparse vector, where the vector has an index for every word in the vocabulary. For example, the phrase the dog jumps is ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "11. Attention Mechanisms \u2014 Dive into Deep <b>Learning</b> 0.17.0 documentation", "url": "http://preview.d2l.ai/d2l-en/master/chapter_attention-mechanisms/index.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai/d2l-en/master/chapter_attention-mechanisms/index.html", "snippet": "In the end, equipped with the more recent <b>multi-head</b> attention and <b>self-attention</b> designs, we will describe the transformer architecture based solely on attention mechanisms. Since their proposal in 2017, transformers have been pervasive in modern deep <b>learning</b> applications, such as in areas of language, vision, speech, and reinforcement <b>learning</b>.", "dateLastCrawled": "2022-01-18T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "From our article about GPT: \u201cThe input is then served to a masked <b>multi-head</b> attention segment, which computes <b>self-attention</b> in a unidirectional way.Here, the residual is added and the result is layer normalized.\u201d Indeed, GPT (which uses the Transformer decoder segment autoregressively during pretraining) and the original Transformer (which performs Seq2Seq), apply a mask in one of the attention modules \u2013 the masked <b>multi-head</b> <b>self-attention</b> subsegment in the decoder segment.. For any ...", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "CoAtNet: how to perfectly combine CNNs and Transformers | by Leonardo ...", "url": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e187ecbf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e...", "snippet": "The <b>multi-head</b> attention block computes <b>self-attention</b> several times with different weight matrices and then concatenates the results together, which are resized to the embedding dimension using ...", "dateLastCrawled": "2022-01-26T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Capturing Multi-Resolution Context by Dilated <b>Self-Attention</b>", "url": "https://www.merl.com/publications/docs/TR2021-036.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.merl.com/publications/docs/TR2021-036.pdf", "snippet": "to <b>machine</b> translation or language modeling, where close-by words are more likely to have a dependent relationship, while only a few distant words or word groups are relevant to trace the semantic con-text and syntax of a sentence [15]. This hypothesis is investigated in this work by combining re-stricted (or time-restricted) <b>self-attention</b> with a dilation mechanism, whereby a high <b>self-attention</b> resolution for neighboring frames and a lower <b>self-attention</b> resolution for distant information ...", "dateLastCrawled": "2021-12-02T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "If you are looking for an <b>analogy</b> between <b>self attention</b> and attention, think of z serving the purpose of context vectors and not global alignment weights. The <b>Transformer</b> . \u26a0\ufe0f A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2) (left) The <b>Transformer</b> architecture. Source: paper. (right) An abstracted version of the same for better ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Dive into Deep <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/", "isFamilyFriendly": true, "displayUrl": "d2l.ai", "snippet": "[Dec 2021] We added a new option to run this book for free: check out SageMaker Studio Lab. [Jul 2021] We have improved the content and added TensorFlow implementations up to Chapter 11. To keep track of the latest updates, just follow D2L&#39;s open-source project. [Jan 2021] Check out the brand-new Chapter: Attention Mechanisms.We have also added PyTorch implementations.", "dateLastCrawled": "2022-01-30T00:18:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(multi-head self-attention)  is like +(giving a set of instructions to a group of people)", "+(multi-head self-attention) is similar to +(giving a set of instructions to a group of people)", "+(multi-head self-attention) can be thought of as +(giving a set of instructions to a group of people)", "+(multi-head self-attention) can be compared to +(giving a set of instructions to a group of people)", "machine learning +(multi-head self-attention AND analogy)", "machine learning +(\"multi-head self-attention is like\")", "machine learning +(\"multi-head self-attention is similar\")", "machine learning +(\"just as multi-head self-attention\")", "machine learning +(\"multi-head self-attention can be thought of as\")", "machine learning +(\"multi-head self-attention can be compared to\")"]}