{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Counterfactual</b> <b>Fairness</b>: Unidentification, Bound and <b>Algorithm</b> (Journal ...", "url": "https://par.nsf.gov/biblio/10126321-counterfactual-fairness-unidentification-bound-algorithm", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/biblio/10126321", "snippet": "<b>Fairness</b>-aware <b>learning</b> studies the problem of building <b>machine</b> <b>learning</b> models that are subject to <b>fairness</b> requirements. <b>Counterfactual</b> <b>fairness</b> is a notion of <b>fairness</b> derived from Pearl&#39;s causal model, which considers a model is fair if for a particular individual or group its prediction in the real world is the same as that in the <b>counterfactual</b> world where the individual(s) had belonged to a different demographic group.", "dateLastCrawled": "2022-01-21T17:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Counterfactual Reasoning in Algorithmic Fairness</b>", "url": "http://fairware.cs.umass.edu/slides/silva.pdf", "isFamilyFriendly": true, "displayUrl": "fairware.cs.umass.edu/slides/silva.pdf", "snippet": "\u2022 We would <b>like</b> to predict Y in a \u201cfair\u201d way, meaning that our predictions should not be \u201cbiased\u201d against particular instances of A. Primitives ... \u2022 Choose any <b>machine</b> <b>learning</b> <b>algorithm</b> of interest, any black-box that takes as inputs observed and unobserved variables in your domain. \u2013 Select a set of variables based on which sets respect <b>counterfactual</b> <b>fairness</b>. \u2013 If necessary, infer unobserved variables from the observed ones. Some Words of Caution \u2022 Structural equations ...", "dateLastCrawled": "2021-11-20T16:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "the <b>machine</b> <b>learning</b> literature, the goal of this experiment is to quantify how our <b>algorithm</b> behav es with \ufb01nite sample sizes while assuming ground truth compatible with a synthetic model ...", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Counterfactual Fairness</b> | DeepAI", "url": "https://deepai.org/publication/counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>counterfactual-fairness</b>", "snippet": "One simple but important implication of the definition of <b>counterfactual fairness</b> is the following: Lemma 1. Let G be the causal graph of the given model (U,V,F). Then ^Y will be counterfactually fair if it is a function of the non-descendants of A. Proof. Let W be any non-descendant of A in G. Then W A\u2190a(U) and W A\u2190a.", "dateLastCrawled": "2022-01-26T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Algorithmic Bias: A <b>Counterfactual</b> Perspective", "url": "https://bitlab.cas.msu.edu/trustworthy-algorithms/whitepapers/Bo%20Cowgill.pdf", "isFamilyFriendly": true, "displayUrl": "https://bitlab.cas.msu.edu/trustworthy-<b>algorithm</b>s/whitepapers/Bo Cowgill.pdf", "snippet": "We discuss an alternative approach to measuring bias and <b>fairness</b> in <b>machine</b> <b>learning</b>: <b>Counterfactual</b> evaluation. In many practical settings, the alternative to a biased <b>algorithm</b> is not an unbiased one, but another decision method such as another <b>algorithm</b> or human discretion. We discuss statistical techniques necessary for <b>counterfactual</b> comparisons, which enable researchers to quantify relative biases without access to the underlying <b>algorithm</b> or its training data. We close by discussing ...", "dateLastCrawled": "2022-01-31T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Reviews: <b>Counterfactual</b> <b>Fairness</b>", "url": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Reviews.html", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Reviews.html", "snippet": "This paper presents an interesting and valuable contribution to the small but growing literature on <b>fairness</b> in <b>machine</b> <b>learning</b>. Specifically, it provides at least three contributions: (1) a definition of <b>counter factual</b> <b>fairness</b>; (2) an <b>algorithm</b> for <b>learning</b> a model under <b>counter factual</b> <b>fairness</b>; and (3) experiments with that <b>algorithm</b>. The value and convincingness of each of these contributions declines steadily. The value of the contributions of the current paper is sufficient for ...", "dateLastCrawled": "2021-11-21T05:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2,", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review", "url": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "isFamilyFriendly": true, "displayUrl": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "snippet": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review Sahil Verma Arthur AI, University of Washington Washington D.C., USA vsahil@cs.washington.edu John Dickerson Arthur AI Washington D.C., USA john@arthur.ai Keegan Hines Arthur AI Washington D.C., USA keegan@arthur.ai Abstract <b>Machine</b> <b>learning</b> plays a role in many deployed decision systems, often in ways that are dif\ufb01cult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship ...", "dateLastCrawled": "2022-01-29T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "9.3 <b>Counterfactual</b> Explanations | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/counterfactual.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>counterfactual</b>.html", "snippet": "In interpretable <b>machine</b> <b>learning</b>, <b>counterfactual</b> explanations can be used to explain predictions of individual instances. The \u201cevent\u201d is the predicted outcome of an instance, the \u201ccauses\u201d are the particular feature values of this instance that were input to the model and \u201ccaused\u201d a certain prediction. Displayed as a graph, the relationship between the inputs and the prediction is very simple: The feature values cause the prediction. FIGURE 9.9: The causal relationships between ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Counterfactual Fairness</b> - NIPS", "url": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf", "snippet": "<b>Counterfactual Fairness</b> Matt Kusner The Alan Turing Institute and University of Warwick mkusner@turing.ac.uk Joshua Loftus New York University loftus@nyu.edu Chris Russell The Alan Turing Institute and University of Surrey crussell@turing.ac.uk Ricardo Silva The Alan Turing Institute and University College London ricardo@stats.ucl.ac.uk Abstract <b>Machine</b> <b>learning</b> can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending ...", "dateLastCrawled": "2022-01-26T15:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Privacy and <b>Fairness</b>: two very different <b>machine</b> <b>learning</b> ideals? | Revue", "url": "https://newsletter.mukulrathi.com/issues/privacy-and-fairness-two-very-different-machine-learning-ideals-717119", "isFamilyFriendly": true, "displayUrl": "https://newsletter.mukulrathi.com/issues/privacy-and-<b>fairness</b>-two-very-different...", "snippet": "<b>Counterfactual</b> <b>fairness</b> could be <b>similar</b> for <b>fairness</b>, where you can guarantee <b>fairness</b> given your assumptions about causality. There\u2019s a catch It appears that current methods for privacy and <b>fairness</b> seem to be conflicting.", "dateLastCrawled": "2022-01-30T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "the <b>machine</b> <b>learning</b> literature, the goal of this experiment is to quantify how our <b>algorithm</b> behav es with \ufb01nite sample sizes while assuming ground truth compatible with a synthetic model ...", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Counterfactual</b> <b>Fairness</b>: Unidentification, Bound and <b>Algorithm</b> ...", "url": "https://www.researchgate.net/publication/334843895_Counterfactual_Fairness_Unidentification_Bound_and_Algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334843895_<b>Counterfactual</b>_<b>Fairness</b>_Un...", "snippet": "<b>Fairness</b>-aware <b>learning</b> studies the problem of building <b>machine</b> <b>learning</b> models that are subject to <b>fairness</b> requirements. <b>Counterfactual</b> <b>fairness</b> is a notion of <b>fairness</b> derived from Pearl&#39;s ...", "dateLastCrawled": "2021-12-23T12:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review", "url": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "isFamilyFriendly": true, "displayUrl": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "snippet": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review Sahil Verma Arthur AI, University of Washington Washington D.C., USA vsahil@cs.washington.edu John Dickerson Arthur AI Washington D.C., USA john@arthur.ai Keegan Hines Arthur AI Washington D.C., USA keegan@arthur.ai Abstract <b>Machine</b> <b>learning</b> plays a role in many deployed decision systems, often in ways that are dif\ufb01cult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship ...", "dateLastCrawled": "2022-01-29T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Verifying Individual <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Models", "url": "http://proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "snippet": "the existing work on verifying bias/<b>fairness</b> in <b>machine</b> <b>learning</b> models considers notions of group <b>fairness</b>/bias (Albarghouthi et al. 2017; Bastani et al. 2019). An individual <b>fairness</b> property considers the worst case (<b>fairness</b> for all <b>similar</b> input pairs, biased if there exists a bad input pair), rather than the average case (with high probability, some notion of parity is maintained between different groups) considered in the group <b>fairness</b> de\ufb01nitions. Hence, the existing techniques for ...", "dateLastCrawled": "2022-02-03T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "9.3 <b>Counterfactual</b> Explanations | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/counterfactual.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>counterfactual</b>.html", "snippet": "In interpretable <b>machine</b> <b>learning</b>, <b>counterfactual</b> explanations can be used to explain predictions of individual instances. The \u201cevent\u201d is the predicted outcome of an instance, the \u201ccauses\u201d are the particular feature values of this instance that were input to the model and \u201ccaused\u201d a certain prediction. Displayed as a graph, the relationship between the inputs and the prediction is very simple: The feature values cause the prediction. FIGURE 9.9: The causal relationships between ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2,", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2109.10458] Achieving <b>Counterfactual</b> <b>Fairness</b> for Causal Bandit", "url": "https://arxiv.org/abs/2109.10458", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2109.10458", "snippet": "Title: Achieving <b>Counterfactual</b> <b>Fairness</b> for Causal Bandit. Authors: Wen Huang, Lu Zhang, Xintao Wu. Download PDF Abstract: In online recommendation, customers arrive in a sequential and stochastic manner from an underlying distribution and the online decision model recommends a chosen item for each arriving individual based on some strategy. We study how to recommend an item at each step to maximize the expected reward while achieving user-side <b>fairness</b> for customers, i.e., customers who ...", "dateLastCrawled": "2021-12-24T07:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "AI bias - what is it and how to avoid it?", "url": "https://levity.ai/blog/ai-bias-how-to-avoid", "isFamilyFriendly": true, "displayUrl": "https://levity.ai/blog/ai-bias-how-to-avoid", "snippet": "<b>Machine</b> <b>learning</b> bias, also known as <b>algorithm</b> bias or artificial intelligence bias, refers to the tendency of algorithms to reflect human biases. It is a phenomenon that arises when an <b>algorithm</b> delivers systematically biased results as a consequence of erroneous assumptions of the <b>machine</b> <b>learning</b> process. In today\u2019s climate of increasing representation and diversity, this becomes even more problematic because algorithms could be reinforcing biases. For example, a facial recognition ...", "dateLastCrawled": "2022-01-30T07:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Counterfactual Fairness</b> | DeepAI", "url": "https://deepai.org/publication/counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>counterfactual-fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation.", "dateLastCrawled": "2022-01-26T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Counterfactual</b> <b>Fairness</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1703.06856/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.06856", "snippet": "In large part, the initial work on <b>fairness</b> in <b>machine</b> <b>learning</b> has focused on formalizing <b>fairness</b> into quantitative definitions and using them to solve a discrimination problem in a certain dataset. Unfortunately, for a practitioner, law-maker, judge, or anyone else who is interested in implementing algorithms that control for discrimination, it <b>can</b> be difficult to decide which definition of <b>fairness</b> to choose for the task at hand. Indeed, we demonstrate that depending on the relationship ...", "dateLastCrawled": "2021-12-15T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Contrastive <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/contrastive-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/contrastive-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "<b>Machine</b> <b>learning</b> after all is a data driven optimal function fitting exercise, thus it has mostly dealt with association, rather than causation [2]. Given, the broad use of <b>machine</b> <b>learning</b> algorithms in the modern world, precautions to ensure the <b>fairness</b> of the decision making process of such algorithms is of great importance [3].", "dateLastCrawled": "2021-12-22T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Trustworthy <b>Machine</b> <b>Learning</b> - Kush R. Varshney - Chapter 10: <b>Fairness</b>", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthy<b>machinelearning</b>.com/trustworthy<b>machinelearning</b>-10.htm", "snippet": "\u00a7 compare and contrast definitions of <b>fairness</b> in a <b>machine</b> <b>learning</b> context, \u00a7 ... but this is just a <b>thought</b> experiment.) <b>Counterfactual</b> <b>fairness</b> <b>can</b> be tested using treatment effect estimation methods from chapter 8. Protected attributes causing different outcomes across groups is an important consideration in many laws and regulations. [11] Suppose you have a full-blown causal graph of all the variables given to you or you discover one from data using the methods of chapter 8. In that ...", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>What if Algorithms Could be Fair</b>? - <b>Human Readable Magazine</b>", "url": "https://humanreadablemag.com/issues/0/articles/what-if-algorithms-could-be-fair/", "isFamilyFriendly": true, "displayUrl": "https://humanreadablemag.com/issues/0/articles/<b>what-if-algorithms-could-be-fair</b>", "snippet": "<b>Counterfactual</b> <b>fairness</b> is a new approach to <b>fairness</b> in <b>machine</b> <b>learning</b>, statistical models, and algorithms. It draws on the new field of causality to go beyond statistical relationships and correlations and to model the root causes of differences between protected groups. A mathematized notion of <b>fairness</b> removes the fluff and emotion from <b>fairness</b>, allowing us to clearly model and compare our beliefs about causal relationships in the world, and to empirically test our claims about bias ...", "dateLastCrawled": "2022-01-04T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Use and Misuse of Counterfactuals in Ethical <b>Machine</b> <b>Learning</b> ...", "url": "https://www.arxiv-vanity.com/papers/2102.05085/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2102.05085", "snippet": "The use of counterfactuals for considerations of algorithmic <b>fairness</b> and explainability is gaining prominence within the <b>machine</b> <b>learning</b> community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the <b>counterfactual</b> approach in <b>machine</b> ...", "dateLastCrawled": "2021-11-18T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "NeurIPS | 2021", "url": "https://nips.cc/Conferences/2021/ScheduleMultitrack?event=21850", "isFamilyFriendly": true, "displayUrl": "https://nips.cc/Conferences/2021/ScheduleMultitrack?event=21850", "snippet": "In our work, we use the framework of <b>counterfactual</b> <b>fairness</b> to train fair <b>machine</b> <b>learning</b> models. We propose a new causal graph for the variables available in the Home Mortgage Disclosure Act (HMDA) data. We use a matching-based approach instead of the latent variable modeling approach, because the former approach does not rely on any modeling assumptions. Furthermore, matching provides us with <b>counterfactual</b> pairs in which the race variable is isolated. We first demonstrate the unfairness ...", "dateLastCrawled": "2022-02-03T03:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Frontiers | Addressing <b>Fairness</b>, Bias, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "While the eventual optimal tuning of an <b>algorithm</b> <b>can</b> thus depend on many factors, including the local system of laws and community values, the overall goal of this paper is to introduce and define the notions of bias, <b>fairness</b>, and appropriate use, as it pertains to <b>machine</b> <b>learning</b> in global health, and also to illustrate how a given <b>machine</b> <b>learning</b> model <b>can</b> be analyzed to identify and quantify issues of bias and <b>fairness</b>. With this goal in mind, this paper is intended for the audience ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Counterfactual Fairness</b> | DeepAI", "url": "https://deepai.org/publication/counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>counterfactual-fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation.", "dateLastCrawled": "2022-01-26T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Algorithmic Bias: A <b>Counterfactual</b> Perspective", "url": "https://bitlab.cas.msu.edu/trustworthy-algorithms/whitepapers/Bo%20Cowgill.pdf", "isFamilyFriendly": true, "displayUrl": "https://bitlab.cas.msu.edu/trustworthy-<b>algorithm</b>s/whitepapers/Bo Cowgill.pdf", "snippet": "We discuss an alternative approach to measuring bias and <b>fairness</b> in <b>machine</b> <b>learning</b>: <b>Counterfactual</b> evaluation. In many practical settings, the alternative to a biased <b>algorithm</b> is not an unbiased one, but another decision method such as another <b>algorithm</b> or human discretion. We discuss statistical techniques necessary for <b>counterfactual</b> comparisons, which enable researchers to quantify relative biases without access to the underlying <b>algorithm</b> or its training data. We close by discussing ...", "dateLastCrawled": "2022-01-31T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reviews: <b>Counterfactual</b> <b>Fairness</b>", "url": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Reviews.html", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Reviews.html", "snippet": "As <b>machine</b> <b>learning</b> takes a more prominent role in decision making related to people, the question of <b>fairness</b> becomes increasingly important. The proposed definition is compelling and, in my opinion, better captures human conceptions of <b>fairness</b> <b>compared</b> to prior, non-<b>counterfactual</b> definitions. Additionally, the paper is well written, easy to follow, and technically correct.", "dateLastCrawled": "2021-11-21T05:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Counterfactual</b> <b>Fairness</b>: Unidentification, Bound and <b>Algorithm</b> ...", "url": "https://www.researchgate.net/publication/334843895_Counterfactual_Fairness_Unidentification_Bound_and_Algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334843895_<b>Counterfactual</b>_<b>Fairness</b>_Un...", "snippet": "<b>Fairness</b>-aware <b>learning</b> studies the problem of building <b>machine</b> <b>learning</b> models that are subject to <b>fairness</b> requirements. <b>Counterfactual</b> <b>fairness</b> is a notion of <b>fairness</b> derived from Pearl&#39;s ...", "dateLastCrawled": "2021-12-23T12:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Adversarial <b>Learning</b> for <b>Counterfactual</b> <b>Fairness</b> | DeepAI", "url": "https://deepai.org/publication/adversarial-learning-for-counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/adversarial-<b>learning</b>-for-<b>counterfactual</b>-<b>fairness</b>", "snippet": "Adversarial <b>Learning</b> for <b>Counterfactual</b> <b>Fairness</b>. 08/30/2020 \u2219 by Vincent Grari, et al. \u2219 AXA \u2219 Laboratoire d&#39;Informatique de Paris 6 \u2219 0 \u2219 share . In recent years, <b>fairness</b> has become an important topic in the <b>machine</b> <b>learning</b> research community. In particular, <b>counterfactual</b> <b>fairness</b> aims at building prediction models which ensure <b>fairness</b> at the most individual level.", "dateLastCrawled": "2021-12-11T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Counterfactual</b> <b>Fairness</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1703.06856/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.06856", "snippet": "In large part, the initial work on <b>fairness</b> in <b>machine</b> <b>learning</b> has focused on formalizing <b>fairness</b> into quantitative definitions and using them to solve a discrimination problem in a certain dataset. Unfortunately, for a practitioner, law-maker, judge, or anyone else who is interested in implementing algorithms that control for discrimination, it <b>can</b> be difficult to decide which definition of <b>fairness</b> to choose for the task at hand. Indeed, we demonstrate that depending on the relationship ...", "dateLastCrawled": "2021-12-15T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Counterfactual</b> Explanations for <b>Machine</b> <b>Learning</b>: A Review", "url": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "isFamilyFriendly": true, "displayUrl": "https://ml-retrospectives.github.io/neurips2020/camera_ready/5.pdf", "snippet": "choose the most appropriate <b>algorithm</b> given the set of assumptions they have and the speed and quality of the generation they want to achieve. 2. 3. Comprehensive and lucid introduction for beginners in the area of <b>counterfactual</b> explana- tions for <b>machine</b> <b>learning</b>. 2 Background This section gives the background about the social implications of <b>machine</b> <b>learning</b>, explainability research in <b>machine</b> <b>learning</b>, and some prior studies about <b>counterfactual</b> explanations. 2.1 Social Implications of ...", "dateLastCrawled": "2022-01-29T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Verifying Individual <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Models", "url": "http://proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v124/george-john20a/george-john20a.pdf", "snippet": "Verifying Individual <b>Fairness</b> in <b>Machine</b> <b>Learning</b> Models Philips George John, Deepak Vijaykeerthy, Diptikalyan Saha IBM Research AI Bengaluru 560 045, India Abstract We consider the problem of whether a given decision model, working with structured data, has individual <b>fairness</b>. Following the work of Dwork, a model is individually biased (or unfair) if there is a pair of valid inputs which are close to each other (according to an ap-propriate metric) but are treated differently by the model ...", "dateLastCrawled": "2022-02-03T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "9.3 <b>Counterfactual</b> Explanations | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/counterfactual.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>counterfactual</b>.html", "snippet": "In interpretable <b>machine</b> <b>learning</b>, <b>counterfactual</b> explanations <b>can</b> be used to explain predictions of individual instances. The \u201cevent\u201d is the predicted outcome of an instance, the \u201ccauses\u201d are the particular feature values of this instance that were input to the model and \u201ccaused\u201d a certain prediction. Displayed as a graph, the relationship between the inputs and the prediction is very simple: The feature values cause the prediction. FIGURE 9.9: The causal relationships between ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Responsible AI practices \u2013 Google AI", "url": "https://ai.google/responsibilities/responsible-ai-practices/?category=fairness", "isFamilyFriendly": true, "displayUrl": "https://ai.google/responsibilities/responsible-ai-practices/?category=<b>fairness</b>", "snippet": "A case-study on the application of <b>fairness</b> in <b>machine</b> <b>learning</b> research to a production classification system, and new insights in how to measure and address algorithmic <b>fairness</b> issues. Research paper <b>Counterfactual</b> <b>fairness</b> in text classification through robustness Provides and compares multiple approaches for addressing <b>counterfactual</b> <b>fairness</b> issues in text models. Research paper Model Cards for Model Reporting Proposes a framework to encourage transparent model reporting. Research ...", "dateLastCrawled": "2022-02-02T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fairness in Machine Learning: Lessons from Political Philosophy</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-lessons-from-political-philosophy", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-lessons-from-political</b>...", "snippet": "This discussion suggests that \u2018<b>fairness</b>\u2019 as used in the fair <b>machine</b> <b>learning</b> community is best understood as a placeholder term for a variety of normative egalitarian considerations. Notably, while egalitarianism is a widely held principle, exactly what it requires is the subject of much debate. I provide an overview of some of this debate and finish with implications for the incorporation of \u2018<b>fairness</b>\u2019 into algorithmic decision-making systems.", "dateLastCrawled": "2021-12-26T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Counterfactual</b> Explanation of <b>Machine</b> <b>Learning</b> Survival Models - IOS Press", "url": "https://content.iospress.com/articles/informatica/infor468", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/informatica/infor468", "snippet": "A method for <b>counterfactual</b> explanation of <b>machine</b> <b>learning</b> survival models is proposed. One of the difficulties of solving the <b>counterfactual</b> explanation problem is that the classes of examples are implicitly defined through outcomes of a <b>machine</b> <b>learning</b> survival model in the form of survival functions. A condition that establishes the difference between survival functions of the original example and the <b>counterfactual</b> is introduced. This condition is based on using a distance between mean ...", "dateLastCrawled": "2022-01-15T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "A lot of what is discussed in the <b>machine</b> <b>learning</b> literature touches on <b>fairness</b> (or rather equivalence in certain outcomes) between groups, yet this narrowly constricts <b>fairness</b> to the notion of equality. Of course, we should think about <b>fairness</b> in the context of prejudiced groups, but we should also ask whether it is fair to an individual. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[PDF] A Survey on Bias and <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | Semantic Scholar", "url": "https://www.semanticscholar.org/paper/A-Survey-on-Bias-and-Fairness-in-Machine-Learning-Mehrabi-Morstatter/0090023afc66cd2741568599057f4e82b566137c", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/A-Survey-on-Bias-and-<b>Fairness</b>-in-<b>Machine</b>...", "snippet": "This survey investigated different real-world applications that have shown biases in various ways, and created a taxonomy for <b>fairness</b> definitions that <b>machine</b> <b>learning</b> researchers have defined to avoid the existing bias in AI systems. With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for <b>fairness</b> has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive ...", "dateLastCrawled": "2022-01-29T11:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "dimensions of <b>machine</b> Causality and the normative", "url": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "isFamilyFriendly": true, "displayUrl": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "snippet": "dimensions of <b>machine</b> <b>learning</b> Joshua Loftus (LSE Statistics) High level intro Causality, what is it good for? Causal <b>fairness</b> In prediction and ranking tasks, and with intersectionality Designing interventions Optimal fair policies, causal interference Concluding thoughts 2 / 27. Tech solutionism, using ML/AI in every situation 3 / 27. Imagination Albert Einstein: Imagination is more important than knowledge. For knowledge is limited, whereas imagination [...] stimulat[es] progress, giving ...", "dateLastCrawled": "2022-01-11T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Stable <b>Learning</b> and its Causal Implication", "url": "http://pengcui.thumedialab.com/papers/Stable%20Learning-tutorial-valse2021.pdf", "isFamilyFriendly": true, "displayUrl": "pengcui.thumedialab.com/papers/Stable <b>Learning</b>-tutorial-valse2021.pdf", "snippet": "Application --- <b>counterfactual</b> visual explanations ... Goyal, Yash, et al. &quot;<b>Counterfactual</b> visual explanations.&quot; International Conference on <b>Machine</b> <b>Learning</b>. PMLR, 2019. Explainability with Causality Application --- causal recommendation 17 He et al. \u201dCollaborative Causal Filtering for Out-of-Distribution Recommendation.&quot; Under review. Caual structure among user features and item features Example . Explainability and OOD \u2022Explainability would be a side product when pursuing OOD with ...", "dateLastCrawled": "2022-01-28T03:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Mitigating Political Bias in Language Models Through Reinforced Calibration", "url": "https://www.cs.dartmouth.edu/~rbliu/aaai_copy.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.dartmouth.edu/~rbliu/aaai_copy.pdf", "snippet": "\ufb01rst proposed <b>counterfactual</b> <b>fairness</b>, which treats data samples equally in actual and <b>counterfactual</b> demographic groups. Zhao et al. mitigated gender bias by augmenting original data with gender-swapping and training a unbiased system on the union of two datasets. Other augmentation techniques have reduced gender bias in hate speech detec-tion (Park, Shin, and Fung 2018; Liu et al. 2020), knowledge graph building (Mitchell et al. 2019) and <b>machine</b> transla-tion (Stanovsky, Smith, and ...", "dateLastCrawled": "2022-01-28T20:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Counterfactual Fairness \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1703.06856/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.06856", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however, previous decisions have been made that are unfairly biased against certain subpopulations (e.g., those of a particular race, gender, or sexual orientation). Because this past data is often biased, <b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating discriminatory practices (or ...", "dateLastCrawled": "2021-12-15T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Abstract - arXiv", "url": "https://arxiv.org/pdf/1703.06856v3.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1703.06856v3.pdf", "snippet": "<b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our de\ufb01nition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real ...", "dateLastCrawled": "2020-08-09T05:32:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(counterfactual fairness)  is like +(machine learning algorithm)", "+(counterfactual fairness) is similar to +(machine learning algorithm)", "+(counterfactual fairness) can be thought of as +(machine learning algorithm)", "+(counterfactual fairness) can be compared to +(machine learning algorithm)", "machine learning +(counterfactual fairness AND analogy)", "machine learning +(\"counterfactual fairness is like\")", "machine learning +(\"counterfactual fairness is similar\")", "machine learning +(\"just as counterfactual fairness\")", "machine learning +(\"counterfactual fairness can be thought of as\")", "machine learning +(\"counterfactual fairness can be compared to\")"]}