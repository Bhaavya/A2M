{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "python - <b>Generating confidence interval for precision recall</b> <b>curve</b> ...", "url": "https://stackoverflow.com/questions/57482356/generating-confidence-interval-for-precision-recall-curve", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/57482356", "snippet": "I have <b>predictions</b> from a trained model and I can generate a precision-recall <b>curve</b> for the data pretty easily, and thus, also the <b>area</b> <b>under</b> the precision-recall <b>curve</b> (AUPRC). However, I&#39;m trying to also generate a 95% <b>confidence</b> interval for the data, which I&#39;m having a hard time finding something for. I&#39;ve looked in sklearn for python and pROC package for R (which does have some usage for <b>PR</b>, just not AUPRC), but I&#39;m not finding anything outside of some pretty high level academic papers ...", "dateLastCrawled": "2022-01-19T10:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Gaining an intuitive understanding of <b>Precision, Recall</b> and <b>Area</b> <b>Under</b> ...", "url": "https://towardsdatascience.com/gaining-an-intuitive-understanding-of-precision-and-recall-3b9df37804a7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/gaining-an-intuitive-<b>under</b>standing-of-precision-and...", "snippet": "Please note that a correct interpretation of a <b>precision-recall</b> <b>curve</b>, however, requires that you also know the ratio of positive samples w.r.t. all samples. 1.1 <b>Area</b> <b>Under</b> the <b>Precision-Recall</b> <b>Curve</b> (<b>PR</b>-AUC) Finally, we arrive at the definition of the metric <b>PR</b>-AUC. The general definition of <b>PR</b>-AUC is finding the <b>area</b> <b>under</b> the precision ...", "dateLastCrawled": "2022-01-31T16:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Evaluation metrics for object detection and segmentation</b>: mAP", "url": "https://kharshit.github.io/blog/2019/09/20/evaluation-metrics-for-object-detection-and-segmentation", "isFamilyFriendly": true, "displayUrl": "https://kharshit.github.io/blog/2019/09/20/<b>evaluation-metrics-for-object-detection</b>-and...", "snippet": "Then compute the AP as the <b>area</b> <b>under</b> this <b>curve</b> by numerical integration. i.e. given <b>the PR</b> <b>curve</b> in orange, calculate the max precision to the right for all the recall points thus getting a new <b>curve</b> in green. Now, take the AUC using integration <b>under</b> the green <b>curve</b>. It would be the AP. The only difference from VOC 2007 here is that we\u2019re ...", "dateLastCrawled": "2022-02-02T13:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Entry 26: Setting thresholds - precision, recall, and</b> ROC - Data ...", "url": "https://julielinx.github.io/blog/26_thresholds_pr_roc/", "isFamilyFriendly": true, "displayUrl": "https://julielinx.github.io/blog/26_thresholds_<b>pr</b>_roc", "snippet": "The <b>area</b> <b>under</b> the <b>curve</b> (AUC) is exactly what it sounds <b>like</b>. <b>The PR</b> <b>curve</b> divides the chart into two sides. The closer the <b>curve</b> is to the upper left the more space will be <b>under</b> that <b>curve</b>. AUC calculates the <b>area</b> that lies underneath the <b>curve</b> to provide a general metric for how well the model performs. <b>PR</b> AUC is the <b>area</b> <b>under</b> the precision / recall <b>curve</b>. Receiver operating characteristic (ROC) <b>curve</b>. This plots the true positive rate (also known as recall and sensitivity) on the y ...", "dateLastCrawled": "2022-01-28T07:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "F1 <b>Score</b> vs ROC AUC vs Accuracy vs <b>PR</b> AUC: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-roc-auc-<b>pr</b>-auc", "snippet": "AUC means <b>area</b> <b>under</b> the <b>curve</b> so to speak about ROC AUC <b>score</b> we need to define ROC <b>curve</b> first. It is a chart that visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart.", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Confusion Matrix</b> weird results? \u00b7 Issue #1665 \u00b7 ultralytics/yolov5 \u00b7 GitHub", "url": "https://github.com/ultralytics/yolov5/issues/1665", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ultralytics/yolov5/issues/1665", "snippet": "@wonh mAP is the <b>area</b> <b>under</b> precision-recall <b>curve</b>. For generating that <b>curve</b> <b>confidence</b> changes from 1 to 0. This means that in a hypothetically perfect scenario, the <b>curve</b> would start in (1,1) and end up in (0,0), so the <b>area</b> <b>under</b> this <b>curve</b> would be 1 (mAP=1). Thanks for your reply.I konw the meaning of mAP. My point is that mAP is the ...", "dateLastCrawled": "2022-01-26T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "This is called the ROC <b>area</b> <b>under</b> <b>curve</b> or ROC AUC or sometimes ROCAUC. The score is a value between 0.0 and 1.0 for a perfect classifier. AUCROC can be interpreted as the probability that the scores given by a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. \u2014 Page 54, Learning from Imbalanced Data Sets, 2018. This single score can be used to compare binary classifier models directly. As such, this score might be the most commonly used ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "machine learning - <b>ROC</b> vs precision-and-recall curves - Cross Validated", "url": "https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/7207", "snippet": "$\\begingroup$ &gt; &quot;the <b>area</b> <b>under</b> the <b>curve</b> represents Rankedness or the probability of correct pairwise ranking&quot; I guess, that is exactly where we disagree - the <b>ROC</b> only demonstrates ranking quality in the plot. However, with the AUC <b>PR</b> is a single number that immediately tells me if which ranking is preferable (i.e., that result I is preferable over result II). The AUC <b>ROC</b> does not have this property.", "dateLastCrawled": "2022-01-27T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding the AUC-ROC <b>Curve</b> in Machine Learning Classification", "url": "https://analyticsindiamag.com/understanding-the-auc-roc-curve-in-machine-learning-classification/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>under</b>standing-the-auc-roc-<b>curve</b>-in-machine-learning...", "snippet": "<b>Area</b> <b>Under</b> <b>Curve</b> or AUC is one of the most widely used metrics for model evaluation. It is generally used for binary classification problems. AUC measures the entire two-dimensional <b>area</b> present underneath the entire ROC <b>curve</b>. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than that of a randomly chosen negative example. The <b>Area</b> <b>Under</b> the <b>Curve</b> provides the ability for a classifier to distinguish between classes and ...", "dateLastCrawled": "2022-02-03T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>sklearn</b>.metrics.auc \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/generated/<b>sklearn</b>.metrics.auc.html", "snippet": "Compute <b>Area</b> <b>Under</b> the <b>Curve</b> (AUC) using the trapezoidal rule. This is a general function, given points on a <b>curve</b>. For computing the <b>area</b> <b>under</b> the ROC-<b>curve</b>, see roc_auc_score. For an alternative way to summarize a precision-recall <b>curve</b>, see average_precision_score. Parameters x ndarray of shape (n,) x coordinates. These must be either monotonic increasing or monotonic decreasing. y ndarray of shape, (n,) y coordinates. Returns auc float. See also . roc_auc_score. Compute the <b>area</b> <b>under</b> ...", "dateLastCrawled": "2022-02-02T23:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Area</b> <b>under</b> the <b>Precision-Recall Curve: Point Estimates and Confidence</b> ...", "url": "https://www.researchgate.net/publication/290094595_Area_under_the_Precision-Recall_Curve_Point_Estimates_and_Confidence_Intervals", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/290094595_<b>Area</b>_<b>under</b>_the_Precision-Recall...", "snippet": "The <b>area</b> <b>under</b> the precision-recall <b>curve</b> (AUCPR) is a single number summary of the information in the precision-recall (<b>PR</b>) <b>curve</b>. <b>Similar</b> to the receiver operating characteristic <b>curve</b>, <b>the PR</b> ...", "dateLastCrawled": "2021-11-18T00:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to efficiently implement <b>Area</b> <b>Under</b> <b>Precision-Recall</b> <b>Curve</b> (<b>PR</b>-AUC ...", "url": "https://towardsdatascience.com/how-to-efficiently-implement-area-under-precision-recall-curve-pr-auc-a85872fd7f14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-efficiently-implement-<b>area</b>-<b>under</b>-precision...", "snippet": "How to efficiently implement <b>Area</b> <b>Under</b> <b>Precision-Recall</b> <b>Curve</b> (<b>PR</b>-AUC) ... again use an over-simplified example containing only 10 samples. For each sample, we calculate the output (predicted <b>confidence</b>) of the neural network and put it together with the corresponding ground truth label in a table, see Tab. 1. Then we sort the entries according to the predicted confidences as in Tab. 2. Python code: <b>predictions</b> = np.array([0.65,0.1,0.15,0.43,0.97,0.24,0.82,0.7,0.32,0.84]) labels = np.array ...", "dateLastCrawled": "2022-02-02T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "F1 <b>Score</b> vs ROC AUC vs Accuracy vs <b>PR</b> AUC: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-roc-auc-<b>pr</b>-auc", "snippet": "AUC means <b>area</b> <b>under</b> the <b>curve</b> so to speak about ROC AUC <b>score</b> we need to define ROC <b>curve</b> first. It is a chart that visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart. Of course, the higher TPR and the lower FPR is for each threshold the better and so classifiers that have curves that are more top-left-side are better. An extensive discussion of ROC <b>Curve</b> and ROC AUC <b>score</b> ...", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "ROC <b>Area</b> <b>Under</b> <b>Curve</b> (AUC) Score. Although the ROC <b>Curve</b> is a helpful diagnostic tool, it can be challenging to compare two or more classifiers based on their curves. Instead, the <b>area</b> <b>under</b> the <b>curve</b> can be calculated to give a single score for a classifier model across all threshold values. This is called the ROC <b>area</b> <b>under</b> <b>curve</b> or ROC AUC ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Assessing the performance of prediction models: a framework for some ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3575184", "snippet": "The performance of prediction models can be assessed using a variety of different methods and metrics. Traditional measures for binary and survival outcomes include the Brier score to indicate overall model performance, the concordance (or c) statistic for discriminative ability (or <b>area</b> <b>under</b> the receiver operating characteristic (ROC) <b>curve</b>), and goodness-of-fit statistics for calibration.. Several new measures have recently been proposed that can be seen as refinements of discrimination ...", "dateLastCrawled": "2022-02-03T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "r - What is &quot;baseline&quot; in <b>precision recall curve</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/251175/what-is-baseline-in-precision-recall-curve", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/251175", "snippet": "<b>Area</b> <b>under</b> the ROC <b>curve</b> or <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b> for imbalanced data? 16. Calculating AUPR in R. 10. What is AUC of <b>PR</b>-<b>curve</b>? 5. <b>PR</b> AUC &lt; 50% with ROC AUC &gt; 90% - model good or bad? 7. Baseline for Precision-Related Metrics. 3. <b>Area</b> <b>Under</b> the <b>Precision Recall curve</b> -<b>similar</b> interpretation to AUROC? 1. What is the expected value of AUROCC for random <b>predictions</b>? 1. What does a &quot;flat region&quot; of <b>precision recall curve</b> imply? See more linked questions. Related. 15. Increasing number of ...", "dateLastCrawled": "2022-02-01T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "machine learning - <b>ROC</b> vs precision-and-recall curves - Cross Validated", "url": "https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/7207", "snippet": "Finally, we show that an algorithm that optimizes the <b>area</b> <b>under</b> the <b>ROC</b> <b>curve</b> is not guaranteed to optimize the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. In other words, in principle, <b>ROC</b> and <b>PR</b> are equally suited to compare results. But for the example case of a result of 20 hits and 1980 misses they show that the differences can be rather drastic, as shown in Figures 11 and 12. Result/<b>curve</b> (I) describes a result where 10 of the 20 hits are in the top ten ranks and the remaining 10 hits are then evenly ...", "dateLastCrawled": "2022-01-27T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>YOLO</b>, YOLOv2, and YOLOv3: All You want to know | by Amro Kamal | Medium", "url": "https://amrokamal-47691.medium.com/yolo-yolov2-and-yolov3-all-you-want-to-know-7e3e92dc4899", "isFamilyFriendly": true, "displayUrl": "https://amrokamal-47691.medium.com/<b>yolo</b>-<b>yolo</b>v2-and-<b>yolo</b>v3-all-you-want-to-know-7e3e92...", "snippet": "A brief definition for the Average Precision is the <b>area</b> <b>under</b> the precision-recall <b>curve</b>. AP combines both precision and recall together. It takes a value between 0 and 1 (higher is better). To get AP =1 we need both the precision and recall to be equal to 1. The mAP is the mean of the AP calculated for all the classes. <b>YOLO</b>! What a name? Many object detection systems need to go through the image more than one time to be able to detect all the objects in the image, or it has to go through ...", "dateLastCrawled": "2022-02-03T10:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Assessment of protein disorder region <b>predictions</b> in CASP10 ...", "url": "https://onlinelibrary.wiley.com/doi/10.1002/prot.24391", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1002/prot.24391", "snippet": "The <b>area</b> <b>under</b> the <b>curve</b> (AUC, or AUC_ROC) is used as an aggregate measure of the overall quality of a prediction method. A value of 1 corresponds to a perfect classifier, while 0.5 indicates a random prediction. Note that the ROC <b>curve</b> analysis works best for the probability estimates that are evenly distributed throughout the range of the allowed values. The \u201cgranularity\u201d of the probability scores can affect smoothness of the ROC curves and, subsequently, the accuracy of the", "dateLastCrawled": "2022-01-07T15:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Observations on the calculations of <b>COCO</b> metrics \u00b7 Issue #56 ... - <b>GitHub</b>", "url": "https://github.com/cocodataset/cocoapi/issues/56", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>coco</b>dataset/<b>coco</b>api/issues/56", "snippet": "You have 1.0 <b>area</b> <b>under</b> the <b>curve</b> when you plot precision and recall <b>curve</b> for these two points. AP is a <b>metric</b> that averages precision over recall. In practice, your system will need to work on certain precision and recall point on your <b>PR</b> <b>curve</b> and that determines the score threshold to show detections. For the case 1, you can find a score threshold that only shows the true positive detection and ignore the false alarm. In that sense, you can have a perfect detection system for the ...", "dateLastCrawled": "2022-01-25T04:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Area</b> <b>under</b> the ROC <b>curve \u2013 assessing discrimination in logistic</b> ...", "url": "https://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://thestatsgeek.com/2014/05/05/<b>area</b>-<b>under</b>-the-roc-<b>curve</b>-assessing-discrimination...", "snippet": "<b>Area</b> <b>under</b> the ROC <b>curve</b> A popular way of summarizing the discrimination ability of a model is to report the <b>area</b> <b>under</b> the ROC <b>curve</b>. We have seen that a model with discrimination ability has an ROC <b>curve</b> which goes closer to the top left hand corner of the plot, whereas a model with no discrimination ability has an ROC <b>curve</b> close to a 45 degree line. Thus the <b>area</b> <b>under</b> the <b>curve</b> ranges from 1, corresponding to perfect discrimination, to 0.5, corresponding to a model with no ...", "dateLastCrawled": "2022-01-29T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "F1 <b>Score</b> vs ROC AUC vs Accuracy vs <b>PR</b> AUC: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-roc-auc-<b>pr</b>-auc", "snippet": "In order to get one number that tells us how good our <b>curve</b> is, we <b>can</b> calculate the <b>Area</b> <b>Under</b> the ROC <b>Curve</b>, or ROC AUC <b>score</b>. The more top-left your <b>curve</b> is the higher the <b>area</b> and hence higher ROC AUC <b>score</b>. Alternatively, it <b>can</b> be shown that ROC AUC <b>score</b> is equivalent to calculating the rank correlation between <b>predictions</b> and targets. From an interpretation standpoint, it is more useful because it tells us that this metric shows how good at ranking <b>predictions</b> your model is. It ...", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "python - <b>Generating confidence interval for precision recall</b> <b>curve</b> ...", "url": "https://stackoverflow.com/questions/57482356/generating-confidence-interval-for-precision-recall-curve", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/57482356", "snippet": "I have <b>predictions</b> from a trained model and I <b>can</b> generate a precision-recall <b>curve</b> for the data pretty easily, and thus, also the <b>area</b> <b>under</b> the precision-recall <b>curve</b> (AUPRC). However, I&#39;m trying to also generate a 95% <b>confidence</b> interval for the data, which I&#39;m having a hard time finding something for. I&#39;ve looked in sklearn for python and pROC package for R (which does have some usage for <b>PR</b>, just not AUPRC), but I&#39;m not finding anything outside of some pretty high level academic papers ...", "dateLastCrawled": "2022-01-19T10:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Better mAP for Object Detection | by Ivan Rala\u0161i\u0107 | Towards Data Science", "url": "https://towardsdatascience.com/a-better-map-for-object-detection-32662767d424", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-better-map-for-object-detection-32662767d424", "snippet": "The precision-recall (<b>PR</b>) <b>curve</b> is a plot of precision as a function of recall. It shows the trade-off between the two metrics for varying <b>confidence</b> values for the model detections. AP@\u03b1 is the <b>Area</b> <b>Under</b> the precision-recall <b>curve</b> (AUC-<b>PR</b>). Mathematically, AP is defined as:", "dateLastCrawled": "2022-02-01T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Section 8 (Week 8)", "url": "http://cs230.stanford.edu/section/8/", "isFamilyFriendly": true, "displayUrl": "cs230.stanford.edu/section/8", "snippet": "Average Precision (AP): the <b>Area</b> <b>Under</b> <b>Curve</b> (AUC) Object detectors create multiple <b>predictions</b>: each image <b>can</b> have multiple predicted objects, and there are many images to run inference on. Each predicted object has a <b>confidence</b> assigned with it: this is how confident the detector is in its prediction. We <b>can</b> choose different <b>confidence</b> thresholds to use, to decide which <b>predictions</b> to accept from the detector. For instance, if we set the threshold to 0.7, then any <b>predictions</b> with ...", "dateLastCrawled": "2022-02-01T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>Area Under</b> an ROC <b>Curve</b>", "url": "http://gim.unmc.edu/dxtests/RoC3.htm", "isFamilyFriendly": true, "displayUrl": "gim.unmc.edu/dxtests/RoC3.htm", "snippet": "The <b>area under</b> the <b>curve</b> is the percentage of randomly drawn pairs for which this is true (that is, the test correctly classifies the two patients in the random pair). Computing the <b>area</b> is more difficult to explain and beyond the scope of this introductory material. Two methods are commonly used: a non-parametric method based on constructing trapeziods <b>under</b> the <b>curve</b> as an approximation of <b>area</b> and a parametric method using a maximum likelihood estimator to fit a smooth <b>curve</b> to the data ...", "dateLastCrawled": "2022-01-28T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>ROC Curve</b>, a Complete Introduction | by Reza Bagheri | Towards Data Science", "url": "https://towardsdatascience.com/roc-curve-a-complete-introduction-2f2da2e0434c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>roc-curve</b>-a-complete-introduction-2f2da2e0434c", "snippet": "To do that we calculate the <b>area</b> <b>under</b> the <b>ROC curve</b> as shown in Fig 22. Figure 22. We call this quantity AUC (<b>Area</b> <b>under</b> the <b>Curve</b>). For an ideal classifier, AUC is the <b>area</b> of a rectangle with length 1, so it is just 1. For a random classifier, it is roughly the <b>area</b> of the lower triangle which is 0.5. For other classifiers, AUC lies between 0.5 and 1. The higher the AUC, the better the classifier is, since it is closer to an ideal classifier. To calculate the AUC in Scikit-learn, you <b>can</b> ...", "dateLastCrawled": "2022-01-30T19:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "scPred: accurate supervised method for cell-type classification from ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6907144/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6907144", "snippet": "Given the imbalance in the proportions of colorectal cancer, the high <b>area</b> <b>under</b> the precision-recall <b>curve</b> and small <b>confidence</b> intervals indicate that scPred is robust to class imbalance in the training data. The high specificity of the model <b>under</b> this scenario implies that a single-cell prediction method would be able to accurately diagnose disease status using scRNA-seq data from a limited number of cells. For example, here, the mean sensitivity for tumor cells is 0.761 and the ...", "dateLastCrawled": "2022-01-19T03:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "machine learning - Advantages of AUC vs standard <b>accuracy</b> - Data ...", "url": "https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/806", "snippet": "Really great question, and one that I find that most people don&#39;t really understand on an intuitive level. AUC is in fact often preferred over <b>accuracy</b> for binary classification for a number of different reasons. First though, let&#39;s talk about exactly what AUC is. Honestly, for being one of the most widely used efficacy metrics, it&#39;s surprisingly obtuse to figure out exactly how AUC works.. AUC stands for <b>Area</b> <b>Under</b> the <b>Curve</b>, which <b>curve</b> you ask?Well, that would be the ROC <b>curve</b>.ROC stands ...", "dateLastCrawled": "2022-01-27T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "scPred : accurate supervised method for <b>cell</b>-type ... - Genome Biology", "url": "https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1862-5", "isFamilyFriendly": true, "displayUrl": "https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1862-5", "snippet": "Given the imbalance in the proportions of colorectal cancer, the high <b>area</b> <b>under</b> the precision-recall <b>curve</b> and small <b>confidence</b> intervals indicate that scPred is robust to class imbalance in the training data. The high specificity of the model <b>under</b> this scenario implies that a <b>single-cell</b> prediction method would be able to accurately diagnose disease status using scRNA-seq data from a limited number of cells. For example, here, the mean sensitivity for tumor cells is 0.761 and the ...", "dateLastCrawled": "2022-01-30T00:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "F1 <b>Score</b> vs ROC AUC vs Accuracy vs <b>PR</b> AUC: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-roc-auc-<b>pr</b>-auc", "snippet": "In order to get one number that tells us how good our <b>curve</b> is, we <b>can</b> calculate the <b>Area</b> <b>Under</b> the ROC <b>Curve</b>, or ROC AUC <b>score</b>. The more top-left your <b>curve</b> is the higher the <b>area</b> and hence higher ROC AUC <b>score</b>. Alternatively, it <b>can</b> be shown that ROC AUC <b>score</b> is equivalent to calculating the rank correlation between <b>predictions</b> and targets. From an interpretation standpoint, it is more useful because it tells us that this metric shows how good at ranking <b>predictions</b> your model is. It ...", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "Each plot <b>can</b> also be summarized with an <b>area</b> <b>under</b> the <b>curve</b> score that <b>can</b> be used to directly compare classification models. In this tutorial, you will discover ROC Curves and <b>Precision-Recall Curves for imbalanced classification</b>. After completing this tutorial, you will know: ROC Curves and Precision-Recall Curves provide a diagnostic tool for binary classification models. ROC AUC and Precision-Recall AUC provide scores that summarize the curves and <b>can</b> be used to compare classifiers ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Gaining an intuitive understanding of <b>Precision, Recall</b> and <b>Area</b> <b>Under</b> ...", "url": "https://towardsdatascience.com/gaining-an-intuitive-understanding-of-precision-and-recall-3b9df37804a7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/gaining-an-intuitive-<b>under</b>standing-of-precision-and...", "snippet": "<b>The PR</b>-AUC hence summarizes the <b>precision-recall</b> <b>curve</b> as a single score and <b>can</b> be used to easily compare different binary neural networks models. Please note that the value of <b>the PR</b>-AUC for a perfect classifier amounts to 1.0. The value of <b>PR</b>-AUC for a random classifier is equal to the ratio of positive samples in a dataset w.r.t. all samples.", "dateLastCrawled": "2022-01-31T16:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Google Cloud Platform - Vertex AI - is there a way to look at a chart ...", "url": "https://stackoverflow.com/questions/70175440/google-cloud-platform-vertex-ai-is-there-a-way-to-look-at-a-chart-of-trainin", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/70175440/google-cloud-platform-vertex-ai-is-there...", "snippet": "AuPRC: The <b>area</b> <b>under</b> the precision-recall (<b>PR</b>) <b>curve</b>, also referred to as average precision. This value ranges from zero to one, where a higher value indicates a higher-quality model. Log loss: The cross-entropy between the model <b>predictions</b> and the target values. This ranges from zero to infinity, where a lower value indicates a higher-quality model. <b>Confidence</b> threshold: A <b>confidence</b> score that determines which <b>predictions</b> to return. A model returns <b>predictions</b> that are at this value or ...", "dateLastCrawled": "2022-01-07T09:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "machine learning - <b>ROC</b> vs precision-and-recall curves - Cross Validated", "url": "https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/7207", "snippet": "Remarkably, when constructing the achievable <b>PR</b> <b>curve</b> one discards exactly the same points omit- ted by the convex hull in <b>ROC</b> space. Consequently, we <b>can</b> efficiently compute the achievable <b>PR</b> <b>curve</b>. [...] Finally, we show that an algorithm that optimizes the <b>area</b> <b>under</b> the <b>ROC</b> <b>curve</b> is not guaranteed to optimize the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>.", "dateLastCrawled": "2022-01-27T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Better mAP for Object Detection | by Ivan Rala\u0161i\u0107 | Towards Data Science", "url": "https://towardsdatascience.com/a-better-map-for-object-detection-32662767d424", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-better-map-for-object-detection-32662767d424", "snippet": "AP@\u03b1 is the <b>Area</b> <b>Under</b> the precision-recall <b>curve</b> (AUC-<b>PR</b>). Mathematically, AP is defined as: Notation: AP@\u03b1 means Average Precision(AP) at the IoU threshold of \u03b1. Therefore AP@0.50 and AP@0.75 mean AP at IoU threshold of 50% and 75% respectively. A high AUC-<b>PR</b> implies high precision and high recall. Naturally, <b>the PR</b> <b>curve</b> has a zig-zag behavior (not monotonically decreasing). Before calculating the AP, we make <b>the PR</b> <b>curve</b> to be monotonically decreasing using the following interpolation ...", "dateLastCrawled": "2022-02-01T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Classification: <b>ROC</b> <b>Curve</b> and AUC | Machine Learning Crash Course ...", "url": "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/classification/<b>roc</b>-and-auc", "snippet": "AUC: <b>Area</b> <b>Under</b> the <b>ROC</b> <b>Curve</b>. AUC stands for &quot;<b>Area</b> <b>under</b> the <b>ROC</b> <b>Curve</b>.&quot; That is, AUC measures the entire two-dimensional <b>area</b> underneath the entire <b>ROC</b> <b>curve</b> (think integral calculus) from (0,0) to (1,1). Figure 5. AUC (<b>Area</b> <b>under</b> the <b>ROC</b> <b>Curve</b>). AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. For ...", "dateLastCrawled": "2022-02-02T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Classification metrics</b> \u00b7 GitBook", "url": "https://apple.github.io/turicreate/docs/userguide/evaluation/classification.html", "isFamilyFriendly": true, "displayUrl": "https://apple.github.io/turicreate/docs/userguide/evaluation/classification.html", "snippet": "The ROC <b>curve</b> <b>can</b> also be defined in the multi-class setting by returning a single <b>curve</b> for each class. <b>Area</b> <b>under</b> the <b>curve</b> (AUC) AUC stands for <b>Area</b> <b>Under</b> the <b>Curve</b>. Here, the <b>curve</b> is the ROC <b>curve</b>. As mentioned above, a good ROC <b>curve</b> has a lot of space <b>under</b> it (because the true positive rate shoots up to 100% very quickly). A bad ROC ...", "dateLastCrawled": "2022-01-28T00:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding the AUC-ROC <b>Curve</b> in Machine Learning Classification", "url": "https://analyticsindiamag.com/understanding-the-auc-roc-curve-in-machine-learning-classification/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>under</b>standing-the-auc-roc-<b>curve</b>-in-machine-learning...", "snippet": "The <b>area</b> <b>under</b> the <b>curve</b> is one of the good ways to estimate the accuracy of the model. An excellent model poses an AUC near to the 1 which tells that it has a good measure of separability. A poor model will have an AUC near 0 which describes that it has the worst measure of separability. In fact, it means it is reciprocating the result and predicting 0s as 1s and 1s as 0s. When an AUC is 0.5, it means the model has no class separation capacity present whatsoever.", "dateLastCrawled": "2022-02-03T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Area</b> <b>under</b> the ROC <b>curve \u2013 assessing discrimination in logistic</b> ...", "url": "https://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://thestatsgeek.com/2014/05/05/<b>area</b>-<b>under</b>-the-roc-<b>curve</b>-assessing-discrimination...", "snippet": "Thus the <b>area</b> <b>under</b> the <b>curve</b> ranges from 1, corresponding to perfect discrimination, to 0.5, corresponding to a model with no discrimination ability. The <b>area</b> <b>under</b> the ROC <b>curve</b> is also sometimes referred to as the c-statistic (c for concordance). The <b>area</b> <b>under</b> the estimated ROC <b>curve</b> (AUC) is reported when we plot the ROC <b>curve</b> in R&#39;s Console.", "dateLastCrawled": "2022-01-29T07:23:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. See <b>PR</b> AUC (<b>Area</b> <b>under</b> <b>the PR</b> <b>Curve</b>). <b>area</b> <b>under</b> the ROC <b>curve</b> . See AUC (<b>Area</b> <b>under</b> the ROC <b>curve</b>). artificial general intelligence. A non-human mechanism that demonstrates a broad range of problem solving, creativity, and adaptability. For example, a program demonstrating artificial general intelligence could translate text, compose symphonies, and excel at games that have not yet been invented. artificial intelligence. A non-human program or model that can solve ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Learning</b> Curves in <b>Machine</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/247934703_Learning_Curves_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/247934703_<b>Learning</b>_<b>Curves</b>_in_<b>Machine</b>_<b>Learning</b>", "snippet": "The <b>area</b> <b>under</b> the receiver operating characteristic (ROC) <b>curve</b> (AUC) was 0.62 (95% confidence interval [CI]: 0.57, 0.68) and the <b>area</b> <b>under</b> the precision\u2010recall <b>curve</b> was 0.58. <b>Learning</b> <b>curve</b> ...", "dateLastCrawled": "2021-12-15T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding <b>AUC</b> - ROC <b>Curve</b> | by Sarang Narkhede | Towards Data Science", "url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>under</b>standing-<b>auc</b>-roc-<b>curve</b>-68b2303cc9c5", "snippet": "In <b>Machine</b> <b>Learning</b>, performance measurement is an essential task. So when it comes to a classification problem, we can count on an <b>AUC</b> - ROC <b>Curve</b>. When we need to check or visualize the performance of the multi-class classification problem, we use the <b>AUC</b> <b>Area</b> <b>Under</b> The <b>Curve</b>) ROC (Receiver Operating Characteristics) <b>curve</b>. It is one of the most important evaluation metrics for checking any classification model\u2019s performance. It is also written as AUROC (<b>Area</b> <b>Under</b> the Receiver Operating ...", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Differential and Integral Calculus - Differentiate with Respect to Anything", "url": "https://machinelearningmastery.com/differential-and-integral-calculus-differentiate-with-respect-to-anything/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/differential-and-integral-calculus-differentiate...", "snippet": "The Sweeping <b>Area</b> <b>Analogy</b>. Perhaps a simpler <b>analogy</b> to help us relate integration to differentiation, is to imagine holding one of the thinly cut slices and dragging it rightwards <b>under</b> the <b>curve</b> in infinitesimally small steps. As it moves rightwards, the thinly cut slice will sweep a larger <b>area</b> <b>under</b> the <b>curve</b>, while its height will change according to the shape of the <b>curve</b>. The question that we would like to answer is, at which rate does the <b>area</b> accumulate as the thin slice sweeps ...", "dateLastCrawled": "2022-01-28T21:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Applying <b>machine</b> <b>learning</b> algorithms to predict default probability in ...", "url": "https://www.sciencedirect.com/science/article/pii/S1057521921002878", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1057521921002878", "snippet": "The results show that, first, based on the AUC (<b>area</b> <b>under</b> the ROC <b>curve</b>) value, accuracy rate and Brier score, the <b>machine</b> <b>learning</b> models can accurately predict the default risk of online borrowers. Second, the integrated discrimination improvement (IDI) test results show that the prediction performance of the <b>machine</b> <b>learning</b> algorithms is significantly better than that of the logistic model. Third, after constructing the investor profit function with misclassification cost, we find that ...", "dateLastCrawled": "2022-01-27T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>What is</b> AUC - <b>ROC</b> in <b>Machine</b> <b>Learning</b> | Overview of <b>ROC</b>", "url": "https://www.mygreatlearning.com/blog/roc-curve/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/blog/<b>roc</b>-<b>curve</b>", "snippet": "By <b>analogy</b>, Higher the AUC, better the model is at distinguishing between patients with the disease and no disease. The <b>ROC</b> <b>curve</b> is plotted with TPR against the FPR where TPR is on the y-axis and FPR is on the x-axis. Defining terms used in AUC and <b>ROC</b> <b>Curve</b>. Consider a two-class prediction problem, in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is ...", "dateLastCrawled": "2022-01-30T19:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Cohort-Derived <b>Machine Learning</b> Models for Individual Prediction of ...", "url": "https://academic.oup.com/jid/article/224/7/1198/5835004", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jid/article/224/7/1198/5835004", "snippet": "Of 12 761 eligible individuals (median baseline eGFR, 103 mL/minute/1.73 m 2), 1192 (9%) developed a CKD after a median of 8 years.We used 64 static and 502 time-changing variables: Across prediction horizons and algorithms and in contrast to expert-based standard models, most <b>machine learning</b> models achieved state-of-the-art predictive performances with areas <b>under</b> the receiver operating characteristic <b>curve</b> and precision recall <b>curve</b> ranging from 0.926 to 0.996 and from 0.631 to 0.956 ...", "dateLastCrawled": "2021-12-15T15:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Driving skill analysis using <b>machine</b> <b>learning</b> The full <b>curve</b> and ...", "url": "https://www.researchgate.net/publication/261398439_Driving_skill_analysis_using_machine_learning_The_full_curve_and_curve_segmented_cases", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261398439_Driving_skill_analysis_using...", "snippet": "In the full <b>curve</b> driving scene, principal component analysis and a support vector <b>machine</b>-based method accurately classified drivers in 95.7 % of cases when using driving data about high- and low ...", "dateLastCrawled": "2022-01-07T09:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Bishop Pattern Recognition And Machine Learning Springer</b> | Xinyue ...", "url": "https://www.academia.edu/34528598/Bishop_Pattern_Recognition_And_Machine_Learning_Springer", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/34528598/<b>Bishop_Pattern_Recognition_And_Machine_Learning_Springer</b>", "snippet": "<b>Bishop Pattern Recognition And Machine Learning Springer</b>. 758 Pages. <b>Bishop Pattern Recognition And Machine Learning Springer</b>. Xinyue Liu. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 36 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package ...", "dateLastCrawled": "2022-02-02T07:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - What is the <b>convex hull</b> in ROC <b>curve</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/120361/what-is-the-convex-hull-in-roc-curve", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/120361/what-is-the-<b>convex-hull</b>-in-roc-<b>curve</b>", "snippet": "Taking the <b>convex hull</b> of the ROC <b>curve</b> points is just a way of enforcing a constraint that the estimated ROC <b>curve</b> be <b>convex</b> (concave down in this case). It is equivalent to assuming that the distributions of the marker in the cases and in the controls are unimodal. In situations where this assumption is reasonable then imposing the convexity constraint is warranted.", "dateLastCrawled": "2022-01-18T16:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Performance Evaluation of Machine Learning Algorithms in Apache</b> ...", "url": "https://www.researchgate.net/publication/330478085_Performance_Evaluation_of_Machine_Learning_Algorithms_in_Apache_Spark_for_Intrusion_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/330478085_Performance_Evaluation_of_<b>Machine</b>...", "snippet": "The <b>area under the PR curve is like</b> the ROC. The . difference is that instead of it being a ratio bet ween the true and . false positive rates, it is a r atio between precision and t rue ...", "dateLastCrawled": "2021-11-04T12:46:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(area under the pr curve)  is like +(confidence in predictions)", "+(area under the pr curve) is similar to +(confidence in predictions)", "+(area under the pr curve) can be thought of as +(confidence in predictions)", "+(area under the pr curve) can be compared to +(confidence in predictions)", "machine learning +(area under the pr curve AND analogy)", "machine learning +(\"area under the pr curve is like\")", "machine learning +(\"area under the pr curve is similar\")", "machine learning +(\"just as area under the pr curve\")", "machine learning +(\"area under the pr curve can be thought of as\")", "machine learning +(\"area under the pr curve can be compared to\")"]}