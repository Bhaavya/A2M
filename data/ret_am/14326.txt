{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> fundamentals I: An <b>analogy</b> | Finn Rietz.dev", "url": "http://www.finnrietz.dev/machine%20learning/part-1-analogy/", "isFamilyFriendly": true, "displayUrl": "www.finnrietz.dev/<b>machine</b> <b>learning</b>/part-1-<b>analogy</b>", "snippet": "And this is what the <b>loss</b> function does, so the <b>loss</b> function for a <b>Machine</b> <b>learning</b> algorithm <b>is like</b> the teacher for the real-world dermatologist in-training. In mathematical terms, the <b>loss</b> function could look something <b>like</b> this: \\(L = (y_i - \\hat{y_i})^2\\), where \\(y_i\\) is the actual output value (the one that the teacher has written down) and \\(\\hat{y_i}\\) is the one our <b>learning</b> algorithm produced.", "dateLastCrawled": "2022-01-16T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bias-Variance Decomposition</b> - mlxtend", "url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/", "isFamilyFriendly": true, "displayUrl": "rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp", "snippet": "We can decompose a <b>loss</b> function such as the <b>squared</b> <b>loss</b> into three terms, a variance, bias, and a noise term (and the same is true for the decomposition of the 0-1 <b>loss</b> later). However, for simplicity, we will ignore the noise term. Before we introduce the <b>bias-variance decomposition</b> of the 0-1 <b>loss</b> for classification, let us start with the decomposition of the <b>squared</b> <b>loss</b> as an easy warm-up exercise to get familiar with the overall concept. The previous section already listed the common ...", "dateLastCrawled": "2022-01-31T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Choosing The Right <b>Loss</b> Function For Deep <b>Learning</b> - VarsityScape", "url": "https://www.varsityscape.com/choosing-the-right-loss-function-for-deep-learning-96181/", "isFamilyFriendly": true, "displayUrl": "https://www.varsityscape.com/choosing-the-right-<b>loss</b>-function-for-deep-<b>learning</b>-96181", "snippet": "Choosing a marking scheme that effectively punishes or rewards a student is very important. This is very similar to choosing the right <b>loss</b> function in a deep <b>learning</b> model. Everything in the above <b>analogy</b> still stands, but in supervised <b>learning</b>, we (teachers) can control the way and methods the student (<b>machine</b>) learns and we (teachers ...", "dateLastCrawled": "2022-01-21T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Mean squared error</b> in <b>machine</b> <b>learning</b> | by Aaron Li | MLearning.ai ...", "url": "https://medium.com/mlearning-ai/gradient-descent-4fda4e3fbdc0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/gradient-descent-4fda4e3fbdc0", "snippet": "For the ones <b>learning</b> <b>machine</b> <b>learning</b> in a bottom up approach, I suggest you try to train some models in Google Co-lab and get an idea of how <b>machine</b> <b>learning</b> works.", "dateLastCrawled": "2022-01-30T03:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Scikit Learn - Stochastic Gradient Descent</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_stochastic_gradient_descent</b>.htm", "snippet": "<b>squared</b>_<b>loss</b> \u2212 It refers to the ordinary least squares fit. huber: SGDRegressor \u2212 correct the outliers by switching from <b>squared</b> to linear <b>loss</b> past a distance of epsilon. The work of \u2018huber\u2019 is to modify \u2018<b>squared</b>_<b>loss</b>\u2019 so that algorithm focus less on correcting outliers.", "dateLastCrawled": "2022-02-03T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "Optimization methods are applied to minimize the <b>loss</b> function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one <b>loss</b> is L0-1 = 1 (m &lt;= 0); in zero-one <b>loss</b>, value of <b>loss</b> is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this <b>loss</b> is it is not differentiable, non-convex, and also NP-hard. Hence, in order to make optimization feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Evaluating a <b>Machine</b> <b>Learning</b> Model: Regression and Classification ...", "url": "https://arnavbansal-8232.medium.com/evaluating-a-machine-learning-model-regression-and-classification-metrics-4f2316e180b4", "isFamilyFriendly": true, "displayUrl": "https://arnavbansal-8232.medium.com/evaluating-a-<b>machine</b>-<b>learning</b>-model-regression-and...", "snippet": "<b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> plays a vital role in all this. The most important part of anything which is done in the field of <b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> is its application. Applications are driven by performance and performance is achieved by better and improved results. Now, there are many automated tools, libraries and scripts which allows you to directly run a ML model without knowing anything about it. This results in fast development, and especially beginners are ...", "dateLastCrawled": "2022-01-05T00:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Linear Regression | Joseph Bakarji", "url": "https://www.josephbakarji.com/science/introduction-to-machine-learning-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.josephbakarji.com/science/introduction-to-<b>machine</b>-<b>learning</b>-linear-regression", "snippet": "This is a good <b>analogy</b> to think about even for more complex <b>machine</b> <b>learning</b> models <b>like</b> deep networks. Before we test our model on the test set, is there something else we can try? Can we do a better job to recover the quadratic model we used to generate the data? Of course! This is where regularization comes in. Linear predictors are great but they\u2019d be even greater if there is a way we can get them to minimize over-fitting and generalize over the validation set without trying all ...", "dateLastCrawled": "2022-01-24T00:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Linear Regression Using Gradient Descent Python", "url": "https://pythonocean.com/blogs/linear-regression-using-gradient-descent-python", "isFamilyFriendly": true, "displayUrl": "https://pythonocean.com/blogs/linear-regression-using-gradient-descent-python", "snippet": "<b>Machine</b> <b>Learning</b> and Data Science. Aug. 26, 2021, 10:22 a.m. Simple Linear regression. Simple Linear regression is one of the simplest and is going to be first AI algorithm which you will learn in this blog. In statistics, a simple linear regression is a linear regression model with a single defining variable. That is, it relates to two-dimensional sample points with an independent variable and a dependent variable (traditionally, x and y coordinates in the Cartesian coordinate system) and ...", "dateLastCrawled": "2022-02-02T04:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias-Variance Decomposition</b> - mlxtend", "url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/", "isFamilyFriendly": true, "displayUrl": "rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp", "snippet": "We can decompose a <b>loss</b> function such as the <b>squared</b> <b>loss</b> into three terms, a variance, bias, and a noise term (and the same is true for the decomposition of the 0-1 <b>loss</b> later). However, for simplicity, we will ignore the noise term. Before we introduce the <b>bias-variance decomposition</b> of the 0-1 <b>loss</b> for classification, let us start with the decomposition of the <b>squared</b> <b>loss</b> as an easy warm-up exercise to get familiar with the overall concept. The previous section already listed the common ...", "dateLastCrawled": "2022-01-31T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction <b>to Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-<b>to-machine</b>-<b>learning</b>-3", "snippet": "However, in <b>machine</b> <b>learning</b> methodology, <b>squared</b> <b>loss</b> will be minimized with respect to ... If one of the supports doesn\u2019t exist, the beam will eventually fall down by moving out of balance. A <b>similar</b> <b>analogy</b> is applied for comparing statistical modeling and <b>machine</b> <b>learning</b> methodologies here. The two-point validation is performed on the statistical modeling methodology on training data using overall model accuracy and individual parameters significance test. Due to the fact that either ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machined Learnings: ML and OR: An <b>analogy</b> with cost-sensitive ...", "url": "http://www.machinedlearnings.com/2010/07/ml-and-or.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2010/07/ml-and-or.html", "snippet": "Nonetheless I&#39;ve been amusing myself by thinking about it, in particular trying to think about it from a <b>machine</b> <b>learning</b> reduction standpoint. The simplest well-understood reduction that I can think of which is analogous to supplying estimates to a linear program is the reduction of cost-sensitive multiclass classification (CSMC) to regression.", "dateLastCrawled": "2021-12-25T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Choosing The Right <b>Loss</b> Function For Deep <b>Learning</b> - VarsityScape", "url": "https://www.varsityscape.com/choosing-the-right-loss-function-for-deep-learning-96181/", "isFamilyFriendly": true, "displayUrl": "https://www.varsityscape.com/choosing-the-right-<b>loss</b>-function-for-deep-<b>learning</b>-96181", "snippet": "Choosing a marking scheme that effectively punishes or rewards a student is very important. This is very <b>similar</b> to choosing the right <b>loss</b> function in a deep <b>learning</b> model. Everything in the above <b>analogy</b> still stands, but in supervised <b>learning</b>, we (teachers) can control the way and methods the student (<b>machine</b>) learns and we (teachers ...", "dateLastCrawled": "2022-01-21T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Scikit Learn - Stochastic Gradient Descent</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_stochastic_gradient_descent</b>.htm", "snippet": "<b>squared</b>_hinge \u2212 <b>similar</b> to \u2018hinge\u2019 <b>loss</b> but it is quadratically penalized. perceptron \u2212 as the name suggests, it is a linear <b>loss</b> which is used by the perceptron algorithm. 2: penalty \u2212 str, \u2018none\u2019, \u2018l2\u2019, \u2018l1\u2019, \u2018elasticnet\u2019 It is the regularization term used in the model. By default, it is L2. We can use L1 or \u2018elasticnet; as well but both might bring sparsity to the model, hence not achievable with L2. 3: alpha \u2212 float, default = 0.0001. Alpha, the constant ...", "dateLastCrawled": "2022-02-03T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "Unsupervised <b>learning</b>: <b>Similar</b> to the teacher-student <b>analogy</b>, in which the instructor does not present and provide feedback to the student and who needs to prepare on his/her own. Unsupervised <b>learning</b> does not have as many are in supervised <b>learning</b>: Principal component analysis (PCA) K-means clustering ; Reinforcement <b>learning</b>: This is the scenario in which multiple decisions need to be taken by an agent prior to reaching the target and it provides a reward, either +1 or -1, rather than ...", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "UVA CS 6316: <b>Machine</b> <b>Learning</b> Lecture 3: Linear Regression Basics", "url": "https://qiyanjun.github.io/2019f-UVA-CS6316-MachineLearning/Lectures/L03-lr.pdf", "isFamilyFriendly": true, "displayUrl": "https://qiyanjun.github.io/2019f-UVA-CS6316-<b>MachineLearning</b>/Lectures/L03-lr.pdf", "snippet": "<b>Machine</b> <b>Learning</b> Lecture 3: Linear Regression Basics Dr. YanjunQi University of Virginia Department of Computer Science 9/18/19 Dr. Yanjun Qi / UVA CS 1. Course Content Plan \u00e8 Six major sections of this course qRegression (supervised) qClassification (supervised) qUnsupervised models qLearning theory qGraphical models qReinforcement <b>Learning</b> 9/18/19 Yanjun Qi / UVA CS 2 Y is a continuous Y is a discrete NO Y About f() About interactions among X1,\u2026Xp Learn program to Interact with its ...", "dateLastCrawled": "2021-09-16T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "Newton\u2019s method is seldom used in <b>machine</b> <b>learning</b> because a. common <b>loss</b> functions are not self-concordant b. Newton\u2019s method does not work well on noisy data c. <b>machine</b> <b>learning</b> researchers don\u2019t really understand linear algebra d. it is generally not practical to form or store the Hessian in such problems, due to large problem size answer : d. <b>Learning</b> with Regression and trees. Module 04. 1. In practice, Line of best fit or regression line is found when _____ a) Sum of residuals ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, the ... the DNN operations) <b>can</b> then <b>be thought</b> of more or less as just a medium for holding and <b>learning</b> that information. Common <b>Loss</b> Functions. <b>Loss</b> functions generally originate from different mathematical areas like statistical analysis, information theory etc., and thus employ a variety of equations for calculating <b>loss</b> in different ways. It shouldn\u2019t come as any surprise then, that each <b>loss</b> ...", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "1 <b>Online Linear Regression</b> - cs.princeton.edu", "url": "https://www.cs.princeton.edu/courses/archive/spring18/cos511/scribe_notes/0411.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.princeton.edu/courses/archive/spring18/cos511/scribe_notes/0411.pdf", "snippet": "In particular, we chose the square <b>loss</b> function and the Euclidean distance <b>squared</b>. How-ever, there are many di erent notions of <b>loss</b> and distance that <b>can</b> be used in the above measure. For example, for an arbitrary <b>loss</b> function L(w;x;y) and the Euclidean norm <b>squared</b> as a measure of distance, we get the following update rule: w t+1 = w t r ...", "dateLastCrawled": "2022-01-19T21:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Overfitting</b> vs Underfitting: The Guiding Philosophy of <b>Machine</b> <b>Learning</b> ...", "url": "https://medium.com/analytics-vidhya/overfitting-vs-underfitting-the-guiding-philosophy-of-machine-learning-c6ce5fe9f9bb", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>overfitting</b>-vs-underfitting-the-guiding-philosophy...", "snippet": "A model <b>can</b> also <b>be thought</b> of as a mathematical function that maps a set of inputs to an output. This set of inputs and the output are different \u2018aspects\u2019 of our model and through <b>machine</b> ...", "dateLastCrawled": "2021-11-10T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Everything I Know About Machine Learning I Learned from</b> Making Soup", "url": "https://bgstieber.github.io/post/everything-i-know-about-machine-learning-i-learned-from-making-soup/", "isFamilyFriendly": true, "displayUrl": "https://bgstieber.github.io/post/<b>everything-i-know-about-machine-learning-i-learned</b>...", "snippet": "Introduction In this post, I\u2019m going to make the claim that we <b>can</b> simplify some parts of the <b>machine</b> <b>learning</b> process by using the <b>analogy</b> of making soup. I think this <b>analogy</b> <b>can</b> improve how a data scientist explains <b>machine</b> <b>learning</b> to a broad audience, and it provides a helpful framework throughout the model building process. Relying on some insight from the CRISP-DM framework, my own experience as an amateur chef, and the well-known iris data set, I\u2019m going to explain why I think ...", "dateLastCrawled": "2022-01-21T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Choosing and Customizing Loss Functions for Image Processing</b>", "url": "https://blog.perceptilabs.com/choosing-and-customizing-loss-functions-for-image-processing/", "isFamilyFriendly": true, "displayUrl": "https://blog.perceptilabs.com/<b>choosing-and-customizing-loss-functions-for-image-processing</b>", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, the ... the DNN operations) <b>can</b> then <b>be thought</b> of more or less as just a medium for holding and <b>learning</b> that information. Common <b>Loss</b> Functions. <b>Loss</b> functions generally originate from different mathematical areas like statistical analysis, information theory etc., and thus employ a variety of equations for calculating <b>loss</b> in different ways. It shouldn&#39;t come as any surprise then, that each <b>loss</b> ...", "dateLastCrawled": "2022-01-28T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "9.1. Lecture 22 \u2014 <b>Learning</b> from data", "url": "https://furnstahl.github.io/Physics-8820/content/Machine_learning/lecture_22.html", "isFamilyFriendly": true, "displayUrl": "https://furnstahl.github.io/Physics-8820/content/<b>Machine</b>_<b>learning</b>/lecture_22.html", "snippet": "The basic idea from here is to make use of the statistical mechanics <b>analogy</b> and introduce a potential \\(\\Lambda = \\Lambda(\\lambda ... by Lovato et al. More recently, <b>machine</b> <b>learning</b> methods have been used to provide more robust inversions than maximum entropy. See <b>Machine</b> <b>learning</b>-based inversion of nuclear responses by Raghavan et al. for details and a comparison to the MaxEnt results. Bayesian methods in <b>machine</b> <b>learning</b> (ML): background remarks\u00b6 ML encompasses a broad array of ...", "dateLastCrawled": "2022-02-03T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "Newton\u2019s method is seldom used in <b>machine</b> <b>learning</b> because a. common <b>loss</b> functions are not self-concordant b. Newton\u2019s method does not work well on noisy data c. <b>machine</b> <b>learning</b> researchers don\u2019t really understand linear algebra d. it is generally not practical to form or store the Hessian in such problems, due to large problem size answer : d. <b>Learning</b> with Regression and trees. Module 04. 1. In practice, Line of best fit or regression line is found when _____ a) Sum of residuals ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Seven Life Lessons from <b>Machine</b> <b>Learning</b> | by Changsin Lee | Medium", "url": "https://changsin.medium.com/seven-life-lessons-from-machine-learning-5620891b21df", "isFamilyFriendly": true, "displayUrl": "https://changsin.medium.com/seven-life-lessons-from-<b>machine</b>-<b>learning</b>-5620891b21df", "snippet": "Alright, you do not need to study <b>Machine</b> <b>Learning</b> to realize the importance of time, but doing <b>Machine</b> <b>Learning</b> requires a significant amount of time investment. <b>Learning</b> the subject itself takes a long time. More importantly, in practice, developing AI systems requires collecting and labeling data, selecting and architecting a model, training and testing it, and finally deploying it to the customers. Each of these steps takes a long time and effort. The most time-consuming and expensive ...", "dateLastCrawled": "2022-01-25T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Linear regression: why is <b>distance</b> *<b>squared</b>* used as an error metric ...", "url": "https://ai.stackexchange.com/questions/2742/linear-regression-why-is-distance-squared-used-as-an-error-metric", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/2742/linear-regression-why-is-<b>distance</b>-<b>squared</b>...", "snippet": "Depending on the surface features (terrain), sensing the angle of the foot (determining gradient) <b>can</b> be counterproductive. The search <b>can</b> becomes chaotic. To extend the intuitive <b>analogy</b>, consider searching for the bottom of the stairs in Escher&#39;s Relativity lithograph.", "dateLastCrawled": "2022-01-21T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>can</b> you explain Deep neural networks, <b>Machine</b> <b>learning</b>, Deep ...", "url": "https://www.quora.com/How-can-you-explain-Deep-neural-networks-Machine-learning-Deep-learning-in-laymans-terms", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-you-explain-Deep-neural-networks-<b>Machine</b>-<b>learning</b>-Deep...", "snippet": "Answer (1 of 10): In one line, deep neural networks are artificial neural networks (ANN) with multiple hidden layers of units between the input and output layers. Image Courtesy: Google The main idea of deep unsupervised <b>learning</b>, as we understand it, is feature extraction. One of the most c...", "dateLastCrawled": "2022-01-30T01:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Comparison of <b>machine</b>-<b>learning</b> methodologies for accurate diagnosis of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8128240/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8128240", "snippet": "To assess feasibility and performance, we performed differential expression analysis and applied and <b>compared</b> different well-established <b>machine</b>-<b>learning</b>-based methodologies for the aim of accurate and reliable diagnosis. Material and methods. Dataset retrieval. <b>Machine</b> <b>learning</b> has been demonstrated as most effective when used for analysis of high-dimensional datasets . To create a large-scale microarray collection of sepsis-related samples, we gathered 214 candidate data series from NCBI ...", "dateLastCrawled": "2022-01-26T11:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bias-Variance Decomposition</b> - mlxtend", "url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/", "isFamilyFriendly": true, "displayUrl": "rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp", "snippet": "<b>Bias-Variance Decomposition</b> of the <b>Squared</b> <b>Loss</b>. We <b>can</b> decompose a <b>loss</b> function such as the <b>squared</b> <b>loss</b> into three terms, a variance, bias, and a noise term (and the same is true for the decomposition of the 0-1 <b>loss</b> later). However, for simplicity, we will ignore the noise term. Before we introduce the <b>bias-variance decomposition</b> of the 0-1 <b>loss</b> for classification, let us start with the decomposition of the <b>squared</b> <b>loss</b> as an easy warm-up exercise to get familiar with the overall concept ...", "dateLastCrawled": "2022-01-31T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why are different <b>loss</b> <b>functions used with different machine</b> <b>learning</b> ...", "url": "https://www.quora.com/Why-are-different-loss-functions-used-with-different-machine-learning-algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-are-different-<b>loss</b>-<b>functions-used-with-different-machine</b>...", "snippet": "Answer (1 of 4): The first way that <b>loss</b> functions are usually split are between those that work on classification tasks and those that work on regression. In a classification task, you chose one of N options, and your pick is right or wrong, there is no partial correctness. In this case, the cr...", "dateLastCrawled": "2022-01-17T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Artificial Intelligence and <b>Machine</b> <b>Learning</b> in Anesthesiology", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6778496/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6778496", "snippet": "Although it may seem clinically bizarre to talk about the <b>squared</b> value of the serum potassium (K 2), it is easy to write a quadratic function that has clinical meaning. For example, the function 35 \u2013 17K \u2013 2K 2 is positive if the value of K lies between 3.5 and 5.0, but turns negative for any more hypokalemic or hyperkalemic value outside that range. This simple example underscores the ways in which the outputs from <b>machine</b> <b>learning</b> algorithms <b>can</b> seem inscrutable or black box. To a ...", "dateLastCrawled": "2022-01-22T08:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "XGBoost empirical Hessian of data points for <b>squared</b> <b>loss</b> function", "url": "https://stats.stackexchange.com/questions/329665/xgboost-empirical-hessian-of-data-points-for-squared-loss-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/329665", "snippet": "1) Based on the assumption of a <b>squared</b> <b>loss</b> function, how <b>can</b> I do this? As far as I understand, Gradient Boosting fits the next base learner (regression tree in my case) to the pseudo-residuals. These <b>can</b> be calculated as a result from the most current boosting estimator at step m, so f_{m-1}(x).", "dateLastCrawled": "2022-01-09T01:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow <b>can</b> be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Calculation of Bias &amp; Variance in</b> python | by Nallaperumal | Analytics ...", "url": "https://medium.com/analytics-vidhya/calculation-of-bias-variance-in-python-8f96463c8942", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>calculation-of-bias-variance-in</b>-python-8f96463c8942", "snippet": "For any <b>machine</b> <b>learning</b> the performance of a model <b>can</b> be determined and characterized in terms of Bias and Variance. In supervised <b>machine</b> <b>learning</b> an algorithm learns a model from training data ...", "dateLastCrawled": "2022-01-29T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "Newton\u2019s method is seldom used in <b>machine</b> <b>learning</b> because a. common <b>loss</b> functions are not self-concordant b. Newton\u2019s method does not work well on noisy data c. <b>machine</b> <b>learning</b> researchers don\u2019t really understand linear algebra d. it is generally not practical to form or store the Hessian in such problems, due to large problem size answer : d. <b>Learning</b> with Regression and trees. Module 04. 1. In practice, Line of best fit or regression line is found when _____ a) Sum of residuals ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Evaluating a <b>Linear Regression Model</b> | <b>Machine</b> <b>Learning</b>, Deep <b>Learning</b> ...", "url": "https://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/", "isFamilyFriendly": true, "displayUrl": "https://www.ritchieng.com/<b>machine</b>-<b>learning</b>-evaluate-<b>linear-regression-model</b>", "snippet": "Simple linear regression <b>can</b> easily be extended to include multiple features. This is called multiple linear regression: y = \u03b2 0 + \u03b2 1 x 1 +... + \u03b2 n x n. Each x represents a different feature, and each feature has its own coefficient. In this case: y = \u03b2 0 + \u03b2 1 \u00d7 T V + \u03b2 2 \u00d7 R a d i o + \u03b2 3 \u00d7 N e w s p a p e r.", "dateLastCrawled": "2022-02-02T19:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "variance - Is $\\frac1{n+1}\\sum_{i=1}^n(X_i-\\overline X)^2$ an ...", "url": "https://stats.stackexchange.com/questions/522910/is-frac1n1-sum-i-1nx-i-overline-x2-an-admissible-estimator-for-si", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/522910/is-frac1n1-sum-i-1nx-i-overline-x2-an...", "snippet": "<b>Cross Validated</b> is a question and answer site for people interested in statistics, <b>machine</b> <b>learning</b>, data analysis, data mining, and data visualization. It only takes a minute to sign up. Sign up to join this community", "dateLastCrawled": "2022-01-08T21:58:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> fundamentals I: An <b>analogy</b> | Finn Rietz.dev", "url": "http://www.finnrietz.dev/machine%20learning/part-1-analogy/", "isFamilyFriendly": true, "displayUrl": "www.finnrietz.dev/<b>machine</b> <b>learning</b>/part-1-<b>analogy</b>", "snippet": "And this is what the <b>loss</b> function does, so the <b>loss</b> function for a <b>Machine</b> <b>learning</b> algorithm is like the teacher for the real-world dermatologist in-training. In mathematical terms, the <b>loss</b> function could look something like this: \\(L = (y_i - \\hat{y_i})^2\\), where \\(y_i\\) is the actual output value (the one that the teacher has written down) and \\(\\hat{y_i}\\) is the one our <b>learning</b> algorithm produced.", "dateLastCrawled": "2022-01-16T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "However, in <b>machine</b> <b>learning</b> methodology, <b>squared</b> <b>loss</b> will be minimized with respect to ... If one of the supports doesn\u2019t exist, the beam will eventually fall down by moving out of balance. A similar <b>analogy</b> is applied for comparing statistical modeling and <b>machine</b> <b>learning</b> methodologies here. The two-point validation is performed on the statistical modeling methodology on training data using overall model accuracy and individual parameters significance test. Due to the fact that either ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machined Learnings: ML and OR: An <b>analogy</b> with cost-sensitive ...", "url": "http://www.machinedlearnings.com/2010/07/ml-and-or.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2010/07/ml-and-or.html", "snippet": "Nonetheless I&#39;ve been amusing myself by thinking about it, in particular trying to think about it from a <b>machine</b> <b>learning</b> reduction standpoint. The simplest well-understood reduction that I can think of which is analogous to supplying estimates to a linear program is the reduction of cost-sensitive multiclass classification (CSMC) to regression.", "dateLastCrawled": "2021-12-25T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bias-Variance Decomposition</b> - mlxtend", "url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/", "isFamilyFriendly": true, "displayUrl": "rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp", "snippet": "We can decompose a <b>loss</b> function such as the <b>squared</b> <b>loss</b> into three terms, a variance, bias, and a noise term (and the same is true for the decomposition of the 0-1 <b>loss</b> later). However, for simplicity, we will ignore the noise term. Before we introduce the <b>bias-variance decomposition</b> of the 0-1 <b>loss</b> for classification, let us start with the decomposition of the <b>squared</b> <b>loss</b> as an easy warm-up exercise to get familiar with the overall concept. The previous section already listed the common ...", "dateLastCrawled": "2022-01-31T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "Optimization methods are applied to minimize the <b>loss</b> function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one <b>loss</b> is L0-1 = 1 (m &lt;= 0); in zero-one <b>loss</b>, value of <b>loss</b> is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this <b>loss</b> is it is not differentiable, non-convex, and also NP-hard. Hence, in order to make optimization feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "<b>Machine</b> <b>Learning</b> A Quantitative Approach Henry H. Liu P PerfMath. ... Bayesian, (4) <b>Analogy</b>, and (5) Unsupervised <b>learning</b>. Pedro Domingos proposed these five ML paradigms, and \u00a71.3 explains briefly what each of these five ML paradigms is about. <b>MACHINE</b> <b>LEARNING</b>: A QUANTITATIVE APPROACH 5 2 <b>Machine</b> <b>Learning</b> Fundamentals Illustrated with Regression 2.1 Try to find a publicly available <b>machine</b> <b>learning</b> dataset and apply an end-to-end procedure similar to the one we used with the fuel economy ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Calculation of Bias &amp; Variance in</b> python | by Nallaperumal | Analytics ...", "url": "https://medium.com/analytics-vidhya/calculation-of-bias-variance-in-python-8f96463c8942", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>calculation-of-bias-variance-in</b>-python-8f96463c8942", "snippet": "For any <b>machine</b> <b>learning</b> the performance of a model can be determined and characterized in terms of Bias and Variance. In supervised <b>machine</b> <b>learning</b> an algorithm learns a model from training data ...", "dateLastCrawled": "2022-01-29T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "3.1. Linear <b>Regression</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_linear-networks/linear-regression.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_linear-networks/linear-<b>regression</b>.html", "snippet": "Key ingredients in a <b>machine</b> <b>learning</b> model are training data, a <b>loss</b> function, an optimization algorithm, and quite obviously, the model itself. Vectorizing makes everything better (mostly math) and faster (mostly code). Minimizing an objective function and performing maximum likelihood estimation can mean the same thing.", "dateLastCrawled": "2022-01-31T04:58:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A novel <b>semi-supervised support vector machine with asymmetric</b> squared ...", "url": "https://link.springer.com/article/10.1007/s11634-020-00390-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11634-020-00390-y", "snippet": "In the field of <b>machine</b> <b>learning</b>, loss function is usually one of the key issues in designing <b>learning</b> algorithms since most problems require it to describe the cost of the discrepancy between the prediction and the observation. In fact, the use of the loss function can be traced back to a long time ago. For example, the least-square loss function for regression was already employed by Legendre, Gauss, and Adrain in the early 19th century (Steinwart and Christmann 2008). At present, various ...", "dateLastCrawled": "2021-11-13T10:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(squared loss)  is like +(machine learning analogy)", "+(squared loss) is similar to +(machine learning analogy)", "+(squared loss) can be thought of as +(machine learning analogy)", "+(squared loss) can be compared to +(machine learning analogy)", "machine learning +(squared loss AND analogy)", "machine learning +(\"squared loss is like\")", "machine learning +(\"squared loss is similar\")", "machine learning +(\"just as squared loss\")", "machine learning +(\"squared loss can be thought of as\")", "machine learning +(\"squared loss can be compared to\")"]}