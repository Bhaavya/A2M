{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Tutorial on <b>Fairness</b> in Machine Learning | by ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/a-tutorial-on-<b>fairness</b>-in-machine-learning-3ff8ba1040cb", "snippet": "4.2 <b>Demographic</b> <b>Parity</b>. <b>Demographic</b> <b>Parity</b>, also called Independence, Statistical <b>Parity</b>, is one of the most well-known criteria for <b>fairness</b>. Formulation: C is independent of A: P\u2080 [C = c] = P\u2081 [C = c] \u2200 c \u2208 {0,1} In our example, this means the acceptance rates of the applicants from the two groups must be <b>equal</b>. In practice, there are ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Equality of Opportunity in Supervised Learning", "url": "https://home.ttic.edu/~nati/Publications/HardtPriceSrebro2016.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Publications/HardtPriceSrebro2016.pdf", "snippet": "little or no <b>training</b> <b>data</b> available within A= 1:Second, <b>demographic</b> <b>parity</b> often cripples the utility that we might hope to achieve. Just imagine the common scenario in which the target variable Y\u2014whether an individual actually defaults or not\u2014is correlated with A:<b>Demographic</b> <b>parity</b> would not allow the ideal predictor Yb= Y;which can hardly be considered discriminatory as it represents the actual outcome. As a result, the loss in utility of introducing <b>demographic</b> <b>parity</b> can be ...", "dateLastCrawled": "2022-01-31T14:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Fairness Week 2: Mitigation", "url": "https://web.stanford.edu/class/cs329t/slides/fairness-Week2.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs329t/slides/fairness-Week2.pdf", "snippet": "Metrics <b>like</b>: those that directly follow from accuracy to <b>training</b> <b>data</b> (true/false positive/negative rates, etc.) Metrics <b>like</b>: <b>demographic</b> <b>parity</b> (disparate impact ratio, statistical <b>parity</b> difference) Metrics <b>like</b>: equalized odds or equalized opportunity (want to equalize the rates of people fairly/unfairly advantaged/disadvantaged)", "dateLastCrawled": "2021-10-21T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How to define <b>fairness</b> to detect and prevent ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/how-to-define-<b>fairness</b>-to-detect-and-prevent...", "snippet": "<b>Demographic</b> <b>Parity</b> states that the proportion of each segment of a protected class (e.g. gender) should receive the positive outcome at <b>equal</b> rates. A positive outcome is the preferred decision, such as \u201cgetting to university\u201d, \u201cgetting a loan\u201d or \u201cbeing shown the ad\u201d. As mentioned earlier, the difference should be ideally zero, but this is usually not the case.", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machine Learning Glossary: <b>Fairness</b> | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/fairness", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/glossary/<b>fairness</b>", "snippet": "For example, if both Lilliputians and Brobdingnagians apply to Glubbdubdrib University, <b>demographic</b> <b>parity</b> is achieved if the percentage of Lilliputians admitted is the same as the percentage of Brobdingnagians admitted, irrespective of whether one group is on average more qualified than the other. Contrast with equalized odds and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain ...", "dateLastCrawled": "2022-02-02T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Data</b> Balance Analysis on Spark | SynapseML", "url": "https://microsoft.github.io/SynapseML/docs/features/responsible_ai/Data%20Balance%20Analysis/", "isFamilyFriendly": true, "displayUrl": "https://microsoft.github.io/SynapseML/docs/features/responsible_ai/<b>Data</b> Balance Analysis", "snippet": "It is unfortunately all too easy to build an ML model that produces biased results for subsets of an overall population, by <b>training</b> or testing the model on biased ground truth <b>data</b>. There are multiple case studies of biased models assisting in granting loans, healthcare, recruitment opportunities and many other decision making tasks. In most of these examples, the <b>data</b> from which these models are trained was the common issue. These findings emphasize how important it is for model creators ...", "dateLastCrawled": "2022-01-31T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "FairGAN : Achieving Fair <b>Data</b> Generation and Classi\ufb01cation through ...", "url": "https://par.nsf.gov/servlets/purl/10175677", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10175677", "snippet": "<b>training</b> <b>data</b>, generating fair <b>data</b> by FairGAN cannot guaran-tee fair classi\ufb01cation for models trained from them. Another disadvantage of FairGAN is that there are classi\ufb01cation-based fairness notions such as equality of odds or equality of opportunity which cannot be achieved via generating fair <b>data</b> alone. Equality of opportunity is an important fairness notion in classi\ufb01cation models [11]. It emphasizes on that individuals who qualify for a desirable outcome should have an <b>equal</b> ...", "dateLastCrawled": "2022-02-01T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning Smooth and Fair Representations</b> | DeepAI", "url": "https://deepai.org/publication/learning-smooth-and-fair-representations", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-smooth-and-fair-representations</b>", "snippet": "h in Figure 1) and \u0394(f,t) then measures the <b>demographic</b> <b>parity</b> of f (see [ 18] ): Definition 2.1. <b>Demographic</b> <b>parity</b> Consider a representation <b>distribution</b> \u03bct induced by a representation mapping t:X \u2192Z. A classifier f:Z\u2192{0,1} used by a <b>data</b> processor satisfies \u03b4\u2212 <b>Demographic</b> <b>Parity</b> on \u03bct if and only if \u0394(f,t)\u2264\u03b4.", "dateLastCrawled": "2021-12-30T00:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning Removes Bias from Algorithms</b> and the Hiring ... - Arena", "url": "https://arena.io/machine-learning-removes-bias-from-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://arena.io/<b>machine-learning-removes-bias-from-algorithms</b>", "snippet": "Arena removed all <b>data</b> from the models that could correlate to protected classifications and then measured <b>demographic</b> <b>parity</b> by: Monitoring the applicant pool\u2019s <b>distribution</b> of protected class <b>data</b> and other characteristics ; Comparing these <b>distribution</b> patterns to the applicant pool that was being predicted for success \u201cThese efforts brought us in line with EEOC compliance thresholds,\u201d explains Myra Norton, President/COO of Arena. \u201cBut we\u2019ve always wanted to go further than a ...", "dateLastCrawled": "2022-01-26T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Demographics Should Not Be the Reason of Toxicity: Mitigating ...", "url": "https://aclanthology.org/2020.acl-main.380.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.380.pdf", "snippet": "non-discrimination <b>distribution</b> to the discrim-ination <b>distribution</b>. Based on this formal-ization, we further propose a model-agnostic debiasing <b>training</b> framework by recovering the non-discrimination <b>distribution</b> using in-stance weighting, which does not require any extra resources or annotations apart from a pre-de\ufb01ned set of <b>demographic</b> identity-terms. Experiments demonstrate that our method can effectively alleviate the impacts of the un-intended biases without signi\ufb01cantly hurting ...", "dateLastCrawled": "2022-01-17T16:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Tutorial on <b>Fairness</b> in Machine Learning | by ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/a-tutorial-on-<b>fairness</b>-in-machine-learning-3ff8ba1040cb", "snippet": "Sample size disparity: If the <b>training</b> <b>data</b> coming from the minority group is much less than those coming from the majority group, ... C=Y satisfies Predictive Rate <b>Parity</b>. <b>Equal</b> chance of success(Y=1) given acceptance(C=1). Flaws: The flaw <b>is similar</b> to the flaw of equality of opportunities: it may not help closing the gap between two groups. The reasoning <b>is similar</b> as before. 4.5 Individual <b>Fairness</b>. Individual <b>fairness</b> is a relatively different notion. The previous three criteria are all ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Sources of unintended <b>bias</b> in <b>training</b> <b>data</b> | by Cristina Goldfain ...", "url": "https://towardsdatascience.com/sources-of-unintended-bias-in-training-data-be5b7f3347d0", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/sources-of-unintended-<b>bias</b>-in-<b>training</b>-<b>data</b>-be5b7f3347d0", "snippet": "<b>Demographic</b> <b>parity</b> is 1 when the probability is independent of group membership; a ratio of 0.8 is reasonable based on the generalization of the 80 percent rule advocated by US EEOC\u2078, and smaller numbers are indicative of <b>bias</b>. The <b>equal</b> opportunity metric highlights the fact that a positive label is often a desirable outcome (\u201can opportunity\u201d such as \u201cthe mortgage loan is approved\u201d) and thus focuses on comparing the True Positive Rate (TPR) across groups: the rate at which ...", "dateLastCrawled": "2022-01-31T06:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Tutorial #1: <b>bias and fairness in AI</b> - Borealis AI", "url": "https://www.borealisai.com/en/blog/tutorial1-bias-and-fairness-ai/", "isFamilyFriendly": true, "displayUrl": "https://www.borealisai.com/en/blog/tutorial1-bias-and-fairness-ai", "snippet": "<b>Demographic</b> <b>parity</b>: The threshold could be chosen so that the same proportion of each group are classified as $\\hat{y} =1$ and given loans (figure 3). We make an <b>equal</b> number of loans to each group despite the different tendencies of each to repay (figure 3b). This has the disadvantage that the true positive and false positive rates might be completely different in different populations (figure 3c). From the perspective of the lender, it is desirable to give loans in proportion to people&#39;s ...", "dateLastCrawled": "2022-02-02T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "DataBalanceAnalysis - Adult Census Income | SynapseML", "url": "https://microsoft.github.io/SynapseML/docs/features/responsible_ai/DataBalanceAnalysis%20-%20Adult%20Census%20Income/", "isFamilyFriendly": true, "displayUrl": "https://microsoft.github.io/SynapseML/docs/features/responsible_ai/<b>Data</b>BalanceAnalysis...", "snippet": "Visualized <b>Demographic</b> <b>Parity</b> of Races to see that Asian-Pac-Islander sees &gt;50k income much more than Other, in addition to other race combinations. <b>Distribution</b> Balance Measures. Calculated <b>Distribution</b> Balance Measures to see that &quot;Sex&quot; is much closer to a perfectly balanced <b>distribution</b> than &quot;Race&quot;.", "dateLastCrawled": "2022-02-01T18:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Exploring Algorithmic Fairness in Deep Speaker Verification", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7974715/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7974715", "snippet": "<b>Demographic</b> <b>parity</b> is fulfilled if people from different protected groups has on average <b>equal</b> classifications. Another definition implies that, if the classifier gets it wrong, it should be equally wrong for all protected groups, since being more wrong for one group would result in harmful outcomes for this group compared to the other ones . Hence, false negative and false positive rates should be <b>equal</b> across different protected groups. Soft biometrics received increasing attention from ...", "dateLastCrawled": "2021-06-09T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/interventions", "snippet": "Kamiran and Calders introduce a pre-processing technique for imposing <b>demographic</b> <b>parity</b> based on reweighting the <b>training</b> <b>data</b>. It is implemented in IBM&#39;s AI Fairness 360 library. How it works . Classifiers can learn bias because representatives of the disadvantaged group with positive outcomes are poorly represented in the <b>training</b> <b>data</b>. The reweighting algorithm proposed by Kamiran and Calders identifies such points and upweights them, so that they have a greater impact on model <b>training</b> ...", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Data</b> Balance Analysis on Spark | SynapseML", "url": "https://microsoft.github.io/SynapseML/docs/features/responsible_ai/Data%20Balance%20Analysis/", "isFamilyFriendly": true, "displayUrl": "https://microsoft.github.io/SynapseML/docs/features/responsible_ai/<b>Data</b> Balance Analysis", "snippet": "It is unfortunately all too easy to build an ML model that produces biased results for subsets of an overall population, by <b>training</b> or testing the model on biased ground truth <b>data</b>. There are multiple case studies of biased models assisting in granting loans, healthcare, recruitment opportunities and many other decision making tasks. In most of these examples, the <b>data</b> from which these models are trained was the common issue. These findings emphasize how important it is for model creators ...", "dateLastCrawled": "2022-01-31T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "One <b>definition of algorithmic fairness: statistical parity</b> \u2013 Math \u2229 ...", "url": "https://jeremykun.com/2015/10/19/one-definition-of-algorithmic-fairness-statistical-parity/", "isFamilyFriendly": true, "displayUrl": "https://jeremykun.com/2015/10/19/one-<b>definition-of-algorithmic-fairness-statistical-parity</b>", "snippet": "First, you could have some historical <b>data</b> you want to train a classifier h on, and usually you\u2019ll be given <b>training</b> labels for the <b>data</b> that tell you whether h(x) should be 1 or -1. In the absence of discrimination, getting high accuracy with respect to the <b>training</b> <b>data</b> is enough. But if there is some historical discrimination against S then the <b>training</b> labels are not trustworthy. As a consequence, achieving statistical <b>parity</b> for S necessarily reduces the accuracy of h. In other words ...", "dateLastCrawled": "2022-01-29T16:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Measure <b>Posttraining Data and Model Bias</b> - Amazon SageMaker", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-post-training-bias.html", "isFamilyFriendly": true, "displayUrl": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-post-<b>training</b>-bias.html", "snippet": "Amazon SageMaker Clarify provides eleven <b>posttraining data and model bias</b> metrics to help quantify various conceptions of fairness. These concepts cannot all be satisfied simultaneously and the selection depends on specifics of the cases involving potential bias being analyzed. Most of these metrics are a combination of the numbers taken from the binary classification confusion matrices for the different <b>demographic</b> groups. Because fairness and bias can be defined by a wide range of metrics ...", "dateLastCrawled": "2022-01-29T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Frontiers | Subgroup Invariant Perturbation for Unbiased Pre-Trained ...", "url": "https://www.frontiersin.org/articles/10.3389/fdata.2020.590296/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/f<b>data</b>.2020.590296", "snippet": "According to various studies, the <b>training</b> <b>data</b> <b>distribution</b> has a huge impact on the model&#39;s performance (Torralba and Efros, 2011; Bolukbasi et al., 2016). Models trained on imbalanced datasets lead to biased outputs. Therefore, different <b>data</b> re-sampling techniques have been proposed by the researchers to balance the <b>training</b> <b>data</b> <b>distribution</b>. This is done either by over-sampling the minority class", "dateLastCrawled": "2022-01-23T01:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>A Reductions Approach to Fair</b> Classification", "url": "http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf", "snippet": "The \ufb01rst de\ufb01nition\u2014<b>demographic</b> (or statistical) <b>parity</b>\u2014 <b>can</b> <b>be thought</b> of as a stronger version of the US <b>Equal</b> Employment Opportunity Commission\u2019s \u201cfour-\ufb01fths rule,\u201d which requires that the \u201cselection rate for any race, sex, or ethnic group [must be at least] four-\ufb01fths (4/5) (or eighty", "dateLastCrawled": "2022-02-02T14:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Equality of opportunity in supervised learning</b> | the morning paper", "url": "https://blog.acolyer.org/2018/05/07/equality-of-opportunity-in-supervised-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2018/05/07/<b>equality-of-opportunity-in-supervised-learning</b>", "snippet": "<b>Equal</b> opportunity does much better than <b>demographic</b> <b>parity</b>, extracting 92.8% of the potential profit available under the max profit model. Even stronger than <b>equal</b> opportunity is equalized odds . Equalized odds requires both the fraction of non-defaulters that qualify for loans and the fraction of defaulters that qualify for loans to be constant across groups.", "dateLastCrawled": "2022-02-03T12:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Discriminative but Not Discriminatory: A Comparison of Fairness ...", "url": "https://deepai.org/publication/discriminative-but-not-discriminatory-a-comparison-of-fairness-definitions-under-different-worldviews", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/discriminative-but-not-discriminatory-a-comparison-of...", "snippet": "A common notion is <b>demographic</b> <b>parity</b>, which requires that the model give the favorable outcome to both groups of people at <b>equal</b> rates. However, sometimes there are reasons to believe that the model should not give <b>equal</b> outcomes, such as when predicting physical strength for different genders. Because of this, jurisdictions that recognize disparate impact also make exceptions for cases where there is sufficient justification for the discriminatory effect, such as a", "dateLastCrawled": "2022-01-16T07:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gender Equality in India - Empowering Women, Empowering India</b> - Hindrise", "url": "https://hindrise.org/resources/gender-equality-in-india-empowering-women-empowering-india/", "isFamilyFriendly": true, "displayUrl": "https://hindrise.org/resources/<b>gender-equality-in-india-empowering-women-empowering-india</b>", "snippet": "Equality between men &amp; women exists when they <b>can</b> share equally in the <b>distribution</b> of power and influence. They are <b>equal</b> if they possess <b>equal</b> opportunities, financial independence, <b>equal</b> access to education, job, and the opportunity to develop personal ambitions, interests, talents. Within Nation and development strategies, gender equality is critical because it enables women to make decisions that impact their overall health and their spouses and families. Gender equality in India is the ...", "dateLastCrawled": "2022-02-02T05:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Fairness metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "Statistical <b>Parity</b>: Sometimes referred to as <b>demographic</b> <b>parity</b>, ... 10% to type WS, 40% to type MS, and 10% to type M. This <b>can</b> <b>be thought</b> of as reflecting user distributions seen in courses from certain domains. Ratings <b>can</b> either be 2 (for liking a course) or 1 (for not liking the course). We use the following probabilities for ratings L and observations O which reflect an observation bias. For example, L W S, S T E M indicates that a female student preferring STEM courses gives a ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Office of <b>Equal Opportunity and Civil Rights FAQs</b> | National Institute ...", "url": "https://nifa.usda.gov/office-equal-opportunity-and-civil-rights-faqs", "isFamilyFriendly": true, "displayUrl": "https://nifa.usda.gov/office-<b>equal-opportunity-and-civil-rights-faqs</b>", "snippet": "Equitable <b>Distribution</b> <b>Training</b> FAQ. Q&amp;A: Equitable <b>Distribution</b> of Benefits Webinar. Question: Is another way to say program vs. outreach activities, direct vs. indirect contacts? Answer: We are not aware of any standardized definition of the terms \u201cdirect contact\u201d or \u201cindirect contact,\u201d so it would depend on how your program defines these terms. Question: So, should we only collect REG <b>data</b> after a program is completed, or <b>can</b> we collect it upfront at registration? Answer: You may ...", "dateLastCrawled": "2022-01-26T06:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Wing size and <b>parity</b> as markers of cohort demography for potential ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/jvec.12406", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/jvec.12406", "snippet": "<b>Parity</b> determination and <b>demographic</b> parameters. <b>Parity</b> was determined from a sample of anophelines from each collection site and period. Of 2,107 Anopheles from WB, 1,633 were dissected for <b>parity</b>; of 1,365 from CH, 1,042 were dissected, and of 16,874 for GH, 1,989 were dissected. <b>Parity</b> was determined by dissecting ovaries under a stereomicroscope (x100) and observing the coiling pattern of tracheolar skeins (Detinova 1962). Seasonal variations in longevity and birth rates for each ...", "dateLastCrawled": "2021-09-17T15:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Diversity, Equity &amp; Inclusion", "url": "https://www.pepsico.com/about/diversity-equity-and-inclusion", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pepsico.com</b>/about/diversity-equity-and-inclusion", "snippet": "Since 2016, we have made external commitments around gender <b>parity</b>, pay equity, and prosperity for all our communities, and each year we continue to celebrate our champions of diversity and inclusion with the Harvey C. Russell and Steve Reinemund Awards. PepsiCo leverages <b>diversity and engagement</b> as a competitive business advantage that fuels innovation, strengthens our reputation, and fosters engagement with employees and members of the communities in which we do business. We strive to ...", "dateLastCrawled": "2022-02-03T00:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Eight truths about <b>diversity</b> and inclusion at work | Deloitte Insights", "url": "https://www2.deloitte.com/us/en/insights/deloitte-review/issue-22/diversity-and-inclusion-at-work-eight-powerful-truths.html", "isFamilyFriendly": true, "displayUrl": "https://www2.deloitte.com/us/en/insights/deloitte-review/issue-22/<b>diversity</b>-and...", "snippet": "It\u2019s about looking beyond <b>demographic</b> <b>parity</b> to the ultimate outcome\u2014<b>diversity</b> of thinking. ... <b>Training</b> is the most popular solution to increase workforce <b>diversity</b>. Research shows that nearly one-half of the midsize companies in the United States mandate <b>diversity</b> <b>training</b>, as do nearly all the Fortune 500. 29 Not surprisingly, the effectiveness of <b>diversity</b> <b>training</b> has come under scrutiny, with some claiming a positive impact (increased <b>diversity</b> representation), while others are ...", "dateLastCrawled": "2022-02-02T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The <b>measure and mismeasure of fairness: a critical review</b> of fair ...", "url": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness", "snippet": "One premise of many models of fairness in machine learning is that you <b>can</b> measure (\u2018prove\u2019) fairness of a machine learning model from within the system \u2013 i.e. from properties of the model itself and perhaps the <b>data</b> it is trained on. Beyond the questions of whether any one model of fairness is better or worse than another, I\u2019m coming to the realisation that this doesn\u2019t hold. To show that a machine learning model is fair, you need information from", "dateLastCrawled": "2022-01-30T11:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Sources of unintended <b>bias</b> in <b>training</b> <b>data</b> | by Cristina Goldfain ...", "url": "https://towardsdatascience.com/sources-of-unintended-bias-in-training-data-be5b7f3347d0", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/sources-of-unintended-<b>bias</b>-in-<b>training</b>-<b>data</b>-be5b7f3347d0", "snippet": "<b>Demographic</b> <b>parity</b> is 1 when the probability is independent of group membership; a ratio of 0.8 is reasonable based on the generalization of the 80 percent rule advocated by US EEOC\u2078, and smaller numbers are indicative of <b>bias</b>. The <b>equal</b> opportunity metric highlights the fact that a positive label is often a desirable outcome (\u201can opportunity\u201d such as \u201cthe mortgage loan is approved\u201d) and thus focuses on comparing the True Positive Rate (TPR) across groups: the rate at which ...", "dateLastCrawled": "2022-01-31T06:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to define <b>fairness</b> to detect and prevent ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/how-to-define-<b>fairness</b>-to-detect-and-prevent...", "snippet": "<b>Demographic</b> <b>Parity</b> states that the proportion of each segment of a protected class (e.g. gender) should receive the positive outcome at <b>equal</b> rates. A positive outcome is the preferred decision, such as \u201cgetting to university\u201d, \u201cgetting a loan\u201d or \u201cbeing shown the ad\u201d. As mentioned earlier, the difference should be ideally zero, but this is usually not the case.", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Exploring Algorithmic Fairness in Deep Speaker Verification", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7974715/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7974715", "snippet": "The first definition, i.e., <b>demographic</b> <b>parity</b>, is based on predicted classifications . <b>Demographic</b> <b>parity</b> is fulfilled if people from different protected groups has on average <b>equal</b> classifications. Another definition implies that, if the classifier gets it wrong, it should be equally wrong for all protected groups, since being more wrong for one group would result in harmful outcomes for this group <b>compared</b> to the other ones . Hence, false negative and false positive rates should be <b>equal</b> ...", "dateLastCrawled": "2021-06-09T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/interventions", "snippet": "<b>Demographic</b> <b>parity</b>. The intervention reduced <b>demographic</b> <b>parity</b> difference only slightly, from 0.193 to 0.174. Since we exclude a priori available information from the <b>training</b> process it is reasonable to expect some reduction in accuracy. However, the influence on achieved accuracy is small, reducing it from 85.3% to 84.9%.", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Are <b>Parity</b>-<b>Based Notions of AI Fairness Desirable</b>?", "url": "http://sites.computer.org/debull/A20dec/p51.pdf", "isFamilyFriendly": true, "displayUrl": "sites.computer.org/debull/A20dec/p51.pdf", "snippet": "Here, if p%(M) is 1 then the <b>demographic</b> groups have an <b>equal</b> probability of being assigned the favorable outcome by the classi\ufb01cation model M, and higher scores are better as they correspond to increased <b>parity</b>. Statistical <b>Parity</b>, a.k.a. <b>Demographic</b> <b>Parity</b>: [21] de\ufb01ned (and criticized, see below) the fairness notion", "dateLastCrawled": "2021-12-24T11:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Parity</b>-based Cumulative Fairness-aware Boosting | DeepAI", "url": "https://deepai.org/publication/parity-based-cumulative-fairness-aware-boosting", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>parity</b>-based-cumulative-fairness-aware-boosting", "snippet": "<b>Data</b>-driven AI systems <b>can</b> lead to discrimination on the basis of protected attributes like gender or race. One reason for this behavior is the encoded societal biases in the <b>training</b> <b>data</b> (e.g., females are underrepresented), which is aggravated in the presence of unbalanced class distributions (e.g., &quot;granted&quot; is the minority class).", "dateLastCrawled": "2022-02-02T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Seamless <b>equal</b> accuracy ratio for inclusive CTC speech recognition ...", "url": "https://www.sciencedirect.com/science/article/pii/S016763932100128X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S016763932100128X", "snippet": "The <b>demographic</b> <b>parity</b> gap <b>can</b> be reduced by massaging dataset labels or giving more weight to samples from disadvantaged groups (Calders et al., 2009). A recent paper (Anahideh and Asudeh, 2020) proposes that <b>demographic</b> <b>parity</b> <b>can</b> select <b>data</b> to create a fair <b>training</b> set, instead of modifying the <b>training</b> labels in an existing dataset.", "dateLastCrawled": "2022-01-02T13:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Walkthrough with UCI Census <b>Data</b> - Google Research", "url": "https://pair-code.github.io/what-if-tool/walkthrough.html", "isFamilyFriendly": true, "displayUrl": "https://pair-code.github.io/what-if-tool/walkthrough.html", "snippet": "In this case, <b>demographic</b> <b>parity</b> <b>can</b> be found with both groups getting loans 16% of the time by having the male threshold at 0.78 and the female threshold at 0.12. Because of the vast difference in the properties of the male and female <b>training</b> <b>data</b> in this 1994 census dataset, we need quite different thresholds to achieve <b>demographic</b> <b>parity</b>.", "dateLastCrawled": "2022-01-30T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Measure <b>Posttraining Data and Model Bias</b> - Amazon SageMaker", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-post-training-bias.html", "isFamilyFriendly": true, "displayUrl": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-post-<b>training</b>-bias.html", "snippet": "Amazon SageMaker Clarify provides eleven <b>posttraining data and model bias</b> metrics to help quantify various conceptions of fairness. These concepts cannot all be satisfied simultaneously and the selection depends on specifics of the cases involving potential bias being analyzed. Most of these metrics are a combination of the numbers taken from the binary classification confusion matrices for the different <b>demographic</b> groups. Because fairness and bias <b>can</b> be defined by a wide range of metrics ...", "dateLastCrawled": "2022-01-29T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning Removes Bias from Algorithms</b> and the Hiring ... - Arena", "url": "https://arena.io/machine-learning-removes-bias-from-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://arena.io/<b>machine-learning-removes-bias-from-algorithms</b>", "snippet": "Arena removed all <b>data</b> from the models that could correlate to protected classifications and then measured <b>demographic</b> <b>parity</b> by: Monitoring the applicant pool\u2019s <b>distribution</b> of protected class <b>data</b> and other characteristics ; Comparing these <b>distribution</b> patterns to the applicant pool that was being predicted for success \u201cThese efforts brought us in line with EEOC compliance thresholds,\u201d explains Myra Norton, President/COO of Arena. \u201cBut we\u2019ve always wanted to go further than a ...", "dateLastCrawled": "2022-01-26T07:50:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Pandas <b>Machine</b> <b>Learning</b> Example", "url": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "snippet": "Regardless of your dataset, <b>demographic</b> <b>parity</b> is a <b>machine</b> <b>learning</b> algorithms. Data Munging It helps us to missing data of wedge form with another. Python with datetime module, i should equal to bring new example <b>machine</b>. Quite possibly the state important part clean the <b>machine</b> <b>learning</b> process is understanding the data you are working with and advantage it relates to reflect task you front to solve. Viewing the corresponding number of dropping down arrow illustrates that are not only all ...", "dateLastCrawled": "2022-01-24T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An example of prediction which complies with <b>Demographic</b> <b>Parity</b> and ...", "url": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-demographic-parity-and-equalizes-group-wise-risks-in-the-context", "isFamilyFriendly": true, "displayUrl": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-<b>demographic</b>...", "snippet": "However, <b>Demographic</b> <b>Parity</b> and EGWR only define fairness on the group level and inspecting the individual level reveals a critical flow of this prediction rule. We have constrained our predictors to those that do not produce Disparate Treatment by prohibiting them from having the sensitive variable as direct input. Nevertheless, enforcing group level fairness constraints (such as DP and EGWR) forces the prediction rule to guess the sensitive attribute corresponding to a given feature vector", "dateLastCrawled": "2022-02-05T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Heidari et al. have written a paper comparing the three criteria \u2013 <b>demographic</b> <b>parity</b>, equality of opportunity, and predictive <b>parity</b> \u2013 to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the <b>analogy</b> is fascinating, it too assumes that we may take what is in the data at face value. In their likening predictive <b>parity</b> to luck egalitarianism, they have to go to especially great lengths, in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "This includes measures such as <b>Demographic</b> <b>Parity</b> / Statistical <b>Parity</b> (Dwork et al., 2012), Equalized Odds Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are trying to test for equivalence. In another survey of fairness definitions, Verma &amp; Rubin (2018) listed 20 definitions of fairness, 13 belonging to ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Starting to think about AI Fairness - Adolfo Eliaz\u00e0t - Artificial ...", "url": "https://adolfoeliazat.com/2022/01/01/starting-to-think-about-ai-fairness-2/", "isFamilyFriendly": true, "displayUrl": "https://adolfoeliazat.com/2022/01/01/starting-to-think-about-ai-fairness-2", "snippet": "() have written a paper comparing the three criteria \u2013 <b>demographic</b> <b>parity</b>, equality of opportunity, and predictive <b>parity</b> \u2013 to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the <b>analogy</b> is fascinating, it too assumes that we may take what is in the data at face value. In their likening predictive <b>parity</b> to luck egalitarianism, they have to go to especially great lengths, in assuming that the", "dateLastCrawled": "2022-02-01T05:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The <b>measure and mismeasure of fairness: a critical review</b> of fair ...", "url": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness", "snippet": "In case you\u2019re wondering where on earth I\u2019m going with this\u2026 it\u2019s a very stretched <b>analogy</b> I\u2019ve been playing with in my mind. One premise of many models of fairness in <b>machine</b> <b>learning</b> is that you can measure (\u2018prove\u2019) fairness of a <b>machine</b> <b>learning</b> model from within the system \u2013 i.e. from properties of the model itself and perhaps the data it is trained on. Beyond the questions of whether any one model of fairness is better or worse than another, I\u2019m coming to the ...", "dateLastCrawled": "2022-01-30T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fighting Money Laundering with Statistics and <b>Machine</b> <b>Learning</b>: An ...", "url": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-machine-learning-an-introduction-and-review", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-<b>machine</b>...", "snippet": "Statistics and <b>machine</b> <b>learning</b> have long promised efficient and robust techniques for AML. So far, though, these remain to manifest [Grint2001]. One reason is that the academic literature is small and fragmented [Leite2019, Ngai2011]. With this paper, we aim to do three things. First, we propose a unified terminology to homogenize and associate research methodologies. Second, we review selected, exemplary methods. Third, we present recent <b>machine</b> <b>learning</b> concepts that have the potential to ...", "dateLastCrawled": "2022-01-28T21:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "50 Years of Test (Un)fairness: Lessons for <b>Machine</b> <b>Learning</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1811.10104/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1811.10104", "snippet": "Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and <b>machine</b> <b>learning</b>. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to ...", "dateLastCrawled": "2021-10-06T04:52:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(demographic parity)  is like +(equal distribution of training data)", "+(demographic parity) is similar to +(equal distribution of training data)", "+(demographic parity) can be thought of as +(equal distribution of training data)", "+(demographic parity) can be compared to +(equal distribution of training data)", "machine learning +(demographic parity AND analogy)", "machine learning +(\"demographic parity is like\")", "machine learning +(\"demographic parity is similar\")", "machine learning +(\"just as demographic parity\")", "machine learning +(\"demographic parity can be thought of as\")", "machine learning +(\"demographic parity can be compared to\")"]}