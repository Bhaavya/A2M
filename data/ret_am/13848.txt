{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Natural Language Processing In Finance : Forecasting Stock</b> Movements", "url": "https://www.rebellionresearch.com/natural-language-processing-in-finance", "isFamilyFriendly": true, "displayUrl": "https://www.rebellionresearch.com/<b>natural-language-processing-in-finance</b>", "snippet": "The <b>masked</b> <b>language</b> <b>model</b> \u201cmasks\u201d a word in it and predicts possible <b>words</b> based off of the <b>words</b> surrounding it. The next sentence prediction <b>model</b> determines whether a particular sentence B follows sentence A in the original text corpus. The advantage of BERT is that it considers the context of a particular word. 2.4.3 FinBERT <b>model</b> . FinBERT is a BERT <b>model</b> pre-trained on financial communication text. One of the goals of FinBERT is to analyze the sentiment of financial text, which as ...", "dateLastCrawled": "2022-01-26T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "FROM Pre-trained Word Embeddings TO Pre-trained <b>Language</b> Models \u2014 Focus ...", "url": "https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/from-pre-trained-word-<b>embedding</b>s-to-pre-trained...", "snippet": "\u201cThe <b>masked</b> <b>language</b> <b>model</b> randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the <b>masked</b> word based only on its context. Unlike left-to-right <b>language</b> <b>model</b> pre-training , the MLM objective allows the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer .\u201d", "dateLastCrawled": "2022-01-29T16:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Simple BERT-Based Approach for Lexical Simplification \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1907.06226/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1907.06226", "snippet": "The <b>masked</b> <b>language</b> <b>model</b> (MLM) used in BERT randomly masks some percentage of the input tokens, and predicts the <b>masked</b> word based on its context. If masking the complex word in a sentence, the idea in MLM is in accordance with generating the candidates of the complex word in LS. Therefore, we introduce an LS approach BERT-LS that uses MLM of BERT for substitution generation. The sentence <b>masked</b> the complex word is not directly feed into the MLM. Our BERT-LS concatenates the original ...", "dateLastCrawled": "2021-12-22T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>PRETRAIN KNOWLEDGE-AWARE LANGUAGE MODELS</b>", "url": "https://openreview.net/pdf?id=OAdGsaptOXy", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=OAdGsaptOXy", "snippet": "<b>language</b> modeling (3) or <b>masked</b> <b>language</b> modeling (1), wherein transformer networks process a sequence of <b>words</b> and are asked to predict the next/<b>masked</b> <b>words</b>. There is no explicit guidance to the transformers that humans prefer them to capture correct, real-world information. As a result, all the knowledge captured in these pretrained <b>language</b> models is only signaled by patterns of co-occuring <b>words</b> in the input sequence that is learned implicitly during pretraining. In this paper, instead ...", "dateLastCrawled": "2022-01-13T17:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) From Universal <b>Language</b> <b>Model</b> to Downstream Task: Improving ...", "url": "https://www.researchgate.net/publication/344336155_From_Universal_Language_Model_to_Downstream_Task_Improving_RoBERTa-Based_Vietnamese_Hate_Speech_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344336155_From_Universal_<b>Language</b>_Mode", "snippet": "We first tune the PhoBERT on our dataset by re-training the <b>model</b> on the <b>Masked</b> <b>Language</b> <b>Model</b> task; then, we employ its encoder for text classification. In order to preserve pre-trained weights ...", "dateLastCrawled": "2021-10-14T02:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Word-level Lexical Normalisation using Context-Dependent ... - DeepAI", "url": "https://deepai.org/publication/word-level-lexical-normalisation-using-context-dependent-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/word-level-lexical-normalisation-using-context...", "snippet": "The channel <b>model</b> predicts the most likely correct form of the given word, while the <b>language</b> <b>model</b> aims to predict the next word given the current word in a similar manner to Word2Vec (Mikolov et al., 2013). The outputs of the two LSTMs are combined at decoding time to predict the output token. The paper also introduces an attention-based RNN <b>model</b>, inspired by", "dateLastCrawled": "2022-01-31T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Khmer Language Model Using ULMFiT</b> | by Phylypo Tum | Medium", "url": "https://medium.com/@phylypo/khmer-language-model-using-ulmfit-b0f8ca4e15be", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@phylypo/<b>khmer-language-model-using-ulmfit</b>-b0f8ca4e15be", "snippet": "The encoding vector contains the ratio of the co-occurrence <b>probabilities</b> of two <b>words</b> explicitly <b>known</b> as a count-based method. Source: Glove: Global Vectors for Word Representation . Pennington ...", "dateLastCrawled": "2022-01-16T21:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Introduction to Transformers in Machine Learning</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in...", "snippet": "Suppose that our goal is to build a <b>language</b> <b>model</b> capable of translating German text into English. In the classic scenario, with more classic approaches, we would learn a <b>model</b> which is capable of making the translation directly. In other <b>words</b>, we are teaching one translator to translate German into English. In other <b>words</b>, the translator needs to be able to speak both languages fluently, understand the relationships between <b>words</b> in the two languages, and so on. While this will work, it ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Question answering with TensorFlow</b> \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/content/question-answering-with-tensorflow/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/content/<b>question-answering-with-tensorflow</b>", "snippet": "This happens on multiple dimensions, # so in the off chance there&#39;s one or two indexes that match # we make sure it matches in all indexes. locs = tf.equal(logits, logloc) # correctsbool: A boolean tensor that indicates for which # <b>words</b> in the context the score always matches the minimum score. correctsbool = tf.reduce_any(tf.logical_and(locs, corrbool),-1) # corrects: A tensor that is simply correctsbool cast to floats. corrects = tf.where(correctsbool, tf.ones_<b>like</b>(correctsbool, dtype=tf ...", "dateLastCrawled": "2022-01-31T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Medical Report Generation Using <b>Deep Learning</b> | by Vysakh Nair ...", "url": "https://towardsdatascience.com/image-captioning-using-deep-learning-fe0d929cf337", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/image-captioning-using-<b>deep-learning</b>-fe0d929cf337", "snippet": "A <b>Masked</b> Loss Function was created for this problem. For eg: If we have a sequence of tokens- [3],[10],[7],[0],[0],[0],[0],[0] We only have 3 <b>words</b> in this sequence, the zeros correspond to the padding which is actually not a part of the report. But the <b>model</b> will think that the zeros are also a part of the sequence and will start learning them ...", "dateLastCrawled": "2022-01-29T18:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Natural Language Processing In Finance : Forecasting Stock</b> Movements", "url": "https://www.rebellionresearch.com/natural-language-processing-in-finance", "isFamilyFriendly": true, "displayUrl": "https://www.rebellionresearch.com/<b>natural-language-processing-in-finance</b>", "snippet": "The pre-training tasks of the BERT <b>model</b> are the <b>masked</b> <b>language</b> <b>model</b> and the next sentence prediction <b>model</b>. The <b>masked</b> <b>language</b> <b>model</b> \u201cmasks\u201d a word in it and predicts possible <b>words</b> based off of the <b>words</b> surrounding it. The next sentence prediction <b>model</b> determines whether a particular sentence B follows sentence A in the original text corpus.", "dateLastCrawled": "2022-01-26T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Word-level Lexical Normalisation using Context-Dependent ... - DeepAI", "url": "https://deepai.org/publication/word-level-lexical-normalisation-using-context-dependent-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/word-level-lexical-normalisation-using-context...", "snippet": "Any tokens that do not appear in a pre-built <b>dictionary</b> are normalised to the most <b>similar</b> token in the <b>dictionary</b> using the ancillary conformer ... In contrast to ELMo, BERT learns deep bidirectional representations by performing a procedure <b>known</b> as the \u201c<b>masked</b> <b>language</b> <b>model</b>\u201d. For every input sentence to the <b>model</b> some number of terms are <b>masked</b> at random, and the <b>model</b> learns to predict the original terms. The primary distinction between BERT and ELMo with respect to LN is that ELMo ...", "dateLastCrawled": "2022-01-31T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "FROM Pre-trained Word Embeddings TO Pre-trained <b>Language</b> Models \u2014 Focus ...", "url": "https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/from-pre-trained-word-<b>embedding</b>s-to-pre-trained...", "snippet": "\u201cThe <b>masked</b> <b>language</b> <b>model</b> randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the <b>masked</b> word based only on its context. Unlike left-to-right <b>language</b> <b>model</b> pre-training , the MLM objective allows the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer .\u201d", "dateLastCrawled": "2022-01-29T16:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A <b>language</b> <b>model</b> that predicts the probability of candidate tokens to fill in blanks in a sequence. For instance, a <b>masked</b> <b>language</b> <b>model</b> can calculate <b>probabilities</b> for candidate word(s) to replace the underline in the following sentence: The ____ in the hat came back. The literature typically uses the string &quot;MASK&quot; instead of an underline ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>PRETRAIN KNOWLEDGE-AWARE LANGUAGE MODELS</b>", "url": "https://openreview.net/pdf?id=OAdGsaptOXy", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=OAdGsaptOXy", "snippet": "<b>language</b> modeling (3) or <b>masked</b> <b>language</b> modeling (1), wherein transformer networks process a sequence of <b>words</b> and are asked to predict the next/<b>masked</b> <b>words</b>. There is no explicit guidance to the transformers that humans prefer them to capture correct, real-world information. As a result, all the knowledge captured in these pretrained <b>language</b> models is only signaled by patterns of co-occuring <b>words</b> in the input sequence that is learned implicitly during pretraining. In this paper, instead ...", "dateLastCrawled": "2022-01-13T17:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) The <b>Language</b> of Proteins: NLP, Machine Learning &amp; Protein Sequences", "url": "https://www.researchgate.net/publication/350388949_The_Language_of_Proteins_NLP_Machine_Learning_Protein_Sequences", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350388949_The_<b>Language</b>_of_Proteins_NLP...", "snippet": "lish <b>language</b> <b>model</b> might be given a <b>masked</b> sentence such as \u2018\u2018The ____ sat on the mat\u201d and be tasked to predict what English <b>words</b> are plausible candidates for the mask token (e.g ...", "dateLastCrawled": "2021-10-05T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Models of visual word recognition", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3843812/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3843812", "snippet": "Connectionism: models expressed as artificial neural networks; this includes, for example, the IA <b>model</b> .These models are intended to capture general properties of neurons, or neuronal populations. Interactive activation (IA) <b>model</b>: the first, and still most influential, form of connectionist <b>model</b> of word recognition. <b>Words</b> are represented as nodes in a network that are connected by inhibitory links (see Figure 1 in main text).. Lexical competition: in both IA models and Bayesian models ...", "dateLastCrawled": "2022-02-03T05:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Introduction to Transformers in Machine Learning</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in...", "snippet": "Suppose that our goal is to build a <b>language</b> <b>model</b> capable of translating German text into English. In the classic scenario, with more classic approaches, we would learn a <b>model</b> which is capable of making the translation directly. In other <b>words</b>, we are teaching one translator to translate German into English. In other <b>words</b>, the translator needs to be able to speak both languages fluently, understand the relationships between <b>words</b> in the two languages, and so on. While this will work, it ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Khmer Language Model Using ULMFiT</b> | by Phylypo Tum | Medium", "url": "https://medium.com/@phylypo/khmer-language-model-using-ulmfit-b0f8ca4e15be", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@phylypo/<b>khmer-language-model-using-ulmfit</b>-b0f8ca4e15be", "snippet": "The encoding vector contains the ratio of the co-occurrence <b>probabilities</b> of two <b>words</b> explicitly <b>known</b> as a count-based method. Source: Glove: Global Vectors for Word Representation . Pennington ...", "dateLastCrawled": "2022-01-16T21:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Question answering with TensorFlow</b> \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/content/question-answering-with-tensorflow/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/content/<b>question-answering-with-tensorflow</b>", "snippet": "It gets some assistance in the form of word vectorization, which can store infomation about the definition of <b>words</b> <b>and their</b> relations to other <b>words</b>. <b>Similar</b> <b>words</b> have <b>similar</b> vectorizations, which means that the network can treat them as nearly the same word. For word vectorization, we\u2019ll use Stanford\u2019s Global Vectors for Word Representation (GloVe), which I\u2019ve discussed previously in more detail", "dateLastCrawled": "2022-01-31T00:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Knowledge-Aware <b>Language</b> <b>Model</b> <b>Pretraining</b> | DeepAI", "url": "https://deepai.org/publication/knowledge-aware-language-model-pretraining", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/knowledge-aware-<b>language</b>-<b>model</b>-<b>pretraining</b>", "snippet": "One potential limitation of these <b>language</b> models is <b>their</b> style of <b>pretraining</b>, e.g, auto-regressive <b>language</b> modeling radford2019language or <b>masked</b> <b>language</b> modeling devlin2019bert, wherein transformer networks process a sequence of <b>words</b> and are asked to predict the next/<b>masked</b> <b>words</b>. There is no explicit guidance to the transformers that humans prefer them to capture correct, real-world information. As a result, all the knowledge captured in these pretrained <b>language</b> models is only ...", "dateLastCrawled": "2022-01-12T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Self-supervised Learning: Generative or Contrastive</b>", "url": "https://www.researchgate.net/publication/342198628_Self-supervised_Learning_Generative_or_Contrastive", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/342198628_Self-supervised_Learning_Generative...", "snippet": "<b>masked</b> <b>language</b> <b>model</b> (MLM) <b>can</b> be regarded as a de- noising AE <b>model</b> because its input masks predicted tokens. T o <b>model</b> text sequence, <b>masked</b> <b>language</b> <b>model</b> (MLM)", "dateLastCrawled": "2022-01-04T06:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Models of visual word recognition", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3843812/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3843812", "snippet": "The most recent models <b>can</b> use realistic lexicons, <b>can</b> simulate data from a range of tasks, and <b>can</b> process <b>words</b> of different lengths. These models are the driving force behind much of the empirical work on reading. I discuss how the data have guided <b>model</b> development and, importantly, I also provide guidelines to help interpret and evaluate the contribution the models make to our understanding of how we read.", "dateLastCrawled": "2022-02-03T05:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Self-supervised Learning</b>: Generative or Contrastive | DeepAI", "url": "https://deepai.org/publication/self-supervised-learning-generative-or-contrastive", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>self-supervised-learning</b>-generative-or-contrastive", "snippet": "<b>Masked</b> <b>words</b>: <b>Masked</b> <b>language</b> <b>model</b>--- ... auto-regressive models <b>can</b> only factorize <b>probabilities</b> according to specific directions (such as right and down). Therefore, <b>masked</b> filters are employed in CNN architecture. Furthermore, two convolutional networks are combined to remove the blind spot in images. Based on PixelCNN, WaveNet \u2013 a generative <b>model</b> for raw audio was proposed. In order to deal with long-range temporal dependencies, dilated causal convolutions are developed to improve ...", "dateLastCrawled": "2022-01-27T23:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Proceedings of the 58th Annual Meeting of the Association for ...", "url": "https://aclanthology.org/volumes/2020.acl-main/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2020.acl-main", "snippet": "<b>Masked</b> <b>language</b> <b>model</b> and autoregressive <b>language</b> <b>model</b> are two types of <b>language</b> models. While pretrained <b>masked</b> <b>language</b> models such as BERT overwhelm the line of natural <b>language</b> understanding (NLU) tasks, autoregressive <b>language</b> models such as GPT are especially capable in natural <b>language</b> generation (NLG). In this paper, we propose a probabilistic masking scheme for the <b>masked</b> <b>language</b> <b>model</b>, which we call probabilistically <b>masked</b> <b>language</b> <b>model</b> (PMLM). We implement a specific PMLM with ...", "dateLastCrawled": "2022-02-03T09:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "PyMC: Bayesian Stochastic Modelling in Python", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3097064", "snippet": "<b>Masked</b> arrays <b>can</b> be generated using NumPy\u2019s ma.<b>masked</b>_equal function: ... In other <b>words</b>, the fitted <b>model</b> <b>can</b> be used to simulate data, and the distribution of the simulated data should resemble the distribution of the actual data. Fortunately, simulating data from the <b>model</b> is a natural component of the Bayesian modelling framework. Recall, from the discussion on imputation of missing data, the posterior predictive distribution: p (y ~ \u2223 y) = \u222b p (y ~ \u2223 \u03b8) f (\u03b8 \u2223 y) d \u03b8 (6 ...", "dateLastCrawled": "2022-01-29T22:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Word-internal /t,d/ deletion in spontaneous speech: Modeling the ...", "url": "https://www.cambridge.org/core/journals/language-variation-and-change/article/abs/wordinternal-td-deletion-in-spontaneous-speech-modeling-the-effects-of-extralinguistic-lexical-and-phonological-factors/AD557904A0464CC1F19CCA7A6A7E5544", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>language</b>-variation-and-change/article/abs/word...", "snippet": "Word <b>probabilities</b> investigated included the word frequency of <b>words</b> containing word-internal /t,d/ tokens, frequencies of adjacent <b>words</b>, and the predictabilities of /t,d/ token <b>words</b> from adjacent <b>words</b>. The frequency of the word containing a word-internal alveolar stop did not predict deletion in the overall dataset, nor did it predict deletion in function or content <b>words</b> when these were tested separately. In the complete dataset, the mean word frequency of <b>words</b> with deleted tokens was ...", "dateLastCrawled": "2022-01-19T11:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Text Generation With LSTM Recurrent Neural Networks in Python with Keras", "url": "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/text-generation-lstm-recur", "snippet": "Kick-start your project with my new book Deep Learning for Natural <b>Language</b> Processing, including step-by-step tutorials and the Python source code files for all examples. Let\u2019s get started. Note: LSTM recurrent neural networks <b>can</b> be slow to train and it is highly recommend that you train them on GPU hardware. You <b>can</b> access GPU hardware in the cloud very cheaply using Amazon Web Services, see the tutorial here. Update Oct/2016: Fixed a few minor comment typos in the code. Update Mar/2017 ...", "dateLastCrawled": "2022-02-03T06:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Components</b> - Rasa", "url": "https://rasa.com/docs/rasa/components/", "isFamilyFriendly": true, "displayUrl": "https://rasa.com/docs/rasa/<b>components</b>", "snippet": "The sentence features <b>can</b> be used in any bag-of-<b>words</b> <b>model</b>. The <b>corresponding</b> classifier <b>can</b> therefore decide what kind of features to use. ... if a list of <b>words</b> that should be treated as Out-Of-Vocabulary is <b>known</b>, it <b>can</b> be set to OOV_<b>words</b> instead of manually changing it in training data or using custom preprocessor. note . This featurizer creates a bag-of-<b>words</b> representation by counting <b>words</b>, so the number of OOV_token in the sentence might be important. note. Providing OOV_<b>words</b> is ...", "dateLastCrawled": "2022-01-30T16:50:00.0000000Z", "language": "id", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "black holes - Why is <b>information</b> indestructible? - Physics Stack Exchange", "url": "https://physics.stackexchange.com/questions/29175/why-is-information-indestructible", "isFamilyFriendly": true, "displayUrl": "https://physics.stackexchange.com/questions/29175", "snippet": "The degree of &quot;randomness&quot; in a random variable <b>can</b> <b>be thought</b> of like this: you <b>can</b> find a <b>model</b> that describes the sequence of variables somewhat - but it is only approximate. A shorter description of a random sequence is to define a <b>model</b> and its boundary conditions, then to code that <b>model</b> and conditions as well as the discrepancies between the observed variables and the <b>model</b>. If the <b>model</b> is better than guessing, this will be a pithier description than simply naming the values in full ...", "dateLastCrawled": "2022-02-02T15:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Models of visual word recognition", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3843812/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3843812", "snippet": "The most recent models <b>can</b> use realistic lexicons, <b>can</b> simulate data from a range of tasks, and <b>can</b> process <b>words</b> of different lengths. These models are the driving force behind much of the empirical work on reading. I discuss how the data have guided <b>model</b> development and, importantly, I also provide guidelines to help interpret and evaluate the contribution the models make to our understanding of how we read.", "dateLastCrawled": "2022-02-03T05:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Knowledge-Aware <b>Language</b> <b>Model</b> <b>Pretraining</b> | DeepAI", "url": "https://deepai.org/publication/knowledge-aware-language-model-pretraining", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/knowledge-aware-<b>language</b>-<b>model</b>-<b>pretraining</b>", "snippet": "One potential limitation of these <b>language</b> models is <b>their</b> style of <b>pretraining</b>, e.g, auto-regressive <b>language</b> modeling radford2019language or <b>masked</b> <b>language</b> modeling devlin2019bert, wherein transformer networks process a sequence of <b>words</b> and are asked to predict the next/<b>masked</b> <b>words</b>. There is no explicit guidance to the transformers that humans prefer them to capture correct, real-world information. As a result, all the knowledge captured in these pretrained <b>language</b> models is only ...", "dateLastCrawled": "2022-01-12T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) From Universal <b>Language</b> <b>Model</b> to Downstream Task: Improving ...", "url": "https://www.researchgate.net/publication/344336155_From_Universal_Language_Model_to_Downstream_Task_Improving_RoBERTa-Based_Vietnamese_Hate_Speech_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344336155_From_Universal_<b>Language</b>_Mode", "snippet": "We first tune the PhoBERT on our dataset by re-training the <b>model</b> on the <b>Masked</b> <b>Language</b> <b>Model</b> task; then, we employ its encoder for text classification. In order to preserve pre-trained weights ...", "dateLastCrawled": "2021-10-14T02:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "PET: Exploiting Patterns makes even small <b>language</b> models few shot ...", "url": "https://wanna-nlp.medium.com/pet-exploiting-patterns-makes-even-small-language-models-few-shot-learners-7bc41fab2c02", "isFamilyFriendly": true, "displayUrl": "https://wanna-nlp.medium.com/pet-exploiting-patterns-makes-even-small-<b>language</b>-<b>models</b>...", "snippet": "The <b>masked</b> token is basically the label of the input (here 1/0) converted to some tokens from the <b>dictionary</b> (here, Yes/No), and this mapping is done using a function <b>known</b> as the verbalizer. For each such pattern, a pre-trained <b>language</b> <b>model</b> is fine-tuned (the number of training samples is limited). Thus, we get as many fine-tuned <b>language</b> models as there are patterns. Each <b>language</b> <b>model</b> is fine-tuned using a weighted sum of the cross-entropy loss while predicting the <b>masked</b> token, and a ...", "dateLastCrawled": "2022-01-18T18:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Simple BERT-Based Approach for Lexical Simplification \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1907.06226/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1907.06226", "snippet": "The <b>masked</b> <b>language</b> <b>model</b> (MLM) used in BERT randomly masks some percentage of the input tokens, and predicts the <b>masked</b> word based on its context. If masking the complex word in a sentence, the idea in MLM is in accordance with generating the candidates of the complex word in LS. Therefore, we introduce an LS approach BERT-LS that uses MLM of BERT for substitution generation. The sentence <b>masked</b> the complex word is not directly feed into the MLM. Our BERT-LS concatenates the original ...", "dateLastCrawled": "2021-12-22T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "FLAVA: A Foundational <b>Language</b> And Vision Alignment <b>Model</b> | DeepAI", "url": "https://deepai.org/publication/flava-a-foundational-language-and-vision-alignment-model", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/flava-a-foundational-<b>language</b>-and-vision-alignment-<b>model</b>", "snippet": "<b>Compared</b> to previous work, our <b>model</b> FLAVA works on a wide range of tasks in each of the vision, <b>language</b>, and vision-and-<b>language</b> domains. FLAVA uses a shared trunk which was pretrained on only openly available public paired data. FLAVA combines dual and fusion encoder approaches into one holistic <b>model</b> that <b>can</b> be pretrained with our novel FLAVA pretraining scheme that leverages pretraining objectives from both categories. FLAVA is designed to be able to take advantage of unpaired unimodal ...", "dateLastCrawled": "2022-02-01T18:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Character-Aware Neural Language Models</b> | Request PDF", "url": "https://www.researchgate.net/publication/281312208_Character-Aware_Neural_Language_Models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/281312208_<b>Character-Aware_Neural_Language_Models</b>", "snippet": "The proposed approach combines <b>probabilities</b> from <b>masked</b> <b>language</b> <b>model</b> and word edit distances to find the best corrections for misspelled <b>words</b>. The chatbot and the entire conversational AI ...", "dateLastCrawled": "2022-01-07T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Natural Language Processing In Finance : Forecasting Stock</b> Movements", "url": "https://www.rebellionresearch.com/natural-language-processing-in-finance", "isFamilyFriendly": true, "displayUrl": "https://www.rebellionresearch.com/<b>natural-language-processing-in-finance</b>", "snippet": "One of the challenges of using the LM <b>dictionary</b> is that the number of classified <b>words</b> is very low <b>compared</b> to the number of <b>words</b> in the reports. This challenge <b>can</b> be addressed in several ways. We next discuss two commonly used methods. The first one is word embeddings and the second one is word2vec models. 2.3.1 Word Embeddings . The objective of word embeddings is to transform <b>words</b> into vectors, with the idea that similar <b>words</b> will end up close to each other in the feature space. One ...", "dateLastCrawled": "2022-01-26T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Face recognition with OpenCV, Python, and deep learning</b> - PyImageSearch", "url": "https://www.pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/2018/06/18/<b>face-recognition-with-opencv-python-and</b>-deep...", "snippet": "Figure 2: An example face recognition dataset was created programmatically with Python and the Bing Image Search API. Shown are six of the characters from the Jurassic Park movie series. Since Jurassic Park (1993) is my favorite movie of all time, and in honor of Jurassic World: Fallen Kingdom (2018) being released this Friday in the U.S., we are going to apply face recognition to a sample of the characters in the films:. Alan Grant, paleontologist (22 images); Claire Dearing, park ...", "dateLastCrawled": "2022-02-03T09:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Image Captioning with Face Recognition using Transformers", "url": "https://www.ijraset.com/research-paper/paper-on-image-captioning-with-face-recognition-using-transformers", "isFamilyFriendly": true, "displayUrl": "https://www.ijraset.com/research-paper/paper-on-image-captioning-with-face-recognition...", "snippet": "A Faster R-CNN-Transformer <b>model</b> is used to limit the length of the output description as there is a possibility of generating description of infinite <b>words</b>.Thus, the threshold of maximum 30 <b>words</b> in a description have been set for a given image. The image captioning is broadly categorized into two modules,one feature extraction from the images with help of pretrained CNN-InceptionV3, and secondly transformer translates the features and objects in an image to a natural sentence. The noun in ...", "dateLastCrawled": "2022-01-29T02:20:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Language</b> Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>language</b>", "snippet": "For instance, a <b>masked</b> <b>language</b> <b>model</b> can calculate probabilities for candidate word(s) to replace the underline in the following sentence: The ____ in the hat came back. The literature typically uses the string &quot;MASK&quot; instead of an underline. For example: The &quot;MASK&quot; in the hat came back. Most modern <b>masked</b> <b>language</b> models are bidirectional.", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>language</b> of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "For example, in the <b>masked</b> <b>language</b> task, some fraction of the tokens in the original text are <b>masked</b> at random, and the <b>language</b> <b>model</b> attempts to predict the original text. (B) (Pre-)trained <b>language</b> models are commonly fine-tuned on downstream tasks over labeled text, through a standard supervised-<b>learning</b> approach. Fine-tuning is typically much faster and provides superior performance than training a <b>model</b> from scratch, especially when labeled data is scarce.", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Illustrated GPT-2 (Visualizing Transformer <b>Language</b> Models) \u2013 Jay ...", "url": "http://jalammar.github.io/illustrated-gpt2/", "isFamilyFriendly": true, "displayUrl": "jalammar.github.io/illustrated-gpt2", "snippet": "GPT-2 <b>Masked</b> Self-Attention; Beyond <b>Language</b> modeling; You\u2019ve Made it! Part 3: Beyond <b>Language</b> Modeling. <b>Machine</b> Translation; Summarization ; Transfer <b>Learning</b>; Music Generation; Part #1: GPT2 And <b>Language</b> Modeling # So what exactly is a <b>language</b> <b>model</b>? What is a <b>Language</b> <b>Model</b>. In The Illustrated Word2vec, we\u2019ve looked at what a <b>language</b> <b>model</b> is \u2013 basically a <b>machine</b> <b>learning</b> <b>model</b> that is able to look at part of a sentence and predict the next word. The most famous <b>language</b> models ...", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Word Embeddings, WordPiece and Language-Agnostic BERT</b> (LaBSE) | by ...", "url": "https://medium.com/mlearning-ai/word-embeddings-wordpiece-and-language-agnostic-bert-labse-98c7626878c7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/<b>word-embeddings-wordpiece-and-language-agnostic-bert</b>...", "snippet": "Word embeddings are the representation of words in a numeric format, which can be understood by a computer. Simplest example would be (Yes, No) represented as (1, 0). But when we are dealing with\u2026", "dateLastCrawled": "2022-02-03T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The <b>language</b> of proteins: NLP, <b>machine</b> <b>learning</b> &amp; protein sequences ...", "url": "https://europepmc.org/article/MED/33897979", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/33897979", "snippet": "For example, in the <b>masked</b> <b>language</b> task, some fraction of the tokens in the original text are <b>masked</b> at random, and the <b>language</b> <b>model</b> attempts to predict the original text. (B) (Pre-)trained <b>language</b> models are commonly fine-tuned on downstream tasks over labeled text, through a standard supervised-<b>learning</b> approach. Fine-tuning is typically much faster and provides superior performance than training a <b>model</b> from scratch, especially when labeled data is scarce.", "dateLastCrawled": "2022-01-23T22:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>GitHub</b> - <b>rosinality/ml-papers</b>: My collection of <b>machine</b> <b>learning</b> papers", "url": "https://github.com/rosinality/ml-papers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rosinality/ml-papers", "snippet": "210413 <b>Masked</b> <b>Language</b> Modeling and the Distributional Hypothesis #<b>language</b>_<b>model</b> #mlm; 210417 mT6 #<b>language</b>_<b>model</b>; 210418 Data-Efficient <b>Language</b>-Supervised Zero-Shot <b>Learning</b> with #multimodal; 210422 ImageNet-21K Pretraining for the Masses #backbone; 210510 Are Pre-trained Convolutions Better than Pre-trained Transformers #nlp #convolution # ...", "dateLastCrawled": "2022-01-31T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An introduction to Deep <b>Learning</b> in Natural <b>Language</b> Processing: Models ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "snippet": "The pre-training was driven by two <b>language</b> <b>model</b> objectives, i.e. <b>Masked</b> <b>Language</b> <b>Model</b> (MLM) and Next Sentence Prediction (NSP). In MLM, showed in Fig. 8 , the network masks a small number of words of the input sequence and it tries to predict them in output, whereas in NSP the network tries to understand the relations between sentences by means of a binary loss.", "dateLastCrawled": "2022-01-04T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "&quot;A Passage to India&quot;: Pre-trained Word Embeddings for Indian Languages ...", "url": "https://deepai.org/publication/a-passage-to-india-pre-trained-word-embeddings-for-indian-languages", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../a-passage-to-india-pre-trained-word-embeddings-for-indian-<b>languages</b>", "snippet": "Tasks ranging from word <b>analogy</b> and spelling correction to more complex ones like Question Answering ... <b>masked</b> <b>language</b> <b>model</b> accuracy for BERT, and so on. We report these values in Table 2. 4 Models and Evaluation. <b>Language</b> as bn gu hi ml mr kn ko ne or pa sa ta te; Perplexity : 455: 354: 183: 518: 1689: 522: 155368: 325: 253: 975: 145: 399: 781: 82: Table 2: ELMo prerplexity scores. In this section, we briefly describe the models created using the approaches mentioned above in the paper ...", "dateLastCrawled": "2022-01-18T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Training and Evaluation of Word Embedding Models for Azerbaijani <b>Language</b>", "url": "https://www.researchgate.net/publication/352759086_Training_and_Evaluation_of_Word_Embedding_Models_for_Azerbaijani_Language", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/352759086_Training_and_Evaluation_of_Word...", "snippet": "feature is that it uses <b>masked</b> <b>language</b> <b>model</b> training objectives. <b>Masked</b> <b>language</b> <b>model</b> objective enables the architecture to be truly bidirectional and have access to the left and right context ...", "dateLastCrawled": "2022-01-11T03:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why are <b>language</b> modeling pre-training objectives considered ...", "url": "https://stats.stackexchange.com/questions/504980/why-are-language-modeling-pre-training-objectives-considered-unsupervised", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/504980/why-are-<b>language</b>-<b>model</b>ing-pre...", "snippet": "From the perspective of the <b>language</b> <b>model</b>, you have well-defined target labels and use supervise <b>learning</b> methods to teach the <b>model</b> to predict the labels. Calling it unsupervised pre-training is certainly sort of paper-publishing marketing, but it is not entirely wrong. It is unsupervised from the perspective of the downstream tasks.", "dateLastCrawled": "2022-01-29T07:03:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Improving Text Generation with Dynamic Masking and Recovering", "url": "https://www.ijcai.org/proceedings/2021/0534.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/proceedings/2021/0534.pdf", "snippet": "tokens, <b>just as masked language model</b> does. Therefore, our approach jointly maximizes both the likelihoods of both sen-tence generation and prediction of masked tokens. We verify the effectiveness and generality of our ap-proach on three types of text generation tasks which use var-ious forms of input data including text, graph, and image. For sequence-to-sequence (seq2seq) generation task (specif-ically, <b>machine</b> translation), our model obtains signi\ufb01cant improvement of 1.01 and 0.90 BLEU ...", "dateLastCrawled": "2022-01-29T07:50:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(masked language model)  is like +(a dictionary of known words and their corresponding probabilities)", "+(masked language model) is similar to +(a dictionary of known words and their corresponding probabilities)", "+(masked language model) can be thought of as +(a dictionary of known words and their corresponding probabilities)", "+(masked language model) can be compared to +(a dictionary of known words and their corresponding probabilities)", "machine learning +(masked language model AND analogy)", "machine learning +(\"masked language model is like\")", "machine learning +(\"masked language model is similar\")", "machine learning +(\"just as masked language model\")", "machine learning +(\"masked language model can be thought of as\")", "machine learning +(\"masked language model can be compared to\")"]}