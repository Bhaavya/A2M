{"src_spec_res": [[], [], [], []], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language <b>Evaluation</b> | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) #language. A score between 0.0 and 1.0, inclusive, indicating the quality of a translation between two human languages (for example, between English and Russian). A <b>BLEU</b> score of 1.0 indicates a perfect translation; a <b>BLEU</b> score of 0.0 indicates a terrible translation. C. causal language model. #language. Synonym for unidirectional language model. See bidirectional language model to contrast different directional approaches in language modeling. crash ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Evaluation</b> of an <b>NLP</b> model \u2014 latest benchmarks | by Ria Kulshrestha ...", "url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>evaluation</b>-of-an-<b>nlp</b>-model-latest-benchmarks-90fd8ce6fae5", "snippet": "<b>BLEU</b> Score \u2014 <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>. As the name suggests, it was originally used to evaluate translations from one language to another. How to calculate <b>BLEU</b> score? Calculating unigram precision: Step 1: Look at each word in the output sentence and assign it a score of 1 if it shows up in any of the reference sentences and 0 if it doesn\u2019t. Step 2: Normalize that count, so that it\u2019s always between 0 and 1, by dividing the number of words that showed up in one of the reference ...", "dateLastCrawled": "2022-01-28T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Natrual language processing basic concepts - language model - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "<b>BLEU</b> stands for <b>bilingual</b> <b>evaluation</b> <b>understudy</b>. It&#39;s an automatic metric to evaluate how close a sequence of text generated by a language model is to a reference. At first, it&#39;s used to evaluate the quality of <b>machine</b> translation text. Now other natural language processing tasks such as task-oriented dialogue generation adopt it as well. For a reference &quot;The man returned to the store&quot;, a generated text &quot;the the man the&quot; would get a BLUE score as below. For each word in the generated text ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "9.7. <b>Sequence</b> to <b>Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>), though originally proposed for evaluating <b>machine</b> translation results [Papineni et al., 2002], has been extensively used in measuring the quality of output sequences for different applications.", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The <b>Computational</b> Limits of Deep <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-computational-limits-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>computational</b>-limits-of-deep-<b>learning</b>", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) [papineni2002bleu] score is a metric for translation and computes the similarity between human translation and <b>machine</b> translation based on n-gram. An n-gram is a continuous sequence of n items from a given text. The score is based on precision, brevity penalty, and clipping. The modified n-gram precision ...", "dateLastCrawled": "2022-01-28T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) pathak2019.pdf | Aditya Kumar Pathak and Priyankit Acharya ...", "url": "https://www.academia.edu/38228943/pathak2019_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38228943/pathak2019_pdf", "snippet": "The standard metric people are using for <b>evaluation</b> of MT systems is <b>BLEU</b> score.<b>Bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) is the algorithm to determine the quality of text translated by a <b>machine</b> translation. Quality is the comparison between <b>machine</b>-translated output to that of human-generated output; the closer <b>machine</b> translation is to human-generated translation, the better is the <b>BLEU</b> score. <b>BLEU</b> score is a n-gram overlap of <b>machine</b> translation to that of reference translation.<b>BLEU</b> \u00bc min ...", "dateLastCrawled": "2021-02-16T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Sequence Models - Deep <b>Learning</b> Specialization 5 - Yuet&#39;s Blog", "url": "https://yestinyang.github.io/2018/02/19/DLS-5-Sequence-Models.html", "isFamilyFriendly": true, "displayUrl": "https://yestinyang.github.io/2018/02/19/DLS-5-Sequence-Models.html", "snippet": "<b>Bleu</b> Score: <b>bilingual</b> <b>evaluation</b> <b>understudy</b>. Evaluate \u2018accuracy\u2019 of a model predicting multiply equally good answers, being a substitute for human evaluating each output Attention Model. Counter the problem of long sentence, which requires the ability of memory but not badly need a NN to do this kind of job. Instead of \u2018remembering\u2019 the ...", "dateLastCrawled": "2022-01-22T18:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Arti cial Intelligence Master Thesis", "url": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "snippet": "2:87 <b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>) score over the baseline; attention model, for German-English translation, and 0:34 <b>BLEU</b> score improvement for Catalan-Spanish trans-lation. Keywords <b>Machine</b> <b>Learning</b>, Deep <b>Learning</b>, Natural Language Processing, Neural <b>Machine</b> Transla-tion", "dateLastCrawled": "2021-12-27T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> translation from text to sign language: a systematic review ...", "url": "https://link.springer.com/article/10.1007/s10209-021-00823-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10209-021-00823-1", "snippet": "The SMT component of the system uses MOSES for generating a language model and decodes the input sentence. The approach uses the <b>BLEU</b> metric for <b>evaluation</b> and reports the scores as <b>BLEU</b>-4 12.64% <b>BLEU</b>-3 19.28% <b>BLEU</b>-2 31.48% <b>BLEU</b>-1 53.17%. The results reported are satisfactory; however, the system needs a virtual avatar tool for completeness.", "dateLastCrawled": "2022-01-30T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Evaluation of machine translation systems and related procedures</b>", "url": "https://www.researchgate.net/publication/326320090_Evaluation_of_machine_translation_systems_and_related_procedures", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/326320090_<b>Evaluation</b>_of_<b>machine</b>_translation...", "snippet": "<b>Evaluation of machine translation systems and related procedures</b>. June 2018 ; Journal of Engineering and Applied Sciences 13(12):3961-3972; Project: <b>Machine</b> <b>learning</b>; Authors: Musatafa Albadr ...", "dateLastCrawled": "2022-01-15T04:04:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bleu (bilingual evaluation understudy))  is like +(analogy of \"translation fingerprints\")", "+(bleu (bilingual evaluation understudy)) is similar to +(analogy of \"translation fingerprints\")", "+(bleu (bilingual evaluation understudy)) can be thought of as +(analogy of \"translation fingerprints\")", "+(bleu (bilingual evaluation understudy)) can be compared to +(analogy of \"translation fingerprints\")", "machine learning +(bleu (bilingual evaluation understudy) AND analogy)", "machine learning +(\"bleu (bilingual evaluation understudy) is like\")", "machine learning +(\"bleu (bilingual evaluation understudy) is similar\")", "machine learning +(\"just as bleu (bilingual evaluation understudy)\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be thought of as\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be compared to\")"]}