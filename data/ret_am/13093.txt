{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "To Pay <b>Attention</b>, the Brain Uses Filters, Not a Spotlight | Quanta Magazine", "url": "https://www.quantamagazine.org/to-pay-attention-the-brain-uses-filters-not-a-spotlight-20190924/", "isFamilyFriendly": true, "displayUrl": "https://www.quantamagazine.org/to-pay-<b>attention</b>-the-brain-uses-filters-not-a-spotlight...", "snippet": "In fact, Halassa\u2019s discovery of the basal ganglia\u2019s role in <b>attention</b> is particularly fascinating. That\u2019s partly because it is such an ancient area of the brain, one that hasn\u2019t typically been viewed as a part of selective <b>attention</b>. \u201cFish have this,\u201d Krauzlis said. \u201cGoing back to the earliest vertebrates, <b>like</b> the lamprey, which ...", "dateLastCrawled": "2022-02-03T04:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Excessive <b>Attention</b>-Seeking and Drama Addiction | <b>Psychology Today</b>", "url": "https://www.psychologytoday.com/us/blog/obesely-speaking/201411/excessive-attention-seeking-and-drama-addiction", "isFamilyFriendly": true, "displayUrl": "https://<b>www.psychologytoday.com</b>/us/blog/obesely-speaking/201411/excessive-<b>attention</b>...", "snippet": "Excessive <b>attention</b>-seeking is not a character flaw. It is a brain wiring response to early developmental trauma caused by neglect.[3] The developing brain observes its environment and wires ...", "dateLastCrawled": "2021-11-19T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "8 Hidden <b>Psychological Problems</b> a Messy <b>Home Can Reveal About Us</b> ...", "url": "https://brightside.me/inspiration-psychology/8-hidden-psychological-problems-a-messy-home-can-reveal-about-us-729710/", "isFamilyFriendly": true, "displayUrl": "https://brightside.me/inspiration-psychology/8-hidden-<b>psychological-problems</b>-a-messy...", "snippet": "Whether you love or hate cleaning is more than just a personality quirk. You can already tell something about <b>a person</b> just by looking at how they do the dishes and where they store their clothes. By paying more <b>attention</b> to the way you organize the space around you, you can learn a whole lot more about your personality. The same works for other people. If you want to try to get a read on someone new that you\u2019ve met, just pay a visit to their home.", "dateLastCrawled": "2022-02-02T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "6 Ways You Can Command A <b>Room</b> <b>Without Saying A Word</b>", "url": "https://www.forbes.com/sites/averyblank/2018/02/13/6-ways-you-can-command-the-meeting-room-without-saying-a-word/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.forbes.com</b>/.../6-ways-you-can-command-the-meeting-<b>room</b>-<b>without-saying-a-word</b>", "snippet": "2. Pay <b>attention</b> to where you sit. If the meeting is around a table, sit at the table and not on the side. Get in the game. Don\u2019t sit on the sidelines. If you think you have less value to offer ...", "dateLastCrawled": "2022-01-26T20:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Be Charismatic and Influential | <b>The Art of Manliness</b>", "url": "https://www.artofmanliness.com/people/social-skills/command-a-room-like-a-man/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.artofmanliness.com</b>/people/social-skills/<b>command-a-room-like-a</b>-man", "snippet": "If we shine a light on every <b>person</b> in the <b>room</b>, we end up being the brightest man there. So, next time you enter a <b>room</b>, forget being charming. Hell, forget about commanding the <b>room</b>. Just focus on how you can make others feel important. The charm and the <b>room</b> will follow naturally. Have any other ideas on commanding a <b>room</b> <b>like</b> a man? Drop a line in the comment box. Related Articles. Previous Next . Never miss an update. Subscribe to the AoM Newsletter. Daily Weekly . I agree to the ...", "dateLastCrawled": "2022-02-01T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "3 Ways <b>to Judge Character</b> - <b>wikiHow</b>", "url": "https://www.wikihow.com/Judge-Character", "isFamilyFriendly": true, "displayUrl": "https://<b>www.wikihow.com</b>/<b>Judge-Character</b>", "snippet": "Pay <b>attention</b> to how other people react to the <b>person</b> you are evaluating. The energy level in the <b>room</b> can help you figure out some key things about someone&#39;s character. Observe how the vibe changes when the new <b>person</b> enters the <b>room</b>. Does the <b>room</b> suddenly seem quiet and uncomfortable? It might be that the <b>person</b> makes others somewhat uneasy.", "dateLastCrawled": "2022-01-31T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to Address Envelopes With Attn: 5 Steps (with Pictures)", "url": "https://www.wikihow.com/Address-Envelopes-With-Attn", "isFamilyFriendly": true, "displayUrl": "https://<b>www.wikihow.com</b>/Address-Envelopes-With-Attn", "snippet": "Write the business or organization name next. The name of the company where the <b>person</b> works goes underneath the &quot;Attn&quot; line. The company name should be written on the second line of the destination address. If the company is large and the <b>person</b> to whom you&#39;re writing works for a particular department, include the department name on a line of it&#39;s own before you write the company name.; Not including the company name on the envelope will not usually prevent your letter from being delivered.", "dateLastCrawled": "2022-02-03T03:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What Does a Messy <b>Room</b> Say About Your Personality", "url": "https://www.verywellmind.com/psychology-of-a-messy-room-4171244", "isFamilyFriendly": true, "displayUrl": "https://www.verywellmind.com/psychology-of-a-messy-<b>room</b>-4171244", "snippet": "In some cases, <b>a person</b>&#39;s desk might look <b>like</b> a jumbled mess, strewn with papers, envelopes, and files. Yet they always seem to know exactly where each and every item is when they need it. Some of the most creative and productive people seem to thrive in this type of environment.", "dateLastCrawled": "2022-02-03T00:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "10 <b>Ways To Tell A Spirit Is In Your Home</b> \u2014 Amanda Linette Meder", "url": "https://www.amandalinettemeder.com/blog/2014/8/24/10-ways-to-tell-a-spirit-is-in-your-home", "isFamilyFriendly": true, "displayUrl": "https://www.amandalinettemeder.com/blog/2014/8/24/10-<b>ways-to-tell-a-spirit-is-in-your-home</b>", "snippet": "If an animal acts a certain way when a new <b>person</b> enters a <b>room</b>, then, they do that specific thing, without apparent new stimuli present, there&#39;s a good chance you&#39;ve got a Spirit who has just stepped in. It could also be that your pet is merely seeing something beyond the standard human range of sight, such as other residual energies. 6 - Children telling you. If a child has told you they see someone in the house, there is probably someone in the house. Children can sense Spirits, and often ...", "dateLastCrawled": "2022-02-02T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "I Got Naked <b>In A Room Full Of</b> People | Ravishly", "url": "https://www.ravishly.com/2017/04/03/i-got-naked-room-full-people", "isFamilyFriendly": true, "displayUrl": "https://www.ravishly.com/2017/04/03/i-got-naked-<b>room</b>-full-people", "snippet": "&quot;Even apparently supportive responses <b>like</b> laughter can shift our <b>attention</b>,&quot; Nic explained. &quot;Making a joke can take away from the moment and prevent us truly listening.&quot; So we heard one another, then we danced and we laughed \u2014 and love sprang up <b>like</b> a vine, curled around each woman and bound us together. The rehearsal <b>room</b> was a place of pure safety, joy, and trust.", "dateLastCrawled": "2022-01-29T18:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>another word for attention-grabbing</b>? | <b>Attention</b>-grabbing ...", "url": "https://www.wordhippo.com/what-is/another-word-for/attention-grabbing.html", "isFamilyFriendly": true, "displayUrl": "https://www.wordhippo.com/what-is/<b>another-word-for/attention-grabbing</b>.html", "snippet": "What is <b>another word for attention-grabbing</b>? Need synonyms for <b>attention</b>-grabbing? Here&#39;s a list of <b>similar</b> words from our thesaurus that you can use instead. Contexts . Attracting notice and <b>attention</b>, or prone to doing so. Captivating or compelling in nature. Attracting notice and <b>attention</b>, or prone to doing so. So important as to warrant being put on the front page of newspapers. Adjective Attracting notice and <b>attention</b>, or prone to doing so. striking. noticeable. conspicuous. arresting ...", "dateLastCrawled": "2022-01-29T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is <b>another word for &quot;attract attention</b>&quot;?", "url": "https://www.wordhippo.com/what-is/another-word-for/attract_attention.html", "isFamilyFriendly": true, "displayUrl": "https://www.wordhippo.com/what-is/<b>another-word-for/attract_attention</b>.html", "snippet": "Need synonyms for attract <b>attention</b>? Here&#39;s a list of <b>similar</b> words from our thesaurus that you can use instead. Contexts To be conspicuous or noticeable. To behave dramatically or showily to impress an audience or observers. To praise or express pride in oneself. To cause to be liked \u2026 more Verb To be conspicuous or noticeable. stand out. be noticeable. be conspicuous. be distinctive. stick out. be noticed. be obvious. be prominent. be seen. be striking. be visible. catch the eye. leap ...", "dateLastCrawled": "2022-02-02T04:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "15 Things Only a <b>Person With ADHD Will Understand</b>", "url": "https://www.lifehack.org/351419/20-things-only-person-with-adhd-will-understand", "isFamilyFriendly": true, "displayUrl": "https://<b>www.lifehack.org</b>/351419/20-things-only", "snippet": "We\u2019ve all walked into the <b>room</b> to search for something, only to forget what we were searching for in the first place. For people diagnosed with ADHD, it is a common occurrence along with many other symptoms that make ordinary life a challenge. <b>Attention</b> Deficit Hyperactivity Disorder or ADHD is a learning disorder that affects people from the age of 6 years and above. Individuals affected by ADHD face challenges focusing on tasks, organizing things, and following instructions. Here are a ...", "dateLastCrawled": "2022-01-29T10:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Week 12_Chapter 18</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/465738973/week-12_chapter-18-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/465738973/<b>week-12_chapter-18</b>-flash-cards", "snippet": "D. paying <b>attention</b> to detail; paying <b>attention</b> to significant events C An example of _____ <b>attention</b> is the ability to focus on one voice among many in a crowded <b>room</b> or a noisy restaurant.", "dateLastCrawled": "2022-01-27T11:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Address Envelopes With Attn: 5 Steps (with Pictures)", "url": "https://www.wikihow.com/Address-Envelopes-With-Attn", "isFamilyFriendly": true, "displayUrl": "https://<b>www.wikihow.com</b>/Address-Envelopes-With-Attn", "snippet": "Write the business or organization name next. The name of the company where the <b>person</b> works goes underneath the &quot;Attn&quot; line. The company name should be written on the second line of the destination address. If the company is large and the <b>person</b> to whom you&#39;re writing works for a particular department, include the department name on a line of it&#39;s own before you write the company name.; Not including the company name on the envelope will not usually prevent your letter from being delivered.", "dateLastCrawled": "2022-02-03T03:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "face to face or in <b>person</b> meeting vs. remote, online or virtual meeting", "url": "https://meetingking.com/face-to-face-meetings-vs-virtual-meetings/", "isFamilyFriendly": true, "displayUrl": "https://meetingking.com/face-to-face-meetings-vs-virtual-meetings", "snippet": "Whether remote or in <b>person</b>, meeting format must be decided during the meeting planning phase of the process. Knowing how the meeting is going to be held in advance helps the invitees plan their attendance \u2013 either their transportation to the meeting and sufficient time to include this, or that they have the right tools \u2013 web, video etc \u2013 set up to be able to attend. MeetingKing supports both of these options allowing meeting coordinators to provide these details at the meeting ...", "dateLastCrawled": "2022-02-02T02:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How Do I get <b>a Deaf Person\u2019s Attention</b>? - Visually Speaking", "url": "https://www.visuallyspeaking.info/how-do-i-get-a-deaf-persons-attention/", "isFamilyFriendly": true, "displayUrl": "https://www.visuallyspeaking.info/how-do-i-get-<b>a-deaf-persons-attention</b>", "snippet": "Normally, a hearing <b>person</b> will start speaking <b>to a person</b> to get their <b>attention</b>, or calling out a name. For Deaf, it\u2019s stomping the floor and hoping the other <b>person</b> feels the vibrations. For Deaf, it\u2019s flashing the lights to get their <b>attention</b>, this usually works with a big group of Deaf people. For Deaf, it\u2019s throwing something soft or light at the other <b>person</b>\u2019s back or line of vision. For Deaf, it\u2019s tapping a <b>person</b>\u2019s shoulder to get them to look at you. Wait, isn\u2019t that ...", "dateLastCrawled": "2022-01-28T04:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Here&#39;s How You Make <b>a Quick and Perfect Introduction</b> | <b>Inc.com</b>", "url": "https://www.inc.com/justin-bariso/this-ted-talk-demonstrates-a-perfect-introduction-in-less-than-60-seconds.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.inc.com</b>/justin-bariso/this-ted-talk-demonstrates-a-perfect-introduction-in...", "snippet": "&quot;OK, now I don&#39;t want to alarm anybody in this <b>room</b>, but it&#39;s just come to my <b>attention</b> that the <b>person</b> to your right is a liar.&quot; OK, Pam; you&#39;ve got us. But wait!", "dateLastCrawled": "2022-02-02T08:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Psychologists Point Out 11 Clothing Colors That <b>Reveal Your Personality</b> ...", "url": "https://brightside.me/inspiration-psychology/psychologists-say-these-11-clothing-colors-reveal-your-personality-458660/", "isFamilyFriendly": true, "displayUrl": "https://brightside.me/inspiration-psychology/psychologists-say-these-11-clothing...", "snippet": "Any bright shades of red draw <b>attention</b> to the <b>person</b> wearing them. People tend to associate the vibrant color with energy, movement, and excitement. Psychologists from the University of Rochester found that men are more attracted to women wearing a ruby tint. &quot;Red is a stimulant for men,&quot; says Abby Calisch, a psychology professor at Eastern Virginia Medical School in Norfolk, Virginia. Those who often wear red are bright, easily excitable, slightly self-centered and also addiction-prone. 5 ...", "dateLastCrawled": "2022-02-03T05:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "5 Behaviors You Notice When <b>Someone Is Romantically Attracted To You</b>", "url": "https://www.powerofpositivity.com/behaviors-reveal-someone-romantically-attracted/", "isFamilyFriendly": true, "displayUrl": "https://www.powerofpositivity.com/behaviors-reveal-someone-romantically-attracted", "snippet": "The question is whether the <b>person</b> you\u2019re romantically attracted to notices those signs or not. But that also extends to you \u2013 do you notice when someone has feelings for you? Or are you oblivious to it? Admittedly, most of us don\u2019t go out of our way to look for signs of attraction. On top of that, many of them are very subtle, giving you no reason to pay any <b>attention</b> to them. Here Are 5 Hidden Behaviors That Reveal <b>Someone Is Romantically Attracted To You</b> 1. Facial Changes Can Show ...", "dateLastCrawled": "2022-02-02T08:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Disorders of <b>Thought</b> Are Severe Mood Disorders: the Selective <b>Attention</b> ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2632389/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2632389", "snippet": "Stimulus 7 is shown as a dashed line of external input because the <b>room</b> may have been warm. Only 5 and 9 are verbalized. Although there are connections to each <b>thought</b> based on the patient&#39;s report, the failure to filter and prioritize causes stimuli to come so fast and demand <b>attention</b> (apparently based on most recent order) that there is not enough time to verbalize all of them. An observer hears only \u201cmom,\u201d \u201cpyramids,\u201d and wanting water, concluding there has been a \u201cblockage of ...", "dateLastCrawled": "2022-02-02T23:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How <b>Selective Attention</b> Works - Verywell Mind", "url": "https://www.verywellmind.com/what-is-selective-attention-2795022", "isFamilyFriendly": true, "displayUrl": "https://www.verywellmind.com/what-is-<b>selective-attention</b>-2795022", "snippet": "Several factors <b>can</b> influence <b>selective attention</b> in spoken messages. The location from where the sound originates <b>can</b> play a role. For example, you are probably more likely to pay <b>attention</b> to a conversation taking place right next to you rather than one several feet away. In his text, &quot;The Psychology of <b>Attention</b>,&quot; psychology professor Harold Pashler notes that simply presenting messages to different ears will not lead to the selection of one message over the other. The two messages must ...", "dateLastCrawled": "2022-02-02T16:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "10 <b>Things Narcissists Do To Be The Center of Attention</b>", "url": "https://www.powerofpositivity.com/things-narcissists-do/", "isFamilyFriendly": true, "displayUrl": "https://www.powerofpositivity.com/things-narcissists-do", "snippet": "Their self-absorbed behavior is hard to miss. Of course, being the center of <b>attention</b> is especially important to a narcissist. So, here are 10 things you may have noticed a narcissist do to <b>be the center of attention</b>. Engaging in a conversation with a narcissist <b>can</b> leave someone feeling like banging their head against a brick wall.", "dateLastCrawled": "2022-02-03T06:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Excessive <b>Attention</b>-Seeking and Drama Addiction | <b>Psychology Today</b>", "url": "https://www.psychologytoday.com/us/blog/obesely-speaking/201411/excessive-attention-seeking-and-drama-addiction", "isFamilyFriendly": true, "displayUrl": "https://<b>www.psychologytoday.com</b>/us/blog/obesely-speaking/201411/excessive-<b>attention</b>...", "snippet": "Without getting and giving <b>attention</b>, you could not have a social species. Getting <b>attention</b> is necessary for life\u2019s vital enterprises and <b>can</b> be the difference between life and death in a ...", "dateLastCrawled": "2021-11-19T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What Does a Messy <b>Room</b> Say About Your Personality", "url": "https://www.verywellmind.com/psychology-of-a-messy-room-4171244", "isFamilyFriendly": true, "displayUrl": "https://www.verywellmind.com/psychology-of-a-messy-<b>room</b>-4171244", "snippet": "If you struggle to stay on task, it <b>can</b> be hard to devote the time and <b>attention</b> needed to keep things tidy. So while you might notice that the <b>room</b> is messy and have intentions to clean it up, finding the concentration and energy levels needed to do the task <b>can</b> feel difficult or even impossible. If you suspect that your messy <b>room</b> might be a sign of problems in your life or a result of depression, don\u2019t be afraid to reach out. Talk about what is going on with your doctor or consult a ...", "dateLastCrawled": "2022-02-03T00:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "20 Ways To Get A Noisy Classroom&#39;s <b>Attention</b>", "url": "https://www.teachthought.com/pedagogy/noisy-classroom/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.teachthought.com</b>/pedagogy/noisy-class<b>room</b>", "snippet": "Stand at the front of the <b>room</b> and say out loud, &quot;Clap once if you hear me, clap twice if you hear me,&quot; while modeling the clap. ... but that doesn\u2019t change the fact that for many of you, simply beginning class <b>can</b> be the most challenging thing you do all day. It\u2019s not easy. My go-to for years what to simply start teaching, somewhat quietly, and hope students caught on, but I found that stressed some students who were trying to hear and couldn\u2019t, so I had to come up with different ...", "dateLastCrawled": "2022-02-03T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Stages <b>and Skills in a Counselling Session</b> - UKEssays.com", "url": "https://www.ukessays.com/essays/psychology/stages-and-skills-in-a-counselling-session.php", "isFamilyFriendly": true, "displayUrl": "https://www.ukessays.com/essays/psychology/stages-<b>and-skills-in-a-counselling-session</b>.php", "snippet": "Although the <b>room</b> was not ideal in its set up due to the limitations of availability I inspected the classroom to satisfy basic Health &amp; Safety requirements such as checking that the doors could be closed fully and that the <b>room</b> would be without interruption. The lighting was not adjustable although it was a suitable, comfortable level. I ensured the blinds adjacent to the seating arrangements were closed and closed the windows to avoid traffic noise. I then arranged the speaker-listener ...", "dateLastCrawled": "2022-02-02T22:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "15 Things Only a <b>Person With ADHD Will Understand</b>", "url": "https://www.lifehack.org/351419/20-things-only-person-with-adhd-will-understand", "isFamilyFriendly": true, "displayUrl": "https://<b>www.lifehack.org</b>/351419/20-things-only", "snippet": "We\u2019ve all walked into the <b>room</b> to search for something, only to forget what we were searching for in the first place. For people diagnosed with ADHD, it is a common occurrence along with many other symptoms that make ordinary life a challenge. <b>Attention</b> Deficit Hyperactivity Disorder or ADHD is a learning disorder that affects people from the age of 6 years and above. Individuals affected by ADHD face challenges focusing on tasks, organizing things, and following instructions. Here are a ...", "dateLastCrawled": "2022-01-29T10:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How Many Seconds to a First Impression? \u2013 Association for Psychological ...", "url": "https://www.psychologicalscience.org/observer/how-many-seconds-to-a-first-impression", "isFamilyFriendly": true, "displayUrl": "https://www.psychologicalscience.org/observer/how-many-seconds-to-a-first-impre", "snippet": "And having a face that looks competent (as opposed to trustworthy or likeable) may matter a lot in whether a <b>person</b> gets elected to public office. Willis and Todorov conducted separate experiments to study judgments from facial appearance, each focusing on a different trait: attractiveness, likeability, competence, trustworthiness, and aggressiveness. Participants were shown photographs of unfamiliar faces for 100 milliseconds (1/10 of a second), 500 milliseconds (half a second), or 1,000 ...", "dateLastCrawled": "2022-01-30T02:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Your Brain Can Only Take So Much Focus</b>", "url": "https://hbr.org/2017/05/your-brain-can-only-take-so-much-focus", "isFamilyFriendly": true, "displayUrl": "https://<b>hbr.org</b>/2017/05/<b>your-brain-can-only-take-so-much-focus</b>", "snippet": "It <b>can</b> drain your energy and make you lose self-control. This energy drain <b>can</b> also make you more impulsive and less helpful. As a result, decisions are poorly <b>thought</b>-out, and you become less ...", "dateLastCrawled": "2022-01-29T06:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Attention span</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Attention_span", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Attention_span</b>", "snippet": "<b>Attention</b> is also increased if the <b>person</b> is able to perform the task fluently, <b>compared</b> <b>to a person</b> who has difficulty performing the task, or to the same <b>person</b> when they are just learning the task. Fatigue, hunger, noise, and emotional stress reduce the time focused on the task. Common estimates for sustained <b>attention</b> to a freely chosen task range from about 5 minutes for a two-year-old child, to a maximum of around 20 minutes in older children and adults. However, there are many studies ...", "dateLastCrawled": "2022-01-30T12:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Focus: <b>Attention</b> Science: The Role of <b>Attention</b> in Learning in the ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6430174/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6430174", "snippet": "Damage to the <b>attention</b> network <b>can</b> result in remarkable attentional deficits. For example, following a stroke in the right side of the parietal cortex, a brain area particularly involved in the voluntary allocation of <b>attention</b>, the stroke patient <b>can</b> lose awareness of visual information toward the left side of space. As a result, the patient may only eat food on the right side of a plate, and fail to read words on the left side of a page", "dateLastCrawled": "2022-02-02T06:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How <b>Selective Attention</b> Works - Verywell Mind", "url": "https://www.verywellmind.com/what-is-selective-attention-2795022", "isFamilyFriendly": true, "displayUrl": "https://www.verywellmind.com/what-is-<b>selective-attention</b>-2795022", "snippet": "Several factors <b>can</b> influence <b>selective attention</b> in spoken messages. The location from where the sound originates <b>can</b> play a role. For example, you are probably more likely to pay <b>attention</b> to a conversation taking place right next to you rather than one several feet away. In his text, &quot;The Psychology of <b>Attention</b>,&quot; psychology professor Harold Pashler notes that simply presenting messages to different ears will not lead to the selection of one message over the other. The two messages must ...", "dateLastCrawled": "2022-02-02T16:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Patient Safety in the Surgical Environment | <b>ACOG</b>", "url": "https://www.acog.org/clinical/clinical-guidance/committee-opinion/articles/2010/09/patient-safety-in-the-surgical-environment", "isFamilyFriendly": true, "displayUrl": "https://<b>www.acog.org</b>/clinical/clinical-guidance/committee-opinion/articles/2010/09/...", "snippet": "ABSTRACT: Ensuring patient safety in the operating <b>room</b> begins before the patient enters the operative suite and includes <b>attention</b> to all applicable types of preventable medical errors (including, for example, medication errors), but surgical errors are unique to this environment. Steps to prevent wrong-site, wrong-<b>person</b>, wrong-procedure errors, or retained foreign objects have been recommended, starting with structured communication between the patient, the surgeon(s), and other members ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Stanford study finds walking improves creativity", "url": "https://news.stanford.edu/2014/04/24/walking-vs-sitting-042414/", "isFamilyFriendly": true, "displayUrl": "https://news.stanford.edu/2014/04/24/walking-vs-sitting-042414", "snippet": "A <b>person</b> walking indoors \u2013 on a treadmill <b>in a room</b> facing a blank wall \u2013 or walking outdoors in the fresh air produced twice as many creative responses <b>compared</b> <b>to a person</b> sitting down, one ...", "dateLastCrawled": "2022-02-02T23:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What Does a Messy <b>Room</b> Say About Your Personality", "url": "https://www.verywellmind.com/psychology-of-a-messy-room-4171244", "isFamilyFriendly": true, "displayUrl": "https://www.verywellmind.com/psychology-of-a-messy-<b>room</b>-4171244", "snippet": "If you struggle to stay on task, it <b>can</b> be hard to devote the time and <b>attention</b> needed to keep things tidy. So while you might notice that the <b>room</b> is messy and have intentions to clean it up, finding the concentration and energy levels needed to do the task <b>can</b> feel difficult or even impossible. If you suspect that your messy <b>room</b> might be a sign of problems in your life or a result of depression, don\u2019t be afraid to reach out. Talk about what is going on with your doctor or consult a ...", "dateLastCrawled": "2022-02-03T00:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "10 Ways Great Speakers Capture People&#39;s <b>Attention</b> | <b>Inc.com</b>", "url": "https://www.inc.com/sims-wyeth/how-to-capture-and-hold-audience-attention.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.inc.com</b>/sims-wyeth/how-to-capture-and-hold-audience-<b>attention</b>.html", "snippet": "He meant that you <b>can</b> capture <b>attention</b> if you remind an audience of a felt need, a pain point, or a threat to their well-being. &quot;Ring around the collar,&quot; was a 1968 ad in which a housewife ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Elevate Your Presence in a <b>Virtual</b> Meeting", "url": "https://hbr.org/2020/04/how-to-elevate-your-presence-in-a-virtual-meeting", "isFamilyFriendly": true, "displayUrl": "https://<b>hbr.org</b>/2020/04/how-to-elevate-your-presence-in-a-<b>virtual</b>-meeting", "snippet": "Using a loud voice will also keep you from mumbling and from speaking too quickly due to the amount of breath required. 3. Frame yourself wisely. Proximity plays a big part in how audiences ...", "dateLastCrawled": "2022-02-02T16:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The Mere Presence of Your Smartphone Reduces Brain Power ... - UT News", "url": "https://news.utexas.edu/2017/06/26/the-mere-presence-of-your-smartphone-reduces-brain-power/", "isFamilyFriendly": true, "displayUrl": "https://news.utexas.edu/2017/06/26/the-mere-presence-of-your-smartphone-reduces-brain...", "snippet": "In another experiment, researchers looked at how a <b>person</b>\u2019s self-reported smartphone dependence \u2014 or how strongly a <b>person</b> feels he or she needs to have a smartphone in order to get through a typical day \u2014 affected cognitive capacity. Participants performed the same series of computer-based tests as the first group and were randomly assigned to keep their smartphones either in sight on the desk face up, in a pocket or bag, or in another <b>room</b>. In this experiment, some participants were ...", "dateLastCrawled": "2022-01-29T03:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Telephone vs. In-<b>Person Interviews: Advantages and Disadvantages</b>", "url": "https://www.recruiter.com/i/telephone-vs-in-person-interviews-advantages-and-disadvantages/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.recruiter.com</b>/i/telephone-vs-in-<b>person-interviews-advantages-and-disadvantages</b>", "snippet": "Focus all your <b>attention</b> on the call, avoid multitasking, and project a positive attitude. Less time to sell yourself: Phone interviews are generally much shorter than in-<b>person</b> interviews. You do not have all the time in the world, but you <b>can</b> still make a sound impression. Learn to sell yourself in 30 seconds and leave the employer wanting more. In-<b>Person</b> Interviews. Face-to-face interviews are formal meetings that happen in <b>person</b> between the hiring manager and the candidate. They are ...", "dateLastCrawled": "2022-02-03T06:46:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Explaining <b>Machine Learning</b> in Layman\u2019s Terms | by Shivam Kollur ...", "url": "https://towardsdatascience.com/explaining-machine-learning-in-laymans-terms-9b92284bdad4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/explaining-<b>machine-learning</b>-in-laymans-terms-9b92284bdad4", "snippet": "This sort of explanation is becomin g more and more important as \u201c<b>machine learning</b>\u201d gains <b>attention</b> as a buzzword. Very rarely will audience members pretend to know how <b>machine learning</b> algorithms will work, and very frequently will they expect an explanation that the average person can understand and relate to. This makes it essential to be able to break down both <b>machine learning</b> as a concept and individual algorithms into digestible pieces. The simplest way to deliver these manageable ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Frontiers | <b>Attention in Psychology, Neuroscience, and Machine Learning</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029", "snippet": "<b>Attention</b> is the important ability to flexibly control limited computational resources. It has been studied in conjunction with many other topics in neuroscience and psychology including awareness, vigilance, saliency, executive control, and <b>learning</b>. It has also recently been applied in several domains in <b>machine</b> <b>learning</b>. The relationship between the study of biological <b>attention</b> and its use as a tool to enhance artificial neural networks is not always clear. This review starts by ...", "dateLastCrawled": "2022-02-03T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning Analogy for Meditation (illustrated</b>) - LessWrong 2.0 ...", "url": "https://www.greaterwrong.com/posts/chHhuCvmZqYLM32gz/machine-learning-analogy-for-meditation-illustrated", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/chHhuCvmZqYLM32gz/<b>machine</b>-<b>learning</b>-<b>analogy</b>-for...", "snippet": "<b>Machine Learning Analogy for Meditation (illustrated</b>) ... as a way to pay <b>attention</b> to \u201cthoughts which lead to action\u201d. There are several reasons why it might be interesting to pay <b>attention</b> to thoughts which lead to action. 1. \u201cWhere\u2019s the steering wheel on this thing, anyway?\u201d [picture: confusing car dashboard] If you\u2019re experiencing \u201cmotivational issues\u201d, then it stands to reason that it might be useful to keep an eye on which thoughts are leading to actions and which are ...", "dateLastCrawled": "2022-01-17T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A mind-boggling <b>analogy</b> between <b>machine</b> <b>learning</b> and quantum physics", "url": "https://www.hhyu.org/posts/fermion/", "isFamilyFriendly": true, "displayUrl": "https://www.hhyu.org/posts/fermion", "snippet": "A recent paper published in PNAS titled \u201cThe Fermi-Dirac distribution provides a calibrated probabilistic output for binary classifiers\u201d caught my <b>attention</b>, because it describes a surprising relationship between <b>machine</b> <b>learning</b> and quantum physics. In fact, surprising is an understatement. Mind-boggling is more like it. According to the <b>analogy</b> developed by the authors, positive samples in binary classification problems are like&amp;mldr; fermions?!", "dateLastCrawled": "2022-01-23T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The simplest explanation of <b>machine learning</b> you\u2019ll ever read | HackerNoon", "url": "https://hackernoon.com/the-simplest-explanation-of-machine-learning-youll-ever-read-bebc0700047c", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/the-simplest-explanation-of-<b>machine-learning</b>-youll-ever-read...", "snippet": "At its core, <b>machine learning</b> is just a thing-labeler, taking your description of something and telling you what label it should get. Which sounds much less interesting than what you read on Hacker News. But would you have gotten excited enough to read about this topic if we\u2019d called it thing-labeling in the first place? Probably not, which goes to show that a bit of marketing and dazzle can be useful for getting this technology the <b>attention</b> it deserves (though not for the reasons you ...", "dateLastCrawled": "2022-01-29T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Why <b>Machine Learning Is A Metaphor For</b> Life \u2013 Adit Deshpande ...", "url": "https://adeshpande3.github.io/Why-Machine-Learning-is-a-Metaphor-For-Life", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/Why-<b>Machine-Learning-is-a-Metaphor-For</b>-Life", "snippet": "Another cool <b>analogy</b> is that of the epsilon greedy policy. This is a term used in reinforcement <b>learning</b> to fight the problem of exploration vs exploitation. The basic idea is that the RL agent will take a random action (instead of the optimal action according to its current policy) with probability \u03b5, in hope of searching a larger area of the state space, and eventually getting a better reward.", "dateLastCrawled": "2022-01-31T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "Understanding <b>Attention</b> In Deep <b>Learning</b>. (I apologize for these blatant self-advertisements, but seriously give it a read. It will help you under Transformers much better. I promise. ) <b>Attention</b> allowed us to focus on parts of our input sequence while we predicted our output sequence. If our model predicted the word \u201crouge\u201d [French translation for the color red], we are very likely to find a high weight-age for the word \u201cred\u201d in our input sequence. So <b>attention</b>, in a way, allowed us ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Positional Encoding for <b>Machine</b> <b>Learning</b> Attention | James D. McCaffrey", "url": "https://jamesmccaffrey.wordpress.com/2020/09/04/positional-encoding-for-machine-learning-attention/", "isFamilyFriendly": true, "displayUrl": "https://jamesmccaffrey.wordpress.com/2020/09/04/positional-encoding-for-<b>machine</b>...", "snippet": "The term \u201cattention\u201d in <b>machine</b> <b>learning</b> is a very general term. One common example is in natural language processing (NLP) where sentences are being processed, for example to translate English to German, or to construct a short summary of a news article. If you process each sentence one word at a time, you can get pretty good results using an LSTM recurrent neural network. But you can get better results if you compute an attention value for each pair of words in each sentence. The ...", "dateLastCrawled": "2022-01-07T06:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequence Modeling with Neural Networks (Part</b> 2): Attention Models - <b>Indico</b>", "url": "https://indicodata.ai/blog/sequence-modeling-neural-networks-part2-attention-models/", "isFamilyFriendly": true, "displayUrl": "https://<b>indico</b>data.ai/blog/sequence-modeling-neural-networks-part2-attention-models", "snippet": "Going back to our dinner example, <b>attention is like</b> choosing a dish to smell and predicting its contents instead of smelling everything at once. Implementing attention is a straightforward modification to our language model. We start by encoding the input sequence with an RNN and hold onto each state it produces. During the decoding phase, we take the state of the decoder network, combine it with the encoder states, and pass this combination to a feedforward network. The feedforward network ...", "dateLastCrawled": "2022-02-01T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Attention as Adaptive Tf-Idf for Deep Learning</b> \u2013 Data Exploration", "url": "http://xplordat.com/2019/07/22/attention-as-adaptive-tf-idf-for-deep-learning/", "isFamilyFriendly": true, "displayUrl": "xplordat.com/2019/07/22/<b>attention-as-adaptive-tf-idf-for-deep-learning</b>", "snippet": "<b>Attention is like</b> <b>tf-idf for deep learning</b>. Both attention and tf-idf boost the importance of some words over others. But while tf-idf weight vectors are static for a set of documents, the attention weight vectors will adapt depending on the particular classification objective. Attention derives larger weights for those words that are influencing the classification objective, thus opening a window into the decision making process with in the deep <b>learning</b> blackbox\u2026 Pay attention! So your ...", "dateLastCrawled": "2022-01-30T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Why your <b>attention is like</b> a piece of contested territory - Gradient Flow", "url": "https://gradientflow.com/why-your-attention-is-like-a-piece-of-contested-territory/", "isFamilyFriendly": true, "displayUrl": "https://gradientflow.com/why-your-<b>attention-is-like</b>-a-piece-of-contested-territory", "snippet": "Why your <b>attention is like</b> a piece of contested territory. Posted by Ben Lorica February 28, 2019 Posted in AI, Data Science Tags: data show, podcast [A version of this post appears on the O\u2019Reilly Radar.] The O\u2019Reilly Data Show Podcast: P.W. Singer on how social media has changed, war, politics, and business. In this episode of the Data Show, I spoke with P.W. Singer, strategist and senior fellow at the New America Foundation, and a contributing editor at Popular Science. He is co ...", "dateLastCrawled": "2022-01-12T15:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Why your attention is like</b> <b>a piece of contested territory</b> \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/radar/podcast/why-your-attention-is-like-a-piece-of-contested-territory/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/radar/podcast/<b>why-your-attention-is-like</b>-a-piece-of-contested...", "snippet": "<b>Why your attention is like</b> a piece of contested territoryData Show Podcast. In this episode of the Data Show, I spoke with P.W. Singer, strategist and senior fellow at the New America Foundation, and a contributing editor at Popular Science. He is co-author of an excellent new book, LikeWar: The Weaponization of Social Media, which explores how ...", "dateLastCrawled": "2021-12-02T04:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>O\u2019Reilly Data Show Podcast</b> \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/radar/topics/oreilly-data-show-podcast/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/radar/topics/<b>oreilly-data-show-podcast</b>", "snippet": "Why your <b>attention is like</b> a piece of contested territory . By Ben Lorica. The <b>O\u2019Reilly Data Show Podcast</b>: P.W. Singer on how social media has changed, war, politics, and business. The technical, societal, and cultural challenges that come with the rise of fake media . By Ben Lorica. The <b>O\u2019Reilly Data Show Podcast</b>: Siwei Lyu on <b>machine</b> <b>learning</b> for digital media forensics and image synthesis. Using <b>machine</b> <b>learning</b> and analytics to attract and retain employees . By Ben Lorica. The O ...", "dateLastCrawled": "2022-01-30T23:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Annotated Bibliography</b> \u2013 Berryville Institute of <b>Machine</b> <b>Learning</b>", "url": "https://berryvilleiml.com/references/", "isFamilyFriendly": true, "displayUrl": "https://berryvilleiml.com/references", "snippet": "\u201cUnderspecification Presents Challenges for Credibility in Modern <b>Machine Learning. ... Attention is like</b> a hopfield layer. Representation; Rendell 2010 \u2014 Insights from the Social <b>Learning</b> Strategies Tournament. Rendell, Luke, Robert Boyd, Daniel Cownden, Marquist Enquist, Kimmo Eriksson, Marc W. Feldman, Laurel Fogarty, Stefano Ghirlanda, Timothy Lillicrap, and Kevin N. Laland. \u201cWhy copy others? Insights from the social <b>learning</b> strategies tournament.\u201d Science 328, no. 5975 (2010 ...", "dateLastCrawled": "2022-01-31T00:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Cognitive Psychology: 10 Most Insightful Experiments", "url": "https://www.spring.org.uk/2021/09/cognitive-psychology.php", "isFamilyFriendly": true, "displayUrl": "https://www.spring.org.uk/2021/09/cognitive-psychology.php", "snippet": "<b>Attention is like</b> a spotlight. We actually have two sets of eyes \u2014 one set real and one virtual, cognitive psychology finds. We have the real eyes moving around in their sockets, but we also have \u2018virtual eyes\u2019 looking around our field of vision, choosing what we pay attention to. People are using their virtual eyes all the time: for example, when they watch each other using their peripheral vision. You don\u2019t need to look directly at an attractive stranger to eye them up, you can ...", "dateLastCrawled": "2022-02-02T22:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "GPT-J-6B: 6B JAX-Based Transformer \u2013 Aran Komatsuzaki", "url": "https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/", "isFamilyFriendly": true, "displayUrl": "https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j", "snippet": "While the model knows what <b>attention is like</b>, it does not know its precise mechanism as in theorem proving. Geese. Prompt ----- Fun Facts About Geese: 1. Geese have impressive visual capabilities! ----- Output: They can see a human face and react to it. 2. Geese produce a lot of noise! They can be heard and seen flying over 100 miles away! 3. Geese are the biggest birds of prey in the world! They have an average wingspan of 45 inches. 4. Geese can fly 1,000 miles per hour! They can travel up ...", "dateLastCrawled": "2022-02-01T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How to Get</b> Out of Your Head | <b>Psychology Today</b>", "url": "https://www.psychologytoday.com/us/blog/get-out-your-mind/201911/how-get-out-your-head", "isFamilyFriendly": true, "displayUrl": "https://<b>www.psychologytoday.com</b>/us/blog/get-out-your-mind/201911/how-get-out-your-head", "snippet": "Training our <b>attention is like</b> <b>learning</b> how to use a high-tech flashlight. We can broaden the beam to highlight a wide area, or we can narrow it to a concentrated beam, depending on the demands of ...", "dateLastCrawled": "2021-11-07T07:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A review on the attention mechanism of <b>deep learning</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "snippet": "The global <b>attention is similar</b> to soft attention. The local attention can be viewed as an interesting blend between the hard and soft attention, in which only a subset of source words are considered at a time. This approach is computationally less expensive than global attention or soft attention. At the same time, unlike hard attention, this approach is differentiable almost everywhere, making it easier to implement and train. 3.2. Forms of input feature. The attention mechanisms can be ...", "dateLastCrawled": "2022-02-02T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The fall of RNN / <b>LSTM</b>. We fell for Recurrent neural networks\u2026 | by ...", "url": "https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-fall-of-rnn-<b>lstm</b>-2d1594c74ce0", "snippet": "Also let us not forget <b>machine</b> translation, ... Note 1: Hierarchical neural <b>attention is similar</b> to the ideas in WaveNet. But instead of a convolutional neural network we use hierarchical attention modules. Also: Hierarchical neural attention can be also bi-directional. Note 2: RNN and <b>LSTM</b> are memory-bandwidth limited problems (see this for details). The processing unit(s) need as much memory bandwidth as the number of operations/s they can provide, making it impossible to fully utilize ...", "dateLastCrawled": "2022-02-01T10:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Attention</b> Mechanism in Neural Networks", "url": "https://devopedia.org/attention-mechanism-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://devopedia.org/<b>attention</b>-mechanism-in-neural-networks", "snippet": "In <b>machine</b> translation, the encoder-decoder architecture is common. The encoder reads a sequence of words and represents it with a high-dimensional real-valued vector. This vector, often called the context vector, is given to the decoder, which then generates another sequence of words in the target language. If the input sequence is very long, a single vector from the encoder doesn&#39;t give enough information for the decoder.", "dateLastCrawled": "2022-02-03T04:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Attention? Attention!", "url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "isFamilyFriendly": true, "displayUrl": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "snippet": "The global <b>attention is similar</b> to the soft attention, while the local one is an interesting blend between hard and soft, an improvement over the hard attention to make it differentiable: the model first predicts a single aligned position for the current target word and a window centered around the source position is then used to compute a context vector. Fig. 8. Global vs local attention (Image source: Fig 2 &amp; 3 in Luong, et al., 2015) Neural Turing Machines. Alan Turing in 1936 proposed a ...", "dateLastCrawled": "2022-01-29T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Illustrated: <b>Self-Attention</b>. A step-by-step guide to <b>self-attention</b> ...", "url": "https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/illustrated-<b>self-attention</b>-2d627e33b20a", "snippet": "Fig. 1.4: Calculating <b>attention</b> scores (blue) from query 1. To obtain <b>attention</b> scores, we start with taking a dot product between Input 1\u2019s query (red) with all keys (orange), including itself.Since there are 3 key representations (because we have 3 inputs), we obtain 3 <b>attention</b> scores (blue). [0, 4, 2] [1, 0, 2] x [1, 4, 3] = [2, 4, 4] [1, 0, 1] Notice that we only use the query from Input 1. Later we\u2019ll work on repeating this same step for the other querys.. Note The above operation ...", "dateLastCrawled": "2022-02-02T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Interpreting network knowledge with attention mechanism</b> for bearing ...", "url": "https://www.sciencedirect.com/science/article/pii/S1568494620307675", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1568494620307675", "snippet": "Nowadays, artificial intelligence and <b>machine</b> <b>learning</b> make fault diagnosis gradually become intelligent, and data-driven intelligent algorithms are receiving more and more attention. However, many methods use the existing deep <b>learning</b> models directly for the analysis of mechanical vibration signals, which is still lack of interpretability to researchers. In this paper, a method based on multilayer bidirectional gated recurrent units with attention mechanism is proposed to access the ...", "dateLastCrawled": "2022-01-16T11:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep <b>Learning</b> <b>Image Feature Recognition</b> Algorithm for Judgment on the ...", "url": "https://www.hindawi.com/journals/complexity/2021/9921095/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/complexity/2021/9921095", "snippet": "This paper uses an improved deep <b>learning</b> algorithm to judge the rationality of the design of landscape <b>image feature recognition</b>. The preprocessing of the image is proposed to enhance the data. The deficiencies in landscape feature extraction are further addressed based on the new model. Then, the two-stage training method of the model is used to solve the problems of long training time and convergence difficulties in deep <b>learning</b>. Innovative methods for zoning and segmentation training of ...", "dateLastCrawled": "2022-02-02T03:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Invited Review - arXiv", "url": "https://arxiv.org/pdf/2004.05809.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2004.05809.pdf", "snippet": "until the introduction of deep <b>learning</b> into MT. Since 2014, neural <b>machine</b> translation (NMT) based on deep neural net-works has quickly developed[8,45,116,122]. In 2016, through extensive experiments on various language pairs, [65,143]demonstrated that NMT has made a big break- through and obtained remarkable improvements compared to SMT, and even approached human-level translation quality [55]. This article attempts to give a review of NMT frame-work, discusses some challenging research ...", "dateLastCrawled": "2021-08-08T23:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Towards the end of <b>deep learning</b> and the beginning of AGI | Towards ...", "url": "https://towardsdatascience.com/towards-the-end-of-deep-learning-and-the-beginning-of-agi-d214d222c4cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/towards-the-end-of-<b>deep-learning</b>-and-the-beginning-of...", "snippet": "And <b>just as attention</b> mechanisms have revolutionized the <b>deep learning</b> field in recent years, so is attention key in how our brain is <b>learning</b> these models. But if our neocortex is making a very large amount of predictions constantly, and adapting to any misalignments between its models and what it perceives, why don\u2019t we notice all of those predictions and instead we perceive one continuous reality? Let\u2019s get there, step by step. Painting by the author Javier Ideami@ideami.com. Through ...", "dateLastCrawled": "2022-02-03T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning</b> to monitor and regulate collective thinking processes ...", "url": "https://link.springer.com/article/10.1007/s11412-018-9270-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11412-018-9270-5", "snippet": "<b>Just as attention</b> poses problems for individual regulation, it too affects and further complicates regulation of group cognition. Collaborative activities pose large demands on attention, as individuals must pay attention to their own thoughts and behaviors, to those of others, to interactions, to developing joint attention in coordination with differing goals, and to the products to be completed. Time constraints can also push groups to focus attention on completing products at the expense ...", "dateLastCrawled": "2022-01-29T19:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "So retrieval is what we needed? \u2013 Towards AI \u2014 The World\u2019s Leading AI ...", "url": "https://towardsai.net/p/l/so-retrieval-is-what-we-needed", "isFamilyFriendly": true, "displayUrl": "https://towardsai.net/p/l/so-retrieval-is-what-we-needed", "snippet": "$5.99 (as of January 20, 2022 06:40 GMT -05:00 - More info Product prices and availability are accurate as of the date/time indicated and are subject to change. Any price and availability information displayed on [relevant Amazon Site(s), as applicable] at the time of purchase will apply to the purchase of this product. %site_host% is a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for sites to earn commission fees by ...", "dateLastCrawled": "2022-01-21T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Summarization", "url": "https://summarization.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://summarization.fastforwardlabs.com", "snippet": "<b>Machine</b> <b>learning</b> does not remove the need for domain expertise altogether. Engineers still need to know which attributes might help. Supervised <b>machine</b> <b>learning</b> will tell you whether sentences with numbers make better summaries, but it won\u2019t give you the idea to try that feature in the first place. The difficult, expensive process of inventing, designing, and implementing features for the model to use is called feature engineering. When engineers have the domain expertise required to ...", "dateLastCrawled": "2022-01-28T22:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "So retrieval is what we needed?. Last month DeepMind published their ...", "url": "https://pub.towardsai.net/so-retrieval-is-what-we-needed-9485e4f9e939", "isFamilyFriendly": true, "displayUrl": "https://pub.towardsai.net/so-retrieval-is-what-we-needed-9485e4f9e939", "snippet": "The concept of retrieval (in NLP models) is not new and has been suggested for NLU models in several papers including \u201cImproving Neural Language Models with a Continuous Cache\u201d by Grave et al. and Retrieval Augmented Generation from Meta-AI or the in the work of <b>machine</b> translation that retrieves translation pairs based on edit distance between source sentences and guide the translation output using the closest retrieved target sentences such as Zhang et al. (2018) and Gu et al. (2018).", "dateLastCrawled": "2022-02-02T03:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Improving Observation Skills</b> | CCMIT", "url": "https://ccmit.mit.edu/observation/", "isFamilyFriendly": true, "displayUrl": "https://ccmit.mit.edu/observation", "snippet": "Improving your observation skills allows you to \u201clisten\u201d with more than just your ears and make better decisions. It also enhances your ability to interact with others and to respond in an appropriate manner. Both are keys to success at work and at home.", "dateLastCrawled": "2022-01-29T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Netnography</b> - ResearchGate", "url": "https://www.researchgate.net/publication/319613944_Netnography", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319613944_<b>Netnography</b>", "snippet": "Abstract. <b>Netnography</b> is a specific approach to conducting ethnography on the internet. It is a qualitative, interpretive research methodology that adapts traditional ethnographic techniques to ...", "dateLastCrawled": "2022-02-02T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A psychology of the film | Humanities and Social Sciences Communications", "url": "https://www.nature.com/articles/s41599-018-0111-y/", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41599-018-0111-y", "snippet": "The cinema as a cultural institution has been studied by academic researchers in the arts and humanities. At present, cultural media studies are the home to the aesthetics and critical analysis of ...", "dateLastCrawled": "2022-01-30T09:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>P. Adaptation Practice</b> - <b>Learning</b> tools &amp; flashcards, for free | <b>Quizlet</b>", "url": "https://quizlet.com/79985855/p-adaptation-practice-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/79985855/<b>p-adaptation-practice</b>-flash-cards", "snippet": "Airways, a suction <b>machine</b>, and oxygen also should be available. If the client is to undergo induction of labor, oxytocin infusion solution can be obtained at a later time. Tongue blades are not necessary. However, the emergency cart should be placed nearby in case the client experiences a seizure. The ultrasound <b>machine</b> may be used at a later point to provide information about the fetus. In many hospitals, the client with severe preeclampsia is admitted to the labor area, where she and the ...", "dateLastCrawled": "2021-11-14T12:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Kevin Foley is back on familiar turf | Shropshire Star", "url": "https://www.shropshirestar.com/sport/football/wolverhampton-wanderers-fc/2021/12/30/kevin-foley-is-back-on-familiar-turf/", "isFamilyFriendly": true, "displayUrl": "https://www.shropshirestar.com/sport/football/wolverhampton-wanderers-fc/2021/12/30/...", "snippet": "Kevin Foley is back in the country, back on his old stomping ground and hoping to get back in the game. The Foley family, including his Auntie, Uncle and cousins from New York. Foley and Collins ...", "dateLastCrawled": "2022-02-03T13:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Introduction to Neural Network - <b>Machine</b> <b>Learning</b> | AI | Data Science ...", "url": "https://www.superdatascience.com/blogs/introduction-to-neural-network", "isFamilyFriendly": true, "displayUrl": "https://www.superdatascience.com/blogs/introduction-to-neural-network", "snippet": "Practically speaking, <b>attention can be thought of as</b> a matrix that, for a given set of input symbols and output symbols, stores a weight that governs how much the words have to do with each other. Attentional mechanisms capture the idea that words, on their own, are not enough to communicate meaning. The context of words in a sentence matters and the units of meaning communicated by a sentence do not cleanly map onto individual words. Semantic content, Word-sense disambiguation, intent, tone ...", "dateLastCrawled": "2021-12-18T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Summarize COVID-19 literature with GPT2</b> \u2013 Vincent Kieuvongngam \u2013 PhD ...", "url": "https://vincentk1991.github.io/GPT2-summarizer/", "isFamilyFriendly": true, "displayUrl": "https://vincentk1991.github.io/GPT2-summarizer", "snippet": "The <b>attention can be thought of as</b> a vector of importance weights, i.e. how strongly the tokens in the input sequences are correlated with the ouput tokens. To visualize the attention, we input the sequence illustrated in table 1and plot the attention as matrix of alignment heatmap. To see what the model learns, we compare the attention before and after the training. Note here that the total unique structures are 6*12 = 64, i.e. 6 decoder layers, each with 12 attention heads. For the sake of ...", "dateLastCrawled": "2022-01-29T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Paper Summary: Neural <b>Machine</b> Translation by Jointly <b>Learning</b> to Align ...", "url": "https://medium.com/@parthakayal1729/paper-summary-neural-machine-translation-by-jointly-learning-to-align-and-translate-a50e802135e9", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@parthakayal1729/paper-summary-neural-<b>machine</b>-translation-by...", "snippet": "The attention-based model learns to assign significance to different parts of the input for each step of the output. In the context of translation, <b>attention can be thought of as</b> \u201calignment ...", "dateLastCrawled": "2021-10-15T08:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introducing a Bayesian model of selective attention based on active ...", "url": "https://www.nature.com/articles/s41598-019-50138-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-019-50138-8", "snippet": "<b>Learning</b> the locations of the features in Yarbus\u2019 task. (A) ... <b>attention can be thought of as</b> the precision of sensory signals given their hidden causes. We appealed to this aspect of active ...", "dateLastCrawled": "2022-02-03T04:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Revolving Door For <b>Machine</b> <b>Learning</b> Models | by Ori Cohen | Towards ...", "url": "https://towardsdatascience.com/the-revolving-door-for-machine-learning-models-14bdfc870906", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-revolving-door-for-<b>machine</b>-<b>learning</b>-models-14bdfc870906", "snippet": "It seems that every subfield in <b>machine</b> <b>learning</b> is a derivative of other fields, however, the true \u201cpicture\u201d is much more complex. This beautiful schema should probably have connecting edges from each subfield to every other subfield. Let\u2019s see how these algorithms &amp; ideas travel between fields. Let\u2019s start with the basics. Nearly every function can be used as a preprocessing method, feature engineering, augmentation, model, or as ensembles. Regression, Classification &amp; Ranking ...", "dateLastCrawled": "2022-01-24T19:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "NLP and Deep <b>Learning</b> - SlideShare", "url": "https://www.slideshare.net/ramaseshanr/nlp-and-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ramaseshanr/nlp-and-deep-<b>learning</b>", "snippet": "In the context of translation, <b>attention can be thought of as</b> &quot;alignment.&quot; Bahdanau et al. argue that the attention scores \u03b1ij at decoding step i signify then words in the source sentence that align with word i in the target. Noting this, we can use attention scores to build an alignment table It is a table mapping of words in the source to corresponding words in the target sentence - based on the learned encoder and decoder from our Seq2Seq NMT system. Neural Network Deep <b>Learning</b> For NLP ...", "dateLastCrawled": "2022-01-25T16:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Revolving Door For <b>Machine</b> <b>Learning</b> Models \u2013 Ramsey Elbasheer ...", "url": "https://ramseyelbasheer.io/2021/08/12/the-revolving-door-for-machine-learning-models/", "isFamilyFriendly": true, "displayUrl": "https://ramseyelbasheer.io/2021/08/12/the-revolving-door-for-<b>machine</b>-<b>learning</b>-models", "snippet": "Once you acquire the basics in statistics, probability theory, information theory, mathematics, algorithms, and <b>machine</b> <b>learning</b>, you realize that you can reuse nearly every algorithm for various purposes and use cases. I claim that methodologies and techniques are not directly attached to a single research field, in fact, a vast amount of methods are used interchangeably across research fields. Core Concepts . Simplifying problems, in order to select the right tools. To understand the ...", "dateLastCrawled": "2021-12-29T16:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Multi\u2010dimensional weighted cross\u2010attention network in crowded scenes ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12298", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12298", "snippet": "In the design of attention mechanisms for visual tasks, <b>attention can be thought of as</b> selecting a small amount of important information from a large amount of information and focusing on this critical information. To obtain more detailed information about the desired target and to suppress other useless information. In this way, limited attention resources can quickly filter out high-value information from a large amount of information, significantly improving visual information processing ...", "dateLastCrawled": "2022-01-15T09:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Effectiveness of Large Batch Training for Neural <b>Machine</b> Translation ...", "url": "https://infohub.delltechnologies.com/p/effectiveness-of-large-batch-training-for-neural-machine-translation-with-intel-xeon/", "isFamilyFriendly": true, "displayUrl": "https://infohub.delltechnologies.com/p/effectiveness-of-large-batch-training-for...", "snippet": "The field of <b>machine</b> language translation is rapidly shifting from statistical <b>machine</b> <b>learning</b> models to efficient neural network architecture designs which can dramatically improve translation quality. However, training a better performing Neural <b>Machine</b> Translation (NMT) model still takes days to weeks depending on the hardware, size of the training corpus and the model architecture. Improving the time-to-solution for NMT training will be crucial if these approaches are to achieve ...", "dateLastCrawled": "2022-02-01T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Cognition Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/400768623/cognition-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/400768623/cognition-flash-cards", "snippet": "Weak AI: Doesn&#39;t matter how they do it, as long as they do; <b>machine</b> <b>learning</b>; which involves brute force and creating behaviours that are human like Strong AI: Machines that act this way are thinking, not simulating thinking This enables us to intelligent systems to model the human brain. Alan Turing, Turing Test. Turing Test: Understanding what it would mean if a computer could think and how we could assess it, it was basically an imitation game; Get a human examiner who has a conversation ...", "dateLastCrawled": "2020-01-13T00:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Explaining Explanations: <b>An Approach to Evaluating Interpretability of</b> ...", "url": "https://deepai.org/publication/explaining-explanations-an-approach-to-evaluating-interpretability-of-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/explaining-explanations-an-approach-to-evaluating...", "snippet": "For example, network <b>attention can be compared to</b> human attention , and disentangled representations can be tested on synthetic datasets that have known latent variables, to determine whether those variables are recovered. Finally, systems that are trained explicitly to generate human-readable explanations can be tested by similarity to test sets, or by human evaluation. One of the difficulties of evaluating explanatory power of explanation-producing systems is that, since the system itself ...", "dateLastCrawled": "2022-01-03T03:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>machine</b> <b>learning</b> interpretability: a survey on methods and metrics", "url": "https://eboacademyinternational.com/blog/machine-learning-interpretability%3A-a-survey-on-methods-and-metrics-029650", "isFamilyFriendly": true, "displayUrl": "https://eboacademyinternational.com/blog/<b>machine</b>-<b>learning</b>-interpretability:-a-survey-on...", "snippet": "Metrics for Evaluating <b>Machine</b> <b>Learning</b> Algorithms. neural networks,\u201d Get the week&#39;s most popular data science and artificial intelligence research sent straight to your inbox every Saturday. autonomous robot experience.\u201d For example, representation layers are characterized according to their ability to serve as feature input for a transfer problem, and both Network Dissection representation units and Concept Activation Vectors are measured according to their ability to detect or ...", "dateLastCrawled": "2021-11-02T14:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "PSYCH C120 - <b>Learning</b> tools &amp; flashcards, for free | <b>Quizlet</b>", "url": "https://quizlet.com/95121835/psych-c120-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/95121835/psych-c120-flash-cards", "snippet": "-&quot;proof positive that a <b>machine</b> could perform tasks heretofor considered intelligent, creative and uniquely human.&quot; September 11, 1956. Symposium on Information Theory at MIT.First AI computer program -- pivotal in terms of the emergence of CognitivePsychology &amp; Cognitve Science. &quot;Thinking <b>machine</b>.&quot; Logic Theorist could prove logical theorems in a way that resembled human performance. Newell &amp; Simon were leaders in building close ties between AI and the new cognitive psychology. We will ...", "dateLastCrawled": "2020-10-01T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Frontiers | Situational Understanding in the Human and the <b>Machine</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fnsys.2021.786252/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fnsys.2021.786252", "snippet": "The Air Force research programs envision developing AI technologies that will ensure battlespace dominance, by radical increases in the speed of battlespace understanding and decision-making. In the last half century, advances in AI have been concentrated in the area of <b>machine</b> <b>learning</b>. Recent experimental findings and insights in systems neuroscience, the biophysics of cognition, and other disciplines provide converging results that set the stage for technologies of <b>machine</b> understanding ...", "dateLastCrawled": "2022-02-02T20:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Explaining Explanations: An Overview of Interpretability of <b>Machine</b> ...", "url": "https://www.academia.edu/52341400/Explaining_Explanations_An_Overview_of_Interpretability_of_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/52341400/Explaining_Explanations_An_Overview_of...", "snippet": "Explaining Explanations: An Overview of Interpretability of <b>Machine</b> <b>Learning</b>. 2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA) Ayesha Bajwa. Download PDF. Download Full PDF Package. This paper. A short summary of this paper. 37 Full PDFs related to this paper. READ PAPER. Explaining Explanations: An Overview of Interpretability of <b>Machine</b> <b>Learning</b>. Download . Explaining Explanations: An Overview of Interpretability of <b>Machine</b> <b>Learning</b>. Ayesha Bajwa ...", "dateLastCrawled": "2022-01-28T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Cognitive Psychology: Connecting Mind, Research and</b> Everyday ... - SILO.PUB", "url": "https://silo.pub/cognitive-psychology-connecting-mind-research-and-everyday-experience.html", "isFamilyFriendly": true, "displayUrl": "https://silo.pub/<b>cognitive-psychology-connecting-mind-research-and</b>-everyday-experience...", "snippet": "By <b>learning</b> many different lists at retention intervals ranging from 19 minutes to 31 days, Ebbinghaus was able to plot the \u201cforgetting curve\u201d in Figure 1.6, which shows savings as a function of retention interval. Ebbinghaus\u2019s experiments were important because they provided a way to quantify memory and therefore plot functions like the forgetting curve that describe the operation of the mind. Notice that although Ebbinghaus\u2019s savings method was very different from Donders ...", "dateLastCrawled": "2022-02-02T19:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Global Insight from Crown Chakra Dynamics in 3D?", "url": "https://www.laetusinpraesens.org/docs10s/chakra.php", "isFamilyFriendly": true, "displayUrl": "https://www.laetusinpraesens.org/docs10s/chakra.php", "snippet": "<b>Learning</b> processes: As in any creative process, the result viewed, imagined as an objective, distracts from the process by which it was achieved. In the case of 3D models, it is the result which is viewed in a few seconds and found to be interesting or otherwise. The process of achieving that result may take many tedious hours and can be considered to be of little interest. There is little trace of that process, except in the <b>learning</b> involved. In the case of the models presented here, it ...", "dateLastCrawled": "2021-12-19T12:42:00.0000000Z", "language": "en", "isNavigational": false}]], "all_bing_queries": ["+(attention)  is like +(a person in a room)", "+(attention) is similar to +(a person in a room)", "+(attention) can be thought of as +(a person in a room)", "+(attention) can be compared to +(a person in a room)", "machine learning +(attention AND analogy)", "machine learning +(\"attention is like\")", "machine learning +(\"attention is similar\")", "machine learning +(\"just as attention\")", "machine learning +(\"attention can be thought of as\")", "machine learning +(\"attention can be compared to\")"]}