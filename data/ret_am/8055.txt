{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding the <b>log</b> <b>loss function</b> | by Susmith Reddy | Analytics ...", "url": "https://medium.com/analytics-vidhya/understanding-the-loss-function-of-logistic-regression-ac1eec2838ce", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/understanding-the-<b>loss-function</b>-of-<b>log</b>istic...", "snippet": "Let\u2019s demystify \u201c<b>Log</b> <b>Loss Function</b>.\u201d. It is important to first understand the <b>log</b> function before jumping into <b>log</b> <b>loss</b>. If we plot y = <b>log</b> (x), the graph in quadrant II looks <b>like</b> this. y ...", "dateLastCrawled": "2022-01-30T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding binary <b>cross-entropy</b> / <b>log</b> <b>loss</b>: a visual explanation ...", "url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-binary-<b>cross-entropy</b>-<b>log</b>-<b>loss</b>-a-visual...", "snippet": "For a binary classification <b>like</b> our example, the typical <b>loss</b> function is the binary <b>cross-entropy</b> / <b>log</b> <b>loss</b>. <b>Loss</b> Function: Binary <b>Cross-Entropy</b> / <b>Log</b> <b>Loss</b>. If you look this <b>loss</b> function up, this is what you\u2019ll find: Binary <b>Cross-Entropy</b> / <b>Log</b> <b>Loss</b>. where y is the label (1 for green points and 0 for red points) and p(y) is the predicted probability of the point being green for all N points. Reading this formula, it tells you that, for each green point (y=1), it adds <b>log</b>(p(y)) to the ...", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding softmax <b>and the negative log-likelihood</b>", "url": "https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/", "isFamilyFriendly": true, "displayUrl": "https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-<b>negative</b>-<b>log</b>...", "snippet": "In practice, the softmax function is used in tandem with the <b>negative</b> <b>log</b>-likelihood (NLL). This <b>loss</b> function is very interesting if we interpret it in relation to the behavior of softmax. First, let\u2019s write down our <b>loss</b> function: \\[L(\\mathbf{y}) = -\\<b>log</b>(\\mathbf{y})\\] This is summed for all the correct classes. Recall that when training a model, we aspire to find the minima of a <b>loss</b> function given a set of parameters (in a neural network, these are the weights and biases). We can ...", "dateLastCrawled": "2022-02-02T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "python - <b>Trouble implementing custom Negative Log-Likelihood</b> <b>loss</b> in ...", "url": "https://stackoverflow.com/questions/58237023/trouble-implementing-custom-negative-log-likelihood-loss-in-pet-example", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/58237023", "snippet": "I am <b>using</b> a pet example to optimize the parameters of a dense layer connected to a softplus and outputing the parameters of a <b>negative</b> binomial distribution. My process has been: 1) Build a custom class for the <b>negative</b> <b>log</b> likelihood <b>loss</b> with forward and backward methods. It gets as input the distribution parameters and a target and outputs ...", "dateLastCrawled": "2022-01-16T05:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What happens when <b>loss</b> are <b>negative</b>? - PyTorch Forums", "url": "https://discuss.pytorch.org/t/what-happens-when-loss-are-negative/47883", "isFamilyFriendly": true, "displayUrl": "https://discuss.pytorch.org/t/what-happens-when-<b>loss</b>-are-<b>negative</b>/47883", "snippet": "Based on my understanding of back prop and <b>gradient</b> <b>descent</b>, <b>Loss</b> is multiplied to <b>gradient</b> when taking a step with <b>gradient</b> <b>descent</b>. So when <b>gradient</b> becomes <b>negative</b>, <b>gradient</b> <b>descent</b> takes a step in the opposite direction. Such idea is well captured when implementing <b>gradient</b> ascent, as it can simply be implemented by multiplying -1 to the <b>loss</b>. Then, what happens if my <b>loss</b> starts from positive and goes below zero? If what I said above is correct, training will go for minimization when...", "dateLastCrawled": "2022-02-02T08:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "A most commonly used method of finding the minimum point of function is \u201c<b>gradient</b> <b>descent</b>\u201d. Think of <b>loss</b> function <b>like</b> undulating mountain and <b>gradient</b> <b>descent</b> <b>is like</b> sliding down the mountain to reach the bottommost point. There is not a single <b>loss</b> function that works for all kind of data. It depends on a number of factors including the presence of outliers, choice of machine learning algorithm, time efficiency of <b>gradient</b> <b>descent</b>, ease of finding the derivatives and confidence of ...", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Reducing <b>Loss</b>: <b>Gradient</b> <b>Descent</b> | Machine Learning Crash Course ...", "url": "https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/reducing-<b>loss</b>/<b>gradient</b>-<b>descent</b>", "snippet": "Calculating the <b>loss function</b> for every conceivable <b>value</b> of \\(w_1\\) over the entire data set would be an inefficient way of finding the convergence point. Let&#39;s examine a better mechanism\u2014very popular in machine learning\u2014called <b>gradient</b> <b>descent</b>. The first stage in <b>gradient</b> <b>descent</b> is to pick a starting <b>value</b> (a starting point) for \\(w_1\\). The starting point doesn&#39;t matter much; therefore, many algorithms simply set \\(w_1\\) to 0 or pick a random <b>value</b>. The following figure shows that we ...", "dateLastCrawled": "2022-02-01T21:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Avoiding numerical overflow when calculating the <b>value</b> AND <b>gradient</b> of ...", "url": "https://stackoverflow.com/questions/20085768/avoiding-numerical-overflow-when-calculating-the-value-and-gradient-of-the-logis", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/20085768", "snippet": "I am specifically interested in calculating the average <b>value</b> and <b>gradient</b> of the Logistic function for given <b>value</b> of beta. The average <b>value</b> of the Logistic function w.r.t to a <b>value</b> of beta is: L = 1/N * sum(<b>log</b>(1+exp(X*beta)),1) The average <b>value</b> of the slope of the Logistic function w.r.t. to a <b>value</b> of b is:", "dateLastCrawled": "2022-01-26T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Calculating <b>Gradient Descent</b> Manually | by Chi-Feng Wang | Towards Data ...", "url": "https://towardsdatascience.com/calculating-gradient-descent-manually-6d9bee09aa0b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/calculating-<b>gradient-descent</b>-manually-6d9bee09aa0b", "snippet": "The graph would thus look something <b>like</b> this: Image 8: Max(0,z) // Source. Looking at that graph, we can immediately see that the derivative is a piecewise function: it\u2019s 0 for all values of z less than or equal to 0, and 1 for all values of z greater than 0, or: Image 9: Derivative of max(0,z) Now that we have both parts, we can multiply them together to get the derivative of our neuron: Image 10: Derivative with respect to the weights of our neuron: max(0, sum(w\u2297x)+b) And substitute z ...", "dateLastCrawled": "2022-02-02T05:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Support Vector Machines &amp; <b>Gradient</b> <b>Descent</b> - Machine Learning Blog", "url": "https://bitmask93.github.io/ml-blog/Support-Vector-Machines-Gradient-Descent/", "isFamilyFriendly": true, "displayUrl": "https://bitmask93.github.io/ml-b<b>log</b>/Support-Vector-Machines-<b>Gradient</b>-<b>Descent</b>", "snippet": "w = np. array ([1,-6]) #Initial Weights eta = 0.05 # learning rate alpha = 0.9 # momentum h = np. zeros_<b>like</b> (w) #This is an additional vector es = 0.0001 #For early stopping n_iter = 1000 #Number of iterations batch_size = 128 lambdh = 0.001 #To control the width of the margin <b>loss</b> = np. zeros (n_iter) #List to store the <b>loss</b> in each iteration for i in range (n_iter): ind = np. random. choice (X. shape [0], batch_size) <b>loss</b> [i] = hinge_<b>loss</b> (X [ind,:], y [ind], w, lambdh) h = alpha * h ...", "dateLastCrawled": "2022-02-01T19:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding the <b>log</b> <b>loss function</b> | by Susmith Reddy | Analytics ...", "url": "https://medium.com/analytics-vidhya/understanding-the-loss-function-of-logistic-regression-ac1eec2838ce", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/understanding-the-<b>loss-function</b>-of-<b>log</b>istic...", "snippet": "Let\u2019s demystify \u201c<b>Log</b> <b>Loss Function</b>.\u201d. It is important to first understand the <b>log</b> function before jumping into <b>log</b> <b>loss</b>. If we plot y = <b>log</b> (x), the graph in quadrant II looks like this. y ...", "dateLastCrawled": "2022-01-30T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An overview of the <b>Gradient Descent</b> algorithm | by Nishit Jain ...", "url": "https://towardsdatascience.com/an-overview-of-the-gradient-descent-algorithm-8645c9e4de1e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/an-overview-of-the-<b>gradient-descent</b>-algorithm-8645c9e4de1e", "snippet": "<b>Log</b> <b>Loss</b> (Cross-Entropy <b>Loss</b>) SVM <b>Loss</b> (Hinge <b>Loss</b>) Learning Rate: This is the hyperparameter that determines the steps the <b>gradient descent</b> algorithm takes. <b>Gradient Descent</b> is too sensitive to the learning rate. If it is too big, the algorithm may bypass the local minimum and overshoot. If it too small, it might increase the total computation time to a very large extent. We will see the effect of the learning rate in depth later in the article. <b>Gradient</b>: Basically, it is a measure of the ...", "dateLastCrawled": "2022-02-01T20:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Cross-entropy <b>loss</b> is often simply referred to as \u201ccross-entropy,\u201d \u201clogarithmic <b>loss</b>,\u201d \u201clogistic <b>loss</b>,\u201d or \u201c<b>log</b> <b>loss</b>\u201d for short. Each predicted probability is compared to the actual class output <b>value</b> (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected <b>value</b>.", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "python - <b>Trouble implementing custom Negative Log-Likelihood</b> <b>loss</b> in ...", "url": "https://stackoverflow.com/questions/58237023/trouble-implementing-custom-negative-log-likelihood-loss-in-pet-example", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/58237023", "snippet": "I am <b>using</b> a pet example to optimize the parameters of a dense layer connected to a softplus and outputing the parameters of a <b>negative</b> binomial distribution. My process has been: 1) Build a custom class for the <b>negative</b> <b>log</b> likelihood <b>loss</b> with forward and backward methods. It gets as input the distribution parameters and a target and outputs ...", "dateLastCrawled": "2022-01-16T05:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What happens when <b>loss</b> are <b>negative</b>? - PyTorch Forums", "url": "https://discuss.pytorch.org/t/what-happens-when-loss-are-negative/47883", "isFamilyFriendly": true, "displayUrl": "https://discuss.pytorch.org/t/what-happens-when-<b>loss</b>-are-<b>negative</b>/47883", "snippet": "Based on my understanding of back prop and <b>gradient</b> <b>descent</b>, <b>Loss</b> is multiplied to <b>gradient</b> when taking a step with <b>gradient</b> <b>descent</b>. So when <b>gradient</b> becomes <b>negative</b>, <b>gradient</b> <b>descent</b> takes a step in the opposite direction. Such idea is well captured when implementing <b>gradient</b> ascent, as it can simply be implemented by multiplying -1 to the <b>loss</b>. Then, what happens if my <b>loss</b> starts from positive and goes below zero? If what I said above is correct, training will go for minimization when...", "dateLastCrawled": "2022-02-02T08:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Logistic Regression</b> as a Neural Network | by Rochak Agrawal | Analytics ...", "url": "https://medium.com/analytics-vidhya/logistic-regression-as-a-neural-network-b5d2a1bd696f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>logistic-regression</b>-as-a-neural-network-b5d2a1bd696f", "snippet": "Let us study why this <b>loss</b> function is good for <b>logistic regression</b>, When y=1 the <b>loss</b> function equates to L(y\u2019,y) = -<b>log</b> y\u2019.As we want the <b>value</b> of <b>loss</b> function to be less, the <b>value</b> of <b>log</b> ...", "dateLastCrawled": "2022-01-27T09:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "One big problem in <b>using</b> MAE <b>loss</b> (for neural nets especially) is that its <b>gradient</b> is the same throughout, which means the <b>gradient</b> will be large even for small <b>loss</b> values. This isn\u2019t good for learning. To fix this, we can use dynamic learning rate which decreases as we move closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate. The <b>gradient</b> of MSE <b>loss</b> is high for larger <b>loss</b> values and decreases as <b>loss</b> approaches 0, making it more ...", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Loss Function</b> (Part II): <b>Logistic Regression</b> | by Shuyu Luo | Towards ...", "url": "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/optimization-<b>loss-function</b>-under-the-hood-part-ii-d20a...", "snippet": "<b>Similar</b> to <b>Gradient</b> <b>Descent</b>, we firstly take the partial derivative of J(\u03b8) that is the slope of J(\u03b8), and note it as f(\u03b8). Instead of decreasing \u03b8 by a certain chosen learning rate \u03b1 multiplied with f(\u03b8) , Newton\u2019s Method gets an updated \u03b8 at the point of intersection of the tangent line of f(\u03b8) at previous \u03b8 and x axis. After amount of iterations, Newton\u2019s Method will converge at f(\u03b8) = 0.", "dateLastCrawled": "2022-02-02T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "It turns out that if we\u2019re given a typical classification problem and a model \\(h_\\theta(x) = \\sigma(Wx_i + b)\\), we can show that (at least theoretically) the cross-entropy <b>loss</b> leads to quicker learning through <b>gradient</b> <b>descent</b> than the MSE <b>loss</b>. This is primarily due to the use of the sigmoid function. First, let\u2019s recall the <b>gradient</b> <b>descent</b> update rule:", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Support Vector Machines &amp; <b>Gradient</b> <b>Descent</b> - Machine Learning Blog", "url": "https://bitmask93.github.io/ml-blog/Support-Vector-Machines-Gradient-Descent/", "isFamilyFriendly": true, "displayUrl": "https://bitmask93.github.io/ml-b<b>log</b>/Support-Vector-Machines-<b>Gradient</b>-<b>Descent</b>", "snippet": "Here the Blue is the Hinge <b>Loss</b> and Green is 0\u20131 <b>loss</b>. Why can\u2019t we use 0\u20131 <b>Loss</b> in SVM instead of Hinge <b>Loss</b>? 0\u20131 <b>loss</b> function is flat so it doesn\u2019t converge well. Also, it is not differentiable at 0. SVM is based on solving an optimization problem that maximize the margin between classes. So in this context, a convex <b>loss</b> function ...", "dateLastCrawled": "2022-02-01T19:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding binary <b>cross-entropy</b> / <b>log</b> <b>loss</b>: a visual explanation ...", "url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-binary-<b>cross-entropy</b>-<b>log</b>-<b>loss</b>-a-visual...", "snippet": "If you are training a binary classifier, chances are you are <b>using</b> binary <b>cross-entropy</b> / <b>log</b> <b>loss</b> as your <b>loss</b> function. Have you ever <b>thought</b> about what exactly does it mean to use this <b>loss</b> function? The thing is, given the ease of use of today\u2019s libraries and frameworks, it is very easy to overlook the true meaning of the <b>loss</b> function used. Motivation. I was looking for a blog post that wo u ld explain the concepts behind binary <b>cross-entropy</b> / <b>log</b> <b>loss</b> in a visually clear and concise ...", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What happens when <b>loss</b> are <b>negative</b>? - PyTorch Forums", "url": "https://discuss.pytorch.org/t/what-happens-when-loss-are-negative/47883", "isFamilyFriendly": true, "displayUrl": "https://discuss.pytorch.org/t/what-happens-when-<b>loss</b>-are-<b>negative</b>/47883", "snippet": "the <b>loss</b> to a smaller (that is, algebraically more <b>negative</b>) <b>value</b>. You could replace your <b>loss</b> with. modified <b>loss</b> = conventional <b>loss</b> - 2 * Pi. and you should get the exact same training results and model performance (except that all values of your <b>loss</b> will be shifted down by 2 * Pi). It is the case that we often use <b>loss</b> functions that become equal to zero when the fit of the model to the training data is perfect, but the optimization algorithms don\u2019t care about this, and they drive ...", "dateLastCrawled": "2022-02-02T08:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>TensorFlow newbie creates a neural</b> net <b>with a negative log likelihood</b> ...", "url": "https://fairyonice.github.io/Create-a-neural-net-with-a-negative-log-likelihood-as-a-loss.html", "isFamilyFriendly": true, "displayUrl": "https://fairyonice.github.io/Create-a-neural-net-<b>with-a-negative-log-likelihood-as</b>-a...", "snippet": "Then we minimize the <b>negative</b> <b>log</b>-likelihood criterion, instead of <b>using</b> MSE as a <b>loss</b>: N L L = \u2211 i <b>log</b> ( \u03c3 2 ( x i)) 2 + ( y i \u2212 \u03bc ( x i)) 2 2 \u03c3 2 ( x i) Notice that when \u03c3 2 ( x i) = 1, the first term of NLL becomes constant, and this <b>loss</b> function becomes essentially the same as the MSE. By modeling \u03c3 2 ( x i), in theory, our model ...", "dateLastCrawled": "2022-02-01T13:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions (cont.) and <b>Loss</b> Functions for Energy Based Models ...", "url": "https://atcold.github.io/pytorch-Deep-Learning/en/week11/11-2/", "isFamilyFriendly": true, "displayUrl": "https://atcold.github.io/pytorch-<b>Deep-Learning</b>/en/week11/11-2", "snippet": "Introduction to <b>Gradient</b> <b>Descent</b> and Backpropagation Algorithm ... <b>Negative</b> <b>Log</b>-Likelihood <b>Loss</b> \\[L_{nll}(W, S) = \\frac{1}{P} \\sum_{i=1}^P (E(W, Y^i, X^i) + \\frac{1}{\\beta} \\<b>log</b> \\int_{y \\in \\mathcal{Y}} e^{\\beta E(W, y, X^i)})\\] This <b>loss</b> function pushes down on the energy of the correct answer while pushing up on the energies of all answers in proportion to their probabilities. This reduces to the perceptron <b>loss</b> when $\\beta \\rightarrow \\infty$. It has been used for a long time in many ...", "dateLastCrawled": "2022-02-02T21:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "neural network - How to interpret <b>loss</b> and accuracy for a machine ...", "url": "https://stackoverflow.com/questions/34518656/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/34518656", "snippet": "<b>Loss</b> <b>value</b> implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of <b>loss</b> after each, or several, iteration(s). The accuracy of a model is usually determined after the model parameters are learned and fixed and no learning is taking place. Then the test samples are fed to the model and the number of mistakes (zero-one <b>loss</b>) the model makes are recorded, after comparison to the true targets. Then the percentage of ...", "dateLastCrawled": "2022-02-02T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>loss</b>-functions-and-optimization-algorithms...", "snippet": "The choice of Optimisation Algorithms and <b>Loss</b> Functions for a deep learning model <b>can</b> play a big role in producing optimum and faster results. Before we begin, let us see how different components ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5.13 <b>Logistic regression</b> and regularization | Computational Genomics with R", "url": "https://compgenomr.github.io/book/logistic-regression-and-regularization.html", "isFamilyFriendly": true, "displayUrl": "https://compgenomr.github.io/book/<b>logistic-regression</b>-and-regularization.html", "snippet": "<b>Logistic regression</b> is very similar to linear regression as a concept and it <b>can</b> <b>be thought</b> of as a \u201cmaximum likelihood estimation\u201d problem where we are trying to find statistical parameters that maximize the likelihood of the observed data being sampled from the statistical distribution of interest. This is also very related to the general cost/<b>loss</b> function approach we see in supervised machine learning algorithms. In the case of binary response variables, the simple linear regression ...", "dateLastCrawled": "2022-01-30T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Cutting Your Losses: <b>Loss</b> Functions &amp; the Sum of <b>Squared</b> Errors <b>Loss</b> ...", "url": "https://dustinstansbury.github.io/theclevermachine/cutting-your-losses", "isFamilyFriendly": true, "displayUrl": "https://dustinstansbury.github.io/theclevermachine/cutting-your-<b>loss</b>es", "snippet": "A helpful interpretation of the SSE <b>loss function</b> is demonstrated in Figure 2.The area of each red square is a literal geometric interpretation of each observation\u2019s contribution to the overall <b>loss</b>. We see that no matter if the errors are positive <b>or negative</b> (i.e. actual \\(y_i\\) are located above or below the black line), the contribution to the <b>loss</b> is always an area, and therefore positive.", "dateLastCrawled": "2022-01-31T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Actor-<b>critic loss function in reinforcement learning</b> - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/321234/actor-critic-loss-function-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/321234/actor-critic-<b>loss</b>-function-in...", "snippet": "Show activity on this post. In actor-critic learning for reinforcement learning, I understand you have an &quot;actor&quot; which is deciding the action to take, and a &quot;critic&quot; that then evaluates those actions, however, I&#39;m confused on what the <b>loss</b> function is actually telling me. In Sutton and Barton&#39;s book page 274 (292 of the pdf) found here http ...", "dateLastCrawled": "2022-02-03T15:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - <b>NaN</b> <b>loss</b> when training regression network - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/37232782", "snippet": "For example, with quantile normalization, if an example is in the 60th percentile of the training set, it gets a <b>value</b> of 0.6. (You <b>can</b> also shift the quantile normalized values down by 0.5 so that the 0th percentile is -0.5 and the 100th percentile is +0.5). Add regularization, either by increasing the dropout rate or adding L1 and L2 penalties to the weights. L1 regularization is analogous to feature selection, and since you said that reducing the number of features to 5 gives good ...", "dateLastCrawled": "2022-01-27T22:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding the <b>log</b> <b>loss function</b> | by Susmith Reddy | Analytics ...", "url": "https://medium.com/analytics-vidhya/understanding-the-loss-function-of-logistic-regression-ac1eec2838ce", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/understanding-the-<b>loss-function</b>-of-<b>log</b>istic...", "snippet": "<b>Gradient</b> <b>descent</b>. The above two characteristics are significant for any <b>loss function</b>. If we have a convex curve, we <b>can</b> apply <b>Gradient</b> <b>Descent</b> Optimization Algorithm, and penalizing the far away ...", "dateLastCrawled": "2022-01-30T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Quick Introduction <b>to Loss</b> Functions in Machine Learning \u2013 hello ML", "url": "https://helloml.org/a-quick-introduction-to-loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://helloml.org/a-quick-introduction-<b>to-loss</b>-functions-in-machine-learning", "snippet": "In both cases, the <b>log</b>-<b>loss</b> is much higher as <b>compared</b> to the <b>log</b>-<b>loss</b> in the last figure. The last figure has a black-dashed line that shows the decision boundary. All the cases to the left side of the line are assigned a label \u201c0\u201d. Cost function. Before we proceed to the next section, a small note \u2013 while learning machine learning, you may come across a similar term called the cost function. There is a difference between a <b>loss</b> function and a cost function. The <b>loss</b> function ...", "dateLastCrawled": "2022-01-02T04:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why not Mean Squared Error(MSE) as a <b>loss</b> function for Logistic ...", "url": "https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-not-mse-as-a-<b>loss</b>-function-for-<b>logistic-regression</b>...", "snippet": "As seen above, <b>loss</b> <b>value</b> <b>using</b> MSE was much much less <b>compared</b> to the <b>loss</b> <b>value</b> computed <b>using</b> the <b>log</b> <b>loss</b> function. Hence it is very clear to us that MSE doesn\u2019t strongly penalize misclassifications even for the perfect mismatch! However, if there is a perfect match between predicted values and actual labels both the <b>loss</b> values would be \u201c0\u201d as shown below. Actual label: \u201c1\u201d Predicted: \u201c1\u201d MSE: (1 - 1)\u00b2 = 0. <b>Log</b> <b>loss</b>: -(1 * <b>log</b>(1) + 0 * <b>log</b>(0)) = 0. Here we have shown that ...", "dateLastCrawled": "2022-01-29T18:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Neural networks are trained <b>using</b> stochastic <b>gradient</b> <b>descent</b> and require that you choose a <b>loss</b> function when designing and configuring your model. There are many <b>loss</b> functions to choose from and it <b>can</b> be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when training a neural network. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "5. <b>Model Metrics</b> \u2014 Machine Learning 101 documentation", "url": "https://machinelearning101.readthedocs.io/en/latest/_pages/05_model_metrics.html", "isFamilyFriendly": true, "displayUrl": "https://machinelearning101.readthedocs.io/en/latest/_pages/05_<b>model_metrics</b>.html", "snippet": "5.3. <b>Log</b> <b>Loss</b>\u00b6. <b>Log</b> <b>loss</b>, also called logistic regression <b>loss</b> or cross-entropy <b>loss</b>, is defined on probability estimates.It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and <b>can</b> be used to evaluate the probability outputs of a model instead of its discrete predictions.", "dateLastCrawled": "2022-01-29T19:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss Functions -when to use which</b> one - Numpy Ninja", "url": "https://www.numpyninja.com/post/loss-functions-when-to-use-which-one", "isFamilyFriendly": true, "displayUrl": "https://www.numpyninja.com/post/<b>loss-functions-when-to-use-which</b>-one", "snippet": "1) Binary Cross Entropy-Logistic regression. If you are training a binary classifier, then you may be <b>using</b> binary cross-entropy as your <b>loss</b> function. Entropy as we know means impurity. The measure of impurity in a class is called entropy. SO <b>loss</b> here is defined as the number of the data which are misclassified.", "dateLastCrawled": "2022-01-26T23:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "One big problem in <b>using</b> MAE <b>loss</b> (for neural nets especially) is that its <b>gradient</b> is the same throughout, which means the <b>gradient</b> will be large even for small <b>loss</b> values. This isn\u2019t good for learning. To fix this, we <b>can</b> use dynamic learning rate which decreases as we move closer to the minima. MSE behaves nicely in this case and will converge even with a fixed learning rate. The <b>gradient</b> of MSE <b>loss</b> is high for larger <b>loss</b> values and decreases as <b>loss</b> approaches 0, making it more ...", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Gradient</b> <b>descent</b> with constant learning rate for a logistic <b>log</b>-<b>loss</b> ...", "url": "https://calculus.subwiki.org/wiki/Gradient_descent_with_constant_learning_rate_for_a_logistic_log-loss_function_of_one_variable", "isFamilyFriendly": true, "displayUrl": "https://calculus.subwiki.org/wiki/<b>Gradient</b>_<b>descent</b>_with_constant_learning_rate_for_a...", "snippet": "Setup. This page includes a detailed discussion of <b>gradient</b> <b>descent</b> with constant learning rate for the logistic <b>log</b>-<b>loss</b> function of one variable.. Function. Explicitly, the function is: where is the logistic function and denotes the natural logarithm. Explicitly, . Note that , so the above <b>can</b> be written as: (we avoid extremes of 0 and 1 because in the extreme case the optimum is at infinity).", "dateLastCrawled": "2021-12-11T04:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "neural network - How to interpret <b>loss</b> and accuracy for a machine ...", "url": "https://stackoverflow.com/questions/34518656/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/34518656", "snippet": "<b>Loss</b> <b>value</b> implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of <b>loss</b> after each, or several, iteration(s). The accuracy of a model is usually determined after the model parameters are learned and fixed and no learning is taking place. Then the test samples are fed to the model and the number of mistakes (zero-one <b>loss</b>) the model makes are recorded, after comparison to the true targets. Then the percentage of ...", "dateLastCrawled": "2022-02-02T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "It turns out that if we\u2019re given a typical classification problem and a model \\(h_\\theta(x) = \\sigma(Wx_i + b)\\), we <b>can</b> show that (at least theoretically) the cross-entropy <b>loss</b> leads to quicker learning through <b>gradient</b> <b>descent</b> than the MSE <b>loss</b>. This is primarily due to the use of the sigmoid function. First, let\u2019s recall the <b>gradient</b> <b>descent</b> update rule:", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machined Learnings: ML and OR: An <b>analogy</b> with cost-sensitive ...", "url": "http://www.machinedlearnings.com/2010/07/ml-and-or.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2010/07/ml-and-or.html", "snippet": "Nonetheless I&#39;ve been amusing myself by thinking about it, in particular trying to think about it from a <b>machine</b> <b>learning</b> reduction standpoint. The simplest well-understood reduction that I can think of which is analogous to supplying estimates to a linear program is the reduction of cost-sensitive multiclass classification (CSMC) to regression.", "dateLastCrawled": "2021-12-25T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "ML Concepts. A.I \u2014 Intelligence showed by machines which is common for humans <b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>.We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Why and How to use <b>Cross Entropy</b>. The fundamental reasons for ...", "url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-and-how-to-use-<b>cross-entropy</b>-4e983cbdd873", "snippet": "A convenient way to think of <b>log</b> <b>loss</b> is as follows: If the model predicts that an observation should be labeled 1 and assigns a high probability to that prediction, a high penalty will be incurred when the true label is 0. If the model had assigned a lower probability to that prediction, a lower penalty would have been incurred. The reason for taking the <b>log</b> of predicted probabilities goes back to the original formulation of entropy. Information Theory looks at entropy as a measure of ...", "dateLastCrawled": "2022-01-31T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Analogy</b> of <b>machine</b> <b>learning</b> and human thinking. [Colour online ...", "url": "https://researchgate.net/figure/Analogy-of-machine-learning-and-human-thinking-Colour-online_fig1_326306245", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/<b>Analogy</b>-of-<b>machine</b>-<b>learning</b>-and-human-thinking-Colour...", "snippet": "Download scientific diagram | <b>Analogy</b> of <b>machine</b> <b>learning</b> and human thinking. [Colour online.] from publication: Application of <b>machine</b>-<b>learning</b> methods in forest ecology: Recent progress and ...", "dateLastCrawled": "2021-06-14T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How To <b>Implement Logistic Regression</b> From Scratch in Python", "url": "https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>implement-logistic-regression</b>-stochastic-gradient...", "snippet": "Kick-start your project with my new book <b>Machine</b> <b>Learning</b> Algorithms From Scratch, including step-by-step tutorials and the Python source code files for all examples. Let\u2019s get started. Update Jan/2017: Changed the calculation of fold_size in cross_validation_split() to always be an integer. Fixes issues with Python 3. Update Mar/2018: Added alternate link to download the dataset as the original appears to have been taken down. Update Aug/2018: Tested and updated to work with Python 3.6 ...", "dateLastCrawled": "2022-02-02T07:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - What is the relation between a <b>loss</b> function and an ...", "url": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-loss-function-and-an-energy-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-<b>loss</b>...", "snippet": "A <b>loss</b> function is a function that measures the distance between the expected value and the actual value of a model (an example of a <b>loss</b> function is the cross entropy).. An energy function can be defined as a function that we want to minimise or maximise and it is a function of the variables of the system. It is referred to as &quot;energy function&quot; because it is often related or compared to the concept of &quot;energy&quot; in physics. These two expression seem to refer to the same concept.", "dateLastCrawled": "2022-01-17T15:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>All Unit MCQ questions of ML</b> \u2013 TheCodingShef", "url": "https://thecodingshef.com/all-unit-mcq-questions-of-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thecodingshef.com/<b>all-unit-mcq-questions-of</b>-<b>machine</b>-<b>learning</b>", "snippet": "Correct option is C. Choose the correct option regarding <b>machine</b> <b>learning</b> (ML) and artificial intelligence (AI) ML is a set of techniques that turns a dataset into a software. AI is a software that can emulate the human mind. ML is an alternate way of programming intelligent machines. All of the above. Correct option is D.", "dateLastCrawled": "2022-01-30T22:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Predicting the 2019 All-<b>NBA teams with machine learning</b> - <b>Dribble Analytics</b>", "url": "https://dribbleanalytics.blog/2019/03/ml-all-nba-predict/", "isFamilyFriendly": true, "displayUrl": "https://<b>dribbleanalytics</b>.blog/2019/03/ml-all-nba-predict", "snippet": "<b>Log loss is like</b> accuracy, but instead of analyzing the labeled predictions, it analyzes the prediction probabilities. This is particularly important given that we\u2019re more interested in the probabilities than we are in the actual labels. A \u201cperfect\u201d model will have a log loss of 0. The table below shows each model\u2019s log loss. Model Log loss; SVC: 0.416: RF: 0.416: KNN: 0.403: DNN: 0.43: The SVC and RF have the same log loss, while the KNN has the lowest. Next, let\u2019s look at the ...", "dateLastCrawled": "2022-01-04T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "[OC] Predicting the 2019 All-<b>NBA teams with machine learning</b> : nba", "url": "https://www.reddit.com/r/nba/comments/aw51j6/oc_predicting_the_2019_allnba_teams_with_machine/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../aw51j6/oc_predicting_the_2019_allnba_teams_with_<b>machine</b>", "snippet": "[OC] Predicting the 2019 All-<b>NBA teams with machine learning</b>. Original Content. This post has a lot of graphs. If you don&#39;t want to click on each one individually, they&#39;re all in an imgur album here. There is a tl;dr and summary infographic at the very end. Introduction . Last year, media members unanimously selected LeBron James to the All-NBA first team, giving him a record 12 All-NBA first team selections. However, given the Lakers recent struggles and LeBron&#39;s absence earlier in the ...", "dateLastCrawled": "2021-10-14T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting the 2019 All-NBA teams with <b>machine</b> <b>learning</b>", "url": "https://dribbleanalytics.blogspot.com/2019/03/ml-all-nba-predict.html", "isFamilyFriendly": true, "displayUrl": "https://dribbleanalytics.blogspot.com/2019/03/ml-all-nba-predict.html", "snippet": "Predicting the 2019 All-NBA teams with <b>machine</b> <b>learning</b> Get link; Facebook; Twitter; Pinterest; Email; Other Apps; March 01, 2019 There is a summary at the bottom if you want to skip to the results. Introduction Last year, media members unanimously selected LeBron James to the All-NBA first team, giving him a record 12 All-NBA first team selections. However, given the Lakers recent struggles and LeBron&#39;s absence earlier in the season, LeBron might miss not only the first team but also the ...", "dateLastCrawled": "2021-12-11T07:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What\u2019s considered a good Log <b>Loss</b> in <b>Machine</b> <b>Learning</b> ? | by Federico ...", "url": "https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@fzammito/whats-considered-a-good-log-<b>loss</b>-in-<b>machine</b>-<b>learning</b>-a529...", "snippet": "<b>Log Loss is similar</b> to the Accuracy, but it will favor models that distinguish more strongly the classes. Log <b>Loss</b> it useful to compare models not only on their output but on their probabilistic ...", "dateLastCrawled": "2022-01-30T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>What is an intuitive explanation for the log</b> loss function? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-for-the-log-loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-an-intuitive-explanation-for-the-log</b>-loss-function", "snippet": "Answer (1 of 8): To me an intuitive explanation is that minimizing the log loss equals minimizing the Kullback-Leibler divergence (Kullback\u2013Leibler divergence - Wikipedia) between the function you want to optimize (for example a neural network) and the true function that generates the data (from ...", "dateLastCrawled": "2022-01-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Key techniques for Evaluating <b>Machine</b> <b>Learning</b> models - Data Analytics", "url": "https://vitalflux.com/key-techniques-evaluating-machine-learning-models-performance/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/key-techniques-evaluating-<b>machine</b>-<b>learning</b>-models-performance", "snippet": "Log loss is used to evaluate the performance of classification <b>machine</b> <b>learning</b> models that are built using classification algorithms such as logistic regression, support vector <b>machine</b> (SVM), random forest, and gradient boosting. The idea behind the use of <b>Log loss is similar</b> to taking a base-e exponential or natural logarithm in order to compare model scores from high-value functions which may indicate poor <b>machine</b> <b>learning</b> model performance. The logarithmic loss value is defined as ...", "dateLastCrawled": "2022-01-31T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss In Machine Learning</b> - 02/2021 - Course f", "url": "https://www.coursef.com/loss-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.coursef.com/<b>loss-in-machine-learning</b>", "snippet": "<b>Log Loss is similar</b> to the Accuracy, but it will favor models that ... Two of the most popular loss functions in <b>machine</b> <b>learning</b> are the 0-1 loss function and the quadratic loss function. The 0-1 loss function is an indicator function that returns 1 when the target and output are not equal and zero otherwise: 0-1 Loss: The quadratic loss is a commonly used symmetric loss \u2026 161 People Used View all course \u203a\u203a Visit Site \u2039 1; 2 \u203a FAQs. Do online classes have tests? Not all online ...", "dateLastCrawled": "2021-02-08T01:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Diagnosing malaria from some symptoms: a <b>machine</b> <b>learning</b> approach and ...", "url": "https://link.springer.com/article/10.1007/s12553-020-00488-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12553-020-00488-5", "snippet": "<b>Machine</b> <b>learning</b> tools have become available in the diagnosis and prediction of diseases, thereby saving costs and improving the likelihood of survivorship, especially in some terminal diseases. In the case of infectious diseases, early diagnosis is highly needed in isolating the subjects to reduce the spread of the disease. Researchers continue to propose new data mining tools that help in the early diagnosis of diseases, reducing the mortality rate, and improving the quality of life of ...", "dateLastCrawled": "2021-12-03T05:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(log loss)  is like +(\"loss\" or negative value when using gradient descent)", "+(log loss) is similar to +(\"loss\" or negative value when using gradient descent)", "+(log loss) can be thought of as +(\"loss\" or negative value when using gradient descent)", "+(log loss) can be compared to +(\"loss\" or negative value when using gradient descent)", "machine learning +(log loss AND analogy)", "machine learning +(\"log loss is like\")", "machine learning +(\"log loss is similar\")", "machine learning +(\"just as log loss\")", "machine learning +(\"log loss can be thought of as\")", "machine learning +(\"log loss can be compared to\")"]}