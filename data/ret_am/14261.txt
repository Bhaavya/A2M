{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Meta Learning for Knowledge Distillation \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2106.04570/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.04570", "snippet": "We present Meta Learning for Knowledge Distillation (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the <b>teacher</b> model is fixed during training. We show the <b>teacher</b> network can learn to better transfer knowledge to the <b>student</b> network (i.e., learning to teach) with the feedback from the performance of the distilled <b>student</b> network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment ...", "dateLastCrawled": "2022-01-22T06:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning to Teach with Dynamic Loss Functions</b> | DeepAI", "url": "https://deepai.org/publication/learning-to-teach-with-dynamic-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-to-teach-with-dynamic-loss-functions</b>", "snippet": "Learning a good <b>teaching</b> model is not trivial, since on the one hand the <b>task</b>-specific objective is usually non-smooth w.r.t. <b>student</b> model outputs, and on the other hand the final evaluation of the <b>student</b> model is incurred on the dev set, disjoint with the training dataset where the <b>teaching</b> process actually happens. We design an efficient gradient based optimization algorithm to optimize <b>teacher</b> models. Specifically, to tackle the first challenge, we smooth the <b>task</b>-specific measure to ...", "dateLastCrawled": "2021-12-26T14:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The-Learning-Documentation-Project/ArxivEnglish.txt at master ...", "url": "https://github.com/gorlapraveen/The-Learning-Documentation-Project/blob/master/docs/ResearchResources/arxiv/ArxivEnglish.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gorlapraveen/The-Learning-Documentation-Project/blob/master/docs/...", "snippet": "Classical Structured Prediction Losses for <b>Sequence to Sequence</b> Learning (Sergey Edunov - 5 October, 2018) We also report new state of the art results on both IWSLT&#39;14 German-English translation as well as Gigaword abstractive summarization. On the larger WMT&#39;14 English-French translation <b>task</b>, sequence-level training achieves 41.5 BLEU which ...", "dateLastCrawled": "2022-01-13T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>deep-learning-nlp-rl-papers</b>/PAPERS2017.md at master - <b>GitHub</b>", "url": "https://github.com/madrugado/deep-learning-nlp-rl-papers/blob/master/PAPERS2017.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>madrugado/deep-learning-nlp-rl-papers</b>/blob/master/PAPERS2017.md", "snippet": "A Copy-Augmented <b>Sequence-to-Sequence</b> Architecture Gives Good Performance on <b>Task</b>-Oriented Dialogue. Authors: Mihail Eric, Christopher D. Manning. Abstract: <b>Task</b>-oriented dialogue focuses on conversational agents that participate in user-initiated dialogues on domain-specific topics. In contrast to chatbots, which simply seek to sustain open-ended meaningful discourse, existing <b>task</b>-oriented agents usually explicitly model user intent and belief states. This paper examines bypassing such an ...", "dateLastCrawled": "2022-01-15T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Jason</b> Eisner\u2019s publications", "url": "https://www.cs.jhu.edu/~jason/papers/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>cs.jhu.edu/~jason/papers</b>", "snippet": "Following the success of neural <b>sequence-to-sequence</b> models in the SIGMORPHON 2016 shared <b>task</b>, all but one of the submissions included a neural component. The results show that high performance can be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data. However, different biasing and data augmentation resulted in disjoint sets of inflected forms being predicted correctly, suggesting that there is ...", "dateLastCrawled": "2022-01-31T06:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>RESEARCH</b> METHODOLOGY - IHM Gwalior", "url": "http://ihmgwalior.net/pdf/research_methodology.pdf", "isFamilyFriendly": true, "displayUrl": "ihmgwalior.net/pdf/<b>research</b>_methodology.pdf", "snippet": "the <b>task</b> you are proposing since you need to do the work yourself. 5. Relevance: Ensure that your study adds to the existing body of knowledge, bridges current gaps and is useful in policy formulation. This will help you to sustain interest in the study. 6. Availability of data: Before finalizing the topic, make sure that data are available. 7.", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Accepted Papers | ISMIR 2021", "url": "https://ismir2021.ismir.net/papers/", "isFamilyFriendly": true, "displayUrl": "https://ismir2021.ismir.net/papers", "snippet": "This <b>sequence-to-sequence</b> approach simplifies transcription by jointly modeling audio features and language-<b>like</b> output dependencies, thus removing the need for <b>task</b>-specific architectures. These results point toward possibilities for creating new Music Information Retrieval models by focusing on dataset creation and labeling rather than custom model design. Neural Waveshaping Synthesis. Ben Hayes Charalampos Saitis Gyorgy Fazekas . Poster session 5. Abstract PDF . We present the Neural ...", "dateLastCrawled": "2022-01-31T19:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Learning to Make <b>Analogies by Contrasting Abstract Relational</b> ... - DeepAI", "url": "https://deepai.org/publication/learning-to-make-analogies-by-contrasting-abstract-relational-structure", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/learning-to-make-analogies-by-contrasting-abstract...", "snippet": "Second, we do not instantiate an explicit cognitive theory or <b>analogy</b>-<b>like</b> computation in our model architecture, but instead use this theoretical insight to inform the way in which the model is trained. 2 Analogies as high-level perception and structure mapping. Perhaps the best-known explanation of human analogical reasoning is Structure Mapping Theory (SMT) (Gentner, 1983). SMT emphasizes the distinction between two means of comparing domains of experience; <b>analogy</b> and similarity ...", "dateLastCrawled": "2021-12-06T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Can you build an AI <b>capable to understand a text</b> and answer questions ...", "url": "https://www.quora.com/Can-you-build-an-AI-capable-to-understand-a-text-and-answer-questions-about-it", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Can-you-build-an-AI-<b>capable-to-understand-a-text</b>-and-answer...", "snippet": "Answer (1 of 4): Yes I can, and have, but it took about 800 days to program and is far from perfect. For instance, <b>like</b> myself, it has the tendency to take \u201ccan you ...", "dateLastCrawled": "2022-01-14T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Teaching</b> a Massive Open Online Course on Natural Language Processing", "url": "https://www.readkong.com/page/teaching-a-massive-open-online-course-on-natural-language-1083544", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/<b>teaching</b>-a-massive-open-online-course-on-natural...", "snippet": "Page topic: &quot;<b>Teaching</b> a Massive Open Online Course on Natural Language Processing&quot;. Created by: Rose Burgess. Language: english.", "dateLastCrawled": "2022-01-17T01:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Learning to Teach with Dynamic Loss Functions</b>", "url": "https://proceedings.neurips.cc/paper/2018/file/8051a3c40561002834e59d566b7430cf-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2018/file/8051a3c40561002834e59d566b7430cf-Paper.pdf", "snippet": "dataset. Towards that end, <b>similar</b> to human <b>teaching</b>, the <b>teacher</b>, a parametric model, dynamically outputs different loss functions that will be used and optimized by its <b>student</b> model at different training stages. We develop an ef\ufb01cient learning method for the <b>teacher</b> model that makes gradient based optimization possible, exempt of the ineffective solutions such as policy optimization. We name our method as \u201c<b>learning to teach with dynamic loss functions</b>\u201d (L2T-DLF for short). Extensive ...", "dateLastCrawled": "2022-01-26T01:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Learning to Teach with Dynamic Loss Functions", "url": "https://arxiv.org/pdf/1810.12081.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1810.12081.pdf", "snippet": "dataset. Towards that end, <b>similar</b> to human <b>teaching</b>, the <b>teacher</b>, a parametric model, dynamically outputs different loss functions that will be used and optimized by its <b>student</b> model at different training stages. We develop an ef\ufb01cient learning method for the <b>teacher</b> model that makes gradient based optimization possible, exempt of the ineffective solutions such as policy optimization. We name our method as \u201clearning to teach with dynamic loss functions\u201d (L2T-DLF for short). Extensive ...", "dateLastCrawled": "2021-10-14T13:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Transformer to CNN: Label-scarce distillation for efficient text ...", "url": "https://deepai.org/publication/transformer-to-cnn-label-scarce-distillation-for-efficient-text-classification", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/transformer-to-cnn-label-scarce-distillation-for...", "snippet": "<b>Similar</b> to the larval-adult form <b>analogy</b> made in Hinton et al. ... from a strong <b>teaching</b> model. Further development of specialized <b>student</b> architectures could similarly surpass <b>teacher</b> performance if appropriately designed to leverage the knowledge gained from a pretrained, <b>task</b>-agnostic <b>teacher</b> model whilst optimizing for <b>task</b>-specific constraints. References. Mikolov et al. [2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words ...", "dateLastCrawled": "2022-01-19T21:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Jason</b> Eisner\u2019s publications", "url": "https://www.cs.jhu.edu/~jason/papers/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>cs.jhu.edu/~jason/papers</b>", "snippet": "Following the success of neural <b>sequence-to-sequence</b> models in the SIGMORPHON 2016 shared <b>task</b>, all but one of the submissions included a neural component. The results show that high performance can be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data. However, different biasing and data augmentation resulted in disjoint sets of inflected forms being predicted correctly, suggesting that there is ...", "dateLastCrawled": "2022-01-31T06:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>deep-learning-nlp-rl-papers</b>/PAPERS2017.md at master - <b>GitHub</b>", "url": "https://github.com/madrugado/deep-learning-nlp-rl-papers/blob/master/PAPERS2017.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>madrugado/deep-learning-nlp-rl-papers</b>/blob/master/PAPERS2017.md", "snippet": "A Copy-Augmented <b>Sequence-to-Sequence</b> Architecture Gives Good Performance on <b>Task</b>-Oriented Dialogue. Authors: Mihail Eric, Christopher D. Manning. Abstract: <b>Task</b>-oriented dialogue focuses on conversational agents that participate in user-initiated dialogues on domain-specific topics. In contrast to chatbots, which simply seek to sustain open-ended meaningful discourse, existing <b>task</b>-oriented agents usually explicitly model user intent and belief states. This paper examines bypassing such an ...", "dateLastCrawled": "2022-01-15T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>RESEARCH</b> METHODOLOGY", "url": "http://ihmgwalior.net/pdf/research_methodology.pdf", "isFamilyFriendly": true, "displayUrl": "ihmgwalior.net/pdf/<b>research</b>_methodology.pdf", "snippet": "the <b>task</b> you are proposing since you need to do the work yourself. 5. Relevance: Ensure that your study adds to the existing body of knowledge, bridges current gaps and is useful in policy formulation. This will help you to sustain interest in the study. 6. Availability of data: Before finalizing the topic, make sure that data are available. 7.", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Learning to Make <b>Analogies by Contrasting Abstract Relational</b> ... - DeepAI", "url": "https://deepai.org/publication/learning-to-make-analogies-by-contrasting-abstract-relational-structure", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/learning-to-make-analogies-by-contrasting-abstract...", "snippet": "The High-Level Perception (HLP) theory of <b>analogy</b> (Chalmers et al., 1992; Mitchell, 1993) instead construes <b>analogy</b> as a function of tightly-interacting perceptual and reasoning processes, positing that the creation of stimulus representations and the alignment of those representations are mutually dependent. For example, when making an <b>analogy</b> between the sea and acoustics, we might represent certain perceptual features (the fact that the sea appears to be moving) and ignore others (the ...", "dateLastCrawled": "2021-12-06T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Transformer to CNN: Label-scarce distillation for ef\ufb01cient text ...", "url": "https://openreview.net/pdf?id=HJxM3hftiX", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=HJxM3hftiX", "snippet": "<b>Similar</b> to the larval-adult form <b>analogy</b> made in [11], high-capacity models with <b>task</b>-agnostic pre-training may be well-suited for <b>task</b> mastery on small datasets (which are common in industry). On the other hand, convolutional <b>student</b> architectures may be more ideal for practical applications by taking advantage of massively parallel computation and a signi\ufb01cantly reduced memory footprint. Our results suggest that the proposed BlendCNN architecture can ef\ufb01ciently achieve higher scores on ...", "dateLastCrawled": "2022-01-28T10:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A study of text-to-speech (TTS) <b>in children&#39;s english learning</b>", "url": "https://www.researchgate.net/publication/281751417_A_study_of_text-to-speech_TTS_in_children's_english_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/281751417_A_study_of_text-to-speech_TTS_in...", "snippet": "To find out if a <b>similar</b> relationship can be found among English-speaking kindergartners, we gave 68 kindergartners a writing <b>task</b> and two oral-segmentation tasks <b>similar</b> to those used by Vernon ...", "dateLastCrawled": "2021-09-19T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Can you build an AI <b>capable to understand a text</b> and answer questions ...", "url": "https://www.quora.com/Can-you-build-an-AI-capable-to-understand-a-text-and-answer-questions-about-it", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Can-you-build-an-AI-<b>capable-to-understand-a-text</b>-and-answer...", "snippet": "Answer (1 of 4): Yes I can, and have, but it took about 800 days to program and is far from perfect. For instance, like myself, it has the tendency to take \u201ccan you ...", "dateLastCrawled": "2022-01-14T06:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Learning to Make <b>Analogies by Contrasting Abstract Relational</b> ... - DeepAI", "url": "https://deepai.org/publication/learning-to-make-analogies-by-contrasting-abstract-relational-structure", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/learning-to-make-analogies-by-contrasting-abstract...", "snippet": "In our visual <b>analogy</b> <b>task</b>, a relation <b>can</b> be instantiated in one of seven different domains: line type, line colour, ... we might still achieve notable improvements in generalization via methods that learn to play the role of <b>teacher</b> by presenting alternatives to the main (<b>student</b>) model, as per Shafto et al. . This underlines the fact that, for established learning algorithms involving negative examples such as (noise) contrastive estimation (Smith &amp; Eisner, 2005; Gutmann &amp; Hyv\u00e4rinen ...", "dateLastCrawled": "2021-12-06T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Paper Digest: ACL 2019 Highlights</b> \u2013 Paper Digest", "url": "https://www.paperdigest.org/2019/07/acl-2019-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2019/07/acl-2019-highlights", "snippet": "To tackle these problems, we first explore to formalize ATE as a <b>sequence-to-sequence</b> (Seq2Seq) learning <b>task</b> where the source sequence and target sequence are composed of words and labels respectively. At the same time, to make Seq2Seq learning suit to ATE where labels correspond to words one by one, we design the gated unit networks to incorporate corresponding word representation into the decoder, and position-aware attention to pay more attention to the adjacent words of a target word.", "dateLastCrawled": "2022-02-02T06:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Jason</b> Eisner\u2019s publications", "url": "https://www.cs.jhu.edu/~jason/papers/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>cs.jhu.edu/~jason/papers</b>", "snippet": "Following the success of neural <b>sequence-to-sequence</b> models in the SIGMORPHON 2016 shared <b>task</b>, all but one of the submissions included a neural component. The results show that high performance <b>can</b> be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data. However, different biasing and data augmentation resulted in disjoint sets of inflected forms being predicted correctly, suggesting that there is ...", "dateLastCrawled": "2022-01-31T06:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Language as cognitive tool kit: How language supports relational <b>thought</b>", "url": "https://www.researchgate.net/publication/312236051_Language_as_cognitive_tool_kit_How_language_supports_relational_thought", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/312236051_Language_as_cognitive_tool_kit_How...", "snippet": "In the present study we tested the Story Gestalt model (St. John, 1992), a classic PDP model of text comprehension, and a <b>Sequence-to-Sequence</b> with Attention model (Bahdanau et al., 2015), a ...", "dateLastCrawled": "2022-01-04T02:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Can</b> you build an AI <b>capable to understand a text</b> and answer questions ...", "url": "https://www.quora.com/Can-you-build-an-AI-capable-to-understand-a-text-and-answer-questions-about-it", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Can</b>-you-build-an-AI-<b>capable-to-understand-a-text</b>-and-answer...", "snippet": "Answer (1 of 4): Yes I <b>can</b>, and have, but it took about 800 days to program and is far from perfect. For instance, like myself, it has the tendency to take \u201c<b>can</b> you ...", "dateLastCrawled": "2022-01-14T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "notes-2/Deep Learning.md at master \u00b7 rsantana-isg/notes-2 \u00b7 GitHub", "url": "https://github.com/rsantana-isg/notes-2/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rsantana-isg/notes-2/blob/master/Deep Learning.md", "snippet": "You <b>can</b> tell someone what <b>thought</b> you are having by producing a string of words that would normally give rise to that <b>thought</b> but this doesn&#39;t mean the <b>thought</b> is a string of symbols in some unambiguous internal language. The new recurrent network translation models make it clear that you <b>can</b> get a very long way by treating a <b>thought</b> as a big state vector. Traditional AI researchers will be horrified by the view that thoughts are merely the hidden states of a recurrent net and even more ...", "dateLastCrawled": "2022-01-02T20:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>deep-learning-nlp-rl-papers</b>/PAPERS2017.md at master - <b>GitHub</b>", "url": "https://github.com/madrugado/deep-learning-nlp-rl-papers/blob/master/PAPERS2017.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>madrugado/deep-learning-nlp-rl-papers</b>/blob/master/PAPERS2017.md", "snippet": "We find that we <b>can</b> substantially improve parsing accuracy by training a single <b>sequence-to-sequence</b> model over multiple KBs, when providing an encoding of the domain at decoding time. Our model achieves state-of-the-art performance on the Overnight dataset (containing eight domains), improves performance over a single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the number of model parameters.", "dateLastCrawled": "2022-01-15T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Appendix to 2020 CS379C Class Discussion Notes", "url": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "snippet": "Figure 4: An example of how state vectors constructed from identical register contents <b>can</b> crop up in performing either an AWK substitution <b>task</b> or a two-pattern SIR <b>task</b>, and produce different results. The panel on the left represents the contents of working memory giving rise to the ambiguous state vector. The next step for both programs is to compare the input stream against the pattern in the third column labeled", "dateLastCrawled": "2022-01-31T03:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Percy Liang</b> - ACL Anthology", "url": "https://aclanthology.org/people/p/percy-liang/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/people/p/<b>percy-liang</b>", "snippet": "In this paper, we propose the <b>task</b> of Textual <b>Analogy</b> Parsing (TAP) to model this higher-order meaning. Given a sentence such as the one above, TAP outputs a frame-style meaning representation which explicitly specifies what is shared (e.g., poverty rates) and what is compared (e.g., White Americans vs. African Americans, 10% vs. 28%) between its component facts. Such a meaning representation <b>can</b> enable new applications that rely on discourse understanding such as automated chart generation ...", "dateLastCrawled": "2021-11-18T23:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Dan Jurafsky</b> - ACL Anthology", "url": "https://aclanthology.org/people/d/dan-jurafsky/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/people/d/<b>dan-jurafsky</b>", "snippet": "In this paper, we propose the <b>task</b> of Textual <b>Analogy</b> Parsing (TAP) to model this higher-order meaning. Given a sentence such as the one above, TAP outputs a frame-style meaning representation which explicitly specifies what is shared (e.g., poverty rates) and what is compared (e.g., White Americans vs. African Americans, 10% vs. 28%) between its component facts. Such a meaning representation <b>can</b> enable new applications that rely on discourse understanding such as automated chart generation ...", "dateLastCrawled": "2021-12-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Paper Digest: ACL 2019 Highlights</b> \u2013 Paper Digest", "url": "https://www.paperdigest.org/2019/07/acl-2019-highlights/", "isFamilyFriendly": true, "displayUrl": "https://www.paperdigest.org/2019/07/acl-2019-highlights", "snippet": "To tackle these problems, we first explore to formalize ATE as a <b>sequence-to-sequence</b> (Seq2Seq) learning <b>task</b> where the source sequence and target sequence are composed of words and labels respectively. At the same time, to make Seq2Seq learning suit to ATE where labels correspond to words one by one, we design the gated unit networks to incorporate corresponding word representation into the decoder, and position-aware attention to pay more attention to the adjacent words of a target word.", "dateLastCrawled": "2022-02-02T06:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Learning to Make <b>Analogies by Contrasting Abstract Relational</b> ... - DeepAI", "url": "https://deepai.org/publication/learning-to-make-analogies-by-contrasting-abstract-relational-structure", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/learning-to-make-analogies-by-contrasting-abstract...", "snippet": "In our visual <b>analogy</b> <b>task</b>, a relation <b>can</b> be instantiated in one of seven different domains: line type, line colour, ... we might still achieve notable improvements in generalization via methods that learn to play the role of <b>teacher</b> by presenting alternatives to the main (<b>student</b>) model, as per Shafto et al. . This underlines the fact that, for established learning algorithms involving negative examples such as (noise) contrastive estimation (Smith &amp; Eisner, 2005; Gutmann &amp; Hyv\u00e4rinen ...", "dateLastCrawled": "2021-12-06T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Jason</b> Eisner\u2019s publications", "url": "https://www.cs.jhu.edu/~jason/papers/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>cs.jhu.edu/~jason/papers</b>", "snippet": "Following the success of neural <b>sequence-to-sequence</b> models in the SIGMORPHON 2016 shared <b>task</b>, all but one of the submissions included a neural component. The results show that high performance <b>can</b> be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data. However, different biasing and data augmentation resulted in disjoint sets of inflected forms being predicted correctly, suggesting that there is ...", "dateLastCrawled": "2022-01-31T06:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>deep-learning-nlp-rl-papers</b>/PAPERS2017.md at master - <b>GitHub</b>", "url": "https://github.com/madrugado/deep-learning-nlp-rl-papers/blob/master/PAPERS2017.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>madrugado/deep-learning-nlp-rl-papers</b>/blob/master/PAPERS2017.md", "snippet": "<b>Sequence-to-Sequence</b> Models <b>Can</b> Directly Transcribe Foreign Speech; A Structured Self-attentive Sentence Embedding; Deep Learning applied to NLP ; Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets; Learning Simpler Language Models with the Delta Recurrent Neural Network Framework; Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering; Question Answering from Unstructured Text by Retrieval and Comprehension; FastQA ...", "dateLastCrawled": "2022-01-15T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning to Teach with Dynamic Loss Functions</b> | DeepAI", "url": "https://deepai.org/publication/learning-to-teach-with-dynamic-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-to-teach-with-dynamic-loss-functions</b>", "snippet": "The eventual goal of the <b>teacher</b> model is that its output <b>can</b> serve as the loss function of the <b>student</b> model to maximize the long-term performance of the <b>student</b>, measured via a <b>task</b>-specific objective such as 0-1 accuracy in classification and BLEU score in sequence prediction , on a stand-alone development dataset. Learning a good <b>teaching</b> model is not trivial, since on the one hand the <b>task</b>-specific objective is usually non-smooth w.r.t. <b>student</b> model outputs, and on the other hand the ...", "dateLastCrawled": "2021-12-26T14:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Meta Learning for Knowledge Distillation \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2106.04570/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.04570", "snippet": "We present Meta Learning for Knowledge Distillation (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the <b>teacher</b> model is fixed during training. We show the <b>teacher</b> network <b>can</b> learn to better transfer knowledge to the <b>student</b> network (i.e., learning to teach) with the feedback from the performance of the distilled <b>student</b> network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment ...", "dateLastCrawled": "2022-01-22T06:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A study of text-to-speech (TTS) <b>in children&#39;s english learning</b>", "url": "https://www.researchgate.net/publication/281751417_A_study_of_text-to-speech_TTS_in_children's_english_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/281751417_A_study_of_text-to-speech_TTS_in...", "snippet": "To find out if a similar relationship <b>can</b> be found among English-speaking kindergartners, we gave 68 kindergartners a writing <b>task</b> and two oral-segmentation tasks similar to those used by Vernon ...", "dateLastCrawled": "2021-09-19T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Artificial Intelligence (AI) Chatbot as Language Learning Medium</b> ...", "url": "https://www.researchgate.net/publication/337711693_Artificial_Intelligence_AI_Chatbot_as_Language_Learning_Medium_An_inquiry", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337711693_Artificial_Intelligence_AI_Chatbot...", "snippet": "chatterbot [5, 6]. Chatbot is a computer pro gram or artificial intelligence which carries out. conversations through aud io or text [7], and interact with users in a particular domain or topic by ...", "dateLastCrawled": "2022-01-30T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Proceedings of the 2021 Conference of the North American Chapter of the ...", "url": "https://aclanthology.org/volumes/2021.naacl-srw/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2021.naacl-srw", "snippet": "In most of neural machine translation distillation or stealing scenarios, the highest-scoring hypothesis of the target model (<b>teacher</b>) is used to train a new model (<b>student</b>). If reference translations are also available, then better hypotheses (with respect to the references) <b>can</b> be oversampled and poor hypotheses either removed or undersampled. This paper explores the sampling method landscape (pruning, hypothesis oversampling and undersampling, deduplication and their combination) with ...", "dateLastCrawled": "2022-01-29T22:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to make my grandma an expert in deep learning in a week ... - quora.com", "url": "https://www.quora.com/How-do-I-make-my-grandma-an-expert-in-deep-learning-in-a-week-and-a-half", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-I-make-my-grandma-an-expert-in-deep-learning-in-a-week...", "snippet": "Answer (1 of 2): I would start at deeplearningbook.org, but to quickly gain true understanding of ML concepts including deep learning your grandma would also probably need a solid math background at least equivalent to a good high school curriculum. Khan Academy videos or maybe a couple MOOCs cou...", "dateLastCrawled": "2022-01-15T03:19:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Read the model framework <b>Encoder-Decoder and Seq2Seq</b> in NLP - easyAI", "url": "https://easyai.tech/en/ai-definition/encoder-decoder-seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://easyai.tech/en/ai-definition/encoder-decoder-seq2seq", "snippet": "Encoder-Decoder This framework is a good illustration of the core ideas of <b>machine</b> <b>learning</b>: ... Seq2Seq (short for <b>Sequence-to-sequence</b>), as literally, enters a sequence and outputs another sequence. The most important aspect of this structure is that the length of the input sequence and the output sequence are variable. For example, the following picture: As shown above: 6 Chinese characters are input, and 3 English words are output. The length of the input and output are different. The ...", "dateLastCrawled": "2022-01-31T17:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Week 3: <b>Sequence to sequence</b> architectures. <b>Sequence to sequence</b> models Language translation for example; Image captioning, caption an image; Picking the most likely model <b>Machine</b> Transation Model Split into a model encoding the sentence; and then a language model. Calculate the probability of an English sentence conditioned on a French sentence.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Week 1 Quiz - Introduction to deep <b>learning</b>. What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI is powering personal devices in our homes and offices, similar to electricity. Through the \u201csmart grid\u201d, AI is delivering a new wave of electricity. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Note: Andrew ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Andrew-NG-Notes/andrewng-p-5-sequence-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence...", "snippet": "<b>Machine</b> translation (<b>sequence to sequence</b>): X: text sequence (in one language) Y: text sequence (in other language) Video activity recognition (sequence to one): X: video frames ; Y: label (activity) Name entity recognition (<b>sequence to sequence</b>): X: text sequence; Y: label sequence; Can be used by seach engines to index different type of words inside a text. All of these problems with different input and output (sequence or not) can be addressed as supervised <b>learning</b> with label data X, Y ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(analogy to a teacher teaching a student)", "+(sequence-to-sequence task) is similar to +(analogy to a teacher teaching a student)", "+(sequence-to-sequence task) can be thought of as +(analogy to a teacher teaching a student)", "+(sequence-to-sequence task) can be compared to +(analogy to a teacher teaching a student)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}