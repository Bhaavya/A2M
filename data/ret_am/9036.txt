{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention, please! A <b>survey of Neural Attention Models in Deep Learning</b> ...", "url": "https://deepai.org/publication/attention-please-a-survey-of-neural-attention-models-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/attention-please-a-<b>survey-of-neural-attention-models</b>-in...", "snippet": "The <b>self-attention</b> <b>layer</b>\u2019s main advantages compared to soft and hard mechanisms are parallel computing <b>ability</b> for a long input. This mechanism <b>layer</b> checks the attention with all the same input elements using simple and easily parallelizable matrix calculations. Figure 9 shows an intuitive example of a <b>self-attention</b> mechanism. Figure 9: <b>Self-Attention</b> examples. a) <b>Self-attention</b> in sentences b) <b>Self-attention</b> in images. The first image shows five representative query locations with color ...", "dateLastCrawled": "2022-01-21T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A review on the attention mechanism of <b>deep learning</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "snippet": "applied the <b>self-attention</b> mechanism to the computer vision task to solve this problem, <b>called</b> non-local attention, as shown in Fig. 13. They proposed the non-local module that got attention masks by calculating the correlation matrix between each spatial point in the feature map, then the attention guided dense contextual information to aggregate. However, this method <b>also</b> has the following problems: 1) Only the positional attention module is involved, not the commonly used channel ...", "dateLastCrawled": "2022-02-02T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Attention in Psychology, Neuroscience, and Machine Learning", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7177153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7177153", "snippet": "In this way, <b>self-attention</b> is more about creating a good encoding than performing a task-<b>specific</b> attention-<b>like</b> selection based on limited resources. In the context of a temporal task, its closest analogue in psychology may be priming because priming alters the encoding of subsequent <b>stimuli</b> based on those that came before. It is of course not the direct goal of machine learning engineers to replicate the brain, but rather to create networks that can be easily trained to perform well on ...", "dateLastCrawled": "2021-12-27T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Frontiers | <b>Attention in Psychology, Neuroscience, and Machine Learning</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029", "snippet": "In this way, <b>self-attention</b> is more about creating a good encoding than performing a task-<b>specific</b> attention-<b>like</b> selection based on limited resources. In the context of a temporal task, its closest analogue in psychology may be priming because priming alters the encoding of subsequent <b>stimuli</b> based on those that came before. It is of course not the direct goal of machine learning engineers to replicate the brain, but rather to create networks that can be easily trained to perform well on ...", "dateLastCrawled": "2022-02-03T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "GitHub - <b>jinglescode/papers</b>: Summaries of machine learning papers", "url": "https://github.com/jinglescode/papers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jinglescode/papers", "snippet": "multi-head <b>self-attention</b> <b>layer</b> with sufficient number of heads is at least as expressive as any convolutional <b>layer</b>; Dynamic Convolution: Attention over Convolution Kernels . increases model complexity without increasing the network depth or width; single convolution kernel per <b>layer</b>, dynamic convolution aggregates multiple parallel convolution kernels dynamically based upon their attentions, which are input dependent; can be easily integrated into existing CNN architectures; Dynamic Group ...", "dateLastCrawled": "2021-09-18T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "RADet: Refine Feature Pyramid Network and Multi-<b>Layer</b> Attention Network ...", "url": "https://www.mdpi.com/2072-4292/12/3/389/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2072-4292/12/3/389/htm", "snippet": "According to cognitive neuroscience, attention can be divided into <b>focus</b> attention, which is active attention, refers to the purposeful and conscious <b>focus</b> on an object, and marked attention, which is passive attention, refers to the attention driven by external <b>stimuli</b> without active intervention. In artificial neural network, attention mechanism generally refers <b>to focus</b> attention. Ref.", "dateLastCrawled": "2022-01-21T00:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow ... - Academia.edu", "url": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow_CONCEPTS_TOOLS_AND_TECHNIQUES_TO_BUILD_INTELLIGENT_SYSTEMS", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and...", "snippet": "Hands-On Machine Learning with <b>Scikit-Learn &amp; TensorFlow CONCEPTS, TOOLS, AND TECHNIQUES TO BUILD INTELLIGENT SYSTEMS</b>", "dateLastCrawled": "2022-01-30T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "neural mechanism of motivation", "url": "https://qa.engineer.kmitl.ac.th/efnftsu/neural-mechanism-of-motivation", "isFamilyFriendly": true, "displayUrl": "https://qa.engineer.kmitl.ac.th/efnftsu/neural-mechanism-of-motivation", "snippet": "Focused attention is the <b>brain&#39;s</b> <b>ability</b> to concentrate its attention on a target stimulus for any period of time.Focused attention is a type of attention that makes it possible to quickly detect relevant <b>stimuli</b>. Finally, the role of dopamine as a feedback mechanism has been identified. Download PDF. For example, the brain circuits that instigate the selection of <b>specific</b> foods have neural components that are distinct from those that instigate thirst motivation ( Carlson, 2014). Here, we ...", "dateLastCrawled": "2022-01-12T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "notes-1/Deep Learning.md at master \u00b7 kirk86/notes-1 \u00b7 <b>GitHub</b>", "url": "https://github.com/kirk86/notes-1/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kirk86/notes-1/blob/master/Deep Learning.md", "snippet": "These concepts are captured by random variables (<b>called</b> hidden units) that have a joint distribution (statistical dependencies) among themselves and with the data, and that allow the learner to capture highly non-linear and complex interactions between the parts (observed random variables) of any observed example (<b>like</b> the pixels in an image). One can <b>also</b> think of these higher-level factors or hidden units as another, more abstract, representation of the data. RBM is parametrized through ...", "dateLastCrawled": "2021-12-24T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Social Psychology</b> Flashcards | Quizlet", "url": "https://quizlet.com/156414423/social-psychology-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/156414423/<b>social-psychology</b>-flash-cards", "snippet": "many private <b>self attention</b> effects are the same whether they result from the psychological state of private self awareness or the personality trait of private self-consciousness individuals high in private self-consciousness behaves more in line with their personal standards and react more strongly to their current moods then do their last self-conscious counterparts", "dateLastCrawled": "2018-11-29T23:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention in Psychology, Neuroscience, and Machine Learning", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7177153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7177153", "snippet": "In this way, <b>self-attention</b> is more about creating a good encoding than performing a task-<b>specific</b> attention-like selection based on limited resources. In the context of a temporal task, its closest analogue in psychology may be priming because priming alters the encoding of subsequent <b>stimuli</b> based on those that came before. It is of course not the direct goal of machine learning engineers to replicate the brain, but rather to create networks that can be easily trained to perform well on ...", "dateLastCrawled": "2021-12-27T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Attention, please! A <b>survey of Neural Attention Models in Deep Learning</b> ...", "url": "https://deepai.org/publication/attention-please-a-survey-of-neural-attention-models-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/attention-please-a-<b>survey-of-neural-attention-models</b>-in...", "snippet": "The <b>self-attention</b> <b>layer</b>\u2019s main advantages compared to soft and hard mechanisms are parallel computing <b>ability</b> for a long input. This mechanism <b>layer</b> checks the attention with all the same input elements using simple and easily parallelizable matrix calculations. Figure 9 shows an intuitive example of a <b>self-attention</b> mechanism. Figure 9: <b>Self-Attention</b> examples. a) <b>Self-attention</b> in sentences b) <b>Self-attention</b> in images. The first image shows five representative query locations with color ...", "dateLastCrawled": "2022-01-21T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A review on the attention mechanism of <b>deep learning</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "snippet": "applied the <b>self-attention</b> mechanism to the computer vision task to solve this problem, <b>called</b> non-local attention, as shown in Fig. 13. They proposed the non-local module that got attention masks by calculating the correlation matrix between each spatial point in the feature map, then the attention guided dense contextual information to aggregate. However, this method <b>also</b> has the following problems: 1) Only the positional attention module is involved, not the commonly used channel ...", "dateLastCrawled": "2022-02-02T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Frontiers | <b>Attention in Psychology, Neuroscience, and Machine Learning</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029", "snippet": "In this way, <b>self-attention</b> is more about creating a good encoding than performing a task-<b>specific</b> attention-like selection based on limited resources. In the context of a temporal task, its closest analogue in psychology may be priming because priming alters the encoding of subsequent <b>stimuli</b> based on those that came before. It is of course not the direct goal of machine learning engineers to replicate the brain, but rather to create networks that can be easily trained to perform well on ...", "dateLastCrawled": "2022-02-03T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What Can Computational Models Learn From <b>Human</b> Selective Attention? A ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7056875/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7056875", "snippet": "Recently, an important application is the <b>self-attention</b> mechanism (Vaswani et al., 2017). Different from soft and hard attention, <b>self-attention</b> does not capture features of mapping between source and target but can learn the inherent structure both within the source and target text. In the above example, \u201cfrom\u201d is more likely to be followed by \u201cGermany.\u201d <b>Self-attention</b> can be applied in each decoder <b>layer</b> of neural networks to achieve distributed processing (Bahdanau et al.,", "dateLastCrawled": "2021-11-04T19:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "GitHub - <b>jinglescode/papers</b>: Summaries of machine learning papers", "url": "https://github.com/jinglescode/papers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jinglescode/papers", "snippet": "<b>self-attention</b> can be an effective stand-alone <b>layer</b>; On the relationship between <b>self-attention</b> and convolutional layers. attention layers can perform convolution, they learn to behave <b>similar</b> to convolutional layers; multi-head <b>self-attention</b> <b>layer</b> with sufficient number of heads is at least as expressive as any convolutional <b>layer</b>", "dateLastCrawled": "2021-09-18T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "RADet: Refine Feature Pyramid Network and Multi-<b>Layer</b> Attention Network ...", "url": "https://www.mdpi.com/2072-4292/12/3/389/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2072-4292/12/3/389/htm", "snippet": "The proposed multi-<b>layer</b> attention network consists of four identical attention <b>layer</b>, which are connected after {P 2, P 3, P 4, P 5}, and the output is {A 2, A 3, A 4, A 5}. As illustrated in Figure 6, each attention <b>layer</b> contains two parts: position attention block and channel attention block. The position attention block is adopted to model ...", "dateLastCrawled": "2022-01-21T00:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "New submissions for Fri, 8 Oct 21 \u00b7 Issue #17 \u00b7 MukundVarmaT/ignore ...", "url": "https://github.com/MukundVarmaT/ignore/issues/17", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/MukundVarmaT/ignore/issues/17", "snippet": "Existing SSL methods <b>focus</b> on learning a model that effectively integrates information from given small labeled data and large unlabeled data, whereas we <b>focus</b> on selecting the right data for SSL without any label or task information, in an <b>also</b> stark contrast to supervised data selection for active learning. Intuitively, instances to be labeled shall collectively have maximum diversity and coverage for downstream tasks, and individually have maximum information propagation utility for SSL ...", "dateLastCrawled": "2022-02-01T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "RJ-ROBBINS Brain-Computer Interface", "url": "http://new.esp.org/recommended/literature/bci/", "isFamilyFriendly": true, "displayUrl": "new.esp.org/recommended/literature/bci", "snippet": "The purpose of this article is to provide a comprehensive survey on learning-based <b>human</b>-machine dialog systems with a <b>focus</b> on the various dialog models. More specifically, we first introduce the fundamental process of establishing a dialog model. Second, we examine the features and classifications of the system dialog model, expound some representative models, and <b>also</b> compare the advantages and disadvantages of different dialog models. Third, we comb the commonly used database and ...", "dateLastCrawled": "2022-02-02T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Social Psychology</b> Flashcards | Quizlet", "url": "https://quizlet.com/156414423/social-psychology-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/156414423/<b>social-psychology</b>-flash-cards", "snippet": "many private <b>self attention</b> effects are the same whether they result from the psychological state of private self awareness or the personality trait of private self-consciousness individuals high in private self-consciousness behaves more in line with their personal standards and react more strongly to their current moods then do their last self-conscious counterparts", "dateLastCrawled": "2018-11-29T23:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What <b>Can</b> Computational Models Learn From <b>Human</b> Selective Attention? A ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7056875/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7056875", "snippet": "<b>Self-attention</b> <b>can</b> be applied in each decoder <b>layer</b> of neural networks to achieve distributed processing (Bahdanau et al., 2014). In this way, <b>self-attention</b> shows good performance and efficiency when the input sentence is too long as in machine translation (Luong et al., 2015 ) or the input image is too large as in computer vision (Peng et al., 2019 ).", "dateLastCrawled": "2021-11-04T19:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8309939/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8309939", "snippet": "<b>Self-attention</b> mechanisms Graph attention networks ... Identifying the relationship between brain regions in relation to <b>specific</b> cognitive <b>stimuli</b> has been an important area of neuroimaging research. An emerging approach is to study this brain dynamic using fMRI data. To identify these brain states, traditional methods rely on acquisition of brain activity over time to accurately decode a brain state. Zhang et al. proposed a GCN for classifying <b>human</b> brain activity on 21 cognitive tasks by ...", "dateLastCrawled": "2022-01-28T05:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Frontiers | <b>Attention in Psychology, Neuroscience, and Machine Learning</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029", "snippet": "In addition to overall levels of arousal and alertness, attention <b>can</b> <b>also</b> be selectively deployed by an awake subject to <b>specific</b> sensory inputs. Studying attention within the context of a <b>specific</b> sensory system allows for tight control over both <b>stimuli</b> and the locus of attention. Generally, to look for this type of attention the task used needs to be quite challenging. For example, in a change detection task, the to-be-detected difference between two <b>stimuli</b> may be very slight. More ...", "dateLastCrawled": "2022-02-03T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The attention schema theory. (A) Visual attention is captured by the ...", "url": "https://researchgate.net/figure/The-attention-schema-theory-A-Visual-attention-is-captured-by-the-image-of-an-apple_fig2_276065623", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/The-attention-schema-theory-A-Visual-attention-is...", "snippet": "The attention schema theory. (A) Visual attention is captured by the image of an apple. On its own, this process results in the <b>ability</b> to accurately process the stimulus features \u2013 shape, color ...", "dateLastCrawled": "2021-08-30T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "notes-1/Deep Learning.md at master \u00b7 kirk86/notes-1 \u00b7 <b>GitHub</b>", "url": "https://github.com/kirk86/notes-1/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kirk86/notes-1/blob/master/Deep Learning.md", "snippet": "Masked <b>Self-Attention</b> Nets: unbounded receptive field; parallel compute: O(1) matrix-vector ops, but O(N^2) factor; O(1) dependency steps ; the state is the captured context: O(N) generative models - flow models. papers and resources &quot;Normalizing Flows for Probabilistic Modeling and Inference&quot; by Papamakarios et al. paper. overview by Eric Jang video overview by Aravind Srinivas video overview (1, 2) by Jonathan Ho video overview by Arsenii Ashukha video. overview of probability flows by ...", "dateLastCrawled": "2021-12-24T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow CONCEPTS ...", "url": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow_CONCEPTS_TOOLS_AND_TECHNIQUES_TO_BUILD_INTELLIGENT_SYSTEMS", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and...", "snippet": "Hands-On Machine Learning with <b>Scikit-Learn &amp; TensorFlow CONCEPTS, TOOLS, AND TECHNIQUES TO BUILD INTELLIGENT SYSTEMS</b>", "dateLastCrawled": "2022-01-30T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "NIPS2017abs.md \u00b7 GitHub", "url": "https://gist.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "isFamilyFriendly": true, "displayUrl": "https://<b>gist</b>.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "snippet": "We <b>also</b> introduce a stronger encoder for visual dialog, and employ a <b>self-attention</b> mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10).", "dateLastCrawled": "2022-01-31T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Social Psychology</b> Flashcards | Quizlet", "url": "https://quizlet.com/156414423/social-psychology-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/156414423/<b>social-psychology</b>-flash-cards", "snippet": "- <b>Ability</b> to control our behaviors uses a lot of cognitive resources, drains a lot of mental energy - people who over regulate are OCD Self-regulation is a self skill, children who are taught to self-regulate tends to be more successful in life. This is a learned trait that at any point in your life, you <b>can</b> increase your <b>ability</b> to self-regulate", "dateLastCrawled": "2018-11-29T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Master&#39;s theses</b> \u2013 Seminar for Statistics | ETH Zurich", "url": "https://math.ethz.ch/sfs/research/master-theses.html", "isFamilyFriendly": true, "displayUrl": "https://math.ethz.ch/sfs/research/master-theses.html", "snippet": "A special <b>focus</b> is <b>also</b> put on the decomposition of the estimated prediction uncertainty into aleatoric (data uncertainty) and epistemic uncertainty (model uncertainty). Our work shows how the additional estimation of predictive uncertainty estimates <b>can</b> contribute to verifying the usability and reliability of DNN in safety-critical predictive maintenance application areas. Zeno Benetti add: Fraud Detection in Ethereum Using Web-scraping and Natural Language Processing Techniques Dr. Markus ...", "dateLastCrawled": "2022-02-03T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Robots and AI: Our Immortality or Extinction - page 21 - The rest ...", "url": "https://forum.arctic-sea-ice.net/index.php?topic=1392.1000", "isFamilyFriendly": true, "displayUrl": "https://forum.arctic-sea-ice.net/index.php?topic=1392.1000", "snippet": "The Generalist Language Model (GLaM) is a mixture of experts (MoE) model, a type of model that <b>can</b> <b>be thought</b> of as having different submodels (or experts) that are each specialized for different inputs. The experts in each <b>layer</b> are controlled by a gating network that activates experts based on the input data. For each token (generally a word ...", "dateLastCrawled": "2022-01-30T05:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention, please! A <b>survey of Neural Attention Models in Deep Learning</b> ...", "url": "https://deepai.org/publication/attention-please-a-survey-of-neural-attention-models-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/attention-please-a-<b>survey-of-neural-attention-models</b>-in...", "snippet": "The <b>self-attention</b> <b>layer</b>\u2019s main advantages <b>compared</b> to soft and hard mechanisms are parallel computing <b>ability</b> for a long input. This mechanism <b>layer</b> checks the attention with all the same input elements using simple and easily parallelizable matrix calculations. Figure 9 shows an intuitive example of a <b>self-attention</b> mechanism. Figure 9: <b>Self-Attention</b> examples. a) <b>Self-attention</b> in sentences b) <b>Self-attention</b> in images. The first image shows five representative query locations with color ...", "dateLastCrawled": "2022-01-21T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What <b>Can</b> Computational Models Learn From <b>Human</b> Selective Attention? A ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7056875/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7056875", "snippet": "The component of the alerting network increases the <b>focus</b> on the potential <b>stimuli</b> of interest, and anatomical mechanisms of alerting are correlated with the thalamic, frontal, and parietal regions. The orienting function is responsible for selecting task-related or survival-related information from all the sensory inputs. The orienting network <b>also</b> determines an attention shift between exogenous attention engagement (bottom-up) and endogenous attention disengagement (top-down). Orienting is ...", "dateLastCrawled": "2021-11-04T19:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A review on the attention mechanism of <b>deep learning</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "snippet": "In some cases, q <b>can</b> <b>also</b> be in the form of a matrix or two vectors according to <b>specific</b> tasks. Then the neural network computes the correlation between queries and keys through the score function f (<b>also</b> <b>called</b> energy function [61] or compatibility function [62] ) to obtain the energy score e that reflects the importance of queries with respect to keys in deciding the next output: (6) e = f ( q , K ) .", "dateLastCrawled": "2022-02-02T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "RADet: Refine Feature Pyramid Network and Multi-<b>Layer</b> Attention Network ...", "url": "https://www.mdpi.com/2072-4292/12/3/389/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2072-4292/12/3/389/htm", "snippet": "According to cognitive neuroscience, attention <b>can</b> be divided into <b>focus</b> attention, which is active attention, refers to the purposeful and conscious <b>focus</b> on an object, and marked attention, which is passive attention, refers to the attention driven by external <b>stimuli</b> without active intervention. In artificial neural network, attention mechanism generally refers <b>to focus</b> attention. Ref.", "dateLastCrawled": "2022-01-21T00:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "GitHub - <b>jinglescode/papers</b>: Summaries of machine learning papers", "url": "https://github.com/jinglescode/papers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jinglescode/papers", "snippet": "<b>self-attention</b> <b>can</b> be an effective stand-alone <b>layer</b>; On the relationship between <b>self-attention</b> and convolutional layers. attention layers <b>can</b> perform convolution, they learn to behave similar to convolutional layers ; multi-head <b>self-attention</b> <b>layer</b> with sufficient number of heads is at least as expressive as any convolutional <b>layer</b>; Dynamic Convolution: Attention over Convolution Kernels. increases model complexity without increasing the network depth or width; single convolution kernel ...", "dateLastCrawled": "2021-09-18T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Frontiers | <b>Attention in Psychology, Neuroscience, and Machine Learning</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029", "snippet": "In addition to overall levels of arousal and alertness, attention <b>can</b> <b>also</b> be selectively deployed by an awake subject to <b>specific</b> sensory inputs. Studying attention within the context of a <b>specific</b> sensory system allows for tight control over both <b>stimuli</b> and the locus of attention. Generally, to look for this type of attention the task used needs to be quite challenging. For example, in a change detection task, the to-be-detected difference between two <b>stimuli</b> may be very slight. More ...", "dateLastCrawled": "2022-02-03T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "RJ-ROBBINS Brain-Computer Interface", "url": "http://new.esp.org/recommended/literature/bci/", "isFamilyFriendly": true, "displayUrl": "new.esp.org/recommended/literature/bci", "snippet": "The purpose of this article is to provide a comprehensive survey on learning-based <b>human</b>-machine dialog systems with a <b>focus</b> on the various dialog models. More specifically, we first introduce the fundamental process of establishing a dialog model. Second, we examine the features and classifications of the system dialog model, expound some representative models, and <b>also</b> compare the advantages and disadvantages of different dialog models. Third, we comb the commonly used database and ...", "dateLastCrawled": "2022-02-02T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow ... - Academia.edu", "url": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow_CONCEPTS_TOOLS_AND_TECHNIQUES_TO_BUILD_INTELLIGENT_SYSTEMS", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39825063/Hands_On_Machine_Learning_with_Scikit_Learn_and...", "snippet": "Hands-On Machine Learning with <b>Scikit-Learn &amp; TensorFlow CONCEPTS, TOOLS, AND TECHNIQUES TO BUILD INTELLIGENT SYSTEMS</b>", "dateLastCrawled": "2022-01-30T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "notes-1/Deep Learning.md at master \u00b7 kirk86/notes-1 \u00b7 <b>GitHub</b>", "url": "https://github.com/kirk86/notes-1/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kirk86/notes-1/blob/master/Deep Learning.md", "snippet": "We argue that both the optimal architecture, number of layers and features/connections at each <b>layer</b>, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input <b>layer</b> with respect to the output <b>layer</b>. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight <b>can</b> lead to new optimality bounds and deep learning ...", "dateLastCrawled": "2021-12-24T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Could learning brain cell differences be the key to learning in humans ...", "url": "https://www.quora.com/Could-learning-brain-cell-differences-be-the-key-to-learning-in-humans-and-AI", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Could-learning-brain-cell-differences-be-the-key-to-learning-in...", "snippet": "Answer: Neurons are not essential to natural intelligence, in the same way as feathers and flapping wings are not essential to aviation. Hence, despite all kinds of &quot;hopeful&quot; terms, simulated neural networks are not naturally intelligent. And they will never become intelligent by themselves. Mor...", "dateLastCrawled": "2022-01-15T22:25:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention in Psychology, Neuroscience, and <b>Machine</b> <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7177153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7177153", "snippet": "These weightings scale the word encodings themselves to create the next <b>layer</b> in the model, a process known as \u201c<b>self-attention</b>.\u201d This process repeats, and eventually interacts with the autoregressive decoder which <b>also</b> has attention mechanisms that allow it to flexibly focus on the encoded input (as in the standard form of attention) and on the previously generated output. The Transformer\u2014the name given to this new attention architecture\u2014outperformed many previous models and quickly ...", "dateLastCrawled": "2021-12-27T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Lecture 7: Transformers</b> - Deep <b>Learning</b>", "url": "https://chinmayhegde.github.io/dl-notes/notes/lecture07/", "isFamilyFriendly": true, "displayUrl": "https://chinmayhegde.github.io/dl-notes/notes/lecture07", "snippet": "<b>Self-Attention</b>. This is the point where papers-blogs-tweets-slides etc start talking about keys/values and attention mechanisms and everything goes a bit haywire. Let\u2019s just ignore all that for now, and instead talk about something <b>called</b> <b>self-attention</b>. The use of the \u201cself-\u201c prefix will become clear later on. Here is how it is defined.", "dateLastCrawled": "2022-02-03T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training ...", "url": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_Self_attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_<b>Self_attention</b>_and_Statef...", "snippet": "<b>Self-attention</b>, <b>also</b> known as intra-attention, is an attention mechanism relat- ing di\ufb00erent positions of a sequence in order to model dependencies between dif- ferent parts of the sequence. This di\ufb00ers from general attention in that instead of seeking to discover the \u201cimportant\u201d parts of the sequence relating to the net- work output, <b>self-attention</b> seeks to \ufb01nd the \u201cimportant\u201d portions of the sequence that relate to each other. This is done in order to leverage those intra ...", "dateLastCrawled": "2022-02-03T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The encoder is composed of a stack of N = 6 identical layers. Each <b>layer</b> has two sub-layers. The first is a multi-head <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by <b>layer</b> normalization. That is, the output ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "<b>Self attention</b>, sometimes <b>called</b> intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. In simpler terms, <b>self attention</b> helps us create similar connections but within the same sentence. Look at the following example: \u201cI poured water from the bottle into the cup until it was full.\u201d it =&gt; cup \u201cI poured water from the bottle into the cup until it was empty.\u201d it=&gt; bottle. By changing one word ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning Papers: Molecules</b> - <b>Machine Learning</b> Applied", "url": "https://machinelearningapplied.com/machine-learning-papers-molecules/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>applied.com/<b>machine-learning-papers-molecules</b>", "snippet": "A <b>self-attention</b> based message passing neural network for predicting molecular lipophilicity and aqueous solubility - Tang et al 2020 . Efficient and accurate prediction of molecular properties, such as lipophilicity and solubility, is highly desirable for rational compound design in chemical and pharmaceutical industries. To this end, we build and apply a graph-neural-network framework <b>called</b> <b>self-attention</b>-based message-passing neural network (SAMPN) to study the relationship between ...", "dateLastCrawled": "2021-12-22T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Solving the Bottleneck of Transformer model | by Cheng He | The Startup ...", "url": "https://chengh.medium.com/evolution-of-fast-and-efficient-transformers-ec0378257994", "isFamilyFriendly": true, "displayUrl": "https://chengh.medium.com/evolution-of-fast-and-efficient-transformers-ec0378257994", "snippet": "Transformer with a stack of 2 encoders and decoders, source The Problem of Transformer: Scales poorly with the length of the input sequence (<b>Self-attention</b> <b>layer</b> becomes the bottleneck in Transformer encoder and decoder block when input sequence grows longer)Requiring quadratic computation time and space to produce all similarity scores in each <b>self-attention</b> <b>layer</b>.; Before we diving deep into the optimizations and mainstream solutions, let\u2019s first take a at look how <b>self-attention</b> and ...", "dateLastCrawled": "2022-01-30T12:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Seven Myths in Machine Learning Research</b> | DeepAI", "url": "https://deepai.org/publication/seven-myths-in-machine-learning-research", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>seven-myths-in-machine-learning-research</b>", "snippet": "Importantly, Vaswani et al. noted that \u201dthe computational cost of a separable convolution is equal to the combination of a <b>self-attention</b> <b>layer</b> and a point-wise feed-forward <b>layer</b>.\u201d Even state-of-the-art GANS find <b>self-attention</b> superior to standard convolutions in its ability to model long-range, multi-scale dependencies [Zhang et al., 2018 ] .", "dateLastCrawled": "2022-01-12T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Misnomers and Confusing Terms in Machine Learning</b>", "url": "https://product.hubspot.com/blog/misnomers-and-confusing-terms-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://product.hubspot.com/blog/<b>misnomers-and-confusing-terms-in-machine-learning</b>", "snippet": "The standard presentation of a multi-<b>layer</b> perceptron includes the statement that this architecture is composed of at least three layers of neurons: an input <b>layer</b>, a hidden <b>layer</b>, and an output <b>layer</b> (Haykin 2009, page 21). An artificial neuron (sorry again, Chollet) is supposed to 1) receive inputs, 2) combine them (often linearly), and 3) produce an output (often non-linearly). Instead, the neurons in the input <b>layer</b> start with a value, do nothing, and hand it off to the next <b>layer</b>. They ...", "dateLastCrawled": "2022-02-03T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Understanding graph neural networks by way of convolutional nets | by ...", "url": "https://medium.com/dida-machine-learning/understanding-graph-neural-networks-by-way-of-convolutional-nets-d7c2f33e8c62", "isFamilyFriendly": true, "displayUrl": "https://medium.com/dida-<b>machine</b>-<b>learning</b>/understanding-graph-neural-networks-by-way-of...", "snippet": "Note <b>also</b> that we have only discussed a single GNN <b>layer</b> here. As with convolutional nets, and deep <b>learning</b> in general, the magic starts to happen when we stack several of those layers.", "dateLastCrawled": "2022-01-03T00:41:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(self-attention (also called self-attention layer))  is like +(human brain\u2019s ability to focus on specific stimuli)", "+(self-attention (also called self-attention layer)) is similar to +(human brain\u2019s ability to focus on specific stimuli)", "+(self-attention (also called self-attention layer)) can be thought of as +(human brain\u2019s ability to focus on specific stimuli)", "+(self-attention (also called self-attention layer)) can be compared to +(human brain\u2019s ability to focus on specific stimuli)", "machine learning +(self-attention (also called self-attention layer) AND analogy)", "machine learning +(\"self-attention (also called self-attention layer) is like\")", "machine learning +(\"self-attention (also called self-attention layer) is similar\")", "machine learning +(\"just as self-attention (also called self-attention layer)\")", "machine learning +(\"self-attention (also called self-attention layer) can be thought of as\")", "machine learning +(\"self-attention (also called self-attention layer) can be compared to\")"]}