{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Disparate</b> Impact in Machine Learning \u00bb Dome | Blog Archive | Boston ...", "url": "https://sites.bu.edu/dome/2020/06/08/disparate-impact-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://sites.bu.edu/dome/2020/06/08/<b>disparate</b>-impact-in-machine-learning", "snippet": "<b>Computer</b> programs are designed to take large amounts of data as inputs, and recognize patterns that are difficult for humans to see. These patterns are so complex, it is sometimes unclear what parts of the data the machine is relying on to make a decision. This type of <b>black</b> <b>box</b> often produces useful predictions, but it is unclear whether the predictions are made based on forbidden inferences. It is possible that machine learning algorithms used in banking, real estate, and employment rely ...", "dateLastCrawled": "2021-12-09T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Racist Algorithm</b>?", "url": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1657&context=mlr", "isFamilyFriendly": true, "displayUrl": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1657&amp;context=mlr", "snippet": "The <b>Black</b> <b>Box</b> Society: The Secret Algorithms That Con-trol Money and Information. By . Frank Pasquale. Cambridge and London: Harvard University Press. 2015. P. 218, $35. Introduction A pie chart satirizing Google\u2019s research and development expenditures imagines a largely tripartite division: omniscience, omnipresence, and om-nipotence. 1. Uber offers its staff what it calls \u201cGod View,\u201d a real-time view of where all its users are going in a city. 2. In his new book, The <b>Black</b> <b>Box</b> Soci ...", "dateLastCrawled": "2022-01-30T00:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding and Addressing Racial Disparities in Health Care", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4194634/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4194634", "snippet": "In 1964, 38 percent of white persons indicated that the government in Washington should see to it that <b>black</b> people get fair <b>treatment</b> in jobs, and 13 percent indicated that they lacked enough interest in the question to favor one side over another. In 1996, the percentage of white persons supporting Federal intervention to ensure fair <b>treatment</b> in jobs declined to 28 percent, while the percentage expressing no interest in the question increased to 36 percent. Fourth, national data on ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How Facebook\u2019s Advertising Algorithms Can Discriminate By Race and ...", "url": "https://techscience.org/a/2021101901/", "isFamilyFriendly": true, "displayUrl": "https://techscience.org/a/2021101901", "snippet": "<b>Disparate</b> <b>treatment</b> focuses on intentional discrimination, which occurs when individuals are treated differently because of their protected class attribute, such as race, color, national origin, religion, sex, disability, or familial status [63]. There can be overt evidence of <b>disparate</b> <b>treatment</b>, as when a lender has a policy of a higher credit limit for older borrowers [64]. Or there can be comparative evidence of <b>disparate</b> <b>treatment</b>, as when two borrowers who are otherwise similar are ...", "dateLastCrawled": "2022-02-02T07:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>BLACK</b> <b>BOX</b> ARTIFICIAL INTELLIGENCE AND THE RULE OF LAW", "url": "https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article=5010&context=lcp", "isFamilyFriendly": true, "displayUrl": "https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article=5010&amp;context=lcp", "snippet": "<b>Black</b> <b>box</b> AI might undermine the rule of law by obscuring biases and allowing unfairness to persist unchecked. There is a growing awareness of both the opacity and bias concerns of <b>black</b> <b>box</b> algorithms in due process contexts, in part because of some recent, troubling cases. Perhaps most well-known, when a predictive AI system labeled Wisconsin resident Eric Loomis at \u201chigh risk\u201d for recidivism, Mr. Loomis challenged his resulting six-year prison 2. This volume takes an inclusive ...", "dateLastCrawled": "2022-01-23T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Algorithms as discrimination detectors", "url": "https://www.pnas.org/content/pnas/117/48/30096.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/<b>pnas</b>/117/48/30096.full.pdf", "snippet": "sense is the ultimate \u201c<b>black</b> <b>box</b>.\u201d With the right legal and reg-ulatory systems in place, algorithms can serve as something akin to a Geiger counter that makes it easier to detect\u2014and hence prevent\u2014discrimination. We emphasize that this is an aspiration, not a prediction. A great deal depends on putting the right legal and regulatory sys-tems in place. Aspirationally, such systems can help not only in detecting discrimination as it is now understood in law, but also in clarifying the ...", "dateLastCrawled": "2021-10-29T10:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Artificial Intelligence</b> Risk &amp; Governance - <b>Artificial Intelligence</b> for ...", "url": "https://ai.wharton.upenn.edu/artificial-intelligence-risk-governance/", "isFamilyFriendly": true, "displayUrl": "https://ai.wharton.upenn.edu/<b>artificial-intelligence</b>-risk-governance", "snippet": "<b>Disparate</b> <b>treatment</b> discrimination could occur when similarly situated individuals are treated differently based on a prohibited basis, but the <b>treatment</b> does not have to be motivated by prejudice or an intent to discriminate. In an AI context, this may potentially occur, for example, when a firm explicitly uses protected class status in an AI system used to underwrite creditworthiness.", "dateLastCrawled": "2022-02-02T07:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Who is afraid of <b>black</b> <b>box</b> algorithms? On the epistemological and ...", "url": "https://jme.bmj.com/content/47/5/329", "isFamilyFriendly": true, "displayUrl": "https://jme.bmj.com/content/47/5/329", "snippet": "The use of <b>black</b> <b>box</b> algorithms in medicine has raised scholarly concerns due to their opaqueness and lack of trustworthiness. Concerns about potential bias, accountability and responsibility, patient autonomy and compromised trust transpire with <b>black</b> <b>box</b> algorithms. These worries connect epistemic concerns with normative issues. In this paper, we outline that <b>black</b> <b>box</b> algorithms are less problematic for epistemic reasons than many scholars seem to believe. By outlining that more ...", "dateLastCrawled": "2022-01-30T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Algorithms as culture: Some tactics for the ethnography of <b>algorithmic</b> ...", "url": "https://journals.sagepub.com/doi/full/10.1177/2053951717738104", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/full/10.1177/2053951717738104", "snippet": "These projects, inspired by historical efforts to expose housing discrimination, treat <b>algorithmic</b> systems as <b>black</b>-<b>box</b> functions: hidden operations that turn inputs into outputs, <b>like</b> mortgage applications into home loans (Diakopoulos, 2013; Sandvig et al., 2014). By varying inputs and examining the corresponding outputs (e.g. constructing personae that vary by apparent race), audit studies can demonstrate \u201c<b>disparate</b> impact\u201d\u2014differences in outcome that affect legally protected classes ...", "dateLastCrawled": "2022-01-31T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Programming Fairness in Algorithms</b> | by Matthew Stewart, PhD Researcher ...", "url": "https://towardsdatascience.com/programming-fairness-in-algorithms-4943a13dd9f8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>programming-fairness-in-algorithms</b>-4943a13dd9f8", "snippet": "Clearly, such an idea would require large amounts of discussion, money, and expertise to implement, but this seems <b>like</b> a potentially workable solution from my perspective. There is still a long way to go to ensure our algorithms are free of both <b>disparate</b> <b>treatment</b> and <b>disparate</b> impact. With a combination of regulations, transparency, human-in ...", "dateLastCrawled": "2022-01-23T21:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Artificial Intelligence</b> Risk &amp; Governance - <b>Artificial Intelligence</b> for ...", "url": "https://ai.wharton.upenn.edu/artificial-intelligence-risk-governance/", "isFamilyFriendly": true, "displayUrl": "https://ai.wharton.upenn.edu/<b>artificial-intelligence</b>-risk-governance", "snippet": "<b>Disparate</b> <b>treatment</b> discrimination could occur when similarly situated individuals are treated differently based on a prohibited basis, but the <b>treatment</b> does not have to be motivated by prejudice or an intent to discriminate. In an AI context, this may potentially occur, for example, when a firm explicitly uses protected class status in an AI system used to underwrite creditworthiness.", "dateLastCrawled": "2022-02-02T07:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How Facebook\u2019s Advertising Algorithms Can Discriminate By Race and ...", "url": "https://techscience.org/a/2021101901/", "isFamilyFriendly": true, "displayUrl": "https://techscience.org/a/2021101901", "snippet": "<b>Disparate</b> <b>treatment</b> focuses on intentional discrimination, which occurs when individuals are treated differently because of their protected class attribute, such as race, color, national origin, religion, sex, disability, or familial status [63]. There can be overt evidence of <b>disparate</b> <b>treatment</b>, as when a lender has a policy of a higher credit limit for older borrowers [64]. Or there can be comparative evidence of <b>disparate</b> <b>treatment</b>, as when two borrowers who are otherwise <b>similar</b> are ...", "dateLastCrawled": "2022-02-02T07:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding and Reducing Bias in <b>Machine Learning</b> | by Jaspreet ...", "url": "https://towardsdatascience.com/understanding-and-reducing-bias-in-machine-learning-6565e23900ac", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-and-reducing-bias-in-<b>machine-learning</b>...", "snippet": "<b>Disparate</b> <b>Treatment</b>: This type of relative discrimination can be detected if a user\u2019s predicted outcome changes on changing the sensitive feature. In the above example, this would mean the algorithm predicting a positive label for returning a loan for a white person and a negative one for a <b>black</b> person, even if all other features are exactly the same.", "dateLastCrawled": "2022-01-28T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding and Addressing Racial Disparities in Health Care", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4194634/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4194634", "snippet": "In a <b>similar</b> vein, there is only weak support for policies to eradicate employment discrimination. In 1964, 38 percent of white persons indicated that the government in Washington should see to it that <b>black</b> people get fair <b>treatment</b> in jobs, and 13 percent indicated that they lacked enough interest in the question to favor one side over ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Programming Fairness in Algorithms</b> | by Matthew Stewart, PhD Researcher ...", "url": "https://towardsdatascience.com/programming-fairness-in-algorithms-4943a13dd9f8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>programming-fairness-in-algorithms</b>-4943a13dd9f8", "snippet": "There is still a long way to go to ensure our algorithms are free of both <b>disparate</b> <b>treatment</b> and <b>disparate</b> impact. With a combination of regulations, transparency, human-in-the-loop, human-on-the-loop, and new and improved variations of statistical parity, we are part of the way there, but this field is still young and there is much work to be done \u2014 watch this space.", "dateLastCrawled": "2022-01-23T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Disparate</b> Dynamics of Gene Body and cis-Regulatory Element Evolution ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8309469/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8309469", "snippet": "Only an A-<b>box</b> in close proximity to the ATG start codon remains, <b>similar</b> to the promoter regions of the other copies. Thus, a single duplication event most likely occurred early in the Populus lineage, and after a period of divergent evolution, the region containing one of the two copies (along with its promoter) potentially underwent multiple local duplication events on chromosome 4.", "dateLastCrawled": "2022-01-24T01:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>Racist Algorithm</b>?", "url": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1657&context=mlr", "isFamilyFriendly": true, "displayUrl": "https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1657&amp;context=mlr", "snippet": "The <b>Black</b> <b>Box</b> Society: The Secret Algorithms That Con-trol Money and Information. By . Frank Pasquale. Cambridge and London: Harvard University Press. 2015. P. 218, $35. Introduction A pie chart satirizing Google\u2019s research and development expenditures imagines a largely tripartite division: omniscience, omnipresence, and om-nipotence. 1. Uber offers its staff what it calls \u201cGod View,\u201d a real-time view of where all its users are going in a city. 2. In his new book, The <b>Black</b> <b>Box</b> Soci ...", "dateLastCrawled": "2022-01-30T00:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Predictably unequal: understanding and addressing</b> concerns that ...", "url": "https://www.nature.com/articles/s41746-020-0304-9", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41746-020-0304-9", "snippet": "Such a causal concept of fairness is closely aligned to the legal concept of <b>disparate</b> <b>treatment</b> (Table 1) 20. According to causal definitions of fairness, <b>similar</b> individuals should not be ...", "dateLastCrawled": "2022-02-02T19:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "TuringBox: An Experimental Platform for the Evaluation of AI Systems", "url": "https://pure.mpg.de/rest/items/item_3020343/component/file_3036194/content", "isFamilyFriendly": true, "displayUrl": "https://pure.mpg.de/rest/items/item_3020343/component/file_3036194/content", "snippet": "3.1 Case Study 1: <b>Disparate</b> <b>Treatment</b> by Body Type in Commercial <b>Computer</b> Vision APIs Recently, <b>computer</b> vision systems have been shown to ex-hibit racial and gender biases[Buolamwini and Gebru, 2018]. In this demonstration, users will examine and quantify bias in commercial <b>computer</b> vision systems with respect to a previ-", "dateLastCrawled": "2022-01-01T17:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>BLACK</b> <b>BOX</b> ARTIFICIAL INTELLIGENCE AND THE RULE OF LAW", "url": "https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article=5010&context=lcp", "isFamilyFriendly": true, "displayUrl": "https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article=5010&amp;context=lcp", "snippet": "<b>Black</b> <b>box</b> AI might undermine the rule of law by obscuring biases and allowing unfairness to persist unchecked. There is a growing awareness of both the opacity and bias concerns of <b>black</b> <b>box</b> algorithms in due process contexts, in part because of some recent, troubling cases. Perhaps most well-known, when a predictive AI system labeled Wisconsin resident Eric Loomis at \u201chigh risk\u201d for recidivism, Mr. Loomis challenged his resulting six-year prison 2. This volume takes an inclusive ...", "dateLastCrawled": "2022-01-23T11:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Algorithms as discrimination detectors | <b>PNAS</b>", "url": "https://www.pnas.org/content/117/48/30096", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/48/30096", "snippet": "Preventing discrimination requires that we have means of detecting it, and this <b>can</b> be enormously difficult when human beings are making the underlying decisions. As applied today, algorithms <b>can</b> increase the risk of discrimination. But as we argue here, algorithms by their nature require a far greater level of specificity than is usually possible with human decision making, and this specificity makes it possible to probe aspects of the decision in additional ways. With the right changes to ...", "dateLastCrawled": "2022-01-28T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Physicians and Implicit Bias: How Doctors May Unwittingly Perpetuate ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3797360/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3797360", "snippet": "Its participants press different <b>computer</b> keys to sort photographs of African American and Caucasian American faces as either \u201c<b>Black</b>\u201d or \u201cWhite\u201d and then sort words like \u201cjoy, wonderful, glorious\u201d and \u201cagony, horrible, evil\u201d into \u201cGood\u201d and \u201cBad\u201d categories, respectively. Next, participants repeat the task, sequentially being asked to press one key when shown a \u201c<b>Black</b>\u201d stimulus or a \u201cBad\u201d word and a different key when shown a \u201cWhite\u201d stimulus or a \u201cGood ...", "dateLastCrawled": "2022-02-03T02:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Glossary of <b>Computer</b> System Software Development Terminology (8/95) | <b>FDA</b>", "url": "https://www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/inspection-guides/glossary-computer-system-software-development-terminology-895", "isFamilyFriendly": true, "displayUrl": "https://<b>www.fda.gov</b>/.../glossary-<b>computer</b>-system-software-development-terminology-895", "snippet": "<b>black</b>-<b>box</b> testing. See: testing, functional. block. (ISO) (1) A string of records, words, or characters that for technical or logical purposes are treated as a unity. (2) A collection of ...", "dateLastCrawled": "2022-01-27T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Algorithms as discrimination detectors", "url": "https://www.pnas.org/content/pnas/117/48/30096.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/<b>pnas</b>/117/48/30096.full.pdf", "snippet": "sense is the ultimate \u201c<b>black</b> <b>box</b>.\u201d With the right legal and reg-ulatory systems in place, algorithms <b>can</b> serve as something akin to a Geiger counter that makes it easier to detect\u2014and hence prevent\u2014discrimination. We emphasize that this is an aspiration, not a prediction. A great deal depends on putting the right legal and regulatory sys-tems in place. Aspirationally, such systems <b>can</b> help not only in detecting discrimination as it is now understood in law, but also in clarifying the ...", "dateLastCrawled": "2021-10-29T10:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Does mitigating ML&#39;s <b>disparate impact require disparate treatment</b>?", "url": "https://www.researchgate.net/publication/321180707_Does_mitigating_ML's_disparate_impact_require_disparate_treatment", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321180707_Does_mitigating_ML", "snippet": "<b>Disparate</b> impact <b>can</b> arise unintentionally and absent <b>disparate</b> <b>treatment</b>. The natural way to reduce <b>disparate</b> impact would be to apply <b>disparate</b> <b>treatment</b> in favor of the disadvantaged group, i.e ...", "dateLastCrawled": "2021-10-19T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "7 Racial Bias and Disparities in Proactive Policing | Proactive ...", "url": "https://www.nap.edu/read/24928/chapter/9", "isFamilyFriendly": true, "displayUrl": "https://www.nap.edu/read/24928/chapter/9", "snippet": "The numbers in this example <b>can</b> easily be modified to generate either relatively higher hit rates for <b>Black</b> stops or lower hit rates for <b>Black</b> stops, even when <b>Black</b> people are subject to racially biased <b>treatment</b>. For example, if the distribution of White suspects does not change but the proportion of <b>Black</b> suspects in the \u201csmells like marijuana\u201d category is reduced to 0.05 and the proportion in the most culpable category is increased to 0.10, the hit rate for <b>Black</b> stops now exceeds ...", "dateLastCrawled": "2022-01-30T06:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Programming Fairness in Algorithms</b> - TOPBOTS", "url": "https://www.topbots.com/programming-fairness-in-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/<b>programming-fairness-in-algorithms</b>", "snippet": "It <b>can</b> be used to our benefits, such as for information (e.g. predicting the weather) and protection (e.g. analyzing <b>computer</b> networks to detect attacks and malware). On the other hand, it has the potential to be weaponized to discriminate at essentially any level. This is not to say that the algorithms are evil for doing this, they are merely learning the representations present in the data, which may themselves have embedded within them the manifestations of historical injustices, as well ...", "dateLastCrawled": "2022-01-30T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "Some of the more sophisticated risk assessments in use today are proprietary <b>computer</b> algorithms created by private companies. These algorithms use statistical probabilities based on factors such as age, employment history, and prior criminal record to predict a defendant\u2019s likelihood of recidivism. In 2016, the investigative journalism non-profit, ProPublica, alleged that a popular risk assessment algorithm called COMPAS was racially biased against <b>black</b> defendants. The corporation behind ...", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Employment Law Final Study Terms Flashcards | Quizlet", "url": "https://quizlet.com/515376237/employment-law-final-study-terms-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/515376237/employment-law-final-study-terms-flash-cards", "snippet": "b. <b>disparate</b> <b>treatment</b> c. <b>disparate</b> impact d. none of these&quot; all of these except a; hire the firm that&#39;s cheaper, even though they discriminate, because they&#39;ll agree to put the clause in their contract, and that&#39;s all you need; besides, you <b>can</b>&#39;t afford to lose the government business &quot;Your firm&#39;s contract to sell office supplies to the federal government requires that you hire only subcontractors who agree not to discriminate, and include a nondiscrimination clause in their contracts with ...", "dateLastCrawled": "2021-11-26T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Thought</b>-Wired Allows Disabled to Control Home Appliances with Mind Alone", "url": "https://singularityhub.com/2011/05/01/thought-wired-allows-disabled-to-control-home-appliances-with-mind-alone/", "isFamilyFriendly": true, "displayUrl": "https://singularityhub.com/2011/05/01/<b>thought</b>-wired-allows-disabled-to-control-home...", "snippet": "<b>Thought</b>-Wired has also applied this BCI to rudimentary communication, using the lift and shift left <b>thought</b> commands for no and yes, respectively (2:01). In all of these cases, keep in mind that the virtual cube is not necessary to execute the command, and the thoughts correlated to the function are sufficient. With this device, the smart-home <b>can</b> become an extension of the mind.", "dateLastCrawled": "2021-12-11T12:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding and Reducing Bias in <b>Machine Learning</b> | by Jaspreet ...", "url": "https://towardsdatascience.com/understanding-and-reducing-bias-in-machine-learning-6565e23900ac", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-and-reducing-bias-in-<b>machine-learning</b>...", "snippet": "<b>Disparate</b> <b>Treatment</b>: This type of relative discrimination <b>can</b> be detected if a user\u2019s predicted outcome changes on changing the sensitive feature. In the above example, this would mean the algorithm predicting a positive label for returning a loan for a white person and a negative one for a <b>black</b> person, even if all other features are exactly ...", "dateLastCrawled": "2022-01-28T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "To Split or Not to Split: The Impact of <b>Disparate</b> <b>Treatment</b> in ...", "url": "https://www.researchgate.net/publication/339228534_To_Split_or_Not_to_Split_The_Impact_of_Disparate_Treatment_in_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339228534_To_Split_or_Not_to_Split_The_Impact...", "snippet": "<b>Disparate</b> impact <b>can</b> arise unintentionally and absent <b>disparate</b> <b>treatment</b>. The natural way to reduce <b>disparate</b> impact would be to apply <b>disparate</b> <b>treatment</b> in favor of the disadvantaged group, i.e ...", "dateLastCrawled": "2022-02-03T05:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding and Addressing Racial Disparities in Health Care", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4194634/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4194634", "snippet": "<b>Compared</b> with white persons, <b>black</b> persons and other minorities have lower levels of access to medical care in the United States due to their ... 38 percent of white persons indicated that the government in Washington should see to it that <b>black</b> people get fair <b>treatment</b> in jobs, and 13 percent indicated that they lacked enough interest in the question to favor one side over another. In 1996, the percentage of white persons supporting Federal intervention to ensure fair <b>treatment</b> in jobs ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How Facebook\u2019s Advertising Algorithms <b>Can</b> Discriminate By Race and ...", "url": "https://techscience.org/a/2021101901/", "isFamilyFriendly": true, "displayUrl": "https://techscience.org/a/2021101901", "snippet": "<b>Disparate</b> <b>treatment</b> focuses on intentional discrimination, which occurs when individuals are treated differently because of their protected class attribute, such as race, color, national origin, religion, sex, disability, or familial status [63]. There <b>can</b> be overt evidence of <b>disparate</b> <b>treatment</b>, as when a lender has a policy of a higher credit limit for older borrowers [64]. Or there <b>can</b> be comparative evidence of <b>disparate</b> <b>treatment</b>, as when two borrowers who are otherwise similar are ...", "dateLastCrawled": "2022-02-02T07:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Does mitigating ML&#39;s <b>disparate</b> impact require <b>disparate</b> <b>treatment</b>?", "url": "https://www.arxiv-vanity.com/papers/1711.07076/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1711.07076", "snippet": "Following related work in law and policy, two notions of prejudice have come to shape the study of fairness in algorithmic decision-making. Algorithms exhibit <b>disparate</b> <b>treatment</b> if they formally treat people differently according to a protected characteristic, like race, or if they intentionally discriminate (even if via proxy variables). Algorithms exhibit <b>disparate</b> impact if they affect subgroups differently. <b>Disparate</b> impact <b>can</b> arise unintentionally and absent <b>disparate</b> <b>treatment</b>. The ...", "dateLastCrawled": "2021-11-12T17:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Answered: 1) Identify the fallacy in the\u2026 | bartleby", "url": "https://www.bartleby.com/questions-and-answers/1-identify-the-fallacy-in-the-following-passage-.-a-i-do-not-agree-on-some-principles-of-capitalism-/499b0d5d-9dfb-4fab-80b6-c7951cbf05e2", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bartleby.com</b>/questions-and-answers/1-identify-the-fallacy-in-the-following...", "snippet": "Q: <b>Disparate</b> <b>treatment</b> is regarded as intentional discrimination and <b>disparate</b> impact is regarded as un... A: <b>Disparate</b> - only able to <b>compared</b> . <b>Disparate</b> <b>treatment</b> is regarded as intentional discrimination w...", "dateLastCrawled": "2022-01-22T10:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Disparate</b> Dynamics of Gene Body and cis-Regulatory Element Evolution ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8309469/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8309469", "snippet": "A characteristic W-<b>box</b>/A-<b>box</b> combination close to the transcription start site was identified in a number of Hordeum vulgare sequences and in one of the Brachipodium distachyon sequences. This might represent an evolutionary conserved gene regulatory element that existed before the splitting of the two lineages. A combination of 2\u20136 W-boxes present in a number of promoters from all three species further indicated that at least one copy of the gene might be regulated via WRKY transcription ...", "dateLastCrawled": "2022-01-24T01:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Glossary of <b>Computer</b> System Software Development Terminology (8/95) | <b>FDA</b>", "url": "https://www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/inspection-guides/glossary-computer-system-software-development-terminology-895", "isFamilyFriendly": true, "displayUrl": "https://<b>www.fda.gov</b>/.../glossary-<b>computer</b>-system-software-development-terminology-895", "snippet": "<b>black</b>-<b>box</b> testing. See: testing, functional. block. (ISO) (1) A string of records, words, or characters that for technical or logical purposes are treated as a unity. (2) A collection of ...", "dateLastCrawled": "2022-01-27T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Model Fairness &amp; Transparency</b>. A Project on Detecting, Understanding ...", "url": "https://medium.com/sfu-cspmp/model-transparency-fairness-552a747b444", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/sfu-cspmp/model-transparency-fairness-552a747b444", "snippet": "1. Introduction. B ias is a prejudice in favor or against a person, group, or a thing that is considered to be unfair. At present times, Machine Learning and Artificial Intelligence play a major ...", "dateLastCrawled": "2022-01-19T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Artificial Intelligence</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/artificial-intelligence", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/<b>computer</b>-science/<b>artificial-intelligence</b>", "snippet": "When there is a lot of <b>disparate</b> data being exchanged at subzero-second speeds, it is difficult to determine the cause of the anomaly, such as a software glitch, cyber-attack, weather event, or hybrid cyber-physical event. It <b>can</b> be very difficult to determine what normal looks like and set the accurate baseline that is needed to detect anomalies. Developing an AI blockchain\u2013enhanced grid requires that the data be broken into observable patterns, which is very challenging from a cyber ...", "dateLastCrawled": "2022-02-02T20:58:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>learning</b> and applications in microbiology", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8498514/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8498514", "snippet": "<b>Machine</b> <b>learning</b> has two main <b>learning</b> modes: supervised (also known as predictive) to make future predictions from training data, and unsupervised (descriptive), which is exploratory in nature without training data, defined target or output (Mitchell 1997). Training data are the initial information used to teach supervised ML algorithms in the process of developing a model, from which the model creates and refines its rules required for prediction. Typically, training data comprises a set ...", "dateLastCrawled": "2021-12-06T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Trustworthy <b>Machine</b> <b>Learning</b> - Kush R. Varshney - Chapter 2: <b>Machine</b> ...", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-02.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthy<b>machinelearning</b>.com/trustworthy<b>machinelearning</b>-02.htm", "snippet": "<b>Machine</b> <b>learning</b> can also be applied in use cases that are new processes for an organization and no exact historical data exists. Here, proxy data must be identified. For example, a health system may wish to start offering home nursing care to indisposed individuals proactively, but may not have data directly applicable for understanding this decision. Data from previous interactions of patients with the health system may be used as a proxy. In other cases, it may be that new data must be ...", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning: A maturing field</b> - Springer", "url": "https://link.springer.com/content/pdf/10.1007/BF00993251.pdf", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/content/pdf/10.1007/BF00993251.pdf", "snippet": "<b>Machine</b> <b>learning</b> conferences in the late &#39;80s drew ten times as many participants as that first CMU workshop: IJCAI and AAAI have very well represented <b>machine</b> <b>learning</b> tracks; this journal was established (with several publishers bidding against each other) and ex- panded from four to six and then to eight issues per year; books on the subject are no longer a rarity; and even undergraduate artificial intelligence curricula routinely include <b>machine</b> <b>learning</b>. What do the steady-as-you-go ...", "dateLastCrawled": "2022-01-22T12:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fairness in machine learning with tractable models</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0950705120308443", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705120308443", "snippet": "<b>Machine</b> <b>Learning</b> techniques have become pervasive across a range of different applications, and are now widely used in areas as <b>disparate</b> as recidivism prediction, consumer credit\u2013risk analysis and insurance pricing. The prevalence of <b>machine</b> <b>learning</b> techniques has raised concerns about the potential for learned algorithms to become biased against certain groups. Many definitions have been proposed in the literature, but the fundamental task of reasoning about probabilistic events is a ...", "dateLastCrawled": "2022-01-20T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Can AI learn good manners</b>? | Odgers Berndtson", "url": "https://www.odgersberndtson.com/en-us/insights/can-ai-learn-good-manners", "isFamilyFriendly": true, "displayUrl": "https://www.odgersberndtson.com/en-us/insights/<b>can-ai-learn-good-manners</b>", "snippet": "\u201cThe challenge of fully mitigating both <b>disparate</b> <b>treatment</b> and <b>disparate</b> impact risks requires a discussion between business leaders, data scientists, and legal experts to determine the best risk management strategy for each application. It also requires a decidedly human-centered approach to instill confidence that <b>machine</b>-generated decisions are being made with the customer\u2019s interest in mind.\u201d", "dateLastCrawled": "2022-01-17T10:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Can AI learn good manners</b>? | Odgers Berndtson", "url": "https://www.odgersberndtson.com/en-gb/insights/can-ai-learn-good-manners", "isFamilyFriendly": true, "displayUrl": "https://www.odgersberndtson.com/en-gb/insights/<b>can-ai-learn-good-manners</b>", "snippet": "\u201cThe challenge of fully mitigating both <b>disparate</b> <b>treatment</b> and <b>disparate</b> impact risks requires a discussion between business leaders, data scientists and legal experts to determine the best risk management strategy for each application. It also requires a decidedly human-centred approach to instil confidence that <b>machine</b>-generated decisions are being made with the customer\u2019s interest in mind.\u201d", "dateLastCrawled": "2022-01-22T11:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Protein function in precision medicine: deep understanding with <b>machine</b> ...", "url": "https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.12307", "isFamilyFriendly": true, "displayUrl": "https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.12307", "snippet": "<b>disparate</b> parts may ultimately lead to the same observable effect. In this <b>analogy</b>, we might argue that medicine has so far been often investing into mitigat- ing the inconvenience with lemons and much less into improving and augmenting the protocols for \ufb01nding the individual causes of problems. In his recent State-of-the-Union address, the US Pres-ident Barack Obama announced the Precision Medicine Initiative, making this challenge a national and interna-tional priority. Precision ...", "dateLastCrawled": "2021-12-12T19:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Democratizing Algorithmic Fairness</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s13347-019-00355-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13347-019-00355-w", "snippet": "<b>Machine</b> <b>learning</b> algorithms can now identify patterns and correlations in (big) datasets and predict outcomes based on the identified patterns and correlations. They can then generate decisions in accordance with the outcomes predicted, and decision-making processes can thereby be automated. Algorithms can inherit questionable values from datasets and acquire biases in the course of (<b>machine</b>) <b>learning</b>. While researchers and developers have taken the problem of algorithmic bias seriously, the ...", "dateLastCrawled": "2022-02-03T18:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "47 <b>Analogy</b> Examples To Make You As Sharp As A Tack (and then some)", "url": "https://www.greetingcardpoet.com/good-analogy-examples-and-definition/", "isFamilyFriendly": true, "displayUrl": "https://www.greetingcardpoet.com/<b>good-analogy-examples-and-definition</b>", "snippet": "The essence of this literary device is to set up a comparison that highlights similarities between two seemingly <b>disparate</b> items. It is by examining how the two items are alike in some way that leads to a clear understanding. Differences between Similes, Metaphors, and Analogies . While similes, metaphors, and analogies are similar in that they all compare two different things, similes and metaphors are figures of speech. In contrast, an <b>analogy</b> is more akin to a logical argument. A writer ...", "dateLastCrawled": "2022-02-02T03:57:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A primer on AI <b>fairness</b>. What it is and the tradeoffs to be made | by ...", "url": "https://towardsdatascience.com/artificial-intelligence-fairness-and-tradeoffs-ce11ac284b63", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/artificial-intelligence-<b>fairness</b>-and-tradeoffs-ce11ac284b63", "snippet": "A <b>machine</b> <b>learning</b> algorithms value is being able to increase the number of true positives and true negatives, which each have a value attached. Each false positive and false negative is costly. The value assigned to each depends on each context. A false negative is more costly in medical situations while a false positive is costlier in death penalty decisions. Expected value is profits that businesses can expect from using the algorithm. The more accurate the model, the higher the profits.", "dateLastCrawled": "2022-02-02T02:42:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(disparate treatment)  is like +(the computer as a black box)", "+(disparate treatment) is similar to +(the computer as a black box)", "+(disparate treatment) can be thought of as +(the computer as a black box)", "+(disparate treatment) can be compared to +(the computer as a black box)", "machine learning +(disparate treatment AND analogy)", "machine learning +(\"disparate treatment is like\")", "machine learning +(\"disparate treatment is similar\")", "machine learning +(\"just as disparate treatment\")", "machine learning +(\"disparate treatment can be thought of as\")", "machine learning +(\"disparate treatment can be compared to\")"]}