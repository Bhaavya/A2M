{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse</b> and Redundant Representations - From Theory to Applications in ...", "url": "https://www.researchgate.net/publication/220691600_Sparse_and_Redundant_Representations_-_From_Theory_to_Applications_in_Signal_and_Image_Processing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220691600_<b>Sparse</b>_and_Redundant...", "snippet": "The <b>sparse</b> <b>representation</b> model for an image patch x \u2208 R n is formally defined as x = D\u03b3, where \u03b3 \u2208 R m denotes the <b>sparse</b> <b>representation</b> w.r.t. to an overcomplete dictionary D \u2208 R n\u00d7m [4 ...", "dateLastCrawled": "2022-01-19T19:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Analogical mapping and inference with</b> binary spatter codes and <b>sparse</b> ...", "url": "https://www.researchgate.net/publication/261148322_Analogical_mapping_and_inference_with_binary_spatter_codes_and_sparse_distributed_memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261148322_Analogical_mapping_and_inference...", "snippet": "<b>Representation</b> <b>Analogy</b> MApper (Drama) [10] is very close . to being a distributed re-implementation of ACME. Ho wever, although the source and target are represented with VSAs, the network which ...", "dateLastCrawled": "2021-11-09T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep Learning: An Introduction from the NLP Perspective", "url": "https://www.cs.jhu.edu/~kevinduh/notes/duh12deeplearn.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.jhu.edu/~kevinduh/notes/duh12deeplearn.pdf", "snippet": "Dictionary learning and <b>sparse</b> reconstruction methods 2 Is multiple-level of representations really necessary in NLP? For Vision problems, there is clear <b>analogy</b> to the brain\u2019s structure, but for language? Maybe: compositionally and recursion in natural language. 3 Black magic required for e ective training, e.g.", "dateLastCrawled": "2022-01-07T22:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Analogy</b> and Abstraction - Wiley Online <b>Library</b>", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12278", "isFamilyFriendly": true, "displayUrl": "https://online<b>library</b>.wiley.com/doi/10.1111/tops.12278", "snippet": "In this <b>analogy</b>, knowledge about a power supply is projected to the mitochondria. But <b>analogy</b> can also operate in mutual alignment1 1 Mutual alignment analogies are those that are processed by structural alignment with few or no projected candidate inferences, According to structure-mapping, all analogies involve structural alignment. Even when the base offers a clear projection (as in the goldmine example), one still must align that structure with the target&#39;s <b>representation</b> in order to ...", "dateLastCrawled": "2022-01-26T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Graph Algorithms in the Language of Linear Algebra</b>", "url": "https://sites.cs.ucsb.edu/~gilbert/cs240a/slides/old/cs240a-GALA.pdf", "isFamilyFriendly": true, "displayUrl": "https://sites.cs.ucsb.edu/~gilbert/cs240a/slides/old/cs240a-GALA.pdf", "snippet": "\u2022 By <b>analogy</b> to numerical scientific computing. . . \u2022 What should the combinatorial BLAS look <b>like</b>? The challenge of the software stack C = A*B y = A*x \u00b5 = xT y Basic Linear Algebra Subroutines (BLAS): Ops/Sec vs. Matrix Size . 4 Outline \u2022 Motivation \u2022 <b>Sparse</b> matrices for graph algorithms \u2022 CombBLAS: <b>sparse</b> arrays and graphs on parallel machines \u2022 KDT: attributed semantic graphs in a high-level language \u2022 Standards for graph algorithm primitives . 5 Multiple-source breadth ...", "dateLastCrawled": "2022-01-31T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>CUSPARSE</b> \u00b7 Julia Packages", "url": "https://www.juliapackages.com/p/cusparse", "isFamilyFriendly": true, "displayUrl": "https://www.juliapackages.com/p/<b>cusparse</b>", "snippet": "A is transformed into CSC format moved to the GPU, then auto-converted to CSR format for you. Thus, d_A is not a transpose of A!Similarly, if you have a matrix in dense format on the GPU (in a CudaArray), you can simply call <b>sparse</b> to turn it into a <b>sparse</b> <b>representation</b>. Right now <b>sparse</b> by default turns the matrix it is given into CSR format. It takes an optional argument that lets you select CSC or HYB:", "dateLastCrawled": "2022-02-02T02:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Linguistic Regularities in Sparse and Explicit</b> Word Representations\u3092\u8aad\u2026", "url": "https://www.slideshare.net/kentonozawa75/linguistic-regularities-in-sparse-and-explicit-word-representations", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/kentonozawa75/<b>linguistic-regularities-in-sparse-and</b>...", "snippet": "<b>Linguistic Regularities in Sparse and Explicit</b> Word Representations\u3092\u8aad\u3093\u3060 1. <b>Linguistic Regularities in Sparse and Explicit</b> Word Representations Omer Levy and Yoav Goldberg \u56f3\u306f\u3059\u3079\u3066\u8ad6\u6587\u304b\u3089\u629c\u7c8b\uff0e \uff08nzw~\uff09\u306f nzw \u306e\u30b3\u30e1\u30f3\u30c8\u306a\u3069 1 / 15 2.", "dateLastCrawled": "2021-12-26T02:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>How does Word2vec solve sparse problem</b>? - Quora", "url": "https://www.quora.com/How-does-Word2vec-solve-sparse-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>How-does-Word2vec-solve-sparse-problem</b>", "snippet": "Answer: Word2vec solves the sparsity problem of models that use word identities as a feature by projecting words to a low dimensional space. You are right that in a given textual collection, words might occur only with limited context. So the word vectors that you learn will be pretty weak - but ...", "dateLastCrawled": "2022-01-21T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Representing text in natural language processing</b> | by Michel Kana, Ph.D ...", "url": "https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>representing-text-in-natural-language-processing</b>-1eead...", "snippet": "A 2-gram (or bigram) is a two-word sequence of words <b>like</b> \u201cplease eat\u201d, \u201ceat your\u201d, or \u201dyour food\u201d. A 3-gram (or trigram) will be a three-word sequence of words <b>like</b> \u201cplease eat your\u201d, or \u201ceat your food\u201d. N-gram language models estimate the probability of the last word given the previous words. For example, given the sequence of words \u201cplease eat your\u201d, the likelihood of the next word is higher for \u201cfood\u201d than for \u201cspoon\u201d. In the later case, our mom will be ...", "dateLastCrawled": "2022-02-02T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Representation and Computation in Cognitive</b> Models - Forbus - 2017 ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12277", "isFamilyFriendly": true, "displayUrl": "https://online<b>library</b>.wiley.com/doi/10.1111/tops.12277", "snippet": "We close with additional thoughts on open questions involving <b>representation</b> in cognition, and an <b>analogy</b>. 2 Setting the stage . There is a set of long\u2010standing misconceptions that need to be cleared out of the way first, so that we can focus on the matter at hand. 2.1 Misconception: Symbolic means serial, logical, and non\u2010numerical. Symbolic models have integrated structural and numerical information from the start of cognitive science. For example, semantic networks used relationships ...", "dateLastCrawled": "2019-12-29T03:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Analogical mapping and inference with</b> binary spatter codes and <b>sparse</b> ...", "url": "https://www.researchgate.net/publication/261148322_Analogical_mapping_and_inference_with_binary_spatter_codes_and_sparse_distributed_memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261148322_Analogical_mapping_and_inference...", "snippet": "<b>Representation</b> <b>Analogy</b> MApper (Drama) [10] is very close . to being a distributed re-implementation of ACME. Ho wever, although the source and target are represented with VSAs, the network which ...", "dateLastCrawled": "2021-11-09T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Analogy</b>-Based Inference Patterns in Pharmacological ... - <b>library</b>.oapen.org", "url": "https://library.oapen.org/bitstream/handle/20.500.12657/50322/Poellinger%202017%20Analogy-Based%20Inference%20Patterns%20in%20Pharmacological%20Research%20FINAL.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://<b>library</b>.oapen.org/bitstream/handle/20.500.12657/50322/Poellinger 2017 <b>Analogy</b>...", "snippet": "<b>Analogy</b>-Based Inference Patterns in Pharmacological Research Roland Poellinger Abstract Analogical arguments are ubiquitous vehicles of knowledge transfer in science and medicine. This paper outlines a Bayesian evidence-amalgamation framework for the purpose of formally exploring different <b>analogy</b>-based infer-ence patterns with respect to their justi\ufb01cation in pharmacological risk assessment. By relating formal explications of similarity, <b>analogy</b>, and analog simulation, three sources of ...", "dateLastCrawled": "2022-01-26T16:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Assignment 2 <b>CSEP 517: Natural Language Processing</b>", "url": "https://courses.cs.washington.edu/courses/csep517/17sp/assignments/assignment2.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.washington.edu/courses/csep517/17sp/assignments/assignment2.pdf", "snippet": "1.Decide on a <b>representation</b> of the distribution of contexts each word v 2V occurs in, as a vector. A common choice is a jVj-length vector containing the counts of all words occuring just before or just after (say, within two word positions) of some token of the word v.1 2.Compress these long, <b>sparse</b> vectors into smaller (roughly 100-dimensional) vectors, usually \u201cdense\u201d (few or no 0s) vector. While we don\u2019t have time to get into more details of standalone methods for constructing word ...", "dateLastCrawled": "2021-11-02T15:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Distributed Representation</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/distributed-representation", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>distributed-representation</b>", "snippet": "It turns out that such a <b>distributed representation</b> is <b>sparse</b>, because only a few of the neurons are active each time. This is in line with what we believe happens in the human brain, where at each time less than 5% of the neurons, in each layer, fire, and the rest remain inactive. In the antipodal of such distributive representations would be to have a single neuron firing each time. In the case of neural networks with more general (compared to 0 and 1) activation functions, the features ...", "dateLastCrawled": "2022-01-05T14:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Analogy</b> and Abstraction - Gentner - 2017 - Wiley Online <b>Library</b>", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12278", "isFamilyFriendly": true, "displayUrl": "https://online<b>library</b>.wiley.com/doi/10.1111/tops.12278", "snippet": "In this <b>analogy</b>, knowledge about a power supply is projected to the mitochondria. But <b>analogy</b> can also operate in mutual alignment1 1 Mutual alignment analogies are those that are processed by structural alignment with few or no projected candidate inferences, According to structure-mapping, all analogies involve structural alignment. Even when the base offers a clear projection (as in the goldmine example), one still must align that structure with the target&#39;s <b>representation</b> in order to ...", "dateLastCrawled": "2022-01-26T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "4. Distributed word representations \u2014 Natural Language Processing 0.0.1 ...", "url": "https://hhexiy.github.io/nlp/2021/notes/distributed_representation.html", "isFamilyFriendly": true, "displayUrl": "https://hhexiy.github.io/nlp/2021/notes/distributed_<b>representation</b>.html", "snippet": "Document <b>representation</b>\u00b6 Let\u2019s start with a familiar setting: we have a set of documents (e.g. movie reviews), now instead of classifying them, we would like to find out which ones are closer. From the last lecture, we already have a (<b>sparse</b>) vector <b>representation</b> for documents. Let\u2019s load the movie reviews.", "dateLastCrawled": "2022-01-29T13:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Code <b>Compass: boosting software reuse through machine learning</b>", "url": "https://www.code-compass.com/blog/intro/", "isFamilyFriendly": true, "displayUrl": "https://www.code-compass.com/blog/intro", "snippet": "The answer is yes: just like word embeddings learn to represent <b>similar</b> words by <b>similar</b> dense vector representations based on the words\u2019 <b>similar</b> context of use, we can learn a dense vector <b>representation</b> of libraries based on their context of use. By <b>analogy</b> with the term \u201cword vectors\u201d, we call our embeddings \u201c<b>library</b> vectors\u201d. The figure below illustrates the key idea:", "dateLastCrawled": "2021-12-25T18:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Representing text in natural language processing</b> | by Michel Kana, Ph.D ...", "url": "https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>representing-text-in-natural-language-processing</b>-1eead...", "snippet": "The process of transforming text into numeric stuff, <b>similar</b> to what we did with the image above, is usually performed by building a language model. These models typically assign probabilities, frequencies or some obscure numbers to words, sequences of words, group of words, section of documents or whole documents. The most common techniques are: 1-hot encoding, N-grams, Bag-of-words, vector semantics (tf-idf), distributional semantics (Word2vec, GloVe). Let\u2019s see if we understand what all ...", "dateLastCrawled": "2022-02-02T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What Are Word Embeddings</b> for Text?", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>what-are-word-embeddings</b>", "snippet": "Word embeddings are a type of word <b>representation</b> that allows words with <b>similar</b> meaning to have a <b>similar</b> <b>representation</b>. They are a distributed <b>representation</b> for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems. In this post, you will discover the word embedding approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How does Word2vec solve sparse problem</b>? - Quora", "url": "https://www.quora.com/How-does-Word2vec-solve-sparse-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>How-does-Word2vec-solve-sparse-problem</b>", "snippet": "Answer: Word2vec solves the sparsity problem of models that use word identities as a feature by projecting words to a low dimensional space. You are right that in a given textual collection, words might occur only with limited context. So the word vectors that you learn will be pretty weak - but ...", "dateLastCrawled": "2022-01-21T04:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Digital Signal Processing</b> - <b>library</b>.utia.cas.cz", "url": "http://library.utia.cas.cz/separaty/2016/ZOI/sorel-0459332.pdf", "isFamilyFriendly": true, "displayUrl": "<b>library</b>.utia.cas.cz/separaty/2016/ZOI/sorel-0459332.pdf", "snippet": "<b>can</b> <b>be thought</b> of as a tool for feature extraction. In signal recon-struction <b>sparse</b> coding <b>can</b> serve as a form of Bayesian prior for image denoising [3], inpainting [4], deblurring [5], super-resolution [6] and audio signal <b>representation</b> [7]. Although \ufb01nding the dic- tionary with which the training signals <b>can</b> be represented with optimal sparsity is strongly NP-hard [8], there is a number of ef-fective heuristic algorithms giving an approximate solution in poly-nomial time [9,10]. <b>Sparse</b> ...", "dateLastCrawled": "2022-01-24T22:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Sparse and Redundant Representations_ From Theory to Applications</b> ...", "url": "https://www.academia.edu/3774175/Sparse_and_Redundant_Representations_From_Theory_to_Applications_in_Signal_and_Image_Processing", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/3774175/<b>Sparse_and_Redundant_Representations_From_Theory</b>_to...", "snippet": "<b>Sparse and Redundant Representations_ From Theory to Applications</b> in Signal and Image Processing. Elham Alaee. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 36 Full PDFs related to this paper. Read Paper. <b>Sparse and Redundant Representations_ From Theory to Applications</b> in Signal and Image Processing. Download ...", "dateLastCrawled": "2022-01-22T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Homework 5 - Advanced Vector Space Models", "url": "https://computational-linguistics-class.org/homework/vector-semantics-2/vector-semantics-2.html", "isFamilyFriendly": true, "displayUrl": "https://computational-linguistics-class.org/homework/vector-semantics-2/vector...", "snippet": "Write a function cluster_with_<b>sparse</b>_<b>representation</b>(word_to_paraphrases_dict, word_to_k_dict). The input and output remains the same as in Task 1, however the clustering of paraphrases will no longer be random and is based on <b>sparse</b> vector <b>representation</b>. We will feature-based (not dense) vector space <b>representation</b>.", "dateLastCrawled": "2022-01-26T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Representation and Computation in Cognitive</b> Models - Forbus - 2017 ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12277", "isFamilyFriendly": true, "displayUrl": "https://online<b>library</b>.wiley.com/doi/10.1111/tops.12277", "snippet": "We close with additional thoughts on open questions involving <b>representation</b> in cognition, and an <b>analogy</b>. 2 Setting the stage. There is a set of long\u2010standing misconceptions that need to be cleared out of the way first, so that we <b>can</b> focus on the matter at hand. 2.1 Misconception: Symbolic means serial, logical, and non\u2010numerical. Symbolic models have integrated structural and numerical information from the start of cognitive science. For example, semantic networks used relationships ...", "dateLastCrawled": "2019-12-29T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Analogy</b> and Abstraction - Gentner - 2017 - Wiley Online <b>Library</b>", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12278", "isFamilyFriendly": true, "displayUrl": "https://online<b>library</b>.wiley.com/doi/10.1111/tops.12278", "snippet": "<b>Analogy</b> is often <b>thought</b> of chiefly as a way to transfer knowledge from one situation to another, and indeed, it often serves that function. Projecting information from a well-understood domain <b>can</b> lend structure to an unfamiliar domain, as in: The mitochondria are the power supply for a cell. In this <b>analogy</b>, knowledge about a power supply is projected to the mitochondria. But <b>analogy</b> <b>can</b> also operate in mutual alignment1 1 Mutual alignment analogies are those that are processed by ...", "dateLastCrawled": "2022-01-26T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Representing text in natural language processing</b> | by Michel Kana, Ph.D ...", "url": "https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>representing-text-in-natural-language-processing</b>-1eead...", "snippet": "Therefore we <b>can</b> define a word by counting what other words occur in its environment, and we <b>can</b> represent the word by a vector, a list of numbers, a point in N-dimensional space. Such a <b>representation</b> is usually called embedding. Computer <b>can</b> use this cheating trick to understand the meaning of words in its context. Word-document <b>representation</b>", "dateLastCrawled": "2022-02-02T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Why do we use word embeddings in NLP? | by Natasha Latysheva | Towards ...", "url": "https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2?source=post_page-----2f20e1b632d2--------------------------------", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2?source=...", "snippet": "Ideally, whatever numerical <b>representation</b> method we come up with would be semantically meaningful \u2014 the numerical values should capture as much of the linguistic meaning of a word as possible. A well-chosen, informative input <b>representation</b> <b>can</b> have a massive impact on overall model performance.", "dateLastCrawled": "2022-01-19T22:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Representation</b> - Open Computing Facility", "url": "https://www.ocf.berkeley.edu/~jfkihlstrom/MemoryWeb/storage/representation.htm", "isFamilyFriendly": true, "displayUrl": "https://www.ocf.berkeley.edu/~jfkihlstrom/MemoryWeb/storage/<b>representation</b>.htm", "snippet": "As far as I <b>can</b> tell, Piaget first employed the schema concept in The Language and <b>Thought</b> of the Child (1926), so one would not expect Piaget to cite Bartlett. But Bartlett didn&#39;t cite Piaget, either. My best guess is that they derived the idea independently -- Bartlett from Henry Head, and Piaget from Immanuel Kant. Oldfield and Zangwill (1942-1943) do not cite Piaget in their discussion of Head and Bartlett, and deny any connection between Bartlett&#39;s views and Kant.", "dateLastCrawled": "2022-01-30T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Can</b> <b>Analogy Help in Science Education Research</b>?", "url": "https://www.researchgate.net/publication/226143368_Can_Analogy_Help_in_Science_Education_Research", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/226143368_<b>Can</b>_<b>Analogy</b>_Help_in_Science...", "snippet": "of analogies and metaphors is important in science itself and their use in teachi ng. science seems a natural extensio n, but textbooks with their own <b>sparse</b> logic, do not. help teachers or ...", "dateLastCrawled": "2021-10-16T12:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How does one measure if one has <b>a good word vector representation</b>? - Quora", "url": "https://www.quora.com/How-does-one-measure-if-one-has-a-good-word-vector-representation", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-does-one-measure-if-one-has-<b>a-good-word-vector-representation</b>", "snippet": "Answer (1 of 3): There are different tasks to measure how good your vectors are. The tasks are divided as intrinsic and extrinsic tasks. Intrinsic tasks * Word similarity evaluation on wordsim353 dataset, simlex999 * Word <b>analogy</b> test provided by Mikolov. Extrinsic tasks * Sentence classific...", "dateLastCrawled": "2022-01-23T01:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse</b> Binary Relation Representations for Genome Graph Annotation ...", "url": "https://www.liebertpub.com/doi/full/10.1089/cmb.2019.0324?d=1/9/2020&mcid=823745612", "isFamilyFriendly": true, "displayUrl": "https://www.liebertpub.com/doi/full/10.1089/cmb.2019.0324?d=1/9/2020&amp;mcid=823745612", "snippet": "Column-major <b>sparse</b> matrix <b>representation</b>. As a simple baseline technique, we compress the positions of the nonzero indices in each column independently using Elias\u2013Fano encoding (Okanohara and Sadakane, 2007). Although this method does not take into account similarity between columns for compression, this feature allows for a trivial parallel construction implementation in which each column is computed in a separate process. For our experiments, this serves as the initial <b>representation</b> ...", "dateLastCrawled": "2021-09-16T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Sparse and Redundant Representations_ From Theory to Applications</b> ...", "url": "https://www.academia.edu/3774175/Sparse_and_Redundant_Representations_From_Theory_to_Applications_in_Signal_and_Image_Processing", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/3774175/<b>Sparse_and_Redundant_Representations_From_Theory</b>_to...", "snippet": "<b>Sparse and Redundant Representations_ From Theory to Applications</b> in Signal and Image Processing. Elham Alaee. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 36 Full PDFs related to this paper. Read Paper. <b>Sparse and Redundant Representations_ From Theory to Applications</b> in Signal and Image Processing. Download ...", "dateLastCrawled": "2022-01-22T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparse</b> <b>methods for direction-of-arrival estimation</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/B9780128118870000110", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/B9780128118870000110", "snippet": "With the development of <b>sparse</b> <b>representation</b> and compressed sensing, the last decade has witnessed a tremendous advance in this research topic. The purpose of this article is to provide an overview of these <b>sparse</b> methods for DOA estimation, with a particular highlight on the recently developed gridless <b>sparse</b> methods, e.g., those based on covariance fitting and the atomic norm. Several future research directions are also discussed.", "dateLastCrawled": "2022-01-29T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Analogical mapping and inference with</b> binary spatter codes and <b>sparse</b> ...", "url": "https://www.researchgate.net/publication/261148322_Analogical_mapping_and_inference_with_binary_spatter_codes_and_sparse_distributed_memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261148322_Analogical_mapping_and_inference...", "snippet": "Vector symbolic architectures (VSAs) are a class of connectionist models for the <b>representation</b> and manipulation of compositional structures, which <b>can</b> be used to model <b>analogy</b>. We study a novel ...", "dateLastCrawled": "2021-11-09T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Representation</b> - Open Computing Facility", "url": "https://www.ocf.berkeley.edu/~jfkihlstrom/MemoryWeb/storage/representation.htm", "isFamilyFriendly": true, "displayUrl": "https://www.ocf.berkeley.edu/~jfkihlstrom/MemoryWeb/storage/<b>representation</b>.htm", "snippet": "<b>Sparse</b> distributed representations often are often used to solve the problem of catastrophic interference, because one <b>sparse</b> coding <b>can</b> be used for A-B, and another for A-C. They learn rapidly, too. But they don&#39;t show much by way of generalization -- which is as bad a problem, for a model of learning, as catastrophic interference.", "dateLastCrawled": "2022-01-30T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Code <b>Compass: boosting software reuse through machine learning</b>", "url": "https://www.code-compass.com/blog/intro/", "isFamilyFriendly": true, "displayUrl": "https://www.code-compass.com/blog/intro", "snippet": "<b>Compared</b> to a <b>sparse</b> encoding, a dense encoding typically has a far smaller number of dimensions (on the order of 100 or 200), and each entry in the vector is now no longer just a binary 0 or 1, but a scalar value. However, <b>compared</b> to the <b>sparse</b> encoding, the \u201cmeaning\u201d of each dimension is no longer obvious: it is learned by a machine learning model from data, rather than assigned by a human. To determine the similarity of two dense vector embeddings, one <b>can</b> compute their cosine ...", "dateLastCrawled": "2021-12-25T18:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Analogy</b> and Abstraction - Gentner - 2017 - Wiley Online <b>Library</b>", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12278", "isFamilyFriendly": true, "displayUrl": "https://online<b>library</b>.wiley.com/doi/10.1111/tops.12278", "snippet": "But <b>analogy</b> <b>can</b> also operate in mutual alignment1 1 Mutual alignment analogies are those that are processed by structural alignment with few or no projected candidate inferences, According to structure-mapping, all analogies involve structural alignment. Even when the base offers a clear projection (as in the goldmine example), one still must align that structure with the target&#39;s <b>representation</b> in order to comprehend it. For example, if you heard, \u201cThat fork is a goldmine,\u201d you would ...", "dateLastCrawled": "2022-01-26T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>How does Word2vec solve sparse problem</b>? - Quora", "url": "https://www.quora.com/How-does-Word2vec-solve-sparse-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>How-does-Word2vec-solve-sparse-problem</b>", "snippet": "Answer: Word2vec solves the sparsity problem of models that use word identities as a feature by projecting words to a low dimensional space. You are right that in a given textual collection, words might occur only with limited context. So the word vectors that you learn will be pretty weak - but ...", "dateLastCrawled": "2022-01-21T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Robust approach for AMC in frequency selective ... - Wiley Online <b>Library</b>", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-com.2018.5688", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.online<b>library</b>.wiley.com/doi/10.1049/iet-com.2018.5688", "snippet": "Each layer is further composed of several nodes, called neurons (from human brain <b>analogy</b>). Neurons in the input layer have the same dimension as that of the training/input data or feature space. The hidden layer learns a transformation function on the basis of training examples. The output layer represents the number of targets amongst which an input is supposed to be classified. We consider a special case of unsupervised feature learning based neural network called <b>sparse</b>-autoencoder and ...", "dateLastCrawled": "2022-01-28T19:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why aren&#39;t <b>hash tables used to store sparse matrices</b>? - Quora", "url": "https://www.quora.com/Why-arent-hash-tables-used-to-store-sparse-matrices", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-arent-<b>hash-tables-used-to-store-sparse-matrices</b>", "snippet": "Answer (1 of 3): They could be, and that might be a reasonable choice if the order of accesses to the matrix were completely random. However, we don\u2019t typically access the elements of a <b>sparse</b> matrix in random order. If elements in a row or column are more typically accessed in sequence, then it ...", "dateLastCrawled": "2022-01-15T03:15:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Neural Networks: Analogies. When our brains form analogies, they\u2026 | by ...", "url": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-networks-analogies-7ebeb3ac5d5e", "snippet": "I\u2019ll outline a potential route to artificial neural networks which exhibit transfer <b>learning</b>: First, <b>Sparse</b> Distributed Representations. Numenta\u2019s Hierarchical Te m poral Memory, along with other techniques, relies upon a <b>sparse</b> distributed <b>representation</b>. An example of this is a very long string of ones and zeroes, where almost all the values are zero \u2014 there is a <b>sparse</b> distribution of the ones. If each digit represented a different thing, like \u2018pointy ears\u2019, \u2018tail ...", "dateLastCrawled": "2022-01-28T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Compressed Sensing Meets <b>Machine</b> <b>Learning</b>: Classification via <b>Sparse</b> ...", "url": "https://nuit-blanche.blogspot.com/2008/05/cs-mini-course-classification-via.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2008/05/cs-mini-course-classification-via.html", "snippet": "Compressed Sensing Meets <b>Machine</b> <b>Learning</b>: Classification via <b>Sparse</b> <b>Representation</b> and Distributed Pattern Recognition This Spring, Allen Yang has given a mini course at Berkeley entitled Compressed Sensing Meets <b>Machine</b> <b>Learning</b>. The three lectures are listed here (it includes accompanying code): lecture 1: Classification via <b>Sparse</b> <b>Representation</b>; lecture 2: Classification of Mixture Subspace Models via <b>Sparse</b> <b>Representation</b>, lecture 3: Distributed Pattern Recognition; The third lecture ...", "dateLastCrawled": "2022-01-25T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "Word embeddings are a type of word <b>representation</b> that allows words with similar meaning to have a similar <b>representation</b>. They are a distributed <b>representation</b> for text that is perhaps one of the key breakthroughs for the impressive performance of deep <b>learning</b> methods on challenging natural language processing problems. In this post, you will discover the word embedding approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Word Embedding: Syntactics or Semantics</b> \u00b7 Shengbin&#39;s Studio", "url": "https://wushbin.github.io/2017/10/09/Word-Embedding-Syntactics-or-Semantics/", "isFamilyFriendly": true, "displayUrl": "https://wushbin.github.io/2017/10/09/<b>Word-Embedding-Syntactics-or-Semantics</b>", "snippet": "<b>Sparse</b> Vector <b>Representation</b>. The co-occurrence matrix in represented each cell by the raw frequency of the co-occurrence of two words. The raw frequency in a matrix may be skewed. Pointwise mutual information PPMI is a good measure for association between words which can tell us how much often the two words occur. The pointwise mutual information is a measure of how often two events x and y occur, compared with what we would expect if they were independent: PMI between two words is ...", "dateLastCrawled": "2022-01-09T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Accelerating Innovation Through <b>Analogy</b> Mining", "url": "http://hyadatalab.com/papers/analogy-kdd17.pdf", "isFamilyFriendly": true, "displayUrl": "hyadatalab.com/papers/<b>analogy</b>-kdd17.pdf", "snippet": "<b>machine</b> <b>learning</b> models that develop similarity metrics suited for <b>analogy</b> mining. We demonstrate that <b>learning</b> purpose and mechanism representations allows us to \u2022nd analogies with higher precision and recall than traditional information-retrieval methods based on TF-IDF, LSA, LDA and GlOVe, in challenging noisy set-tings. Furthermore, we ...", "dateLastCrawled": "2022-01-29T02:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Art of Vector <b>Representation</b> of Words | by ASHISH RANA | Towards Data ...", "url": "https://towardsdatascience.com/art-of-vector-representation-of-words-5e85c59fee5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/art-of-vector-<b>representation</b>-of-words-5e85c59fee5", "snippet": "Like, huge memory required for storing and processing such vectors. Along, with <b>sparse</b> nature of these vectors.For example, the size of |V| is very large like 3M for Google 1T corpus. This notation will fail in terms of computation overhead caused by <b>representation</b> power of this system. Also, no notion of similarity is captured. Cosine similarity b/w unique words is zero and Euclidean distance is always sqrt(2). Meaning, no semantic information is getting expressed with this <b>representation</b> ...", "dateLastCrawled": "2022-01-30T21:17:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse Adaptive Local Machine Learning</b> Algorithms for Sensing and Analytics", "url": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1000&context=mcecs_mentoring", "isFamilyFriendly": true, "displayUrl": "https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1000&amp;context=mcecs...", "snippet": "Fig. 2: A <b>sparse representation can be thought of as</b> the dot product of a dictionary vector and a sparse code vector. Given a . dictionary . of general components, we can use a . sparse code. to select as few of them as possible to reconstruct an image of interest (Fig. 2). This reconstruction is called a . sparse representation. Sparse Coding. Image processing is expensive. Instead of working with the original image, we can identify its most relevant components and discard the rest. This ...", "dateLastCrawled": "2021-08-31T12:20:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparse representation)  is like +(library analogy)", "+(sparse representation) is similar to +(library analogy)", "+(sparse representation) can be thought of as +(library analogy)", "+(sparse representation) can be compared to +(library analogy)", "machine learning +(sparse representation AND analogy)", "machine learning +(\"sparse representation is like\")", "machine learning +(\"sparse representation is similar\")", "machine learning +(\"just as sparse representation\")", "machine learning +(\"sparse representation can be thought of as\")", "machine learning +(\"sparse representation can be compared to\")"]}