{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss Functions in Deep Learning: An Overview</b>", "url": "https://analyticsindiamag.com/loss-functions-in-deep-learning-an-overview/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>loss-functions-in-deep-learning-an-overview</b>", "snippet": "A data analyst with expertise in statistical analysis, data visualization ready to serve the industry using various analytical platforms. I look forward to having in-depth knowledge of machine learning and data science.", "dateLastCrawled": "2022-02-02T02:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is the difference between a <b>loss function</b> and an <b>error</b> function ...", "url": "https://stats.stackexchange.com/questions/359043/what-is-the-difference-between-a-loss-function-and-an-error-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/359043", "snippet": "Thanks for contributing an answer to Cross Validated! Please be sure to answer the question.Provide details and share your research! But avoid \u2026. Asking for help, clarification, or responding to other answers.", "dateLastCrawled": "2022-02-01T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "Quadratic (<b>Like</b> MSE) for small values, and linear for large values (<b>like</b> MAE). The Huber <b>loss</b> combines both MSE and MAE. Here you can see the graph with the different tuning of the hyperparameter ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "<b>Loss</b> functions are different based on your problem statement to which machine learning is being applied. The cost function is another term used interchangeably for the <b>loss</b> function, but it holds a slightly different meaning. A <b>loss</b> function is for a single training example, while a cost function is an average <b>loss</b> over the complete train dataset.", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Keras</b> <b>Loss</b> Functions: Everything You Need to Know - neptune.ai", "url": "https://neptune.ai/blog/keras-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>keras</b>-<b>loss</b>-functions", "snippet": "LogCosh <b>Loss</b> works <b>like</b> the mean squared <b>error</b>, but will not be so strongly affected by the occasional wildly incorrect prediction. \u2014 TensorFlow Docs. Huber <b>loss</b>. For regression problems that are less sensitive to outliers, the Huber <b>loss</b> is used. y_true = [12, 20, 29., 60.] y_pred = [14., 18., 27., 55.] h = tf.<b>keras</b>.losses.Huber() h(y_true, y_pred).numpy() Learning Embeddings Triplet <b>Loss</b> . You can also compute the triplet <b>loss</b> with semi-hard negative mining via TensorFlow addons. The ...", "dateLastCrawled": "2022-02-02T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding the 3 most common <b>loss</b> functions for Machine Learning ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Losses</b> - Keras", "url": "https://keras.io/api/losses/", "isFamilyFriendly": true, "displayUrl": "https://keras.io/api/<b>losses</b>", "snippet": "The add_<b>loss</b>() API. <b>Loss</b> functions applied to the output of a model aren&#39;t the only way to create <b>losses</b>. When writing the call method of a custom layer or a subclassed model, you may want to compute scalar quantities that you want to minimize during training (e.g. regularization <b>losses</b>). You can use the add_<b>loss</b>() layer method to keep track of such <b>loss</b> terms.. Here&#39;s an example of a layer that adds a sparsity regularization <b>loss</b> based on the L2 norm of the inputs:", "dateLastCrawled": "2022-02-03T13:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "<b>Loss</b> functions are a key part of any machine learning model: they define an objective against which the performance of your model is measured, and the setting of weight parameters learned by the model is determined by minimizing a chosen <b>loss</b> function. There are several different common <b>loss</b> functions to choose from: the cross-entropy <b>loss</b>, the mean-squared <b>error</b>, the huber <b>loss</b>, and the hinge <b>loss</b> - just to name a few. Given a particular model, each <b>loss</b> function has particular properties ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "A <b>loss</b> function is a measure of how good a prediction model does in terms of being able to predict the expected outcome. A most commonly used method of finding the minimum point of function is \u201cgradient descent\u201d. Think of <b>loss</b> function <b>like</b> undulating mountain and gradient descent <b>is like</b> sliding down the mountain to reach the bottommost point.", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss Functions in Deep Learning: An Overview</b>", "url": "https://analyticsindiamag.com/loss-functions-in-deep-learning-an-overview/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>loss-functions-in-deep-learning-an-overview</b>", "snippet": "A data analyst with expertise in statistical analysis, data visualization ready to serve the industry using various analytical platforms. I look forward to having in-depth knowledge of machine learning and data science.", "dateLastCrawled": "2022-02-02T02:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "model.compile(<b>loss</b>=&#39;...&#39;, optimizer=opt) # fit model. history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0) Now that we have the basis of a problem and model, we can take a look evaluating three common <b>loss</b> functions that are appropriate for a regression predictive modeling problem.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "There are several different common <b>loss</b> functions to choose from: the cross-entropy <b>loss</b>, the mean-squared <b>error</b>, the huber <b>loss</b>, and the hinge <b>loss</b> - just to name a few. Given a particular model, each <b>loss</b> function has particular properties that make it interesting - for example, the (L2-regularized) hinge <b>loss</b> comes with the maximum-margin ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Mean Squared <b>Error</b> vs Cross entropy <b>loss</b> function - Data Analytics", "url": "https://vitalflux.com/mean-squared-error-vs-cross-entropy-loss-function/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/mean-squared-<b>error</b>-vs-cross-entropy-<b>loss</b>-function", "snippet": "Cross-entropy <b>loss</b> is very <b>similar</b> to cross entropy. They both measure the difference between an actual probability and predicted probability, but cross entropy uses log probabilities while cross-entropy <b>loss</b> uses negative log probabilities (which are then multiplied by -log(p)) . Log probabilities can be converted into regular numbers for ease of computation using a softmax function. Cross-entropy <b>loss</b> is also used in time series classification problems such as forecasting weather or stock ...", "dateLastCrawled": "2022-02-03T10:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>loss</b>-functions-and-optimization-algorithms...", "snippet": "Embedding <b>loss</b> functions: It deals with problems where we have to measure whether two inputs are <b>similar</b> or dissimilar. Some examples are: 1. L1 Hinge <b>Error</b>- Calculates the L1 distance between two ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Cross-entropy <b>loss</b> is often simply referred to as \u201ccross-entropy,\u201d \u201clogarithmic <b>loss</b>,\u201d \u201clogistic <b>loss</b>,\u201d or \u201clog <b>loss</b>\u201d for short. Each predicted probability is compared to the actual class output value (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected value.", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Training Loss</b> and Validation <b>Loss</b> in Deep Learning - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/48226086/training-loss-and-validation-loss-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/48226086", "snippet": "The <b>loss</b> is calculated as a moving average over all processed batches, meaning that in the early training stage when <b>loss</b> drops quickly the first batch of an epoch will have a much higher <b>loss</b> than the last. When the epoch is finished, the shown <b>training loss</b> will NOT represent the <b>training loss</b> at the end of the epoch but the average <b>training loss</b> from start to end of the epoch. Therefore, it&#39;s oftentimes higher than the validation <b>loss</b>, which is calculated at the end of the epoch entirely.", "dateLastCrawled": "2022-02-03T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is the relationship between the <b>accuracy</b> and the <b>loss</b> in deep ...", "url": "https://datascience.stackexchange.com/questions/42599/what-is-the-relationship-between-the-accuracy-and-the-loss-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/42599", "snippet": "<b>Loss</b> depends on how you predict classes for your classification problem. For example, your model use probabilities to predict binary class cat or non-cats between 1 and 0. So if probability of cat is 0.6, then the probability of non-cat is 0.4. In this case, picture is classified as cat. <b>Loss</b> will be sum of the difference between predicted probability of the real class of the test picture and 1. In reality log <b>loss</b> is used for binary classification, I just gave the idea of what <b>loss</b> is.", "dateLastCrawled": "2022-01-26T03:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "neural network - Validation MSE <b>loss is different from validation</b> MSE ...", "url": "https://stackoverflow.com/questions/42258829/validation-mse-loss-is-different-from-validation-mse-metric", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/42258829", "snippet": "I have written a model in Keras (with theano backend) and compile my model like this: model.compile(Adam(0.001), <b>loss</b>=&#39;mse&#39;, metrics=[&#39;mse&#39;, &#39;mae&#39;]) i.e. my objective <b>loss</b> function is mean squared...", "dateLastCrawled": "2022-01-22T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - how to weight KLD <b>loss</b> vs reconstruction <b>loss</b> in ...", "url": "https://stats.stackexchange.com/questions/332179/how-to-weight-kld-loss-vs-reconstruction-loss-in-variational-auto-encoder", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/332179", "snippet": "(One can extend to softmax for multiclass classification by using a categorical distribution to lead to cross-entropy <b>loss</b>.) For MNIST (or black and white images) data set, each pixel is either 0 or 1, and therefore, we can use binary cross-entropy <b>loss</b> as the reconstruction <b>loss</b> in the VAE to predict the probability that the value of a pixel is 1.", "dateLastCrawled": "2022-02-02T16:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>loss</b>-functions-and-optimization-algorithms...", "snippet": "The choice of Optimisation Algorithms and <b>Loss</b> Functions for a deep learning model <b>can</b> play a big role in producing optimum and faster results. Before we begin, let us see how different components ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Grief and <b>loss</b> - the effects of grief and how to deal with it ...", "url": "https://www.healthdirect.gov.au/grief-loss", "isFamilyFriendly": true, "displayUrl": "https://www.healthdirect.gov.au/grief-<b>loss</b>", "snippet": "Grief is the natural emotional response to the <b>loss</b> of someone close, such as a family member or friend. Grief <b>can</b> also occur after a serious illness, a divorce or other significant losses. Grief often involves intense sadness, and sometimes feelings of shock, numbness or even denial and anger. For most people, healing occurs with time.", "dateLastCrawled": "2022-02-02T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Uncertainties and Error Propagation</b>", "url": "https://www.geol.lsu.edu/jlorenzo/geophysics/uncertainties/Uncertaintiespart2.html", "isFamilyFriendly": true, "displayUrl": "https://www.geol.lsu.edu/jlorenzo/geophysics/uncertainties/Uncertaintiespart2.html", "snippet": "In the case of addition and subtraction we <b>can</b> best explain with an example. Suppose one object is measured to have a mass of 9.9 gm and a second object is measured on a different balance to have a mass of 0.3163 gm. What is the total mass? We write the numbers with question marks at places where we lack information. Thus 9.9???? gm and 0.3163? gm. Adding them with the decimal points lined up we see . 09.9???? 00.3163? 10.2???? = 10.2 gm. In the case of multiplication or division we <b>can</b> use ...", "dateLastCrawled": "2022-02-03T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Windows 10 Blue Screen of Death \u2013 You <b>Can</b> Quickly Fix It", "url": "https://www.partitionwizard.com/partitionmagic/blue-screen-error-during-windows-10-upgrade.html", "isFamilyFriendly": true, "displayUrl": "https://www.partitionwizard.com/partitionmagic/blue-screen-<b>error</b>-during-windows-10...", "snippet": "After knowing causes for Windows 10 Stop <b>Error</b>, next let&#39;s talk about how to fix Windows 10 BSOD. Before repairing Windows 10 BSOD, you&#39;d better recover the data or back up all useful data saved in the system partition via MiniTool Partition Wizard Pro Ultimate Edition and Pro Edition respectively, which could help you avoid data <b>loss</b>. In ...", "dateLastCrawled": "2022-02-02T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Negative mean squared error</b> ? | Data Science and Machine Learning | Kaggle", "url": "https://www.kaggle.com/questions-and-answers/154600", "isFamilyFriendly": true, "displayUrl": "https://www.kaggle.com/questions-and-answers/154600", "snippet": "What you are looking for is a maximization problem i.e. wanting R2 to be close to 1. The sklearn implementation is done as a minimization problem so either you <b>can</b> implement a custom method or take the absolute value of your result. Note that keras, pytorch and tf implementation for mse is what you would expect. It <b>can</b> get slightly confusing.", "dateLastCrawled": "2022-02-03T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What does RMSE really mean?. Root <b>Mean Square</b> <b>Error</b> (RMSE) is a\u2026 | by ...", "url": "https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e", "snippet": "These errors, <b>thought</b> of as random variables, might have Gaussian distribution with mean \u03bc and standard deviation \u03c3, but any other distribution with a square-integrable PDF (probability density function) would also work.We want to think of \u0177\u1d62 as an underlying physical quantity, such as the exact distance from Mars to the Sun at a particular point in time.", "dateLastCrawled": "2022-02-02T23:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "There Is No <b>Such Thing as Conscious Thought</b> - <b>Scientific American</b>", "url": "https://www.scientificamerican.com/article/there-is-no-such-thing-as-conscious-thought/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.scientificamerican.com</b>/article/there-is-no-<b>such-thing-as-conscious-thought</b>", "snippet": "There Is No <b>Such Thing as Conscious Thought</b>. Philosopher Peter Carruthers insists that conscious <b>thought</b>, judgment and volition are illusions. They arise from processes of which we are forever ...", "dateLastCrawled": "2022-01-28T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Doc Rivers addresses Seth Curry only taking two 3s in <b>loss</b> to Wizards", "url": "https://sixerswire.usatoday.com/2022/01/17/doc-rivers-addresses-seth-curry-only-taking-two-3s-in-loss-to-wizards/", "isFamilyFriendly": true, "displayUrl": "https://sixerswire.usatoday.com/2022/01/17/doc-rivers-addresses-seth-curry-only-taking...", "snippet": "I <b>thought</b> he went too far separation in getting him another shot which as a team, you gotta see that. Especially when a guy makes the first shot. That\u2019s where we lack at times in terms of court generals on the floor seeing that. I had to call a timeout to call a play to get him involved and that shouldn\u2019t happen, but it was just one of those games.\u201d", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Browns <b>can</b>\u2019t finish, brutal <b>loss</b> drops them outside playoffs | The Star", "url": "https://www.thestar.com/sports/football/2021/12/21/browns-cant-finish-brutal-loss-drops-them-outside-playoffs.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.thestar.com</b>/sports/football/2021/12/21/browns-<b>can</b>t-finish-brutal-<b>loss</b>-drops...", "snippet": "Browns <b>can</b>\u2019t finish, brutal <b>loss</b> drops them outside playoffs. CLEVELAND (AP) \u2014 Improbable ended up being impossible for the snake-bit Browns. Three seconds from pulling out an all-hands-on ...", "dateLastCrawled": "2022-01-29T13:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Warriors&#39; inability to find Steph Curry with game on line hurts", "url": "https://www.mercurynews.com/2021/12/29/warriors-deserve-credit-for-comeback-criticism-for-late-execution-in-loss-to-nuggets/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mercurynews.com</b>/2021/12/29/warriors-deserve-credit-for-comeback-criticism...", "snippet": "The 19-year-old lottery pick saw an opportunity to challenge Nuggets star Nikola Joki\u0107, but was denied near the rim on a play the Warriors <b>thought</b> could have been whistled for a shooting foul.", "dateLastCrawled": "2022-02-01T12:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "<b>Loss</b> functions are different based on your problem statement to which machine learning is being applied. The cost function is another term used interchangeably for the <b>loss</b> function, but it holds a slightly different meaning. A <b>loss</b> function is for a single training example, while a cost function is an average <b>loss</b> over the complete train dataset.", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "When you face large errors due to outliers, you <b>can</b> try again with a lower \\(\\delta\\); if your errors are too small to be picked up by your Huber <b>loss</b>, you <b>can</b> increase the delta instead. And there\u2019s another thing, which we also mentioned when discussing the MAE: it produces large gradients when you optimize your model by means of gradient descent, even when your errors are small (Grover, 2019).", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>loss</b>-functions-and-optimization-algorithms...", "snippet": "Embedding <b>loss</b> functions: It deals with problems where we have to measure whether two inputs are similar or dissimilar. Some examples are: 1. L1 Hinge <b>Error</b>- Calculates the L1 distance between two ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>What Are Loss Functions in ML</b> <b>And Why Are They Important</b>", "url": "https://analyticsindiamag.com/what-are-loss-functions-in-ml-and-why-are-they-important/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>what-are-loss-functions-in-ml</b>-<b>and-why-are-they-important</b>", "snippet": "Mean-Squared <b>error</b>; Cross-entropy <b>loss</b>; Hinge <b>loss</b>; Huber <b>Loss</b>; Mean-Squared <b>Loss</b>. Mean Squared <b>Error</b>(MSE) is used to measure the accuracy of an estimator. The lesser the value of MSE, the better are the predictions. This expression <b>can</b> be defined as the mean value of the squared deviations of the predicted values from that of true values. Here \u2018n\u2019 denotes the total number of samples in the data. Cross-Entropy. Cross entropy is a widely popular concept of information theory. It is the ...", "dateLastCrawled": "2022-02-02T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "For example, the cross-entropy <b>loss</b> would invoke a much higher <b>loss</b> than the hinge <b>loss</b> if our (un-normalized) scores were \\([10, 8, 8]\\) versus \\([10, -10, -10]\\), where the first class is correct. In fact, the (multi-class) hinge <b>loss</b> would recognize that the correct class score already exceeds the other scores by more than the margin, so it will invoke zero <b>loss</b> on both scores. Once the margins are satisfied, the SVM will no longer optimize the weights in an attempt to \u201cdo better ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Common <b>Loss</b> functions in machine learning | by Ravindra Parmar ...", "url": "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/common-<b>loss</b>-functions-in-machine-learning-46af0ffc4d23", "snippet": "Broadly, <b>loss</b> functions <b>can</b> be classified into two major categories depending upon the type of learning task we are dealing with \u2014 Regression losses and Classification losses. In classification, we are trying to predict output from set of finite categorical values i.e Given large data set of images of hand written digits, categorizing them into one of 0\u20139 digits. Regression, on the other hand, deals with predicting a continuous value for example given floor area, number of rooms, size of ...", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "Now that we have the basis of a problem and model, we <b>can</b> take a look evaluating three common <b>loss</b> functions that are appropriate for a binary classification predictive modeling problem. Although an MLP is used in these examples, the same <b>loss</b> functions <b>can</b> be used when training CNN and RNN models for binary classification. Binary Cross-Entropy ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "L1 and <b>L2: loss function and regularization</b> | Develop Paper", "url": "https://developpaper.com/l1-and-l2-loss-function-and-regularization/", "isFamilyFriendly": true, "displayUrl": "https://developpaper.com/l1-and-<b>l2-loss-function-and-regularization</b>", "snippet": "Of the least square <b>loss</b> functionL2 regularization\uff1a L2 regularization refers to theThe sum of squares and then the square root. effect. L1 regularization. Advantages: the output is sparse, that is, a sparse model <b>can</b> be generated, which <b>can</b> be used for feature selection; to a certain extent, L1 <b>can</b> also prevent over fitting", "dateLastCrawled": "2022-02-03T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Training Loss</b> and Validation <b>Loss</b> in Deep Learning - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/48226086/training-loss-and-validation-loss-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/48226086", "snippet": "If validation <b>loss</b> &lt;&lt; <b>training loss</b> you <b>can</b> call it underfitting. Your aim is to make the validation <b>loss</b> as low as possible. Some overfitting is nearly always a good thing. All that matters in the end is: is the validation <b>loss</b> as low as you <b>can</b> get it. This often occurs when the <b>training loss</b> is quite a bit lower. Also check how to prevent overfitting. Share. Follow edited Feb 3 &#39;21 at 12:24. answered Jun 2 &#39;19 at 11:45. prosti prosti. 32.8k 8 8 gold badges 152 152 silver badges 134 134 ...", "dateLastCrawled": "2022-02-03T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>the advantage/disadvantage of Hinge-loss compared</b> to cross ...", "url": "https://www.quora.com/What-is-the-advantage-disadvantage-of-Hinge-loss-compared-to-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-advantage-disadvantage-of-Hinge-loss-compared</b>-to...", "snippet": "Answer (1 of 2): Cross Entropy (or Log <b>Loss</b>), Hing <b>Loss</b> (SVM <b>Loss</b>), Squared <b>Loss</b> etc. are different forms of <b>Loss</b> functions. Log <b>Loss</b> in the classification context gives Logistic Regression, while the Hinge <b>Loss</b> is Support Vector Machines. Logistic Regression and SVMs both are linear classifiers,...", "dateLastCrawled": "2022-01-29T16:27:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> fundamentals I: An <b>analogy</b> | Finn Rietz.dev", "url": "http://www.finnrietz.dev/machine%20learning/part-1-analogy/", "isFamilyFriendly": true, "displayUrl": "www.finnrietz.dev/<b>machine</b> <b>learning</b>/part-1-<b>analogy</b>", "snippet": "And this is what the <b>loss</b> function does, so the <b>loss</b> function for a <b>Machine</b> <b>learning</b> algorithm is like the teacher for the real-world dermatologist in-training. In mathematical terms, the <b>loss</b> function could look something like this: \\(L = (y_i - \\hat{y_i})^2\\), where \\(y_i\\) is the actual output value (the one that the teacher has written down) and \\(\\hat{y_i}\\) is the one our <b>learning</b> algorithm produced.", "dateLastCrawled": "2022-01-16T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "Minimize a <b>loss</b> function in ... and it has been used for conducting research and for deploying <b>machine</b> <b>learning</b> systems into production across more than a dozen areas of computer science and other ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Is training <b>a neural network</b> like forming a habit? | Blog", "url": "https://jmsbrdy.com/blog/habit-formation-as-analogy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://jmsbrdy.com/blog/habit-formation-as-<b>analogy</b>-for-<b>machine</b>-<b>learning</b>", "snippet": "Reward: during backpropagation, update our input weights according to the <b>loss</b> function; In fact, the <b>analogy</b> also works at the level of the network as a whole: Cue: transform our input example and input it into the first layer of the network; Routine: the network processes the input through its layers to produce a result; Reward: calculate how accurate the result is \u2013 compared to the labeling of the input example \u2013 and backpropagate; So, from a process perspective there do seem to broad ...", "dateLastCrawled": "2021-12-29T12:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Human learning as an analogy of machine learning</b> - Weina Jin, MD", "url": "https://weina.me/ml-vs-human-learning/", "isFamilyFriendly": true, "displayUrl": "https://weina.me/ml-vs-human-<b>learning</b>", "snippet": "<b>Human learning as an analogy of machine learning</b>. 5 minute read. Published: July 24, 2018. These days, during my reading of computer vision papers, I discover a recurrent theme: to orient CNN-based network to a specific CV task, most papers focus on designing new architectures of the network and/or <b>loss</b> functions. This approach seems obvious.", "dateLastCrawled": "2020-07-13T08:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2111.09109] Physics-guided <b>Loss</b> Functions Improve Deep <b>Learning</b> ...", "url": "https://arxiv.org/abs/2111.09109", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2111.09109", "snippet": "Computer Science &gt; <b>Machine</b> <b>Learning</b>. arXiv:2111.09109 (cs) [Submitted on 13 Nov 2021] ... we analyse the <b>analogy</b> between DNN solvers and traditional iterative algorithms and discuss how important physical phenomena cannot be effectively incorporated in the training process. We show the importance of including near-field priors in the <b>learning</b> process of DNNs. To this end, we propose new designs of <b>loss</b> functions which incorporate multiple-scattering based near-field quantities (such as ...", "dateLastCrawled": "2022-01-07T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The intuition of <b>Triplet Loss</b>. Getting an essence of how <b>loss</b> is\u2026 | by ...", "url": "https://medium.com/analytics-vidhya/triplet-loss-b9da35be21b8", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>triplet-loss</b>-b9da35be21b8", "snippet": "Many of us feel <b>Machine</b> <b>learning</b> is a black box that takes some input and gives out some fantastic output. In recent years, this same Black box has been creating wonders by acting as a mimic of\u2026", "dateLastCrawled": "2022-01-23T03:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "The high-level supervised <b>learning</b> process. Before we can actually introduce the concept of loss, we\u2019ll have to take a look at the high-level supervised <b>machine</b> <b>learning</b> process.All supervised training approaches fall under this process, which means that it is equal for deep neural networks such as MLPs or ConvNets, but also for SVMs.. Let\u2019s take a look at this training process, which is cyclical in nature.", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>machine</b> <b>learning</b> - Why there is sudden drop in loss after every epoch ...", "url": "https://stackoverflow.com/questions/57248723/why-there-is-sudden-drop-in-loss-after-every-epoch", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/57248723", "snippet": "<b>machine</b>-<b>learning</b> keras deep-<b>learning</b> loss-function. Share. Follow edited Jul 29 &#39;19 at 12:40. Community Bot. 1 1 1 silver badge. asked Jul 29 &#39;19 at 7:09. Rahul Anand Rahul Anand. 389 1 1 gold badge 3 3 silver badges 15 15 bronze badges. Add a comment | 2 Answers Active Oldest Votes. 11 Note: This answer is assuming you are using Keras -- you might want to add this information to your post or at least add a relevant tag. ...", "dateLastCrawled": "2022-01-21T00:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>machine</b> <b>learning</b> - In training a triplet network, I first have a solid ...", "url": "https://stats.stackexchange.com/questions/475655/in-training-a-triplet-network-i-first-have-a-solid-drop-in-loss-but-eventually", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/475655/in-training-a-triplet-network-i-first...", "snippet": "Changing the losses changes the tasks, so comparing the value of semi-hard loss to batch hard <b>loss is like</b> comparing apples to oranges. Because of how semi-hard loss is defined, its value will always be smaller than ordinary triplet loss. But we still want to achieve the inequality $(*)$! To make a consistent comparison as training progresses, you should measure the loss on the hardest task throughout training to confirm that the model is, indeed, improving as you change tasks during ...", "dateLastCrawled": "2022-02-03T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Comparing images in frequency domain. \u201cSpectral loss\u201d \u2013 does it make ...", "url": "https://bartwronski.com/2021/07/06/comparing-images-in-frequency-domain-spectral-loss-does-it-make-sense/", "isFamilyFriendly": true, "displayUrl": "https://bartwronski.com/2021/07/06/comparing-images-in-frequency-domain-spectral-loss...", "snippet": "I\u2019ve touched upon loss functions in my previous <b>machine</b> <b>learning</b> oriented posts (I\u2019ll highlight the separable filter optimization and generating blue noise through optimization, where in both I discuss some properties of a good loss), but for a fast recap \u2013 in <b>machine</b> <b>learning</b>, loss function is a \u201ccost\u201d that the optimization process tries to minimize. Loss functions are designed to capture aspects of the process / function that we want to \u201cimprove\u201d or solve. They can be also ...", "dateLastCrawled": "2022-01-28T22:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "class4x.pdf - <b>Machine</b> <b>Learning</b> Topic 4 \\u2022\\u2008Perceptron Online ...", "url": "https://www.coursehero.com/file/107174025/class4xpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/107174025/class4xpdf", "snippet": "\u2022Multi-Layer Network can handle more complex decisions \u20221-layer: is linear, can\u2019t handle XOR \u2022Each layer adds more flexibility (but more parameters!) \u2022Each node splits its input space with linear hyperplane \u20222-layer: if last layer is AND operation, get convex hull \u20222-layer: can do almost anything multi-layer can by fanning out the inputs at 2 nd layer \u2022Note: Without loss of generality, we can omit the 1 and \u03b8 0 Multi-Layer Neural Networks", "dateLastCrawled": "2022-01-08T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "neural networks - Explanation of <b>Spikes</b> in training loss vs. iterations ...", "url": "https://stats.stackexchange.com/questions/303857/explanation-of-spikes-in-training-loss-vs-iterations-with-adam-optimizer", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/303857/explanation-of-<b>spikes</b>-in-training...", "snippet": "Cross Validated is a question and answer site for people interested in statistics, <b>machine</b> <b>learning</b>, data analysis, data mining, and data visualization. It only takes a minute to sign up. Sign up to join this community. Anybody can ask a question Anybody can answer The best answers are voted up and rise to the top Home Public; Questions; Tags Users Unanswered Teams. Stack Overflow for Teams \u2013 Collaborate and share knowledge with a private group. Create a free Team What is Teams? Teams ...", "dateLastCrawled": "2022-01-27T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - <b>how to classify Iris flowers</b> - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/43057/how-to-classify-iris-flowers", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/43057/<b>how-to-classify-iris-flowers</b>", "snippet": "<b>machine</b>-<b>learning</b> neural-network ai. Share. Improve this question. Follow asked Dec 23 &#39;18 at 10:21. Fahd Fahd. 9 1 1 bronze badge $\\endgroup$ 5 $\\begingroup$ If you did that what would be your loss? $\\endgroup$ \u2013 Robin Nicole. Dec 23 &#39;18 at 10:44 ...", "dateLastCrawled": "2022-01-11T15:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Alan-D-Chen</b> (<b>Alan D Chen</b>) \u00b7 <b>GitHub</b>", "url": "https://github.com/Alan-D-Chen", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>Alan-D-Chen</b>", "snippet": "\ud83d\udd25 CDIoU and CDIoU <b>loss is like</b> a convenient plug-in that can be used in multiple models. CDIoU and CDIoU loss have different excellent performances in several models such as Faster R-CNN, YOLOv4, Re\u2026 Python 22 6 PCA_ICA_DEMO Public. Demo for PCA(Principal Component Analysis) &amp; ICA(Independent Component Analysis) in data analysis in Python and image separation written in MATLAB Python 8 2 meachine_<b>learning</b> Public. \u7b80\u5355\u7ebf\u6027\u56de\u5f52\uff0c\u591a\u5143\u7ebf\u6027\u56de\u5f52\uff0c\u975e\u7ebf\u6027\u56de\u5f52\uff0cKmeans\u7b97\u6cd5 ...", "dateLastCrawled": "2021-12-29T17:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] Looking for papers on treating regression as classification vs ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7gun87/d_looking_for_papers_on_treating_regression_as/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7gun87/d_looking_for_papers_on...", "snippet": "Doing the L2 <b>loss is like</b> doing maximum likelihood on a gaussian with a fixed variance - so the bad regression here is largely coming from the gaussian being mis-specified. I think the richer question would involve comparing approaches that consider the ordering vs. approaches that don t consider the ordering but where both have flexible enough distributions.", "dateLastCrawled": "2021-01-17T22:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Hearing Loss Simulator</b> - Find Out What <b>Hearing Loss is Like</b>", "url": "https://www.starkey.com/hearing-loss-simulator", "isFamilyFriendly": true, "displayUrl": "https://www.starkey.com/<b>hearing-loss-simulator</b>", "snippet": "Find out what they&#39;re experiencing with our <b>Hearing Loss Simulator</b>. Choose a situation. Select the <b>hearing loss</b> level you want to hear. Click Play. Set your computer volume to 50% for the best experience. Start.", "dateLastCrawled": "2022-02-02T08:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What\u2019s considered a good Log <b>Loss</b> in <b>Machine</b> <b>Learning</b> ? | by Federico ...", "url": "https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@fzammito/whats-considered-a-good-log-<b>loss</b>-in-<b>machine</b>-<b>learning</b>-a529...", "snippet": "Log <b>Loss is similar</b> to the Accuracy, but it will favor models that distinguish more strongly the classes. Log <b>Loss</b> it useful to compare models not only on their output but on their probabilistic ...", "dateLastCrawled": "2022-01-30T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Introduction to loss functions used in Deep Metric <b>Learning</b>. | Towards ...", "url": "https://towardsdatascience.com/metric-learning-loss-functions-5b67b3da99a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/metric-<b>learning</b>-loss-functions-5b67b3da99a5", "snippet": "Contributors : Jake Buglione, Sethu Hareesh Kolluru Recent advancements in <b>deep learning</b> have made it possible to learn a similarity measure for a set o f images using a deep metric <b>learning</b> network that maps visually similar images onto nearby locations in an embedding manifold, and visually dissimilar images apart from each other. Deep features learned using this approach result in well discriminative features with compact intra-product variance and well separated inter-product differences ...", "dateLastCrawled": "2022-01-25T23:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> Cats and Dogs\u2019 Breeds Classifier | by Mariana Santos ...", "url": "https://towardsdatascience.com/machine-learning-cats-and-dogs-breeds-classifier-b26a9df45000", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine</b>-<b>learning</b>-cats-and-dogs-breeds-classifier-b26a9...", "snippet": "The accuracy of both the training and validation show similar curves and values, and even the train <b>loss is similar</b>, even though it is somewhat lower with the lower <b>learning</b> rate. The biggest difference is in the validation loss. With the larger <b>learning</b> rate, this curve did not converge to a value, probably because it was \u201chopping\u201d through the local minimum, due to the larger step. In this experience, we concluded that 0.001 is the best <b>learning</b> of all compared.", "dateLastCrawled": "2022-01-30T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Key techniques for Evaluating <b>Machine</b> <b>Learning</b> models - Data Analytics", "url": "https://vitalflux.com/key-techniques-evaluating-machine-learning-models-performance/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/key-techniques-evaluating-<b>machine</b>-<b>learning</b>-models-performance", "snippet": "Log loss is used to evaluate the performance of classification <b>machine</b> <b>learning</b> models that are built using classification algorithms such as logistic regression, support vector <b>machine</b> (SVM), random forest, and gradient boosting. The idea behind the use of Log <b>loss is similar</b> to taking a base-e exponential or natural logarithm in order to compare model scores from high-value functions which may indicate poor <b>machine</b> <b>learning</b> model performance. The logarithmic loss value is defined as ...", "dateLastCrawled": "2022-01-31T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[2110.01601] DiffNet: Neural Field Solutions of Parametric Partial ...", "url": "https://arxiv.org/abs/2110.01601", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2110.01601", "snippet": "Computer Science &gt; <b>Machine</b> <b>Learning</b>. arXiv:2110.01601 (cs) [Submitted on 4 Oct 2021] ... (FEM <b>loss) is similar</b> to an energy functional that produces improved solutions, satisfies \\textit{a priori} mesh convergence, and can model Dirichlet and Neumann boundary conditions. We prove theoretically, and illustrate with experiments, convergence results analogous to mesh convergence analysis deployed in finite element solutions to PDEs. These results suggest that a mesh-based neural network ...", "dateLastCrawled": "2021-10-05T12:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Faster R-CNN step by step, Part II</b> | Notes for <b>machine</b> <b>learning</b>", "url": "https://dongjk.github.io/code/object+detection/keras/2018/06/10/Faster_R-CNN_step_by_step,_Part_II.html", "isFamilyFriendly": true, "displayUrl": "https://dongjk.github.io/code/object+detection/keras/2018/06/10/<b>Faster_R-CNN_step_by</b>...", "snippet": "regression <b>loss is similar</b> to RPN, using smooth l1 loss. there have 800 values but only 4 values are participant the gradient calculation. Summary. In this two posts, we have learnt how to implement <b>Faster R-CNN step by</b> step, how to prepare training data.", "dateLastCrawled": "2022-01-29T05:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - <b>hinge loss</b> vs logistic loss advantages and ...", "url": "https://stats.stackexchange.com/questions/146277/hinge-loss-vs-logistic-loss-advantages-and-disadvantages-limitations", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/146277/<b>hinge-loss</b>-vs-logistic-loss...", "snippet": "<b>machine</b>-<b>learning</b> svm loss-functions computer-vision. Share. Cite. Improve this question. Follow edited Jul 23 &#39;18 at 15:41. DHW. 644 3 3 silver badges 13 13 bronze badges. asked Apr 14 &#39;15 at 11:18. user570593 user570593. 1,059 2 2 gold badges 12 12 silver badges 19 19 bronze badges $\\endgroup$ Add a comment | 3 Answers Active Oldest Votes. 31 $\\begingroup$ Logarithmic loss minimization leads to well-behaved probabilistic outputs. <b>Hinge loss</b> leads to some (not guaranteed) sparsity on the ...", "dateLastCrawled": "2022-01-26T09:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Tabular Playground Series \u2013 June 2021 (Part 3) \u2013 <b>MACHINE</b> <b>LEARNING</b> CONCEPTS", "url": "https://srirangatarun.wordpress.com/2021/11/14/tabular-playground-series-june-2021-part-3/", "isFamilyFriendly": true, "displayUrl": "https://srirangatarun.wordpress.com/2021/11/14/tabular-playground-series-june-2021-part-3", "snippet": "The gap between the training and validation <b>loss is similar</b> to that of lightgbm, and lower than that of xgboost. So overfitting is not a major concern here. Additionally, catboost shows a strong LB performance with a score of 1.76 (very close to that of xgboost). catboost\u2019s CPU implementation is very fast compared to that of xgboost. catboost trains 20 estimators in just 6 seconds, compared to xgboost\u2019s 30. catboost, like xgboost, shows an impressive speed-up on GPU, going from 5.780 to ...", "dateLastCrawled": "2022-01-01T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What is an intuitive explanation for the log</b> loss function? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-for-the-log-loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-an-intuitive-explanation-for-the-log</b>-loss-function", "snippet": "Answer (1 of 8): To me an intuitive explanation is that minimizing the log loss equals minimizing the Kullback-Leibler divergence (Kullback\u2013Leibler divergence - Wikipedia) between the function you want to optimize (for example a neural network) and the true function that generates the data (from ...", "dateLastCrawled": "2022-01-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Best <b>Artificial Intelligence</b> Course (AIML) by UT Austin", "url": "https://www.mygreatlearning.com/pg-program-artificial-intelligence-course", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/pg-program-<b>artificial-intelligence</b>-course", "snippet": "In <b>learning</b> a projection where the inputs can be distinguished, the triplet <b>loss is similar</b> to metric <b>learning</b>. The triplet loss is used for understanding the score vectors for the images. You can use the score vectors of face descriptors for verifying the faces in Euclidean Space. Natural Language Processing 4 Quizzes 2 Projects 4 Quizzes 2 Projects Learn how to work with natural language processing with Python using traditional <b>machine</b> <b>learning</b> methods. Then, deep dive into the realm of ...", "dateLastCrawled": "2022-02-01T17:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>learning</b> for metal additive manufacturing: Towards a physics ...", "url": "https://www.sciencedirect.com/science/article/pii/S0278612521002259", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0278612521002259", "snippet": "<b>Machine</b> <b>learning</b> (ML) has shown to be an effective alternative to physical models for quality prediction and process optimization of metal additive manufacturing (AM). However, the inherent \u201cblack box\u201d nature of ML techniques such as those represented by artificial neural networks has often presented a challenge to interpret ML outcomes in the framework of the complex thermodynamics that govern AM. While the practical benefits of ML provide an adequate justification, its utility as a ...", "dateLastCrawled": "2022-01-27T04:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "commonplace book redux \u2013 a diary of quotations", "url": "https://quotables.github.io/", "isFamilyFriendly": true, "displayUrl": "https://quotables.github.io", "snippet": "English-<b>learning</b> infants under the age of six months distinguish phonemes used in Czech, Hindi, and Inslekampx (a Native American language), but English-speaking adults cannot, even with five hundred trials of training or a year of university coursework. Adult ears can tell the sounds apart, though, when the consonants are stripped from the syllables and presented alone as chirpy sounds; they just cannot tell them apart as phonemes. [\u2026]", "dateLastCrawled": "2022-02-01T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Full text of &quot;91288819 Tosh Pursuit Of History 5th Ed&quot;", "url": "https://archive.org/stream/91288819ToshPursuitOfHistory5thEd/91288819-Tosh-Pursuit-of-History-5th-Ed_djvu.txt", "isFamilyFriendly": true, "displayUrl": "https://archive.org/stream/91288819ToshPursuitOfHistory5thEd/91288819-Tosh-Pursuit-of...", "snippet": "An illustration of a computer application window Wayback <b>Machine</b>. An illustration of an open book. Books. An illustration of two cells of a film strip. Video. An illustration of an audio speaker. Audio. An illustration of a 3.5&quot; floppy disk. Software. An illustration of two photographs. Images. An illustration of a heart shape Donate. An illustration of text ellipses. More. An icon used to represent a menu that can be toggled by interacting with this icon. ...", "dateLastCrawled": "2022-01-31T09:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Read <b>Mushoku Tensei</b> (WN),Free online novel online reading, online book ...", "url": "https://readnovelfreeonline.com/mushoku-tensei-wn/volume-5-h", "isFamilyFriendly": true, "displayUrl": "https://readnovelfreeonline.com/<b>mushoku-tensei</b>-wn/volume-5-h", "snippet": "If I remember correctly, I was <b>learning</b> swordsmanship at my house. It was an everyday life of being scolded by my father. Even when I put in a bit of work, he would complain about everything and hit me. &quot;Do you think the you of that time could have survived on the Magic Continent?&quot; &quot;Hah, Gisu, that entire premise is strange. Rudi you know, had ...", "dateLastCrawled": "2022-01-29T13:30:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Notes on <b>Machine</b> <b>Learning</b> 3: <b>Decision theory</b>", "url": "https://cveai.github.io/notes/2018/03/27/mm-ml-3.html", "isFamilyFriendly": true, "displayUrl": "https://cveai.github.io/notes/2018/03/27/mm-ml-3.html", "snippet": "(ML 3.6) The Big Picture (part 2) Core ideas &amp; methods of ML: (not necessarily disjoint) Exact inference (usually not possible) Multivariate Gaussian (very nice) / Conjugate priors / Graphical models (use DP)", "dateLastCrawled": "2022-01-02T01:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What\u2019s My Line? <b>Next Sentence Prediction</b> in RunwayML with BERT | by ...", "url": "https://medium.com/runwayml/whats-my-line-next-sentence-prediction-in-runway-ad76cbf28c86", "isFamilyFriendly": true, "displayUrl": "https://medium.com/runwayml/whats-my-line-<b>next-sentence-prediction</b>-in-runway-ad76cbf28c86", "snippet": "The <b>loss can be thought of as</b> how much the model is surprised by the sequence. The lower the loss, the more likely it judges the sequence to be. Results: I\u2019m not sure what a score of 4.0966539 ...", "dateLastCrawled": "2022-01-31T03:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Defect <b>Detection</b> in Products using Image Segmentation | by Vinithavn ...", "url": "https://medium.com/analytics-vidhya/defect-detection-in-products-using-image-segmentation-a87a8863a9e5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/defect-<b>detection</b>-in-products-using-image...", "snippet": "Dice <b>loss can be thought of as</b> 1-Dice coefficient where Dice coefficient is defined as, Dice coefficient=2* area of overlap area of intersection. You can read more about these metrics here. 5 ...", "dateLastCrawled": "2022-02-03T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Automatic Di\ufb00erentiation and <b>Neural Networks</b> 1 Introduction", "url": "https://people.cs.umass.edu/~domke/courses/sml2010/07autodiff_nnets.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.cs.umass.edu/~domke/courses/sml2010/07autodiff_nnets.pdf", "snippet": "Statistical <b>Machine</b> <b>Learning</b> Notes 7 Automatic Di\ufb00erentiation and <b>Neural Networks</b> Instructor: Justin Domke 1 Introduction The name \u201cneuralnetwork\u201d is sometimes used torefer tomany things (e.g. Hop\ufb01eld networks, self-organizing maps). In these notes, we are only interested in the most common type of neural network, the multi-layer perceptron. A basic problem in <b>machine</b> <b>learning</b> is function approximation. We have some inputs x\u02c6 and some outputs y\u02c6, and we want to \ufb01t some function f ...", "dateLastCrawled": "2022-01-28T01:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Decision-Theoretic Approximations for Machine Learning</b>", "url": "https://www.ijcai.org/Proceedings/13/Papers/487.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/13/Papers/487.pdf", "snippet": "<b>Decision-Theoretic Approximations for Machine Learning</b> M. Ehsan Abbasnejad Abstract Decision theory focuses on the problem of mak-ing decisions under uncertainty. This uncertainty arises from the unknown aspects of the state of the world the decision maker is in or the unknown util- ity function of performing actions. The uncertainty can be modeled as a probability distribution captur-ing our belief about the world the decision maker is in. Upon making new observations, the decision maker ...", "dateLastCrawled": "2022-02-02T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Automatic Di\ufb00erentiation and Neural Networks</b>", "url": "https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf", "snippet": "Statistical <b>Machine</b> <b>Learning</b> Notes 8 <b>Automatic Di\ufb00erentiation and Neural Networks</b> Instructor: Justin Domke Contents 1 Introduction 1 2 Automatic Di\ufb00erentiation 2 3 Multi-Layer Perceptrons 5 4 MNIST 7 5 Backpropagation 10 6 Discussion 13 1 Introduction The name \u201cneuralnetwork\u201d is sometimes used torefer tomany things (e.g. Hop\ufb01eld networks, self-organizing maps). In these notes, we are only interested in the most common type of neural network, the multi-layer perceptron. A basic ...", "dateLastCrawled": "2022-01-28T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A cascaded fully convolutional network framework for dilated pancreatic ...", "url": "https://link.springer.com/article/10.1007/s11548-021-02530-x", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11548-021-02530-x", "snippet": "Deep-<b>learning</b>-base methods have successfully solved many challenging tasks in image processing, such as classification [8, 25], ... The Dice <b>loss can be thought of as</b> the minimization of the Dice score subtracted by one, which is minimized toward 0 to achieve optimal segmentation performance. Focal loss is proposed to dynamically rescale cross entropy loss and is conducive to imbalance problems . The voxel-wise Focal loss function is expressed as $$\\begin{aligned} {\\mathcal {L}}_F = -\\frac{1 ...", "dateLastCrawled": "2022-01-30T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Exploring deep neural networks via layer-peeled model: Minority ...", "url": "https://www.pnas.org/content/118/43/e2103091118", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/118/43/e2103091118", "snippet": "The remarkable development of deep <b>learning</b> over the past decade relies heavily on sophisticated heuristics and tricks. To better exploit its potential in the coming decade, perhaps a rigorous framework for reasoning about deep <b>learning</b> is needed, which, however, is not easy to build due to the intricate details of neural networks. For near-term purposes, a practical alternative is to develop a mathematically tractable surrogate model, yet maintaining many characteristics of neural networks.", "dateLastCrawled": "2021-12-30T16:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Network Traffic Anomaly Detection Using Recurrent Neural Networks", "url": "https://www.researchgate.net/publication/324104291_Network_Traffic_Anomaly_Detection_Using_Recurrent_Neural_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324104291_Network_Traffic_Anomaly_Detection...", "snippet": "Next, a supervised <b>machine</b> <b>learning</b> algorithm one-class SVM is trained to generalize the behavior model in order to predict user behavior anomalies. Results show that One-Class SVM is the most ...", "dateLastCrawled": "2022-01-26T23:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Adversarial Examples are Just Bugs</b>, Too - Latest articles about <b>machine</b> ...", "url": "https://distill.pub/2019/advex-bugs-discussion/response-5/", "isFamilyFriendly": true, "displayUrl": "https://distill.pub/2019/advex-bugs-discussion/response-5", "snippet": "Adversarial Examples With No Features. Using the above, we can construct adversarial examples which do not suffice for <b>learning</b>. Here, we replicate the Ilyas et al. experiment that \u201cNon-robust features suffice for standard classification\u201d (Section 3.2 of ), but show that it fails for our construction of adversarial examples.. To review, the Ilyas et al. non-robust experiment was:", "dateLastCrawled": "2022-01-31T12:36:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(loss)  is like +(error)", "+(loss) is similar to +(error)", "+(loss) can be thought of as +(error)", "+(loss) can be compared to +(error)", "machine learning +(loss AND analogy)", "machine learning +(\"loss is like\")", "machine learning +(\"loss is similar\")", "machine learning +(\"just as loss\")", "machine learning +(\"loss can be thought of as\")", "machine learning +(\"loss can be compared to\")"]}