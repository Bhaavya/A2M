{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to use <b>hinge</b> &amp; <b>squared</b> <b>hinge</b> <b>loss</b> with TensorFlow 2 and Keras ...", "url": "https://www.machinecurve.com/index.php/2019/10/15/how-to-use-hinge-squared-hinge-loss-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinecurve</b>.com/index.php/2019/10/15/how-to-use-<b>hinge</b>-<b>squared</b>-<b>hinge</b>-<b>loss</b>...", "snippet": "<b>Squared</b> <b>hinge</b> <b>loss</b> may then be what you are looking for, especially when you already considered the <b>hinge</b> <b>loss</b> function for your machine learning problem. <b>Squared</b> <b>hinge</b> <b>loss</b> is nothing else but a square of the output of the <b>hinge</b>\u2019s \\(max(\u2026)\\) function. It generates a <b>loss</b> function as illustrated above, compared to regular <b>hinge</b> <b>loss</b>. As you can see, larger errors are punished more significantly than with traditional <b>hinge</b>, whereas smaller errors are punished slightly lightlier ...", "dateLastCrawled": "2022-01-31T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Squared versus Hinge losses: SVC versus</b> RLS | Causeway", "url": "https://cvstuff.wordpress.com/2014/11/29/latex-l_1-versus-latex-l_2-loss-a-svm-example/", "isFamilyFriendly": true, "displayUrl": "https://cvstuff.wordpress.com/2014/11/29/latex-l_1-versus-latex-l_2-<b>loss</b>-a-svm-example", "snippet": "Using <b>squared</b> <b>loss</b> instead of <b>Hinge</b> <b>loss</b> is another story, which one no longer calls it SVC (Support Vector Classification) but RLS (Regularized Least <b>Squared</b> regression problem). Of course, <b>squared</b> <b>loss</b> does not offer a sparse solution. In order to see the difference between the two problems, I generate a toy dataset with two classes in 2D, and attempt to use the solver sklearn.svm.LinearSVC with two choices \u2018l1\u2019 (<b>hinge</b> <b>loss</b>) and \u2018l2\u2019 (<b>squared</b> <b>loss</b>). Notice how the solution of \u2018l2 ...", "dateLastCrawled": "2022-02-02T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions in Neural Networks - theaidream.com", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "Use the <b>Squared</b> <b>Hinge</b> <b>loss</b> function on problems involving yes/no (binary) decisions. Especially, when you\u2019re not interested in knowing how certain the classifier is about the classification. Namely, when you don\u2019t care about the classification probabilities. Use in combination with the tanh() the activation function in the last layer of the neural network.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding <b>loss</b> functions : <b>Hinge loss</b> | by Kunal Chowdhury ...", "url": "https://medium.com/analytics-vidhya/understanding-loss-functions-hinge-loss-a0ff112b40a1", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/understanding-<b>loss</b>-functions-<b>hinge-loss</b>-a0ff112b40a1", "snippet": "<b>H inge loss</b> in Support Vector Machines. From our SVM model, we know that <b>hinge loss</b> = [ 0, 1- yf (x) ]. Looking at the graph for SVM in Fig 4, we can see that for yf (x) \u2265 1, <b>hinge loss</b> is \u2018 0 ...", "dateLastCrawled": "2022-01-31T10:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>sklearn.svm.LinearSVC</b> \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html", "snippet": "<b>loss</b> {\u2018<b>hinge</b>\u2019, \u2018<b>squared</b>_<b>hinge</b>\u2019}, default=\u2019<b>squared</b>_<b>hinge</b> \u2019 Specifies the <b>loss</b> function. \u2018<b>hinge</b>\u2019 is the standard SVM <b>loss</b> (used e.g. by the SVC class) while \u2018<b>squared</b>_<b>hinge</b>\u2019 is the square of the <b>hinge</b> <b>loss</b>. The combination of penalty=&#39;l1&#39; and <b>loss</b>=&#39;<b>hinge</b>&#39; is not supported. dual bool, default=True. Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples &gt; n_features. tol float, default=1e-4. Tolerance for stopping ...", "dateLastCrawled": "2022-02-02T20:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A definitive explanation to the <b>Hinge Loss</b> for Support Vector Machines ...", "url": "https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-definitive-explanation-to-<b>hinge-loss</b>-for-support...", "snippet": "Photo by Patrick Tomasso on Unsplash. Now, let\u2019s examine the <b>hinge loss</b> for a number of predictions made by a hypothetical SVM: actual predicted <b>hinge loss</b> ===== [0] +1 0.97 0.03 [1] +1 1.20 0.00 [2] +1 0.00 1.00 [3] +1 -0.25 1.25 [4] -1 -0.88 0.12 [5] -1 -1.01 0.00 [6] -1 -0.00 1.00 [7] -1 0.40 1.40. One key characteristic of the SVM and the <b>Hinge loss</b> is that the boundary separates negative and positive instances as +1 and -1, with -1 being on the left side of the boundary and +1 being ...", "dateLastCrawled": "2022-02-02T07:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Why <b>is squared hinge loss differentiable? - Quora</b>", "url": "https://www.quora.com/Why-is-squared-hinge-loss-differentiable", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-<b>is-squared-hinge-loss-differentiable</b>", "snippet": "Answer (1 of 4): Let\u2019s start by defining the <b>hinge</b> <b>loss</b> function h(x) = max(1-x,0). Now let\u2019s think about the derivative h\u2019(x). This does not exist at x = 1 because the left and right limits do not converge to the same number (ie: the derivative is undefined at x=1, but it is -1 for x&lt;1 and 0 fo...", "dateLastCrawled": "2022-02-03T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "There are several different common <b>loss</b> functions to choose from: the cross-entropy <b>loss</b>, the mean-<b>squared</b> error, the huber <b>loss</b>, and the <b>hinge</b> <b>loss</b> - just to name a few. Given a particular model, each <b>loss</b> function has particular properties that make it interesting - for example, the (L2-regularized) <b>hinge</b> <b>loss</b> comes with the maximum-margin ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>the advantage/disadvantage of Hinge-loss compared</b> to cross ...", "url": "https://www.quora.com/What-is-the-advantage-disadvantage-of-Hinge-loss-compared-to-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-advantage-disadvantage-of-Hinge-loss-compared</b>-to...", "snippet": "Answer (1 of 2): Cross Entropy (or Log <b>Loss</b>), Hing <b>Loss</b> (SVM <b>Loss</b>), <b>Squared</b> <b>Loss</b> etc. are different forms of <b>Loss</b> functions. Log <b>Loss</b> in the classification context gives Logistic Regression, while the <b>Hinge</b> <b>Loss</b> is Support Vector Machines. Logistic Regression and SVMs both are linear classifiers,...", "dateLastCrawled": "2022-01-29T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - value <b>error happens when using GridSearchCV</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/29902190/value-error-happens-when-using-gridsearchcv", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/29902190", "snippet": "ValueError: Unsupported set of arguments: penalty=&#39;l1&#39; is only supported when dual=&#39;false&#39;., Parameters: penalty=&#39;l1&#39;, <b>loss</b>=&#39;<b>squared</b>_<b>hinge</b>&#39;, dual=False. Anyone has idea what I should do to deal with that?", "dateLastCrawled": "2022-01-24T02:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "There are several different common <b>loss</b> functions to choose from: the cross-entropy <b>loss</b>, the mean-<b>squared</b> error, the huber <b>loss</b>, and the <b>hinge</b> <b>loss</b> - just to name a few. Given a particular model, each <b>loss</b> function has particular properties that make it interesting - for example, the (L2-regularized) <b>hinge</b> <b>loss</b> comes with the maximum-margin ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding <b>loss</b> functions : <b>Hinge loss</b> | by Kunal Chowdhury ...", "url": "https://medium.com/analytics-vidhya/understanding-loss-functions-hinge-loss-a0ff112b40a1", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/understanding-<b>loss</b>-functions-<b>hinge-loss</b>-a0ff112b40a1", "snippet": "<b>H inge loss</b> in Support Vector Machines. From our SVM model, we know that <b>hinge loss</b> = [ 0, 1- yf (x) ]. Looking at the graph for SVM in Fig 4, we can see that for yf (x) \u2265 1, <b>hinge loss</b> is \u2018 0 ...", "dateLastCrawled": "2022-01-31T10:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "10: Empirical Risk Minimization - Cornell University", "url": "https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote10.html", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote10.html", "snippet": "What can you say about the <b>hinge</b>-<b>loss</b> and the log-<b>loss</b> as $\\left.z\\rightarrow-\\infty\\right.$? Commonly Used Regression <b>Loss</b> Functions Regression algorithms (where a prediction can lie anywhere on the real-number line) also have their own host of <b>loss</b> functions: <b>Loss</b> $\\ell(h_{\\mathbf{w}}(\\mathbf{x}_i,y_i))$ Comments; <b>Squared</b> <b>Loss</b> $\\left.(h(\\mathbf{x}_{i})-y_{i})^{2}\\right.$ Most popular regression <b>loss</b> function ; Estimates Mean Label ; ADVANTAGE: Differentiable everywhere ; DISADVANTAGE ...", "dateLastCrawled": "2022-02-03T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>sklearn.svm.LinearSVC</b> \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html", "snippet": "LinearSVC (penalty = &#39;l2&#39;, <b>loss</b> = &#39;<b>squared</b>_<b>hinge</b>&#39;, *, dual = True, tol = 0.0001, C = 1.0, multi_class = &#39;ovr&#39;, fit_intercept = True, intercept_scaling = 1, class_weight = None, verbose = 0, random_state = None, max_iter = 1000) [source] \u00b6 Linear Support Vector Classification. <b>Similar</b> to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and <b>loss</b> functions and should scale better to large ...", "dateLastCrawled": "2022-02-02T20:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Why <b>is squared hinge loss differentiable? - Quora</b>", "url": "https://www.quora.com/Why-is-squared-hinge-loss-differentiable", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-<b>is-squared-hinge-loss-differentiable</b>", "snippet": "Answer (1 of 4): Let\u2019s start by defining the <b>hinge</b> <b>loss</b> function h(x) = max(1-x,0). Now let\u2019s think about the derivative h\u2019(x). This does not exist at x = 1 because the left and right limits do not converge to the same number (ie: the derivative is undefined at x=1, but it is -1 for x&lt;1 and 0 fo...", "dateLastCrawled": "2022-02-03T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>the advantage/disadvantage of Hinge-loss compared</b> to cross ...", "url": "https://www.quora.com/What-is-the-advantage-disadvantage-of-Hinge-loss-compared-to-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-advantage-disadvantage-of-Hinge-loss-compared</b>-to...", "snippet": "Answer (1 of 2): Cross Entropy (or Log <b>Loss</b>), Hing <b>Loss</b> (SVM <b>Loss</b>), <b>Squared</b> <b>Loss</b> etc. are different forms of <b>Loss</b> functions. Log <b>Loss</b> in the classification context gives Logistic Regression, while the <b>Hinge</b> <b>Loss</b> is Support Vector Machines. Logistic Regression and SVMs both are linear classifiers,...", "dateLastCrawled": "2022-01-29T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Scikit Learn - Support Vector Machines</b> - <b>Tutorialspoint</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_support_vector_machines.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_support_vector_machines</b>.htm", "snippet": "<b>loss</b> \u2212 string, <b>hinge</b>, <b>squared</b>_<b>hinge</b> (default = <b>squared</b>_<b>hinge</b>) It represents the <b>loss</b> function where \u2018<b>hinge</b>\u2019 is the standard SVM <b>loss</b> and \u2018<b>squared</b>_<b>hinge</b>\u2019 is the square of <b>hinge</b> <b>loss</b>. Implementation Example. Following Python script uses sklearn.svm.LinearSVC class \u2212. from sklearn.svm import LinearSVC from sklearn.datasets import make_classification X, y = make_classification(n_features = 4, random_state = 0) LSVCClf = LinearSVC(dual = False, random_state = 0, penalty = &#39;l1&#39;,tol ...", "dateLastCrawled": "2022-02-02T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Different <b>loss</b> functions: Linear <b>hinge</b> <b>loss</b> \u2113 h (y i , f (x i ...", "url": "https://researchgate.net/figure/Different-loss-functions-Linear-hinge-loss-l-h-y-i-f-x-i-max0-1-y-i-f-x-i_fig1_259221404", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Different-<b>loss</b>-functions-Linear-<b>hinge</b>-<b>loss</b>-l-h-y-i-f-x...", "snippet": "Download scientific diagram | Different <b>loss</b> functions: Linear <b>hinge</b> <b>loss</b> \u2113 h (y i , f (x i )) = max(0, 1 \u2212 y i f (x i )), <b>Squared</b> <b>Hinge</b> <b>loss</b> \u2113 sqh (y i , f (x i )) = max(0, 1 \u2212 y i f (x i ...", "dateLastCrawled": "2021-08-02T11:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Keras - Model Compilation</b> - <b>Tutorialspoint</b>", "url": "https://www.tutorialspoint.com/keras/keras_model_compilation.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/keras/<b>keras_model_compilation</b>.htm", "snippet": "mean_<b>squared</b>_logarithmic_error; <b>squared</b>_<b>hinge</b>; <b>hinge</b>; categorical_<b>hinge</b>; logcosh; huber_<b>loss</b>; categorical_crossentropy; sparse_categorical_crossentropy; binary_crossentropy; kullback_leibler_divergence; poisson; cosine_proximity; is_categorical_crossentropy; All above <b>loss</b> function accepts two arguments \u2212 . y_true \u2212 true labels as tensors. y_pred \u2212 prediction with same shape as y_true. Import the losses module before using <b>loss</b> function as specified below \u2212. from keras import losses ...", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - value <b>error happens when using GridSearchCV</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/29902190/value-error-happens-when-using-gridsearchcv", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/29902190", "snippet": "ValueError: Unsupported set of arguments: penalty=&#39;l1&#39; is only supported when dual=&#39;false&#39;., Parameters: penalty=&#39;l1&#39;, <b>loss</b>=&#39;<b>squared</b>_<b>hinge</b>&#39;, dual=False. Anyone has idea what I should do to deal with that?", "dateLastCrawled": "2022-01-24T02:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to use <b>hinge</b> &amp; <b>squared</b> <b>hinge</b> <b>loss</b> with TensorFlow 2 and Keras ...", "url": "https://www.machinecurve.com/index.php/2019/10/15/how-to-use-hinge-squared-hinge-loss-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinecurve</b>.com/index.php/2019/10/15/how-to-use-<b>hinge</b>-<b>squared</b>-<b>hinge</b>-<b>loss</b>...", "snippet": "<b>Squared</b> <b>hinge</b> <b>loss</b> may then be what you are looking for, especially when you already considered the <b>hinge</b> <b>loss</b> function for your machine learning problem. <b>Squared</b> <b>hinge</b> <b>loss</b> is nothing else but a square of the output of the <b>hinge</b>\u2019s \\(max(\u2026)\\) function. It generates a <b>loss</b> function as illustrated above, compared to regular <b>hinge</b> <b>loss</b>. As you <b>can</b> see, larger errors are punished more significantly than with traditional <b>hinge</b>, whereas smaller errors are punished slightly lightlier ...", "dateLastCrawled": "2022-01-31T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> Functions | <b>Loss</b> Functions in <b>Machine Learning</b> - 360DigiTMG", "url": "https://360digitmg.com/loss-functions-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://360digitmg.com/<b>loss</b>-functions-in-<b>machine-learning</b>", "snippet": "<b>Squared</b> <b>Hinge</b> <b>Loss</b>: This is an extension of the <b>hinge</b> <b>loss</b> and it is quite simply the square of the <b>hinge</b> <b>loss</b> function. Since this is a square of the original <b>loss</b>, it has some mathematical properties that make it easier to calculate the gradients. This is perfectly suitable for Yes or No type of questions where the deviation in probability is ...", "dateLastCrawled": "2022-01-30T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Multiclass Boosting with <b>Hinge</b> <b>Loss</b> based on Output Coding", "url": "https://ai.stanford.edu/~tianshig/papers/multiclassHingeBoost-ICML2011.pdf", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/~tianshig/papers/multiclass<b>Hinge</b>Boost-ICML2011.pdf", "snippet": "other methods that are based on the multiclass <b>hinge</b> <b>loss</b> and <b>can</b> <b>be thought</b> of as a relaxation of the dis-crete coding matrix to real values. Both R\u00e4tsch et al. (2003) and Amit et al. (2007) learn the real-valued coding matrix and the embedding functions jointly based on the <b>hinge</b> <b>loss</b>. The former used the alter- nating method to optimize the bi-convex problem and the latter proposed a convex reformulation. More gen-erally, Zou et al. (2008) outlined a class of smooth con-vex <b>loss</b> ...", "dateLastCrawled": "2022-01-16T15:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "In this post, I\u2019ll discuss three common <b>loss</b> functions: the mean-<b>squared</b> (MSE) <b>loss</b>, cross-entropy <b>loss</b>, and the <b>hinge</b> <b>loss</b>. These are the most commonly used functions I\u2019ve seen used in traditional machine learning and deep learning models, so I <b>thought</b> it would be a good idea to figure out the underlying theory behind each one, and when to prefer one over the others.", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "The <b>Hinge</b> <b>Loss</b> Equation def <b>Hinge</b>(yhat, y): return np.max(0,1 - yhat * y) Where y is the actual label (-1 or 1) and \u0177 is the prediction; The <b>loss</b> is 0 when the signs of the labels and prediction ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Gradient of <b>Hinge loss</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/4608/gradient-of-hinge-loss", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/4608", "snippet": "This answer is useful. 39. This answer is not useful. Show activity on this post. To get the gradient we differentiate the <b>loss</b> with respect to i th component of w. Rewrite <b>hinge loss</b> in terms of w as f ( g ( w)) where f ( z) = max ( 0, 1 \u2212 y z) and g ( w) = x \u22c5 w. Using chain rule we get. \u2202 \u2202 w i f ( g ( w)) = \u2202 f \u2202 z \u2202 g \u2202 w i.", "dateLastCrawled": "2022-02-03T02:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Hinge</b> <b>loss</b> question - Data Science Stack Exchange", "url": "https://datascience.stackexchange.com/questions/58847/hinge-loss-question", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/58847/<b>hinge</b>-<b>loss</b>-question", "snippet": "Bookmark this question. Show activity on this post. <b>Hinge</b> <b>loss</b> is usually defined as. L ( y, y ^) = m a x ( 0, 1 \u2212 y y ^) What I don&#39;t understand is why are we comparing zero with 1 \u2212 y y ^ instead of some other constant. Why not make it 2 \u2212 y y ^, or 2 \u2212 y y ^ or just take y y ^, to check if the observation would be on the right side ...", "dateLastCrawled": "2022-01-07T16:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Predicting accurate probabilities with a ranking</b> <b>loss</b> - ICML", "url": "https://icml.cc/2012/papers/372.pdf", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/2012/papers/372.pdf", "snippet": "<b>squared</b> error, say, <b>can</b> <b>be thought</b> to yield meaningful probability estimates. The <b>hinge</b> <b>loss</b> of SVMs, \u2018(y;s\u02c6) = max(0;1 (2y 1)s\u02c6), is Bayes consistent but does not cor-respond to a proper <b>loss</b> function, which is why SVMs do not output meaningful probabilities (Platt,1999). 3. Analysis of existing paradigms to learn accurate probabilities We now analyze two major paradigms for probability esti-mation, and study their possible failure modes. 3.1. Optimization of a proper <b>loss</b> A direct ...", "dateLastCrawled": "2022-01-19T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>correct implementation of Hinge loss minimization for gradient descent</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/2yoi5v/correct_implementation_of_hinge_loss_minimization/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../2yoi5v/<b>correct_implementation_of_hinge_loss_minimization</b>", "snippet": "<b>correct implementation of Hinge loss minimization for gradient descent</b>. Close. 0. Posted by 5 years ago. Archived. <b>correct implementation of Hinge loss minimization for gradient descent</b>. I copied the <b>hinge</b> <b>loss</b> function from here (also LossC and LossFunc upon which it&#39;s based. Then I included it in my gradient descent algorithm like so: ...", "dateLastCrawled": "2021-02-16T06:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Multilabelmarginloss - PyTorch Forums", "url": "https://discuss.pytorch.org/t/multilabelmarginloss/87022", "isFamilyFriendly": true, "displayUrl": "https://discuss.pytorch.org/t/multilabelmargin<b>loss</b>/87022", "snippet": "Creates a criterion that optimizes a multi-class classification <b>hinge</b> <b>loss</b> (margin-based <b>loss</b>) between input x (a 2D mini-batch Tensor) and output y . Based on the shape information it should also work for your current output and target shapes. Let me know, if it would work for you. 1 Like. xolotl18 (Giacomo Zema) June 28, 2020, 11:57pm #7. Thank you very much, as soon as I try it I will let you know. xolotl18 (Giacomo Zema) July 3, 2020, 10:45am #8. It works. But when I tried implementing a ...", "dateLastCrawled": "2022-01-24T01:30:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to use <b>hinge</b> &amp; <b>squared</b> <b>hinge</b> <b>loss</b> with TensorFlow 2 and Keras ...", "url": "https://www.machinecurve.com/index.php/2019/10/15/how-to-use-hinge-squared-hinge-loss-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinecurve</b>.com/index.php/2019/10/15/how-to-use-<b>hinge</b>-<b>squared</b>-<b>hinge</b>-<b>loss</b>...", "snippet": "Today, we\u2019ll cover two closely related <b>loss</b> functions that <b>can</b> be used in neural networks \u2013 and hence in TensorFlow 2 based Keras \u2013 that behave similar to how a Support Vector Machine generates a decision boundary for classification: the <b>hinge</b> <b>loss</b> and <b>squared</b> <b>hinge</b> <b>loss</b>. In this blog, you\u2019ll first find a brief introduction to the two ...", "dateLastCrawled": "2022-01-31T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "For example, the cross-entropy <b>loss</b> would invoke a much higher <b>loss</b> than the <b>hinge</b> <b>loss</b> if our (un-normalized) scores were \\([10, 8, 8]\\) versus \\([10, -10, -10]\\), where the first class is correct. In fact, the (multi-class) <b>hinge</b> <b>loss</b> would recognize that the correct class score already exceeds the other scores by more than the margin, so it will invoke zero <b>loss</b> on both scores. Once the margins are satisfied, the SVM will no longer optimize the weights in an attempt to \u201cdo better ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions in Neural Networks - theaidream.com", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "The <b>squared</b> <b>hinge</b> <b>loss</b> is a <b>loss</b> function used for \u201cmaximum margin\u201d binary classification problems. Mathematically it is defined as: where \u0177 is the predicted value and y is either 1 or -1. Thus, the <b>squared</b> <b>hinge</b> <b>loss</b> \u2192 0, when the true and predicted labels are the same and when \u0177\u2265 1 (which is an indication that the classifier is sure that it\u2019s the correct label). The <b>squared</b> <b>hinge</b> <b>loss</b> \u2192 quadratically increasing with the error, when when the true and predicted labels are not ...", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is <b>the advantage/disadvantage of Hinge-loss compared</b> to cross ...", "url": "https://www.quora.com/What-is-the-advantage-disadvantage-of-Hinge-loss-compared-to-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-advantage-disadvantage-of-Hinge-loss-compared</b>-to...", "snippet": "Answer (1 of 2): Cross Entropy (or Log <b>Loss</b>), Hing <b>Loss</b> (SVM <b>Loss</b>), <b>Squared</b> <b>Loss</b> etc. are different forms of <b>Loss</b> functions. Log <b>Loss</b> in the classification context gives Logistic Regression, while the <b>Hinge</b> <b>Loss</b> is Support Vector Machines. Logistic Regression and SVMs both are linear classifiers,...", "dateLastCrawled": "2022-01-29T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) A study on l2-<b>loss</b> (<b>squared</b> <b>hinge</b>-<b>loss</b>) multi-class SVM | Chih ...", "url": "https://www.academia.edu/2834199/A_study_on_l2_loss_squared_hinge_loss_multi_class_SVM", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2834199/A_study_on_l2_<b>loss</b>_<b>squared</b>_<b>hinge</b>_<b>loss</b>_multi_class_SVM", "snippet": "In binary classification, <b>squared</b> <b>hinge</b> <b>loss</b> (L2 <b>loss</b>) is a common alter- native to L1 <b>loss</b>, but surprisingly we have not found any paper studying details of Crammer and Singer\u2019s method with L2 <b>loss</b>. This is inconvenient because, for example, we do not know what the dual problem is.1 Although the dual problem of two-class SVM using L2 <b>loss</b> is ...", "dateLastCrawled": "2022-01-27T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Why <b>is squared hinge loss differentiable? - Quora</b>", "url": "https://www.quora.com/Why-is-squared-hinge-loss-differentiable", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-<b>is-squared-hinge-loss-differentiable</b>", "snippet": "Answer (1 of 4): Let\u2019s start by defining the <b>hinge</b> <b>loss</b> function h(x) = max(1-x,0). Now let\u2019s think about the derivative h\u2019(x). This does not exist at x = 1 because the left and right limits do not converge to the same number (ie: the derivative is undefined at x=1, but it is -1 for x&lt;1 and 0 fo...", "dateLastCrawled": "2022-02-03T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "machine learning - <b>hinge loss</b> vs logistic <b>loss</b> advantages and ...", "url": "https://stats.stackexchange.com/questions/146277/hinge-loss-vs-logistic-loss-advantages-and-disadvantages-limitations", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/146277/<b>hinge-loss</b>-vs-logistic-<b>loss</b>...", "snippet": "<b>Hinge loss</b> leads to some (not guaranteed) sparsity on the dual, but it doesn&#39;t help at probability estimation. Instead, it punishes misclassifications (that&#39;s why it&#39;s so useful to determine margins): diminishing <b>hinge-loss</b> comes with diminishing across margin misclassifications. So, summarizing:", "dateLastCrawled": "2022-01-26T09:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Multi-class SVM Loss - PyImageSearch</b>", "url": "https://www.pyimagesearch.com/2016/09/05/multi-class-svm-loss/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/2016/09/05/<b>multi-class-svm-loss</b>", "snippet": "Another related, common <b>loss</b> function you may come across is the <b>squared</b> <b>hinge</b> <b>loss</b>: The <b>squared</b> term penalizes our <b>loss</b> more heavily by squaring the output. This leads to a quadratic growth in <b>loss</b> rather than a linear one. As for which <b>loss</b> function you should use, that is entirely dependent on your dataset. It\u2019s typical to see the standard <b>hinge</b> <b>loss</b> function used more often, but on some datasets the <b>squared</b> variation might obtain better accuracy \u2014 overall, this is a hyperparameter ...", "dateLastCrawled": "2022-02-02T22:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to implement <b>squared</b> <b>hinge</b> <b>loss</b> - PyTorch Forums", "url": "https://discuss.pytorch.org/t/how-to-implement-squared-hinge-loss/85629", "isFamilyFriendly": true, "displayUrl": "https://discuss.pytorch.org/t/how-to-implement-<b>squared</b>-<b>hinge</b>-<b>loss</b>/85629", "snippet": "Hi everyone, I need to implement the squred <b>hinge</b> <b>loss</b> in order to train a neural network using a svm-like classifier on the last layer. It is an image classification problem on cifar dataset, so it is a multi class classification. The idea is to have multiple \u201cscores\u201d for each output neuron of the network and the network should be trained minimizing the sum of the losses for each class. Is there an already implemented version of this <b>loss</b>? If not, what is the easiest way to implement it ...", "dateLastCrawled": "2021-12-15T06:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - Why SGDClassifier with <b>hinge</b> <b>loss</b> is faster than SVC ...", "url": "https://stackoverflow.com/questions/60061412/why-sgdclassifier-with-hinge-loss-is-faster-than-svc-implementation-in-scikit-le", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/60061412", "snippet": "The sklearn SVM is computationally expensive <b>compared</b> to sklearn SGD classifier with <b>loss</b>=&#39;<b>hinge</b>&#39;. Hence we use SGD classifier which is faster. This is good only for linear SVM. If we are using &#39;rbf&#39; kernel, then SGD is not suitable.", "dateLastCrawled": "2022-01-29T00:40:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the following stages occur: Stage 1: Underfitting stage \u2013 high train and high test errors (or low ...", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, <b>squared</b> <b>hinge</b> <b>loss</b> function (as against <b>hinge</b> <b>loss</b> function) and l2 penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "However, in <b>machine</b> <b>learning</b> methodology, <b>squared</b> <b>loss</b> will be minimized with respect to ... <b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A study on L2-<b>loss (Squared Hinge-Loss) multiclass SVM</b> | Request PDF", "url": "https://www.researchgate.net/publication/235884495_A_study_on_L2-loss_Squared_Hinge-Loss_multiclass_SVM", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235884495_A_study_on_L2-<b>loss</b>_<b>Squared</b>_<b>Hinge</b>...", "snippet": "Taking the <b>analogy</b> to classification task, it has been previously studied [13] that using the <b>squared</b> <b>hinge</b> <b>loss</b> in SVM would yield better accuracy when \u03bb is large. In this case, underfitting ...", "dateLastCrawled": "2021-12-14T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Metrics to Evaluate Classification and Regression Algorithms | by ...", "url": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and-regression-algorithms-1554f1e00a75", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and...", "snippet": "<b>Hinge</b> <b>loss</b>. cross-entropy <b>loss</b> / log <b>loss</b>. likelihood <b>loss</b>. MSE / Quadratic <b>loss</b> / L2 <b>loss</b>: Mean <b>Squared</b> Error, or MSE <b>loss</b> is the default <b>loss</b> to use for regression problems. Mathematically, it ...", "dateLastCrawled": "2022-01-17T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "3.3. Metrics and scoring: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "<b>hinge</b>_<b>loss</b> (y_true, pred_decision, *[, ...]) Average <b>hinge</b> <b>loss</b> (non-regularized). ... \u201cThe Matthews correlation coefficient is used in <b>machine</b> <b>learning</b> as a measure of the quality of binary (two-class) classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Models 1.1 Support vector <b>machine</b> 1.1.1 Principle 1.1.2 Kernel 1.1.3 Soft margin SVM 1.1.4 <b>Hinge</b> <b>loss</b> view 1.1.5 Multi-class SVM 1.1.6 Extensions 1.2 Tree-based models 1.2.1 Decision tree 1.2.2 Random forest 1.2.3 Gradient boosted decision trees 1.2.4 Tools 1.3 EM Principle 1.4 MaxEnt 1.4.1 Entropy 1.5 Model selection 1.5.1 Under-fitting / Over-fitting 1.5.2 Model ensemble, sklearn 2.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Fast generalization rates for distance metric</b> <b>learning</b> - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-018-5734-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-018-5734-0", "snippet": "For example, the <b>squared</b> <b>hinge</b> <b>loss</b> \\(\\ell _s^1(x) = \\max (1-x, 0) ... <b>Analogy</b>-preserving semantic embedding for visual object categorization. In Proceedings of the 30th international conference on <b>machine</b> <b>learning</b>, Atlanta, GA (pp. 639\u2013647). Jin, R., Wang, S., &amp; Zhou, Y. (2010). Regularized distance metric <b>learning</b>: Theory and algorithm. Advances in neural information processing systems (Vol. 23, pp. 862\u2013870). Cambridge, MA: MIT Press. Google Scholar Kulis, B. (2012). Metric <b>learning</b>: A ...", "dateLastCrawled": "2021-12-28T14:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Liblinear does not support L1-regularized L1-<b>loss</b> ( <b>hinge</b> <b>loss</b> ...", "url": "https://www.quora.com/Liblinear-does-not-support-L1-regularized-L1-loss-hinge-loss-support-vector-classification-Why", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Liblinear-does-not-support-L1-regularized-L1-<b>loss</b>-<b>hinge</b>-<b>loss</b>...", "snippet": "Answer (1 of 3): On the web site of Liblinear, it is stated that L1-regularized SVM does not give higher accuracy but may be slower in training. Source: LIBLINEAR FAQ Indeed based on my current research, L1-regularized, L1-<b>loss</b> SVM does not perform particularly well when # features is larger th...", "dateLastCrawled": "2022-01-14T20:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "We\u2019re then using <b>machine</b> <b>learning</b> for ... The <b>squared hinge loss is like</b> the hinge formula displayed above, but then the \\(max()\\) function output is squared. This helps achieving two things: Firstly, it makes the loss value more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add to the loss more significantly than smaller errors. Note that simiarly, this may also mean that you\u2019ll need to inspect your dataset for the presence of such outliers first. Secondly ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\u635f\u5931\u51fd\u6570 - \u7b97\u6cd5\u6742\u8d27\u94fa - bjmsong.github.io", "url": "https://bjmsong.github.io/2020/02/21/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/", "isFamilyFriendly": true, "displayUrl": "https://bjmsong.github.io/2020/02/21/\u635f\u5931\u51fd\u6570", "snippet": "the training data is fed into the <b>machine</b> <b>learning</b> model; Loss : compare between some actual targets and predicted targets; the lower the loss, the more the set of targets and the set of predictions resemble each other; the more they resemble each other, the better the <b>machine</b> <b>learning</b> model performs. Backward pass", "dateLastCrawled": "2021-12-27T11:43:00.0000000Z", "language": "zh_chs", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(squared hinge loss)  is like +(hinge loss)", "+(squared hinge loss) is similar to +(hinge loss)", "+(squared hinge loss) can be thought of as +(hinge loss)", "+(squared hinge loss) can be compared to +(hinge loss)", "machine learning +(squared hinge loss AND analogy)", "machine learning +(\"squared hinge loss is like\")", "machine learning +(\"squared hinge loss is similar\")", "machine learning +(\"just as squared hinge loss\")", "machine learning +(\"squared hinge loss can be thought of as\")", "machine learning +(\"squared hinge loss can be compared to\")"]}