{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "NIPS2017abs.md \u00b7 GitHub", "url": "https://gist.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "isFamilyFriendly": true, "displayUrl": "https://<b>gist</b>.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is ubiquitous in machine learning and underlies most supervised learning methods. While there is a large body of work on algorithms for various <b>ERM</b> problems, the exact computational complexity of <b>ERM</b> is still not understood. We address this issue for multiple popular <b>ERM</b> problems including kernel SVMs, kernel ridge regression, and training the final layer of a neural network. In particular, we give conditional hardness results for these problems based on ...", "dateLastCrawled": "2022-01-31T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>ICML 2018 Abstracts</b> \u00b7 GitHub", "url": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "snippet": "In this paper, we derive bounds on the mutual information of the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) procedure for both 0-1 and strongly-convex loss classes. We prove that under the Axiom of Choice, the existence of an <b>ERM</b> learning rule with a vanishing mutual information is equivalent to the assertion that the loss class has a finite VC dimension, thus bridging information theory with statistical learning theory. Similarly, an asymptotic bound on the mutual information is established for ...", "dateLastCrawled": "2022-01-19T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Understanding Machine Learning Theory Algorithms</b> | PDF | Machine ...", "url": "https://www.scribd.com/document/286988649/Understanding-Machine-Learning-Theory-Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/286988649/<b>Understanding-Machine-Learning-Theory-Algorithms</b>", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length (MDL ) learning rules, which shows how can a machine learn. We quantify the amount of data needed for learning using the <b>ERM</b>, SRM, and MDL rules and show how learning might fail by deriving viii. a no-free-lunch theorem. We also discuss how much computation time is required for learning. In the second part of the book we describe various learning algorithms. For some of the ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Variance-based regularization with convex objectives", "url": "https://www.researchgate.net/publication/308980716_Variance-based_regularization_with_convex_objectives", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/308980716_Variance-based_regularization_with...", "snippet": "Established approaches to obtain generalization bounds in data-driven optimization and machine learning mostly build on solutions from <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), which depend crucially on ...", "dateLastCrawled": "2022-01-06T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding Machine Learning: From Theory To Algorithms 1107057132 ...", "url": "https://ebin.pub/understanding-machine-learning-from-theory-to-algorithms-1107057132-9781107057135.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/understanding-machine-learning-from-theory-to-algorithms-1107057132...", "snippet": "2.3 <b>EMPIRICAL</b> <b>RISK</b> <b>MINIMIZATION</b> WITH INDUCTIVE BIAS We have just demonstrated that the <b>ERM</b> rule might lead to overfitting. Rather than giving up on the <b>ERM</b> paradigm, we will look for ways to rectify it. We will search for conditions under which there is a guarantee that <b>ERM</b> does not overfit, namely, conditions under which when the <b>ERM</b> predictor has good performance with respect to the training data, it is also highly likely to perform well over the underlying data distribution. A common ...", "dateLastCrawled": "2021-12-31T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding Machine Learning 9781107057135, 1107057132 - DOKUMEN.PUB", "url": "https://dokumen.pub/understanding-machine-learning-9781107057135-1107057132.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/understanding-machine-learning-9781107057135-1107057132.html", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> with Inductive Bias We have just demonstrated that the <b>ERM</b> rule might lead to overfitting. Rather than giving up on the <b>ERM</b> paradigm, we will look for ways to rectify it. We will search for conditions under which there is a guarantee that <b>ERM</b> does not overfit, namely, conditions under which when the <b>ERM</b> predictor has good performance with respect to the training data, it is also highly likely to perform well over the underlying data distribution. A common solution ...", "dateLastCrawled": "2022-01-31T07:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "Intuitively, overfitting occurs when our hypothesis fits the training data too well (perhaps <b>like</b> the everyday experience that a person who provides a perfect detailed explanation for each of his single actions may raise suspicion). 2.3 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> with Inductive Bias We have just demonstrated that the <b>ERM</b> rule might lead to overfitting. Rather than giving up on the <b>ERM</b> paradigm, we will look for ways to rectify it. We will search for conditions under which there is a ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Understanding Machine Learning: From Theory</b> to Algorithms | Keep ...", "url": "https://www.academia.edu/40679311/Understanding_Machine_Learning_From_Theory_to_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40679311/<b>Understanding_Machine_Learning_From_Theory</b>_to_Algorithms", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-23T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "nips-scraper/dump.sql at master \u00b7 <b>JasonBenn/nips-scraper</b> \u00b7 GitHub", "url": "https://github.com/JasonBenn/nips-scraper/blob/master/dump.sql", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/JasonBenn/nips-scraper/blob/master/dump.sql", "snippet": "Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or <b>empirical</b> <b>risk</b> <b>minimization</b> with convex constraints. In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on ...", "dateLastCrawled": "2021-09-22T20:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "nips 2017 - cpr-rss.github.io", "url": "https://cpr-rss.github.io/rss/nips2017.xml", "isFamilyFriendly": true, "displayUrl": "https://cpr-rss.github.io/rss/nips2017.xml", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is ubiquitous in machine learning and underlies most supervised learning methods. While there is a large body of work on algorithms for various <b>ERM</b> problems, the exact computational complexity of <b>ERM</b> is still not understood. We address this issue for multiple popular <b>ERM</b> problems including kernel SVMs, kernel ridge regression, and training the final layer of a neural network. In particular, we give conditional hardness results for these problems based on ...", "dateLastCrawled": "2022-01-18T01:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Learning how to approve <b>updates to machine learning algorithms</b> in non ...", "url": "https://deepai.org/publication/learning-how-to-approve-updates-to-machine-learning-algorithms-in-non-stationary-settings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/learning-how-to-approve-updates-to-machine-learning...", "snippet": "<b>Similar</b> smoothness assumptions have been advocated in previous works (Rakhlin et al., 2011) ... In this section, we show how approval strategies can be defined as a sequence of penalized <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) problems. The objective function in these <b>ERM</b> problems includes two regularization terms: one expresses our optimism in modifications proposed by the model developer, and the other expresses our optimism in our ability to predict future performance using historical data. By ...", "dateLastCrawled": "2021-12-03T23:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding Machine Learning: From Theory To Algorithms 1107057132 ...", "url": "https://ebin.pub/understanding-machine-learning-from-theory-to-algorithms-1107057132-9781107057135.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/understanding-machine-learning-from-theory-to-algorithms-1107057132...", "snippet": "2.3 <b>EMPIRICAL</b> <b>RISK</b> <b>MINIMIZATION</b> WITH INDUCTIVE BIAS We have just demonstrated that the <b>ERM</b> rule might lead to overfitting. Rather than giving up on the <b>ERM</b> paradigm, we will look for ways to rectify it. We will search for conditions under which there is a guarantee that <b>ERM</b> does not overfit, namely, conditions under which when the <b>ERM</b> predictor has good performance with respect to the training data, it is also highly likely to perform well over the underlying data distribution. A common ...", "dateLastCrawled": "2021-12-31T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>understanding-machine-learning</b>-theory-algorithms[1] Pages 1 - 50 - Flip ...", "url": "https://fliphtml5.com/flqg/grxi/basic", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/flqg/grxi/basic", "snippet": "This learning paradigm \u2013 coming up with a predictor h that minimizes LS(h) \u2013 is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> or <b>ERM</b> for short.2.2.1 Something May Go Wrong \u2013 Over\ufb01tting Although the <b>ERM</b> rule seems very natural, without being careful, this approach may fail miserably. To demonstrate such a failure, let us go back to the problem of learning to", "dateLastCrawled": "2021-12-18T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "NIPS2017abs.md \u00b7 GitHub", "url": "https://gist.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "isFamilyFriendly": true, "displayUrl": "https://<b>gist</b>.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is ubiquitous in machine learning and underlies most supervised learning methods. While there is a large body of work on algorithms for various <b>ERM</b> problems, the exact computational complexity of <b>ERM</b> is still not understood. We address this issue for multiple popular <b>ERM</b> problems including kernel SVMs, kernel ridge regression, and training the final layer of a neural network. In particular, we give conditional hardness results for these problems based on ...", "dateLastCrawled": "2022-01-31T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Understanding Machine Learning Theory Algorithms</b> | PDF | Machine ...", "url": "https://www.scribd.com/document/286988649/Understanding-Machine-Learning-Theory-Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/286988649/<b>Understanding-Machine-Learning-Theory-Algorithms</b>", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length (MDL) learning rules, which shows how can a machine learn. We quantify the amount of data needed for learning using the <b>ERM</b>, SRM, and MDL rules and show how learning might fail by deriving viii. a no-free-lunch theorem. We also discuss how much computation time is required for learning. In the second part of the book we describe various learning algorithms. For some of the ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Understanding Machine Learning</b> | PDF | Machine Learning | Statistics", "url": "https://www.scribd.com/document/246616207/Understanding-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/246616207", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum ... the environment, <b>playing</b> the role of the teacher, can be best thought of as passive apples drop, stars shine, and the rain falls without regard to the needs of the learner. We model such learning scenarios by postulating that the training data (or the learners experience) is generated by some random process. This is the basic building block in the branch of statistical learning. Finally ...", "dateLastCrawled": "2022-01-29T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Variance-based regularization with convex objectives", "url": "https://www.researchgate.net/publication/308980716_Variance-based_regularization_with_convex_objectives", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/308980716_Variance-based_regularization_with...", "snippet": "<b>Similar</b> phenomenon is also observed in machine learning where the solution of the <b>risk</b> <b>minimization</b> problem (1) is not guaranteed to be robust or generalizable [1,2,14,16,21, 25, 44,45,46]. The ...", "dateLastCrawled": "2022-01-06T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>ICML 2018 Abstracts</b> \u00b7 GitHub", "url": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/cwhy/2db49912f1d22495d2788ea89e933045", "snippet": "In this paper, we derive bounds on the mutual information of the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) procedure for both 0-1 and strongly-convex loss classes. We prove that under the Axiom of Choice, the existence of an <b>ERM</b> learning rule with a vanishing mutual information is equivalent to the assertion that the loss class has a finite VC dimension, thus bridging information theory with statistical learning theory. Similarly, an asymptotic bound on the mutual information is established for ...", "dateLastCrawled": "2022-01-19T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "Intuitively, overfitting occurs when our hypothesis fits the training data too well (perhaps like the everyday experience that a person who provides a perfect detailed explanation for each of his single actions may raise suspicion). 2.3 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> with Inductive Bias We have just demonstrated that the <b>ERM</b> rule might lead to overfitting. Rather than giving up on the <b>ERM</b> paradigm, we will look for ways to rectify it. We will search for conditions under which there is a ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "nips-scraper/dump.sql at master \u00b7 <b>JasonBenn/nips-scraper</b> \u00b7 GitHub", "url": "https://github.com/JasonBenn/nips-scraper/blob/master/dump.sql", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/JasonBenn/nips-scraper/blob/master/dump.sql", "snippet": "We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by <b>empirical</b> <b>risk</b> <b>minimization</b>. Our bound comes paired with a candidate attack that nearly realizes the bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB ...", "dateLastCrawled": "2021-09-22T20:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Understanding Machine Learning</b> | PDF | Machine Learning | Statistics", "url": "https://www.scribd.com/document/246616207/Understanding-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/246616207", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length ... <b>playing</b> the role of the teacher, <b>can</b> be best <b>thought</b> of as passive apples drop, stars shine, and the rain falls without regard to the needs of the learner. We model such learning scenarios by postulating that the training data (or the learners experience) is generated by some random process. This is the basic building block in the branch of statistical learning. Finally ...", "dateLastCrawled": "2022-01-29T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Understanding Machine Learning Theory Algorithms</b> | PDF | Machine ...", "url": "https://www.scribd.com/document/286988649/Understanding-Machine-Learning-Theory-Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/286988649/<b>Understanding-Machine-Learning-Theory-Algorithms</b>", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length (MDL) learning rules, which shows how <b>can</b> a machine learn. We quantify the amount of data needed for learning using the <b>ERM</b>, SRM, and MDL rules and show how learning might fail by deriving viii. a no-free-lunch theorem. We also discuss how much computation time is required for learning. In the second part of the book we describe various learning algorithms. For some of the ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>understanding-machine-learning</b>-theory-algorithms[1] Pages 1 - 50 - Flip ...", "url": "https://fliphtml5.com/flqg/grxi/basic", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/flqg/grxi/basic", "snippet": "We describethe <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM),and Minimum Description Length (MDL) learning rules, which shows \u201chow cana machine learn\u201d. We quantify the amount of data needed for learning usingthe <b>ERM</b>, SRM, and MDL rules and show how learning might fail by deriving viii a \u201cno-free-lunch\u201d theorem. We also discuss how much computation time is re- quired for learning. In the second part of the book we describe various learning algorithms. For some ...", "dateLastCrawled": "2021-12-18T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding Machine Learning: From Theory To Algorithms 1107057132 ...", "url": "https://ebin.pub/understanding-machine-learning-from-theory-to-algorithms-1107057132-9781107057135.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/understanding-machine-learning-from-theory-to-algorithms-1107057132...", "snippet": "2.3 <b>EMPIRICAL</b> <b>RISK</b> <b>MINIMIZATION</b> WITH INDUCTIVE BIAS We have just demonstrated that the <b>ERM</b> rule might lead to overfitting. Rather than giving up on the <b>ERM</b> paradigm, we will look for ways to rectify it. We will search for conditions under which there is a guarantee that <b>ERM</b> does not overfit, namely, conditions under which when the <b>ERM</b> predictor has good performance with respect to the training data, it is also highly likely to perform well over the underlying data distribution. A common ...", "dateLastCrawled": "2021-12-31T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "NIPS2017abs.md \u00b7 GitHub", "url": "https://gist.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "isFamilyFriendly": true, "displayUrl": "https://<b>gist</b>.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is ubiquitous in machine learning and underlies most supervised learning methods. While there is a large body of work on algorithms for various <b>ERM</b> problems, the exact computational complexity of <b>ERM</b> is still not understood. We address this issue for multiple popular <b>ERM</b> problems including kernel SVMs, kernel ridge regression, and training the final layer of a neural network. In particular, we give conditional hardness results for these problems based on ...", "dateLastCrawled": "2022-01-31T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "Intuitively, overfitting occurs when our hypothesis fits the training data too well (perhaps like the everyday experience that a person who provides a perfect detailed explanation for each of his single actions may raise suspicion). 2.3 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> with Inductive Bias We have just demonstrated that the <b>ERM</b> rule might lead to overfitting. Rather than giving up on the <b>ERM</b> paradigm, we will look for ways to rectify it. We will search for conditions under which there is a ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding Machine Learning 9781107057135, 1107057132 - DOKUMEN.PUB", "url": "https://dokumen.pub/understanding-machine-learning-9781107057135-1107057132.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/understanding-machine-learning-9781107057135-1107057132.html", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> with Inductive Bias We have just demonstrated that the <b>ERM</b> rule might lead to overfitting. Rather than giving up on the <b>ERM</b> paradigm, we will look for ways to rectify it. We will search for conditions under which there is a guarantee that <b>ERM</b> does not overfit, namely, conditions under which when the <b>ERM</b> predictor has good performance with respect to the training data, it is also highly likely to perform well over the underlying data distribution. A common solution ...", "dateLastCrawled": "2022-01-31T07:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Learning how to approve <b>updates to machine learning algorithms</b> in non ...", "url": "https://deepai.org/publication/learning-how-to-approve-updates-to-machine-learning-algorithms-in-non-stationary-settings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/learning-how-to-approve-updates-to-machine-learning...", "snippet": "In this section, we show how approval strategies <b>can</b> be defined as a sequence of penalized <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) problems. The objective function in these <b>ERM</b> problems includes two regularization terms: one expresses our optimism in modifications proposed by the model developer, and the other expresses our optimism in our ability to predict future performance using historical data. By varying the", "dateLastCrawled": "2021-12-03T23:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Understanding Machine Learning: From Theory</b> to Algorithms | Keep ...", "url": "https://www.academia.edu/40679311/Understanding_Machine_Learning_From_Theory_to_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40679311/<b>Understanding_Machine_Learning_From_Theory</b>_to_Algorithms", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-23T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Known, the Unknown, and the Unknowable in Financial <b>Risk</b> Management ...", "url": "https://ebin.pub/the-known-the-unknown-and-the-unknowable-in-financial-risk-management-measurement-and-theory-advancing-practice-0691128839-9780691128832.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/the-known-the-unknown-and-the-unknowable-in-financial-<b>risk</b>-management...", "snippet": "A mutual-\u00adlike structure <b>can</b> still achieve some degree of <b>risk</b> pooling for the idiosyncratic <b>risk</b>, and the systematic <b>risk</b> (whether from stage 1 or stage 2 of the lottery) <b>can</b> be shared across the population at <b>risk</b> by devices such as ex post dividends, assessments, or taxes.3 Kunreuther and Pauly present a case study of catastrophic <b>risk</b> insurance that blends aspects of simple insurance for K and mutual insurance for u, arguing 3 Borch\u2019s theory closely parallels the capital asset pricing ...", "dateLastCrawled": "2022-01-20T19:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Variance-based regularization with convex objectives", "url": "https://www.researchgate.net/publication/308980716_Variance-based_regularization_with_convex_objectives", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/308980716_Variance-based_regularization_with...", "snippet": "Established approaches to obtain generalization bounds in data-driven optimization and machine learning mostly build on solutions from <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), which depend crucially on ...", "dateLastCrawled": "2022-01-06T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>NIPS 2018 Abstract</b> \u00b7 GitHub", "url": "https://gist.github.com/cwhy/3d9fdd54a75a6f698c929f691e9d4d83", "isFamilyFriendly": true, "displayUrl": "https://<b>gist</b>.github.com/cwhy/3d9fdd54a75a6f698c929f691e9d4d83", "snippet": "As opposed to standard <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), distributionally robust optimization aims to minimize the worst-case <b>risk</b> over a larger ambiguity set containing the original <b>empirical</b> distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove a generalization bound that involves the covering number properties of the original <b>ERM</b> problem. As an illustrative ...", "dateLastCrawled": "2022-01-02T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "NIPS2017abs.md \u00b7 GitHub", "url": "https://gist.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "isFamilyFriendly": true, "displayUrl": "https://<b>gist</b>.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is ubiquitous in machine learning and underlies most supervised learning methods. While there is a large body of work on algorithms for various <b>ERM</b> problems, the exact computational complexity of <b>ERM</b> is still not understood. We address this issue for multiple popular <b>ERM</b> problems including kernel SVMs, kernel ridge regression, and training the final layer of a neural network. In particular, we give conditional hardness results for these problems based on ...", "dateLastCrawled": "2022-01-31T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Understanding Machine Learning</b> | PDF | Machine Learning | Statistics", "url": "https://www.scribd.com/document/246616207/Understanding-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/246616207", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length (MDL ... <b>playing</b> the role of the teacher, <b>can</b> be best thought of as passive apples drop, stars shine, and the rain falls without regard to the needs of the learner. We model such learning scenarios by postulating that the training data (or the learners experience ) is generated by some random process. This is the basic building block in the branch of statistical learning ...", "dateLastCrawled": "2022-01-29T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Understanding Machine Learning Theory Algorithms</b> | PDF | Machine ...", "url": "https://www.scribd.com/document/286988649/Understanding-Machine-Learning-Theory-Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/286988649/<b>Understanding-Machine-Learning-Theory-Algorithms</b>", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length (MDL ) learning rules, which shows how <b>can</b> a machine learn. We quantify the amount of data needed for learning using the <b>ERM</b>, SRM, and MDL rules and show how learning might fail by deriving viii. a no-free-lunch theorem. We also discuss how much computation time is required for learning. In the second part of the book we describe various learning algorithms. For some of the ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Understanding Machine Learning 9781107057135, 1107057132</b> - DOKUMEN.PUB", "url": "https://dokumen.pub/understanding-machine-learning-9781107057135-1107057132-w-5259393.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/<b>understanding-machine-learning-9781107057135-1107057132</b>-w-5259393.html", "snippet": "2.3 <b>EMPIRICAL</b> <b>RISK</b> <b>MINIMIZATION</b> WITH INDUCTIVE BIAS We have just demonstrated that the <b>ERM</b> rule might lead to overfitting. Rather than giving up on the <b>ERM</b> paradigm, we will look for ways to rectify it. We will search for conditions under which there is a guarantee that <b>ERM</b> does not overfit, namely, conditions under which when the <b>ERM</b> predictor has good performance with respect to the training data, it is also highly likely to perform well over the underlying data distribution. A common ...", "dateLastCrawled": "2022-01-19T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-algorithms...", "snippet": "Intuitively, overfitting occurs when our hypothesis fits the training data too well (perhaps like the everyday experience that a person who provides a perfect detailed explanation for each of his single actions may raise suspicion). 2.3 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> with Inductive Bias We have just demonstrated that the <b>ERM</b> rule might lead to overfitting. Rather than giving up on the <b>ERM</b> paradigm, we will look for ways to rectify it. We will search for conditions under which there is a ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Understanding Machine Learning: From Theory</b> to Algorithms | Keep ...", "url": "https://www.academia.edu/40679311/Understanding_Machine_Learning_From_Theory_to_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40679311/<b>Understanding_Machine_Learning_From_Theory</b>_to_Algorithms", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-23T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "nips-scraper/dump.sql at master \u00b7 <b>JasonBenn/nips-scraper</b> \u00b7 GitHub", "url": "https://github.com/JasonBenn/nips-scraper/blob/master/dump.sql", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/JasonBenn/nips-scraper/blob/master/dump.sql", "snippet": "Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or <b>empirical</b> <b>risk</b> <b>minimization</b> with convex constraints. In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on ...", "dateLastCrawled": "2021-09-22T20:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Hal S. Scott</b> | Harvard Law School", "url": "https://hls.harvard.edu/faculty/directory/10781/Scott", "isFamilyFriendly": true, "displayUrl": "https://hls.harvard.edu/faculty/directory/10781/Scott", "snippet": "Systemic <b>risk</b> is the <b>risk</b> that the failure of one significant institution <b>can</b> cause or significantly contribute to the failure of other significant institutions. This paper addresses the five most important policies for dealing with systemic <b>risk</b>: the imposition of capital requirements, the use of clearinghouses and exchanges for over-the-counter derivatives, the resolution of insolvent institutions, emergency lending by the Federal Reserve and the structure of the regulatory system. The ...", "dateLastCrawled": "2022-01-30T22:05:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> and Stochastic Gradient Descent for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "where F(Sn) is the <b>empirical</b> distribution.2 The <b>ERM</b> dogma is to select the predictor \u03c0\u02c6\u03b8 n given by \u02c6\u03b8 n = argmin\u03b8 R\u02c6(\u03b8,Sn). That is, the objective function that de\ufb01nes <b>learning</b> is the <b>empirical</b> <b>risk</b>. <b>ERM</b> has two useful properties. (1) It provides a prin-cipled framework for de\ufb01ning new <b>machine</b> <b>learning</b> methods. In particular, when ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Statistical <b>Learning</b> Theory and the C-Loss cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the <b>Risk</b> functional as L(.) is called the Loss function, and minimize it w.r.t. w achieving the best possible loss. But we can not do this ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Computational and Statistical <b>Learning</b> Theory", "url": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) / Sample Average Approximation (SAA): Collect sample z1UYU zm ... SGD for <b>Machine</b> <b>Learning</b> Initialize S 4 L r At iteration t: Draw T \u00e7\u00e1U \u00e71\u00de If U \u00e7 S \u00e7 \u00e1\u00f6 T \u00e7 O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00e7\u00f6 T \u00e7 else: S \u00e7 &gt; 5 Z S \u00e7 Return S % \u00cd L 5 \u00cd \u00c3 \u00cd S \u00e7 \u00e7 @ 5 Draw T 5\u00e1U 5 \u00e1\u00e5\u00e1 T \u00e0 \u00e1U \u00e0 1\u00de Initialize S 4 L r At iteration t: Pick E \u00d0 s\u00e5I at random If U \u00dc S \u00e7 \u00e1\u00f6 T \u00dc O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00dc\u00f6 T \u00dc else: S \u00e7 &gt; 5 Z S \u00e7 S \u00e7 &gt; 5 Z ...", "dateLastCrawled": "2022-01-26T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Basics of <b>Machine</b> <b>Learning</b>", "url": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_<b>learning</b>.pdf", "snippet": "This is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) COMPSCI 527 \u2014 Computer Vision Basics of <b>Machine</b> <b>Learning</b> 15/26. Loss and <b>Risk</b> <b>Machine</b> <b>Learning</b> and the Statistical <b>Risk</b> <b>ERM</b>: w^ 2argmin w2R m L T(w) In <b>machine</b> <b>learning</b>, we go much farther: We also want h to do well on previously unseen inputs To relate past and future data, assume that all data comes from the same joint probability distribution p(x;y) p is called the generative data model or just model The goal of <b>machine</b> <b>learning</b> is to ...", "dateLastCrawled": "2021-11-06T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, <b>empirical</b> <b>risk</b>, motivation for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Further Reading, Supplementary: Jan 12: Consistency of <b>ERM</b>, Sufficient condition for <b>ERM</b> as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture 1: Reinforcement <b>Learning</b>: What and Why?", "url": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "snippet": "<b>machine</b> <b>learning</b> and is referred to as <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). 3 Challenges of reinforcement <b>learning</b> Consider the cart pole balancing problem, where a cart carrying an unactuated pole \ufb02oats on a straight horizontal track. The cart is actuated by a torque applied either to the right or the left direction. Seeherefor a real cart ...", "dateLastCrawled": "2021-09-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2006.09461] Robust <b>Compressed Sensing using Generative Models</b> - arXiv", "url": "https://arxiv.org/abs/2006.09461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2006.09461", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-06-27T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_<b>Machine</b>s", "snippet": "The principle used is <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) over a set of possible functions, called hypothesis space. Formally this can be written as minimizing the <b>empirical</b> . error: \u2211 = l. 1 i. x ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ToyotaTechnologicalInstituteatChicago UniversityofTexasatAustin surbhi ...", "url": "https://arxiv.org/pdf/2005.07652", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2005.07652", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, \u02c6h \u2208 RERM U(S) ,argmin h\u2208H 1 m Xm i=1 sup z\u2208U(x) 1 [h(z) 6= y]. In this paper, we provide necessary and su\ufb03cient conditions on perturbation sets U, under which the robust empirical risk minimization (RERM) problem is e\ufb03ciently solvable in the realizable setting. We show that an e\ufb03cient ...", "dateLastCrawled": "2021-10-06T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Efficiently Learning Adversarially Robust Halfspaces with</b> Noise | DeepAI", "url": "https://deepai.org/publication/efficiently-learning-adversarially-robust-halfspaces-with-noise", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>efficiently-learning-adversarially-robust-halfspaces</b>...", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, ^ h \u2208 R E R M U ( S ) \u225c argmin h \u2208 H 1 m m \u2211 i = 1 sup z \u2208 U ( x ) 1 [ h ( z ) \u2260 y ] .", "dateLastCrawled": "2021-12-05T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficiently <b>Learning</b> Adversarially Robust Halfspaces with Noise", "url": "http://proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "snippet": "remains a major challenge in <b>machine</b> <b>learning</b>. A line of work has shown that predictors learned by deep neural networks are not robust to adversarial examples (Szegedy et al.,2014;Biggio et al.,2013;Goodfellow et al.,2015). This has led to a long line of research studying different aspects of robustness to adversarial examples. In this paper, we consider the problem of distribution-independent <b>learning</b> of halfspaces that are robust to ad-versarial examples at test time, also referred to as ...", "dateLastCrawled": "2021-11-21T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(empirical risk minimization (erm))  is like +(playing sports safely)", "+(empirical risk minimization (erm)) is similar to +(playing sports safely)", "+(empirical risk minimization (erm)) can be thought of as +(playing sports safely)", "+(empirical risk minimization (erm)) can be compared to +(playing sports safely)", "machine learning +(empirical risk minimization (erm) AND analogy)", "machine learning +(\"empirical risk minimization (erm) is like\")", "machine learning +(\"empirical risk minimization (erm) is similar\")", "machine learning +(\"just as empirical risk minimization (erm)\")", "machine learning +(\"empirical risk minimization (erm) can be thought of as\")", "machine learning +(\"empirical risk minimization (erm) can be compared to\")"]}