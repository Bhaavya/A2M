{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>The selective directed forgetting effect: Can people</b> <b>forget</b> only part ...", "url": "https://www.researchgate.net/publication/24281501_The_selective_directed_forgetting_effect_Can_people_forget_only_part_of_a_text", "isFamilyFriendly": true, "displayUrl": "https://www.research<b>gate</b>.net/publication/24281501", "snippet": "While some studies have shown that providing a cue to <b>selectively</b> <b>forget</b> one subset of previously learned facts may cause specific <b>forgetting</b> of this <b>information</b>, little is known about the ...", "dateLastCrawled": "2022-01-03T22:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Deep Learning : Intro to LSTM (Long Short Term Memory) | by HIMANSHU ...", "url": "https://medium.com/@himanshunpatel01/deep-learning-intro-to-lstm-long-short-term-memory-ce504dc6e585", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@himanshunpatel01/deep-learning-intro-to-lstm-long-short-term...", "snippet": "This is basically very similar to the <b>forget</b> <b>gate</b> and acts as a filter for all the <b>information</b> from h_t-1 and x_t. Creating a vector containing all possible values that can be added (as perceived ...", "dateLastCrawled": "2022-01-27T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Hybrid LSTM Self-Attention Mechanism Model for Forecasting</b> the Reform ...", "url": "https://www.hindawi.com/journals/cin/2021/6689204/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/cin/2021/6689204", "snippet": "First, remember that the <b>forget</b> <b>gate</b>\u2019s function controls the proportion of cells <b>selectively</b> <b>forgetting</b> <b>information</b>. Its input is the output at the previous time t \u2212 1 and the input at the current time t .", "dateLastCrawled": "2022-01-11T22:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Time series prediction for the epidemic trends of COVID-19 using the ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7437443/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7437443", "snippet": "The structure of LSTM consists of <b>forget</b> <b>gate</b>, input <b>gate</b> and output <b>gate</b>. The role of the <b>forget</b> <b>gate</b> is to <b>forget</b> the <b>information</b> in the cell state <b>selectively</b>. The input <b>gate</b> is to determine what new <b>information</b> is stored in the cell state. Finally, the output <b>gate</b> determines what value we want to output.", "dateLastCrawled": "2022-01-24T20:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lont Short Term Memory (LSTM) Networks Simple Tutorial-", "url": "http://sefidian.com/2019/08/15/long-short-term-memory-lstm-simply-explained-tutorial/", "isFamilyFriendly": true, "displayUrl": "sefidian.com/2019/08/15/long-short-term-memory-lstm-simply-explained-tutorial", "snippet": "<b>Forget</b> <b>Gate</b>; Input <b>Gate</b>; Output <b>Gate</b>; Text generation using LSTMs. 1. Flashback: A look into Recurrent Neural Networks (RNN) Take an example of sequential data, which can be the stock market\u2019s data for a particular stock. A simple machine learning model or an Artificial Neural Network may learn to predict the stock prices based on a number of features: the volume of the stock, the opening value etc. While the price of the stock depends on these features, it is also largely dependent on the ...", "dateLastCrawled": "2021-12-03T15:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Forgetting</b> as a form of adaptive engram cell plasticity | Nature ...", "url": "https://www.nature.com/articles/s41583-021-00548-3", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41583-021-00548-3", "snippet": "A hypothesized benefit of the interaction between these two opposing processes is that important <b>information</b> may be <b>selectively</b> ... brains <b>forget</b>, some common themes have emerged. Perhaps most ...", "dateLastCrawled": "2022-01-29T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Attention-Augmented</b> Machine Memory | SpringerLink", "url": "https://link.springer.com/article/10.1007/s12559-021-09854-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12559-021-09854-5", "snippet": "The attention <b>gate</b> works on the <b>forget</b> <b>gate</b> and the input <b>gate</b>. To some extent, it determines which part of <b>information</b> should be ignored and which part of <b>information</b> AAMM should pay attention to. To the end, the attention <b>gate</b> maintains the learning process of AAMM focusing on important <b>information</b> and neglecting irrelevant <b>information</b>. Functionally, AAMM can not only <b>selectively</b> remember critical historical <b>information</b> but notice more noteworthy <b>information</b> during the sequence learning ...", "dateLastCrawled": "2022-01-03T18:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "M R L LSTM LANGUAGE MODELS - Venues | OpenReview", "url": "https://openreview.net/pdf?id=9ITXiTrAoT", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=9ITXiTrAoT", "snippet": "f) is the value of the <b>forget</b> <b>gate</b>, which depends only on the <b>forget</b> <b>gate</b> bias b f. Equation 1 shows that LSTM memory exhibits exponential decay with characteristic <b>forgetting</b> time T= (2) 1 logf 0 = 1 log(1 + e b f): That is, values in the cell state tend to shrink by a factor of eevery Ttimesteps. We refer to the <b>forgetting</b> time in Equation 2 ...", "dateLastCrawled": "2022-01-06T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Learning \u2013 Getting Started with Long Short</b> Term Memory \u2013 The next ...", "url": "https://datamafia2.wordpress.com/2018/05/18/deep-learning-getting-started-with-long-short-term-memory/", "isFamilyFriendly": true, "displayUrl": "https://datamafia2.wordpress.com/2018/05/18/<b>deep-learning-getting-started-with-long</b>...", "snippet": "<b>Forget</b> <b>Gate</b>; Input <b>Gate</b>; Output <b>Gate</b>; Text generation using LSTMs. 1. Flashback: A look into Recurrent Neural Networks (RNN) Take an example of sequential data, which can be the stock market\u2019s data for a particular stock. A simple machine learning model or an Artificial Neural Network may learn to predict the stock prices based on a number of features: the volume of the stock, the opening value etc. While the price of the stock depends on these features, it is also largely dependent on the ...", "dateLastCrawled": "2021-12-28T17:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Time Series Forecasting with Deep Learning</b> and Attention ... - TOPBOTS", "url": "https://www.topbots.com/time-series-forecasting-with-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/<b>time-series-forecasting-with-deep-learning</b>", "snippet": "The first <b>gate</b> is the <b>forget</b> <b>gate</b>. This <b>gate</b> decides which <b>information</b> should be deleted or saved. The <b>information</b> from the previous hidden state and the <b>information</b> from the current input are passed through the sigmoid function. An output is close to 0 it means that the <b>information</b> can be forgotten, while an output close to 1 means that the <b>information</b> must be saved.", "dateLastCrawled": "2022-02-01T08:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Selective cues to forget can fail to cause forgetting</b> | Request PDF", "url": "https://www.researchgate.net/publication/234041646_Selective_cues_to_forget_can_fail_to_cause_forgetting", "isFamilyFriendly": true, "displayUrl": "https://www.research<b>gate</b>.net/publication/234041646_<b>Selective_cues_to_forget_can</b>_fail...", "snippet": "While some studies have shown that providing a cue <b>to selectively</b> <b>forget</b> one subset of previously learned facts may cause specific <b>forgetting</b> of this <b>information</b>, little is known about the ...", "dateLastCrawled": "2021-08-28T00:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Slower is <b>Better: Revisiting the Forgetting Mechanism in</b> LSTM for ...", "url": "https://deepai.org/publication/slower-is-better-revisiting-the-forgetting-mechanism-in-lstm-for-slower-information-decay", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/slower-is-<b>better-revisiting-the-forgetting-mechanism-in</b>...", "snippet": "Here, we propose a power law <b>forget</b> <b>gate</b>, which instead learns to <b>forget</b> <b>information</b> along a slower power law decay function. Specifically, the new <b>gate</b> learns to control the power law decay factor, p, allowing the network to adjust the <b>information</b> decay rate according to task demands. Our experiments show that an LSTM with power law <b>forget</b> gates (pLSTM) can effectively capture long-range dependencies beyond hundreds of elements on image classification, language modeling, and categorization ...", "dateLastCrawled": "2021-12-06T11:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep Learning : Intro to LSTM (Long Short Term Memory) | by HIMANSHU ...", "url": "https://medium.com/@himanshunpatel01/deep-learning-intro-to-lstm-long-short-term-memory-ce504dc6e585", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@himanshunpatel01/deep-learning-intro-to-lstm-long-short-term...", "snippet": "This is basically very <b>similar</b> to the <b>forget</b> <b>gate</b> and acts as a filter for all the <b>information</b> from h_t-1 and x_t. Creating a vector containing all possible values that can be added (as perceived ...", "dateLastCrawled": "2022-01-27T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Understanding RNNs, LSTMs and GRUs</b> | by Nicholas Asquith | Towards Data ...", "url": "https://towardsdatascience.com/understanding-rnns-lstms-and-grus-ed62eb584d90", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>understanding-rnns-lstms-and-grus</b>-ed62eb584d90", "snippet": "(1) the update <b>gate</b> acts <b>similar</b> to the <b>forget</b> and input <b>gate</b> of an LSTM, it decides what <b>information</b> to keep and which to throw away, and what new <b>information</b> to add. The candidate hidden state is done the same way as the LSTM, the difference is that, inside of the candidate computation, it will reset some of the previous <b>Gate</b> (vectors between 0\u20131, defined by linear transformations).", "dateLastCrawled": "2022-02-03T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Attention-Augmented</b> Machine Memory | SpringerLink", "url": "https://link.springer.com/article/10.1007/s12559-021-09854-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12559-021-09854-5", "snippet": "The attention <b>gate</b> works on the <b>forget</b> <b>gate</b> and the input <b>gate</b>. To some extent, it determines which part of <b>information</b> should be ignored and which part of <b>information</b> AAMM should pay attention to. To the end, the attention <b>gate</b> maintains the learning process of AAMM focusing on important <b>information</b> and neglecting irrelevant <b>information</b>. Functionally, AAMM can not only <b>selectively</b> remember critical historical <b>information</b> but notice more noteworthy <b>information</b> during the sequence learning ...", "dateLastCrawled": "2022-01-03T18:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "LSTM-EFG <b>for wind power forecasting</b> based on sequential correlation ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167739X18314420", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167739X18314420", "snippet": "Input layer door and <b>forget</b>-<b>gate</b> both work on the state of cells. But the role of the input <b>gate</b> is <b>to selectively</b> record new <b>information</b> into the cell state, while <b>forget</b>-<b>gate</b> is aimed at <b>selectively</b> <b>forgetting</b> <b>information</b> about cell states. The output layer <b>gate</b> acts on the hidden layer to output <b>information</b>.", "dateLastCrawled": "2022-01-17T21:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Hybrid LSTM Self-Attention Mechanism Model for Forecasting</b> the Reform ...", "url": "https://www.hindawi.com/journals/cin/2021/6689204/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/cin/2021/6689204", "snippet": "First, remember that the <b>forget</b> <b>gate</b>\u2019s function controls the proportion of cells <b>selectively</b> <b>forgetting</b> <b>information</b>. Its input is the output at the previous time t \u2212 1 and the input at the current time t .", "dateLastCrawled": "2022-01-11T22:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Short-term electrical load forecasting method based on stacked auto ...", "url": "https://link.springer.com/article/10.1007/s12065-018-00196-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12065-018-00196-0", "snippet": "Therefore, the function of expressing the degree of <b>forgetting</b> of the hidden state is equivalent to the <b>forgetting</b> <b>gate</b>, and <b>selectively</b> <b>forgetting</b> some irrelevant <b>information</b>. The function of \\((1 - {z_t}) \\times {h_{t - 1}}\\) indicating the degree of selective memory of the current node <b>information</b> is equivalent to selecting some relevant <b>information</b>.", "dateLastCrawled": "2022-02-02T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Remembering to <b>Forget</b>: A Dual Role for Sleep Oscillations in Memory ...", "url": "https://europepmc.org/article/MED/30930746", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/MED/30930746", "snippet": "Active <b>forgetting</b> is deliberate, organized and acts <b>to selectively</b> clear unnecessary <b>information</b> from the brain. Comparatively, passive <b>forgetting</b> is unintentional and disadvantageous, often causing adaptive <b>information</b> to become decreasingly available and veridical. Notably, the active <b>forgetting</b> described here refers to a series of adaptive, sleep-dependent processes organized for gist extraction and the clearance of unimportant memories and differs from other forms of active <b>forgetting</b> ...", "dateLastCrawled": "2021-07-17T15:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Deep Learning \u2013 Getting Started with Long Short</b> Term Memory \u2013 The next ...", "url": "https://datamafia2.wordpress.com/2018/05/18/deep-learning-getting-started-with-long-short-term-memory/", "isFamilyFriendly": true, "displayUrl": "https://datamafia2.wordpress.com/2018/05/18/<b>deep-learning-getting-started-with-long</b>...", "snippet": "<b>Forget</b> <b>Gate</b>; Input <b>Gate</b>; Output <b>Gate</b>; Text generation using LSTMs. 1. Flashback: A look into Recurrent Neural Networks (RNN) Take an example of sequential data, which can be the stock market\u2019s data for a particular stock. A simple machine learning model or an Artificial Neural Network may learn to predict the stock prices based on a number of features: the volume of the stock, the opening value etc. While the price of the stock depends on these features, it is also largely dependent on the ...", "dateLastCrawled": "2021-12-28T17:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Written Memories: Understanding, Deriving and Extending the</b> LSTM - R2RT", "url": "https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html", "isFamilyFriendly": true, "displayUrl": "https://r2rt.com/<b>written-memories-understanding-deriving-and-extending-the</b>-lstm.html", "snippet": "In the real-world, we <b>can</b> only keep so many things in mind at once; in order to make room for new <b>information</b>, we need to <b>selectively</b> <b>forget</b> the least relevant old <b>information</b>. In order for our RNN cells to do this, they need a mechanism for selective <b>forgetting</b>. With that, there are just two more steps to deriving the LSTM: we need to determine a mechanism for selectivity, and; we need to glue the pieces together. Gates as a mechanism for selectivity. Selective reading, writing and ...", "dateLastCrawled": "2022-01-26T10:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Lont Short Term Memory (LSTM) Networks Simple Tutorial-", "url": "http://sefidian.com/2019/08/15/long-short-term-memory-lstm-simply-explained-tutorial/", "isFamilyFriendly": true, "displayUrl": "sefidian.com/2019/08/15/long-short-term-memory-lstm-simply-explained-tutorial", "snippet": "<b>Forget</b> <b>Gate</b>; Input <b>Gate</b>; Output <b>Gate</b>; Text generation using LSTMs. 1. Flashback: A look into Recurrent Neural Networks (RNN) Take an example of sequential data, which <b>can</b> be the stock market\u2019s data for a particular stock. A simple machine learning model or an Artificial Neural Network may learn to predict the stock prices based on a number of ...", "dateLastCrawled": "2021-12-03T15:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep Learning Fall 2019 Lecture 2: RNN &amp; LSTM &amp; Attention</b>", "url": "https://zhims.github.io/doc/note/datascience/DL/lec2.pdf", "isFamilyFriendly": true, "displayUrl": "https://zhims.github.io/doc/note/datascience/DL/lec2.pdf", "snippet": "ow of <b>information</b> (a) (b) Figure 2.6: ow of <b>information</b> 1.<b>information</b> propagates along the chain like on a conveyor belt 2.<b>information</b> <b>can</b> ow unchanged and is only <b>selectively</b> changed (vector addition) by \u02d9-gates LSTM: <b>forget</b> <b>gate</b> Figure 2.7: <b>forget</b> <b>gate</b> where f t= \u02d9(W f[h t 1;x t] + b f) (2.14) 1.keeping or <b>forgetting</b> of stored content?", "dateLastCrawled": "2021-11-06T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Frontiers | Remembering to <b>Forget</b>: A Dual Role for Sleep Oscillations ...", "url": "https://www.frontiersin.org/articles/10.3389/fncel.2019.00071/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fncel.2019.00071", "snippet": "Active <b>forgetting</b> is deliberate, organized and acts to <b>selectively</b> clear unnecessary <b>information</b> from the brain. Comparatively, passive <b>forgetting</b> is unintentional and disadvantageous, often causing adaptive <b>information</b> to become decreasingly available and veridical. Notably, the active <b>forgetting</b> described here refers to a series of adaptive, sleep-dependent processes organized for gist extraction and the clearance of unimportant memories and differs from other forms of active <b>forgetting</b> ...", "dateLastCrawled": "2021-12-05T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Active Forgetting: Adaptation of Memory by Prefrontal</b> Control | Annual ...", "url": "https://www.annualreviews.org/doi/full/10.1146/annurev-psych-072720-094140", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/full/10.1146/annurev-psych-072720-094140", "snippet": "Over the past century, psychologists have discussed whether <b>forgetting</b> might arise from active mechanisms that promote memory loss to achieve various functions, such as minimizing errors, facilitating learning, or regulating one&#39;s emotional state. The past decade has witnessed a great expansion in knowledge about the brain mechanisms underlying active <b>forgetting</b> in its varying forms. A core discovery concerns the role of the prefrontal cortex in exerting top-down control over mnemonic ...", "dateLastCrawled": "2022-02-01T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Many Faces <b>of Forgetting: Toward a Constructive View</b> of <b>Forgetting</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2211368119301767", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2211368119301767", "snippet": "In addition to revealing the mnemonic costs and benefits stemming from instructions to <b>forget</b> previously encoded <b>information</b> sets, the list-method directed <b>forgetting</b> paradigm (Bjork et al., 1968) has informed how shifts in the contents of conscious awareness (e.g., Estes, 1955, Howard and Kahana, 2002, Mensink and Raaijmakers, 1988; see Manning, Norman, &amp; Kahana, 2014, for a review) <b>can</b> set the stage for <b>forgetting</b>. In this particular laboratory task, participants study two lists of items ...", "dateLastCrawled": "2022-02-02T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Remembering can cause forgetting: Retrieval dynamics in</b> long-term ...", "url": "https://www.academia.edu/13957397/Remembering_can_cause_forgetting_Retrieval_dynamics_in_long_term_memory", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/13957397/<b>Remembering_can_cause_forgetting_Retrieval_dynamics</b>...", "snippet": "<b>Remembering can cause forgetting: Retrieval dynamics in</b> long-term memory. Journal of Experimental Psychology: Learning, Memory, and Cognition, 1994. Robert Bjork. Michael Anderson. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. <b>Remembering can cause forgetting: Retrieval dynamics in</b> long-term memory . Download ...", "dateLastCrawled": "2022-01-04T20:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Why does it take time to <b>forget</b> things easily that I am scared of and I ...", "url": "https://www.quora.com/Why-does-it-take-time-to-forget-things-easily-that-I-am-scared-of-and-I-dont-like-and-what-is-the-reason-it-comes-repeatedly-to-my-mind", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-does-it-take-time-to-<b>forget</b>-things-easily-that-I-am-scared...", "snippet": "Answer (1 of 3): Emotionally charged memories are stored differently in the brain. An average, pleasant memory of a red ball bouncing across the street would be broken up and stored in different areas: the pleasant emotion you felt in one area, the color of redness in another area, the concept of...", "dateLastCrawled": "2022-01-18T22:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "\u201cNever get so <b>busy making a living that you forget to</b> make a life ...", "url": "https://www.workitdaily.com/quotes-about-work-life-balance/never-get-so-busy-making-a-living-that-you-forget-to-make-a-life-dolly-parton", "isFamilyFriendly": true, "displayUrl": "https://www.workitdaily.com/quotes-about-work-life-balance/never-get-so-busy-making-a...", "snippet": "In a few short years, our ability to virtually network in the absence of travel and in-person events, convey volumes of <b>information</b>, and create seismic impact has exploded with social media. Out-of-touch, one-dimensional blog posts, reposting lackluster content produced by an uninformed marketing department, or depending on \u201c<b>thought</b> leadership\u201d as the primary strategy to stand out from competitors has no statistically measurable impact on changing buying behavior. Instead, adding to in ...", "dateLastCrawled": "2022-01-28T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is it <b>like to have a really bad memory</b>? - Quora", "url": "https://www.quora.com/What-is-it-like-to-have-a-really-bad-memory", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-it-<b>like-to-have-a-really-bad-memory</b>", "snippet": "Answer (1 of 22): The worst thing about having a really bad memory is all the embarrassing situations you get yourself into. The second worst is appearing more dumb than you really are. You constantly worry about offending people because you <b>can</b>&#39;t remember them (especially people important to y...", "dateLastCrawled": "2021-12-05T01:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Selective cues to forget can fail to cause forgetting</b> | Request PDF", "url": "https://www.researchgate.net/publication/234041646_Selective_cues_to_forget_can_fail_to_cause_forgetting", "isFamilyFriendly": true, "displayUrl": "https://www.research<b>gate</b>.net/publication/234041646_<b>Selective_cues_to_forget_can</b>_fail...", "snippet": "While some studies have shown that providing a cue <b>to selectively</b> <b>forget</b> one subset of previously learned facts may cause specific <b>forgetting</b> of this <b>information</b>, little is known about the ...", "dateLastCrawled": "2021-08-28T00:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Slower is <b>Better: Revisiting the Forgetting Mechanism in</b> LSTM for ...", "url": "https://deepai.org/publication/slower-is-better-revisiting-the-forgetting-mechanism-in-lstm-for-slower-information-decay", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/slower-is-<b>better-revisiting-the-forgetting-mechanism-in</b>...", "snippet": "By empirically testing the performance of LSTM with a power law <b>forget</b> <b>gate</b> (pLSTM) and examining the gating behavior, we find that pLSTM exhibits two major advantages <b>compared</b> to other LSTM models: (1) The decay factor p, which controls the rates of <b>information</b> decay, <b>can</b> be flexibly learned to capture a wide range of timescales based on the task demands. This allows the network to learn <b>information</b> over hundreds of timesteps without prior knowledge for initialization. (2) The power law ...", "dateLastCrawled": "2021-12-06T11:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Hybrid LSTM Self-Attention Mechanism Model for Forecasting</b> the Reform ...", "url": "https://www.hindawi.com/journals/cin/2021/6689204/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/cin/2021/6689204", "snippet": "(1) Forgotten <b>Gate</b>. First, remember that the <b>forget</b> <b>gate</b>\u2019s function controls the proportion of cells <b>selectively</b> <b>forgetting</b> <b>information</b>. Its input is the output at the previous time t \u2212 1 and the input at the current time t.", "dateLastCrawled": "2022-01-11T22:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Attention-Augmented</b> Machine Memory | SpringerLink", "url": "https://link.springer.com/article/10.1007/s12559-021-09854-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12559-021-09854-5", "snippet": "The attention <b>gate</b> works on the <b>forget</b> <b>gate</b> and the input <b>gate</b>. To some extent, it determines which part of <b>information</b> should be ignored and which part of <b>information</b> AAMM should pay attention to. To the end, the attention <b>gate</b> maintains the learning process of AAMM focusing on important <b>information</b> and neglecting irrelevant <b>information</b>. Functionally, AAMM <b>can</b> not only <b>selectively</b> remember critical historical <b>information</b> but notice more noteworthy <b>information</b> during the sequence learning ...", "dateLastCrawled": "2022-01-03T18:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Alleviating catastrophic <b>forgetting</b> using context-dependent gating and ...", "url": "https://www.pnas.org/content/115/44/E10467", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/115/44/E10467", "snippet": "Artificial neural networks <b>can</b> suffer from catastrophic <b>forgetting</b>, in which learning a new task causes the network to <b>forget</b> how to perform previous tasks. While previous studies have proposed various methods that <b>can</b> alleviate <b>forgetting</b> over small numbers (\u2a7d10) of tasks, it is uncertain whether they <b>can</b> prevent <b>forgetting</b> across larger numbers of tasks. In this study, we propose a neuroscience-inspired scheme, called \u201ccontext-dependent gating,\u201d in which mostly nonoverlapping sets of ...", "dateLastCrawled": "2022-01-25T01:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Responsible remembering and <b>forgetting</b> as contributors to memory for ...", "url": "https://www.researchgate.net/publication/348636237_Responsible_remembering_and_forgetting_as_contributors_to_memory_for_important_information/fulltext/609bb26b458515a04c48c25d/Responsible-remembering-and-forgetting-as-contributors-to-memory-for-important-information.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.research<b>gate</b>.net/publication/348636237_Responsible_remembering_and...", "snippet": "tant <b>information</b>, <b>forgetting</b> is also a critical component of memory. Specifically, to avoid <b>forgetting</b> important things, people may have developed the ability to <b>forget</b> <b>information</b> they do not ...", "dateLastCrawled": "2022-01-27T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Active Forgetting: Adaptation of Memory by Prefrontal</b> Control | Annual ...", "url": "https://www.annualreviews.org/doi/full/10.1146/annurev-psych-072720-094140", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/full/10.1146/annurev-psych-072720-094140", "snippet": "Over the past century, psychologists have discussed whether <b>forgetting</b> might arise from active mechanisms that promote memory loss to achieve various functions, such as minimizing errors, facilitating learning, or regulating one&#39;s emotional state. The past decade has witnessed a great expansion in knowledge about the brain mechanisms underlying active <b>forgetting</b> in its varying forms. A core discovery concerns the role of the prefrontal cortex in exerting top-down control over mnemonic ...", "dateLastCrawled": "2022-02-01T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Learning by Active <b>Forgetting</b> for Neural Networks | DeepAI", "url": "https://deepai.org/publication/learning-by-active-forgetting-for-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/learning-by-active-<b>forgetting</b>-for-neural-networks", "snippet": "<b>Compared</b> with peer algorithms (L1 and L2 regularisation ), both of which <b>can</b> reduce the model complexity to fit the data, the use of the <b>forgetting</b> layer <b>can</b> better handle the mismatch between the network size and the data. Quantitatively, the <b>forgetting</b> layer <b>can</b> substantially increase the generalisability of the model, i.e., a nearly 13% improvement in test accuracy, <b>compared</b> to the vanilla sgd. By comparison, the L1 algorithm has a slight improvement and L2 has a moderate improvement, but ...", "dateLastCrawled": "2021-12-19T21:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Written Memories: Understanding, Deriving and Extending the</b> LSTM - R2RT", "url": "https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html", "isFamilyFriendly": true, "displayUrl": "https://r2rt.com/<b>written-memories-understanding-deriving-and-extending-the</b>-lstm.html", "snippet": "In the real-world, we <b>can</b> only keep so many things in mind at once; in order to make room for new <b>information</b>, we need <b>to selectively</b> <b>forget</b> the least relevant old <b>information</b>. In order for our RNN cells to do this, they need a mechanism for selective <b>forgetting</b>. With that, there are just two more steps to deriving the LSTM: we need to determine a mechanism for selectivity, and; we need to glue the pieces together. Gates as a mechanism for selectivity. Selective reading, writing and ...", "dateLastCrawled": "2022-01-26T10:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Understanding RNNs, LSTMs and GRUs</b> | by Nicholas Asquith | Towards Data ...", "url": "https://towardsdatascience.com/understanding-rnns-lstms-and-grus-ed62eb584d90", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>understanding-rnns-lstms-and-grus</b>-ed62eb584d90", "snippet": "(1) the update <b>gate</b> acts similar to the <b>forget</b> and input <b>gate</b> of an LSTM, it decides what <b>information</b> to keep and which to throw away, and what new <b>information</b> to add. The candidate hidden state is done the same way as the LSTM, the difference is that, inside of the candidate computation, it will reset some of the previous <b>Gate</b> (vectors between 0\u20131, defined by linear transformations).", "dateLastCrawled": "2022-02-03T07:24:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "deep <b>learning</b> - How is the LSTM RNN <b>forget</b> <b>gate</b> calculated? - Data ...", "url": "https://datascience.stackexchange.com/questions/32217/how-is-the-lstm-rnn-forget-gate-calculated", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/32217", "snippet": "You need to look at first term of the next step: C t = f t \u2299 C t \u2212 1 + i t \u2299 C \u00af t. The vector f t that is the output from the <b>forget</b> <b>gate</b>, is used as element-wise multiply against the previous cell state C t \u2212 1. It is this stage where individual elements of C are &quot;remembered&quot; or &quot;forgotten&quot;. Due to the sigmoid function, the vector f ...", "dateLastCrawled": "2022-02-02T10:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> and Theological Traditions of <b>Analogy</b>", "url": "https://www.researchgate.net/publication/349470559_Machine_Learning_and_Theological_Traditions_of_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.research<b>gate</b>.net/publication/349470559_<b>Machine</b>_<b>Learning</b>_and_Theological...", "snippet": "theories of <b>analogy</b> to <b>machine</b> <b>learning</b> has brought us here, since much of it was developed, in the first place, in thinking about the use of shared vocabulary for creature and creator.", "dateLastCrawled": "2021-11-04T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine Learning: Text Generation, A Summary</b> \u2013 Alan&#39;s Blog", "url": "https://achungweb.wordpress.com/2017/04/14/machine-learning-text-generation-a-summary/", "isFamilyFriendly": true, "displayUrl": "https://achungweb.wordpress.com/2017/04/14/<b>machine-learning-text-generation-a-summary</b>", "snippet": "The <b>Forget</b> <b>Gate</b>. The <b>Forget</b> <b>Gate</b> is used to eliminate some of the information that currently exists in the cell state; for example, if we are processing language, we might want to <b>forget</b> the gender of the previous noun, since that information has no benefit to processing the next word.", "dateLastCrawled": "2022-01-20T17:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "9.2. Long Short-Term Memory (<b>LSTM</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/lstm.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/<b>lstm</b>.html", "snippet": "9.2.1.1. Input <b>Gate</b>, <b>Forget</b> <b>Gate</b>, and Output <b>Gate</b>\u00b6. Just like in GRUs, the data feeding into the <b>LSTM</b> gates are the input at the current time step and the hidden state of the previous time step, as illustrated in Fig. 9.2.1.They are processed by three fully-connected layers with a sigmoid activation function to compute the values of the input, <b>forget</b>. and output gates.", "dateLastCrawled": "2022-02-02T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Bird\u2019s-Eye View <b>Of Artificial Intelligence, Machine Learning, Neural</b> ...", "url": "https://medium.com/geekculture/birds-eye-view-of-artificial-intelligence-machine-learning-neural-networks-language-part-2-a53d93495de1", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/birds-eye-view-of-artificial-intelligence-<b>machine</b>...", "snippet": "The <b>analogy</b> is like someone carefully taking ... The gates in the system are the Input <b>Gate</b>, <b>Forget</b> <b>Gate</b> and Output <b>Gate</b>. The part of the cell known as the cell state acts as a memory that keeps ...", "dateLastCrawled": "2021-05-17T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Long Short Term Memory(LSTM) and <b>Gated Recurrent</b> Units(GRU) | by ...", "url": "https://prvnk10.medium.com/long-short-term-memory-lstm-and-gated-recurrent-units-gru-240d8a62db9", "isFamilyFriendly": true, "displayUrl": "https://prvnk10.medium.com/long-short-term-memory-lstm-and-<b>gated-recurrent</b>-units-gru...", "snippet": "<b>Forget</b> <b>Gate</b> (ft) is again some values between 0 to 1. This decides what fraction of s(t-1) to retain in the final computation of st . And this ft is again a standard recipe, its a function of some inputs which happens to be xt and previous intermediate state ( h(t-1) ) in this case and is also a function of some parameter which are Uf , Wf , bf , in this case also we learn these parameters from the data.", "dateLastCrawled": "2022-01-30T07:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "<b>Forget</b> <b>Gate</b> The <b>forget</b> factor; Remember <b>Gate</b> Combines the long term memory from the <b>forget</b> <b>gate</b> and short term memory from the learn <b>gate</b> and SIMPLY ADD THEM; and generate the new long term memory. Use <b>Gate</b> Takes whatever is useful from both long term and short term memories; and generate the new short term memory.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Deep <b>Learning</b> : Intro to LSTM (Long Short Term Memory) | by HIMANSHU ...", "url": "https://medium.com/@himanshunpatel01/deep-learning-intro-to-lstm-long-short-term-memory-ce504dc6e585", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@himanshunpatel01/deep-<b>learning</b>-intro-to-lstm-long-short-term...", "snippet": "A simple <b>machine</b> <b>learning</b> model or an Artificial Neural Network may learn to predict the stock prices based on a number of features: the volume of the stock, the opening value etc. While the price ...", "dateLastCrawled": "2022-01-27T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does Francois Chollet say in his book &#39;Deep <b>Learning</b>&#39; that LSTM ...", "url": "https://www.quora.com/Why-does-Francois-Chollet-say-in-his-book-Deep-Learning-that-LSTM-neural-networks-are-not-helpful-for-sentiment-analysis-problems-I-mean-there-are-plenty-of-studies-about-sentiment-analysis-and-LSTM-models", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-does-Francois-Chollet-say-in-his-book-Deep-<b>Learning</b>-that...", "snippet": "Answer (1 of 2): I have not read the book yet (I got a copy from Manning recently) so I cannot say that the line (or any paraphrase of it) appears in the book or not. A blanket statement like \u201cX don\u2019t work for Y\u201d is totally wrong in <b>Machine</b> <b>Learning</b> (a lot of papers just get written modifying met...", "dateLastCrawled": "2022-01-14T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>What is an intuitive explanation of working of</b> LSTM? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-of-working-of-LSTM", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-an-intuitive-explanation-of-working-of</b>-LSTM", "snippet": "Answer (1 of 4): LSTM can be thought as a person who has very good memory but don\u2019t care the insignificant details in the past if they are not useful at the future ...", "dateLastCrawled": "2022-01-13T08:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Frontiers | <b>Machine</b> <b>Learning</b> and Metaheuristic Methods for Renewable ...", "url": "https://www.frontiersin.org/articles/10.3389/fceng.2021.665415/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fceng.2021.665415", "snippet": "The global trend toward a green sustainable future encouraged the penetration of renewable energies into the electricity sector to satisfy various demands of the market. Successful and steady integrations of renewables into the microgrids necessitate building reliable, accurate wind and solar power forecasters adopting these renewables&#39; stochastic behaviors. In a few reported literature studies, <b>machine</b> <b>learning</b>- (ML-) based forecasters have been widely utilized for wind power and solar ...", "dateLastCrawled": "2021-12-11T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> and Metaheuristic Methods for Renewable Power ...", "url": "https://www.researchgate.net/publication/351110482_Machine_Learning_and_Metaheuristic_Methods_for_Renewable_Power_Forecasting_A_Recent_Review", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/351110482_<b>Machine</b>_<b>Learning</b>_and_Metaheuristic...", "snippet": "I. ABSTRACT-<b>Machine</b> <b>learning</b> (ML) is increasingly touching all fields in our life and making a huge impact on their improvement. In this paper, we present an overview of several used methods in ML ...", "dateLastCrawled": "2022-01-07T01:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Artificial intelligence-based approach for atrial fibrillation ...", "url": "https://www.sciencedirect.com/science/article/pii/S1746809421008673", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1746809421008673", "snippet": "The <b>machine</b> <b>learning</b>\u2013based methods are limited in their ability to process the data in raw form as those need feature extractor for that purpose, which can be applied to the classifier for the detection and classification of the input pattern . A deep <b>learning</b> approach overcomes this problem since it allows the model to be fed with the raw data for detection and classification.", "dateLastCrawled": "2021-12-06T14:32:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(forget gate)  is like +(selectively forgetting information)", "+(forget gate) is similar to +(selectively forgetting information)", "+(forget gate) can be thought of as +(selectively forgetting information)", "+(forget gate) can be compared to +(selectively forgetting information)", "machine learning +(forget gate AND analogy)", "machine learning +(\"forget gate is like\")", "machine learning +(\"forget gate is similar\")", "machine learning +(\"just as forget gate\")", "machine learning +(\"forget gate can be thought of as\")", "machine learning +(\"forget gate can be compared to\")"]}