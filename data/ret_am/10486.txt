{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Transfer learning from <b>pre-trained</b> models | by Pedro Marcelino ...", "url": "https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/transfer-learning-from-<b>pre-trained</b>-<b>models</b>-f2393f124751", "snippet": "By contrast, if the dataset is large and the number of parameters is small, you can improve your <b>model</b> by training more layers to the new <b>task</b> since overfitting is not an issue. Freeze the convolutional base. This case corresponds to an extreme situation of the train/freeze trade-off. The main idea is to keep the convolutional base in its original form and then use its outputs to feed the classifier. You\u2019re using the <b>pre-trained</b> <b>model</b> as a fixed feature extraction mechanism, which can be ...", "dateLastCrawled": "2022-02-02T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "4 <b>Pre-Trained</b> CNN Models to Use for Computer Vision with Transfer ...", "url": "https://towardsdatascience.com/4-pre-trained-cnn-models-to-use-for-computer-vision-with-transfer-learning-885cb1b2dfc", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/4-<b>pre-trained</b>-cnn-<b>models</b>-to-use-for-computer-vision...", "snippet": "Development of an Open Source <b>Pre-trained</b> <b>Model</b> by a Third Party; Repurposing the <b>Model</b>; Fine Tuning for the Problem; Development of an Open Source <b>Pre-trained</b> <b>Model</b>. A <b>pre-trained</b> <b>model</b> is a <b>model</b> created and <b>trained</b> by someone else to solve a problem that is similar to ours. In practice, someone is almost always a tech giant or a group of ...", "dateLastCrawled": "2022-02-03T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Pre-Trained</b> Models for NLP Tasks Using PyTorch \u00b7 IndoTutorial", "url": "https://indobenchmark.github.io/tutorials/pytorch/deep%20learning/nlp/2020/10/18/basic-pytorch-en", "isFamilyFriendly": true, "displayUrl": "https://indobenchmark.github.io/tutorials/pytorch/deep learning/nlp/2020/10/18/basic...", "snippet": "BERT is a very popular <b>pre-trained</b> contextualized language <b>model</b> that stands for Bidirectional Encoder Representations from Transformers. Just <b>like</b> what it says in its name, BERT makes use of transformers, the attention mechanism that takes contextual relations between words in a text into account. Before BERT, the most popular techniques were recurrent based models, which read the text input sequentially. What makes BERT better is that it removes the first order markov assumption and ...", "dateLastCrawled": "2022-01-29T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is meant be <b>pre-trained</b> <b>model</b> in CNN? Are they <b>already</b> <b>trained</b> on ...", "url": "https://www.quora.com/What-is-meant-be-pre-trained-model-in-CNN-Are-they-already-trained-on-that-particular-classes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-meant-be-<b>pre-trained</b>-<b>model</b>-in-CNN-Are-they-<b>already</b>...", "snippet": "Answer (1 of 3): A CNN that has been <b>trained</b> on a related large scale problem such as ImageNet can be used in other visual recognition tasks without the need to train the first few layers. Those fixed layers are fixed feature detectors. The upper layers can be fine tuned to match the current pro...", "dateLastCrawled": "2022-01-28T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Keras Tutorial : Fine-tuning</b> <b>pre-trained</b> models | LearnOpenCV", "url": "https://learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/", "isFamilyFriendly": true, "displayUrl": "https://www.learnopencv.com/keras-tutorial-fine", "snippet": "The <b>task</b> of fine-tuning a network is to tweak the parameters of an <b>already</b> <b>trained</b> network so that it adapts to the new <b>task</b> at hand. As explained here, the initial layers learn very general features and as we go higher up the network, the layers tend to learn patterns more <b>specific</b> to the <b>task</b> it is being <b>trained</b> on. Thus, for fine-tuning, we ...", "dateLastCrawled": "2022-01-30T10:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fast-Tracking <b>Hand Gesture Recognition AI Applications with Pretrained</b> ...", "url": "https://developer.nvidia.com/blog/fast-tracking-hand-gesture-recognition-ai-applications-with-pretrained-models-from-ngc/", "isFamilyFriendly": true, "displayUrl": "https://developer.nvidia.com/blog/fast-tracking-hand-gesture-recognition-ai...", "snippet": "A <b>pretrained</b> <b>model</b>, as the name suggests, is a <b>model</b> that has been previously <b>trained</b> on a particular representative dataset. It contains the weights and biases fine-tuned for this representation. To accelerate development, you can initialize your own models with <b>pretrained</b> ones. This typically helps you save time and allows you to run more iterations to refine the <b>model</b>. The technique is transfer learning.", "dateLastCrawled": "2022-02-01T23:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Pretrained</b> Deep Neural Networks - MATLAB &amp; Simulink", "url": "https://www.mathworks.com/help/deeplearning/ug/pretrained-convolutional-neural-networks.html", "isFamilyFriendly": true, "displayUrl": "https://www.mathworks.com/help/deeplearning/ug/<b>pretrained</b>-convolutional-neural...", "snippet": "You can use these activations as features to train another <b>machine</b> learning <b>model</b>, such as a support vector <b>machine</b> (SVM). For more information, see Feature Extraction. For an example, see Extract Image Features Using <b>Pretrained</b> Network. Transfer Learning: Take layers from a network <b>trained</b> on a large data set and fine-tune on a new data set. For more information, see Transfer Learning. For a simple example, see Get Started with Transfer Learning. To try more <b>pretrained</b> networks, see Train ...", "dateLastCrawled": "2022-01-30T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "python - <b>Pre-Trained models for text Classification</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/65262832/pre-trained-models-for-text-classification", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/65262832/<b>pre-trained-models-for-text-classification</b>", "snippet": "The <b>task</b> we are taking about is called Zero-Shot Topic Classification - predicting a topic that the <b>model</b> has not been <b>trained</b> on. This paradigm is supported by Hugging Face library, you can read more here.The most common <b>pre-trained</b> <b>model</b> is Bart Large MNLI - the checkpoint for bart-large after being <b>trained</b> on the MNLI dataset.Here is a simple example, showing the classification of phrase &quot;I <b>like</b> hot dogs&quot; without any preliminary training:", "dateLastCrawled": "2022-01-20T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "keras - How to train Tensorflow&#39;s <b>pre trained</b> BERT on MLM <b>task</b>? ( Use ...", "url": "https://stackoverflow.com/questions/70830464/how-to-train-tensorflows-pre-trained-bert-on-mlm-task-use-pre-trained-model", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/70830464/how-to-train-tensorflows-<b>pre-trained</b>-bert...", "snippet": "Before Anyone suggests pytorch and other things, I am looking specifically for Tensorflow + <b>pretrained</b> + MLM <b>task</b> only. I know, there are lots of blogs for PyTorch and lots of blogs for fine tuning ( Classification) on Tensorflow.. Coming to the problem, I got a language <b>model</b> which is English + LaTex where a text data can represent any text from Physics, Chemistry, MAths and Biology and any typical example can look something <b>like</b> this: Link to OCR image &quot;Find the value of function x in the ...", "dateLastCrawled": "2022-01-28T02:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What <b>are pre-trained word embeddings in NLP</b>? - Quora", "url": "https://www.quora.com/What-are-pre-trained-word-embeddings-in-NLP", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>are-pre-trained-word-embeddings-in-NLP</b>", "snippet": "Answer (1 of 10): To appreciate word embeddings, think first about how we usually represent words - lexically. As a Quora user, you are of course a very literate person, but consider for a moment some unfortunate aspects of the written word: * Words that are similar semantically - dog, hound, m...", "dateLastCrawled": "2022-01-21T14:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Transfer learning from <b>pre-trained</b> models | by Pedro Marcelino ...", "url": "https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/transfer-learning-from-<b>pre-trained</b>-<b>models</b>-f2393f124751", "snippet": "A <b>pre-trained</b> <b>model</b> is a <b>model</b> that was <b>trained</b> on a large benchmark dataset to solve a problem <b>similar</b> to the one that we want to solve. Accordingly, due to the computational cost of training such models, it is common practice to import and use models from published literature (e.g. VGG , Inception , MobileNet ).", "dateLastCrawled": "2022-02-02T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "4 <b>Pre-Trained</b> CNN Models to Use for Computer Vision with Transfer ...", "url": "https://towardsdatascience.com/4-pre-trained-cnn-models-to-use-for-computer-vision-with-transfer-learning-885cb1b2dfc", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/4-<b>pre-trained</b>-cnn-<b>models</b>-to-use-for-computer-vision...", "snippet": "Development of an Open Source <b>Pre-trained</b> <b>Model</b> by a Third Party; Repurposing the <b>Model</b>; Fine Tuning for the Problem; Development of an Open Source <b>Pre-trained</b> <b>Model</b>. A <b>pre-trained</b> <b>model</b> is a <b>model</b> created and <b>trained</b> by someone else to solve a problem that <b>is similar</b> to ours. In practice, someone is almost always a tech giant or a group of ...", "dateLastCrawled": "2022-02-03T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Transfer Learning</b> From <b>Pre-Trained</b> <b>Model</b> for Image Recognition | by ...", "url": "https://sagarsonwane230797.medium.com/transfer-learning-from-pre-trained-model-for-image-facial-recognition-8b0c2038d5f0", "isFamilyFriendly": true, "displayUrl": "https://sagarsonwane230797.medium.com/<b>transfer-learning</b>-from-<b>pre-trained</b>-<b>model</b>-for...", "snippet": "Image Classification does the <b>task</b> of classifying a <b>specific</b> image to a set of possible categories and Image Recognition refers to the ability of software to identify objects, places, people, writing and actions in images. One of the example of Image Classification is Identification of Cars and Bikes. Since there is so much work that has <b>already</b> been done on Image Recognition and Classification, we can use a technique in <b>Machine</b> Learning to solve Image Classification problems which can ...", "dateLastCrawled": "2022-02-03T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Pre-Trained</b> Models for NLP Tasks Using PyTorch \u00b7 IndoTutorial", "url": "https://indobenchmark.github.io/tutorials/pytorch/deep%20learning/nlp/2020/10/18/basic-pytorch-en", "isFamilyFriendly": true, "displayUrl": "https://indobenchmark.github.io/tutorials/pytorch/deep learning/nlp/2020/10/18/basic...", "snippet": "BERT is a very popular <b>pre-trained</b> contextualized language <b>model</b> that stands for Bidirectional Encoder Representations from Transformers. Just like what it says in its name, BERT makes use of transformers, the attention mechanism that takes contextual relations between words in a text into account. Before BERT, the most popular techniques were recurrent based models, which read the text input sequentially. What makes BERT better is that it removes the first order markov assumption and ...", "dateLastCrawled": "2022-01-29T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is meant be <b>pre-trained</b> <b>model</b> in CNN? Are they <b>already</b> <b>trained</b> on ...", "url": "https://www.quora.com/What-is-meant-be-pre-trained-model-in-CNN-Are-they-already-trained-on-that-particular-classes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-meant-be-<b>pre-trained</b>-<b>model</b>-in-CNN-Are-they-<b>already</b>...", "snippet": "Answer (1 of 3): A CNN that has been <b>trained</b> on a related large scale problem such as ImageNet can be used in other visual recognition tasks without the need to train the first few layers. Those fixed layers are fixed feature detectors. The upper layers can be fine tuned to match the current pro...", "dateLastCrawled": "2022-01-28T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Reusing a <b>pre-trained</b> Deep Learning <b>model</b> on a new <b>task</b> - Medium", "url": "https://medium.com/analytics-vidhya/reusing-a-pre-trained-deep-learning-model-on-a-new-task-transfer-learning-1c0a25a92dfb", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/reusing-a-<b>pre-trained</b>-deep-learning-<b>model</b>-on-a-new...", "snippet": "For years, NLP domain lacked an established referential dataset and source <b>task</b> for learning generalizable base models, thus the community of \u2018fine-tuning <b>pre-trained</b>\u2019 models wasn\u2019t that big.", "dateLastCrawled": "2022-01-30T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Hugging Face <b>Pre-trained</b> Models: Find the Best One for Your <b>Task</b> ...", "url": "https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/hugging-face-<b>pre-trained</b>-<b>models</b>-find-the-best", "snippet": "When you are working on a <b>Machine</b> learning problem, adapting an existing solution and repurposing it can help you get to a solution much faster. Using existing models, not just aid <b>machine</b> learning engineers or data scientists but also helps companies to save computational costs as it requires less training. There are many companies that provide open source libraries containing <b>pre-trained</b> models and Hugging Face is one of them. Hugging Face first launched its chat platform back in 2017. To ...", "dateLastCrawled": "2022-02-02T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Pretrained</b> Deep Neural Networks - MATLAB &amp; Simulink", "url": "https://www.mathworks.com/help/deeplearning/ug/pretrained-convolutional-neural-networks.html", "isFamilyFriendly": true, "displayUrl": "https://www.mathworks.com/help/deeplearning/ug/<b>pretrained</b>-convolutional-neural...", "snippet": "You can use these activations as features to train another <b>machine</b> learning <b>model</b>, such as a support vector <b>machine</b> (SVM). For more information, see Feature Extraction. For an example, see Extract Image Features Using <b>Pretrained</b> Network. Transfer Learning: Take layers from a network <b>trained</b> on a large data set and fine-tune on a new data set. For more information, see Transfer Learning. For a simple example, see Get Started with Transfer Learning. To try more <b>pretrained</b> networks, see Train ...", "dateLastCrawled": "2022-01-30T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Blog #2 \u2013 Deep analysis on BERT <b>pre-trained</b> <b>model</b> and its potential ...", "url": "https://hkunlp.wordpress.com/2021/03/11/2-blog-deep-analysis-on-bert-pre-trained-model-and-its-potential-defects-in-natural-language-processing-solution-to-the-technical-problems/", "isFamilyFriendly": true, "displayUrl": "https://hkunlp.wordpress.com/2021/03/11/2-blog-deep-analysis-on-bert-<b>pre-trained</b>-<b>model</b>...", "snippet": "<b>Pre-trained</b> language <b>model</b> has formed a new NLP paradigm: using large-scale text corpora for pre-training, fine-tuning small data sets for <b>specific</b> tasks, and reducing the difficulty of a single NLP <b>task</b>. The emergence of \u2018BERT\u2019 <b>model</b> completely changed the relationship between word vector generated by pre-training and <b>specific</b> NLP tasks in the downstream, and proposed the concept of keel-level training word vector.", "dateLastCrawled": "2022-01-12T08:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What <b>are pre-trained word embeddings in NLP</b>? - Quora", "url": "https://www.quora.com/What-are-pre-trained-word-embeddings-in-NLP", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>are-pre-trained-word-embeddings-in-NLP</b>", "snippet": "Answer (1 of 10): To appreciate word embeddings, think first about how we usually represent words - lexically. As a Quora user, you are of course a very literate person, but consider for a moment some unfortunate aspects of the written word: * Words that are <b>similar</b> semantically - dog, hound, m...", "dateLastCrawled": "2022-01-21T14:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Blog #2 \u2013 Deep analysis on BERT <b>pre-trained</b> <b>model</b> and its potential ...", "url": "https://hkunlp.wordpress.com/2021/03/11/2-blog-deep-analysis-on-bert-pre-trained-model-and-its-potential-defects-in-natural-language-processing-solution-to-the-technical-problems/", "isFamilyFriendly": true, "displayUrl": "https://hkunlp.wordpress.com/2021/03/11/2-blog-deep-analysis-on-bert-<b>pre-trained</b>-<b>model</b>...", "snippet": "<b>Pre-trained</b> language <b>model</b> has formed a new NLP paradigm: using large-scale text corpora for pre-training, fine-tuning small data sets for <b>specific</b> tasks, and reducing the difficulty of a single NLP <b>task</b>. The emergence of \u2018BERT\u2019 <b>model</b> completely changed the relationship between word vector generated by pre-training and <b>specific</b> NLP tasks in the downstream, and proposed the concept of keel-level training word vector.", "dateLastCrawled": "2022-01-12T08:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "All About <b>Pretrained</b> Models - MathWorks", "url": "https://explore.mathworks.com/all-about-pretrained-models", "isFamilyFriendly": true, "displayUrl": "https://explore.mathworks.com/all-about-<b>pretrained</b>-<b>models</b>", "snippet": "Once you have a handle on how you want to train your <b>model</b>, you <b>can</b> move to the next section to see if you <b>can</b> improve your results. Start here: GoogLeNet, VGG-16, VGG-19, and AlexNet . Higher accuracy models. These models cover your image-based workflows, such as image classification, object detection, and semantic segmentation. Most networks, including the basic models above, fall into this category. The difference between the Basic and the Higher Accuracy Models is these models may ...", "dateLastCrawled": "2022-02-02T10:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deploying a pretrained GPT-2 model on</b> AWS - KDnuggets", "url": "https://www.kdnuggets.com/2019/12/deploying-pretrained-gpt-2-model-aws.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2019/12/deploying-<b>pretrained</b>-gpt-2-<b>model</b>-aws.html", "snippet": "Why <b>can</b>\u2019t we get rid of EC2 and DynamoDB, and execute the whole thing in Lambda alone? The problem is that we don\u2019t have enough ephemeral disk space on Lambda to store the <b>pre-trained</b> LM weights. Remember, we are taking advantage of a <b>pre-trained</b> network. This means we have to store its parameters somewhere and load them into the <b>model</b> to ...", "dateLastCrawled": "2022-01-29T06:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Image Classification using Pre-trained Models</b> in PyTorch | LearnOpenCV", "url": "https://learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/", "isFamilyFriendly": true, "displayUrl": "https://learnopencv.com/pytorch-for-beginners-<b>image-classification-using-pre-trained</b>...", "snippet": "1.2. Loading <b>Pre-Trained</b> Network using TorchVision. Now that we are equipped with the knowledge of <b>model</b> inference and know what a <b>pre-trained</b> <b>model</b> means, let\u2019s see how we <b>can</b> use them with the help of TorchVision module. First, let\u2019s install the TorchVision module using the command given below.", "dateLastCrawled": "2022-02-02T17:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What are the <b>pre-trained</b> word embeddings that are good for sentiment ...", "url": "https://www.quora.com/What-are-the-pre-trained-word-embeddings-that-are-good-for-sentiment-analysis-as-most-popular-once-like-glove-and-word2vev-model-the-syntactic-and-semantic-nature-of-word-but-ignore-the-sentiment-of-words", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-the-<b>pre-trained</b>-word-embeddings-that-are-good-for...", "snippet": "Answer: Well, generally, for sentiment analysis, you\u2019d be matching words to a dictionary (not embedding them). This generally happens after cleaning the text. You could create something custom with the sentiment scoring of each word and some dimensionality reduction technique, though. This might ...", "dateLastCrawled": "2022-01-20T23:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Transfer Learning</b> in NLP. Using <b>already</b> <b>trained</b> robust models to\u2026 | by ...", "url": "https://towardsdatascience.com/transfer-learning-in-nlp-fecc59f546e4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transfer-learning</b>-in-nlp-fecc59f546e4", "snippet": "But considering most of the <b>machine</b> learning tasks are domain <b>specific</b>, the <b>trained</b> models usually fail to generalize the conditions that it has never seen before. The real world is not like the <b>trained</b> data set, it contains lot of messy data and the <b>model</b> will make a ill prediction in such condition. The ability to transfer the knowledge of a <b>pre-trained</b> <b>model</b> into a new condition is generally referred to as <b>transfer learning</b>. <b>T r ansfer learning</b> has been heavily used in computer vision ...", "dateLastCrawled": "2022-02-02T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> learning - Should a <b>model</b> be re-<b>trained</b> if new observations are ...", "url": "https://datascience.stackexchange.com/questions/12761/should-a-model-be-re-trained-if-new-observations-are-available", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/12761", "snippet": "Once a <b>model</b> is <b>trained</b> and you get new data which <b>can</b> be used for training, you <b>can</b> load the previous <b>model</b> and train onto it. For example, you <b>can</b> save your <b>model</b> as a .pickle file and load it and train further onto it when new data is available. Do note that for the <b>model</b> to predict correctly, the new training data should have a similar distribution as the past data. Predictions tend to degrade based on the dataset you are using.", "dateLastCrawled": "2022-01-25T11:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Best <b>pre trained</b> <b>model</b> for object detection \u2014 best <b>pre-trained</b> models ...", "url": "https://nasta-boden.com/exploring-opencvs-deep-learning-object-detection-library-e51fe7c82246osv-18621kobkzc", "isFamilyFriendly": true, "displayUrl": "https://nasta-boden.com/exploring-opencvs-deep-learning-object-detection-library-e51fe...", "snippet": "After installation, we <b>can</b> use a <b>pre-trained</b> <b>model</b> or build a new one from scratch. For example here&#39;s how you <b>can</b> detect objects on your image using <b>model</b> <b>pre-trained</b> on COCO dataset : ./darknet detect cfg/yolov3.cfg yolov3.weights data/my_image.jp In this blog, I will be training a <b>machine</b> learning <b>model</b> for custom object detection using TensorFlow 1.x in Google Colab. Following is the roadmap for it. Roadmap. Collect the dataset of images and label them to get their XML files Our python ...", "dateLastCrawled": "2022-01-14T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is <b>GPT-3 really the future of NLP</b>? - Endila", "url": "https://www.endila.com/post/is-gpt-3-really-the-future-of-nlp", "isFamilyFriendly": true, "displayUrl": "https://www.endila.com/post/is-<b>gpt-3-really-the-future-of-nlp</b>", "snippet": "Similar models to GPT-3 are usually <b>trained</b> on a large corpus of text and are then fine-tuned to perform a <b>specific</b> <b>task</b> (say, <b>machine</b> translation) and only that <b>task</b>. Taking <b>pre-trained</b> models and fine-tuning them to solve <b>specific</b> problems has become a popular and successful trend in the field of NLP. The method is helping developers to shortcut <b>model</b> development and gain benefits more quickly and in a more cost-efficient manner. GPT-3, by contrast, goes one step further and does not ...", "dateLastCrawled": "2022-01-31T03:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Classifying <b>Hate Speech</b>: an overview | by Jacob Crabb | Towards Data ...", "url": "https://towardsdatascience.com/classifying-hate-speech-an-overview-d307356b9eba", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/classifying-<b>hate-speech</b>-an-overview-d307356b9eba", "snippet": "A <b>model</b> is \u201cthe output of the algorithm that <b>trained</b> with data\u201d . <b>Machine</b> learning algorithms, or models, <b>can</b> classify text for us but are sensitive to small changes, like removing the spaces between words that are hateful. This change <b>can</b> drastically reduce the negative score a sentence receives . Learning models <b>can</b> be fooled into labeling their inputs incorrectly. A crucial challenge for <b>machine</b> learning algorithms is understanding the context. The insidious nature of <b>hate speech</b> is ...", "dateLastCrawled": "2022-02-03T02:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Pre-Trained</b> <b>Machine</b> Learning Models vs Models <b>Trained</b> from Scratch | by ...", "url": "https://heartbeat.comet.ml/pre-trained-machine-learning-models-vs-models-trained-from-scratch-63e079ed648f", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/<b>pre-trained</b>-<b>machine</b>-learning-<b>models</b>-vs-<b>models</b>-<b>trained</b>-from...", "snippet": "The fine-tuned models get kind of a head start, as the <b>pre-trained</b> <b>model</b> has <b>already</b> learned high-level features. That means the models <b>trained</b> from scratch cannot converge as fast as the fine-tuned models. Though this makes fine-tuned models better, one should also consider the time and resources it takes to pre-train a <b>model</b> on large benchmark datasets like ImageNet. Over a million images are <b>trained</b> for many iterations during ImageNet pre-training. So for the random initialized training ...", "dateLastCrawled": "2022-01-26T02:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Acquiring Knowledge from <b>Pre-trained</b> <b>Model</b> to Neural <b>Machine</b> ...", "url": "https://deepai.org/publication/acquiring-knowledge-from-pre-trained-model-to-neural-machine-translation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/acquiring-knowledge-from-<b>pre-trained</b>-<b>model</b>-to-neural...", "snippet": "Acquiring Knowledge from <b>Pre-trained</b> <b>Model</b> to Neural <b>Machine</b> Translation. 12/04/2019 \u2219 by Rongxiang Weng, et al. \u2219 Nanjing University \u2219 0 \u2219 share . Pre-training and fine-tuning have achieved great success in the natural language process field. The standard paradigm of exploiting them includes two steps: first, pre-training a <b>model</b>, e.g. BERT, with a large scale unlabeled monolingual data.", "dateLastCrawled": "2021-12-15T08:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Transfer Learning</b> From <b>Pre-Trained</b> <b>Model</b> for Image Recognition | by ...", "url": "https://sagarsonwane230797.medium.com/transfer-learning-from-pre-trained-model-for-image-facial-recognition-8b0c2038d5f0", "isFamilyFriendly": true, "displayUrl": "https://sagarsonwane230797.medium.com/<b>transfer-learning</b>-from-<b>pre-trained</b>-<b>model</b>-for...", "snippet": "Image Classification does the <b>task</b> of classifying a <b>specific</b> image to a set of possible categories and Image Recognition refers to the ability of software to identify objects, places, people, writing and actions in images. One of the example of Image Classification is Identification of Cars and Bikes. Since there is so much work that has <b>already</b> been done on Image Recognition and Classification, we <b>can</b> use a technique in <b>Machine</b> Learning to solve Image Classification problems which <b>can</b> ...", "dateLastCrawled": "2022-02-03T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Hugging Face <b>Pre-trained</b> Models: Find the Best One for Your <b>Task</b> ...", "url": "https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/hugging-face-<b>pre-trained</b>-<b>models</b>-find-the-best", "snippet": "There is a lot more we <b>can</b> do and look at rather than just comparing these two metrics, but for the purpose of this article, looking at these metrics and the translation results \u2013 we <b>can</b> conclude that MarianMT and mBART <b>pre-trained</b> <b>model</b> and fine-tuned <b>model</b> performed better than T5. mBART performed slightly better than MarianMT as it was able to recognize more words in the input text and might be able to perform better with more training.", "dateLastCrawled": "2022-02-02T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Why use a <b>pre-trained</b> <b>model</b> rather than creating your own? | by Florin ...", "url": "https://medium.com/udacity-pytorch-challengers/why-use-a-pre-trained-model-rather-than-creating-your-own-d0e3a17e202f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-pytorch-challengers/why-use-a-<b>pre-trained</b>-<b>model</b>-rather-than...", "snippet": "A <b>pre-trained</b> <b>model</b> represents a <b>model</b> that was <b>trained</b> for a certain <b>task</b> on the ImageNet data set . In PyTorch\u2019s case there are several very popular <b>model</b> architectures that are available to ...", "dateLastCrawled": "2022-02-02T18:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Pre-trained</b> <b>model</b> - Stack Exchange", "url": "https://datascience.stackexchange.com/questions/48127/what-is-the-difference-between-offline-trained-model-and-pretrained-model", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/48127/what-is-the-difference-between...", "snippet": "Every <b>pre-trained</b> <b>model</b> is an offline-<b>trained</b> <b>model</b>, but not the reverse. Offline training is any training that leaves the <b>model</b> unchanged when new observations arrive, i.e. it has an end. Online training constantly updates the <b>model</b> with the help of new, incoming observations without using the previous training points, although, having a limited memory of previous samples <b>compared</b> to all seen samples is OK. Therefore, training offline periodically on all the training points, no matter how ...", "dateLastCrawled": "2022-01-22T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What <b>are advantages and disadvantages between pre-trained</b> and <b>trained</b> ...", "url": "https://www.quora.com/What-are-advantages-and-disadvantages-between-pre-trained-and-trained-models", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>are-advantages-and-disadvantages-between-pre-trained</b>-and...", "snippet": "Answer (1 of 2): <b>Trained</b> and <b>pre-trained</b> is usually the same thing. It\u2019s just a <b>model</b> that was <b>already</b> <b>trained</b> and has calculated weights with it. Sometimes people also share weights of particular layers, usually first ones, as they generalize better. Main advantage - you don\u2019t need to train it ...", "dateLastCrawled": "2022-01-26T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Pretrained</b> Deep Neural Networks - MATLAB &amp; Simulink", "url": "https://www.mathworks.com/help/deeplearning/ug/pretrained-convolutional-neural-networks.html", "isFamilyFriendly": true, "displayUrl": "https://www.mathworks.com/help/deeplearning/ug/<b>pretrained</b>-convolutional-neural...", "snippet": "You <b>can</b> take a <b>pretrained</b> image classification network that has <b>already</b> learned to extract powerful and informative features from natural images and use it as a starting point to learn a new <b>task</b>. The majority of the <b>pretrained</b> networks are <b>trained</b> on a subset of the ImageNet database , which is used in the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) . These networks have been <b>trained</b> on more than a million images and <b>can</b> classify images into 1000 object categories, such as ...", "dateLastCrawled": "2022-01-30T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python - <b>Pre-Trained models for text Classification</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/65262832/pre-trained-models-for-text-classification", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/65262832/<b>pre-trained-models-for-text-classification</b>", "snippet": "The <b>task</b> we are taking about is called Zero-Shot Topic Classification - predicting a topic that the <b>model</b> has not been <b>trained</b> on. This paradigm is supported by Hugging Face library, you <b>can</b> read more here.The most common <b>pre-trained</b> <b>model</b> is Bart Large MNLI - the checkpoint for bart-large after being <b>trained</b> on the MNLI dataset.Here is a simple example, showing the classification of phrase &quot;I like hot dogs&quot; without any preliminary training:", "dateLastCrawled": "2022-01-20T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> learning - Should a <b>model</b> be re-<b>trained</b> if new observations are ...", "url": "https://datascience.stackexchange.com/questions/12761/should-a-model-be-re-trained-if-new-observations-are-available", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/12761", "snippet": "Once a <b>model</b> is <b>trained</b> and you get new data which <b>can</b> be used for training, you <b>can</b> load the previous <b>model</b> and train onto it. For example, you <b>can</b> save your <b>model</b> as a .pickle file and load it and train further onto it when new data is available. Do note that for the <b>model</b> to predict correctly, the new training data should have a similar distribution as the past data. Predictions tend to degrade based on the dataset you are using.", "dateLastCrawled": "2022-01-25T11:13:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Pre-trained</b> Models - Value <b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> Technology", "url": "https://valueml.com/transfer-learning-approach-pre-trained-models-classifying-imagenet-classes-with-resnet50-in-python/", "isFamilyFriendly": true, "displayUrl": "https://valueml.com/transfer-<b>learning</b>-<b>approach-pre-trained-models-classifying</b>-imagenet...", "snippet": "Transfer <b>Learning</b> enables us to use the <b>pre-trained</b> models from other people by making small relevant changes. Basically, Transfer <b>Learning</b> (TL) is a <b>Machine</b> <b>Learning</b> technique that trains a new <b>model</b> for a particular problem based on the knowledge gained by solving some other problem. For example, the knowledge gained while <b>learning</b> to recognize trucks could be applied to recognize cars.", "dateLastCrawled": "2022-01-21T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Finding the Word <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-word-<b>analogy</b>-from-given-words-using-word2vec...", "snippet": "In the word <b>analogy</b> task, we complete the sentence ... What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the word <b>analogy</b>. In this tutorial, we will be using Word2Vec <b>model</b> and a <b>pre-trained</b> <b>model</b> named \u2018GoogleNews-vectors-negative300.bin\u2018 which is trained on over 50 Billion words by Google. Each word inside the <b>pre-trained</b> dataset is embedded in a 300-dimensional space and the words which are similar in context/meaning are placed closer to each other in ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Should I Learn Machine Learning</b>? | GenUI", "url": "https://www.genui.com/resources/ml-for-developers", "isFamilyFriendly": true, "displayUrl": "https://www.genui.com/resources/ml-for-developers", "snippet": "But it will almost always be best to start with a <b>pre-trained</b> <b>model</b>, from a more general dataset, and then fine-tune it to fit your specific domain. For example, most image recognition models are based on <b>pre-trained</b> models from ImageNet, a dataset of more than 14 million, hand-labeled images divided into over 20,000 classes (like \u201cbicycle\u201d, \u201cstrawberry\u201d, \u201csky\u201d).", "dateLastCrawled": "2022-01-30T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Word2Vec in Gensim Explained for Creating Word Embedding Models ...", "url": "https://machinelearningknowledge.ai/word2vec-in-gensim-explained-for-creating-word-embedding-models-pretrained-and-custom/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>knowledge.ai/word2vec-in-gensim-explained-for-creating-word...", "snippet": "<b>Machine</b> <b>learning</b> and ... This is another way putting that word2vec can draw the <b>analogy</b> that if Man is to Woman then Kind is to Queen! The publicly released <b>model</b> of word2vec by Google consists of 300 features and the <b>model</b> is trained in the Google news dataset. The vocabulary size of the <b>model</b> is around 1.6 billion words. However, this might have taken a huge time for the <b>model</b> to be trained on but they have applied a method of simple subsampling approach to optimize the time. Word2Vec ...", "dateLastCrawled": "2022-02-02T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Transfer <b>Learning</b>: The Highest Leverage Deep <b>Learning</b> Skill You Can Learn", "url": "https://www.the-analytics.club/transfer-learning", "isFamilyFriendly": true, "displayUrl": "https://www.the-analytics.club/transfer-<b>learning</b>", "snippet": "Transfer <b>learning</b> is a <b>machine</b> <b>learning</b> technique in which a <b>model</b> trained on a specific task is reused as part of the training process for another, different task. Here is a simple <b>analogy</b> to help you understand how transfer <b>learning</b> works: imagine that one person has learned everything there is to know about dogs. In contrast, another person has learned everything about cats. If both people are asked, \u201cWhat\u2019s an animal with four legs, a tail, and barks?\u201d The person who knows all ...", "dateLastCrawled": "2022-01-29T09:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Detailed Guide To Transfer <b>Learning</b> and How It Works | Domino", "url": "https://blog.dominodatalab.com/guide-to-transfer-learning-for-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://blog.dominodatalab.com/guide-to-transfer-<b>learning</b>-for-deep-<b>learning</b>", "snippet": "Transfer <b>learning</b> is a <b>machine</b> <b>learning</b> technique that reuses a completed <b>model</b> that was developed for one task as the starting point for a new <b>model</b> to accomplish a new task. The knowledge used by the first <b>model</b> is thus transferred to the second <b>model</b>. The phrase \u201ctransfer <b>learning</b>\u201d comes from human psychology, wherein a person who knows ...", "dateLastCrawled": "2022-01-27T04:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Using Deep <b>Learning</b> for Image Analogies | by Tomer Amit | Towards Data ...", "url": "https://towardsdatascience.com/using-deep-learning-for-image-analogies-aa2e7d7af337", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-deep-<b>learning</b>-for-image-analogies-aa2e7d7af337", "snippet": "I will use the <b>pre-trained</b> VGG16 image classification <b>model</b>. The <b>model</b> consists of CNN layers stacked one after another, connected by max pooling layers. The input of the network is a 244\u00d7244\u00d73 image (i.e image width and length are 244 pixels, and 3 channels), and after applying all the convolutional layers, we get a 7\u00d77\u00d7512 array. (diagram taken from deeplearning.ai course by Andrew Ng, \u201cConvolutional Neural Networks\u201d) At the end of the network we have an additional flattening layer ...", "dateLastCrawled": "2022-01-19T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Transfer <b>Learning to solve a Classification Problem</b> :: InBlog", "url": "https://inblog.in/Transfer-Learning-to-solve-a-Classification-Problem-9bihoVsKsV", "isFamilyFriendly": true, "displayUrl": "https://inblog.in/Transfer-<b>Learning-to-solve-a-Classification-Problem</b>-9bihoVsKsV", "snippet": "Why we need <b>pre-Trained</b> <b>Model</b>? Transfer <b>Learning</b> via VGG16; Building a <b>Model</b>; Code Walk Through; Result and Evaluation; Introduction: Neural networks are very different type of the <b>model</b> as compared to the Supervised <b>Learning</b>,. The most important things about deep <b>learning</b> <b>model</b> is it is very hard to train. It requires lots of the resources that a small company can\u2019t bear. RAM on a <b>machine</b> is cheap and is available in plenty. You need hundreds of GB\u2019s of RAM to run a super complex ...", "dateLastCrawled": "2021-11-25T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - <b>Merging pretrained models in Word2Vec</b>? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/30482669/merging-pretrained-models-in-word2vec", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/30482669", "snippet": "How do i merge these two huge <b>pre-trained</b> vectors? or how do i train a new <b>model</b> and update vectors on top of another? I see that C based word2vec does not support batch training. I am looking to compute word <b>analogy</b> from these two models. I believe that vectors learned from these two sources will produce pretty good results. <b>machine</b>-<b>learning</b> word2vec. Share. Follow edited May 28 &#39;15 at 14:04. pbu. asked May 27 &#39;15 at 12:37. pbu pbu. 2,706 7 7 gold badges 37 37 silver badges 62 62 bronze ...", "dateLastCrawled": "2022-01-22T22:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Transformer Neural Network In Deep <b>Learning</b> - Overview - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/transformer-neural-network-in-deep-learning-overview/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/transformer-neural-network-in-deep-<b>learning</b>-overview", "snippet": "The successor to GPT and GPT2 is the GPT3, and is one of the most controversial <b>pre-trained</b> models, by OpenAI the large-scale transformer-based language <b>model</b> has been trained on 175 billion parameters, which is 10 times more than any previous non-sparsed language <b>model</b>. The <b>model</b> has been trained to achieve strong performance on much NLP dataset, including task translation, answering questions, as well as several other tasks.", "dateLastCrawled": "2022-02-02T11:22:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(pre-trained model)  is like +(a machine that is already \"trained\" for a specific task)", "+(pre-trained model) is similar to +(a machine that is already \"trained\" for a specific task)", "+(pre-trained model) can be thought of as +(a machine that is already \"trained\" for a specific task)", "+(pre-trained model) can be compared to +(a machine that is already \"trained\" for a specific task)", "machine learning +(pre-trained model AND analogy)", "machine learning +(\"pre-trained model is like\")", "machine learning +(\"pre-trained model is similar\")", "machine learning +(\"just as pre-trained model\")", "machine learning +(\"pre-trained model can be thought of as\")", "machine learning +(\"pre-trained model can be compared to\")"]}