{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "BioBERT: a pre-trained biomedical <b>language</b> representation <b>model</b> for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7703786/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7703786", "snippet": "Due to the nature of <b>language</b> modeling where future words cannot be seen, previous <b>language</b> models were limited to a combination of two <b>unidirectional</b> <b>language</b> models (i.e. left-to-right and right-to-left). BERT uses a masked <b>language</b> <b>model</b> that predicts randomly masked words in a sequence, and hence can be used for learning bidirectional representations. Also, it obtains state-of-the-art performance on most NLP tasks, while requiring minimal task-specific architectural modification ...", "dateLastCrawled": "2022-02-02T22:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>BioBERT</b>: a pre-trained biomedical <b>language</b> representation <b>model</b> for ...", "url": "https://academic.oup.com/bioinformatics/article/36/4/1234/5566506", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/bioinformatics/article/36/4/1234/5566506", "snippet": "BERT (Devlin et al., 2019) is a contextualized word representation <b>model</b> that is based on a masked <b>language</b> <b>model</b> and pre-trained using bidirectional transformers (Vaswani et al., 2017). Due to the nature of <b>language</b> modeling where future words cannot be seen, previous <b>language</b> models were limited to a combination of two <b>unidirectional</b> <b>language</b> models (i.e. left-to-right and right-to-left). BERT uses a masked <b>language</b> <b>model</b> that predicts randomly masked words in a sequence, and hence can be ...", "dateLastCrawled": "2022-01-19T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>BERT</b> Explained: State of the art <b>language</b> <b>model</b> for NLP | by Rani Horev ...", "url": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bert</b>-explained-state-of-the-art-<b>language</b>-<b>model</b>-for-nlp...", "snippet": "The paper\u2019s results show that a <b>language</b> <b>model</b> which is bidirectionally trained can have a deeper sense of <b>language</b> context and flow than single-direction <b>language</b> models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible. Background. In the field of computer vision, researchers have repeatedly shown the value of transfer learning \u2014 pre-training a neural network <b>model</b> on a known ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Unidirectional</b> Ensemble Recognition and Translation of Phrasal Sign ...", "url": "https://www.researchgate.net/publication/346601102_Unidirectional_Ensemble_Recognition_and_Translation_of_Phrasal_Sign_Language_from_ASL_to_ISL", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/346601102_<b>Unidirectional</b>_Ensemble_Recognition...", "snippet": "This system is called Sign <b>Language</b> Translator and Gesture Recognition. We developed <b>a smart</b> glove that captures the gesture of the hand and interprets these gestures into readable text. This text ...", "dateLastCrawled": "2021-11-11T17:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Quanta Magazine", "url": "https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they-understand-20191017/", "isFamilyFriendly": true, "displayUrl": "https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they...", "snippet": "Before 2018, one of NLP\u2019s main pretraining tools was something <b>like</b> a <b>dictionary</b>. Known as word embeddings, this <b>dictionary</b> encoded associations between words as numbers in a way that deep neural networks could accept as input \u2014 akin to giving the person inside a Chinese room a crude vocabulary book to work with. But a neural network pretrained with word embeddings is still blind to the meaning of words at the sentence level. \u201cIt would think that \u2018a man bit the dog\u2019 and \u2018a dog ...", "dateLastCrawled": "2022-01-30T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Conversion of <b>the English-Xhosa Dictionary for Nurses</b> to a Linguistic ...", "url": "https://www.mdpi.com/2078-2489/9/11/274/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2078-2489/9/11/274/htm", "snippet": "<b>The English-Xhosa Dictionary for Nurses</b> (EXDN) is a bilingual, <b>unidirectional</b> printed <b>dictionary</b> in the public domain, with English and isiXhosa as the <b>language</b> pair. By extending the digitisation efforts of EXDN from a human-readable digital object to a machine-readable state, using Resource Description Framework (RDF) as the data <b>model</b>, semantically interoperable structured data can be created, thus enabling EXDN\u2019s data to be reused, aggregated and integrated with other <b>language</b> ...", "dateLastCrawled": "2021-11-23T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Dictionaries</b> - SlideShare", "url": "https://www.slideshare.net/Seadet/dictionaries-28061503", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/Seadet/<b>dictionaries</b>-28061503", "snippet": "Bilingual <b>dictionaries</b> A bilingual <b>dictionary</b> or translation <b>dictionary</b> is a specialized <b>dictionary</b> used to translate words or phrases from one <b>language</b> to another. Bilingual <b>dictionaries</b> can be <b>unidirectional</b>, meaning that they list the meanings of words of one <b>language</b> in another, or can be bidirectional, allowing translation to and from both languages. Bidirectional bilingual <b>dictionaries</b> usually consist of two sections, each listing words and phrases of one <b>language</b> alphabetically along ...", "dateLastCrawled": "2022-01-17T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Andrew-NG-Notes/andrewng-p-5-sequence-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/.../Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-<b>models</b>.md", "snippet": "Neural <b>language</b> <b>model</b>: Let&#39;s start with an example: ... A simple sentiment classification <b>model</b> would be <b>like</b> this: The embedding matrix may have been trained on say 100 billion words. Number of features in word embedding is 300. We can use sum or average given all the words then pass it to a softmax classifier. That makes this classifier works for short or long sentences. One of the problems with this simple <b>model</b> is that it ignores words order. For example &quot;Completely lacking in good taste ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>OMNIDIRECTIONAL</b> | meaning in the Cambridge English <b>Dictionary</b>", "url": "https://dictionary.cambridge.org/dictionary/english/omnidirectional", "isFamilyFriendly": true, "displayUrl": "https://<b>dictionary</b>.cambridge.org/<b>dictionary</b>/english/<b>omnidirectional</b>", "snippet": "<b>omnidirectional</b> definition: 1. used to describe an antenna (= a piece of electronic equipment that connects radio or computer\u2026. Learn more.", "dateLastCrawled": "2022-01-27T14:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "&quot;<b>Asynchronous&quot; vs. &quot;Synchronous&quot;: Time To</b> Learn The ... - <b>Dictionary</b>.com", "url": "https://www.dictionary.com/e/asynchronous-vs-synchronous/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>dictionary</b>.com/e/asynchronous-vs-synchronous", "snippet": "How to use each word. Asynchronous and synchronous are antonyms as they mean the complete opposite of each other. Both words utilize the Greek syn-, meaning \u201ctogether,\u201d while asynchronous has the prefix a-, meaning \u201cnot.\u201d. So keep in mind that when referring to something that happens at the same time, it\u2019s synchronous, but for anything that doesn\u2019t occur at the same time and is instead staggered or delayed, it\u2019s asynchronous.. For example, in communication, speaking in person ...", "dateLastCrawled": "2022-02-03T06:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "BioBERT: a pre-trained biomedical <b>language</b> representation <b>model</b> for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7703786/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7703786", "snippet": "Due to the nature of <b>language</b> modeling where future words cannot be seen, previous <b>language</b> models were limited to a combination of two <b>unidirectional</b> <b>language</b> models (i.e. left-to-right and right-to-left). BERT uses a masked <b>language</b> <b>model</b> that predicts randomly masked words in a sequence, and hence can be used for learning bidirectional representations. Also, it obtains state-of-the-art performance on most NLP tasks, while requiring minimal task-specific architectural modification ...", "dateLastCrawled": "2022-02-02T22:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>BioBERT</b>: a pre-trained biomedical <b>language</b> representation <b>model</b> for ...", "url": "https://academic.oup.com/bioinformatics/article/36/4/1234/5566506", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/bioinformatics/article/36/4/1234/5566506", "snippet": "BERT (Devlin et al., 2019) is a contextualized word representation <b>model</b> that is based on a masked <b>language</b> <b>model</b> and pre-trained using bidirectional transformers (Vaswani et al., 2017). Due to the nature of <b>language</b> modeling where future words cannot be seen, previous <b>language</b> models were limited to a combination of two <b>unidirectional</b> <b>language</b> models (i.e. left-to-right and right-to-left). BERT uses a masked <b>language</b> <b>model</b> that predicts randomly masked words in a sequence, and hence can be ...", "dateLastCrawled": "2022-01-19T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Comparison of BERT, OpenAI GPT and ELMo <b>model</b> architectures [5 ...", "url": "https://researchgate.net/figure/Comparison-of-BERT-OpenAI-GPT-and-ELMo-model-architectures-5_fig1_338931711", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Comparison-of-BERT-OpenAI-GPT-and-ELMo-<b>model</b>...", "snippet": "In natural <b>language</b> processing (NLP), enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and <b>similar</b> trends are emerging in ...", "dateLastCrawled": "2021-05-13T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Unidirectional</b> Ensemble Recognition and Translation of Phrasal Sign ...", "url": "https://www.researchgate.net/publication/346601102_Unidirectional_Ensemble_Recognition_and_Translation_of_Phrasal_Sign_Language_from_ASL_to_ISL", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/346601102_<b>Unidirectional</b>_Ensemble_Recognition...", "snippet": "This system is called Sign <b>Language</b> Translator and Gesture Recognition. We developed a <b>smart</b> glove that captures the gesture of the hand and interprets these gestures into readable text. This text ...", "dateLastCrawled": "2021-11-11T17:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>BERT</b> Explained: State of the art <b>language</b> <b>model</b> for NLP | by Rani Horev ...", "url": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bert</b>-explained-state-of-the-art-<b>language</b>-<b>model</b>-for-nlp...", "snippet": "The paper\u2019s results show that a <b>language</b> <b>model</b> which is bidirectionally trained can have a deeper sense of <b>language</b> context and flow than single-direction <b>language</b> models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible. Background. In the field of computer vision, researchers have repeatedly shown the value of transfer learning \u2014 pre-training a neural network <b>model</b> on a known ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Andrew-NG-Notes/andrewng-p-5-sequence-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/.../Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-<b>models</b>.md", "snippet": "Neural <b>language</b> <b>model</b>: Let&#39;s start with an example: ... Negative sampling allows you to do something <b>similar</b> to the skip-gram <b>model</b>, but with a much more efficient learning algorithm. We will create a different learning problem. Given this example: &quot;I want a glass of orange juice to go along with my cereal&quot; The sampling will look like this: Context Word target; orange: juice: 1: orange: king: 0: orange: book: 0: orange: the: 0: orange: of: 0: We get positive example by using the same skip ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine-readable dictionary</b> - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Machine-readable_dictionary", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Machine-readable_dictionary</b>", "snippet": "A bilingual <b>dictionary</b> or translation <b>dictionary</b> is a specialized <b>dictionary</b> used to translate words or phrases from one <b>language</b> to another. Bilingual dictionaries can be <b>unidirectional</b>, meaning that they list the meanings of words of one <b>language</b> in another, or can be bidirectional, allowing translation to and from both languages.Bidirectional bilingual dictionaries usually consist of two sections, each listing words and phrases of one <b>language</b> alphabetically along with their translation.", "dateLastCrawled": "2021-09-03T21:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bilingual dictionary</b> - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Bilingual_dictionary", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Bilingual_dictionary</b>", "snippet": "Printed bilingual dictionaries come in all sizes and shapes. A <b>bilingual dictionary</b> or translation <b>dictionary</b> is a specialized <b>dictionary</b> used to translate words or phrases from one <b>language</b> to another. Bilingual dictionaries can be <b>unidirectional</b>, meaning that they list the meanings of words of one <b>language</b> in another, or can be bidirectional, allowing translation to and from both languages.Bidirectional bilingual dictionaries usually consist of two sections, each listing words and phrases ...", "dateLastCrawled": "2021-12-02T18:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Microprocessor or Microcontroller?", "url": "https://courses.cs.washington.edu/courses/cse466/15au/pdfs/lectures/02-Microprocessors-Microcontrollers.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.washington.edu/courses/cse466/15au/pdfs/lectures/02-Microprocessors...", "snippet": "&quot; <b>Model</b> 4004 &quot; 4-bit; 2300 transistors, 640 bytes of memory, 108 KHz clock speed . 9/27/14 9 First Processors ! Intel released the 8086, a 16-bit microprocessor, in 1978 ! Motorola followed with the MC68000 as their 16-bit processor &quot; The 16-bit processor works with 16 bit words, rather than 8 bit words &quot; Instructions are executed faster &quot; Provide single instructions for more complex instructions such as multiply and divide ! 16 bit processors evolved into 32 bit processors ! Intel released ...", "dateLastCrawled": "2022-01-26T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Practice | GeeksforGeeks | A computer science portal for geeks", "url": "https://practice.geeksforgeeks.org/answers/vaishali+bhatia/", "isFamilyFriendly": true, "displayUrl": "https://practice.geeksforgeeks.org/answers/vaishali+bhatia", "snippet": "Waterfall <b>Model</b> is a flow based <b>model</b>, in which we pass every phase once, and can not go back to that phase again. Its most eminent drawback is that if there is any change in requirements, we cannot make any changes to the requirement section. Iterative <b>Model</b> is somewhat <b>similar</b> to waterfall <b>model</b> but herein we can always come back to previous phases, and make the changes accordingly.", "dateLastCrawled": "2022-02-02T11:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Quanta Magazine", "url": "https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they-understand-20191017/", "isFamilyFriendly": true, "displayUrl": "https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they...", "snippet": "\u201cBefore BERT, <b>unidirectional</b> <b>language</b> modeling was the standard, even though it is an unnecessarily restrictive constraint,\u201d said Kenton Lee, a research scientist at Google. Each of these three ingredients \u2014 a deep pretrained <b>language</b> <b>model</b>, attention and bidirectionality \u2014 existed independently before BERT. But until Google released ...", "dateLastCrawled": "2022-01-30T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The Future of Computational Linguistics: On Beyond Alchemy", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8089371/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8089371", "snippet": "If we have an OOV such as <b>unidirectional</b> with no links in the knowledge graph, but we <b>can</b> infer that <b>unidirectional</b> is near directional, <b>can</b> we infer much of the sound (phonemes and stress) and meaning (ontology and/or embeddings) for the OOV from the known word plus the word formation process. That is, if we have the meaning (ontology and/or embeddings) for the known word, directional, and we have the meaning for lots of pairs of words, \u2329 x, u n i + x \u232a that are connected by the uni ...", "dateLastCrawled": "2021-12-07T17:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How does machine translation work? | by Chier Hu | Mr. Translator | Medium", "url": "https://medium.com/mr-translator/how-does-machine-translation-work-a4efaa60eac7?source=post_internal_links---------5----------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mr-translator/how-does-machine-translation-work-a4efaa60eac7?source=...", "snippet": "This <b>can</b> also be calculated by a Markov <b>model</b>. The third factor P (Y) is a constant, because you want to translate the sentence Y, it is a certain thing, you <b>can</b> imagine its probability as 1 ...", "dateLastCrawled": "2021-08-11T09:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Conversion of <b>the English-Xhosa Dictionary for Nurses</b> to a Linguistic ...", "url": "https://www.mdpi.com/2078-2489/9/11/274/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2078-2489/9/11/274/htm", "snippet": "<b>The English-Xhosa Dictionary for Nurses</b> (EXDN) is a bilingual, <b>unidirectional</b> printed <b>dictionary</b> in the public domain, with English and isiXhosa as the <b>language</b> pair. By extending the digitisation efforts of EXDN from a human-readable digital object to a machine-readable state, using Resource Description Framework (RDF) as the data <b>model</b>, semantically interoperable structured data <b>can</b> be created, thus enabling EXDN\u2019s data to be reused, aggregated and integrated with other <b>language</b> ...", "dateLastCrawled": "2021-11-23T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "English/Arabic/English Machine Translation: A His\u2026 \u2013 Meta \u2013 \u00c9rudit", "url": "https://www.erudit.org/en/journals/meta/1900-v1-n1-meta979/011612ar/", "isFamilyFriendly": true, "displayUrl": "https://www.erudit.org/en/journals/meta/1900-v1-n1-meta979/011612ar", "snippet": "This translation <b>can</b> be \u201c<b>unidirectional</b>\u201d translating in one direction as in the case of English into Arabic, \u201cbi-directional\u201d translating in both directions as from English into Arabic and from Arabic into English or even multidirectional translation back and forth between more than two languages or <b>language</b> pairs. Another aspect <b>can</b> be added to this definition which is the presence of a computer system as an initiative for translation. Hahn (2004) distinguishes between \u201cAutonomous ...", "dateLastCrawled": "2022-02-03T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Detailed explanation of Bert <b>model</b> | Develop Paper", "url": "https://developpaper.com/detailed-explanation-of-bert-model/", "isFamilyFriendly": true, "displayUrl": "https://developpaper.com/detailed-explanation-of-bert-<b>model</b>", "snippet": "<b>Smart</b> readers should think of what is \u201cpre training\u201d, but they are very ignorant. Let\u2019s explain it in detail below. Figure 2 \u2013 Application of pre training in image field . Suppose we have two similar tasks a and B (both <b>can</b> be image processing tasks), task a is our target task, and Task B is a task that <b>can</b> be trained in advance: We train Task B to get a CNN <b>model</b> B; Due to the universality of CNN shallow features, we <b>can</b> do the following two kinds of processing: Fine tuning: the ...", "dateLastCrawled": "2021-12-23T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Questioning for Controversial and Critical Thinking Dialogues in ...", "url": "https://www.academia.edu/69782342/Questioning_for_Controversial_and_Critical_Thinking_Dialogues_in_the_Social_Studies_Classroom", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69782342/Questioning_for_Controversial_and_Critical_Thinking...", "snippet": "The American Heritage <b>Dictionary</b> (1991) defines a question as \u201can expression of inquiry that invites or calls for a reply\u201d (p.1015). It subsequently defines the word, in the case as a noun, with ten definitions clustered in seven primary groups. As a verb, it <b>can</b> be delineated as either transitive or intransitive with the <b>dictionary</b> hav- Sean Lennon is an associate professor of undergraduate and graduate programs in the Department of Middle Grades, Secondary, Reading, and Deaf Education ...", "dateLastCrawled": "2022-02-01T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A review on the <b>long short-term memory</b> <b>model</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10462-020-09838-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10462-020-09838-1", "snippet": "Natural <b>language</b> processing is the field of research that explores how computers <b>can</b> be used to understand and manipulate natural <b>language</b> text or speech to do useful things (Chowdhury 2003). For example, dialog systems\u2014also known as conversational agents\u2014allow human beings to interact with a machine via speech. Speech recognition with the use of the LSTM <b>model</b> was first performed in Graves et al.", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) \u0643\u062a\u0627\u0628 ... \u062f\u0644\u064a\u0644 \u0643\u0627\u0645\u0628\u0631\u062f\u062c \u0644\u062a\u062f\u0631\u064a\u0633 \u0627\u0644\u0644\u063a\u0629 \u0627\u0644\u0625\u0646\u062c\u0644\u064a\u0632\u064a\u0629 \u0644\u063a\u064a\u0631 \u0627\u0644\u0646\u0627\u0637\u0642\u064a\u0646 \u0628\u0647\u0627 ...", "url": "https://www.academia.edu/15485822/%D9%83%D8%AA%D8%A7%D8%A8_%D8%AF%D9%84%D9%8A%D9%84_%D9%83%D8%A7%D9%85%D8%A8%D8%B1%D8%AF%D8%AC_%D9%84%D8%AA%D8%AF%D8%B1%D9%8A%D8%B3_%D8%A7%D9%84%D9%84%D8%BA%D8%A9_%D8%A7%D9%84%D8%A5%D9%86%D8%AC%D9%84%D9%8A%D8%B2%D9%8A%D8%A9_%D9%84%D8%BA%D9%8A%D8%B1_%D8%A7%D9%84%D9%86%D8%A7%D8%B7%D9%82%D9%8A%D9%86_%D8%A8%D9%87%D8%A7", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/15485822/\u0643\u062a\u0627\u0628_\u062f\u0644\u064a\u0644_\u0643\u0627\u0645\u0628\u0631\u062f\u062c_\u0644\u062a\u062f\u0631\u064a\u0633...", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2021-12-03T06:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>For foreign language NLP, would it</b> be best to use a native syntax ...", "url": "https://www.quora.com/For-foreign-language-NLP-would-it-be-best-to-use-a-native-syntax-parser-etc-or-use-machine-translation", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>For-foreign-language-NLP-would-it</b>-be-best-to-use-a-native-syntax...", "snippet": "Answer (1 of 3): Translating to English for further processing is always tempting but almost never a good idea. Mistakes early on in a pipeline get magnified, and translation is a very hard task so there will be many mistakes. An example of task for which it may be a less bad idea would be explo...", "dateLastCrawled": "2022-01-22T06:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "BioBERT: a pre-trained biomedical <b>language</b> representation <b>model</b> for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7703786/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7703786", "snippet": "Due to the nature of <b>language</b> modeling where future words cannot be seen, previous <b>language</b> models were limited to a combination of two <b>unidirectional</b> <b>language</b> models (i.e. left-to-right and right-to-left). BERT uses a masked <b>language</b> <b>model</b> that predicts randomly masked words in a sequence, and hence <b>can</b> be used for learning bidirectional representations. Also, it obtains state-of-the-art performance on most NLP tasks, while requiring minimal task-specific architectural modification ...", "dateLastCrawled": "2022-02-02T22:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Comparison of BERT, OpenAI GPT and ELMo <b>model</b> architectures [5 ...", "url": "https://researchgate.net/figure/Comparison-of-BERT-OpenAI-GPT-and-ELMo-model-architectures-5_fig1_338931711", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Comparison-of-BERT-OpenAI-GPT-and-ELMo-<b>model</b>...", "snippet": "The visual comparison between different <b>language</b> models is shown in Figure 1. It <b>can</b> be seen that BERT is deeply bidirectional, Open AI GPT is <b>unidirectional</b>, and ELMo is shallowly bidirectional ...", "dateLastCrawled": "2021-05-13T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>BioBERT</b>: a pre-trained biomedical <b>language</b> representation <b>model</b> for ...", "url": "https://academic.oup.com/bioinformatics/article/36/4/1234/5566506", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/bioinformatics/article/36/4/1234/5566506", "snippet": "BERT (Devlin et al., 2019) is a contextualized word representation <b>model</b> that is based on a masked <b>language</b> <b>model</b> and pre-trained using bidirectional transformers (Vaswani et al., 2017). Due to the nature of <b>language</b> modeling where future words cannot be seen, previous <b>language</b> models were limited to a combination of two <b>unidirectional</b> <b>language</b> models (i.e. left-to-right and right-to-left). BERT uses a masked <b>language</b> <b>model</b> that predicts randomly masked words in a sequence, and hence <b>can</b> be ...", "dateLastCrawled": "2022-01-19T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>BERT</b> Explained: State of the art <b>language</b> <b>model</b> for NLP | by Rani Horev ...", "url": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bert</b>-explained-state-of-the-art-<b>language</b>-<b>model</b>-for-nlp...", "snippet": "The paper\u2019s results show that a <b>language</b> <b>model</b> which is bidirectionally trained <b>can</b> have a deeper sense of <b>language</b> context and flow than single-direction <b>language</b> models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible. Background. In the field of computer vision, researchers have repeatedly shown the value of transfer learning \u2014 pre-training a neural network <b>model</b> on a known ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Bidirectional Recurrent Neural Network <b>Language</b> <b>Model</b>: Cross ...", "url": "https://www.academia.edu/44082991/Bidirectional_Recurrent_Neural_Network_Language_Model_Cross_Entropy_Churn_Metrics_for_Defect_Prediction_Modeling", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/44082991/Bidirectional_Recurrent_Neural_Network_<b>Language</b>...", "snippet": "Software Defect Prediction (SDP) plays an active area in many research domain of Software Quality of Assurance (SQA). Many existing research studies are based on software traditional metric sets and defect prediction models are built in machine", "dateLastCrawled": "2021-09-19T03:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Unidirectional</b> Ensemble Recognition and Translation of Phrasal Sign ...", "url": "https://www.researchgate.net/publication/346601102_Unidirectional_Ensemble_Recognition_and_Translation_of_Phrasal_Sign_Language_from_ASL_to_ISL", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/346601102_<b>Unidirectional</b>_Ensemble_Recognition...", "snippet": "An efficient sign <b>language</b> recognition system (SLRS) <b>can</b> recognize the gestures of sign <b>language</b> to ease the communication between the signer and non-signer community. In this paper, a computer ...", "dateLastCrawled": "2021-11-11T17:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Andrew-NG-Notes/andrewng-p-5-sequence-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/.../Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-<b>models</b>.md", "snippet": "Character-level <b>language</b> <b>model</b> has some pros and cons <b>compared</b> to the word-level <b>language</b> <b>model</b> Pros: There will be no &lt;UNK&gt; token - it <b>can</b> create any word. Cons: The main disadvantage is that you end up with much longer sequences. Character-level <b>language</b> models are not as good as word-level <b>language</b> models at capturing long range dependencies between how the the earlier parts of the sentence also affect the later part of the sentence. Also more computationally expensive and harder to train ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "&quot;<b>Asynchronous&quot; vs. &quot;Synchronous&quot;: Time To</b> Learn The ... - <b>Dictionary</b>.com", "url": "https://www.dictionary.com/e/asynchronous-vs-synchronous/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>dictionary</b>.com/e/asynchronous-vs-synchronous", "snippet": "How to use each word. Asynchronous and synchronous are antonyms as they mean the complete opposite of each other. Both words utilize the Greek syn-, meaning \u201ctogether,\u201d while asynchronous has the prefix a-, meaning \u201cnot.\u201d. So keep in mind that when referring to something that happens at the same time, it\u2019s synchronous, but for anything that doesn\u2019t occur at the same time and is instead staggered or delayed, it\u2019s asynchronous.. For example, in communication, speaking in person ...", "dateLastCrawled": "2022-02-03T06:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Computers Are Learning to Read\u2014But They&#39;re Still Not So <b>Smart</b> | WIRED", "url": "https://www.wired.com/story/computers-are-learning-to-read-but-theyre-still-not-so-smart/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.wired.com</b>/story/computers-are-learning-to-read-but-theyre-still-not-so-<b>smart</b>", "snippet": "Unlike other pretrained <b>language</b> models, many of which are created by having neural networks read terabytes of text from left to right, BERT\u2019s <b>model</b> reads left to right and right to left at the ...", "dateLastCrawled": "2022-01-29T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Microprocessor or Microcontroller?", "url": "https://courses.cs.washington.edu/courses/cse466/15au/pdfs/lectures/02-Microprocessors-Microcontrollers.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.washington.edu/courses/cse466/15au/pdfs/lectures/02-Microprocessors...", "snippet": "&quot; <b>Model</b> 4004 &quot; 4-bit; 2300 transistors, 640 bytes of memory, 108 KHz clock speed . 9/27/14 9 First Processors ! Intel released the 8086, a 16-bit microprocessor, in 1978 ! Motorola followed with the MC68000 as their 16-bit processor &quot; The 16-bit processor works with 16 bit words, rather than 8 bit words &quot; Instructions are executed faster &quot; Provide single instructions for more complex instructions such as multiply and divide ! 16 bit processors evolved into 32 bit processors ! Intel released ...", "dateLastCrawled": "2022-01-26T02:37:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Language</b> Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>language</b>", "snippet": "A term used to describe a system that evaluates the text that both precedes and follows a target section of text. In contrast, a <b>unidirectional</b> system only evaluates the text that precedes a target section of text. For example, consider a masked <b>language</b> <b>model</b> that must determine probabilities for the word(s) representing the underline in the following question:. What is the _____ with you? A <b>unidirectional</b> <b>language</b> <b>model</b> would have to base its probabilities only on the context provided by ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Word Acquisition in Neural <b>Language</b> Models | Transactions of the ...", "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00444/109271/Word-Acquisition-in-Neural-Language-Models", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/.../tacl_a_00444/109271/Word-Acquisition-in-Neural-<b>Language</b>-<b>Models</b>", "snippet": "A quadratic <b>model</b> of log-frequency also provided a slightly better fit for <b>unidirectional</b> <b>language</b> models (R 2 = 0.93 to 0.94), particularly for high-frequency words; in <b>language</b> models, this could be due either to a floor effect on age of acquisition for high-frequency words or to slower <b>learning</b> of function words. Regardless, significant effects of other predictors remained the same when using a quadratic <b>model</b> for log-frequency.", "dateLastCrawled": "2022-02-02T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Fine-tuned <b>Language Models for Text Classification</b> | DeepAI", "url": "https://deepai.org/publication/fine-tuned-language-models-for-text-classification", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fine-tuned-<b>language-models-for-text-classification</b>", "snippet": "In <b>analogy</b>, a hypercolumn for a word or sentence in NLP is the concatenation of embeddings at different layers in a pretrained <b>model</b>. and is used by peters2017semi, deepcontext2017, Wieting2017, Conneau2017, and Mccann2017 who use <b>language</b> modeling, paraphrasing, entailment, and <b>Machine</b> Translation (MT) respectively for pretraining. Specifically, deepcontext2017 require engineered custom architectures, while we show state-of-the-art performance with the same basic architecture across a range ...", "dateLastCrawled": "2021-12-23T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Deep Learning</b> for NLP - GitHub Pages", "url": "https://strikingloo.github.io/wiki-articles/machine-learning/deep-learning-NLP", "isFamilyFriendly": true, "displayUrl": "https://strikingloo.github.io/wiki-articles/<b>machine</b>-<b>learning</b>/<b>deep-learning</b>-NLP", "snippet": "Then feed to your main <b>model</b> both a char-RNN rep\u2019n, a word embedding and, after going through a bi-directional LSTM, concatenate hidden states with the concatenated hidden states of the (now pre-trained and frozen) <b>language</b> <b>model</b>. This beat SOTA by a narrow margin (0.3) but it was a much simpler <b>model</b> than the competition. ELMo", "dateLastCrawled": "2021-09-30T03:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Machine learning, artificial neural networks and social</b> research", "url": "https://www.researchgate.net/publication/344171463_Machine_learning_artificial_neural_networks_and_social_research", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344171463_<b>Machine</b>_<b>learning</b>_artificial_neural...", "snippet": "<b>Machine</b> <b>Learning</b> (ML) is an automatic <b>learning</b> process in which data sets are processed (Di Franco and Santurro, 2020). An ML system learns directly from the data and learns to connect one or more ...", "dateLastCrawled": "2022-02-02T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Conceptual models of programming environments: how learners use ...", "url": "https://www.academia.edu/68126562/Conceptual_models_of_programming_environments_how_learners_use_the_glass_box", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/68126562/Conceptual_<b>models</b>_of_programming_environments_how...", "snippet": "A similar <b>model</b> of <b>learning</b> underlies much of the work in this area: in particular work on <b>learning</b> by <b>analogy</b>, but it is often not made explicit. Based on such a framework, Mayer (1975) proposed a concrete <b>model</b> for teaching a BASIC-like <b>language</b>. This provides analogies for four functional units of the computer; and can either be presented as a diagram or as a board using actual parts. The helpfulness of this <b>model</b> was investigated in a study where subjects read a short manual describing ...", "dateLastCrawled": "2022-01-24T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Machine</b> <b>Learning</b> and Cognitive Algorithms for Engineering ...", "url": "https://www.researchgate.net/publication/271022039_Machine_Learning_and_Cognitive_Algorithms_for_Engineering_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/271022039_<b>Machine</b>_<b>Learning</b>_and_Cognitive...", "snippet": "<b>Machine</b> <b>Learning</b> and <b>Cognitive Algorithms for Engineering Applications</b> . October 2015; International Journal of Cognitive Informatics and Natural Intelligence 7(4):64-82; DOI:10.4018/ijcini ...", "dateLastCrawled": "2021-10-18T19:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine learning</b>, artificial neural networks and social research ...", "url": "https://link.springer.com/article/10.1007/s11135-020-01037-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11135-020-01037-y", "snippet": "<b>Machine learning</b> (ML), and particularly algorithms based on artificial neural networks (ANNs), constitute a field of research lying at the intersection of different disciplines such as mathematics, statistics, computer science and neuroscience. This approach is characterized by the use of algorithms to extract knowledge from large and heterogeneous data sets. In addition to offering a brief introduction to ANN algorithms-based ML, in this paper we will focus our attention on its possible ...", "dateLastCrawled": "2022-01-27T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On the character of Indian Stock Markets: A <b>Machine</b> <b>Learning</b> Approach ...", "url": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-machine-learning-approach", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/on-the-character-of-indian-stock-markets-a-<b>machine</b>-<b>learning</b>-approach", "snippet": "On the character of Indian Stock Markets: A <b>Machine</b> <b>Learning</b> Approach. Shubham popli Northcap University Gurgaon, Haryana. Abstract- The enterprise of forecasting the stock market is as old as the market itself, ranging from the many traditional approaches like regression analysis and linear methods like AR, MA, ARIMA and ARMA, and of course fuzzier methods like experts intuitions and sentiment analysis of news cycles.", "dateLastCrawled": "2021-12-29T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Andrew-NG-Notes/andrewng-p-5-sequence-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/.../Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-<b>models</b>.md", "snippet": "What is a <b>language</b> <b>model</b>. Let&#39;s say we are solving a speech recognition problem and someone says a sentence that can be interpreted into to two sentences: The apple and pair salad; The apple and pear salad; Pair and pear sounds exactly the same, so how would a speech recognition application choose from the two. That&#39;s where the <b>language</b> <b>model</b> comes in. It gives a probability for the two sentences and the application decides the best based on this probability. The job of a <b>language</b> <b>model</b> is ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(unidirectional language model)  is like +(a \"smart\" dictionary)", "+(unidirectional language model) is similar to +(a \"smart\" dictionary)", "+(unidirectional language model) can be thought of as +(a \"smart\" dictionary)", "+(unidirectional language model) can be compared to +(a \"smart\" dictionary)", "machine learning +(unidirectional language model AND analogy)", "machine learning +(\"unidirectional language model is like\")", "machine learning +(\"unidirectional language model is similar\")", "machine learning +(\"just as unidirectional language model\")", "machine learning +(\"unidirectional language model can be thought of as\")", "machine learning +(\"unidirectional language model can be compared to\")"]}