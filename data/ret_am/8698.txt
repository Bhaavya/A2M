{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "cikm 2018 - <b>Sentence</b> Similarity - <b>GitHub</b>", "url": "https://github.com/MarvinLSJ/LSTM-siamese", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/MarvinLSJ/LSTM-siamese", "snippet": "21400 Labeled <b>Spanish</b> <b>sentence</b> pairs &amp; English <b>sentence</b> pairs are provided; 55669 <b>Unlabeled</b> <b>Spanish</b> sentences &amp; corresponding English translations are provided. Test Data. 5000 <b>Spanish</b> <b>sentence</b> pairs . Goal and Evaluation. Predicting the similarity of <b>Spanish</b> <b>sentence</b> pairs in test set. Evaluated result by logloss. ML Model. Developed by freedomwyl in Link. Deep Model. Common thoughts would be finding a way to represent sentences and calculate their similarity, with a little elaboration ...", "dateLastCrawled": "2022-01-26T16:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning unsupervised embeddings for textual similarity</b> with ...", "url": "https://www.depends-on-the-definition.com/unsupervised-text-embeddings-with-transformers/", "isFamilyFriendly": true, "displayUrl": "https://www.depends-on-the-definition.com/unsupervised-text-embeddings-with-transformers", "snippet": "<b>Learning unsupervised embeddings for textual similarity with transformers</b>. In this article, we look at SimCSE, a sim ple c ontrastive <b>s entence</b> e mbedding framework, which can be used to produce superior <b>sentence</b> embeddings, from either <b>unlabeled</b> or labeled data. The idea behind the unsupervised SimCSE is to simply predicts the input <b>sentence</b> ...", "dateLastCrawled": "2022-01-25T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> from Positive and <b>Unlabeled</b> Examples: A Survey | Request PDF", "url": "https://www.researchgate.net/publication/4348735_Learning_from_Positive_and_Unlabeled_Examples_A_Survey", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/4348735_<b>Learning</b>_from_Positive_and_<b>Unlabeled</b>...", "snippet": "Positive-<b>Unlabeled</b> (PU) <b>learning</b> [3], [4] is a type of semisupervised <b>learning</b> where only some of the positive cases are labeled, while the <b>unlabeled</b> data consists of both positive and negative ...", "dateLastCrawled": "2021-10-27T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Enhancing Multilingual Sentence Embeddings with Semantic Similarity</b>", "url": "https://megagon.ai/blog/emu-enhancing-multilingual-sentence-embeddings-with-semantic-similarity/", "isFamilyFriendly": true, "displayUrl": "https://megagon.ai/blog/emu-<b>enhancing-multilingual-sentence-embeddings-with-semantic</b>...", "snippet": "But for emerging technologies <b>like</b> AI and machine <b>learning</b>, the difference in semantics between languages can be a tremendous barrier that limits the innovative potential of applications. To solve this problem, we developed Emu, a neural network model that can enhance multilingual <b>sentence</b> embeddings via semantic similarity.", "dateLastCrawled": "2022-02-01T21:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Applications: NLP</b> - CS 182: Deep <b>Learning</b>", "url": "https://cs182sp21.github.io/static/slides/lec-13.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs182sp21.github.io/static/slides/lec-13.pdf", "snippet": "Deep <b>learning</b> works best when we have a lot of data Good news: there is plenty of data of text out there! Bad news: most of it is <b>unlabeled</b> 1,000s of times more data without labels (i.e., valid English text in books, news, web) vs. labeled/paired data (e.g., English/French translations) The big challenge: how can we use freely available and ...", "dateLastCrawled": "2022-01-29T13:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "From Exemplar to Grammar: A Probabilistic Analogy\u2010Based Model of ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2009.01031.x", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2009.01031.x", "snippet": "For <b>example</b>, for a PCFG to capture a multiword unit <b>like</b> Everything you always wanted to know about X but were afraid to ask, we either need to take this entire expression as right-handside of the PCFG-rule or we need to use separate categories that are specially made up for this <b>sentence</b>. While such a PCFG can still recognize this long multiword unit, it would thus fail to generalize with other parts of the grammar. A PTSG is more flexible in this respect, in that it allows for productive ...", "dateLastCrawled": "2021-08-18T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Explanation of BERT Model - NLP - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/explanation-of-bert-model-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/explanation-of-bert-model-nlp", "snippet": "Semi-supervised <b>Learning</b>: ... For <b>Example</b>, the paper achieves great results just by using a single layer NN on the BERT model in the classification task. ELMo Word Embeddings: This article is good for recapping Word Embedding. It also discusses Word2Vec and its implementation. Basically, word Embeddings for a word is the projection of a word to a vector of numerical values based on its meaning. There are many popular words Embedding such as Word2vec, GloVe, etc. ELMo was different from these ...", "dateLastCrawled": "2022-02-02T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "1. The Basics - Natural <b>Language Annotation for Machine Learning</b> [Book]", "url": "https://www.oreilly.com/library/view/natural-language-annotation/9781449332693/ch01.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/natural-language-annotation/9781449332693/ch01.html", "snippet": "In the first <b>sentence</b> in the following <b>example</b>, each tag is expressed directly as real text. That is, they are all consuming tags (\u201cpromoted\u201d is marked as an Event, \u201cbefore\u201d is marked as a TempRel, and \u201cthe summer\u201d is marked as a Timex). Notice, however, that in the second <b>sentence</b>, there is no explicit temporal relation in the text, even though we know that it\u2019s something <b>like</b> \u201con\u201d. So, we actually insert a TempRel with the value of \u201con\u201d in our corpus, but the tag is ...", "dateLastCrawled": "2022-02-01T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Getting started with NLP: Word Embeddings, GloVe and Text ...", "url": "https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html", "isFamilyFriendly": true, "displayUrl": "https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/...", "snippet": "or using a word to predict a target context, which is called skip-gram, for <b>example</b>, we\u2019d <b>like</b> to predict c context words having one target word on the input. The latter method usually produces more accurate results on large datasets. A well trained set of word vectors will place similar words close to each other in that space. The words oak, elm and birch might cluster in one corner, while war, conflict and strife huddle together in another. Glove; The Global Vectors for Word ...", "dateLastCrawled": "2022-02-02T08:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "SENTIMENTAL ANALYSIS USING <b>VADER</b>. interpretation and classification of ...", "url": "https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/sentimental-analysis-using-<b>vader</b>-a3415fef7664", "snippet": "The above <b>sentence</b> consists of two polarities!!! <b>VADER</b>. <b>VADER</b> ( Valence Aware Dictionary for Sentiment Reasoning) is a model used for text sentiment analysis that is sensitive to both polarity (positive/negative) and intensity (strength) of emotion. It is available in the NLTK package and can be applied directly to <b>unlabeled</b> text data.", "dateLastCrawled": "2022-02-02T23:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Learning</b> from Positive and <b>Unlabeled</b> Examples: A Survey | Request PDF", "url": "https://www.researchgate.net/publication/4348735_Learning_from_Positive_and_Unlabeled_Examples_A_Survey", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/4348735_<b>Learning</b>_from_Positive_and_<b>Unlabeled</b>...", "snippet": "Positive-<b>Unlabeled</b> (PU) <b>learning</b> [3], [4] is a type of semisupervised <b>learning</b> where only some of the positive cases are labeled, while the <b>unlabeled</b> data consists of both positive and negative ...", "dateLastCrawled": "2021-10-27T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Enhancing Multilingual Sentence Embeddings with Semantic Similarity</b>", "url": "https://megagon.ai/blog/emu-enhancing-multilingual-sentence-embeddings-with-semantic-similarity/", "isFamilyFriendly": true, "displayUrl": "https://megagon.ai/blog/emu-<b>enhancing-multilingual-sentence-embeddings-with-semantic</b>...", "snippet": "But multilingual support remains an elusive bottleneck for machine <b>learning</b> applications. For <b>example</b>, if you trained a chatbot with data written in English, it would not be able to answer queries in other languages such as <b>Spanish</b> or German. You would need to collect data for each desired language and train the chatbot with it to make this work (a monumental, if not impossible, task). To circumvent this obstacle, AI researchers created multilingual <b>sentence</b> embedding models. These models ...", "dateLastCrawled": "2022-02-01T21:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning unsupervised embeddings for textual similarity</b> with ...", "url": "https://www.depends-on-the-definition.com/unsupervised-text-embeddings-with-transformers/", "isFamilyFriendly": true, "displayUrl": "https://www.depends-on-the-definition.com/unsupervised-text-embeddings-with-transformers", "snippet": "<b>Learning unsupervised embeddings for textual similarity with transformers</b>. In this article, we look at SimCSE, a sim ple c ontrastive <b>s entence</b> e mbedding framework, which can be used to produce superior <b>sentence</b> embeddings, from either <b>unlabeled</b> or labeled data. The idea behind the unsupervised SimCSE is to simply predicts the input <b>sentence</b> ...", "dateLastCrawled": "2022-01-25T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A developmental shift from <b>similar to language-specific strategies</b> in ...", "url": "https://www.sciencedirect.com/science/article/pii/S0010027709002418", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0010027709002418", "snippet": "The proposed <b>sentence</b> structure is more common with manner verbs than with path verbs in <b>Spanish</b>. However, this <b>sentence</b> frame should have less of a direct impact than in the sentences used by Hohenstein, therefore giving a more neutral test of language-specific influences. To keep Japanese as <b>similar</b> as possible in this respect, the verb inflection \u201c-teiru\u201d was added to the novel verb. The verb inflection \u201c-teiru\u201d <b>is similar</b> to an English light verb, like \u201cdoing\u201d and marks the ...", "dateLastCrawled": "2022-01-27T23:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) A developmental shift from <b>similar</b> to language-specific ...", "url": "https://www.academia.edu/18541014/A_developmental_shift_from_similar_to_language_specific_strategies_in_verb_acquisition_A_comparison_of_English_Spanish_and_Japanese", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/18541014/A_developmental_shift_from_<b>similar</b>_to_language...", "snippet": "A developmental shift from <b>similar to language-specific strategies in verb acquisition</b>: A comparison of English, <b>Spanish</b>, and Japanese. Cognition, 2010. Roberta Golinkoff. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package. Translate PDF. Related Papers. Twenty-five years using the Intermodal Preferential Looking Paradigm to study ...", "dateLastCrawled": "2022-01-21T06:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "From Exemplar to Grammar: A Probabilistic Analogy\u2010Based Model of ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2009.01031.x", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2009.01031.x", "snippet": "<b>Similar</b> to (recent versions of) DOP, U-DOP analyzes a new <b>sentence</b> out of the largest and most frequent subtrees from trees of previous sentences. The fundamental difference with the supervised DOP approach is that U-DOP takes into account subtrees from all possible (binary) trees of previous sentences rather than from a set of manually annotated trees. Although we do not claim that the U-DOP model in this paper provides any near-to-complete theory of language acquisition, we will show that ...", "dateLastCrawled": "2021-08-18T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Applications: NLP</b> - CS 182: Deep <b>Learning</b>", "url": "https://cs182sp21.github.io/static/slides/lec-13.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs182sp21.github.io/static/slides/lec-13.pdf", "snippet": "Deep <b>learning</b> works best when we have a lot of data Good news: there is plenty of data of text out there! Bad news: most of it is <b>unlabeled</b> 1,000s of times more data without labels (i.e., valid English text in books, news, web) vs. labeled/paired data (e.g., English/French translations) The big challenge: how can we use freely available and ...", "dateLastCrawled": "2022-01-29T13:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fine-tune with data without labels</b> \u00b7 Issue #89 \u00b7 UKPLab/<b>sentence</b> ...", "url": "https://github.com/UKPLab/sentence-transformers/issues/89", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/UKPLab/<b>sentence</b>-transformers/issues/89", "snippet": "You need some structure / label that encode which <b>sentence</b> pairs should be <b>similar</b> or unsimilar. <b>Learning</b> this from thin-air is obviously not possible, you need to tell somehow the computer what you judge as <b>similar</b> and what not. Sometimes, you can use some structure in your documents. For <b>example</b>, two <b>sentence</b> on the same topic (from the same doc) should be more <b>similar</b> than two <b>sentence</b> from different documents. Or neighbouring sentences should be more <b>similar</b> than non-neighbouring ...", "dateLastCrawled": "2021-09-03T23:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Getting started with NLP: Word Embeddings, GloVe and Text ...", "url": "https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html", "isFamilyFriendly": true, "displayUrl": "https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/...", "snippet": "Word embeddings are a type of word representation that allows words with <b>similar</b> meaning to have a <b>similar</b> representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep <b>learning</b> methods on challenging natural language processing problems. Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one ...", "dateLastCrawled": "2022-02-02T08:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) A <b>SEMINAR REPORT On Machine Learning</b> | Amrit Kumar Sah - Academia.edu", "url": "https://www.academia.edu/41176121/A_SEMINAR_REPORT_On_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/41176121/A_<b>SEMINAR_REPORT_On_Machine_Learning</b>", "snippet": "Machine <b>learning</b> is effectively a method of data analysis that works by automating the process of building data models. 1.1.1 Supervised <b>learning</b> Supervised <b>learning</b> is the machine <b>learning</b> task of <b>learning</b> a function that maps an input to an output based on <b>example</b> input-output pairs. It infers a function from labeled training data consisting ...", "dateLastCrawled": "2022-02-01T19:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Learning Distributed Representations of Sentences from Unlabelled</b> ...", "url": "https://www.researchgate.net/publication/305334586_Learning_Distributed_Representations_of_Sentences_from_Unlabelled_Data", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/305334586_<b>Learning</b>_Distributed...", "snippet": "These <b>sentence</b> embedding approaches generally fall into two categories: one is based on supervised <b>learning</b>, including: InferSent (Conneau et al., 2017), Universal <b>Sentence</b> Encoder (Cer et al ...", "dateLastCrawled": "2022-01-18T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning unsupervised embeddings for textual similarity</b> with ...", "url": "https://www.depends-on-the-definition.com/unsupervised-text-embeddings-with-transformers/", "isFamilyFriendly": true, "displayUrl": "https://www.depends-on-the-definition.com/unsupervised-text-embeddings-with-transformers", "snippet": "In this article, we look at SimCSE, a simple contrastive <b>sentence</b> embedding framework, which <b>can</b> be used to produce superior <b>sentence</b> embeddings, from either <b>unlabeled</b> or labeled data. The idea behind the unsupervised SimCSE is to simply predicts the input <b>sentence</b> itself, with only dropout used as noise. The same input <b>sentence</b> is passed to the pre-trained encoder twice and obtain two embeddings as \u201cpositive pairs\u201d, by applying independently sampled dropout masks. Due to the dropout ...", "dateLastCrawled": "2022-01-25T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Augmenting Data for Sarcasm Detection with <b>Unlabeled</b> Conversation Context", "url": "https://aclanthology.org/2020.figlang-1.2.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.figlang-1.2.pdf", "snippet": "<b>Spanish</b>, Dutch]. 2.3.2 Augmentation with <b>Unlabeled</b> Data We also generate additional training samples using the <b>unlabeled</b> data: [c 1;c 2; ;c n;r 1]. This ap-proach is tremendously useful since a huge amount of <b>unlabeled</b> dialogue threads <b>can</b> be collected at little cost. As shown in Figure3, the procedures for <b>unlabeled</b> augmentation are as follows ...", "dateLastCrawled": "2022-01-16T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "PAUSE: Positive and Annealed <b>Unlabeled</b> <b>Sentence</b> Embedding | Request PDF", "url": "https://www.researchgate.net/publication/357121237_PAUSE_Positive_and_Annealed_Unlabeled_Sentence_Embedding", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357121237_PAUSE_Positive_and_Annealed...", "snippet": "Request PDF | On Jan 1, 2021, Lele Cao and others published PAUSE: Positive and Annealed <b>Unlabeled</b> <b>Sentence</b> Embedding | Find, read and cite all the research you need on ResearchGate", "dateLastCrawled": "2021-12-20T22:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "cikm 2018 - <b>Sentence</b> Similarity - <b>GitHub</b>", "url": "https://github.com/MarvinLSJ/LSTM-siamese", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/MarvinLSJ/LSTM-siamese", "snippet": "55669 <b>Unlabeled</b> <b>Spanish</b> sentences &amp; corresponding English translations are provided. Test Data. 5000 <b>Spanish</b> <b>sentence</b> pairs. Goal and Evaluation . Predicting the similarity of <b>Spanish</b> <b>sentence</b> pairs in test set. Evaluated result by logloss. ML Model. Developed by freedomwyl in Link. Deep Model. Common thoughts would be finding a way to represent sentences and calculate their similarity, with a little elaboration, here comes the basic model. Basic Model: LSTM-Siamese. Name Origin. The name ...", "dateLastCrawled": "2022-01-26T16:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) A <b>SEMINAR REPORT On Machine Learning</b> | Amrit Kumar Sah - Academia.edu", "url": "https://www.academia.edu/41176121/A_SEMINAR_REPORT_On_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/41176121/A_<b>SEMINAR_REPORT_On_Machine_Learning</b>", "snippet": "Machine <b>learning</b> is effectively a method of data analysis that works by automating the process of building data models. 1.1.1 Supervised <b>learning</b> Supervised <b>learning</b> is the machine <b>learning</b> task of <b>learning</b> a function that maps an input to an output based on <b>example</b> input-output pairs. It infers a function from labeled training data consisting ...", "dateLastCrawled": "2022-02-01T19:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An introduction to Deep <b>Learning</b> in Natural Language Processing: Models ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "snippet": "An <b>example</b> of a few NLP tasks applied to the input <b>sentence</b> ... These methods <b>can</b> be trained on large scale <b>unlabeled</b> corpora through a language model objective function, which is a probability distribution over sequences of words. On the other hand, supervised methods use explicit labels to develop meaningful representations used in downstream tasks. As a primer attempt of unsupervised method, the simple average pooling of word vectors has been explored to derive <b>sentence</b> vectors ...", "dateLastCrawled": "2022-01-04T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Recent <b>Trends in Natural Language Processing Using Deep Learning</b> | by ...", "url": "https://medium.com/@kanchansarkar/recent-trends-in-natural-language-processing-using-deep-learning-a1469fbd2ef", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@kanchansarkar/recent-<b>trends-in-natural-language-processing</b>-using...", "snippet": "Given that a <b>sentence</b> has n words, the <b>sentence</b> <b>can</b> now be represented as an embedding matrix W \u2208 R^(n\u00d7d) . Fig. 4 depicts such a <b>sentence</b> as an input to the CNN framework. Let wi:i+j refer to ...", "dateLastCrawled": "2022-01-22T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Importing Hugging Face models into Spark NLP | by Jose Juan Martinez ...", "url": "https://medium.com/spark-nlp/importing-huggingface-models-into-sparknlp-8c63bdea671d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/spark-nlp/importing-huggingface-models-into-sparknlp-8c63bdea671d", "snippet": "In this article we are going to show two examples of how to import Hugging Face embeddings models into Spark NLP, and another <b>example</b> showcasing a bulk importing of 7 BertForSequenceClassification\u2026", "dateLastCrawled": "2022-02-03T12:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine <b>learning</b> - Bug in sentiment analysis and classification for ...", "url": "https://datascience.stackexchange.com/questions/84587/bug-in-sentiment-analysis-and-classification-for-unlabeled-text", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/84587/bug-in-sentiment-analysis-and...", "snippet": "For <b>example</b>,I found that TextBlob recognizes -0.70 polarity in &quot;fewer people are dying every day&quot; (negative comment) or the transformers pipeline recognizes &quot;The audience here in the hall has promised to remain silent.&quot; as a negative comment with 0.99 percent certainty! Why do you think it&#39;s happening? Is there any way we <b>can</b> prevent this? Is there any way better than this for analyzing the sentiment of <b>unlabeled</b> text? Also, I&#39;m not comfortable with sentences like &quot;Oh, Really?!&quot; being ...", "dateLastCrawled": "2022-01-08T02:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Self-training and Co-<b>training applied to Spanish Named Entity Recognition</b>", "url": "http://www.kozareva.com/papers/micai05scner.pdf", "isFamilyFriendly": true, "displayUrl": "www.kozareva.com/papers/micai05scner.pdf", "snippet": "Recently there has been a great interest in the area of weakly supervised <b>learning</b>, where <b>unlabeled</b> data has been utilized in addition to the labeled one. In ma-chine <b>learning</b>, the classi\ufb02ers crucially rely on labeled training data, which was previously created from <b>unlabeled</b> one with some associated cost. Self-training and co-training algorithms allow a classi\ufb02er to start with few labeled examples, to produce an initial weak classi\ufb02er and later to use only the <b>unlabeled</b> data for ...", "dateLastCrawled": "2022-01-21T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Graph-based Approach for Positive and <b>Unlabeled</b> <b>Learning</b> | Request PDF", "url": "https://www.researchgate.net/publication/354292457_A_Graph-based_Approach_for_Positive_and_Unlabeled_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354292457_A_Graph-based_Approach_for_Positive...", "snippet": "Abstract. Positive and <b>Unlabeled</b> <b>Learning</b> (PUL) uses <b>unlabeled</b> documents and a few positive documents for retrieving a set of \u201dinterest\u201d documents from a text collection. Usually, PUL ...", "dateLastCrawled": "2022-01-29T00:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> from Positive and <b>Unlabeled</b> Examples: A Survey | Request PDF", "url": "https://www.researchgate.net/publication/4348735_Learning_from_Positive_and_Unlabeled_Examples_A_Survey", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/4348735_<b>Learning</b>_from_Positive_and_<b>Unlabeled</b>...", "snippet": "As described in [16], PU <b>learning</b> methods <b>can</b> be divided into three classes: a) the first, called two-step strategy, tries to identify some reliable negative examples in the <b>unlabeled</b> data, and ...", "dateLastCrawled": "2021-10-27T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Fine-Tuned BERT-Based Transfer <b>Learning</b> Approach for Text Classification", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8742153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8742153", "snippet": "Transfer <b>learning</b> is a phenomenon or task in which the information gained from <b>unlabeled</b> data <b>can</b> be used in relative tasks with a small labeled dataset. And that small labeled dataset achieves high accuracy with the help of previous information. NLP transformers have gained promising accuracy in every practice as <b>compared</b> to ML and DL techniques. They have written in their research that the key idea behind TL is to grab information from related areas to help systems based on machine ...", "dateLastCrawled": "2022-01-17T11:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Distantly Supervised Named Entity Recognition using Positive-Unlabeled</b> ...", "url": "https://deepai.org/publication/distantly-supervised-named-entity-recognition-using-positive-unlabeled-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>distantly-supervised-named-entity-recognition</b>-using...", "snippet": "With this consideration, we propose to formulate the task as a positive-<b>unlabeled</b> (PU) <b>learning</b> problem and accordingly introduce a novel PU <b>learning</b> algorithm to perform the task. In our proposed method, the labeled entity words form the positive (P) data and the rest form the <b>unlabeled</b> (U) data for PU <b>learning</b>. We proved that the proposed algorithm <b>can</b> unbiasedly and consistently estimate the task loss as if there is fully labeled data, under the assumption that the labeled P data <b>can</b> ...", "dateLastCrawled": "2021-12-24T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning unsupervised embeddings for textual similarity</b> with ...", "url": "https://www.depends-on-the-definition.com/unsupervised-text-embeddings-with-transformers/", "isFamilyFriendly": true, "displayUrl": "https://www.depends-on-the-definition.com/unsupervised-text-embeddings-with-transformers", "snippet": "In this article, we look at SimCSE, a simple contrastive <b>sentence</b> embedding framework, which <b>can</b> be used to produce superior <b>sentence</b> embeddings, from either <b>unlabeled</b> or labeled data. The idea behind the unsupervised SimCSE is to simply predicts the input <b>sentence</b> itself, with only dropout used as noise. The same input <b>sentence</b> is passed to the pre-trained encoder twice and obtain two embeddings as \u201cpositive pairs\u201d, by applying independently sampled dropout masks. Due to the dropout ...", "dateLastCrawled": "2022-01-25T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "From Exemplar to Grammar: A Probabilistic Analogy\u2010Based Model of ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2009.01031.x", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2009.01031.x", "snippet": "As a first approximation we will limit the set of all possible trees to <b>unlabeled</b> binary trees. However, we <b>can</b> easily relax the binary restriction, and we will briefly come back <b>to learning</b> category labels at the end of this paper. Conceptually, we <b>can</b> distinguish three <b>learning</b> phases under U-DOP (though we will see that U-DOP operates rather differently from a computational point of view): 1 . Assign all possible (<b>unlabeled</b> binary) trees to a set of given sentences. 2 . Divide the binary ...", "dateLastCrawled": "2021-08-18T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Explanation of BERT Model - NLP - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/explanation-of-bert-model-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/explanation-of-bert-model-nlp", "snippet": "For <b>Example</b>, the paper achieves great results just by using a single layer NN on the BERT model in the classification task. ELMo Word Embeddings: This article is good for recapping Word Embedding. It also discusses Word2Vec and its implementation. Basically, word Embeddings for a word is the projection of a word to a vector of numerical values based on its meaning. There are many popular words Embedding such as Word2vec, GloVe, etc. ELMo was different from these embeddings because it gives ...", "dateLastCrawled": "2022-02-02T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding Language using <b>XLNet</b> with autoregressive pre-training ...", "url": "https://medium.com/@zxiao2015/understanding-language-using-xlnet-with-autoregressive-pre-training-9c86e5bea443", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@zxiao2015/understanding-language-using-<b>xlnet</b>-with-autoregressive...", "snippet": "Language modeling is essentially predicting the next word in a <b>sentence</b> given previous words. These language modeling methods pretrain neural networks on large-scale <b>unlabeled</b> text corpora before ...", "dateLastCrawled": "2022-01-22T05:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "1. The Basics - Natural <b>Language Annotation for Machine Learning</b> [Book]", "url": "https://www.oreilly.com/library/view/natural-language-annotation/9781449332693/ch01.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/natural-language-annotation/9781449332693/ch01.html", "snippet": "Two well-known corpora <b>can</b> <b>be compared</b> for their effort to balance the content of the texts. The Penn TreeBank (Marcus et al. 1993) is a ... Use the NLTK tagger to assign POS tags to the <b>example</b> <b>sentence</b> shown here, and then with other sentences that might be more ambiguous: &gt;&gt;&gt; from nltk import pos_tag, word_tokenize &gt;&gt;&gt; pos_tag(word_tokenize(&quot;This is a test.&quot;)) Look for places where the tagger doesn\u2019t work, and think about what rules might be causing these errors. For <b>example</b>, what ...", "dateLastCrawled": "2022-02-01T18:07:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Types of <b>Machine</b> <b>Learning</b>. How do machines learn? There are many\u2026 | by ...", "url": "https://medium.com/@sameerkhan9/types-of-machine-learning-b046528d65f3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@sameerkhan9/types-of-<b>machine</b>-<b>learning</b>-b046528d65f3", "snippet": "Unsupervised <b>learning</b> is a type of <b>learning</b> in which the <b>machine</b> must infer the function of input and outputs with <b>unlabeled</b> data. It is given data but the job of the <b>machine</b> is to figure out some ...", "dateLastCrawled": "2021-11-23T16:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>The Types Of Machine Learning | Let\u2019s Understand Part</b> 2. \u2013 ML for Lazy", "url": "https://mlforlazy.in/the-types-of-machine-learning-lets-understand-part-2/", "isFamilyFriendly": true, "displayUrl": "https://mlforlazy.in/<b>the-types-of-machine-learning-lets-understand-part</b>-2", "snippet": "The <b>analogy</b>. In layman\u2019s language or in words that are easy to understand, semi-supervised <b>learning</b> is like supervising a student for a short amount of time and then letting him go and wander the field independently. It solves classification problems. That means that you will need some supervised parts. Then at the same time, you have to train the model on large datasets of unlabelled data, for which you need the unsupervised part of <b>machine</b> <b>learning</b>. The central concept is to cluster ...", "dateLastCrawled": "2022-01-09T08:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine Learning Tutorial</b> - Learn <b>Machine Learning</b> - Intellipaat", "url": "https://intellipaat.com/blog/tutorial/machine-learning-tutorial/", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/blog/tutorial/<b>machine-learning-tutorial</b>", "snippet": "Now, let\u2019s apply the same <b>analogy</b> to a <b>machine</b>. Let\u2019s say we feed in different images of apples to the <b>machine</b> and all of these images have the label \u201capple\u201d associated with them. Similarly, we will feed in different images of oranges to the <b>machine</b> and all of these images would have the label \u201corange\u201d associated with them. So, here we are feeding in input data to the <b>machine</b> which is labeled. So, this part in supervised <b>learning</b>, where the <b>machine</b> learns all the features of the ...", "dateLastCrawled": "2022-02-02T15:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How does <b>Machine Learning</b> work?. What? Is that Wall-E? I guess you ...", "url": "https://medium.datadriveninvestor.com/what-is-machine-learning-and-how-does-it-work-ea4b2a6b1d32", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/what-is-<b>machine-learning</b>-and-how-does-it-work-ea...", "snippet": "Time for a bizarre attempt at an <b>analogy</b> for <b>machine learning</b> methods\u2026 There are three main methods of <b>machine learning</b>. Supervised <b>Learning</b>; Unsupervised <b>Learning</b>; Reinforcement <b>Learning</b>; For simplicity\u2019s sake, let\u2019s think of each of these methods as students at a school. And if we were to be considered a coder trying to code an ML program to tackle a certain problem, we could call ourselves teachers, in charge of teaching these students how to succeed at solving some problems. \ud83d\udc68 ...", "dateLastCrawled": "2022-01-28T02:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Unsupervised <b>Machine</b> <b>Learning</b>: Examples and Use Cases | <b>AltexSoft</b>", "url": "https://www.altexsoft.com/blog/unsupervised-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>altexsoft</b>.com/blog/unsupervised-<b>machine</b>-<b>learning</b>", "snippet": "Unsupervised <b>machine</b> <b>learning</b> is the process of inferring underlying hidden patterns from historical data. Within such an approach, a <b>machine</b> <b>learning</b> model tries to find any similarities, differences, patterns, and structure in data by itself. No prior human intervention is needed. Let\u2019s get back to our <b>example</b> of a child\u2019s experiential <b>learning</b>. Picture a toddler. The child knows what the family cat looks like (provided they have one) but has no idea that there are a lot of other cats ...", "dateLastCrawled": "2022-02-03T02:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning from positive</b> and <b>unlabeled</b> data: a survey - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "snippet": "<b>Learning from positive</b> and <b>unlabeled</b> data or PU <b>learning</b> is the setting where a learner only has access to positive examples and <b>unlabeled</b> data. The assumption is that the <b>unlabeled</b> data can contain both positive and negative examples. This setting has attracted increasing interest within the <b>machine</b> <b>learning</b> literature as this type of data naturally arises in applications such as medical diagnosis and knowledge base completion. This article provides a survey of the current state of the art ...", "dateLastCrawled": "2022-02-02T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is <b>Machine</b> <b>Learning</b>? \u2014 Learned from K-Drama Start-Up | by Richardy ...", "url": "https://richardylobosapan.medium.com/what-is-machine-learning-learned-from-k-drama-start-up-a1328882808d", "isFamilyFriendly": true, "displayUrl": "https://richardylobosapan.medium.com/what-is-<b>machine</b>-<b>learning</b>-learned-from-k-drama...", "snippet": "<b>Machi n e</b> <b>learning</b> is just like Tarzan in Do-San\u2019s <b>analogy</b>. The process of <b>learning</b> of Tarzan begins with observations of data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that provided. The primary aim is to allow the computers, in this case Tarzan, to learn automatically without human intervention or assistance and adjust actions accordingly. A question then arises, why is <b>machine</b> ...", "dateLastCrawled": "2022-01-07T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Starting Your Journey to Master <b>Machine Learning</b> with Python | by ...", "url": "https://towardsdatascience.com/starting-your-journey-to-master-machine-learning-with-python-d0bd47ebada9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/starting-your-journey-to-master-<b>machine-learning</b>-with...", "snippet": "What is <b>Machine Learning</b>? <b>Machine Learning</b> is the ability of a program to learn and improve its efficiency automatically without being explicitly programmed to do so. This means that given a training set you can train the <b>machine learning</b> model and it will understand how a model exactly works. Upon being tested on a test set, validation set, or any other unseen data, the model will still be able to evaluate the particular task. Let us understand this with a simple <b>example</b>. Assume we have a ...", "dateLastCrawled": "2022-01-25T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>Learning</b>: A Layman Overview \u2013 Academic Techie", "url": "http://academictechie.com/machine-learning-a-layman-overview/", "isFamilyFriendly": true, "displayUrl": "academictechie.com/<b>machine</b>-<b>learning</b>-a-layman-overview", "snippet": "For <b>example</b>, let us take the case of a <b>Machine</b> <b>Learning</b> algorithm used to play chess. ... For a very (a little too) fundamental <b>analogy</b>, imagine a teacher supervising a class. The teacher already knows the correct answers but the <b>learning</b> process doesn\u2019t stop until the students learn the answers as well. Supervised <b>Machine</b> <b>Learning</b> presently makes up a majority of the applications that are being utilized throughout the world. The input variable (x) is associated with the output variable (y ...", "dateLastCrawled": "2022-01-13T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Detecting Textual Analogies Using Semi-Supervised <b>Learning</b>", "url": "https://portfolios.cs.earlham.edu/wp-content/uploads/2018/12/Detecting-textual-analogies-Rei.pdf", "isFamilyFriendly": true, "displayUrl": "https://portfolios.cs.earlham.edu/wp-content/uploads/2018/12/Detecting-textual...", "snippet": "other <b>machine</b> <b>learning</b> techniques. <b>Analogy</b> is an essential aspect of human communication, understanding, and knowledge sharing. However, strategies for detecting textual analogies using machines are largely unexplored. There is also no standard corpus of textual analogies. This paper presents a system for detecting analogies in a given text using two semi supervised <b>learning</b> techniques; trans-ductive support vector machines (TSVMs) and label propagation. Count vectorization, tf-idf, and hash ...", "dateLastCrawled": "2021-10-17T14:11:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "2148 IEEE TRANSACTIONS ON NEURAL NETWORKS AND <b>LEARNING</b> SYSTEMS, VOL. 26 ...", "url": "http://www.kerenfu.top/sources/FLAP2015.pdf", "isFamilyFriendly": true, "displayUrl": "www.kerenfu.top/sources/FLAP2015.pdf", "snippet": "conventional <b>machine</b> <b>learning</b> algorithms. As a kind of iteration-based algorithm, it is proven that FLAP can converge more quickly than other iterative methods by analyzing the relationship between the convergence rate and the eigenvalues of the iteration matrix. We show that eigenvalues of the iteration matrix in FLAP are close to 1, while those in other methods may scatter in a wide range. This difference makes FLAP is superior to other iterative methods in terms of convergence speed. We ...", "dateLastCrawled": "2021-11-19T02:43:00.0000000Z", "language": "en", "isNavigational": false}]], "all_bing_queries": ["+(unlabeled example)  is like +(learning Spanish from a sentence)", "+(unlabeled example) is similar to +(learning Spanish from a sentence)", "+(unlabeled example) can be thought of as +(learning Spanish from a sentence)", "+(unlabeled example) can be compared to +(learning Spanish from a sentence)", "machine learning +(unlabeled example AND analogy)", "machine learning +(\"unlabeled example is like\")", "machine learning +(\"unlabeled example is similar\")", "machine learning +(\"just as unlabeled example\")", "machine learning +(\"unlabeled example can be thought of as\")", "machine learning +(\"unlabeled example can be compared to\")"]}