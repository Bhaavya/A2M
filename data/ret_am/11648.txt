{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What are Embeddings? How Do They Help AI Understand the Human ... - <b>Artezio</b>", "url": "https://www.artezio.com/pressroom/blog/what-are-embeddings-how-do-they-help-ai-understand-human-world/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>artezio</b>.com/pressroom/blog/what-are-<b>embeddings</b>-how-do-they-help-ai...", "snippet": "You will find an interactive acquaintance with the world of <b>three-dimensional</b> <b>space</b> with points corresponding to both normative acts and random textual narratives. If the link for any reason is inoperable, then have a look at the following image. The number of publications on the use of embeddings in the development of AI systems is increasing ...", "dateLastCrawled": "2022-02-02T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Trading spaces: building <b>three-dimensional</b> nets from two-dimensional ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3438570/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3438570", "snippet": "S 2 can only be embedded in <b>three-dimensional</b> <b>space</b> in one way: the two-dimensional sphere. In contrast, there are many simple embeddings of E 2 into <b>three-dimensional</b> <b>space</b>. The simplest is the usual <b>embedding</b> of the plane in E 3, extending without limit in two independent directions (figure 2 a). Alternatively, one or both of the two ...", "dateLastCrawled": "2021-09-10T16:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Embedding Space</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/embedding-space", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>embedding-space</b>", "snippet": "A number of <b>space</b>-filling curves other than the ones above have been proposed, with the goal of maintaining proximity in <b>space</b> also in the one-dimensional <b>embedding</b> the curve defines. Since the data structure based on a <b>space</b>-filling curve must adapt the partition pattern dynamically, <b>space</b>-filling curves usually have recursive definitions.", "dateLastCrawled": "2022-01-20T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Sabine Hossenfelder: Backreaction: What does the <b>universe</b> expand into ...", "url": "https://backreaction.blogspot.com/2021/05/what-does-universe-expand-into-do-we.html", "isFamilyFriendly": true, "displayUrl": "https://backreaction.blogspot.com/2021/05/what-does-<b>universe</b>-expand-into-do-we.html", "snippet": "That <b>three dimensional</b> <b>space</b> is called the \u201c<b>embedding</b> <b>space</b>\u201d. The <b>embedding</b> <b>space</b> itself is flat, it doesn\u2019t have curvature. If you embed the sphere, you immediately see that it\u2019s curved. But that\u2019s NOT how it works in general relativity. In general relativity we are asking how we can find out what the curvature of <b>space</b>-time is, while living inside it. There\u2019s no outside. There\u2019s no <b>embedding</b> <b>space</b>. So, for the sphere that\u2019d mean, we\u2019d have to ask how\u2019d we find out it ...", "dateLastCrawled": "2022-01-28T18:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Caging Loops in Shape <b>Embedding</b> <b>Space</b>: Theory and Computation | DeepAI", "url": "https://deepai.org/publication/caging-loops-in-shape-embedding-space-theory-and-computation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/caging-loops-in-shape-<b>embedding</b>-<b>space</b>-theory-and...", "snippet": "A caging loop is a closed curve in <b>three dimensional</b> <b>space</b> computed around some part of the target object and used to guide robot grippers to form a caging grasp. Existing methods on 3D caging grasp are based either on the geometric (e.g. ) or the topological (e.g. ) information of the target surface, or even both . A common issue to these methods is that the computed caging curves seriously depend on topological and geometrical features of objects, while being oblivious to the relative size ...", "dateLastCrawled": "2021-12-11T14:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "general relativity - Visualizing wormholes without <b>embedding</b> spacetime ...", "url": "https://physics.stackexchange.com/questions/639451/visualizing-wormholes-without-embedding-spacetime", "isFamilyFriendly": true, "displayUrl": "https://<b>physics.stackexchange</b>.com/.../visualizing-wormholes-without-<b>embedding</b>-<b>space</b>time", "snippet": "Just imagine ordinary <b>three-dimensional</b> <b>space</b>, and &quot;cut out&quot; two spherical regions of equal size. Then identify the surface of one sphere with the other. That is, proclaim that anything that enters one sphere immediately emerges out of the opposite side of the other....each sphere is one of the mouths of a wormhole. This is a wormhole of precisely zero length; if you enter one sphere, you instantly emerge out of the other (The word", "dateLastCrawled": "2022-01-23T02:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Embedding</b> in Machine Learning | Cathy\u2019s Notes", "url": "https://cathyqian.github.io/2020/08/27/Embedding_In_Machine_Learning.html", "isFamilyFriendly": true, "displayUrl": "https://cathyqian.github.io/2020/08/27/<b>Embedding</b>_In_Machine_Learning.html", "snippet": "<b>Embedding</b> in the context of deep learning is to map high-dimensional vectors or categorical variables to relatively low-dimensional <b>space</b> so that similar items are close to each other. It can be applied to any high dimensional, sparse or categorical features, i.e., IP addresses from ad impressions, raw pixels in a video, audio data from speech, texts from a job description.", "dateLastCrawled": "2022-01-22T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Hyperbolic Geometry and</b> Poincar\u00e9 Embeddings | Bounded Rationality", "url": "https://bjlkeng.github.io/posts/hyperbolic-geometry-and-poincare-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://bjlkeng.github.io/posts/<b>hyperbolic-geometry-and</b>-poincare-<b>embeddings</b>", "snippet": "In special relativity, dimension 1 is considered the &quot;time&quot;-<b>like</b> dimension while the others are &quot;<b>space</b>&quot;-<b>like</b> dimensions. But I don&#39;t think this helps with the intuition all that much. For now, we just need to know that one of the dimensions is treated differently, while the others are very similar to our regular Euclidean <b>space</b>. Hyperboloid . The next thing we need to set up is the hyperboloid. A hyperboloid is a generalization of a hyperbola in two dimensions. If you take a hyperbola and ...", "dateLastCrawled": "2022-01-31T04:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>2D space embedded in 3D space</b> | Physics Forums", "url": "https://www.physicsforums.com/threads/2d-space-embedded-in-3d-space.939440/", "isFamilyFriendly": true, "displayUrl": "https://www.physicsforums.com/threads/<b>2d-space-embedded-in-3d-space</b>.939440", "snippet": "The points making up the two-dimensional sphere are a subset of the points that make up the <b>three-dimensional</b> <b>space</b>, but the tangent <b>space</b> is a completely different mathematical object. It&#39;s a vector <b>space</b> and its elements are vectors, not points in the 3D <b>space</b>. This might also be a good time to mention that we don&#39;t need the 3D <b>space</b> at all. Mathematically what we have is a &quot;manifold&quot;, which is a set of points, a mapping from those points to pairs (and &quot;pairs&quot; is why say it&#39;s a two ...", "dateLastCrawled": "2022-01-15T06:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to prove that <b>the projective plane cannot embed into</b> the 3 ... - Quora", "url": "https://www.quora.com/How-do-I-prove-that-the-projective-plane-cannot-embed-into-the-3-dimensional-Euclidean-space", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-I-prove-that-<b>the-projective-plane-cannot-embed-into</b>-the-3...", "snippet": "Answer (1 of 3): Suppose there is such an <b>embedding</b>. The normal bundle to \\mathbb{P}^2 is non-trivial since otherwise the unit normal could be used to provide orientation to \\mathbb{P}^2, which is a non-orientable manifold. Consider the tubular neighborhood TU of \\mathbb{P}^2 inside the non-tri...", "dateLastCrawled": "2022-01-29T21:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Embedding Space</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/embedding-space", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>embedding-space</b>", "snippet": "If the text <b>embedding</b> has been learned correctly, the distance between the projections of dog and scarab tags in the text <b>embedding space</b> should be bigger than the one between dog and cat tags, but smaller than the one between other pairs not related at all. If the CNN has correctly learned to embed the images of those animals in the text <b>embedding space</b>, the distance between the dog and the cat image embeddings should be <b>similar</b> than the one between their tags embeddings (and the same for ...", "dateLastCrawled": "2022-01-20T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Graph <b>Embedding</b>: Understanding Graph <b>Embedding</b> Algorithms", "url": "https://www.tigergraph.com/blog/understanding-graph-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.tigergraph.com/blog/<b>understanding-graph-embeddings</b>", "snippet": "But in the <b>three-dimensional</b> <b>space</b> version, the distance calculation adds an additional term for the hight or z-axis. Distance calculations in an <b>embedding</b> work in <b>similar</b> ways. The key is that instead of just two or three dimensions, we may have 200 or 300 dimensions. The only difference is the addition of one distance term for each new dimension. Word <b>Embedding</b> Analogies. Much of the knowledge we have gained in the <b>graph embeddings</b> movement has come from the world of natural language ...", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Manifold learning: an informal introduction", "url": "https://fdresearchblog.files.wordpress.com/2019/02/informal-introduction-to-manifold-learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://fdresearchblog.files.wordpress.com/2019/02/informal-introduction-to-manifold...", "snippet": "Euclidean <b>space</b>, and a cube <b>is similar</b> <b>to three-dimensional</b> Euclidean <b>space</b>. Figure 5: An <b>embedding</b> of a manifold in two-dimensional Euclidean <b>space</b>, using Principal Component Analysis. The manifold on the left is already em-bedded into <b>three-dimensional</b> Euclidean <b>space</b>, but PCA is able to nd a two-", "dateLastCrawled": "2022-01-19T21:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What are Embeddings? How Do They Help AI Understand the Human ... - <b>Artezio</b>", "url": "https://www.artezio.com/pressroom/blog/what-are-embeddings-how-do-they-help-ai-understand-human-world/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>artezio</b>.com/pressroom/blog/what-are-<b>embeddings</b>-how-do-they-help-ai...", "snippet": "You will find an interactive acquaintance with the world of <b>three-dimensional</b> <b>space</b> with points corresponding to both normative acts and random textual narratives. If the link for any reason is inoperable, then have a look at the following image. The number of publications on the use of embeddings in the development of AI systems is increasing ...", "dateLastCrawled": "2022-02-02T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Embedding</b> in Machine Learning | Cathy\u2019s Notes", "url": "https://cathyqian.github.io/2020/08/27/Embedding_In_Machine_Learning.html", "isFamilyFriendly": true, "displayUrl": "https://cathyqian.github.io/2020/08/27/<b>Embedding</b>_In_Machine_Learning.html", "snippet": "<b>Embedding</b> in the context of deep learning is to map high-dimensional vectors or categorical variables to relatively low-dimensional <b>space</b> so that <b>similar</b> items are close to each other. It can be applied to any high dimensional, sparse or categorical features, i.e., IP addresses from ad impressions, raw pixels in a video, audio data from speech, texts from a job description.", "dateLastCrawled": "2022-01-22T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>2D space embedded in 3D space</b> | Physics Forums", "url": "https://www.physicsforums.com/threads/2d-space-embedded-in-3d-space.939440/", "isFamilyFriendly": true, "displayUrl": "https://www.physicsforums.com/threads/<b>2d-space-embedded-in-3d-space</b>.939440", "snippet": "However, you can&#39;t do something <b>similar</b> with the tangent <b>space</b>, because the tangent <b>space</b> at a given point isn&#39;t a submanifold of the <b>three-dimensional</b> <b>space</b>. Indeed, the term &quot;tangent ##\\it{plane}##&quot; is something of a misnomer - the tangent <b>space</b> at a given point can only be represented as a plane if you&#39;re drawing a picture in which your curved manifold is embedded in a higher-dimensional flat <b>space</b>. And in that case the relationship between coordinates on the plane and coordinates in the ...", "dateLastCrawled": "2022-01-15T06:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Skinning a Parameterization of <b>Three-Dimensional</b> <b>Space</b> for Neural ...", "url": "http://physbam.stanford.edu/~fedkiw/papers/stanford2020-03.pdf", "isFamilyFriendly": true, "displayUrl": "physbam.stanford.edu/~fedkiw/papers/stanford2020-03.pdf", "snippet": "4 <b>Embedding</b> cloth in the KDSM In continuum mechanics, deformation is de\ufb01ned as a mapping from a material <b>space</b> to the world <b>space</b>, and one typically decomposes this mapping into purely rigid components and geometric strain measures, see e.g. [12]. <b>Similar</b> in spirit, we envision the T-pose KDSM as the material <b>space</b> and", "dateLastCrawled": "2022-02-01T11:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Graph <b>Representation</b> Learning \u2014 Network Embeddings (Part 1) | by ...", "url": "https://towardsdatascience.com/graph-representation-learning-network-embeddings-d1162625c52b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/graph-<b>representation</b>-learning-network-<b>embeddings</b>-d...", "snippet": "The properties of the graph have to be preserved in the <b>embedding</b> <b>space</b>: for instance, nodes characterized by <b>similar</b> connections in the original graph achieve a close vector <b>representation</b> at the end of the learning process (in a few lines, we will see how this is correct specifically for structural embeddings). From another perspective, node <b>embedding</b> learning can be viewed as a dimensionality reduction process useful for scalability purposes. Indeed, the dimension of the learned vectors ...", "dateLastCrawled": "2022-01-30T00:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Superspace Groups without the Embedding</b>: The Link between Superspace ...", "url": "https://ui.adsabs.harvard.edu/abs/1996PhRvL..76.1489D/abstract", "isFamilyFriendly": true, "displayUrl": "https://ui.adsabs.harvard.edu/abs/1996PhRvL..76.1489D/abstract", "snippet": "The symmetry classification of <b>three-dimensional</b> periodic or aperiodic crystals is given a coordinate independent formulation which establishes the precise connection between Fourier-<b>space</b> and superspace crystallography. Superspace groups emerge without having to embed an aperiodic crystal in a higher-dimensional <b>space</b>.", "dateLastCrawled": "2021-03-15T13:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Deformation Embedding for Point-Based Elastoplastic Simulation</b>", "url": "https://cal.cs.umbc.edu/Papers/Jones-2014-DEF/Jones-2014-DEF.pdf", "isFamilyFriendly": true, "displayUrl": "https://cal.cs.umbc.edu/Papers/Jones-2014-DEF/Jones-2014-DEF.pdf", "snippet": "general, have a zero-stress state that can be realized in <b>three-dimensional</b> <b>space</b>. Put another way, the rest <b>space</b> is not embed-dable in <b>three-dimensional</b> <b>space</b>. This fact poses a challenge be-cause elastic forces depend on a mapping from rest <b>space</b> to de-formed or world <b>space</b>. Some authors have addressed this chal- lenge by keeping plastic offsets from an initial rest state, but this places limitations on the amount of plastic deformation that is pos-sible. Others have abandoned the rest ...", "dateLastCrawled": "2022-01-06T04:39:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Embedding Space</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/embedding-space", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>embedding-space</b>", "snippet": "If the CNN has correctly learned to embed the images of those animals in the text <b>embedding space</b>, the distance between the dog and the cat image embeddings should be similar than the one between their tags embeddings (and the same for any pair). So the point given by the pair should fall in the identity line. Furthermore, that distance should be closer to the coordinates origin than the point given by the dog and scarab pair, which should also fall in the identity line and nearer to the ...", "dateLastCrawled": "2022-01-20T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Graph <b>Embedding</b>: Understanding Graph <b>Embedding</b> Algorithms", "url": "https://www.tigergraph.com/blog/understanding-graph-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.tigergraph.com/blog/<b>understanding-graph-embeddings</b>", "snippet": "The problem also <b>can</b> be generalized into three dimensions for two points in <b>space</b>. But in the <b>three-dimensional</b> <b>space</b> version, the distance calculation adds an additional term for the hight or z-axis. Distance calculations in an <b>embedding</b> work in similar ways. The key is that instead of just two or three dimensions, we may have 200 or 300 ...", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Sabine Hossenfelder: Backreaction: What does the <b>universe</b> expand into ...", "url": "https://backreaction.blogspot.com/2021/05/what-does-universe-expand-into-do-we.html", "isFamilyFriendly": true, "displayUrl": "https://backreaction.blogspot.com/2021/05/what-does-<b>universe</b>-expand-into-do-we.html", "snippet": "That <b>three dimensional</b> <b>space</b> is called the \u201c<b>embedding</b> <b>space</b>\u201d. The <b>embedding</b> <b>space</b> itself is flat, it doesn\u2019t have curvature. If you embed the sphere, you immediately see that it\u2019s curved. But that\u2019s NOT how it works in general relativity. In general relativity we are asking how we <b>can</b> find out what the curvature of <b>space</b>-time is, while living inside it. There\u2019s no outside. There\u2019s no <b>embedding</b> <b>space</b>. So, for the sphere that\u2019d mean, we\u2019d have to ask how\u2019d we find out it ...", "dateLastCrawled": "2022-01-28T18:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A journey <b>through multiple dimensions and transformations in</b> <b>SPACE</b> | by ...", "url": "https://medium.com/artists-and-machine-intelligence/a-journey-through-multiple-dimensions-and-transformations-in-space-the-final-frontier-d8435d81ca51", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artists-and-machine-intelligence/a-journey-through-multiple...", "snippet": "The piece of paper itself occupies <b>three-dimensional</b> <b>space</b>, ... the <b>thought</b> vectors from the romance novels occupy a different location of the <b>thought</b> <b>embedding</b> <b>space</b> to the <b>thought</b> vectors from ...", "dateLastCrawled": "2021-08-01T09:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "general relativity - Visualizing wormholes without <b>embedding</b> spacetime ...", "url": "https://physics.stackexchange.com/questions/639451/visualizing-wormholes-without-embedding-spacetime", "isFamilyFriendly": true, "displayUrl": "https://<b>physics.stackexchange</b>.com/.../visualizing-wormholes-without-<b>embedding</b>-<b>space</b>time", "snippet": "In 2d you <b>can</b> imagine two separate black holes in a flat <b>space</b> by making the <b>space</b> grid of both holes vary in time, so matter is pulled in by both holes. Now a wormhole is the constant time solution of the extended Schwarzschild metric, in which a white hole <b>can</b> be present (the white hole is a time-reversed black hole which exists only in theory). Everything leaves a white hole (instead of nothing leaving a black hole). But which of the two holes in the wormhole is the black one and which ...", "dateLastCrawled": "2022-01-23T02:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Math Camp 1: Functional analysis", "url": "https://www.mit.edu/~9.520/spring09/Classes/mathcamp01.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mit.edu/~9.520/spring09/Classes/mathcamp01.pdf", "snippet": "Function <b>space</b> A function <b>space</b> is a <b>space</b> made of functions. Each function in the <b>space</b> <b>can</b> <b>be thought</b> of as a point. Ex-amples: 1. C[a,b], the set of all real-valued continuous functions in the interval [a,b]; 2. L1[a,b], the set of all real-valued functions whose ab-solute value is integrable in the interval [a,b]; 3. L2[a,b], the set of all real-valued functions square inte-grable in the interval [a,b] Note that the functions in 2 and 3 are not necessarily continuous! Metric <b>space</b> By a ...", "dateLastCrawled": "2022-02-02T08:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "differential geometry - <b>Can</b> general relativity be explained by ...", "url": "https://physics.stackexchange.com/questions/267916/can-general-relativity-be-explained-by-equations-describing-a-fabric-of-space-em", "isFamilyFriendly": true, "displayUrl": "https://physics.stackexchange.com/questions/267916", "snippet": "For example, the large scale homogeneous/ isotropic universe defined by the Friedmann\u2013Lema\u00eetre\u2013Robertson\u2013Walker (FLRW) metric <b>can</b> indeed <b>be thought</b> of as being embedded in five dimensional <b>space</b>, with signature $(1,\\,4)$, because it <b>can</b> be partitioned into foliations of <b>space</b>, with the foliations indexed by a universal time co-ordinate, and the spatial foliations are isometric to <b>three dimensional</b> spheres / hyperboloids in $\\mathbb{R}^4$ with only the scale factor and energy ...", "dateLastCrawled": "2022-01-21T04:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>2D space embedded in 3D space</b> | Physics Forums", "url": "https://www.physicsforums.com/threads/2d-space-embedded-in-3d-space.939440/", "isFamilyFriendly": true, "displayUrl": "https://www.physicsforums.com/threads/<b>2d-space-embedded-in-3d-space</b>.939440", "snippet": "<b>2D space embedded in 3D space</b>. In trying to explain the concept of curved <b>space</b>, many books use the example of the surface of a sphere, which <b>can</b> be considered as a curved 2D <b>space</b> embedded in a higher dimensional, 3D <b>space</b>. I could derive, starting from , that the metric, or the line element, on the surface of the sphere - which we now ...", "dateLastCrawled": "2022-01-15T06:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "terminology - Is there such thing as a &quot;3-<b>dimensional</b> surface ...", "url": "https://math.stackexchange.com/questions/3315735/is-there-such-thing-as-a-3-dimensional-surface", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/3315735/is-there-such-thing-as-a-3...", "snippet": "When I see things that I know to be wrong, I seek to fix them (though it <b>can</b> take a long time to get to it). I <b>thought</b> it would be interesting to, in a case where I&#39;m not sure, look for a consensus among the community in this way. The usage of the phrase in question refers to a surface embedded in $3$-<b>dimensional</b> <b>space</b>. In particular, an exhibit in which one <b>can</b> view $2$-<b>dimensional</b> solutions to equations in $3$ variables is advertised as: &quot;Bring formulas to life by exploring the multiple ...", "dateLastCrawled": "2022-01-24T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>can</b> <b>space</b> time be curved if <b>space time is three dimensional</b>? - Quora", "url": "https://www.quora.com/How-can-space-time-be-curved-if-space-time-is-three-dimensional", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-<b>space</b>-time-be-curved-if-<b>space-time-is-three-dimensional</b>", "snippet": "Answer (1 of 8): OK. Time for a quick lesson on the concept of intrinsic curvature. Let\u2019s begin. The surface of a sphere has no depth so it is determined by two numbers, such as latitude and longitude. So it is two dimensional. It\u2019s clearly embedded in a <b>three dimensional</b> <b>space</b>, and curved in ...", "dateLastCrawled": "2022-01-13T18:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Embedding Space</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/embedding-space", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>embedding-space</b>", "snippet": "If the text <b>embedding</b> has been learned correctly, the distance between the projections of dog and scarab tags in the text <b>embedding space</b> should be bigger than the one between dog and cat tags, but smaller than the one between other pairs not related at all. If the CNN has correctly learned to embed the images of those animals in the text <b>embedding space</b>, the distance between the dog and the cat image embeddings should be similar than the one between their tags embeddings (and the same for ...", "dateLastCrawled": "2022-01-20T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Embedding</b> in Machine Learning | Cathy\u2019s Notes", "url": "https://cathyqian.github.io/2020/08/27/Embedding_In_Machine_Learning.html", "isFamilyFriendly": true, "displayUrl": "https://cathyqian.github.io/2020/08/27/<b>Embedding</b>_In_Machine_Learning.html", "snippet": "<b>Embedding</b> in the context of deep learning is to map high-dimensional vectors or categorical variables to relatively low-dimensional <b>space</b> so that similar items are close to each other. It <b>can</b> be applied to any high dimensional, sparse or categorical features, i.e., IP addresses from ad impressions, raw pixels in a video, audio data from speech, texts from a job description.", "dateLastCrawled": "2022-01-22T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Caging Loops in Shape <b>Embedding</b> <b>Space</b>: Theory and Computation | DeepAI", "url": "https://deepai.org/publication/caging-loops-in-shape-embedding-space-theory-and-computation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/caging-loops-in-shape-<b>embedding</b>-<b>space</b>-theory-and...", "snippet": "Caging Loops in Shape <b>Embedding</b> <b>Space</b>: Theory and Computation. 07/31/2018 \u2219 by Jian Liu, et al. \u2219 0 \u2219 share We propose to synthesize feasible caging grasps for a target object through computing Caging Loops, a closed curve defined in the shape <b>embedding</b> <b>space</b> of the object. Different from the traditional methods, our approach decouples caging loops from the surface geometry of target objects through working in the <b>embedding</b> <b>space</b>. This enables us to synthesize caging loops encompassing ...", "dateLastCrawled": "2021-12-11T14:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "On Dimensionality <b>of Embedding Space in Multidimensional Scaling</b> ...", "url": "https://www.deepdyve.com/lp/ios-press/on-dimensionality-of-embedding-space-in-multidimensional-scaling-m5RnvLKAoQ", "isFamilyFriendly": true, "displayUrl": "https://www.<b>deepdyve</b>.com/lp/ios-press/on-dimensionality-of-<b>embedding</b>-<b>space</b>-in...", "snippet": "Both artificial and practical data sets have been used. The images in <b>three-dimensional</b> <b>embedding</b> <b>space</b> normally show the structural properties of sets of considered objects with acceptable accuracy, and widening of applications of stereo screens makes <b>three-dimensional</b> visualization very attractive.", "dateLastCrawled": "2020-05-21T00:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Deep Audio <b>Embedding</b> - Complex Objects", "url": "https://m-lin-dm.github.io/Deep_audio_embedding/", "isFamilyFriendly": true, "displayUrl": "https://m-lin-dm.github.io/Deep_audio_<b>embedding</b>", "snippet": "I created an autoencoder-based, end-to-end architecture that 1. embeds audio as a trajectory in a low-dimensional <b>embedding</b> <b>space</b>, 2. encourages the encoder network to produce smoother trajectories, and 3. forecasts the next time-step in <b>embedding</b> <b>space</b>. I asked what effect (2.) would have on forecasting accuracy, when <b>compared</b> with traditional autoencoders.", "dateLastCrawled": "2021-12-15T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Sabine Hossenfelder: Backreaction: What does the <b>universe</b> expand into ...", "url": "https://backreaction.blogspot.com/2021/05/what-does-universe-expand-into-do-we.html", "isFamilyFriendly": true, "displayUrl": "https://backreaction.blogspot.com/2021/05/what-does-<b>universe</b>-expand-into-do-we.html", "snippet": "That <b>three dimensional</b> <b>space</b> is called the \u201c<b>embedding</b> <b>space</b>\u201d. The <b>embedding</b> <b>space</b> itself is flat, it doesn\u2019t have curvature. If you embed the sphere, you immediately see that it\u2019s curved. But that\u2019s NOT how it works in general relativity. In general relativity we are asking how we <b>can</b> find out what the curvature of <b>space</b>-time is, while living inside it. There\u2019s no outside. There\u2019s no <b>embedding</b> <b>space</b>. So, for the sphere that\u2019d mean, we\u2019d have to ask how\u2019d we find out it ...", "dateLastCrawled": "2022-01-28T18:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Topology of <b>Three Dimensional</b> Manifolds and the <b>Embedding</b> Problems in ...", "url": "https://www.jstor.org/stable/1971088", "isFamilyFriendly": true, "displayUrl": "https://www.jstor.org/stable/1971088", "snippet": "Cyclic group actions on <b>three dimensional</b> euclidean <b>space</b> ..... 474 9. The splitting theorem for finite groups acting on a <b>three dimensional</b> manifold ..... 479 References ..... 483 In [21], the authors observed that the topological methods in the theory of <b>three dimensional</b> manifolds <b>can</b> be modified to settle some old problems in the classical theory of minimal surfaces in euclidean <b>space</b>. While we were mainly interested in minimal surfaces in that paper, we found that the theory of minimal ...", "dateLastCrawled": "2022-01-22T00:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Cross-modal semantic autoencoder with <b>embedding</b> consensus | Scientific ...", "url": "https://www.nature.com/articles/s41598-021-92750-7", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-92750-7", "snippet": "The methods are <b>compared</b> on the WIKI dataset. It <b>can</b> be observed from Table ... By mapping the image and text to the <b>embedding</b> consensus <b>space</b>, CSAEC <b>can</b> contain enough raw data information. \\(V ...", "dateLastCrawled": "2022-01-26T15:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Hyperbolic Geometry and</b> Poincar\u00e9 Embeddings | Bounded Rationality", "url": "https://bjlkeng.github.io/posts/hyperbolic-geometry-and-poincare-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://bjlkeng.github.io/posts/<b>hyperbolic-geometry-and</b>-poincare-<b>embeddings</b>", "snippet": "You <b>can</b> see that the Poincar\u00e9 <b>embedding</b> shows a massive improvement over the other methods using the average rank and mean average precision. Rank in this context means where did the actual link distance rank relative to all ground truth negative examples. Ideally it should be rank 1. With even as little as 5 dimensions, the Poincar\u00e9 embeddings massively outperform the two other distance functions with 200 dimension. This really shows the efficiency of the <b>embedding</b>.", "dateLastCrawled": "2022-01-31T04:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>can</b> the universe be flat <b>if it is three dimensional? - Quora</b>", "url": "https://www.quora.com/How-can-the-universe-be-flat-if-it-is-three-dimensional", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-the-universe-be-flat-<b>if-it-is-three-dimensional</b>", "snippet": "Answer (1 of 12): In this context, \u201cflat\u201d is not meant to be understood as \u201cflat like a pancake,\u201d but rather \u201cflat like Euclidean <b>space</b>,\u201d in the sense that there is no curvature\u2014if you measure distances, your measurements will agree with what the Pythagorean theorem tells you they should be. Cont...", "dateLastCrawled": "2022-01-30T12:30:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "snippet": "A suitable representation is therefore essential for the success of <b>analogy</b>-based <b>learning</b> to rank. Therefore, we propose a method for analogical <b>embedding</b>, i.e., for <b>embedding</b> the data in a target <b>space</b> such that, in this <b>space</b>, the aforementioned <b>analogy</b> assumption is as valid and strongly pronounced as possible. This is accomplished by means of a neural network with a quadruple Siamese structure, which is trained on a suitably designed set of examples in the form of quadruples of objects ...", "dateLastCrawled": "2022-01-17T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://homepages.uni-paderborn.de/ahmadim/IDA%202021.pdf", "isFamilyFriendly": true, "displayUrl": "https://homepages.uni-paderborn.de/ahmadim/IDA 2021.pdf", "snippet": "7 Intelligent Systems and <b>Machine</b> <b>Learning</b> <b>Embedding</b> By ignoring irrelevant or noisy features, the performance can often be improved Common feature selection techniques tailored for the case of <b>analogy</b>-based <b>learning</b> to rank. <b>Analogy</b>-based <b>learning</b> to rank (able2rank) 8 Intelligent Systems and <b>Machine</b> <b>Learning</b> Extension to feature vectors Degree of <b>analogy</b>. Analogical <b>Embedding</b> 9 Intelligent Systems and <b>Machine</b> <b>Learning</b> Positive example: preferences on both sides are coherent Negative ...", "dateLastCrawled": "2022-01-06T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Finding the Word <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-word-<b>analogy</b>-from-given-words-using-word2vec...", "snippet": "In the word <b>analogy</b> task, ... What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the word <b>analogy</b>. In this tutorial, we will be using Word2Vec model and a pre-trained model named \u2018GoogleNews-vectors-negative300.bin\u2018 which is trained on over 50 Billion words by Google. Each word inside the pre-trained dataset is embedded in a 300-dimensional <b>space</b> and the words which are similar in context/meaning are placed closer to each other in the <b>space</b>. Methodology to ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Graph <b>Embedding</b> for Deep <b>Learning</b> | by Flawnson Tong | Towards Data Science", "url": "https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/overview-of-deep-<b>learning</b>-on-graph-<b>embeddings</b>-4305c10ad4a4", "snippet": "Using an <b>analogy</b> with word2vec, if a document is made of sentences (which is then made of words), then a graph is made of sub-graphs ... Graph <b>embedding</b> techniques take graphs and embed them in a lower dimensional continuous latent <b>space</b> before passing that representation through a <b>machine</b> <b>learning</b> model. Walk <b>embedding</b> methods perform graph traversals with the goal of preserving structure and features and aggregates these traversals which can then be passed through a recurrent neural ...", "dateLastCrawled": "2022-02-01T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evaluating vector-<b>space</b> models of <b>analogy</b>", "url": "https://cocosci.princeton.edu/papers/vector_space_analogy_cogsci2017_final.pdf", "isFamilyFriendly": true, "displayUrl": "https://cocosci.princeton.edu/papers/vector_<b>space</b>_<b>analogy</b>_cogsci2017_final.pdf", "snippet": "Recent <b>machine</b> <b>learning</b> methods for deriving vector-<b>space</b> embeddings of words (e.g., word2vec) have achieved considerable success in natural language processing. These vector spaces have also been shown to exhibit a surprising ca-pacity to capture verbal analogies, with similar results for nat-ural images, giving new life to a classic model of analogies as parallelograms that was \ufb01rst proposed by cognitive scientists. We evaluate the parallelogram model of <b>analogy</b> as applied to modern word ...", "dateLastCrawled": "2021-09-24T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting ...", "url": "https://www.researchgate.net/figure/In-the-word-embedding-space-the-analogy-pairs-exhibit-interesting-algebraic_fig1_319370400", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/In-the-word-<b>embedding</b>-<b>space</b>-the-<b>analogy</b>-pairs...", "snippet": "Download scientific diagram | In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting algebraic relationships. from publication: Visual Exploration of Semantic Relationships in Neural ...", "dateLastCrawled": "2021-12-21T11:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-word2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, word <b>embedding</b> is used to map words into vectors of real numbers. There are various word <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce word embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector <b>space</b>, with each unique word in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "snippet": "With the emergence of word <b>embedding</b> models, a lot of progress has been made in NLP, essentially assuming that a word <b>analogy</b> like m a n: k i n g:: w o m a n: q u e e n is an instance of a parallelogram within the underlying vector <b>space</b>. In this paper, we depart from this assumption to adopt a <b>machine</b> <b>learning</b> approach, i.e., <b>learning</b> a substitute of the parallelogram model. To achieve our goal, we first review the formal modeling of analogical proportions, highlighting the properties which ...", "dateLastCrawled": "2021-11-13T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "DeepLearning <b>series: Natural Language Processing and Word Embeddings</b> ...", "url": "https://medium.com/machine-learning-bites/deeplearning-series-natural-language-processing-and-word-embeddings-70599080efc9", "isFamilyFriendly": true, "displayUrl": "https://medium.com/<b>machine</b>-<b>learning</b>-bites/deep<b>learning</b>-series-natural-language...", "snippet": "<b>Learning</b> word embeddings: When we implement an algorithm to learn word embeddings, what we end up <b>learning</b> is an <b>embedding</b> matrix. For a 300-feature <b>embedding</b> and a 10,000-word vocabulary, the ...", "dateLastCrawled": "2021-10-27T13:32:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Zero-shot <b>learning</b> via discriminative representation extraction ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "snippet": "The pioneer work in ZSL can be traced to Larochelle et al. , where it verified that when test images belong to some classes that are not available at training stage, a <b>machine</b> <b>learning</b> system can still figure out what a test image is. Due to the importance of zero-shot <b>learning</b>, the number of proposed approaches has increased steadily recently.The number of new zero-shot <b>learning</b> approaches proposed every year was increasing.", "dateLastCrawled": "2021-10-30T07:08:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A self-supervised domain-general <b>learning</b> framework for human ventral ...", "url": "https://www.nature.com/articles/s41467-022-28091-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-022-28091-4", "snippet": "On this view, the <b>embedding space can be thought of as</b> a high-fidelity perceptual interface, with useful visual primitives over which separate conceptual representational systems can operate.", "dateLastCrawled": "2022-01-25T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Spectral Af\ufb01ne-Kernel Embeddings</b> - NSF", "url": "https://par.nsf.gov/servlets/purl/10039348", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10039348", "snippet": "Since <b>machine</b> <b>learn-ing</b> algorithms struggle with high dimensions (an issue known as the curse of dimensionality in this context), one typically needs to map these data points from their high-dimensional space into a lower dimensional space without signi\ufb01cant distortion. Mapping data (living in RD with D\u02db1 but sampling a manifold of low in-trinsic dimensionality d \u02ddD) into a low-dimensional <b>embedding space can be thought of as</b> a preliminary feature extraction step in <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-29T21:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting affinity ties in a surname network", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "snippet": "<b>Machine</b> <b>learning</b>-based approaches for knowledge graph completion To cover the broadest possible range of methods and architectures in the evaluation, we identified representative methods of different model families, taking care that these methods achieve state-of-the-art performances in knowledge graph completion and have open-source implementations that favor the reproducibility of the reported results.", "dateLastCrawled": "2021-09-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(embedding space)  is like +(three-dimensional space)", "+(embedding space) is similar to +(three-dimensional space)", "+(embedding space) can be thought of as +(three-dimensional space)", "+(embedding space) can be compared to +(three-dimensional space)", "machine learning +(embedding space AND analogy)", "machine learning +(\"embedding space is like\")", "machine learning +(\"embedding space is similar\")", "machine learning +(\"just as embedding space\")", "machine learning +(\"embedding space can be thought of as\")", "machine learning +(\"embedding space can be compared to\")"]}