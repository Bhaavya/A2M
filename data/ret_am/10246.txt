{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding <b>Masked</b> <b>Language</b> Models (MLM) and Causal <b>Language</b> Models ...", "url": "https://towardsdatascience.com/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>masked</b>-<b>language</b>-<b>models</b>-mlm-and-causal...", "snippet": "<b>Masked</b> <b>Language</b> <b>Model</b> Explained. Under <b>Masked</b> <b>Lan g uage</b> Modelling, we typically mask a certain % of words in a given sentence and the <b>model</b> is expected to predict those <b>masked</b> words based on other words in that sentence. Such a training scheme makes this <b>model</b> bidirectional in nature because the representation of the <b>masked</b> word is learnt based on the words that occur it\u2019s left as well as right.You can also visualize this <b>like</b> a fill-in-the-blanks kind of a problem statement.. Below ...", "dateLastCrawled": "2022-02-02T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Masked-Language Modeling With BERT</b> | by James Briggs | Towards Data Science", "url": "https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>masked</b>-<b>language</b>-<b>model</b>ling-with-bert-7d49793e5d2c", "snippet": "BERT\u2019s bidirectional biceps \u2014 image by author. B ERT, everyone\u2019s favorite transformer costs Google ~$7K to train [1] (and who knows how much in R&amp;D costs). From there, we write a couple of lines of code to use the same <b>model</b> \u2014 all for free. BERT has enjoyed unparalleled success in NLP thanks to two unique training approaches, <b>masked</b>-<b>language</b> modeling (MLM), and next sentence prediction (NSP).", "dateLastCrawled": "2022-02-03T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Easy <b>Masked</b> <b>Language Modeling with Machine Learning</b> and HuggingFace ...", "url": "https://www.machinecurve.com/index.php/2021/03/02/easy-masked-language-modeling-with-machine-learning-and-huggingface-transformers/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2021/03/02/easy-<b>masked</b>-<b>language</b>-<b>model</b>ing-with...", "snippet": "Last Updated on 30 March 2021. <b>Masked</b> <b>Language</b> Modeling (MLM) is a <b>language</b> task very common in Transformer architectures today. It involves masking part of the input, then <b>learning</b> a <b>model</b> to predict the missing tokens \u2013 essentially reconstructing the non-<b>masked</b> input. MLM is often used within pretraining tasks, to give models the opportunity to learn textual patterns from unlabeled data.", "dateLastCrawled": "2022-01-31T01:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is <b>a masked language model, and how</b> is it related to BERT? - Quora", "url": "https://www.quora.com/What-is-a-masked-language-model-and-how-is-it-related-to-BERT", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>a-masked-language-model-and-how</b>-is-it-related-to-BERT", "snippet": "Answer (1 of 2): <b>Masked</b> <b>language</b> modeling is an example of autoencoding <b>language</b> modeling (the output is reconstructed from corrupted input) - we typically mask one or more of words in a sentence and have the <b>model</b> predict those <b>masked</b> words given the other words in sentence. By training the mode...", "dateLastCrawled": "2022-02-03T01:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is <b>BERT (Language Model</b>) and How Does It Work?", "url": "https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model", "isFamilyFriendly": true, "displayUrl": "https://www.techtarget.com/searchenterpriseai/definition/<b>BERT-language-model</b>", "snippet": "The objective of <b>Masked</b> <b>Language</b> <b>Model</b> (MLM) training is to hide a word in a sentence and then have the program predict what word has been hidden (<b>masked</b>) based on the hidden word&#39;s context. The objective of Next Sentence Prediction training is to have the program predict whether two given sentences have a logical, sequential connection or whether their relationship is simply random.", "dateLastCrawled": "2022-02-02T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Masked</b> <b>Language</b> Modeling | Papers With Code", "url": "https://paperswithcode.com/task/masked-language-modeling", "isFamilyFriendly": true, "displayUrl": "https://paperswithcode.com/task/<b>masked</b>-<b>language</b>-<b>model</b>ing", "snippet": "<b>Language</b> <b>model</b> pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. 4. Paper Code Talking-Heads Attention. tensorflow/models \u2022 \u2022 5 Mar 2020. We introduce &quot;talking-heads attention&quot; - a variation on multi-head attention which includes linearprojections across the attention-heads dimension, immediately before and after the softmax operation. While inserting only a small number of additional parameters and a moderate ...", "dateLastCrawled": "2022-02-06T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Larger-Scale Transformers for Multilingual <b>Masked</b> <b>Language</b> Modeling", "url": "https://aclanthology.org/2021.repl4nlp-1.4.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2021.repl4nlp-1.4.pdf", "snippet": "dataset (Wenzek et al.,2019). Our two <b>new</b> multi-lingual <b>masked</b> <b>language</b> <b>model</b> dubbed XLM-R XL and XLM-R XXL, with 3.5 and 10.7 billion parame-ters respectively, signi\ufb01cantly outperform the previ-ous XLM-R <b>model</b> on cross-lingual understanding benchmarks and obtain competitive performance with the multilingual T5 models (Raffel et al.,2019; Xue et al.,2020). We show that they can even out-perform RoBERTa-Large (Liu et al.,2019) on the GLUE benchmark (Wang et al.,2018). Recent multilingual ...", "dateLastCrawled": "2022-01-30T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Universal Sentence Representation <b>Learning</b> with Conditional <b>Masked</b> ...", "url": "https://aclanthology.org/2021.emnlp-main.502.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2021.emnlp-main.502.pdf", "snippet": "Universal Sentence Representation <b>Learning</b> with Conditional <b>Masked</b> <b>Language</b> <b>Model</b> Ziyi Yang1, Yinfei Yang 2, Daniel Cer , Jax Law , Eric Darve1 1 Stanford University {ziyi.yang,darve}@stanford.edu 2Google Research {yinfeiy,cer,jaxlaw}@google.com Abstract This paper presents a novel training method, Conditional <b>Masked</b> <b>Language</b> Modeling (CMLM), to effectively learn sentence repre-sentations on large scale unlabeled corpora. CMLM integrates sentence representation <b>learning</b> into MLM training by ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "nlp - What is the typical accuracy of <b>masked</b> <b>language</b> models during ...", "url": "https://datascience.stackexchange.com/questions/81792/what-is-the-typical-accuracy-of-masked-language-models-during-bert-pretraining", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/81792/what-is-the-typical-accuracy-of...", "snippet": "BERT is a subword <b>language</b> <b>model</b> (uses WordPiece tokenizer) so if you use large vocab then sentences are usually tokenized to &quot;smaller&quot; subword units that are easier to predict. I trained a BERT-<b>like</b> <b>model</b> for Czech <b>language</b>, which is morphologically more complex than English, and got top@1 test accuracy ~0.55, top@3 test accuracy ~0.68 after 1.5M steps with 30K vocab size.", "dateLastCrawled": "2022-01-19T18:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Building State-of-the-Art <b>Language</b> Models with <b>BERT</b> | by Ankit Singh ...", "url": "https://medium.com/saarthi-ai/bert-how-to-build-state-of-the-art-language-models-59dddfa9ac5d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/saarthi-ai/<b>bert</b>-how-to-build-state-of-the-art-<b>language</b>-<b>models</b>-59...", "snippet": "A highly unconventional method of training a <b>masked</b> <b>language</b> <b>model</b> is to randomly replace some percentage of words with [MASK] tokens. <b>BERT</b> is trained to do this, <b>like</b>, for every example, <b>BERT</b> ...", "dateLastCrawled": "2022-01-31T18:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>a masked language model, and how</b> is it related to BERT? - Quora", "url": "https://www.quora.com/What-is-a-masked-language-model-and-how-is-it-related-to-BERT", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>a-masked-language-model-and-how</b>-is-it-related-to-BERT", "snippet": "Answer (1 of 2): <b>Masked</b> <b>language</b> modeling is an example of autoencoding <b>language</b> modeling (the output is reconstructed from corrupted input) - we typically mask one or more of words in a sentence and have the <b>model</b> predict those <b>masked</b> words given the other words in sentence. By training the mode...", "dateLastCrawled": "2022-02-03T01:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "50+ <b>NLP Interview Questions and Answers</b> in 2022 - Great <b>Learning</b>", "url": "https://www.mygreatlearning.com/blog/nlp-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/blog/nlp-interview-questions", "snippet": "What is <b>Masked</b> <b>Language</b> <b>Model</b>? <b>Masked</b> <b>language</b> models help learners to understand deep representations in downstream tasks by taking an output from the corrupt input. This <b>model</b> is often used to predict the words to be used in a sentence. 9. What is the difference between NLP and CI(Conversational Interface)? The difference between NLP and CI is as follows: Natural <b>Language</b> Processing (NLP) Conversational Interface (CI) NLP attempts to help machines understand and learn how <b>language</b> concepts ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Transformer-based <b>Language</b> Models | The Ezra Tech Blog", "url": "https://techblog.ezra.com/an-overview-of-different-transformer-based-language-models-c9d3adafead8", "isFamilyFriendly": true, "displayUrl": "https://techblog.ezra.com/an-<b>overview-of-different-transformer-based-language-models</b>-c...", "snippet": "<b>Masked</b> <b>Language</b> <b>Model</b>. This task is used to allow for the <b>model</b>\u2019s bidirectionality. The standard conditional <b>language</b> models where the target word is predicted from the previous or next word do not allow for both left-to-right and right-to-left training. This is because with bidirectional conditioning the word indirectly sees itself. In order to achieve the bidirectional nature of the BERT <b>language</b> <b>model</b>, 15% of the training tokens are <b>masked</b> using a special [MASK] token. The <b>model</b> is ...", "dateLastCrawled": "2022-01-27T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Fine-tuning <b>Bert</b> <b>language</b> <b>model</b> to get better results on text ...", "url": "https://medium.com/analytics-vidhya/fine-tuning-bert-language-model-to-get-better-results-on-text-classification-3dac5e3c348e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/fine-tuning-<b>bert</b>-<b>language</b>-<b>model</b>-to-get-better...", "snippet": "It is a bidirectional transformer pre-trained <b>model</b> developed using a combination of two tasks namely: <b>masked</b> <b>language</b> modeling objective and next sentence prediction on a large corpus.", "dateLastCrawled": "2022-01-31T14:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Building State-of-the-Art <b>Language</b> Models with <b>BERT</b> | by Ankit Singh ...", "url": "https://medium.com/saarthi-ai/bert-how-to-build-state-of-the-art-language-models-59dddfa9ac5d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/saarthi-ai/<b>bert</b>-how-to-build-state-of-the-art-<b>language</b>-<b>models</b>-59...", "snippet": "A highly unconventional method of training a <b>masked</b> <b>language</b> <b>model</b> is to randomly replace some percentage of words with [MASK] tokens. <b>BERT</b> is trained to do this, like, for every example, <b>BERT</b> ...", "dateLastCrawled": "2022-01-31T18:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Larger-Scale Transformers for Multilingual <b>Masked</b> <b>Language</b> Modeling", "url": "https://aclanthology.org/2021.repl4nlp-1.4.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2021.repl4nlp-1.4.pdf", "snippet": "ing task of multilingual <b>masked</b> <b>language</b> <b>model</b>-30 ing. We show promising results for cross-lingual understanding: XLM-R XXL can both obtain <b>a new</b> state of the art on some cross-lingual understanding benchmarks and outperform the RoBERTa-Large <b>model</b> on the English GLUE benchmark (Wang et al.,2018). This suggests that very large-scale multilingual models may be able to bene\ufb01t from the best of both worlds: obtaining strong performance on high-resource languages while still allowing for zero ...", "dateLastCrawled": "2022-01-30T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "Recently, the pre-trained <b>language</b> <b>model</b>, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural <b>language</b> understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural <b>language</b> inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to <b>a new</b> <b>model</b>, StructBERT, by incorporating <b>language</b> structures into pre ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Tips and Tricks to Train State-Of-The-Art NLP Models - neptune.ai", "url": "https://neptune.ai/blog/tips-to-train-nlp-models", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/tips-to-train-nlp-<b>models</b>", "snippet": "<b>Masked</b> <b>Language</b> <b>Model</b> objective. When pre-trained with the <b>Masked</b> <b>Language</b> <b>Model</b> objective, the <b>model</b> is trained to predict a word based on its left and right context. To train with the MLM objective a small percentage of the input tokens is <b>masked</b> randomly i.e. the token to be <b>masked</b> is replaced with a special [MASK] token. Causal <b>Language</b> <b>Model</b> objective: When pre-trained with the <b>language</b> <b>model</b> objective, the <b>model</b> is trained to predict the next word in a sentence given the words in the ...", "dateLastCrawled": "2022-01-29T21:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "deep <b>learning</b> - Continual pre-training vs. Fine-tuning a <b>language</b> <b>model</b> ...", "url": "https://stackoverflow.com/questions/68461204/continual-pre-training-vs-fine-tuning-a-language-model-with-mlm", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/68461204", "snippet": "I have some custom data I want to use to further pre-train the BERT <b>model</b>. I\u2019ve tried the two following approaches so far: Starting with a pre-trained BERT checkpoint and continuing the pre-training with <b>Masked</b> <b>Language</b> Modeling (MLM) + Next Sentence Prediction (NSP) heads (e.g. using BertForPreTraining <b>model</b>)Starting with a pre-trained BERT <b>model</b> with the MLM objective (e.g. using the BertForMaskedLM <b>model</b> assuming we don\u2019t need NSP for the pretraining part.); But I\u2019m still confused ...", "dateLastCrawled": "2022-01-27T19:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>A NEW</b> <b>TYPE OF MASKED FORM PRIMING</b> | Studies in Second <b>Language</b> ...", "url": "https://www.cambridge.org/core/journals/studies-in-second-language-acquisition/article/new-type-of-masked-form-priming/046773B7CDE945439C77A6AF3390EBAC", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/studies-in-second-<b>language</b>-acquisition/article/...", "snippet": "Indeed, the self-rated English proficiency of each bilingual participant corresponded to one of the two highest levels of China\u2019s Standards of English <b>Language</b> Ability (National <b>Language</b> Standard GF 0018\u20132018), namely, level 8 or 9. The mean age of the bilinguals was 25.6 (SD = 6.2), with their age of English acquisition being 9.8 (SD = 2.61). All bilingual participants received \uffe520 cash for their participation.", "dateLastCrawled": "2022-01-27T21:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Universal Sentence Representation <b>Learning</b> with Conditional <b>Masked</b> ...", "url": "https://aclanthology.org/2021.emnlp-main.502.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2021.emnlp-main.502.pdf", "snippet": "Universal Sentence Representation <b>Learning</b> with Conditional <b>Masked</b> <b>Language</b> <b>Model</b> Ziyi Yang1, Yinfei Yang 2, Daniel Cer , Jax Law , Eric Darve1 1Stanford University {ziyi.yang,darve}@stanford.edu 2Google Research {yinfeiy,cer,jaxlaw}@google.com Abstract This paper presents a novel training method, Conditional <b>Masked</b> <b>Language</b> Modeling (CMLM), to effectively learn sentence repre-sentations on large scale unlabeled corpora. CMLM integrates sentence representation <b>learning</b> into MLM training by ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Emerging Cross-lingual Structure in Pretrained <b>Language</b> Models", "url": "https://aclanthology.org/2020.acl-main.536.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.acl-main.536.pdf", "snippet": "representation <b>learning</b> from a <b>language</b> <b>model</b>. The representations are used in a transfer <b>learning</b> setup to improve performance on a variety of down- stream NLP tasks. Follow-up work byHoward and Ruder(2018);Radford et al.(2018) further improves on this idea by \ufb01ne-tuning the entire <b>lan-guage</b> <b>model</b>. BERT (Devlin et al.,2019) signi\ufb01-cantly outperforms these methods by introducing a <b>masked</b>-<b>language</b> <b>model</b> and next-sentence pre-diction objectives combined with a bi-directional transformer ...", "dateLastCrawled": "2021-12-30T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Vokenization: Multimodel Learning for Vision</b> and <b>Language</b> - ML@B Blog", "url": "https://ml.berkeley.edu/blog/posts/vokens/", "isFamilyFriendly": true, "displayUrl": "https://ml.berkeley.edu/blog/posts/vokens", "snippet": "A voken is an image that corresponds with a given <b>language</b> token and <b>can</b> <b>be thought</b> of as a visualization of a token. Figure 2: A visualization of <b>model</b>-generated vokens in which the <b>model</b> classifies which visual voken corresponds with the <b>language</b> token. Vokenization supervises <b>language</b> <b>learning</b> using visual information. It extrapolates ...", "dateLastCrawled": "2022-01-21T00:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Transformer-based <b>Language</b> Models | The Ezra Tech Blog", "url": "https://techblog.ezra.com/an-overview-of-different-transformer-based-language-models-c9d3adafead8", "isFamilyFriendly": true, "displayUrl": "https://techblog.ezra.com/an-<b>overview-of-different-transformer-based-language-models</b>-c...", "snippet": "<b>Masked</b> <b>Language</b> <b>Model</b>. This task is used to allow for the <b>model</b>\u2019s bidirectionality. The standard conditional <b>language</b> models where the target word is predicted from the previous or next word do not allow for both left-to-right and right-to-left training. This is because with bidirectional conditioning the word indirectly sees itself. In order to achieve the bidirectional nature of the BERT <b>language</b> <b>model</b>, 15% of the training tokens are <b>masked</b> using a special [MASK] token. The <b>model</b> is ...", "dateLastCrawled": "2022-01-27T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>ELECTRA Vs BERT\u2013 A comparative study</b> | by Bijula Ratheesh | Medium", "url": "https://bijular.medium.com/electra-vs-bert-a-comparative-study-36f07ba88e61", "isFamilyFriendly": true, "displayUrl": "https://bijular.medium.com/electra-vs-bert-a-comparative-study-36f07ba88e61", "snippet": "The <b>masked</b> <b>language</b> <b>model</b> randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the <b>masked</b> word based only on its context. Unlike left-to right <b>language</b> <b>model</b> pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows to pretrain a deep bidirectional Transformer. In addition to the <b>masked</b> <b>language</b> <b>model</b>, a", "dateLastCrawled": "2022-01-13T04:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Language Modeling</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/language_modeling.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>language_modeling</b>.html", "snippet": "Since left-to-right neural <b>language</b> models <b>can</b> <b>be thought</b> of as classifiers, ... When reading <b>a new</b> text, how much is a <b>model</b> &quot;surprised&quot;? Similar to how good models of a physical world have to agree well with the real world, good <b>language</b> models have to agree well with the real text. This is the main idea of evaluation: if a text we give to a <b>model</b> is somewhat close to what a <b>model</b> would expect, then it is a good <b>model</b>. Cross-Entropy and Perplexity. But how to evaluate if &quot;a text is ...", "dateLastCrawled": "2022-01-29T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5 <b>NLP</b> Models That You Need to Know About | by Sara A. Metwalli ...", "url": "https://towardsdatascience.com/5-nlp-models-that-you-need-to-know-about-754594a3225b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/5-<b>nlp</b>-<b>models</b>-that-you-need-to-know-about-754594a3225b", "snippet": "\u21163: RoBERTa. RoBERTa is a natural <b>language</b> processing <b>model</b> that is built upon BERT specifically to overcome some of its weaknesses and enhance its performance. RoBERTa was the result of research conducted by researchers from Facebook AI and the University of Washington. The research group worked on analyzing the performance of the bidirectional context analysis and identified some changes that <b>can</b> be implemented to enhance the performance of BERT by using a larger, <b>new</b> dataset for ...", "dateLastCrawled": "2022-02-03T14:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Hugging Face: How to test <b>masked</b> <b>language</b> <b>model</b> after training it ...", "url": "https://www.reddit.com/r/LanguageTechnology/comments/nt04tj/hugging_face_how_to_test_masked_language_model/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/nt04tj/hugging_face_how_to_test_<b>masked</b>_<b>language</b>_<b>model</b>", "snippet": "I have trained the <b>model</b> using my own dataset, which has worked fine, but I don&#39;t know how to actually use the <b>model</b>, as the notebook does not include an example on how to do this, sadly. On the Hugging Face website, this is the code used in the example; hence, I want to do this exact thing but with my <b>model</b>:", "dateLastCrawled": "2021-06-07T16:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Develop <b>a Word-Level Neural Language Model and</b> Use it to ...", "url": "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/how-to-develop-<b>a-word-level-neural-language-model</b>...", "snippet": "How to use the learned <b>language</b> <b>model</b> to generate <b>new</b> text with similar statistical properties as the source text. Kick-start your project with my <b>new</b> book Deep <b>Learning</b> for Natural <b>Language</b> Processing, including step-by-step tutorials and the Python source code files for all examples. Let\u2019s get started. Update Apr/2018: Fixed type in <b>model</b> description; Update May/2020: Fixed a typo in the expectation of the <b>model</b>. How to Develop <b>a Word-Level Neural Language Model and</b> Use it to Generate ...", "dateLastCrawled": "2022-01-27T15:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Top 75 Natural <b>Language</b> Processing (<b>NLP) Interview Questions</b>", "url": "https://www.analytixlabs.co.in/blog/nlp-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.analytixlabs.co.in/blog/<b>nlp-interview-questions</b>", "snippet": "Embeddings (Word): It is the process of embedding each token as a vector before passing it into a machine <b>learning</b> <b>model</b>. Embeddings <b>can</b> also be done on phrases and characters as well, apart from words. N-grams: It is a continuous sequence (similar to the power set in number theory) of n-tokens of a given text. Transformers: They are deep <b>learning</b> architectures that <b>can</b> have the ability to parallelize computations. Transformers are used to learn long term dependencies. Parts of Speech (POS ...", "dateLastCrawled": "2022-02-02T06:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Easy <b>Masked</b> <b>Language Modeling with Machine Learning</b> and HuggingFace ...", "url": "https://www.machinecurve.com/index.php/2021/03/02/easy-masked-language-modeling-with-machine-learning-and-huggingface-transformers/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2021/03/02/easy-<b>masked</b>-<b>language</b>-<b>model</b>ing-with...", "snippet": "Last Updated on 30 March 2021. <b>Masked</b> <b>Language</b> Modeling (MLM) is a <b>language</b> task very common in Transformer architectures today. It involves masking part of the input, then <b>learning</b> a <b>model</b> to predict the missing tokens \u2013 essentially reconstructing the non-<b>masked</b> input. MLM is often used within pretraining tasks, to give models the opportunity to learn textual patterns from unlabeled data.", "dateLastCrawled": "2022-01-31T01:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Larger-Scale Transformers for Multilingual <b>Masked</b> <b>Language</b> Modeling", "url": "https://aclanthology.org/2021.repl4nlp-1.4.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2021.repl4nlp-1.4.pdf", "snippet": "dataset (Wenzek et al.,2019). Our two <b>new</b> multi-lingual <b>masked</b> <b>language</b> <b>model</b> dubbed XLM-R XL and XLM-R XXL, with 3.5 and 10.7 billion parame-ters respectively, signi\ufb01cantly outperform the previ-ous XLM-R <b>model</b> on cross-lingual understanding benchmarks and obtain competitive performance with the multilingual T5 models (Raffel et al.,2019; Xue et al.,2020). We show that they <b>can</b> even out-perform RoBERTa-Large (Liu et al.,2019) on the GLUE benchmark (Wang et al.,2018). Recent multilingual ...", "dateLastCrawled": "2022-01-30T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>a masked language model, and how</b> is it related to BERT? - Quora", "url": "https://www.quora.com/What-is-a-masked-language-model-and-how-is-it-related-to-BERT", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>a-masked-language-model-and-how</b>-is-it-related-to-BERT", "snippet": "Answer (1 of 2): <b>Masked</b> <b>language</b> modeling is an example of autoencoding <b>language</b> modeling (the output is reconstructed from corrupted input) - we typically mask one or more of words in a sentence and have the <b>model</b> predict those <b>masked</b> words given the other words in sentence. By training the mode...", "dateLastCrawled": "2022-02-03T01:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "nlp - What is the typical accuracy of <b>masked</b> <b>language</b> models during ...", "url": "https://datascience.stackexchange.com/questions/81792/what-is-the-typical-accuracy-of-masked-language-models-during-bert-pretraining", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/81792/what-is-the-typical-accuracy-of...", "snippet": "BERT is a subword <b>language</b> <b>model</b> (uses WordPiece tokenizer) so if you use large vocab then sentences are usually tokenized to &quot;smaller&quot; subword units that are easier to predict. I trained a BERT-like <b>model</b> for Czech <b>language</b>, which is morphologically more complex than English, and got top@1 test accuracy ~0.55, top@3 test accuracy ~0.68 after 1.5M steps with 30K vocab size.", "dateLastCrawled": "2022-01-19T18:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Building State-of-the-Art <b>Language</b> Models with <b>BERT</b> | by Ankit Singh ...", "url": "https://medium.com/saarthi-ai/bert-how-to-build-state-of-the-art-language-models-59dddfa9ac5d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/saarthi-ai/<b>bert</b>-how-to-build-state-of-the-art-<b>language</b>-<b>models</b>-59...", "snippet": "For the <b>masked</b> <b>language</b> <b>model</b> training of <b>BERT</b>, there are a few steps that have to be followed. A highly unconventional method of training a <b>masked</b> <b>language</b> <b>model</b> is to randomly replace some ...", "dateLastCrawled": "2022-01-31T18:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "LAnoBERT : System Log Anomaly Detection based on BERT <b>Masked</b> <b>Language</b> <b>Model</b>", "url": "https://deepai.org/publication/lanobert-system-log-anomaly-detection-based-on-bert-masked-language-model", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/lanobert-system-log-anomaly-detection-based-on-bert...", "snippet": "When the BERT <b>model</b> pre-trained with natural <b>language</b> was used, the F1 score in the BGL data was 0.9020, which was improved by 0.0271 <b>compared</b> to the <b>model</b> trained from scratch; by contrast, the F1 score in the HDFS data was 0.9304, which was decreased by 0.0341 <b>compared</b> to the <b>model</b> trained from scratch. These results indicate that the HDFS data consisting of a very simple log structure have a degraded performance when a <b>model</b> that has learned the context of natural <b>language</b> is used. The ...", "dateLastCrawled": "2022-02-03T04:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "W2v-BERT: Combining Contrastive <b>Learning</b> and <b>Masked</b> <b>Language</b> Modeling ...", "url": "https://arxiv.org/abs/2108.06209v2", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2108.06209v2", "snippet": "Abstract: Motivated by the success of <b>masked</b> <b>language</b> modeling~(MLM) in pre-training natural <b>language</b> processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation <b>learning</b>. w2v-BERT is a framework that combines contrastive <b>learning</b> and MLM, where the former trains the <b>model</b> to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the <b>model</b> to learn contextualized speech representations via ...", "dateLastCrawled": "2021-11-09T10:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "We introduce <b>a new</b> <b>language</b> representation <b>model</b> called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent <b>language</b> representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations <b>can</b> be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>BERT</b> Explained: State of the art <b>language</b> <b>model</b> for NLP | by Rani Horev ...", "url": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bert</b>-explained-state-of-the-art-<b>language</b>-<b>model</b>-for-nlp...", "snippet": "When training the <b>BERT</b> <b>model</b>, <b>Masked</b> LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies. How to use <b>BERT</b> (Fine-tuning) Using <b>BERT</b> for a specific task is relatively straightforward: <b>BERT</b> <b>can</b> be used for a wide variety of <b>language</b> tasks, while only adding a small layer to the core <b>model</b>: Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "BERT: Is it possible to filter the predicted tokens in <b>masked</b> <b>language</b> ...", "url": "https://stackoverflow.com/questions/68502532/bert-is-it-possible-to-filter-the-predicted-tokens-in-masked-language-modelling", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/68502532/bert-is-it-possible-to-filter-the...", "snippet": "I have trained a <b>masked</b> <b>language</b> <b>model</b> using my own dataset, which contains sentences with emojis (trained on 20,000 entries). Now, when I make predictions, I want emojis to be in the output, however, most of the predicted tokens are words, so I think that the emojis are right at the bottom of the list somewhere, as they must be less frequent tokens <b>compared</b> to the words.", "dateLastCrawled": "2022-01-17T10:48:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Language</b> Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>language</b>", "snippet": "For instance, a <b>masked</b> <b>language</b> <b>model</b> can calculate probabilities for candidate word(s) to replace the underline in the following sentence: The ____ in the hat came back. The literature typically uses the string &quot;MASK&quot; instead of an underline. For example: The &quot;MASK&quot; in the hat came back. Most modern <b>masked</b> <b>language</b> models are bidirectional.", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The Illustrated GPT-2 (Visualizing Transformer <b>Language</b> Models) \u2013 Jay ...", "url": "http://jalammar.github.io/illustrated-gpt2/", "isFamilyFriendly": true, "displayUrl": "jalammar.github.io/illustrated-gpt2", "snippet": "GPT-2 <b>Masked</b> Self-Attention; Beyond <b>Language</b> modeling; You\u2019ve Made it! Part 3: Beyond <b>Language</b> Modeling. <b>Machine</b> Translation; Summarization ; Transfer <b>Learning</b>; Music Generation; Part #1: GPT2 And <b>Language</b> Modeling # So what exactly is a <b>language</b> <b>model</b>? What is a <b>Language</b> <b>Model</b>. In The Illustrated Word2vec, we\u2019ve looked at what a <b>language</b> <b>model</b> is \u2013 basically a <b>machine</b> <b>learning</b> <b>model</b> that is able to look at part of a sentence and predict the next word. The most famous <b>language</b> models ...", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>language</b> of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "For example, in the <b>masked</b> <b>language</b> task, some fraction of the tokens in the original text are <b>masked</b> at random, and the <b>language</b> <b>model</b> attempts to predict the original text. (B) (Pre-)trained <b>language</b> models are commonly fine-tuned on downstream tasks over labeled text, through a standard supervised-<b>learning</b> approach. Fine-tuning is typically much faster and provides superior performance than training a <b>model</b> from scratch, especially when labeled data is scarce.", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Text Style Transfer for Bias Mitigation using <b>Masked</b> <b>Language</b> Modeling ...", "url": "https://www.researchgate.net/publication/358145352_Text_Style_Transfer_for_Bias_Mitigation_using_Masked_Language_Modeling", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/358145352_Text_Style_Transfer_for_Bias...", "snippet": "In the \\emph{infill} step, we utilize a pre-trained <b>Masked</b> <b>Language</b> <b>Model</b> (MLM) to infill the <b>masked</b> positions by predicting words or phrases conditioned on the context\\footnote{In this paper ...", "dateLastCrawled": "2022-01-29T13:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Natrual <b>language</b> processing basic concepts - <b>language</b> <b>model</b> - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "Before deep <b>learning</b>&#39;s domination in natural <b>language</b> processing, a <b>language</b> <b>model</b> is basically a large lookup table, recording frequencies of different combinations of words&#39; occurrences in a large corpus. Now it&#39;s a neural network trained on a corpus or dataset. In addition, a causal <b>language</b> <b>model</b>(e.g., GPT) predicts the next word, and a <b>masked</b> <b>language</b> <b>model</b>(e.g., BERT) fills the blank given the rest of a sentence. If you input &quot;The man ____ to the store&quot; to BERT, it will predict the ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An introduction to Deep <b>Learning</b> in Natural <b>Language</b> Processing: Models ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "snippet": "The pre-training was driven by two <b>language</b> <b>model</b> objectives, i.e. <b>Masked</b> <b>Language</b> <b>Model</b> (MLM) and Next Sentence Prediction (NSP). In MLM, showed in Fig. 8 , the network masks a small number of words of the input sequence and it tries to predict them in output, whereas in NSP the network tries to understand the relations between sentences by means of a binary loss.", "dateLastCrawled": "2022-01-04T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - <b>rosinality/ml-papers</b>: My collection of <b>machine</b> <b>learning</b> papers", "url": "https://github.com/rosinality/ml-papers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rosinality/ml-papers", "snippet": "210413 <b>Masked</b> <b>Language</b> Modeling and the Distributional Hypothesis #<b>language</b>_<b>model</b> #mlm; 210417 mT6 #<b>language</b>_<b>model</b>; 210418 Data-Efficient <b>Language</b>-Supervised Zero-Shot <b>Learning</b> with #multimodal; 210422 ImageNet-21K Pretraining for the Masses #backbone; 210510 Are Pre-trained Convolutions Better than Pre-trained Transformers #nlp #convolution # ...", "dateLastCrawled": "2022-01-31T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Word Embeddings, WordPiece and Language-Agnostic BERT</b> (LaBSE) | by ...", "url": "https://medium.com/mlearning-ai/word-embeddings-wordpiece-and-language-agnostic-bert-labse-98c7626878c7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/<b>word-embeddings-wordpiece-and-language-agnostic-bert</b>...", "snippet": "Word embeddings are the representation of words in a numeric format, which can be understood by a computer. Simplest example would be (Yes, No) represented as (1, 0). But when we are dealing with\u2026", "dateLastCrawled": "2022-02-03T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "14.8. <b>Bidirectional Encoder</b> Representations from Transformers (BERT ...", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_natural-<b>language</b>-processing-pretraining/bert.html", "snippet": "As illustrated in Section 8.3, a <b>language</b> <b>model</b> predicts a token using the context on its left. To encode context bidirectionally for representing each token, BERT randomly masks tokens and uses tokens from the bidirectional context to predict the <b>masked</b> tokens in a self-supervised fashion. This task is referred to as a <b>masked</b> <b>language</b> <b>model</b>.", "dateLastCrawled": "2022-02-02T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "We do however often want to create a <b>machine</b> <b>learning</b> <b>model</b> that can perform one task really well. This is where finetuning comes in: using a labeled corpus, which is often smaller, we can then train the pretrained <b>model</b> further, with an additional or replacing NLP task. The end result is a <b>model</b> that has been pretrained on the large unlabeled corpus and which is finetuned to a specific <b>language</b> task, such as summarization, text generation in a particular domain, or translation.", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Improving Text Generation with Dynamic Masking and Recovering", "url": "https://www.ijcai.org/proceedings/2021/0534.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/proceedings/2021/0534.pdf", "snippet": "tokens, <b>just as masked language model</b> does. Therefore, our approach jointly maximizes both the likelihoods of both sen-tence generation and prediction of masked tokens. We verify the effectiveness and generality of our ap-proach on three types of text generation tasks which use var-ious forms of input data including text, graph, and image. For sequence-to-sequence (seq2seq) generation task (specif-ically, <b>machine</b> translation), our model obtains signi\ufb01cant improvement of 1.01 and 0.90 BLEU ...", "dateLastCrawled": "2022-01-29T07:50:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(masked language model)  is like +(learning a new language)", "+(masked language model) is similar to +(learning a new language)", "+(masked language model) can be thought of as +(learning a new language)", "+(masked language model) can be compared to +(learning a new language)", "machine learning +(masked language model AND analogy)", "machine learning +(\"masked language model is like\")", "machine learning +(\"masked language model is similar\")", "machine learning +(\"just as masked language model\")", "machine learning +(\"masked language model can be thought of as\")", "machine learning +(\"masked language model can be compared to\")"]}