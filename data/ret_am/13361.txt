{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Deep Q-Network</b> (<b>DQN</b>)-III. <b>DQN</b> performance and use | by Jordi TORRES.AI ...", "url": "https://towardsdatascience.com/deep-q-network-dqn-iii-c5a83b0338d2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>deep-q-network</b>-<b>dqn</b>-iii-c5a83b0338d2", "snippet": "This is the third of three posts devoted to present the basics of <b>Deep Q-Network</b> (<b>DQN</b>), in which we present how we can use TensorBoard in order to help us in the process of parameter turning. We also show how we can visualize the behaviour of our Agent. See you in the next post! <b>Deep</b> Reinforcement Learning Explained Series. by UPC Barcelona Tech and Barcelona Supercomputing Center. A relaxed introductory series that gradually and with a practical approach introduces the reader to this ...", "dateLastCrawled": "2022-01-22T23:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>DeepMellow: Removing the Need for</b> a Target Network in <b>Deep</b> Q-Learning", "url": "https://www.ijcai.org/Proceedings/2019/0379.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/2019/0379.pdf", "snippet": "<b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) is an algorithm that achieves human-level performance in complex domains <b>like</b> Atari games. One of the important elements of <b>DQN</b> is its use of a target network, which is necessary to stabilize learning. We argue that using a target network is incompatible with online reinforcement learning, and it is possible to achieve faster and more stable learning without a target network when we use Mellowmax, an alternative softmax operator. We derive novel properties of Mellowmax ...", "dateLastCrawled": "2022-01-11T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep</b> <b>Q-Network</b> (<b>DQN</b>)-II. Experience Replay and Target Networks | by ...", "url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>deep</b>-<b>q-network</b>-<b>dqn</b>-ii-b6bf911b6b2c", "snippet": "This is the second post devoted to <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>), in the \u201c<b>Deep</b> Reinforcement Learning Explained\u201d series, in which we will analyse some challenges that appear when we apply <b>Deep</b> Learning to Reinforcement Learning. We will also present in detail the code that solves the OpenAI Gym Pong game using the <b>DQN</b> <b>network</b> introduced in the previous post.. Spanish version of this publication", "dateLastCrawled": "2022-02-02T11:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>DQN</b> Algorithm: A father-son tale. The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b> ...", "url": "https://medium.com/analytics-vidhya/dqn-algorithm-a-father-son-tale-b4bf6ff1ae2f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>dqn</b>-algorithm-a-father-son-tale-b4bf6ff1ae2f", "snippet": "The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) Reinforcement learning algorithm has a surprisingly simple and real life analogy with which it can be explained. It helps understand the sequence of operations involved by\u2026", "dateLastCrawled": "2022-01-13T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) on LunarLander-v2 | Chan`s Jupyter", "url": "https://goodboychan.github.io/python/reinforcement_learning/pytorch/udacity/2021/05/07/DQN-LunarLander.html", "isFamilyFriendly": true, "displayUrl": "https://goodboychan.github.io/.../pytorch/udacity/2021/05/07/<b>DQN</b>-LunarLander.html", "snippet": "<b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) on LunarLander-v2. In this post, We will take a hands-on-lab of Simple <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) on openAI LunarLander-v2 environment. This is the coding exercise from udacity <b>Deep</b> Reinforcement Learning Nanodegree. May 7, 2021 \u2022 Chanseok Kang \u2022 6 min read. Python Reinforcement_Learning PyTorch Udacity.", "dateLastCrawled": "2022-02-01T02:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Deep</b> Q Learning and <b>Deep</b> Q Networks (<b>DQN</b>) Intro ... - Python Programming", "url": "https://pythonprogramming.net/deep-q-learning-dqn-reinforcement-learning-python-tutorial/", "isFamilyFriendly": true, "displayUrl": "https://pythonprogramming.net/<b>deep</b>-q-learning-<b>dqn</b>-reinforcement-learning-python-tutorial", "snippet": "A typical <b>DQN</b> model might look something <b>like</b>: The <b>DQN</b> neural network model is a regression model, which typically will output values for each of our possible actions. These values will be continuous float values, and they are directly our Q values. As we enage in the environment, we will do a .predict() to figure out our next move (or move randomly). When we do a .predict(), we will get the 3 float values, which are our Q values that map to actions. We will then do an argmax on these, <b>like</b> ...", "dateLastCrawled": "2022-01-30T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Reinforcement Learning (<b>DQN</b>) Tutorial \u2014 <b>PyTorch</b> Tutorials 1.10.1+cu102 ...", "url": "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html", "isFamilyFriendly": true, "displayUrl": "https://<b>pytorch</b>.org/tutorials/intermediate/reinforcement_q_learning.html", "snippet": "This tutorial shows how to use <b>PyTorch</b> to train a <b>Deep</b> Q Learning (<b>DQN</b>) agent on the CartPole-v0 task from the OpenAI Gym. Task. The agent has to decide between two actions - moving the cart left or right - so that the pole attached to it stays upright. You can find an official leaderboard with various algorithms and visualizations at the Gym website. cartpole. As the agent observes the current state of the environment and chooses an action, the environment transitions to a new state, and ...", "dateLastCrawled": "2022-02-03T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - AdrianHsu/<b>breakout-Deep-Q-Network</b>: Reinforcement Learning ...", "url": "https://github.com/AdrianHsu/breakout-Deep-Q-Network", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/AdrianHsu/<b>breakout-Deep-Q-Network</b>", "snippet": "<b>breakout-Deep-Q-Network</b>. \ud83c\udfc3 [Reinforcement Learning] tensorflow implementation of <b>Deep</b> <b>Q Network</b> (<b>DQN</b>), Dueling <b>DQN</b> and Double <b>DQN</b> performed on Atari Breakout Game. Installation. Type the following command to install OpenAI Gym Atari environment. $ pip3 install opencv-python gym gym[atari]", "dateLastCrawled": "2022-01-30T05:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why is my <b>DQN</b> (<b>Deep</b> <b>Q Network</b>) not learning? - reinforcement-learning ...", "url": "https://discuss.pytorch.org/t/why-is-my-dqn-deep-q-network-not-learning/126336", "isFamilyFriendly": true, "displayUrl": "https://discuss.pytorch.org/t/why-is-my-<b>dqn</b>-<b>deep</b>-<b>q-network</b>-not-learning/126336", "snippet": "I am training a <b>DQN</b> (<b>Deep</b> <b>Q Network</b>) on a CartPole problem from OpenAI\u2019s gym, but the total score from an episode decreases, instead of increasing. I don\u2019t know if it is helpful but I noticed that the AI prefers one action over another and refuses to do anything else (unless it is forced by the epsilon greedy strategy), at least for some time. I tried my best, but I just can\u2019t figure out what is going on. Here is my code: (qlearning.py) import torch as t import torch.nn as nn import ...", "dateLastCrawled": "2022-02-02T19:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "tensorflow - <b>Deep</b> <b>Q Network is not learning</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/49840892/deep-q-network-is-not-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/49840892", "snippet": "If I look at those plots in my answer, it seems <b>like</b> all of the algorithms in that plot (which are all slightly more advanced than vanilla <b>DQN</b>) only start increasing above an average episode reward of 0 at about 10% of the first &quot;block&quot;. That first &quot;block&quot; in the figure is for 50 million frames seen by the agent, so the 10% point would be at roughly 5 million frames.", "dateLastCrawled": "2022-01-25T03:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Reinforcement Learning Explained Visually (Part</b> 5): <b>Deep</b> Q Networks ...", "url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning-explained-visually-part</b>-5-<b>deep</b>-q...", "snippet": "The underlying principle of a <b>Deep</b> <b>Q Network</b> is very <b>similar</b> to the Q Learning algorithm. It starts with arbitrary Q-value estimates and explores the environment using the \u03b5-greedy policy. And at its core, it uses the same notion of dual actions, a current action with a current Q-value and a target action with a target Q-value, for its update logic to improve its Q-value estimates. <b>DQN</b> Architecture Components. The <b>DQN</b> architecture has two neural nets, the <b>Q network</b> and the Target networks ...", "dateLastCrawled": "2022-01-31T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep</b> <b>Q-Network</b> (<b>DQN</b>)-II. Experience Replay and Target Networks | by ...", "url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>deep</b>-<b>q-network</b>-<b>dqn</b>-ii-b6bf911b6b2c", "snippet": "This is the second post devoted to <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>), ... However, both the states s and s\u2019 have only one step between them. This makes them very <b>similar</b>, and it\u2019s very hard for a Neural <b>Network</b> to distinguish between them. When we perform an update of our Neural Networks\u2019 parameters to make Q(s, a) closer to the desired result, we can indirectly alter the value produced for Q(s\u2019, a\u2019) and other states nearby. This can make our training very unstable. To make training more stable ...", "dateLastCrawled": "2022-02-02T11:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep Q-Networks (DQN</b>) \u2014 GenRL 0.1 documentation", "url": "https://genrl.readthedocs.io/en/latest/usage/tutorials/Deep/DQN.html", "isFamilyFriendly": true, "displayUrl": "https://genrl.readthedocs.io/en/latest/usage/tutorials/<b>Deep</b>/<b>DQN</b>.html", "snippet": "<b>DQN</b> uses a neural network as a function approximator and objective is to get as close to the Bellman Expectation of the Q-value function as possible. This is done by minimising the loss function which is defined as. E ( s, a, s \u2032, r) \u223c D [ r + \u03b3 m a x a \u2032 Q ( s \u2032, a \u2032; \u03b8 i \u2212) \u2212 Q ( s, a; \u03b8 i)] 2. Unlike in regular Q-learning ...", "dateLastCrawled": "2022-01-31T11:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Deep Q</b> Networks (<b>DQN</b>) \u00b7 <b>Deep</b> Reinforcement Learning", "url": "https://stevenschmatz.gitbooks.io/deep-reinforcement-learning/content/deep-q-networks.html", "isFamilyFriendly": true, "displayUrl": "https://stevenschmatz.gitbooks.io/<b>deep</b>-reinforcement-learning/content/<b>deep-q</b>-networks.html", "snippet": "<b>Deep Q</b> Networks (<b>DQN</b>) <b>DQN</b> Extensions Double <b>DQN</b> Dueling <b>DQN</b> Prioritized Experience Replay ... The <b>DQN</b> showing optimal strategy on Breakout\u2014moving the ball into the top zone. Since then, a lot of progress has been made, and <b>DQN</b> is no longer the architecture of choice for new problems. However, for the time, it was revolutionary. Google proceeded to buy DeepMind Technologies for more than $500M USD. Let&#39;s start by talking about the intuition behind <b>DQN</b>. Basic idea. Rather than finding a ...", "dateLastCrawled": "2022-01-30T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "[<b>Deep</b> <b>Q-Network</b>]How to exclude ops at auto-differential of Tensorflow ...", "url": "https://stackoverflow.com/questions/45937774/deep-q-networkhow-to-exclude-ops-at-auto-differential-of-tensorflow", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/45937774", "snippet": "I am trying to create a <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) <b>similar</b> to Deepmind DQN3.0 using Tensorflow, but I am having some difficulties. I think that the cause is TensorFlow&#39;s auto-differential approach. Plea...", "dateLastCrawled": "2022-01-19T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Q-<b>targets, Double DQN and Dueling DQN</b> | AI Summer", "url": "https://theaisummer.com/Taking_Deep_Q_Networks_a_step_further/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/Taking_<b>Deep</b>_Q_Networks_a_step_further", "snippet": "Double <b>Deep</b> <b>Q Network</b>. To address maximization bias, we use two <b>Deep</b> Q Networks. On the one hand, the <b>DQN</b> is responsible for the selection of the next action (the one with the maximum value) as always. On the other hand, the Target network is responsible for the evaluation of that action.", "dateLastCrawled": "2022-02-02T19:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "GitHub - smitkiri/<b>deep</b>_q_learning: Implementation of various <b>Deep</b> Q ...", "url": "https://github.com/smitkiri/deep_q_learning", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/smitkiri/<b>deep</b>_q_learning", "snippet": "<b>Deep</b> Q-Networks. Implementation of various <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) variants for the state and image inputs of the CartPole-v1 environment. This project was a part of the Fall 2021 Reinforcement Learning course (CS 5180) at Northeastern University. This project aims to implement and analyze some of the most popular variants of <b>DQN</b> by testing them ...", "dateLastCrawled": "2022-01-05T18:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>DQN</b> \u2014 Stable <b>Baselines</b> 2.10.2 documentation", "url": "https://stable-baselines.readthedocs.io/en/master/modules/dqn.html", "isFamilyFriendly": true, "displayUrl": "https://stable-<b>baselines</b>.readthedocs.io/en/master/modules/<b>dqn</b>.html", "snippet": "<b>DQN</b>\u00b6 <b>Deep</b> <b>Q Network</b> (<b>DQN</b>) and its extensions (Double-<b>DQN</b>, Dueling-<b>DQN</b>, Prioritized Experience Replay). Warning. The <b>DQN</b> model does not support stable_<b>baselines</b>.common.policies, as a result it must use its own policy models (see <b>DQN</b> Policies). Available Policies. MlpPolicy: Policy object that implements <b>DQN</b> policy, using a MLP (2 layers of 64) LnMlpPolicy: Policy object that implements <b>DQN</b> policy, using a MLP (2 layers of 64), with layer normalisation: CnnPolicy: Policy object that ...", "dateLastCrawled": "2022-02-03T12:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deepmind Deep Q Network (DQN) 3D Convolution</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/34692318/deepmind-deep-q-network-dqn-3d-convolution", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/34692318", "snippet": "<b>Deepmind Deep Q Network (DQN) 3D Convolution</b>. Ask Question Asked 5 years, 11 months ago. Active 5 years, 11 months ago. Viewed 712 times 3 1. I was reading the deepmind nature paper on <b>DQN</b> network. I almost got everything about it except one. I don&#39;t know why no one has asked this question before but it seems a little odd to me anyway. My question: Input <b>to DQN</b> is a 84*84*4 image. The first convolution layer consists of 32 filters of 8*8 with stide 4. I want to know what is the result of ...", "dateLastCrawled": "2022-01-03T21:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - <b>deepmind</b>/<b>dqn</b>_zoo: <b>DQN</b> Zoo is a collection of reference ...", "url": "https://github.com/deepmind/dqn_zoo", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>deepmind</b>/<b>dqn</b>_zoo", "snippet": "<b>DQN</b> Zoo. <b>DQN</b> Zoo is a collection of reference implementations of reinforcement learning agents developed at <b>DeepMind</b> based on the <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) agent.. It aims to be research-friendly, self-contained and readable. Each agent is implemented using JAX, Haiku and RLax, and is a best-effort replication of the corresponding paper implementation.Each agent reproduces results on the standard set of 57 Atari games, on average.", "dateLastCrawled": "2022-02-01T23:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Going Deeper Into Reinforcement Learning: Understanding Deep</b>-Q-Networks", "url": "https://danieltakeshi.github.io/2016/12/01/going-deeper-into-reinforcement-learning-understanding-dqn/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/2016/12/01/<b>going-deeper-into-reinforcement-learning</b>...", "snippet": "The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) algorithm, as introduced by DeepMind in a NIPS 2013 workshop paper, and later published in Nature 2015 <b>can</b> be credited with revolutionizing reinforcement learning. In this post, therefore, I would like to give a guide to a subset of the <b>DQN</b> algorithm. This is a continuation of an earlier reinforcement learning article about linear function approximators. My contribution here will be orthogonal to my previous post about the preprocessing steps for game frames. Before ...", "dateLastCrawled": "2022-02-03T15:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "4. <b>Deep</b> Q-Networks - <b>Reinforcement Learning</b> [Book]", "url": "https://www.oreilly.com/library/view/reinforcement-learning/9781492072386/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/<b>reinforcement-learning</b>/9781492072386/ch04.html", "snippet": "<b>Deep</b> Q-networks was the breakthrough paper, but neural networks have been used in RL for a long time. 22 Given the flexibility of neural networks, you <b>can</b> find as many improvements to <b>DQN</b> as the number of papers on <b>deep</b> learning. The key insight is that although nonlinear function approximators are unruly and may not converge, they have the incredible ability to approximate any function. This opens the door to applications that were previously deemed too complex.", "dateLastCrawled": "2022-01-29T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Episodic Memory <b>and Deep Q-Networks - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/episodic-memory-and-deep-q-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/episodic-memory-and-<b>deep</b>-q-networks", "snippet": "<b>Deep</b> Q-Networks (<b>DQN</b>): A well-established technique to perform the above task is Q-learning, where we decide on a function called Q-function which is important for the success of the algorithm. <b>DQN</b> uses the neural networks as Q-function to approximate the action values Q(s, a, \\theta) where the parameter of network and (s,a) represents the state-action pair .", "dateLastCrawled": "2022-01-16T05:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Q-<b>targets, Double DQN and Dueling DQN</b> | AI Summer", "url": "https://theaisummer.com/Taking_Deep_Q_Networks_a_step_further/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/Taking_<b>Deep</b>_Q_Networks_a_step_further", "snippet": "Double <b>Deep</b> <b>Q Network</b>. To address maximization bias, we use two <b>Deep</b> Q Networks. On the one hand, the <b>DQN</b> is responsible for the selection of the next action (the one with the maximum value) as always. On the other hand, the Target network is responsible for the evaluation of that action. The trick is that the target value is not automatically produced by the maximum Q-value, but by the Target network. In other words, we call forth the Target network to calculate the target Q value of taking ...", "dateLastCrawled": "2022-02-02T19:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Deep</b> <b>Q-network (DQN) Based Path Planning Method for Mobile Robots</b> ...", "url": "https://www.researchgate.net/publication/335423316_A_Deep_Q-network_DQN_Based_Path_Planning_Method_for_Mobile_Robots", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335423316_A_<b>Deep</b>_<b>Q-network</b>_<b>DQN</b>_Based_Path...", "snippet": "<b>Deep</b> <b>Q Network</b> (<b>DQN</b>) is an evolutionary algorithm based on the combination of <b>deep</b> learning and reinforcement learning. With the excellent performance in computer games [4,5], <b>DQN</b> has been ...", "dateLastCrawled": "2022-01-19T21:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Advanced DQNs: Playing <b>Pac-man</b> with <b>Deep</b> Reinforcement <b>Learning</b> | by ...", "url": "https://towardsdatascience.com/advanced-dqns-playing-pac-man-with-deep-reinforcement-learning-3ffbd99e0814", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/advanced-<b>dqn</b>s-playing-<b>pac-man</b>-with-<b>deep</b>-reinforcement...", "snippet": "In 2013, DeepMind published the first version of its <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>), a computer program capabl e of human-level performance on a number of classic Atari 2600 games. Just like a human, the algorithm played based on its vision of the screen. Starting from scratch, it discovered gameplay strategies that let it meet (and in many cases, exceed) human benchmarks. In the years since, researchers have made a number of improvements that super-charge performance and solve games faster than ever ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Building the Ultimate AI Agent for Doom using Duelling Double <b>Deep</b> Q ...", "url": "https://towardsdatascience.com/building-the-ultimate-ai-agent-for-doom-using-dueling-double-deep-q-learning-ea2d5b8cdd9f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/building-the-ultimate-ai-agent-for-doom-using-dueling...", "snippet": "<b>Deep</b> Q-learning (<b>DQN</b>) Double <b>Deep</b> Q-learning (DDQN) Duelling <b>Deep</b> Q-learning (DuelDQN) Overall, vanilla <b>Deep</b> Q-learning is a highly flexible and responsive onli n e reinforcement learning approach that utilizes rapid intra-episodic updates to it\u2019s estimations of state-action (Q) values in an environment in order to maximize reward. Q-learning <b>can</b> <b>be thought</b> of as an off-policy approach to TD, where the algorithm aims to select state-action pairs of highest value independent of the current ...", "dateLastCrawled": "2022-01-09T08:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>A Framework of Hierarchical Deep</b> <b>Q-Network for Portfolio Management</b>", "url": "https://www.researchgate.net/publication/349201448_A_Framework_of_Hierarchical_Deep_Q-Network_for_Portfolio_Management", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349201448_<b>A_Framework_of_Hierarchical_Deep</b>_Q...", "snippet": "build a <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) that <b>can</b> solve prob-lems with in\ufb01nite state space, e.g., portfolio manage-ment. However, in the above description, we \ufb01nd that. the number of actions increases ...", "dateLastCrawled": "2022-01-11T21:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "tensorflow - <b>Deep</b> <b>Q Network is not learning</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/49840892/deep-q-network-is-not-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/49840892", "snippet": "If I look at those plots in my answer, it seems like all of the algorithms in that plot (which are all slightly more advanced than vanilla <b>DQN</b>) only start increasing above an average episode reward of 0 at about 10% of the first &quot;block&quot;. That first &quot;block&quot; in the figure is for 50 million frames seen by the agent, so the 10% point would be at roughly 5 million frames.", "dateLastCrawled": "2022-01-25T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>and when should we update the</b> Q-<b>target in deep Q-learning</b> ...", "url": "https://ai.stackexchange.com/questions/21485/how-and-when-should-we-update-the-q-target-in-deep-q-learning", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/21485/how-<b>and-when-should-we-update-the</b>-q...", "snippet": "The update form $\\theta^{\\prime} \\leftarrow \\tau \\theta+(1-\\tau) \\theta^{\\prime}$ (where $\\theta&#39;$ and $\\theta$ represent the weights of the target network and the current network, respectively) does exist and is correct.. It is called soft update and it has been used in the <b>Deep</b> Deterministic Policy Gradient (DDPG) paper, which uses the concept of a target network like <b>DQN</b>. The authors of the paper state that: The weights of these target networks are then updated by having them slowly track ...", "dateLastCrawled": "2022-01-20T16:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>pair of interrelated neural networks in</b> <b>Deep</b> <b>Q-Network</b> | by Rafael ...", "url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-<b>pair-of-interrelated-neural-networks-in</b>-<b>dqn</b>-f0f58e09b3c4", "snippet": "A <b>pair of interrelated neural networks in</b> <b>Deep</b> <b>Q-Network</b>. In <b>DQN</b> and Double <b>DQN</b> models, comparing two interrelated neural networks is crucial. Rafael Stekolshchik. Mar 18, 2020 \u00b7 10 min read. source: 123rf.com. We will follow a few steps that have been taken in the fight against correlations and overestimations in the development of the <b>DQN</b> and Double <b>DQN</b> algorithms. As an example of the <b>DQN</b> and Double <b>DQN</b> applications, we present the training results for the CartPole-v0 and CartPole-v1 ...", "dateLastCrawled": "2022-01-29T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep</b> <b>Q-Network</b> based resource allocation for UAV-assisted Ultra-Dense ...", "url": "https://www.sciencedirect.com/science/article/pii/S138912862100284X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S138912862100284X", "snippet": "<b>Compared</b> with traditional emergency networks, the access method is more flexible and the response speed is faster. ... Then, a <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) algorithm is designed to solve the problem of optimal dynamic real-time power allocation. <b>Compared</b> with other resource allocation algorithms, <b>DQN</b> is more suitable for solving the problem of high computational complexity caused by excessive data volume. \u2022 The simulation results demonstrate that the proposed resource allocation algorithm <b>can</b> ...", "dateLastCrawled": "2022-01-20T16:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep</b> <b>Q Network</b> vs Policy Gradients - An Experiment on VizDoom with ...", "url": "https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html", "isFamilyFriendly": true, "displayUrl": "https://flyyufelix.github.io/2017/10/12/<b>dqn</b>-vs-pg.html", "snippet": "<b>Deep</b> <b>Q Network</b> vs Policy Gradients - An Experiment on VizDoom with Keras. October 12, 2017 After a brief stint with several interesting computer vision projects, include this and this, I\u2019ve recently decided to take a break from computer vision and explore reinforcement learning, another exciting field.Similar to computer vision, the field of reinforcement learning has experienced several important breakthroughs made possible by the <b>deep</b> learning revolution.", "dateLastCrawled": "2022-01-31T19:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Deep</b> <b>Q-network</b>-based traffic signal control models", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256405", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256405", "snippet": "Especially, a <b>deep</b> <b>Q-network</b> (<b>DQN</b>) is one of the predominant reinforcement learning algorithms designed to overcome the limitations of existing Q-learning algorithms . Q-learning has a Q function that estimates the Q value for each state\u2013action pair and determines whether to perform a specific action in a specific state based on this. The Q function is called an action-value function and <b>can</b> directly estimate the optimal action-value function", "dateLastCrawled": "2021-09-03T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "GitHub - smitkiri/<b>deep</b>_q_learning: Implementation of various <b>Deep</b> Q ...", "url": "https://github.com/smitkiri/deep_q_learning", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/smitkiri/<b>deep</b>_q_learning", "snippet": "<b>Deep</b> Q-Networks. Implementation of various <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) variants for the state and image inputs of the CartPole-v1 environment. This project was a part of the Fall 2021 Reinforcement Learning course (CS 5180) at Northeastern University. This project aims to implement and analyze some of the most popular variants of <b>DQN</b> by testing them ...", "dateLastCrawled": "2022-01-05T18:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>A Theoretical Analysis of Deep Q</b>-Learning", "url": "http://proceedings.mlr.press/v120/yang20a/yang20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v120/yang20a/yang20a.pdf", "snippet": "game in Minimax-<b>DQN</b>, which <b>can</b> be ef\ufb01ciently attained via linear programming. Despite such a difference, both these two methods <b>can</b> be viewed as approximately applying the Bellman opera-tor to the <b>Q-network</b>. Thus, borrowing the analysis of <b>DQN</b>, we also establish theoretical results for Minimax-<b>DQN</b>. Speci\ufb01cally, we quantify the suboptimality ...", "dateLastCrawled": "2022-02-01T14:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Deep</b> <b>Reinforcement</b> Learning for <b>Video Games</b> Made Easy | by Andreas Holm ...", "url": "https://towardsdatascience.com/deep-reinforcement-learning-for-video-games-made-easy-6f7d06b75a65", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>deep</b>-<b>reinforcement</b>-learning-for-<b>video-games</b>-made-easy-6...", "snippet": "Atari Pong using <b>DQN</b> agent. In this post, we will investigate how easily we <b>can</b> train a <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) agent (Mnih et al., 2015) for Atari 2600 games using the Google <b>reinforcement</b> learning library Dopamine.While many RL libraries exist, this library is specifically designed with four essential features in mind:. Easy experimentation", "dateLastCrawled": "2022-02-02T18:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Q-<b>targets, Double DQN and Dueling DQN</b> | AI Summer", "url": "https://theaisummer.com/Taking_Deep_Q_Networks_a_step_further/", "isFamilyFriendly": true, "displayUrl": "https://theaisummer.com/Taking_<b>Deep</b>_Q_Networks_a_step_further", "snippet": "Double <b>Deep</b> <b>Q Network</b>. To address maximization bias, we use two <b>Deep</b> Q Networks. On the one hand, the <b>DQN</b> is responsible for the selection of the next action (the one with the maximum value) as always. On the other hand, the Target network is responsible for the evaluation of that action. The trick is that the target value is not automatically produced by the maximum Q-value, but by the Target network. In other words, we call forth the Target network to calculate the target Q value of taking ...", "dateLastCrawled": "2022-02-02T19:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Understanding DQN+HER</b> \u2013 <b>Deep</b> Robotics", "url": "https://deeprobotics.wordpress.com/2018/03/07/bitflipper-herdqn/", "isFamilyFriendly": true, "displayUrl": "https://<b>deep</b>robotics.wordpress.com/2018/03/07/bitflipper-her<b>dqn</b>", "snippet": "<b>DQN</b>+HER Algorithm. <b>Q-network</b> architecture. In <b>DQN</b>+HER the <b>Q-network</b> takes as input the current state of the agent and the goal of the current episode and outputs Q values for each action in the action space,similar to that of <b>DQN</b>. **Note: HER <b>can</b> be used with any off policy RL algorithms and in this article when we write HER, we mean <b>DQN</b>+HER.", "dateLastCrawled": "2022-01-27T09:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to use a different model to <b>deep</b> neural network with reinforcement ...", "url": "https://datascience.stackexchange.com/questions/37643/how-to-use-a-different-model-to-deep-neural-network-with-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/37643/how-to-use-a-different-model-to...", "snippet": "Well, if you remove the DNN, I would not call that a <b>Deep</b> <b>Q-Network</b> anymore.. but it is definitely possible to remove that and still consider the approach as Reinforcement Learning. Actually, the function of the <b>deep</b> neural network is just to approximate the Q-value function.", "dateLastCrawled": "2022-01-18T01:41:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>DQN</b> Algorithm: A father-son tale. The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b> ...", "url": "https://medium.com/analytics-vidhya/dqn-algorithm-a-father-son-tale-b4bf6ff1ae2f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>dqn</b>-algorithm-a-father-son-tale-b4bf6ff1ae2f", "snippet": "The <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) Reinforcement <b>learning</b> algorithm has a surprisingly simple and real life <b>analogy</b> with which it can be explained. It helps understand the sequence of operations involved by ...", "dateLastCrawled": "2022-01-13T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep Q-Learning with Keras and Gym</b> \u00b7 Keon&#39;s Blog", "url": "https://keon.github.io/deep-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://keon.github.io/<b>deep</b>-q-<b>learning</b>", "snippet": "If we use the <b>analogy</b> of the bicycle, we can define reward as the distance from the original starting point. ## <b>Deep</b> Reinforcement <b>Learning</b> Google\u2019s DeepMind published its famous paper Playing Atari with <b>Deep</b> Reinforcement <b>Learning</b>, in which they introduced a new algorithm called <b>Deep</b> <b>Q Network</b> (<b>DQN</b> for short) in 2013. It demonstrated how an ...", "dateLastCrawled": "2022-02-01T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Graying the Black Box: Understanding DQNs</b> | the morning paper", "url": "https://blog.acolyer.org/2016/03/02/graying-the-black-box-understanding-dqns/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2016/03/02/<b>graying-the-black-box-understanding-dqns</b>", "snippet": "<b>Deep</b> Reinforcement <b>Learning</b> (DRL) applies <b>Deep</b> Neural Networks to reinforcement <b>learning</b>. The <b>Deep</b> Mind team used a DRL algorithm called <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) to learn how to play the Atari games. In \u2018Graying the Black Box,\u2019 Zahavy et al. look at three of those games \u2013 Breakout, Pacman, and Seaquest \u2013 and develop a new visualization and interaction approach that helps to shed insight on what it is that <b>DQN</b> is actually <b>learning</b>.", "dateLastCrawled": "2022-01-20T19:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Using <b>Keras and Deep Q-Network to Play FlappyBird</b> | Ben Lau", "url": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "isFamilyFriendly": true, "displayUrl": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "snippet": "What is <b>Deep</b> <b>Q-Network</b>? <b>Deep</b> <b>Q-Network</b> is a <b>learning</b> algorithm developed by Google DeepMind to play Atari games. They demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable because it demonstrates the algorithm is generic enough to play various games. The following post is a must-read for those who are interested in <b>deep</b> reinforcement <b>learning</b>. Demystifying <b>Deep</b> ...", "dateLastCrawled": "2022-01-30T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Reinforcement Learning for On-Demand Logistics</b> - <b>DoorDash Engineering Blog</b>", "url": "https://doordash.engineering/2018/09/10/reinforcement-learning-for-on-demand-logistics/", "isFamilyFriendly": true, "displayUrl": "https://doordash.engineering/2018/09/10/<b>reinforcement-learning-for-on-demand-logistics</b>", "snippet": "This approach is known as <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) and is very useful when feature dimensionality is high and data volume is also high. Reinforcement learned assignment . Now we will discuss how we applied reinforcement <b>learning</b> to the DoorDash assignment problem. To formulate the assignment problem in a way that\u2019s suitable for reinforcement <b>learning</b>, we made the following definitions. State: The outstanding deliveries and working Dashers, since they represent the current status of the world ...", "dateLastCrawled": "2022-01-18T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Guide to Reinforcement <b>Learning with Python and TensorFlow</b>", "url": "https://rubikscode.net/2021/07/13/deep-q-learning-with-python-and-tensorflow-2-0/", "isFamilyFriendly": true, "displayUrl": "https://rubikscode.net/2021/07/13/<b>deep</b>-q-<b>learning-with-python-and-tensorflow</b>-2-0", "snippet": "In the previous two articles we started exploring the interesting universe of reinforcement <b>learning</b>.First we went through the basics of third paradigm within <b>machine</b> <b>learning</b> \u2013 reinforcement <b>learning</b>.Just to freshen up our memory, we saw that approach of this type of <b>learning</b> is unlike the previously explored supervised and unsupervised <b>learning</b>. In reinforcement <b>learning</b>, self-<b>learning</b> agent learns how to interact with the environment and solve a problem within it. In this article, we ...", "dateLastCrawled": "2022-02-03T13:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Solving Combinatorial Problems with Machine Learning Methods</b> | Request PDF", "url": "https://www.researchgate.net/publication/333525406_Solving_Combinatorial_Problems_with_Machine_Learning_Methods", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333525406_Solving_Combinatorial_Problems_with...", "snippet": "Next we discuss <b>Deep</b> <b>Q-Network</b> (<b>DQN</b>) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised <b>learning</b>, and ...", "dateLastCrawled": "2022-01-23T14:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep</b> Reinforcement <b>Learning</b> for Crowdsourced Urban Delivery - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0191261521001636", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0191261521001636", "snippet": "We propose a new <b>deep</b> reinforcement <b>learning</b> (DRL)-based approach to tackling this assignment problem. A <b>deep</b> <b>Q network</b> (<b>DQN</b>) algorithm is trained which entails two salient features of experience replay and target network that enhance the efficiency, convergence, and stability of DRL training. More importantly, this paper makes three methodological contributions: 1) presenting a comprehensive and novel characterization of crowdshipping system states that encompasses spatial-temporal and ...", "dateLastCrawled": "2022-01-19T19:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Reinforcement Learning for Formula 1</b> Race Strategy | by Ashref Maiza ...", "url": "https://towardsdatascience.com/reinforcement-learning-for-formula-1-race-strategy-7f29c966472a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning-for-formula-1</b>-race-strategy-7f29...", "snippet": "Reinforcement <b>Learning</b> (RL) is an advanced <b>machine</b> <b>learning</b> (ML) technique which takes a very different approach to training models than other <b>machine</b> <b>learning</b> methods. Its super power is that it learns very complex behaviors without requiring any labeled training data, and can make short term decisions while optimizing for a longer term goal. RL in the context of Formula 1 racing. In RL, an agent learns the optimal behavior to perform a certain task by interacting directly with the ...", "dateLastCrawled": "2022-02-02T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Applications of <b>Reinforcement Learning</b> in Real World | by garychl ...", "url": "https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/applications-of-<b>reinforcement-learning</b>-in-real-world-1a...", "snippet": "RL, known as a semi-supervised <b>learning</b> model in <b>machine</b> <b>learning</b>, is a technique to allow an agent to take actions and interact with an environment so as to maximize the total rewards. RL is usually modeled as a Markov Decision Process (MDP). Source: <b>Reinforcement Learning</b>:An Introduction. Imagine a baby is given a TV remote control at your home (environment). In simple terms, the baby (agent) will first observe and construct his/her own representation of the environment (state). Then the ...", "dateLastCrawled": "2022-02-02T20:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(deep q-network (dqn))  is like +(dqn)", "+(deep q-network (dqn)) is similar to +(dqn)", "+(deep q-network (dqn)) can be thought of as +(dqn)", "+(deep q-network (dqn)) can be compared to +(dqn)", "machine learning +(deep q-network (dqn) AND analogy)", "machine learning +(\"deep q-network (dqn) is like\")", "machine learning +(\"deep q-network (dqn) is similar\")", "machine learning +(\"just as deep q-network (dqn)\")", "machine learning +(\"deep q-network (dqn) can be thought of as\")", "machine learning +(\"deep q-network (dqn) can be compared to\")"]}