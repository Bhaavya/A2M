{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "lab meetings | <b>Pillow Lab Blog</b>", "url": "https://pillowlab.wordpress.com/category/lab-meetings/", "isFamilyFriendly": true, "displayUrl": "https://pillowlab.wordpress.com/category/lab-meetings", "snippet": "In fact, <b>gradient</b> <b>descent</b> is one of the most intuitive concepts in math: You are a <b>hiker</b> on top of a mountain, and you need to get <b>down</b>. What direction and how far do you move? Inspired by this, I started reading about other ways to choose optimization hyperparameters. I came up with a step-by-step procedure that I now follow for every problem. It\u2019s largely inspired by Smith 2018 and Jordan 2018. This procedure outputs: 1) reasonable values for the hyperparameters, 2) intuition about the ...", "dateLastCrawled": "2021-12-22T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\u5ddd\u5d0e\u5e02\u516c\u5f0f\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\uff1a\u30c8\u30c3\u30d7\u30da\u30fc\u30b8", "url": "https://www.city.kawasaki.jp/", "isFamilyFriendly": true, "displayUrl": "https://www.city.kawasaki.jp", "snippet": "\u5ddd\u5d0e\u5e02\u9632\u707d\u60c5\u5831\u30dd\u30fc\u30bf\u30eb\u30b5\u30a4\u30c8. \u7dca\u6025\u60c5\u5831\u30fb\u65e5\u9803\u306e\u5099\u3048. \u753b\u50cf\u5207\u66ff\u30dc\u30bf\u30f3\u3092\u30b9\u30ad\u30c3\u30d7\u3059\u308b. \u73fe\u5728\u3001\u30b3\u30ed\u30ca\u611f\u67d3\u8005\u304c\u6025\u5897\u3057\u3001\u533b\u7642\u63d0\u4f9b\u4f53\u5236\u306b\u91cd\u5927\u306a\u5f71\u97ff\u304c\u51fa\u3066\u3044\u307e\u3059\u3002. \u3042\u306a\u305f\u81ea\u8eab\u3084\u3054\u5bb6\u65cf\u3001\u307f\u3093\u306a\u3092\u5b88\u308b\u305f\u3081\u3001\u4eca\u4e00\u5ea6\u611f\u67d3\u5bfe\u7b56\u306e\u5fb9\u5e95\u3092\u304a\u9858\u3044\u3057\u307e\u3059\u3002. \u30de\u30b9\u30af ...", "dateLastCrawled": "2022-02-03T03:46:00.0000000Z", "language": "ja", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Achiever Papers - We help students improve their academic standing", "url": "https://achieverpapers.com/", "isFamilyFriendly": true, "displayUrl": "https://achieverpapers.com", "snippet": "100% money-back guarantee. With our money back guarantee, our customers have the right to request and get a refund at any stage of their order in case something goes wrong.", "dateLastCrawled": "2022-02-02T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Navy Removal Scout 800 Pink Pill Assasin Expo Van Travel Bothell ...", "url": "https://www.scribd.com/document/531005187/70048773907-navy-removal-scout-800-pink-pill-assasin-expo-van-travel-bothell-punishment-shred-norelco-district-ditch-required-anyhow", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/531005187/70048773907-navy-removal-scout-800-pink-pill...", "snippet": "James Tw - When You Love Someone 39. Imagine Dragons - Demons 40. Gabrielle - Out Of Reach 41. James Bay - Hold Back The River 42. Bastille - Pompeii 43. 3 Doors <b>Down</b> - Here Without You 44. Dean Lewis - Waves 45. Empire Of The Sun - <b>Walking</b> On A Dream 46. Machine Gun Kelly, Camila Cabello - Bad Things 47. Pia Mia, Chris Brown, Tyga - Do It ...", "dateLastCrawled": "2022-02-02T11:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fox Files</b> | <b>Fox News</b>", "url": "https://www.foxnews.com/shows/fox-files", "isFamilyFriendly": true, "displayUrl": "https://<b>www.foxnews.com</b>/shows/<b>fox-files</b>", "snippet": "<b>FOX FILES</b> combines in-depth news reporting from a variety of <b>Fox News</b> on-air talent. The program will feature the breadth, power and journalism of rotating <b>Fox News</b> anchors, reporters and producers.", "dateLastCrawled": "2022-02-02T20:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "lab meetings | <b>Pillow Lab Blog</b>", "url": "https://pillowlab.wordpress.com/category/lab-meetings/", "isFamilyFriendly": true, "displayUrl": "https://pillowlab.wordpress.com/category/lab-meetings", "snippet": "The optimization\u2014<b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) with all of its hyperparameters (learning rate, momentum, \u2026) and variants\u2014was an afterthought, and I chose hyperparameters that seemed reasonable. Still, a nagging question persisted in the back of my mind: What if different hyperparameters led to even better performance? It was an obvious case of fear-of-missing-out (FOMO).", "dateLastCrawled": "2021-12-22T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Achiever Papers - We help students improve their academic standing", "url": "https://achieverpapers.com/", "isFamilyFriendly": true, "displayUrl": "https://achieverpapers.com", "snippet": "This service <b>is similar</b> to paying a tutor to help improve your skills. Our online services is trustworthy and it cares about your learning and your degree. Hence, you should be sure of the fact that our online essay help cannot harm your academic life. You can freely use the academic papers written to you as they are original and perfectly referenced. Our essay writing services will help you when nothing else seems to be working . Whenever students face academic hardships, they tend to run ...", "dateLastCrawled": "2022-02-02T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Navy Removal Scout 800 Pink Pill Assasin Expo Van Travel Bothell ...", "url": "https://www.scribd.com/document/531005187/70048773907-navy-removal-scout-800-pink-pill-assasin-expo-van-travel-bothell-punishment-shred-norelco-district-ditch-required-anyhow", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/531005187/70048773907-navy-removal-scout-800-pink-pill...", "snippet": "James Tw - When You Love Someone 39. Imagine Dragons - Demons 40. Gabrielle - Out Of Reach 41. James Bay - Hold Back The River 42. Bastille - Pompeii 43. 3 Doors <b>Down</b> - Here Without You 44. Dean Lewis - Waves 45. Empire Of The Sun - <b>Walking</b> On A Dream 46. Machine Gun Kelly, Camila Cabello - Bad Things 47. Pia Mia, Chris Brown, Tyga - Do It ...", "dateLastCrawled": "2022-02-02T11:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fox Files</b> | <b>Fox News</b>", "url": "https://www.foxnews.com/shows/fox-files", "isFamilyFriendly": true, "displayUrl": "https://<b>www.foxnews.com</b>/shows/<b>fox-files</b>", "snippet": "<b>FOX FILES</b> combines in-depth news reporting from a variety of <b>Fox News</b> on-air talent. The program will feature the breadth, power and journalism of rotating <b>Fox News</b> anchors, reporters and producers.", "dateLastCrawled": "2022-02-02T20:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "lab meetings | <b>Pillow Lab Blog</b>", "url": "https://pillowlab.wordpress.com/category/lab-meetings/", "isFamilyFriendly": true, "displayUrl": "https://pillowlab.wordpress.com/category/lab-meetings", "snippet": "The optimization\u2014<b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) with all of its hyperparameters (learning rate, momentum, \u2026) and variants\u2014was an afterthought, and I chose hyperparameters that seemed reasonable. Still, a nagging question persisted in the back of my mind: What if different hyperparameters led to even better performance? It was an obvious case of fear-of-missing-out (FOMO). Grid search (aka brute force) and black-box optimization techniques should be last resorts. Due to FOMO, I began ...", "dateLastCrawled": "2021-12-22T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\u5ddd\u5d0e\u5e02\u516c\u5f0f\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\uff1a\u30c8\u30c3\u30d7\u30da\u30fc\u30b8", "url": "https://www.city.kawasaki.jp/", "isFamilyFriendly": true, "displayUrl": "https://www.city.kawasaki.jp", "snippet": "\u5ddd\u5d0e\u5e02\u9632\u707d\u60c5\u5831\u30dd\u30fc\u30bf\u30eb\u30b5\u30a4\u30c8. \u7dca\u6025\u60c5\u5831\u30fb\u65e5\u9803\u306e\u5099\u3048. \u753b\u50cf\u5207\u66ff\u30dc\u30bf\u30f3\u3092\u30b9\u30ad\u30c3\u30d7\u3059\u308b. \u73fe\u5728\u3001\u30b3\u30ed\u30ca\u611f\u67d3\u8005\u304c\u6025\u5897\u3057\u3001\u533b\u7642\u63d0\u4f9b\u4f53\u5236\u306b\u91cd\u5927\u306a\u5f71\u97ff\u304c\u51fa\u3066\u3044\u307e\u3059\u3002. \u3042\u306a\u305f\u81ea\u8eab\u3084\u3054\u5bb6\u65cf\u3001\u307f\u3093\u306a\u3092\u5b88\u308b\u305f\u3081\u3001\u4eca\u4e00\u5ea6\u611f\u67d3\u5bfe\u7b56\u306e\u5fb9\u5e95\u3092\u304a\u9858\u3044\u3057\u307e\u3059\u3002. \u30de\u30b9\u30af ...", "dateLastCrawled": "2022-02-03T03:46:00.0000000Z", "language": "ja", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Achiever Papers - We help students improve their academic standing", "url": "https://achieverpapers.com/", "isFamilyFriendly": true, "displayUrl": "https://achieverpapers.com", "snippet": "You <b>can</b> have an assignment that is too complicated or an assignment that needs to be completed sooner than you <b>can</b> manage. You also need to have time for a social life and this might not be possible due to school work. The good news is that course help online is here to take care of all this needs to ensure all your assignments are completed on time and you have time for other important activities. We also understand you have a number of subjects to learn and this might make it hard for you ...", "dateLastCrawled": "2022-02-02T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Navy Removal Scout 800 Pink Pill Assasin Expo Van Travel Bothell ...", "url": "https://www.scribd.com/document/531005187/70048773907-navy-removal-scout-800-pink-pill-assasin-expo-van-travel-bothell-punishment-shred-norelco-district-ditch-required-anyhow", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/531005187/70048773907-navy-removal-scout-800-pink-pill...", "snippet": "James Tw - When You Love Someone 39. Imagine Dragons - Demons 40. Gabrielle - Out Of Reach 41. James Bay - Hold Back The River 42. Bastille - Pompeii 43. 3 Doors <b>Down</b> - Here Without You 44. Dean Lewis - Waves 45. Empire Of The Sun - <b>Walking</b> On A Dream 46. Machine Gun Kelly, Camila Cabello - Bad Things 47. Pia Mia, Chris Brown, Tyga - Do It ...", "dateLastCrawled": "2022-02-02T11:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fox Files</b> | <b>Fox News</b>", "url": "https://www.foxnews.com/shows/fox-files", "isFamilyFriendly": true, "displayUrl": "https://<b>www.foxnews.com</b>/shows/<b>fox-files</b>", "snippet": "<b>FOX FILES</b> combines in-depth news reporting from a variety of <b>Fox News</b> on-air talent. The program will feature the breadth, power and journalism of rotating <b>Fox News</b> anchors, reporters and producers.", "dateLastCrawled": "2022-02-02T20:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "lab meetings | <b>Pillow Lab Blog</b>", "url": "https://pillowlab.wordpress.com/category/lab-meetings/", "isFamilyFriendly": true, "displayUrl": "https://pillowlab.wordpress.com/category/lab-meetings", "snippet": "The optimization\u2014<b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) with all of its hyperparameters (learning rate, momentum, \u2026) and variants\u2014was an afterthought, and I chose hyperparameters that seemed reasonable. Still, a nagging question persisted in the back of my mind: What if different hyperparameters led to even better performance? It was an obvious case of fear-of-missing-out (FOMO). Grid search (aka brute force) and black-box optimization techniques should be last resorts. Due to FOMO, I began ...", "dateLastCrawled": "2021-12-22T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Achiever Papers - We help students improve their academic standing", "url": "https://achieverpapers.com/", "isFamilyFriendly": true, "displayUrl": "https://achieverpapers.com", "snippet": "You <b>can</b> have an assignment that is too complicated or an assignment that needs to be completed sooner than you <b>can</b> manage. You also need to have time for a social life and this might not be possible due to school work. The good news is that course help online is here to take care of all this needs to ensure all your assignments are completed on time and you have time for other important activities. We also understand you have a number of subjects to learn and this might make it hard for you ...", "dateLastCrawled": "2022-02-02T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Words 333333 | PDF | Internet | Computing - Scribd", "url": "https://it.scribd.com/document/451354867/words-333333-txt", "isFamilyFriendly": true, "displayUrl": "https://it.scribd.com/document/451354867/<b>words-333333-txt</b>", "snippet": "<b>words-333333.txt</b> - Free ebook download as Text File (.txt), PDF File (.pdf) or read book online for free.", "dateLastCrawled": "2022-02-02T17:43:00.0000000Z", "language": "it", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Navy Removal Scout 800 Pink Pill Assasin Expo Van Travel Bothell ...", "url": "https://www.scribd.com/document/531005187/70048773907-navy-removal-scout-800-pink-pill-assasin-expo-van-travel-bothell-punishment-shred-norelco-district-ditch-required-anyhow", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/531005187/70048773907-navy-removal-scout-800-pink-pill...", "snippet": "James Tw - When You Love Someone 39. Imagine Dragons - Demons 40. Gabrielle - Out Of Reach 41. James Bay - Hold Back The River 42. Bastille - Pompeii 43. 3 Doors <b>Down</b> - Here Without You 44. Dean Lewis - Waves 45. Empire Of The Sun - <b>Walking</b> On A Dream 46. Machine Gun Kelly, Camila Cabello - Bad Things 47. Pia Mia, Chris Brown, Tyga - Do It ...", "dateLastCrawled": "2022-02-02T11:45:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! | by Aishwarya V ...", "url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>stochastic-gradient-descent</b>-clearly-explained-53d239905d31", "snippet": "<b>Stochastic gradient descent</b> is a very popular and common algorithm used in various <b>Machine</b> <b>Learning</b> algorithms, most importantly forms the basis of Neural Networks. In this article, I have tried my\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! Aishwarya V Srinivasan. Sep 7, 2019 \u00b7 4 min read. <b>Stochastic gradient descent</b> is a ...", "dateLastCrawled": "2022-02-02T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Stochastic gradient descent</b> - The <b>Learning</b> <b>Machine</b>", "url": "https://the-learning-machine.com/article/optimization/stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://the-<b>learning</b>-<b>machine</b>.com/article/optimization/<b>stochastic-gradient-descent</b>", "snippet": "<b>Stochastic gradient descent</b> (<b>SGD</b>) is an approach for unconstrained optimization.<b>SGD</b> is the workhorse of optimization for <b>machine</b> <b>learning</b> approaches. It is used as a faster alternative for training support vector machines and is the preferred optimization routine for deep <b>learning</b> approaches.. In this article, we will motivate the formulation for <b>stochastic gradient descent</b> and provide interactive demos over multiple univariate and multivariate functions to show it in action.", "dateLastCrawled": "2022-01-26T05:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> in Theory and Practice", "url": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) is the most widely used optimization method in the <b>machine</b> <b>learning</b> community. Researchers in both academia and industry have put considerable e ort to optimize <b>SGD</b>\u2019s runtime performance and to develop a theoretical framework for its empirical success. For example, recent advancements in deep neural networks have been largely achieved because, surprisingly, <b>SGD</b> has been found adequate to train them. Here we present three works highlighting desirable ...", "dateLastCrawled": "2022-01-30T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> <b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b>", "url": "http://www.bel.utcluj.ro/dce/didactic/eai/04_GradientDescent_ML.pdf", "isFamilyFriendly": true, "displayUrl": "www.bel.utcluj.ro/dce/didactic/eai/04_<b>GradientDescent</b>_ML.pdf", "snippet": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> Elements of Artificial Intelligence G. Oltean BGD vs. <b>SGD</b> The summation part is important, especially with the concept of batch <b>gradient</b> <b>descent</b> (BGD) vs. <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>). In Batch <b>Gradient</b> <b>Descent</b>, all the training data is taken into consideration to take a single step (one training epoch ...", "dateLastCrawled": "2022-02-03T10:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Scikit Learn - Stochastic Gradient Descent</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_stochastic_gradient_descent</b>.htm", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) is a simple yet efficient optimization algorithm used to find the values of parameters/coefficients of functions that minimize a cost function. In other words, it is used for discriminative <b>learning</b> of linear classifiers under convex loss functions such as SVM and Logistic regression. It has been successfully applied to large-scale datasets because the update to the coefficients is performed for each training instance, rather than at the end of instances.", "dateLastCrawled": "2022-02-03T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Adam, <b>Momentum and Stochastic Gradient Descent</b> - <b>Machine</b> <b>Learning</b> From ...", "url": "https://mlfromscratch.com/optimizers-explained/", "isFamilyFriendly": true, "displayUrl": "https://mlfromscratch.com/optimizers-explained", "snippet": "The basic difference between batch <b>gradient</b> <b>descent</b> (BGD) and <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>), is that we only calculate the cost of one example for each step in <b>SGD</b>, but in BGD, we have to calculate the cost for all training examples in the dataset. Trivially, this speeds up neural networks greatly. Exactly this is the motivation behind <b>SGD</b>. The equation for <b>SGD</b> is used to update parameters in a neural network \u2013 we use the equation to update parameters in a backwards pass, using ...", "dateLastCrawled": "2022-02-02T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Hitchhiker\u2019s Guide to Optimization in <b>Machine Learning</b> | by Aman ...", "url": "https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-machine-learning-edcf5a104210", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-<b>machine</b>...", "snippet": "<b>Machine Learning</b> is the ideal culmination of Applied Mathematics and Computer Science, where we train and use data-driven applications to run inferences on the available data. In this article, you get to learn what optimizing an ML model means, with an overview of <b>Gradient</b> <b>Descent</b> and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>).", "dateLastCrawled": "2022-02-02T19:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> using PyTorch | by Ashish Pandey | Geek ...", "url": "https://medium.com/geekculture/stochastic-gradient-descent-using-pytotch-bdd3ba5a3ae3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/<b>stochastic</b>-<b>gradient</b>-<b>descent</b>-using-pytotch-bdd3ba5a3ae3", "snippet": "Nearly all approaches start with the basic idea of multiplying the <b>gradient</b> by some small number, called the <b>learning</b> rate (LR). The <b>learning</b> rate is often a number between 0.001 and 0.1, although ...", "dateLastCrawled": "2022-01-29T14:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Lecture 2: Neural Nets</b> - Deep <b>Learning</b>", "url": "https://chinmayhegde.github.io/dl-notes/notes/lecture02/", "isFamilyFriendly": true, "displayUrl": "https://chinmayhegde.github.io/dl-notes/notes/lecture02", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) One issue with <b>gradient</b> <b>descent</b> is the uncomfortable fact that one needs to repeatedly compute the <b>gradient</b>. Let us see why this can be challenging. The <b>gradient</b> <b>descent</b> iteration for the least squares loss is given by: \\[w_{k+1} = w_k + \\alpha_k \\sum_{i=1}^n (y_i - \\langle w_k, x_i \\rangle) x_i\\] So, per ...", "dateLastCrawled": "2022-02-03T00:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent in Logistic Regression [Explained for Beginners</b> ...", "url": "https://www.upgrad.com/blog/gradient-descent-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/<b>gradient-descent-in-logistic-regression</b>", "snippet": "It\u2019s massive, and hence there was a need for a slightly modified <b>Gradient</b> <b>Descent</b> Algorithm, namely \u2013 <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Algorithm (<b>SGD</b>). The only difference <b>SGD</b> has with Normal <b>Gradient</b> <b>Descent</b> is that, in <b>SGD</b>, we don\u2019t deal with the entire training instance at a single time. In <b>SGD</b>, we compute the <b>gradient</b> of the cost function for just a single random example at each iteration. Now, doing so brings down the time taken for computations by a huge margin especially for large ...", "dateLastCrawled": "2022-01-28T16:49:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Gossip <b>Learning</b> as a Decentralized Alternative to Federated <b>Learning</b>", "url": "http://publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "isFamilyFriendly": true, "displayUrl": "publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "snippet": "Federated <b>learning</b> is adistributed <b>machine</b> <b>learning</b> approach for computing models over data collected by edge devices. Most impor-tantly, the data itself is not collected centrally, but a master-worker ar-chitecture is applied where a master node performs aggregation and the edge devices are the workers, not unlike the parameter server approach. Gossip <b>learning</b> also assumes that the data remains at the edge devices, but it requires no aggregation server or any central component. In this ...", "dateLastCrawled": "2022-01-27T14:06:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(stochastic gradient descent (sgd))  is like +(hiker walking down a hill)", "+(stochastic gradient descent (sgd)) is similar to +(hiker walking down a hill)", "+(stochastic gradient descent (sgd)) can be thought of as +(hiker walking down a hill)", "+(stochastic gradient descent (sgd)) can be compared to +(hiker walking down a hill)", "machine learning +(stochastic gradient descent (sgd) AND analogy)", "machine learning +(\"stochastic gradient descent (sgd) is like\")", "machine learning +(\"stochastic gradient descent (sgd) is similar\")", "machine learning +(\"just as stochastic gradient descent (sgd)\")", "machine learning +(\"stochastic gradient descent (sgd) can be thought of as\")", "machine learning +(\"stochastic gradient descent (sgd) can be compared to\")"]}