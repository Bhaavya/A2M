{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Guide to <b>Classification</b> on Imbalanced Datasets - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/guide-to-classification-on-imbalanced-datasets-d6653aa5fa23", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/guide-to-<b>classification</b>-on-imbalanced-<b>dataset</b>s-d6653aa5fa23", "snippet": "A between-class imbalance occurs <b>when there</b> is an imbalance in the <b>number</b> of <b>data</b> <b>points</b> contained within each class. An example of this is shown below: An illustration of between-class imbalance. We have a large <b>number</b> of <b>data</b> <b>points</b> for the red class but relatively <b>few</b> for the white class. An example of this would be a mammography <b>dataset</b>, which uses images known as mammograms to predict breast cancer. Consider the <b>number</b> of mammograms related to positive and negative cancer diagnoses: The ...", "dateLastCrawled": "2022-02-03T01:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Solving the <b>data sparsity problem</b> in destination prediction", "url": "https://link.springer.com/article/10.1007/s00778-014-0369-7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-014-0369-7", "snippet": "However, this approach encounters the \u201c<b>data sparsity problem</b>\u201d, i.e., the available historical trajectories are far from enough to cover all possible query trajectories, which considerably limits the <b>number</b> of query trajectories that can obtain predicted destinations. We propose a novel method named Sub-Trajectory Synthesis (SubSyn) to address the <b>data sparsity problem</b>. SubSyn first decomposes historical trajectories into sub-trajectories comprising two adjacent locations, and then ...", "dateLastCrawled": "2021-11-18T11:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Machine Learning in Medicine", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5831252/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5831252", "snippet": "Some algorithms such as non-linear support vector machines 15 can be extremely robust in a variety of situations even where the <b>number</b> of predictive <b>features</b> is <b>very</b> large compared <b>to the number</b> of <b>training</b> examples, a situation where over-fitting often occurs. Finally, accepting the limitations of each class of algorithms, some practitioners use a process called blending, merging the outputs of multiple different algorithms (also discussed below).", "dateLastCrawled": "2022-02-02T20:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "All about Feature <b>Scaling</b>. Scale <b>data</b> for better performance of\u2026 | by ...", "url": "https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/all-about-feature-<b>scaling</b>-bcc0ad75cb35", "snippet": "Machine learning algorithm just sees <b>number</b> \u2014 if <b>there</b> is a vast difference in the range say <b>few</b> ranging in thousands and <b>few</b> ranging in the tens, and it makes the underlying assumption that higher ranging numbers have superiority of some sort. So these more significant <b>number</b> starts playing a more decisive role while <b>training</b> the model. The machine learning algorithm works on numbers and does not know what that <b>number</b> represents. A weight of 10 grams and a price of 10 dollars represents ...", "dateLastCrawled": "2022-01-29T06:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>How does Word2vec solve sparse problem</b>? - Quora", "url": "https://www.quora.com/How-does-Word2vec-solve-sparse-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>How-does-Word2vec-solve-sparse-problem</b>", "snippet": "Answer: Word2vec solves the <b>sparsity</b> problem of models that use word identities as a feature by projecting words to a low dimensional space. You are right that in a given textual collection, words might occur only with limited context. So the word vectors that you learn will be pretty weak - but ...", "dateLastCrawled": "2022-01-21T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Answers to Selected Questions and Problems", "url": "https://media.lanecc.edu/users/loftl/CS275/TextReferenceMaterial/SelectedProblemSolutions/Answers%20to%20Selected%20Questions%20and%20Problems-Edition9.doc", "isFamilyFriendly": true, "displayUrl": "https://media.lanecc.edu/users/loftl/CS275/TextReferenceMaterial...", "snippet": "The rule may be phrased as All that is needed is <b>there</b>, and all that is <b>there</b> is needed. 9. A good <b>data</b> dictionary provides a precise description of the characteristics of all of the entities and attributes found within the database. The <b>data</b> dictionary thus makes it easy to check for the existence of synonyms and homonyms, to check whether all attributes exist to support required reports, and to verify appropriate relationship representations. The <b>data</b> dictionary&#39;s contents are developed ...", "dateLastCrawled": "2022-02-02T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - dontless/<b>Machine-Learning-Foundations-A-Case-Study-Approach</b> ...", "url": "https://github.com/dontless/Machine-Learning-Foundations-A-Case-Study-Approach", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dontless/<b>Machine-Learning-Foundations-A-Case-Study-Approach</b>", "snippet": "not interested in what a person does not <b>like</b> want to quickly discover relatively <b>few</b> liked items ^ imbalanced class problem Users have short attention span, so we want to recommend fewer items . higher cost to missing liked item Recall = (# liked &amp; shown) / (# liked) e.g 3/5 Precision = (# liked &amp; shown) / (# shown) e.g. 3/11 Optimal recommenders. How to maximize recall? recommend all my liked items. in this case precision is really small. e.g. 1000 total products, of which i liked 6 items ...", "dateLastCrawled": "2022-01-30T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "12 Types of Neural Networks Activation Functions: How to Choose? - V7Labs", "url": "https://www.v7labs.com/blog/neural-networks-activation-functions", "isFamilyFriendly": true, "displayUrl": "https://www.v7labs.com/blog/neural-networks-activation-functions", "snippet": "So the output of all the neurons will be of the same sign. This makes the <b>training</b> of the neural network more difficult and unstable. Tanh Function (Hyperbolic Tangent) Tanh function is <b>very</b> similar to the sigmoid/logistic activation function, and even has the same S-shape with the difference in output range of -1 to 1. In Tanh, the larger the ...", "dateLastCrawled": "2022-02-03T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: <b>Data</b> ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "<b>There</b> are several ways to make a model more robust to outliers, from different <b>points</b> of view (<b>data</b> preparation or model building). An outlier in the question and answer is assumed being unwanted, unexpected, or a must-be-wrong value to the human\u2019s knowledge so far (e.g. no one is 200 years old) rather than a rare event which is possible but rare.", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "Associative Rule Mining is one of the techniques to discover patterns in <b>data</b> <b>like</b> <b>features</b> (dimensions) which occur together and <b>features</b> (dimensions) which are correlated. It is mostly used in Market-based Analysis to find how frequently an itemset occurs in a transaction. Association rules have to satisfy minimum support and minimum confidence at the <b>very</b> same time. Association rule generation generally comprised of two different steps: \u201cA min support threshold is given to obtain all ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparsity</b> in Deep <b>Neural Networks - An Empirical Investigation with</b> ...", "url": "https://deepai.org/publication/sparsity-in-deep-neural-networks-an-empirical-investigation-with-tensorquant", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>sparsity</b>-in-deep-neural-networks-an-empirical...", "snippet": "When <b>training</b> on a distributed system, the <b>sparsity</b> in the gradients can help to reduce the amount of <b>data</b> which needs to be transferred to compute an update. So in this section, the <b>sparsity</b> of the gradient is investigated during <b>training</b>. <b>Similar</b> to the weights, <b>sparsity</b> needs to be enforced. In a single <b>training</b> run, the same threshold is applied to all gradients in every step. L2 regularization is used during <b>training</b>.", "dateLastCrawled": "2022-01-17T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Guide to <b>Classification</b> on Imbalanced Datasets - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/guide-to-classification-on-imbalanced-datasets-d6653aa5fa23", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/guide-to-<b>classification</b>-on-imbalanced-<b>dataset</b>s-d6653aa5fa23", "snippet": "A between-class imbalance occurs when <b>there</b> is an imbalance in the <b>number</b> of <b>data</b> <b>points</b> contained within each class. An example of this is shown below: An illustration of between-class imbalance. We have a large <b>number</b> of <b>data</b> <b>points</b> for the red class but relatively <b>few</b> for the white class. An example of this would be a mammography <b>dataset</b>, which uses images known as mammograms to predict breast cancer. Consider the <b>number</b> of mammograms related to positive and negative cancer diagnoses: The ...", "dateLastCrawled": "2022-02-03T01:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The State of <b>Sparsity</b> in Deep Neural Networks | DeepAI", "url": "https://deepai.org/publication/the-state-of-sparsity-in-deep-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-state-of-<b>sparsity</b>-in-deep-neural-networks", "snippet": "To account for the presence of <b>sparsity</b> at the start of <b>training</b>, they scale the variance of the initial weight distribution by the <b>number</b> of non-zeros in the matrix. They additionally train a variant where they increase the <b>number</b> of <b>training</b> steps (up to a factor of 2x) such that <b>the re</b>-trained model uses approximately the same <b>number</b> of FLOPs during <b>training</b> as model trained with sparsification as part of the optimization process. They refer to these two experiments as \u201dscratch-e\u201d and ...", "dateLastCrawled": "2022-01-09T01:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Solving the <b>data sparsity problem</b> in destination prediction", "url": "https://link.springer.com/article/10.1007/s00778-014-0369-7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-014-0369-7", "snippet": "However, this approach encounters the \u201c<b>data sparsity problem</b>\u201d, i.e., the available historical trajectories are far from enough to cover all possible query trajectories, which considerably limits the <b>number</b> of query trajectories that can obtain predicted destinations. We propose a novel method named Sub-Trajectory Synthesis (SubSyn) to address the <b>data sparsity problem</b>. SubSyn first decomposes historical trajectories into sub-trajectories comprising two adjacent locations, and then ...", "dateLastCrawled": "2021-11-18T11:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machine Learning in Medicine", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5831252/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5831252", "snippet": "Some algorithms such as non-linear support vector machines 15 can be extremely robust in a variety of situations even where the <b>number</b> of predictive <b>features</b> is <b>very</b> large compared <b>to the number</b> of <b>training</b> examples, a situation where over-fitting often occurs. Finally, accepting the limitations of each class of algorithms, some practitioners use a process called blending, merging the outputs of multiple different algorithms (also discussed below).", "dateLastCrawled": "2022-02-02T20:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "12 Types of Neural Networks Activation Functions: How to Choose? - V7Labs", "url": "https://www.v7labs.com/blog/neural-networks-activation-functions", "isFamilyFriendly": true, "displayUrl": "https://www.v7labs.com/blog/neural-networks-activation-functions", "snippet": "\ud83d\udca1 Activation Function helps the neural network to use important information while suppressing irrelevant <b>data</b> <b>points</b>. ... Tanh function is <b>very</b> <b>similar</b> to the sigmoid/logistic activation function, and even has the same S-shape with the difference in output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0. Tanh Function (Hyperbolic Tangent ...", "dateLastCrawled": "2022-02-03T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>How does Word2vec solve sparse problem</b>? - Quora", "url": "https://www.quora.com/How-does-Word2vec-solve-sparse-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>How-does-Word2vec-solve-sparse-problem</b>", "snippet": "Answer: Word2vec solves the <b>sparsity</b> problem of models that use word identities as a feature by projecting words to a low dimensional space. You are right that in a given textual collection, words might occur only with limited context. So the word vectors that you learn will be pretty weak - but ...", "dateLastCrawled": "2022-01-21T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Answers to Selected Questions and Problems", "url": "https://media.lanecc.edu/users/loftl/CS275/TextReferenceMaterial/SelectedProblemSolutions/Answers%20to%20Selected%20Questions%20and%20Problems-Edition9.doc", "isFamilyFriendly": true, "displayUrl": "https://media.lanecc.edu/users/loftl/CS275/TextReferenceMaterial...", "snippet": "The rule may be phrased as All that is needed is <b>there</b>, and all that is <b>there</b> is needed. 9. A good <b>data</b> dictionary provides a precise description of the characteristics of all of the entities and attributes found within the database. The <b>data</b> dictionary thus makes it easy to check for the existence of synonyms and homonyms, to check whether all attributes exist to support required reports, and to verify appropriate relationship representations. The <b>data</b> dictionary&#39;s contents are developed ...", "dateLastCrawled": "2022-02-02T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Topic Modeling with <b>Latent Dirichlet Allocation</b> - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/topic-modeling-with-latent-dirichlet-allocation-e7ff75290f8", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/topic-modeling-with-<b>latent-dirichlet-allocation</b>-e7ff75290f8", "snippet": "A <b>few</b> years later, LDA was applied to the field of machine learning by Blei et al., 2003, ... Essentially, the t-SNE technique works to convert similarities between <b>data</b> <b>points</b> to joint probabilities, and then tries to minimize the KL divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional <b>data</b>. The dimensionality of the LDA model is determined by the <b>number</b> of <b>features</b> set during <b>training</b> (usually a minimum of 10,000); therefore, t-SNE can be ...", "dateLastCrawled": "2022-02-03T02:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - dontless/<b>Machine-Learning-Foundations-A-Case-Study-Approach</b> ...", "url": "https://github.com/dontless/Machine-Learning-Foundations-A-Case-Study-Approach", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dontless/<b>Machine-Learning-Foundations-A-Case-Study-Approach</b>", "snippet": "Filtering <b>data</b>: One of the key <b>features</b> we used in our model was the <b>number</b> of square feet of living space (sqft_living) in the house. For this part, we are going to use the idea of filtering (selecting) <b>data</b>. In particular, we are going to use logical filters to select rows of an SFrame. You can find more info in the Logical Filter section of ...", "dateLastCrawled": "2022-01-30T23:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Sparse Reconstruction Techniques in MRI: Methods, Applications, and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4948115/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4948115", "snippet": "MRI <b>Data</b> Collection and Acceleration. MRI <b>data</b> are collected in a space that is mathematically related to the image, called k-space. Once the k-space <b>data</b> have been collected, they <b>can</b> then be transformed into an image using the mathematical function that relates the two spaces, the Fourier Transform (as shown in Figure 1a).Collecting a complete set of k-space <b>points</b> <b>can</b> be time-consuming, and <b>there</b> are many situations in which it is desirable to accelerate the <b>data</b> collection process by ...", "dateLastCrawled": "2022-01-25T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Statistical Learning with Sparsity The Lasso and Generalizations</b> Pages ...", "url": "https://fliphtml5.com/mofx/mxtu/basic/201-250", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/mofx/mxtu/basic/201-250", "snippet": "A <b>number</b> of au-thors have explored how <b>sparsity</b> <b>can</b> still allow for consistent estimation ofprincipal components even when p N . Johnstone and Lu (2009) propose atwo-stage procedure, based on thresholding the diagonal of the sample covari-ance matrix in order to isolate the highest variance coordinates, and then per-forming PCA in the reduced-dimensional space. They prove consistency of thismethod even when p/N stays bounded away from zero, but allow only polyno-mial growth of p as a ...", "dateLastCrawled": "2022-01-11T15:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Machine Learning in Medical Imaging", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4220564/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4220564", "snippet": "For example, Figure 10 (adapted from ) illustrates that a simple linear model <b>can</b> outperform a flexible nonlinear model (in this case an ANN) until <b>there</b> are enough <b>data</b> examples to support estimation of the greater <b>number</b> of parameters inherent in the nonlinear model. Nevertheless, these issues are frequently ignored in the current brain mapping literature when discussing or comparing different analysis techniques.", "dateLastCrawled": "2022-02-03T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep neural network with weight <b>sparsity</b> control and pre-<b>training</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S1053811915003985", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1053811915003985", "snippet": "Based on the results of a three-way ANOVA with three factors, including the use of <b>sparsity</b> control, use of pre-<b>training</b>, and the <b>number</b> of hidden layers of the DNN, the statistical significance of the interaction between the <b>number</b> of hidden layers and the use of pre-<b>training</b> (Bonferroni-corrected p &lt; 10 \u2212 7; d.f. = 499) was greater than that between the <b>number</b> of hidden layers and the use of <b>sparsity</b> control (Bonferroni-corrected p &lt; 0.05; d.f. = 499). Pre-<b>training</b> benefitted DNNs with a ...", "dateLastCrawled": "2021-12-28T15:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Make Some ROOM for the Zeros: <b>Data</b> <b>Sparsity</b> in Secure Distributed ...", "url": "https://www.researchgate.net/publication/331864762_Make_Some_ROOM_for_the_Zeros_Data_Sparsity_in_Secure_Distributed_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/331864762_Make_Some_ROOM_for_the_Zeros_<b>Data</b>...", "snippet": "Exploiting <b>data</b> <b>sparsity</b> is crucial for the scalability of many <b>data</b> analysis tasks. However, while <b>there</b> is an increasing interest in efficient secure computation protocols for distributed ...", "dateLastCrawled": "2022-01-31T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Topic Modeling with <b>Latent Dirichlet Allocation</b> - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/topic-modeling-with-latent-dirichlet-allocation-e7ff75290f8", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/topic-modeling-with-<b>latent-dirichlet-allocation</b>-e7ff75290f8", "snippet": "Essentially, the t-SNE technique works to convert similarities between <b>data</b> <b>points</b> to joint probabilities, and then tries to minimize the KL divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional <b>data</b>. The dimensionality of the LDA model is determined by the <b>number</b> of <b>features</b> set during <b>training</b> (usually a minimum of 10,000); therefore, t-SNE <b>can</b> be used to reduce the dimensions to a 2-D embedding that offers a visualization of the clustering ...", "dateLastCrawled": "2022-02-03T02:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "12 Types of Neural Networks Activation Functions: How to Choose? - V7Labs", "url": "https://www.v7labs.com/blog/neural-networks-activation-functions", "isFamilyFriendly": true, "displayUrl": "https://www.v7labs.com/blog/neural-networks-activation-functions", "snippet": "\ud83d\udca1 Activation Function helps the neural network to use important information while suppressing irrelevant <b>data</b> <b>points</b>. ... The output of the sigmoid function was in the range of 0 to 1, which <b>can</b> <b>be thought</b> of as probability. But\u2014 This function faces certain problems. Let\u2019s suppose we have five output values of 0.8, 0.9, 0.7, 0.8, and 0.6, respectively. How <b>can</b> we move forward with it? The answer is: We <b>can</b>\u2019t. The above values don\u2019t make sense as the sum of all the classes/output ...", "dateLastCrawled": "2022-02-03T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: <b>Data</b> ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "This overfitting of the <b>training</b> <b>data</b> <b>can</b> negatively affect the modeling power of the method and cripple the predictive accuracy. 2. Explain what regularization is and why it is useful. Regularization is the process of adding a tuning parameter to a model to induce smoothness in order to prevent overfitting. This is most often done by adding a constant multiple to an existing weight vector. This constant is often either the L1 (Lasso) or L2 (ridge), but <b>can</b> in actuality <b>can</b> be any norm. The ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Random Oversampling and Undersampling for Imbalanced Classification", "url": "https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/random-oversampling-and-undersampling-for...", "snippet": "Imbalanced datasets are those where <b>there</b> is a severe skew in the class distribution, such as 1:100 or 1:1000 examples in the minority class to the majority class. This bias in the <b>training</b> dataset <b>can</b> influence many machine learning algorithms, leading some to ignore the minority class entirely. This is a problem as it is typically the minority class on which", "dateLastCrawled": "2022-02-03T00:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Higher criticism thresholding: Optimal feature selection when useful ...", "url": "https://www.pnas.org/content/105/39/14790", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/105/39/14790", "snippet": "Moreover, reported misclassification rates are relatively high. Hence, the dimension p of the feature vector is <b>very</b> large, and although <b>there</b> may be numerous useful <b>features</b>, they are relatively rare and individually quite weak. Consider the following rare/weak feature model (RW feature model). We suppose the contrast vector \u03bc to be nonzero in only k out of p elements, where \u03b5 = k/p is small, that is, close to zero. As an example, we might have p = 10,000, k = 100, and so \u03b5 = k/p = 0.01 ...", "dateLastCrawled": "2022-01-17T16:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Solving the <b>data sparsity problem</b> in destination prediction | SpringerLink", "url": "https://link.springer.com/article/10.1007/s00778-014-0369-7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-014-0369-7", "snippet": "<b>Compared</b> to the uniform grid partitioning strategy, the two proposed strategies result in <b>data</b> <b>points</b> more evenly distributed in different cells, and hence a smaller variance, i.e., a smaller variation in the <b>number</b> of <b>data</b> <b>points</b> per cell. This leads to a larger \\(H\\), i.e., less information loss.", "dateLastCrawled": "2021-11-18T11:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>How does Word2vec solve sparse problem</b>? - Quora", "url": "https://www.quora.com/How-does-Word2vec-solve-sparse-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>How-does-Word2vec-solve-sparse-problem</b>", "snippet": "Answer: Word2vec solves the <b>sparsity</b> problem of models that use word identities as a feature by projecting words to a low dimensional space. You are right that in a given textual collection, words might occur only with limited context. So the word vectors that you learn will be pretty weak - but ...", "dateLastCrawled": "2022-01-21T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Power <b>of Sparsity in Convolutional Neural Networks</b> | Request PDF", "url": "https://www.researchgate.net/publication/313879627_The_Power_of_Sparsity_in_Convolutional_Neural_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/313879627_The_Power_of_<b>Sparsity</b>_in...", "snippet": "This structure <b>can</b> either be hand-tuned such as &quot;structured <b>sparsity</b>&quot; for transformers [Child et al. 2019], <b>sparsity</b> determined in a pre-<b>training</b> phase [You et al. 2020], or <b>data</b>-independent ...", "dateLastCrawled": "2022-01-13T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sparsity</b>-promoting algorithms for the discovery of informative Koopman ...", "url": "https://deepai.org/publication/sparsity-promoting-algorithms-for-the-discovery-of-informative-koopman-invariant-subspaces", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>sparsity</b>-promoting-algorithms-for-the-disco<b>very</b>-of...", "snippet": "<b>There</b> have been a <b>few</b> attempts towards mode selection in EDMD/KDMD. Brunton et al. present an iterative method that augments the dictionary of EDMD until a convergence criterion is reached for the subspace. This is effectively a recursive implementation of EDMD. Recently, Haseli and Cort\u00e9s showed that given a sufficient amount of <b>data</b>, if <b>there</b> is any accurate Koopman eigenfunction spanned by the dictionary, it must correspond to one of the obtained eigenvectors. Moreover, they proposed the ...", "dateLastCrawled": "2021-12-14T10:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Guide to <b>Classification</b> on Imbalanced Datasets - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/guide-to-classification-on-imbalanced-datasets-d6653aa5fa23", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/guide-to-<b>classification</b>-on-imbalanced-<b>dataset</b>s-d6653aa5fa23", "snippet": "A between-class imbalance occurs when <b>there</b> is an imbalance in the <b>number</b> of <b>data</b> <b>points</b> contained within each class. An example of this is shown below: An illustration of between-class imbalance. We have a large <b>number</b> of <b>data</b> <b>points</b> for the red class but relatively <b>few</b> for the white class. An example of this would be a mammography <b>dataset</b>, which uses images known as mammograms to predict breast cancer. Consider the <b>number</b> of mammograms related to positive and negative cancer diagnoses: The ...", "dateLastCrawled": "2022-02-03T01:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Machine Learning in Medicine", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5831252/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5831252", "snippet": "Some algorithms such as non-linear support vector machines 15 <b>can</b> be extremely robust in a variety of situations even where the <b>number</b> of predictive <b>features</b> is <b>very</b> large <b>compared</b> <b>to the number</b> of <b>training</b> examples, a situation where over-fitting often occurs. Finally, accepting the limitations of each class of algorithms, some practitioners use a process called blending, merging the outputs of multiple different algorithms (also discussed below).", "dateLastCrawled": "2022-02-02T20:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: <b>Data</b> ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "This overfitting of the <b>training</b> <b>data</b> <b>can</b> negatively affect the modeling power of the method and cripple the predictive accuracy. 2. Explain what regularization is and why it is useful. Regularization is the process of adding a tuning parameter to a model to induce smoothness in order to prevent overfitting. This is most often done by adding a constant multiple to an existing weight vector. This constant is often either the L1 (Lasso) or L2 (ridge), but <b>can</b> in actuality <b>can</b> be any norm. The ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Topic Modeling with <b>Latent Dirichlet Allocation</b> - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/topic-modeling-with-latent-dirichlet-allocation-e7ff75290f8", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/topic-modeling-with-<b>latent-dirichlet-allocation</b>-e7ff75290f8", "snippet": "The same approach <b>can</b> be used for choosing the <b>number</b> of <b>features</b>, which is equivalent to setting a fixed size for the vocabulary. The greater the <b>number</b> of <b>features</b>, the longer the LDA model will take to train; however, a sufficiently-sized vocabulary is necessary to capture the most important words for clustering of topics. Generally, setting the <b>number</b> of <b>features</b> to 10,000 is a good starting point for most models. This value needs to be fine-tuned depending on the size of the dataset and ...", "dateLastCrawled": "2022-02-03T02:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "12 Types of Neural Networks Activation Functions: How to Choose? - V7Labs", "url": "https://www.v7labs.com/blog/neural-networks-activation-functions", "isFamilyFriendly": true, "displayUrl": "https://www.v7labs.com/blog/neural-networks-activation-functions", "snippet": "So the output of all the neurons will be of the same sign. This makes the <b>training</b> of the neural network more difficult and unstable. Tanh Function (Hyperbolic Tangent) Tanh function is <b>very</b> similar to the sigmoid/logistic activation function, and even has the same S-shape with the difference in output range of -1 to 1. In Tanh, the larger the ...", "dateLastCrawled": "2022-02-03T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Over-parameterization: Pitfalls and Opportunities", "url": "https://icml.cc/virtual/2021/workshop/8357", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2021/workshop/8357", "snippet": "We show that the min 2-norm interpolator of <b>training</b> <b>data</b> <b>can</b> be susceptible even to adversaries who <b>can</b> only perturb the low-dimensional inputs and not the high-dimensional lifted <b>features</b> directly. The adversarial vulnerability arises because of a phenomena we term spatial localization: the predictions of the learned model are markedly more sensitive in the vicinity of <b>training</b> <b>points</b> than elsewhere. This sensitivity is crucially a consequence of feature lifting and <b>can</b> have consequences ...", "dateLastCrawled": "2022-02-02T15:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Sparsity</b> is an essential feature of many contemporary data problems. Remote sensing, various forms of automated screening and other high throughput measurement devices collect a large amount of ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Regularization \u2014 Understanding L1 and L2 regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2...", "snippet": "The <b>sparsity</b> feature used in L1 regularization has been used extensively as a feature selection mechanism in <b>machine</b> <b>learning</b>. Feature selection is a mechanism which inherently simplifies a ...", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "An E\ufb03cient Sparse Metric <b>Learning</b> in High ... - <b>Machine</b> <b>Learning</b>", "url": "http://machinelearning.org/archive/icml2009/papers/46.pdf", "isFamilyFriendly": true, "displayUrl": "<b>machinelearning</b>.org/archive/icml2009/papers/46.pdf", "snippet": "This <b>sparsity</b> prior of <b>learning</b> distance metric serves to regularize the com-plexity of the distance model especially in the \u201cless example number p and high dimension d\u201d setting. Theoretically, by <b>analogy</b> to the covariance estimation problem, we \ufb01nd the proposed distance <b>learning</b> algorithm has a consistent result at rate O!&quot;# m2 logd $% n &amp; to the target distance matrix with at most m nonzeros per row. Moreover, from the imple-mentation perspective, this! 1-penalized log-determinant ...", "dateLastCrawled": "2021-11-19T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Dynamical <b>machine</b> <b>learning</b> volumetric reconstruction of objects ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8027224/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8027224", "snippet": "The sequence index in the angle of illumination plays the role of discrete time in the dynamical system <b>analogy</b>. Thus, the imaging problem turns into a problem of nonlinear system identification, which also suggests dynamical <b>learning</b> as a better fit to regularize the reconstructions. We devised a Recurrent Neural Network (RNN) architecture with a novel Separable-Convolution Gated Recurrent Unit (SC-GRU) as the fundamental building block. Through a comprehensive comparison of several ...", "dateLastCrawled": "2022-01-08T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Discovering governing equations from data</b> by sparse identification of ...", "url": "https://www.pnas.org/content/pnas/113/15/3932.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/<b>pnas</b>/113/15/3932.full.pdf", "snippet": "examples. In this work, we combin e <b>sparsity</b>-promoting techniques and <b>machine</b> <b>learning</b> with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only as-sumption about the structureof the model is that there are onlya few important terms that govern the dy namics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to ...", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "What are the <b>basic concepts in machine learning</b>? I found that the best way to discover and get a handle on the <b>basic concepts in machine learning</b> is to review the introduction chapters to <b>machine learning</b> textbooks and to watch the videos from the first model in online courses. Pedro Domingos is a lecturer and professor on <b>machine learning</b> at the University of Washing and", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Dynamical machine learning volumetric reconstruction of</b> objects ...", "url": "https://www.nature.com/articles/s41377-021-00512-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41377-021-00512-x", "snippet": "Recently, thanks to a ground-breaking observation from 2010 that <b>sparsity</b> can be learnt by a deep neural network 48, the idea of using <b>machine</b> <b>learning</b> to approximate solutions to inverse problems ...", "dateLastCrawled": "2022-02-02T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[2107.02306] Connectivity Matters: Neural Network Pruning Through the ...", "url": "https://arxiv.org/abs/2107.02306", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2107.02306", "snippet": "Further, equipped with effective <b>sparsity</b> as a reference frame, we partially reconfirm that random pruning with appropriate <b>sparsity</b> allocation across layers performs as well or better than more sophisticated algorithms for pruning at initialization [Su et al., 2020]. In response to this observation, using a simple <b>analogy</b> of pressure distribution in coupled cylinders from physics, we design novel layerwise <b>sparsity</b> quotas that outperform all existing baselines in the context of random pruning.", "dateLastCrawled": "2021-07-07T06:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective function to perform maximum or minimum evaluation. In reality, optimization is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "regression - Why L1 norm for sparse models - Cross Validated", "url": "https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/45643", "snippet": "There are many norms that lead to <b>sparsity</b> (e.g., as you mentioned, any Lp norm with p &lt;= 1). In general, any norm with a sharp corner at zero induces <b>sparsity</b>. So, going back to the original question - the L1 norm induces <b>sparsity</b> by having a discontinuous gradient at zero (and any other penalty with this property will do so too). $\\endgroup$", "dateLastCrawled": "2022-01-26T23:44:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Learning Neural Representations for Network Anomaly Detection</b>", "url": "https://www.researchgate.net/publication/325797465_Learning_Neural_Representations_for_Network_Anomaly_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/325797465_<b>Learning</b>_Neural_Representations_for...", "snippet": "Many <b>machine</b> <b>learning</b> algorithms have been. Manuscript received December 22, 2017; revised March 13, 2018. This. work is funded by Vietnam International Education De velopment (VIED) and. by ...", "dateLastCrawled": "2021-12-06T22:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Self-representation based dual-graph regularized <b>feature selection</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation ... Her current research interests include pattern recognition and <b>machine</b> <b>learning</b>. Licheng Jiao (SM\u05f389) received the B.S. degree from Shanghai Jiaotong University, Shanghai, China, in 1982, the M.S. and Ph.D. degrees from Xi\u05f3an Jiaotong University, Xi\u05f3an, China, in 1984 and 1990, respectively. From 1990 to 1991, he was a postdoctoral Fellow in the National Key Laboratory for Radar Signal ...", "dateLastCrawled": "2021-11-22T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Self-representation based dual-graph regularized feature selection ...", "url": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "snippet": "<b>machine</b> <b>learning</b> and computer vision \ufb01elds [41]. <b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation [41]. Taking into account of manifold <b>learning</b> and feature selection, and inspired by the self-representation property and the idea of dual-regularization <b>learning</b> [44,45], we propose a novel feature selection algorithm for clustering, named self-representation based dual-graph regularized feature selection clustering (DFSC). This algorithm ...", "dateLastCrawled": "2022-02-02T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Unsupervised feature selection</b> by <b>regularized self-representation</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation. With the above considerations, in this paper we propose a simple yet very effective <b>unsupervised feature selection</b> method by exploiting the self-representation ability of features. The feature matrix is represented over itself to find the representative feature components. The representation residual is minimized by L 2, 1-norm loss to reduce the effect of outlier samples. Different from the ...", "dateLastCrawled": "2022-01-24T12:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Talk <b>Archive</b> - Research on Algorithms and Incentives in Networks", "url": "https://rain.stanford.edu/schedule/archive.shtml", "isFamilyFriendly": true, "displayUrl": "https://rain.stanford.edu/schedule/<b>archive</b>.shtml", "snippet": "McFowland\u2019s research interests\u2014which lie at the intersection of Information Systems, <b>Machine</b> <b>Learning</b>, and Public Policy\u2014include the development of computationally efficient algorithms for large-scale statistical <b>machine</b> <b>learning</b> and \u201cbig data\u201d analytics. More specifically, his research seeks to demonstrate that many real-world problems faced by organizations, and society more broadly, can be reduced to the tasks of anomalous pattern detection and discovery. As a data and ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Talks - <b>sites.google.com</b>", "url": "https://sites.google.com/view/dssseminarseries/talks", "isFamilyFriendly": true, "displayUrl": "https://<b>sites.google.com</b>/view/dssseminarseries/talks", "snippet": "Abstracts &amp; Bios for upcoming talks", "dateLastCrawled": "2022-01-27T14:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Sparse representations for text categorization</b>", "url": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text_categorization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text...", "snippet": "<b>Machine</b> <b>learning</b> for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is ...", "dateLastCrawled": "2021-12-10T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Continual Learning via Neural Pruning</b> | DeepAI", "url": "https://deepai.org/publication/continual-learning-via-neural-pruning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>continual-learning-via-neural-pruning</b>", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more efficient use of resources in machines with memory constraints.", "dateLastCrawled": "2021-12-30T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Non-negative data-<b>driven mapping of structural connections</b> with ...", "url": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "snippet": "For ICA, <b>sparsity can be thought of as</b> a proxy for independence. 3.5. In-vivo data decompositions. For real data, we decomposed group-average tractography matrices, using independent component analysis (ICA) and non-negative matrix factorisation (NMF), with a range of model orders K. ICA was initialised with regular PCA, in which the first 500 components were retained (explaining 97% of the total variance). ICA was applied to the reduced dataset using the FastICA algorithm (Hyv\u00e4rinen and ...", "dateLastCrawled": "2021-10-11T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Sparse Representations for Text Categorization</b> | Dimitri Kanevsky ...", "url": "https://www.academia.edu/2738730/Sparse_Representations_for_Text_Categorization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2738730/<b>Sparse_Representations_for_Text_Categorization</b>", "snippet": "A Comparative Study of <b>Machine</b> <b>Learning</b> Methods for Verbal Autopsy Text Classification. By Eric S Atwell and Samuel Danso. CSC435 book proposal. By Russell Frith. Higher-Order Smoothing: A Novel Semantic Smoothing Method for Text Classification. By Murat C Ganiz, Mitat Poyraz, and Zeynep Kilimci. INFORMATION RETRIEVAL. By febi k. Introduction to information retrieval. By Valeria Mesi. Download pdf. \u00d7 Close Log In. Log In with Facebook Log In with Google. Sign Up with Apple. or. Email ...", "dateLastCrawled": "2021-10-13T23:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Continual <b>Learning</b> via Neural Pruning \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1903.04476/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1903.04476", "snippet": "We introduce Continual <b>Learning</b> via Neural Pruning (CLNP), a new method aimed at lifelong <b>learning</b> in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the ...", "dateLastCrawled": "2021-11-07T10:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Continual <b>Learning</b> via Neural Pruning", "url": "https://openreview.net/pdf?id=Hyl_XXYLIB", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=Hyl_XXYLIB", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much at-tention from the <b>machine</b> <b>learning</b> community in recent years. The main obstacle for effective continual <b>learning</b> is the problem of cata-strophic forgetting: machines trained on new problems forget about", "dateLastCrawled": "2022-01-05T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Abstract - arXiv.org e-Print archive", "url": "https://arxiv.org/pdf/1903.04476", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1903.04476", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more ef\ufb01cient use of resources in machines with memory constraints. There is also great interest in continual <b>learning</b> from a more long term ...", "dateLastCrawled": "2021-10-25T14:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Introduction to compressed sensing</b>", "url": "https://www.researchgate.net/publication/220043734_Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220043734_<b>Introduction_to_compressed_sensing</b>", "snippet": "systems control, clustering, and <b>machine</b> <b>learning</b> [14, 15, 58, 61, 89, 193, 217, 240, 244]. Low-dimensional manifolds hav e also been prop osed as approximate mod-", "dateLastCrawled": "2022-01-14T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Introduction to compressed sensing</b> | Marco Duarte - Academia.edu", "url": "https://www.academia.edu/1443164/Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/1443164/<b>Introduction_to_compressed_sensing</b>", "snippet": "<b>Introduction to Compressed Sensing</b> For any x \u2208 \u03a3k , we can associate a k-face of C n with the support and sign pattern of x. One can show that the number of k-faces of AC n is precisely the number of index sets of size k for which signals supported on them can be recovered by (1.12) with B (y) = {z : Az = y}.", "dateLastCrawled": "2022-01-21T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Compressed Sensing : Theory and Applications</b> | Kutyniok, Gitta Eldar ...", "url": "https://b-ok.africa/book/2086657/84a688", "isFamilyFriendly": true, "displayUrl": "https://b-ok.africa/book/2086657/84a688", "snippet": "You can write a book review and share your experiences. Other readers will always be interested in your opinion of the books you&#39;ve read. Whether you&#39;ve loved the book or not, if you give your honest and detailed thoughts then people will find new books that are right for them.", "dateLastCrawled": "2021-12-26T07:22:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparsity)  is like +(when there are very few training data points relative to the number of desired features)", "+(sparsity) is similar to +(when there are very few training data points relative to the number of desired features)", "+(sparsity) can be thought of as +(when there are very few training data points relative to the number of desired features)", "+(sparsity) can be compared to +(when there are very few training data points relative to the number of desired features)", "machine learning +(sparsity AND analogy)", "machine learning +(\"sparsity is like\")", "machine learning +(\"sparsity is similar\")", "machine learning +(\"just as sparsity\")", "machine learning +(\"sparsity can be thought of as\")", "machine learning +(\"sparsity can be compared to\")"]}