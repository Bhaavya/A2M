{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sequence-to-Sequence</b> Models Can Directly Translate <b>Foreign</b> Speech", "url": "https://www.isca-speech.org/archive/pdfs/interspeech_2017/weiss17_interspeech.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.isca-speech.org/archive/pdfs/interspeech_2017/weiss17_interspeech.pdf", "snippet": "languages by multi-<b>task</b> training <b>sequence-to-sequence</b> speech translation and recognition models with a shared encoder net-work can improve performance by a further 1.4 BLEU points. Index Terms: speech translation, <b>sequence-to-sequence</b> model 1. Introduction <b>Sequence-to-sequence</b> models were recently introduced as a pow-erful new method for translation [1, 2]. Subsequently, the model has been adapted and applied to various tasks such as image captioning [3, 4], pose prediction [5], and ...", "dateLastCrawled": "2021-10-24T07:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequence-to-Sequence</b> Models Can Directly Translate <b>Foreign</b> Speech ...", "url": "https://www.arxiv-vanity.com/papers/1703.08581/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.08581", "snippet": "The model does not explicitly transcribe the speech into text in the source <b>language</b>, nor does it require supervision from the ground truth source <b>language</b> transcription during training. We apply a slightly modified <b>sequence-to-sequence</b> with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex <b>task</b>, illustrating the power of attention-based models. A single model trained end-to-end obtains state-of-the-art ...", "dateLastCrawled": "2021-10-17T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequence-to-Sequence</b> Models Can Directly Transcribe <b>Foreign</b> Speech ...", "url": "https://www.researchgate.net/publication/315667124_Sequence-to-Sequence_Models_Can_Directly_Transcribe_Foreign_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315667124_<b>Sequence-to-Sequence</b>_Models_Can...", "snippet": "In addition, we find that making use of the training data in both languages by multi-<b>task</b> training <b>sequence-to-sequence</b> speech translation and recognition models with a shared encoder network can ...", "dateLastCrawled": "2022-01-18T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sequence-to-Sequence Models Can Directly Translate Foreign Speech</b> ...", "url": "https://www.researchgate.net/publication/319185738_Sequence-to-Sequence_Models_Can_Directly_Translate_Foreign_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319185738_<b>Sequence-to-Sequence</b>_Models_Can...", "snippet": "<b>Sequence-to-sequence</b> based speech translation has shown ... of <b>translating</b> utterances in one <b>language</b> into text in a different <b>language</b>. Direct ST is an emerging paradigm that consists in ...", "dateLastCrawled": "2021-12-11T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "15 <b>Other Sequence-to-sequence Applications</b>", "url": "http://www.phontron.com/class/mtandseq2seq2017/mt-spring2017.chapter15.pdf", "isFamilyFriendly": true, "displayUrl": "www.phontron.com/class/mtandseq2seq2017/mt-spring2017.chapter15.pdf", "snippet": "a single <b>language</b>, <b>translating</b>, for example, English into English. 15.2.1 Summarization One typical example of this is text summarization. In the summarization <b>task</b>, one is required to take a larger body of text and convert it into a smaller amount of text containing the same information for browsing purposes. This can be done at a number of levels: Sentence Compression: The problem of compressing a single sentence into a shorter single sentence [31]. Single-document Summarization: The ...", "dateLastCrawled": "2021-12-15T18:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Discontinuous Grammar as <b>a Foreign</b> <b>Language</b>", "url": "https://deepai.org/publication/discontinuous-grammar-as-a-foreign-language", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/discontinuous-grammar-as-<b>a-foreign</b>-<b>language</b>", "snippet": "10/20/21 - In order to achieve deep natural <b>language</b> understanding, syntactic constituent parsing is a vital step, highly demanded by many ar...", "dateLastCrawled": "2022-01-11T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Grammar as a Foreign Language</b> | DeepAI", "url": "https://deepai.org/publication/grammar-as-a-foreign-language", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>grammar-as-a-foreign-language</b>", "snippet": "<b>Grammar as a Foreign Language</b>. 12/23/2014 . \u2219 . by Oriol Vinyals, et al. \u2219. Google \u2219. 0 \u2219. share Syntactic constituency parsing is a fundamental problem in natural <b>language</b> processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced <b>sequence-to-sequence</b> model achieves state-of-the-art results on the most ...", "dateLastCrawled": "2022-01-20T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Grammar as <b>a Foreign</b> <b>Language</b> | \u0141ukasz Kaiser - Academia.edu", "url": "https://www.academia.edu/24867358/Grammar_as_a_Foreign_Language", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/24867358/Grammar_as_<b>a_Foreign</b>_<b>Language</b>", "snippet": "As a <b>sequence-to-sequence</b> prediction model it is somewhat related to the incremental parsing models, pioneered by [26] and extended by [27]. Such linear time parsers however typically need some <b>task</b>-specific constraints and might build up the parse in multiple passes. Relatedly, [13] present excellent parsing results with a single left-to-right pass, but require a stack to explicitly delay making decisions and a parsing-specific transition strategy in order to achieve good parsing accuracies ...", "dateLastCrawled": "2022-01-13T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Natural <b>Language</b> Processing on Hinglish \u2013 DataDev", "url": "https://itsdatalytical.wordpress.com/2021/04/13/natural-language-processing-on-hinglish/", "isFamilyFriendly": true, "displayUrl": "https://itsdatalytical.wordpress.com/2021/04/13/natural-<b>language</b>-processing-on-hinglish", "snippet": "A typical <b>sequence to sequence</b> model has two parts \u2013 an encoder and a decoder. Both the parts are practically two different neural network models combined into one giant network. Broadly, the <b>task</b> of an encoder network is to understand the input sequence, and create a smaller dimensional representation of it. This representation is then ...", "dateLastCrawled": "2022-01-12T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Language</b> <b>Translation</b> with RNNs. Build a recurrent neural network (RNN ...", "url": "https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>language</b>-<b>translation</b>-with-rnns-d84d43b40571", "snippet": "Building the Pipeline. Below is a summary of the various preprocessing and modeling steps. The high-level steps include: Preprocessing: load and examine data, cleaning, tokenization, padding; Modeling: build, train, and test the model; Prediction: generate specific translations of English to French, and compare the output translations to the ground truth translations; Iteration: iterate on the model, experimenting with different architectures; For a more detailed walkthrough including the ...", "dateLastCrawled": "2022-02-02T17:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sequence-to-Sequence Models Can Directly Translate Foreign Speech</b> ...", "url": "https://www.researchgate.net/publication/319185738_Sequence-to-Sequence_Models_Can_Directly_Translate_Foreign_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319185738_<b>Sequence-to-Sequence</b>_Models_Can...", "snippet": "<b>Sequence-to-sequence</b> based speech translation ... level sub-<b>task</b>. As a result, models with <b>similar</b> performance on these benchmarks may have unobserved performance differences on the other sub ...", "dateLastCrawled": "2021-12-11T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "15 <b>Other Sequence-to-sequence Applications</b>", "url": "http://www.phontron.com/class/mtandseq2seq2017/mt-spring2017.chapter15.pdf", "isFamilyFriendly": true, "displayUrl": "www.phontron.com/class/mtandseq2seq2017/mt-spring2017.chapter15.pdf", "snippet": "a single <b>language</b>, <b>translating</b>, for example, English into English. 15.2.1 Summarization One typical example of this is text summarization. In the summarization <b>task</b>, one is required to take a larger body of text and convert it into a smaller amount of text containing the same information for browsing purposes. This can be done at a number of levels: Sentence Compression: The problem of compressing a single sentence into a shorter single sentence [31]. Single-document Summarization: The ...", "dateLastCrawled": "2021-12-15T18:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "AI Quantification of <b>Language</b> Puzzle to <b>Language</b> Learning ...", "url": "http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "snippet": "The <b>language</b> model is the formalization of this idea. As mentioned in [2], <b>translating</b> a sentence from one <b>language</b> to the other is easy if the structure of the languages <b>is similar</b>. In a case where structures are different, and sentences are taken as a sequence, <b>sequence-to-sequence</b> technique can be applied for the translation.", "dateLastCrawled": "2021-11-13T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ICON2021 SSMT Tutorial", "url": "https://www.cfilt.iitb.ac.in/events/ICON2021_SSMT_Tutorial.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cfilt.iitb.ac.in/events/ICON2021_SSMT_Tutorial.pdf", "snippet": "\u2013 An attention-based <b>sequence-to-sequence</b> neural network which can directly translate speech from one <b>language</b> into speech in another <b>language</b> \u2022 Direct S2ST with Discrete Units by Facebook AI and Johns Hopkins University (2021) , state-of-the-art in Direct S2ST 16", "dateLastCrawled": "2022-01-23T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Encoder-Decoder Shift-Reduce Syntactic Parsing</b>", "url": "http://medialab.di.unipi.it/depling/assets/docs/day5/05-liu+zhang.pdf", "isFamilyFriendly": true, "displayUrl": "medialab.di.unipi.it/depling/assets/docs/day5/05-liu+zhang.pdf", "snippet": "\u2013 A general <b>sequence-to-sequence</b> <b>task</b> \u2013 Given a sentence x. 1, x. 2, \u2026, x. n, the goal is to generate a corresponding sequence of actions a. 1, a. 2,\u2026,a . m. \u2013 Possible for other transition-based systems. Motivation. v. Encoder-decoder neural networks \u2013 Encoder --- recurrent neural networks to represent sentences \u2013 decoder --- recurrent neural networks to output what we want in sequence. \u2013 Simple structures. encoder decoder. Motivation. v. Neural machine translation with ...", "dateLastCrawled": "2021-09-16T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sequence-to-Sequence</b> Models Can Directly Transcribe <b>Foreign</b> Speech ...", "url": "https://www.researchgate.net/publication/315667124_Sequence-to-Sequence_Models_Can_Directly_Transcribe_Foreign_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315667124_<b>Sequence-to-Sequence</b>_Models_Can...", "snippet": "In addition, we find that making use of the training data in both languages by multi-<b>task</b> training <b>sequence-to-sequence</b> speech translation and recognition models with a shared encoder network can ...", "dateLastCrawled": "2022-01-18T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "AI Quantification of <b>Language</b> Puzzle to <b>Language</b> Learning Generalization", "url": "https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=1927&context=etd_projects", "isFamilyFriendly": true, "displayUrl": "https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=1927&amp;context=etd_projects", "snippet": "Learning <b>a foreign</b> <b>language</b> is always a challenging <b>task</b>. The spread of the internet not only connected different parts of the world but also provided some powerful tools. Online <b>language</b> learning is one such tool people find appealing nowadays. There are many tools available on the internet which can help a user learn a new <b>language</b> at their own pace. According to Efficacy of New <b>Language</b> Application by R. Vesselinov and J. Grego, very little research has been done to study the ...", "dateLastCrawled": "2021-11-22T14:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Translating</b> synthetic natural <b>language</b> to database queries with a ...", "url": "https://www.nature.com/articles/s41598-021-98019-3", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-98019-3", "snippet": "The overall objective of a <b>sequence-to-sequence</b> model is to learn a latent representation (embedding) of an input <b>language</b>, in order to be capable of rebuilding the equivalent sentence in a ...", "dateLastCrawled": "2022-01-31T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Grammar as <b>a Foreign</b> <b>Language</b> | \u0141ukasz Kaiser - Academia.edu", "url": "https://www.academia.edu/24867358/Grammar_as_a_Foreign_Language", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/24867358/Grammar_as_<b>a_Foreign</b>_<b>Language</b>", "snippet": "As a <b>sequence-to-sequence</b> prediction model it is somewhat related to the incremental parsing models, pioneered by [26] and extended by [27]. Such linear time parsers however typically need some <b>task</b>-specific constraints and might build up the parse in multiple passes. Relatedly, [13] present excellent parsing results with a single left-to-right pass, but require a stack to explicitly delay making decisions and a parsing-specific transition strategy in order to achieve good parsing accuracies ...", "dateLastCrawled": "2022-01-13T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Language</b> <b>Translation</b> with RNNs. Build a recurrent neural network (RNN ...", "url": "https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>language</b>-<b>translation</b>-with-rnns-d84d43b40571", "snippet": "There are nearly 7,000 different languages worldwide. As our world becomes increasingly connected, <b>language</b> <b>translation</b> provides a critical cultural and economic bridge between people from different countries and ethnic groups. Some of the more obvious use-cases include: business: international trade, investment, contracts, finance; commerce: travel, purchase of <b>foreign</b> goods and services, customer support; media: accessing information via search, sharing information via social networks ...", "dateLastCrawled": "2022-02-02T17:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sequence-to-Sequence</b> Models <b>Can</b> Directly Transcribe <b>Foreign</b> Speech ...", "url": "https://www.researchgate.net/publication/315667124_Sequence-to-Sequence_Models_Can_Directly_Transcribe_Foreign_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315667124_<b>Sequence-to-Sequence</b>_Models_<b>Can</b>...", "snippet": "In addition, we find that making use of the training data in both languages by multi-<b>task</b> training <b>sequence-to-sequence</b> speech translation and recognition models with a shared encoder network <b>can</b> ...", "dateLastCrawled": "2022-01-18T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Multi-<b>task</b> <b>Sequence to Sequence Learning</b> | DeepAI", "url": "https://deepai.org/publication/multi-task-sequence-to-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/multi-<b>task</b>-<b>sequence-to-sequence-learning</b>", "snippet": "Despite the popularity of multi-<b>task</b> learning and <b>sequence to sequence learning</b>, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only one recent publication by Dong et al. which applies a seq2seq models for machine translation, where the goal is to translate from one <b>language</b> to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach \u2013 for tasks that <b>can</b> have an ...", "dateLastCrawled": "2022-02-02T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Direct speech-to-speech translation with a <b>sequence-to-sequence</b> model ...", "url": "https://deepai.org/publication/direct-speech-to-speech-translation-with-a-sequence-to-sequence-model", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/direct-speech-to-speech-translation-with-a-sequence-to...", "snippet": "We present an attention-based <b>sequence-to-sequence</b> neural network which <b>can</b> directly translate speech from one <b>language</b> into speech in another <b>language</b>, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another <b>language</b>, corresponding to the translated content (in a different canonical voice).", "dateLastCrawled": "2021-12-17T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>GitHub</b> - tensorflow/nmt: TensorFlow Neural Machine <b>Translation</b> Tutorial", "url": "https://github.com/tensorflow/nmt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/tensorflow/nmt", "snippet": "<b>Sequence-to-sequence</b> (seq2seq) models (Sutskever et al., 2014, Cho et al., 2014) have enjoyed great success in a variety of tasks such as machine <b>translation</b>, speech recognition, and text summarization. This tutorial gives readers a full understanding of seq2seq models and shows how to build a competitive seq2seq model from scratch. We focus on the <b>task</b> of Neural Machine <b>Translation</b> (NMT) which was the very first testbed for seq2seq models with wild", "dateLastCrawled": "2022-02-03T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) CFGs-2-NLU: <b>Sequence-to-Sequence</b> Learning for Mapping Utterances ...", "url": "https://www.researchgate.net/publication/305638225_CFGs-2-NLU_Sequence-to-Sequence_Learning_for_Mapping_Utterances_to_Semantics_and_Pragmatics", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/305638225_CFGs-2-NLU_<b>Sequence-to-Sequence</b>...", "snippet": "In this paper, we present a novel approach to natural <b>language</b> understanding that utilizes context-free grammars (CFGs) in conjunction with <b>sequence-to-sequence</b> (seq2seq) deep learning.", "dateLastCrawled": "2021-12-14T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Translatotron 2: Robust direct speech-to-speech translation \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2107.08661/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2107.08661", "snippet": "Translatotron (Jia et al., 2019b) is the first direct S2ST model, which is a <b>sequence-to-sequence</b> model trained in a multi-objective <b>task</b>. It has shown reasonable translation quality and speech naturalness, but still underperformed a baseline of ST + TTS cascade by a large margin. It also demonstrated the capacity of retaining speaker\u2019s voice during the translation, by leveraging a speaker encoder separately trained in a speaker verification <b>task</b> (Wan et al., 2018; Jia et al., 2018 ...", "dateLastCrawled": "2022-01-30T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "2016 MultiTaskSequencetoSequenceLear - GM-RKB", "url": "https://www.gabormelli.com/RKB/2016_MultiTaskSequencetoSequenceLear", "isFamilyFriendly": true, "displayUrl": "https://www.gabormelli.com/RKB/2016_Multi<b>TaskSequencetoSequence</b>Lear", "snippet": "This paper examines three multi-<b>task</b> learning (MTL) settings for <b>sequence to sequence</b> models: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder <b>can</b> be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and ...", "dateLastCrawled": "2021-07-06T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Hi, robot: Why robotics and <b>language</b> need each other", "url": "https://knowablemagazine.org/article/technology/2020/teaching-robots-to-talk", "isFamilyFriendly": true, "displayUrl": "https://knowablemagazine.org/article/technology/2020/teaching-robots-to-talk", "snippet": "Implementing a \u201c<b>sequence-to-sequence</b>\u201d architecture, this system takes in a sequence of words and outputs a sequence of action commands, rather like <b>translating</b> from one <b>language</b> to another. In between is a neural network, an arrangement of simple computing elements roughly mimicking the brain\u2019s wiring. The network has sub-networks specialized for handling <b>language</b> (the instructions) and images (what the virtual robot sees). When it succeeds during training, the active neural ...", "dateLastCrawled": "2022-02-03T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An Overview of Multi-<b>Task</b> Learning in <b>Speech Recognition</b>", "url": "http://jrmeyer.github.io/asr/2020/03/21/overview-mtl-in-asr.html", "isFamilyFriendly": true, "displayUrl": "jrmeyer.github.io/asr/2020/03/21/overview-mtl-in-asr.html", "snippet": "The intuition as to why this approach works is that non-native speakers will perceive sounds from <b>a foreign</b> <b>language</b> using their native phonemic system, and enough overlap should exist between the two languages to help train the acoustic model. In the relatively new field of spoken <b>language</b> translation, where speech from one <b>language</b> is mapped directly to a text translation in a second <b>language</b>, these researchers 63 64 created multiple auxiliary tasks by either recognizing the speech of the ...", "dateLastCrawled": "2022-01-29T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Technological disruption in <b>foreign</b> <b>language</b> teaching: The rise of ...", "url": "https://www.cambridge.org/core/journals/language-teaching/article/technological-disruption-in-foreign-language-teaching-the-rise-of-simultaneous-machine-translation/A31BA5AB690B370B01535EF2D1AFAE42", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/<b>language</b>-teaching/article/technological...", "snippet": "For this article, I will concentrate on the effects technology may have on <b>language</b> teachers instructing adults in <b>a foreign</b> <b>language</b> (FL) setting (i.e., teaching a <b>language</b> in a context where the <b>language</b> is not commonly spoken). Here technology may play an even more important role in radically changing the economic dynamics of FL learning and teaching. However, the concern is not technology replacing the FL teacher, but rather technology replacing the need and the demand for FL learning in ...", "dateLastCrawled": "2021-08-08T08:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Multi-<b>task</b> <b>Sequence to Sequence Learning</b> | DeepAI", "url": "https://deepai.org/publication/multi-task-sequence-to-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/multi-<b>task</b>-<b>sequence-to-sequence-learning</b>", "snippet": "Despite the popularity of multi-<b>task</b> learning and <b>sequence to sequence learning</b>, there has been little work in combining MTL with seq2seq learning. To the best of our knowledge, there is only one recent publication by Dong et al. which applies a seq2seq models for machine translation, where the goal is to translate from one <b>language</b> to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach \u2013 for tasks that <b>can</b> have an ...", "dateLastCrawled": "2022-02-02T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequence-to-Sequence</b> Models <b>Can</b> Directly Transcribe <b>Foreign</b> Speech ...", "url": "https://www.researchgate.net/publication/315667124_Sequence-to-Sequence_Models_Can_Directly_Transcribe_Foreign_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315667124_<b>Sequence-to-Sequence</b>_Models_<b>Can</b>...", "snippet": "In addition, we find that making use of the training data in both languages by multi-<b>task</b> training <b>sequence-to-sequence</b> speech translation and recognition models with a shared encoder network <b>can</b> ...", "dateLastCrawled": "2022-01-18T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequence-to-Sequence</b> Models <b>Can</b> Directly Translate <b>Foreign</b> Speech ...", "url": "https://www.arxiv-vanity.com/papers/1703.08581/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.08581", "snippet": "A single model trained end-to-end obtains state-of-the-art performance on the Fisher Callhome Spanish-English speech translation <b>task</b>, outperforming a cascade of independently trained <b>sequence-to-sequence</b> speech recognition and machine translation models by 1.8 BLEU points on the Fisher test set. In addition, we find that making use of the training data in both languages by multi-<b>task</b> training <b>sequence-to-sequence</b> speech translation and recognition models with a shared encoder network <b>can</b> ...", "dateLastCrawled": "2021-10-17T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sequence-to-Sequence Models Can Directly Translate Foreign Speech</b> ...", "url": "https://www.researchgate.net/publication/319185738_Sequence-to-Sequence_Models_Can_Directly_Translate_Foreign_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319185738_<b>Sequence-to-Sequence</b>_Models_<b>Can</b>...", "snippet": "<b>Sequence-to-sequence</b> based speech translation has shown very good potential over the traditional cascaded system (Berard et al., 2016;Goldwater et al., 2017; Weiss et al., 2017) with end-to-end ...", "dateLastCrawled": "2021-12-11T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Neural spelling correction: <b>translating</b> incorrect sentences to correct ...", "url": "https://link.springer.com/article/10.1007/s11042-020-09148-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11042-020-09148-2", "snippet": "A rule-based spelling corrector <b>can</b> correct incorrect parts without disturbing the structure of the original sentence, but in the case of the <b>sequence to sequence</b>-based [1, 6, 29] method it faces the crucial problem of repetition, omission, and UNK(Unknown) leading the model to perform either overcorrection or destroy the sentence structure.", "dateLastCrawled": "2021-11-26T14:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "2016 MultiTaskSequencetoSequenceLear - GM-RKB", "url": "https://www.gabormelli.com/RKB/2016_MultiTaskSequencetoSequenceLear", "isFamilyFriendly": true, "displayUrl": "https://www.gabormelli.com/RKB/2016_Multi<b>TaskSequencetoSequence</b>Lear", "snippet": "This paper examines three multi-<b>task</b> learning (MTL) settings for <b>sequence to sequence</b> models: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder <b>can</b> be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and ...", "dateLastCrawled": "2021-07-06T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Translating</b> synthetic natural <b>language</b> to database queries with a ...", "url": "https://www.nature.com/articles/s41598-021-98019-3", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-98019-3", "snippet": "The overall objective of a <b>sequence-to-sequence</b> model is to learn a latent representation (embedding) of an input <b>language</b>, in order to be capable of rebuilding the equivalent sentence in a ...", "dateLastCrawled": "2022-01-31T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Answering Questions over RDF by Neural Machine <b>Translating</b>", "url": "http://ceur-ws.org/Vol-2721/paper549.pdf", "isFamilyFriendly": true, "displayUrl": "ceur-ws.org/Vol-2721/paper549.pdf", "snippet": "<b>language</b> questions are fed to the network on a word-level. 3.3 Template-based <b>Translating</b> Considering SPARQL as <b>a foreign</b> <b>language</b> is a novel and direct method in the KBQA <b>task</b>, which turns a question into a SPARQL query with machine trans-lation. However, it would fail to accurately translate the entities and predicates", "dateLastCrawled": "2021-12-20T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Language</b> <b>Translation</b> with RNNs. Build a recurrent neural network (RNN ...", "url": "https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>language</b>-<b>translation</b>-with-rnns-d84d43b40571", "snippet": "There are nearly 7,000 different languages worldwide. As our world becomes increasingly connected, <b>language</b> <b>translation</b> provides a critical cultural and economic bridge between people from different countries and ethnic groups. Some of the more obvious use-cases include: business: international trade, investment, contracts, finance; commerce: travel, purchase of <b>foreign</b> goods and services, customer support; media: accessing information via search, sharing information via social networks ...", "dateLastCrawled": "2022-02-02T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "MuST-C: A multilingual corpus for end-to-end speech translation ...", "url": "https://www.sciencedirect.com/science/article/pii/S0885230820300887", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0885230820300887", "snippet": "Sizable ASR and MT datasets have been widely exploited also in the closely related <b>task</b> of spoken <b>language</b> translation (SLT), which consists of <b>translating</b> a speech input signal into a text in another <b>language</b>. Indeed, an intuitive solution to the problem is to set up a pipeline of independently trained systems, in which the input audio is passed to an ASR component, whose output transcription is then processed by an MT component Ney, 1999, Matusov, Kanthak, Ney, 2006, Waibel, F\u00fcgen, 2008 ...", "dateLastCrawled": "2022-01-14T20:51:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Translation vs. Dialogue: A Comparative Analysis of Sequence-to</b> ...", "url": "https://www.researchgate.net/publication/348346295_Translation_vs_Dialogue_A_Comparative_Analysis_of_Sequence-to-Sequence_Modeling", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348346295_Translation_vs_Dialogue_A...", "snippet": "word <b>analogy</b> <b>task</b> measured in ac- curacy (shown as percentage). ... <b>Sequence to sequence</b> <b>learning</b> with neural networks. In. NIPS. 4121. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan ...", "dateLastCrawled": "2022-01-22T03:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Virtualization of stateful services via machine learning</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11219-019-09468-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11219-019-09468-z", "snippet": "We employ classification-based and <b>sequence-to-sequence</b>-based <b>machine</b> <b>learning</b> algorithms in developing our solutions. Classification is a supervised <b>learning</b> method where the <b>task</b> is assigning given inputs to corresponding classes. A <b>sequence-to-sequence</b> model is a deep neural network architecture where the input and the output are sequences. We demonstrate the validity of our approaches on three datasets. Our evaluation shows that we obtain 75 % to 81 % accuracy on subject datasets with ...", "dateLastCrawled": "2022-01-11T05:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Introducing IceCAPS: Microsoft\u2019s Framework</b> for Advanced Conversation ...", "url": "https://www.kdnuggets.com/2019/09/introducing-icecaps-microsofts-framework-advanced-conversation-modeling.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2019/09/<b>introducing-icecaps-microsofts-framework</b>-advanced...", "snippet": "Conceptually, multi-<b>task</b> <b>learning</b> is a subfield of <b>machine</b> <b>learning</b> that focuses on models that exploit commonalities between different <b>task</b> in order to achieve a common goal. More specifically, multi-<b>task</b> <b>learning</b> shares subset of parameters among multiple tasks so those tasks can make use of shared feature representations. For example, this technique has been used in conversational modeling to combine general conversational data with unpaired utterances; by pairing a conversational model ...", "dateLastCrawled": "2022-01-18T16:48:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(translating a foreign language)", "+(sequence-to-sequence task) is similar to +(translating a foreign language)", "+(sequence-to-sequence task) can be thought of as +(translating a foreign language)", "+(sequence-to-sequence task) can be compared to +(translating a foreign language)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}