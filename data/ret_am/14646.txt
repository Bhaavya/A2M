{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>Complete Guide On Dimensionality Reduction</b> | by Chaitanyanarava ...", "url": "https://medium.com/analytics-vidhya/a-complete-guide-on-dimensionality-reduction-62d9698013d2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-<b>complete-guide-on-dimensionality-reduction</b>-62d...", "snippet": "In statistics, machine learning, and information theory, dimensionality <b>reduction</b> or <b>dimension</b> <b>reduction</b> is the process of <b>reduction</b> of n-dimensions to a k-dimensions where k&lt;&lt;n. Dimensionality ...", "dateLastCrawled": "2022-01-28T01:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Dimension</b> <b>Reduction</b> - Princeton University", "url": "https://www.cs.princeton.edu/courses/archive/spring12/cos424/pdf/dimension-reduction.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.princeton.edu/courses/archive/spring12/cos424/pdf/<b>dimension</b>-<b>reduction</b>.pdf", "snippet": "There are many reasons for performing <b>dimension</b> <b>reduction</b> \u2013 <b>Store</b> large data sets by storing a reduced representation \u2013 Explore the data, e.g., when q =2 we can plot high dimensional data. \u2013 Generalize data, by computing regressions and predictors about the reduced repre-sentation. If p is correlated (e.g., text) than <b>dimension</b> <b>reduction</b> is an alternative to sparse prediction. \u2013 Generalize data by \ufb01lling in missing values of the row x. For example, in collaborative \ufb01ltering x ...", "dateLastCrawled": "2021-08-27T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding Dimensionality <b>Reduction</b> for Machine Learning | by Rajat ...", "url": "https://towardsdatascience.com/understanding-dimensionality-reduction-for-machine-learning-ad9a3811bd89", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>dimension</b>ality-<b>reduction</b>-for-machine...", "snippet": "One of the two dimensions of the plot is <b>going</b> to be the output. So the question we should ask ourselves is: How do we represent all of our features, which could be anywhere from hundreds to thousands, in a single <b>dimension</b>? This is where dimensionality <b>reduction</b> comes into play! What is Dimensionality <b>Reduction</b>? Dimensionality <b>Reduction</b> is a technique in Machine Learning that reduces the number of features in your dataset. The great thing about dimensionality <b>reduction</b> is that it does not ...", "dateLastCrawled": "2022-02-03T10:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Dimension</b> <b>Reduction</b> for Clustering in R (PCA and other methods) - Stack ...", "url": "https://stackoverflow.com/questions/43235812/dimension-reduction-for-clustering-in-r-pca-and-other-methods", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43235812", "snippet": "It seems <b>like</b> this question is more about how to do a proper <b>dimension</b> <b>reduction</b> analysis which is really more of a statistical question which should go on Cross Validated or Data Science. If the problem is really coding this in R, then the question should include a reproducible example with sample input data (it should not be your entire data set).", "dateLastCrawled": "2022-01-26T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Dimensionality reduction for image and texture set compression</b> | Bart ...", "url": "https://bartwronski.com/2020/05/21/dimensionality-reduction-for-image-and-texture-set-compression/", "isFamilyFriendly": true, "displayUrl": "https://bartwronski.com/2020/05/21/<b>dimensionality-reduction-for-image-and</b>-texture-set...", "snippet": "The SVD is <b>going</b> to look <b>like</b>: ... Right: Same data with \u201ccentered\u201d SVD shows perfect decorrelation and a significant range <b>reduction</b> of the second <b>dimension</b>, while more information is represented in the first <b>dimension</b>. As presented, shifting the fit center to the mean of the data, shifts the line fit and achieves perfect \u201cdecorrelating\u201d properties. PCA that runs on covariance matrices does this centering implicitly, while in the case of SVD we need to do it manually. In general, we ...", "dateLastCrawled": "2022-01-29T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "EETimes - Feature <b>dimension</b> <b>reduction</b> slowdown", "url": "https://www.eetimes.com/feature-dimension-reduction-slowdown/", "isFamilyFriendly": true, "displayUrl": "https://www.ee<b>times</b>.com/feature-<b>dimension</b>-<b>reduction</b>-slowdown", "snippet": "Feature <b>dimension</b> <b>reduction</b> slowdown. By EETimes 03.20.2012 0. Share Post. Share on Facebook. Share on Twitter. The semiconductor industry is facing the challenge that the two-year feature <b>dimension</b> cycle is over, and we are <b>going</b> into a highly unclear phase. 1. The 32/28-nm wafer volume ramp-up from the foundry vendors is already on a three-year cycle. 45/40-nm was at 10 percent of revenues in Q4/2009, and 32/28-nm will be at 10 percent in Q4/2012. 2. The 22-nm FinFET high-volume ramp-up is ...", "dateLastCrawled": "2022-01-07T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "machine learning - PCA <b>on Neural Networks dimensions reduction</b>? - Data ...", "url": "https://datascience.stackexchange.com/questions/17727/pca-on-neural-networks-dimensions-reduction", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/.../pca-<b>on-neural-networks-dimensions-reduction</b>", "snippet": "So, PCA will surely perform <b>dimension</b> <b>reduction</b> given that data has some values which don&#39;t add info to your model. You can call this Feature Extraction or Feature Filtering. The reason I don&#39;t use PCA <b>like</b> algorithms is because I want to implement Neural Network classification, and I need the real parameters. What makes you think, post PCA you ...", "dateLastCrawled": "2022-01-24T14:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "PCA <b>for dimension reduction on repeated measures</b> - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/525300/pca-for-dimension-reduction-on-repeated-measures", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/525300/pca-for-<b>dimension</b>-<b>reduction</b>-on...", "snippet": "PCA <b>for dimension reduction on repeated measures</b>. Bookmark this question. Show activity on this post. I have a dataset with 1000 individuals who each came in for a minimum of 1 to a maximum of 10 visits during which we measured their arm strength in various directions (forward, back, sideways etc.). I would <b>like</b> to do PCA to combine these ...", "dateLastCrawled": "2022-01-19T11:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Master Dimensionality <b>Reduction</b> with these 5 Must-Know Applications of ...", "url": "https://medium.com/analytics-vidhya/master-dimensionality-reduction-with-these-5-must-know-applications-of-singular-value-777299940b89", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/analytics-vidhya/master-<b>dimension</b>ality-<b>reduction</b>-with-these-5-must...", "snippet": "Overview. Singular Value Decomposition (SVD) is a common dimensionality <b>reduction</b> technique in data science; We will discuss 5 must-know applications of SVD here and understand their role in data ...", "dateLastCrawled": "2022-01-05T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - how can autoencoder reduce dimensionality? - Data ...", "url": "https://datascience.stackexchange.com/questions/33084/how-can-autoencoder-reduce-dimensionality", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/33084", "snippet": "Autoencoders are trained using both encoder and decoder section, but after training then only the encoder is used, and the decoder is trashed. So, if you want to obtain the <b>dimensionality reduction</b> you have to set the layer between encoder and decoder of a <b>dimension</b> lower than the input&#39;s one. Then trash the decoder, and use that middle layer ...", "dateLastCrawled": "2022-01-30T05:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>Complete Guide On Dimensionality Reduction</b> | by Chaitanyanarava ...", "url": "https://medium.com/analytics-vidhya/a-complete-guide-on-dimensionality-reduction-62d9698013d2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-<b>complete-guide-on-dimensionality-reduction</b>-62d...", "snippet": "In statistics, machine learning, and information theory, dimensionality <b>reduction</b> or <b>dimension</b> <b>reduction</b> is the process of <b>reduction</b> of n-dimensions to a k-dimensions where k&lt;&lt;n. Dimensionality ...", "dateLastCrawled": "2022-01-28T01:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding Dimensionality <b>Reduction</b> for Machine Learning | by Rajat ...", "url": "https://towardsdatascience.com/understanding-dimensionality-reduction-for-machine-learning-ad9a3811bd89", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>dimension</b>ality-<b>reduction</b>-for-machine...", "snippet": "One of the two dimensions of the plot is <b>going</b> to be the output. So the question we should ask ourselves is: How do we represent all of our features, which could be anywhere from hundreds to thousands, in a single <b>dimension</b>? This is where dimensionality <b>reduction</b> comes into play! What is Dimensionality <b>Reduction</b>? Dimensionality <b>Reduction</b> is a technique in Machine Learning that reduces the number of features in your dataset. The great thing about dimensionality <b>reduction</b> is that it does not ...", "dateLastCrawled": "2022-02-03T10:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "python - <b>Convolution layer dimension reduction</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/44130759/convolution-layer-dimension-reduction", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/44130759", "snippet": "<b>Convolution layer dimension reduction</b>. Ask Question Asked 4 years, 8 months ago. Active 4 years, 8 months ago. Viewed 79 <b>times</b> 0 I am working with the Cifar-10 dataset. The original shape of the data was (50000, 3072) where there are 50000 images and for each, the first 1024 pixels are red, next 1024 are green, next 1024 are blue. They are stored in row major format, so the first 32 pixels of red are the reds for the first row of 32 pixels. So far I have managed to transform it into a numpy ...", "dateLastCrawled": "2022-01-23T08:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "python - <b>Dimension</b> <b>Reduction</b> in <b>Categorical</b> Data with missing values ...", "url": "https://stackoverflow.com/questions/2837850/dimension-reduction-in-categorical-data-with-missing-values", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/2837850", "snippet": "There is no a priori theory to choose the specification of the model so one of the key tasks is <b>dimension</b> <b>reduction</b> before running the regression. While I am aware of several methods for <b>dimension</b> <b>reduction</b> for continuous variables I am not aware of a <b>similar</b> statical literature for <b>categorical</b> data (except, perhaps, as a part of correspondence analysis which is basically a variation of principal component analysis on frequency table). Let me also add that the dataset is of moderate size ...", "dateLastCrawled": "2022-01-22T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Master Dimensionality <b>Reduction</b> with these 5 Must-Know Applications of ...", "url": "https://medium.com/analytics-vidhya/master-dimensionality-reduction-with-these-5-must-know-applications-of-singular-value-777299940b89", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/analytics-vidhya/master-<b>dimension</b>ality-<b>reduction</b>-with-these-5-must...", "snippet": "Overview. Singular Value Decomposition (SVD) is a common dimensionality <b>reduction</b> technique in data science; We will discuss 5 must-know applications of SVD here and understand their role in data ...", "dateLastCrawled": "2022-01-05T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Dimensionality reduction for image and texture set compression</b> | Bart ...", "url": "https://bartwronski.com/2020/05/21/dimensionality-reduction-for-image-and-texture-set-compression/", "isFamilyFriendly": true, "displayUrl": "https://bartwronski.com/2020/05/21/<b>dimensionality-reduction-for-image-and</b>-texture-set...", "snippet": "<b>Dimensionality reduction for image and texture set compression</b>. Teaser image: Example PBR Texture set compressed with presented technique.Textures credit cc0textures, Lennart Demes. In this blog post I am <b>going</b> to describe some of my past investigations on reducing the number of channels in textures / texture sets automatically and generally ...", "dateLastCrawled": "2022-01-29T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Word Embedding Tutorial | Word2vec</b> Model Gensim Example", "url": "https://www.guru99.com/word-embedding-word2vec.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/word-embedding-word2vec.html", "snippet": "Word Embedding is a word representation type that allows machine learning algorithms to understand words with <b>similar</b> meanings. It is a language modeling and feature learning technique to map words into vectors of real numbers using neural networks, probabilistic models, or <b>dimension</b> <b>reduction</b> on the word co-occurrence matrix. Some word embedding models are Word2vec (Google), Glove (Stanford), and fastest (Facebook). Word Embedding is also called as distributed semantic model or distributed ...", "dateLastCrawled": "2022-02-02T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "r - How to <b>use SVD for dimensionality reduction</b> - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/64370/how-to-use-svd-for-dimensionality-reduction", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/64370", "snippet": "As @Hongooi points out, svd is the main algorithm behind PCA and thus the other post is asking a very <b>similar</b> question. By &quot;<b>dimension</b> <b>reduction</b>&quot;, what is really meant is that a smaller number of linear predictors can be used to explain a large portion of the data. $\\endgroup$ \u2013 Marc in the box. Jul 15 &#39;13 at 14:38 | Show 1 more comment. 1 Answer Active Oldest Votes. 5 $\\begingroup$ I think your confusion comes from the fact that the PCA truncation is <b>going</b> to reconstruct the full ...", "dateLastCrawled": "2022-01-13T04:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "dimensionality <b>reduction</b> - Interpreting plot of PCA results (from 3 to ...", "url": "https://stats.stackexchange.com/questions/66518/interpreting-plot-of-pca-results-from-3-to-2-dimensions", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/66518/interpreting-plot-of-pca-results-from...", "snippet": "Moreover, the transformations in the XZ and YZ planes look very <b>similar</b>. In fact, the starting positions in the XZ and YZ planes look very <b>similar</b>. This isn&#39;t surprising: in this example, X and Y are so tightly correlated, their practically interchangeable. PCA is a technique that let&#39;s us say (in this example), &quot;Hey, these variables are so close, we don&#39;t really need both. Let&#39;s pretend our data is two dimensional instead of three dimensional, because it may as well be.&quot;", "dateLastCrawled": "2022-01-24T18:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - <b>Reduce dimension, then apply SVM</b> - <b>Data Science Stack Exchange</b>", "url": "https://datascience.stackexchange.com/questions/8521/reduce-dimension-then-apply-svm", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/8521", "snippet": "To do this, you rank your features with respect to the objective. \u03c7 2 ( D, t, c) = \u2211 e t \u2208 0, 1 \u2211 e c \u2208 0, 1 ( N e t e c \u2212 E e t e c) 2 E e t e c, where N is the observed frequency of a term in D, E is its expected frequency, and t and c denote term and class, respectively. You can easily compute this in sklearn, unless you want the ...", "dateLastCrawled": "2022-01-25T22:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Dimensionality reduction for image and texture set compression</b> | Bart ...", "url": "https://bartwronski.com/2020/05/21/dimensionality-reduction-for-image-and-texture-set-compression/", "isFamilyFriendly": true, "displayUrl": "https://bartwronski.com/2020/05/21/<b>dimensionality-reduction-for-image-and</b>-texture-set...", "snippet": "<b>Dimensionality reduction for image and texture set compression</b>. Teaser image: Example PBR Texture set compressed with presented technique.Textures credit cc0textures, Lennart Demes. In this blog post I am <b>going</b> to describe some of my past investigations on reducing the number of channels in textures / texture sets automatically and generally ...", "dateLastCrawled": "2022-01-29T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Master Dimensionality <b>Reduction</b> with these 5 Must-Know Applications of ...", "url": "https://medium.com/analytics-vidhya/master-dimensionality-reduction-with-these-5-must-know-applications-of-singular-value-777299940b89", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/analytics-vidhya/master-<b>dimension</b>ality-<b>reduction</b>-with-these-5-must...", "snippet": "Overview. Singular Value Decomposition (SVD) is a common dimensionality <b>reduction</b> technique in data science; We will discuss 5 must-know applications of SVD here and understand their role in data ...", "dateLastCrawled": "2022-01-05T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "c++ - Parallel <b>dimension</b> <b>reduction</b> (3D to 2D with sum) using Cuda ...", "url": "https://stackoverflow.com/questions/47993833/parallel-dimension-reduction-3d-to-2d-with-sum-using-cuda", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/47993833", "snippet": "For the moment, I&#39;ll ignore the D <b>dimension</b> as the operation <b>can</b> <b>be thought</b> of as reducing a matrix containing NxN entries where every entry contains <b>multiple</b> floats. If your matrix is stored in row-major order and you want to reduce each row to its sum (or column-major and column <b>reduction</b>), the answer is simple:", "dateLastCrawled": "2022-01-15T00:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "python - <b>Reduce dimension, then apply SVM</b> - <b>Data Science Stack Exchange</b>", "url": "https://datascience.stackexchange.com/questions/8521/reduce-dimension-then-apply-svm", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/8521", "snippet": "To do this, you rank your features with respect to the objective. \u03c7 2 ( D, t, c) = \u2211 e t \u2208 0, 1 \u2211 e c \u2208 0, 1 ( N e t e c \u2212 E e t e c) 2 E e t e c, where N is the observed frequency of a term in D, E is its expected frequency, and t and c denote term and class, respectively. You <b>can</b> easily compute this in sklearn, unless you want the ...", "dateLastCrawled": "2022-01-25T22:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Consumer Behaviour through the Eyes of Neurophysiological Measures ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6766676/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6766676", "snippet": "The total number of neuromarketing papers published so far is 16500 (source: Google Scholar in March 2019). In 2008, Hubert and Kenning [] reported more than 800,000 Google hits for the term \u201cneuromarketing,\u201d and in 2012, the same search yielded over 1.4 million hits, underlining the rising interest in this topic.In 2018, there have been more than 3 million hits for the term \u201cneuromarketing.\u201d Nowadays, companies around the world that offer neuromarketing research services are growing.", "dateLastCrawled": "2022-01-28T04:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>detailed discussion on tensors, why</b> it is so important in deep ...", "url": "https://dibyendudeb.com/tensors-in-deep-learning-an-introduction/", "isFamilyFriendly": true, "displayUrl": "https://<b>dibyendudeb</b>.com/tensors-in-deep-learning-an-introduction", "snippet": "Though tensors up to 4 <b>dimension</b> are more common, some <b>times</b> to sore videos 5-D tensors are also used. Theoretically there is no such limitation in <b>dimension</b>. For the sake of data storage in an organized manner any n number of dimensions <b>can</b> be used. 5-D tensors. This is the type of tensors when we need to <b>store</b> data with yet another <b>dimension</b>. Video data <b>can</b> be an ideal example where 5-D tensors are used. If we take an example of a 5-minute video of 1080 HD resolution, then what will be the ...", "dateLastCrawled": "2022-01-28T23:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "linear algebra - Proof for Column Space of matrix and its reduced row ...", "url": "https://math.stackexchange.com/questions/3256078/proof-for-column-space-of-matrix-and-its-reduced-row-echelon-having-the-same-dim", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/3256078/proof-for-column-space-of-matrix-and...", "snippet": "...either you eliminate rows to produce the same image (row <b>reduction</b>) or you eliminate columns which are obtained as combinations of others. (Again I&#39;m planting a simple idea first which needs some fleshing out but takes again a simple memorable step. One stops when the simple step seems enough to be clear to you or your audience.) Either way you are describing a final state where the number of independent rows/columns is the same.", "dateLastCrawled": "2022-01-27T23:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "This 2000-Year-Old Pigment <b>Can</b> Eliminate The Third <b>Dimension</b>", "url": "https://gizmodo.com/this-2000-year-old-pigment-can-eliminate-the-third-dime-1661476168", "isFamilyFriendly": true, "displayUrl": "https://<b>gizmodo.com</b>/this-2000-year-old-pigment-<b>can</b>-eliminate-the-third-dime-1661476168", "snippet": "This 2000-Year-Old Pigment <b>Can</b> Eliminate The Third <b>Dimension</b>. By. Esther Inglis-Arkell. 11/21/14 11:00AM. Comments ( 108) Han purple is an ancient pigment that wasn&#39;t reconstructed by modern ...", "dateLastCrawled": "2022-02-03T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Adapting to the next normal in retail | McKinsey", "url": "https://www.mckinsey.com/industries/retail/our-insights/adapting-to-the-next-normal-in-retail-the-customer-experience-imperative", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mckinsey.com</b>/industries/retail/our-insights/adapting-to-the-next-normal-in...", "snippet": "Many of the opportunities <b>can</b> be captured quickly by changing <b>store</b> hours, improving scheduling, and altering the mix of full- and part-time employees. Some retailers have started to move certain elements of the experience outside of the <b>store</b> (for example, returns in the parking lot), given limits on customers per square foot in stores. A structured effort focused on addressing new operational needs and implementing leaner practices <b>can</b> yield labor efficiencies of 5 to 15 percent.", "dateLastCrawled": "2022-02-02T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Multiversal Heroine Harem v.0.5</b> [CYOA] [OC] [WIP] : nsfwcyoa", "url": "https://www.reddit.com/r/nsfwcyoa/comments/ig8rr1/multiversal_heroine_harem_v05_cyoa_oc_wip/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/nsfwcyoa/comments/ig8rr1/<b>multiversal_heroine_harem_v05_cyoa</b>...", "snippet": "Team Player <b>can</b> no longer be bought <b>multiple</b> <b>times</b> normally. However, the new Refinement &#39;Extended Connection&#39; gives additional uses of not only Team Player, but There is Another and Rogue&#39;s Gallery as well. This allows all three of them to be used <b>multiple</b> <b>times</b>, but limits the length of daisy chains, rewarding those who know their superhero lore and keeping the default roster relevant. Reshape Body now allows you to alter the muscle tone and fat on a Heroine, in addition to its other ...", "dateLastCrawled": "2022-02-02T21:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>Complete Guide On Dimensionality Reduction</b> | by Chaitanyanarava ...", "url": "https://medium.com/analytics-vidhya/a-complete-guide-on-dimensionality-reduction-62d9698013d2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/a-<b>complete-guide-on-dimensionality-reduction</b>-62d...", "snippet": "A feature with high degree of fit is more likely to be choosed <b>compared</b> with the feature with low degree of fit. <b>Reduction</b> of features <b>can</b> also help us to tackle this problem. Suppose using PCA ...", "dateLastCrawled": "2022-01-28T01:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Dimensionality reduction for image and texture set compression</b> | Bart ...", "url": "https://bartwronski.com/2020/05/21/dimensionality-reduction-for-image-and-texture-set-compression/", "isFamilyFriendly": true, "displayUrl": "https://bartwronski.com/2020/05/21/<b>dimensionality-reduction-for-image-and</b>-texture-set...", "snippet": "<b>Dimensionality reduction for image and texture set compression</b>. Teaser image: Example PBR Texture set compressed with presented technique.Textures credit cc0textures, Lennart Demes. In this blog post I am <b>going</b> to describe some of my past investigations on reducing the number of channels in textures / texture sets automatically and generally ...", "dateLastCrawled": "2022-01-29T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "EETimes - Feature <b>dimension</b> <b>reduction</b> slowdown", "url": "https://www.eetimes.com/feature-dimension-reduction-slowdown/", "isFamilyFriendly": true, "displayUrl": "https://www.ee<b>times</b>.com/feature-<b>dimension</b>-<b>reduction</b>-slowdown", "snippet": "Feature <b>dimension</b> <b>reduction</b> slowdown. By EETimes 03.20.2012 0. Share Post . Share on Facebook. Share on Twitter. The semiconductor industry is facing the challenge that the two-year feature <b>dimension</b> cycle is over, and we are <b>going</b> into a highly unclear phase. 1. The 32/28-nm wafer volume ramp-up from the foundry vendors is already on a three-year cycle. 45/40-nm was at 10 percent of revenues in Q4/2009, and 32/28-nm will be at 10 percent in Q4/2012. 2. The 22-nm FinFET high-volume ramp-up ...", "dateLastCrawled": "2022-01-07T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Chapter 34 Large datasets | Introduction to Data Science", "url": "https://rafalab.github.io/dsbook/large-datasets.html", "isFamilyFriendly": true, "displayUrl": "https://rafalab.github.io/dsbook/large-datasets.html", "snippet": "<b>Dimension</b> <b>reduction</b> <b>can</b> often be described as applying a transformation \\(A\\) to a matrix \\(X\\) with many columns that moves the information contained in \\ (X\\) to the first few columns of \\(Z=AX\\), then keeping just these few informative columns, thus reducing the <b>dimension</b> of the vectors contained in the rows. 34.5.3 Orthogonal transformations (advanced) Note that we divided the above by \\(\\sqrt{2}\\) to account for the differences in dimensions when comparing a 2 <b>dimension</b> distance to a 1 ...", "dateLastCrawled": "2022-02-01T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "pca - <b>Reduce data set dimension to one variable</b> - <b>Cross Validated</b>", "url": "https://stats.stackexchange.com/questions/281641/reduce-data-set-dimension-to-one-variable", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/281641/<b>reduce-data-set-dimension-to-one-variable</b>", "snippet": "Viewed 1k <b>times</b> 3 $\\begingroup$ Closed. This question needs to be ... Then <b>dimension</b> <b>reduction</b> may not be what you are at. But a PCA <b>can</b> tell you how much that item loads\u2014or contributes to\u2014that component. It depends on your theoretical assumptions for how you think the data were generated and if you have a factor structure already in mind. But it seems to me from the limited information given that you are after a PCA. I describe more on the difference between PCA and PAF (a type of EFA ...", "dateLastCrawled": "2022-01-10T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How do I know whether to use linear or nonlinear <b>dimension</b> <b>reduction</b> ...", "url": "https://stats.stackexchange.com/questions/301767/how-do-i-know-whether-to-use-linear-or-nonlinear-dimension-reduction-method-give", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/301767/how-do-i-know-whether-to-use-linear...", "snippet": "Viewed 627 <b>times</b> 4 1 $\\begingroup$ I want to use <b>dimension</b> <b>reduction</b> method for a high dimensional data set Is there any possible way to assess the &quot;non-linearity&quot; of the data first to give me the insight of whether I should use linear method (e.g. PCA) or nonlinear method (e.g. NLPCA)? I read several literatures and neither of them could provide a justification of when should I use linear of nonlinear method. They always use examples of &quot;swiss roll&quot; to indicate a data set which I should use ...", "dateLastCrawled": "2022-01-15T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Word <b>embedding</b>. What are <b>word embeddings</b>? Why we use\u2026 | by Manjeet ...", "url": "https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/word-<b>embedding</b>-2d05d270b285", "snippet": "We <b>can</b> visualize the learned vectors by projecting them down to 2 dimensions using for instance something like the t-SNE dimensionality <b>reduction</b> technique. When we inspect these visualizations it ...", "dateLastCrawled": "2022-01-29T19:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How <b>can</b> I calculate the entropy <b>reduction</b> in my system I&#39;m <b>going</b> to ...", "url": "https://security.stackexchange.com/questions/231723/how-can-i-calculate-the-entropy-reduction-in-my-system-im-going-to-introduce", "isFamilyFriendly": true, "displayUrl": "https://security.stackexchange.com/questions/231723/how-<b>can</b>-i-calculate-the-entropy...", "snippet": "I am the creator of LessPass, a deterministic password generator. The core of LessPass is not very complicated. I have 2 methods calc_entropy and render_password:. calc_entropy transforms the master password + site + login into a very large integer that I call entropy (I don&#39;t know if it&#39;s the right term, but it represents my amount of randomness in my system). It uses pbkdf2 100k iterations.", "dateLastCrawled": "2022-01-24T23:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "machine learning - <b>Multiple</b>-input <b>multiple</b>-output CNN with custom loss ...", "url": "https://datascience.stackexchange.com/questions/42798/multiple-input-multiple-output-cnn-with-custom-loss-function", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/42798", "snippet": "In my opinion, PyTorch has the easiest framework for this task. Also, you <b>can</b> find the implementation of an encoder-decoder network here. This is a frequently used technique when you need to compute, for example, the distance between a few inputs. But, I have doubts about the concatenation part. Which <b>dimension</b> do you need for outputs? If it is ...", "dateLastCrawled": "2022-01-24T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Options to Increase <b>Warehouse</b> <b>Storage Capacity</b>", "url": "https://mwpvl.com/html/options_to_increase_warehouse_storage_capacity.html", "isFamilyFriendly": true, "displayUrl": "https://mwpvl.com/html/options_to_increase_<b>warehouse</b>_<b>storage_capacity</b>.html", "snippet": "Each storage location <b>can</b> be used to <b>store</b> a different SKU; <b>Multiple</b> types of forklifts <b>can</b> be used to <b>store</b> and retrieve pallets; Aisle widths <b>can</b> be very narrow, narrow or wide aisle depending on the type of mobile equipment being used. Suitable for full case picking and less than case picking environments; Single deep post and beam racks provide the lowest cost per pallet position of any racking system option available. The storage and retrieval of pallets is fast and efficient; Pallets ...", "dateLastCrawled": "2022-01-29T08:09:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Dimension</b> <b>reduction</b> ... and it has been used for conducting research and for deploying <b>machine</b> <b>learning</b> systems into production across more than a dozen areas of computer science and other fields ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. ... K-means algorithm with weighting and <b>dimension</b> <b>reduction</b> components of similarity measure. Simplify balls of string to warm colors and cool colors before untangling. Can be reformulated as a graph clustering problem. Partition subcomponents of a graph based on flow equations. www.simplepastimes.com 40. Multivariate technique similar to mode or density clustering. Find peaks and valleys in data according to an input function on the ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Colleen M. Farrelly</b> - cours.polymtl.ca", "url": "https://cours.polymtl.ca/mth6301/mth8302/Farrelly-Machine_Learning_by_Analogy.pdf", "isFamilyFriendly": true, "displayUrl": "https://cours.polymtl.ca/mth6301/mth8302/Farrelly-<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>.pdf", "snippet": "<b>Dimension</b> <b>reduction</b>/mappingpre-processing Principle component, manifold <b>learning</b>\u2026 Hybrid of neural network methods and tree models. 32 Aggregation of multiple. types of models. Like a small town election. Different people have different views of the politics and care about different issues. Different modeling methods capture different pieces of the data and vote in different pieces. Leverage strengths, minimize weaknesses Diversity of methods to better explore underlying data geometry ...", "dateLastCrawled": "2021-12-14T17:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Modern <b>Dimension</b> <b>Reduction</b>. (arXiv:2103.06885v1 [cs.LG ...", "url": "https://www.machinelearningfreaks.com/modern-dimension-reduction-arxiv2103-06885v1-cs-lg/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinelearning</b>freaks.com/modern-<b>dimension</b>-<b>reduction</b>-arxiv2103-06885v1-cs-lg", "snippet": "Data are not only ubiquitous in society, but are increasingly complex both in size and dimensionality. <b>Dimension</b> <b>reduction</b> offers researchers and scholars the ability to make such complex, high dimensional data spaces simpler and more manageable. This Element offers readers a suite of modern unsupervised <b>dimension</b> <b>reduction</b> techniques along with hundreds of lines of R [\u2026]", "dateLastCrawled": "2021-07-03T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "4. Dimensionality <b>Reduction</b> Techniques and PCA \u2013 The Unsupervised ...", "url": "https://dev2u.net/2021/10/01/4-dimensionality-reduction-techniques-and-pca-the-unsupervised-learning-workshop/", "isFamilyFriendly": true, "displayUrl": "https://dev2u.net/2021/10/01/4-<b>dimension</b>ality-<b>reduction</b>-techniques-and-pca-the...", "snippet": "Dimensionality <b>reduction</b> techniques have many uses in <b>machine</b> <b>learning</b>, as the ability to extract the useful information of a dataset can provide performance boosts in many <b>machine</b> <b>learning</b> problems. They can be particularly useful in unsupervised as opposed to supervised <b>learning</b> methods because the dataset does not contain any ground truth labels or targets to achieve. In unsupervised <b>learning</b>, the training environment is being used to organize the data in a way that is appropriate for the ...", "dateLastCrawled": "2022-01-26T07:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>MACHINE</b> <b>LEARNING</b> (15A05706) - VEMU", "url": "http://vemu.org/uploads/lecture_notes/20_12_2019_484640891.pdf", "isFamilyFriendly": true, "displayUrl": "vemu.org/uploads/lecture_notes/20_12_2019_484640891.pdf", "snippet": "3 Unit-III : (Dimensionality <b>Reduction</b>) 3.1 Introduction 76 3.2 Unit-III notes 72-92 3.3 Solved Problems 3.4 Part A Questions 93 3.5 Part B Questions 94 4 Unit-IV : (Linear Discrimination) 4.1 Introduction 95 4.2 Unit-IV notes 95-110 4.3 Solved Problems 4.4 Part A Questions 111 4.5 Part B Questions 112 5 Unit-V : (Kernel Machines) 5.1 Introduction 113 5.2 Unit-V notes 113-146 5.3 Solved Problems 5.4 Part A Questions 147 5.5 Part B Questions 148 . 2 UNIT-1 1. What Is <b>Machine</b> <b>Learning</b>? <b>Machine</b> ...", "dateLastCrawled": "2022-01-28T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Dimensionality <b>Reduction</b> Using <b>Factor Analysis</b> | by Chiranjit Majumdar ...", "url": "https://medium.com/@chiranjit7/dimensionality-reduction-using-factor-analysis-8aa754465afc", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@chiranjit7/<b>dimension</b>ality-<b>reduction</b>-using-<b>factor-analysis</b>-8aa754465afc", "snippet": "Dimensionality <b>reduction</b> technique plays a very crucial role to handle this situation. This is a key test to perform while doing feature engineering. <b>Factor analysis</b> will help you to understand ...", "dateLastCrawled": "2022-01-30T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Understanding Machine Learning by Analogy</b> with a Simple Contour Map ...", "url": "https://contemplations.blog/machine-learning-analogy-countour-map/", "isFamilyFriendly": true, "displayUrl": "https://<b>contemplations</b>.blog/<b>machine</b>-<b>learning</b>-<b>analogy</b>-countour-map", "snippet": "The Basis for <b>Machine</b> <b>Learning</b> by <b>Analogy</b>, Using a Contour Map. In this post, we will take a closer look at <b>Machine</b> <b>Learning</b> and its nephew, Deep <b>Learning</b>. There is no \u201c<b>Learning</b>\u201d (in the human sense) in either <b>Machine</b> <b>learning</b> or Deep <b>Learning</b>, there are only quite simple and readily available mathematical procedures which allow us to adapt parameters of many kinds of parameterized systems (or networks), such as a neural network, in such a way that the system (or network), together with ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Data Mining and <b>Machine</b> <b>Learning</b> in Astronomy - Nicholas M. Ball ...", "url": "https://ned.ipac.caltech.edu/level5/March11/Ball/Ball2.html", "isFamilyFriendly": true, "displayUrl": "https://ned.ipac.caltech.edu/level5/March11/Ball/Ball2.html", "snippet": "In many ways, <b>dimension reduction is similar</b> to classification, in the sense that a larger number of input attributes is reduced to a smaller number of outputs. Many classification schemes in fact directly use PCA. Other dimension reduction methods utilize the same or similar algorithms to those used for the actual data mining: an ANN can perform PCA when set up as an autoencoder, and kernel methods can act as generalizations of PCA. A binary genetic algorithm Section 2.4.4) can be used in ...", "dateLastCrawled": "2022-01-30T22:43:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(dimension reduction)  is like +(going to the store multiple times)", "+(dimension reduction) is similar to +(going to the store multiple times)", "+(dimension reduction) can be thought of as +(going to the store multiple times)", "+(dimension reduction) can be compared to +(going to the store multiple times)", "machine learning +(dimension reduction AND analogy)", "machine learning +(\"dimension reduction is like\")", "machine learning +(\"dimension reduction is similar\")", "machine learning +(\"just as dimension reduction\")", "machine learning +(\"dimension reduction can be thought of as\")", "machine learning +(\"dimension reduction can be compared to\")"]}