{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>How does deep reinforcement learning work</b>?", "url": "https://treehozz.com/how-does-deep-reinforcement-learning-work", "isFamilyFriendly": true, "displayUrl": "https://treehozz.com/<b>how-does-deep-reinforcement-learning-work</b>", "snippet": "Q-<b>Learning</b> is a value-based reinforcement <b>learning</b> <b>algorithm</b> which is used to find the optimal action-selection policy using a <b>Q function</b>. Our goal is to maximize the value function Q. This function can be estimated using Q-<b>Learning</b>, which iteratively updates Q(s,a) using the Bellman equation.", "dateLastCrawled": "2022-01-25T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>machine</b> <b>learning</b> - How to set up a <b>Q function</b> approximator using neural ...", "url": "https://stats.stackexchange.com/questions/389241/how-to-set-up-a-q-function-approximator-using-neural-net-for-ddpg", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/389241/how-to-set-up-a-<b>q-function</b>-approximat...", "snippet": "Actor-critic <b>algorithm</b>. Deep <b>learning</b> can solve the problem of high observation dimensional data it only handles discrete and low-dimensional action-spaces. This <b>algorithm</b> relies on finding the action that maximizes the action-value function, which in continuous spaces requires solving a complex optimization problem. Obvious solutions <b>like</b> ...", "dateLastCrawled": "2022-01-16T10:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Autonomous algorithmic collusion: Q\u2010<b>learning</b> under sequential pricing ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/1756-2171.12383", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/1756-2171.12383", "snippet": "Q-<b>learning</b>, <b>like</b> any reinforcement <b>learning</b> <b>algorithm</b>, consists of two interacting modules: a <b>learning</b> module that processes the observed information and an action-selection module that balances exploitation (choosing the currently perceived optimal action) with exploration (choosing perhaps another action, to learn what happens). Each module as it is applied in this setting is discussed in turn.", "dateLastCrawled": "2022-01-26T23:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How do I know when a <b>Q-learning</b> <b>algorithm</b> converges? - Cross Validated", "url": "https://stats.stackexchange.com/questions/206944/how-do-i-know-when-a-q-learning-algorithm-converges", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/206944", "snippet": "For example rollout 5 episodes, take average return $ G $ and compare that with best possible $ G_{max} $ (if that info is available) or with 2-3 previous results with something <b>like</b> RMSE. UPD. This one was incorrect. Due to randomness involved in an <b>algorithm</b> you cannot do that. This will work only for value iteration. Track <b>Q-function</b> updates.", "dateLastCrawled": "2022-01-27T04:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Why doesn&#39;t <b>Q-learning</b> converge when using function approximation ...", "url": "https://ai.stackexchange.com/questions/11679/why-doesnt-q-learning-converge-when-using-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/11679/why-doesnt-<b>q-learning</b>-converge-when-using...", "snippet": "$\\begingroup$ @nbro The proof doesn&#39;t say that explicitly, but it assumes an exact representation of the <b>Q-function</b> (that is, that exact values are computed and stored for every state/action pair). For infinite state spaces, it&#39;s clear that this exact representation can be infinitely large in the worst case (simple example: let Q(s,a) = sth digit of pi).", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "neural networks - Is <b>tabular Q-learning considered interpretable</b> ...", "url": "https://ai.stackexchange.com/questions/13921/is-tabular-q-learning-considered-interpretable", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/13921/is-tabular-q-<b>learning</b>-considered...", "snippet": "Based on my hypothesis, I managed to create a tabular Q-<b>learning</b> based <b>algorithm</b> which uses limited domain knowledge to perform on-par/outperform the deep Q-<b>learning</b> based approaches. Given that model interpretability is a subjective and sometimes vague topic, I was wondering if my <b>algorithm</b> should be considered interpretable. The way I ...", "dateLastCrawled": "2022-01-13T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "neural network - Is this a Q-<b>learning</b> <b>algorithm</b> or just brute force ...", "url": "https://datascience.stackexchange.com/questions/28915/is-this-a-q-learning-algorithm-or-just-brute-force", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/28915", "snippet": "One of the big advantages of Q <b>Learning</b> is that it will learn an optimal policy even whilst exploring - this is known as off-policy <b>learning</b>, whilst your <b>algorithm</b> is on-policy, i.e. it learns about the values of how it is currently behaving. This is why you have to reduce the exploration rate over time - and that can be a problem because the exploration rate schedule is a hyper-parameter of your <b>learning</b> <b>algorithm</b> that may need careful tuning.", "dateLastCrawled": "2022-01-20T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "ARIMA Model - <b>Machine</b> <b>Learning</b> Plus", "url": "https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinelearning</b>plus.com/time-series/arima-model-time-series-forecasting-python", "snippet": "Because, forecasting a time series (<b>like</b> demand and sales) is often of tremendous commercial value. In most manufacturing companies, it drives the fundamental business planning, procurement and production activities. Any errors in the forecasts will ripple down throughout the supply chain or any business context for that matter. So it\u2019s important to get the forecasts accurate in order to save on costs and is critical to success. Not just in manufacturing, the techniques and concepts behind ...", "dateLastCrawled": "2022-02-02T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Advanced DQNs: Playing <b>Pac-man</b> with Deep Reinforcement <b>Learning</b> | by ...", "url": "https://towardsdatascience.com/advanced-dqns-playing-pac-man-with-deep-reinforcement-learning-3ffbd99e0814", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/advanced-dqns-playing-<b>pac-man</b>-with-deep-reinforcement...", "snippet": "Just <b>like</b> a human, the <b>algorithm</b> played based on its vision of the screen. Starting from scratch, it discovered gameplay strategies that let it meet (and in many cases, exceed) human benchmarks. In the years since, researchers have made a number of improvements that super-charge performance and solve games faster than ever before. We\u2019ve been working to implement these advancements in Keras \u2014 the open source, highly accessible <b>machine</b> <b>learning</b> framework \u2014 and in this post, we\u2019ll walk ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "In a deep Q-network, what are the advantages/disadvantages of ...", "url": "https://www.quora.com/In-a-deep-Q-network-what-are-the-advantages-disadvantages-of-enumerating-all-possible-action-states-even-if-actions-are-independent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-a-deep-Q-network-what-are-the-advantages-disadvantages-of...", "snippet": "Answer (1 of 3): Q <b>learning</b> requires the creation of a two-dimensional array. The first dimension is the number of possible states, while the second dimension is the number of possible actions. Each state-action pair maintains a Q value, which is the expected discounted long-term reward for selec...", "dateLastCrawled": "2022-01-14T00:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Explainability in deep reinforcement learning</b>", "url": "https://www.researchgate.net/publication/347992255_Explainability_in_deep_reinforcement_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../347992255_<b>Explainability_in_deep_reinforcement_learning</b>", "snippet": "<b>Machine</b> <b>Learning</b> algorithms, ... the <b>Q-function</b> is also vector v alued and each component giv es action. values that accoun t for only a reward type. The sum of each of those vector-v alued ...", "dateLastCrawled": "2022-01-21T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>How does deep reinforcement learning work</b>?", "url": "https://treehozz.com/how-does-deep-reinforcement-learning-work", "isFamilyFriendly": true, "displayUrl": "https://treehozz.com/<b>how-does-deep-reinforcement-learning-work</b>", "snippet": "Q-<b>Learning</b> is a value-based reinforcement <b>learning</b> <b>algorithm</b> which is used to find the optimal action-selection policy using a <b>Q function</b>. Our goal is to maximize the value function Q. This function can be estimated using Q-<b>Learning</b>, which iteratively updates Q(s,a) using the Bellman equation.", "dateLastCrawled": "2022-01-25T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "markov process - <b>Learning</b> optimal value function without using Q ...", "url": "https://stats.stackexchange.com/questions/354927/learning-optimal-value-function-without-using-q-learning", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/354927/<b>learning</b>-optimal-value-function...", "snippet": "In Q-<b>learning</b>, there is a <b>Q function</b> and we update this function using Q-<b>learning</b> <b>algorithm</b> until convergence. When . Stack Exchange Network. Stack Exchange network consists of 178 Q&amp;A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange. Loading\u2026 0 +0; Tour Start here for a quick overview of the site Help Center Detailed answers to any questions you might have Meta ...", "dateLastCrawled": "2022-01-22T12:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "neural networks - Is <b>tabular Q-learning considered interpretable</b> ...", "url": "https://ai.stackexchange.com/questions/13921/is-tabular-q-learning-considered-interpretable", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/13921/is-tabular-q-<b>learning</b>-considered...", "snippet": "Based on my hypothesis, I managed to create a tabular Q-<b>learning</b> based <b>algorithm</b> which uses limited domain knowledge to perform on-par/outperform the deep Q-<b>learning</b> based approaches. Given that model interpretability is a subjective and sometimes vague topic, I was wondering if my <b>algorithm</b> should be considered interpretable. The way I ...", "dateLastCrawled": "2022-01-13T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Autonomous algorithmic collusion: Q\u2010<b>learning</b> under sequential pricing ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/1756-2171.12383", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/1756-2171.12383", "snippet": "Q-<b>learning</b>, like any reinforcement <b>learning</b> <b>algorithm</b>, consists of two interacting modules: a <b>learning</b> module that processes the observed information and an action-selection module that balances exploitation (choosing the currently perceived optimal action) with exploration (choosing perhaps another action, to learn what happens). Each module as it is applied in this setting is discussed in turn.", "dateLastCrawled": "2022-01-26T23:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Q-<b>learning algorithm in reinforcement learning</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/184596/q-learning-algorithm-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/184596", "snippet": "The link above provides a simple solved example of how Q-<b>learning</b> works. It is very intuitive and fairly easy to follow. It repetitively updates the Q ( s, a) matrix until it converges. So for every row s, I will select the element with the highest value and the find the corresponding column a. This means that the max a i will be the optimum ...", "dateLastCrawled": "2022-01-22T19:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Advanced DQNs: Playing <b>Pac-man</b> with Deep Reinforcement <b>Learning</b> | by ...", "url": "https://towardsdatascience.com/advanced-dqns-playing-pac-man-with-deep-reinforcement-learning-3ffbd99e0814", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/advanced-dqns-playing-<b>pac-man</b>-with-deep-reinforcement...", "snippet": "DQN, and <b>similar</b> algorithms like AlphaGo and TRPO, fall under the category of reinforcement <b>learning</b> (RL), a subset of <b>machine</b> <b>learning</b>. In reinforcement <b>learning</b>, an agent exists within an environment and looks to maximize some kind of reward. It takes an action, which changes the environment and feeds it the reward associated with that change. Then it takes a look at its new state and settles on its next action, repeating the process endlessly or until the environment terminates. This ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Can deep reinforcement <b>learning</b> algorithms be deterministic in their ...", "url": "https://ai.stackexchange.com/questions/21369/can-deep-reinforcement-learning-algorithms-be-deterministic-in-their-reproducibi", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/21369/can-deep-reinforcement-<b>learning</b>...", "snippet": "Tour Start here for a quick overview of the site Help Center Detailed answers to any questions you might have Meta Discuss the <b>workings</b> and policies of this site About Us Learn more about Stack Overflow the company ...", "dateLastCrawled": "2022-01-10T14:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "neural network - Is this a Q-<b>learning</b> <b>algorithm</b> or just brute force ...", "url": "https://datascience.stackexchange.com/questions/28915/is-this-a-q-learning-algorithm-or-just-brute-force", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/28915", "snippet": "One of the big advantages of Q <b>Learning</b> is that it will learn an optimal policy even whilst exploring - this is known as off-policy <b>learning</b>, whilst your <b>algorithm</b> is on-policy, i.e. it learns about the values of how it is currently behaving. This is why you have to reduce the exploration rate over time - and that can be a problem because the exploration rate schedule is a hyper-parameter of your <b>learning</b> <b>algorithm</b> that may need careful tuning.", "dateLastCrawled": "2022-01-20T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "In a deep Q-network, what are the advantages/disadvantages of ...", "url": "https://www.quora.com/In-a-deep-Q-network-what-are-the-advantages-disadvantages-of-enumerating-all-possible-action-states-even-if-actions-are-independent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-a-deep-Q-network-what-are-the-advantages-disadvantages-of...", "snippet": "Answer (1 of 3): Q <b>learning</b> requires the creation of a two-dimensional array. The first dimension is the number of possible states, while the second dimension is the number of possible actions. Each state-action pair maintains a Q value, which is the expected discounted long-term reward for selec...", "dateLastCrawled": "2022-01-14T00:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - How to set up a <b>Q function</b> approximator using neural ...", "url": "https://stats.stackexchange.com/questions/389241/how-to-set-up-a-q-function-approximator-using-neural-net-for-ddpg", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/389241/how-to-set-up-a-<b>q-function</b>-approximat...", "snippet": "Deep <b>learning</b> <b>can</b> solve the problem of high observation dimensional data it only handles discrete and low-dimensional action-spaces. This <b>algorithm</b> relies on finding the action that maximizes the action-value function, which in continuous spaces requires solving a complex optimization problem. Obvious solutions like action-space discretization lead to the explosion of the numbers of discrete actions. One of the solutions is used actor-critic approach. It consists of two components: actor ...", "dateLastCrawled": "2022-01-16T10:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Advanced DQNs: Playing <b>Pac-man</b> with Deep Reinforcement <b>Learning</b> | by ...", "url": "https://towardsdatascience.com/advanced-dqns-playing-pac-man-with-deep-reinforcement-learning-3ffbd99e0814", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/advanced-dqns-playing-<b>pac-man</b>-with-deep-reinforcement...", "snippet": "An important detail of RL theory is that the <b>Q function</b> <b>can</b> actually be split up into the sum of two independent terms: the value function V(s), which represents the value of being in the current state, and an advantage function A(s,a), which represents the relative importance of each action, and helps ensure that the agent takes the best action possible, even if that choice may not have any immediate impact on the game score.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reinforcement <b>Learning</b> from scratch | Ai Works", "url": "https://ai.works-hub.com/learn/reinforcement-learning-from-scratch-2ceb2", "isFamilyFriendly": true, "displayUrl": "https://ai.works-hub.com/learn/reinforcement-<b>learning</b>-from-scratch-2ceb2", "snippet": "This means we need to learn a <b>Q function</b> that depends not only on the action (the chest we pick), but the state (what the color of the background is). This version of the problem is called Contextual Multi-armed Bandits. Surprisingly, we <b>can</b> use the same approach as before. The only thing we need to add is an extra dense layer to our neural ...", "dateLastCrawled": "2022-01-19T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding why in deep reinforcement <b>learning</b> correlations in the ...", "url": "https://datascience.stackexchange.com/questions/26631/understanding-why-in-deep-reinforcement-learning-correlations-in-the-data-reduce", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/26631", "snippet": "There is for example a paper Asynchronous Methods for Deep Reinforcement <b>Learning</b> (Mnih et. al. 2016, ICML) where they explain in the introduction that this was &quot;previously <b>thought</b>&quot; to be unstable. The results from the <b>algorithm</b> proposed in this paper (AC3) shows that DRL <b>can</b> be stable.", "dateLastCrawled": "2022-01-08T14:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>machine</b> <b>learning</b> - Is RL applicable to environments that are totally ...", "url": "https://datascience.stackexchange.com/questions/39544/is-rl-applicable-to-environments-that-are-totally-random", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/39544/is-rl-applicable-to-environments...", "snippet": "The RL <b>algorithm</b> will over time learn the likely distribution, but it <b>can</b> only base its own predictions on state variables that you have let it observe. You might be able to go one better: If demand is mostly independent of the stock levels that you hold, then you <b>can</b> separate the problem of predicting it and use more robust supervised <b>learning</b> to add a &quot;predicted demand&quot; feature to the state.", "dateLastCrawled": "2022-01-25T13:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Playing Blackjack using Model-free <b>Reinforcement Learning</b> in Google ...", "url": "https://towardsdatascience.com/playing-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/playing-blackjack-using-model-free-reinforcement...", "snippet": "Monte-Carlo Prediction <b>Algorithm</b>: In order to construct better policies, we need to first be able to evaluate any policy. If an agent follows a policy for many episodes, using Monte-Carlo Prediction, we <b>can</b> construct the Q-table (i.e. \u201cestimate\u201d the action-value function ) from the results from these episodes.", "dateLastCrawled": "2022-02-01T19:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - Why a Random Reward in One-step Dynamics MDP? - Data ...", "url": "https://datascience.stackexchange.com/questions/47436/why-a-random-reward-in-one-step-dynamics-mdp", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/47436/why-a-random-reward-in-one-step...", "snippet": "This answer is not useful. Show activity on this post. In general, R t + 1 is is a random variable with conditional probability distribution P r ( R t + 1 = r | S t = s, A t = a). So it <b>can</b> potentially take on a different value each time action a is taken in state s. Some problems don&#39;t require any randomness in their reward function.", "dateLastCrawled": "2022-01-22T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "reinforcement <b>learning</b> - What is &quot;<b>experience replay</b>&quot; and what are its ...", "url": "https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/20535", "snippet": "It also says this method <b>can</b> improve the off policy <b>learning</b> also . Because in Q <b>learning</b> with act according to epsilon-greedy policy but update values functions according to greedy policy. So when every time step our neural net parameters get updated by mini batch statistics which is more importantly not related to exact time step statistics but what happened before this also help to uncorrelated the data .", "dateLastCrawled": "2022-02-02T22:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Formulation of states for this RL problem and other ...", "url": "https://stats.stackexchange.com/questions/229676/formulation-of-states-for-this-rl-problem-and-other-questions", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/229676/formulation-of-states-for-this-rl...", "snippet": "I <b>thought</b> about giving multiple rewards to the agent after getting each subgoal, but that would be &#39;cheating&#39;. I believe it is standard in RL that the reward is given at the end of the task. I would some your valued insights into this problem. Feel free to send comments or more questions if there are any. <b>machine</b>-<b>learning</b> algorithms reinforcement-<b>learning</b>. Share. Cite. Improve this question. Follow edited Apr 13 &#39;17 at 12:44. Community Bot. 1. asked Aug 13 &#39;16 at 15:35. cgo cgo. 7,275 10 10 ...", "dateLastCrawled": "2022-01-07T14:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Applying linear function approximation to reinforcement <b>learning</b> ...", "url": "https://stats.stackexchange.com/questions/122294/applying-linear-function-approximation-to-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/122294/applying-linear-function...", "snippet": "Applying the architecture of theta and F (s,a) from that page to Sutton&#39;s <b>algorithm</b> works very well. Suppose you have 4 possible actions in a state. Create a reward Q distribution (in this case a 4-value array), with one value for each possible action in the given state. Iterate over each action, and for that action, populate the feature space ...", "dateLastCrawled": "2022-01-08T07:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - Clamping <b>Q function</b> to it&#39;s theoretical maximum, yes ...", "url": "https://datascience.stackexchange.com/questions/24598/clamping-q-function-to-its-theoretical-maximum-yes-or-no", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/24598", "snippet": "on a bootstrap method (Q-<b>Learning</b> or any TD-<b>learning</b> approach) off policy (<b>learning</b> optimal policy from non-optimal behaviour*, which is a feature of Q-<b>learning</b>) This combination is often unstable and difficult to train. Your Q value clamping is one way to help stabilise values. Some features of DQN approach are also designed to deal with this ...", "dateLastCrawled": "2022-01-12T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "normal distribution - Finding the <b>Q function</b> for the EM <b>algorithm</b> ...", "url": "https://stats.stackexchange.com/questions/491920/finding-the-q-function-for-the-em-algorithm", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/.../491920/finding-the-<b>q-function</b>-for-the-em-<b>algorithm</b>", "snippet": "The EM <b>algorithm</b> is used to find (local) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly. Typically these models involve latent variables in addition to unknown parameters and known data observations. That is, either missing values exist among the data, or the model <b>can</b> be formulated more simply by assuming the existence of further unobserved data points. For example, a mixture model <b>can</b> be described more simply by assuming that ...", "dateLastCrawled": "2022-01-19T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "neural network - Is this a Q-<b>learning</b> <b>algorithm</b> or just brute force ...", "url": "https://datascience.stackexchange.com/questions/28915/is-this-a-q-learning-algorithm-or-just-brute-force", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/28915", "snippet": "Question 1: <b>Can</b> I successfully argue that I am estimating the reward based on history, and still claim the <b>algorithm</b> is reinforced <b>learning</b> or even Q-<b>learning</b>? Question 2: If I replace the reward lookup which is based on the board layout, with a neural network, where the board layout is the input and the reward is the output, could the <b>algorithm</b> be regarded as deep reinforcement <b>learning</b>?", "dateLastCrawled": "2022-01-20T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "algorithmic trading - <b>Optimal execution</b> and reinforcement <b>learning</b> ...", "url": "https://quant.stackexchange.com/questions/3551/optimal-execution-and-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://quant.stackexchange.com/.../3551/<b>optimal-execution</b>-and-reinforcement-<b>learning</b>", "snippet": "We provide another way to extend optimal control of trading to <b>machine</b> <b>learning</b>: &#39;simply&#39; write the optimal scheduling problem (for instance over 70 intervals of 5 minutes) with a closed loop controller emulated by a neural network, and wait for the (simulated) end of the day to back propagate over the 70 uses of your neural controller. No need to go through an approximation of a <b>Q-function</b>. It is straightforward and <b>can</b> be easily <b>compared</b> to a non-<b>learning</b> approach.", "dateLastCrawled": "2022-01-27T12:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Can</b> deep reinforcement <b>learning</b> algorithms be deterministic in their ...", "url": "https://ai.stackexchange.com/questions/21369/can-deep-reinforcement-learning-algorithms-be-deterministic-in-their-reproducibi", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/21369/<b>can</b>-deep-reinforcement-<b>learning</b>...", "snippet": "Tour Start here for a quick overview of the site Help Center Detailed answers to any questions you might have Meta Discuss the <b>workings</b> and policies of this site About Us Learn more about Stack Overflow the company ...", "dateLastCrawled": "2022-01-10T14:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Playing Blackjack using Model-free <b>Reinforcement Learning</b> in Google ...", "url": "https://towardsdatascience.com/playing-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/playing-blackjack-using-model-free-reinforcement...", "snippet": "Monte-Carlo Prediction <b>Algorithm</b>: In order to construct better policies, we need to first be able to evaluate any policy. If an agent follows a policy for many episodes, using Monte-Carlo Prediction, we <b>can</b> construct the Q-table (i.e. \u201cestimate\u201d the action-value function ) from the results from these episodes.", "dateLastCrawled": "2022-02-01T19:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Explainability in deep reinforcement learning</b>", "url": "https://www.researchgate.net/publication/347992255_Explainability_in_deep_reinforcement_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../347992255_<b>Explainability_in_deep_reinforcement_learning</b>", "snippet": "standard <b>Machine</b> <b>Learning</b> models, ... where each component is the reward for a certain t ype so that actions <b>can</b> <b>be compared</b> in terms of trade-o\ufb00s. among the types. In the same way, the Q ...", "dateLastCrawled": "2022-01-21T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>How does deep reinforcement learning work</b>?", "url": "https://treehozz.com/how-does-deep-reinforcement-learning-work", "isFamilyFriendly": true, "displayUrl": "https://treehozz.com/<b>how-does-deep-reinforcement-learning-work</b>", "snippet": "Usually, Deep <b>Learning</b> takes more time to train as <b>compared</b> to <b>Machine</b> <b>Learning</b>. The main reason is that there are so many parameters in a Deep <b>Learning</b> <b>algorithm</b>. Whereas <b>Machine</b> <b>Learning</b> takes much less time to train, ranging from a few seconds to a few hours. What is the difference between deep <b>learning</b> and traditional <b>learning</b> models? The most important difference between deep <b>learning</b> and traditional <b>machine</b> <b>learning</b> is its performance as the scale of data increases. When the data is ...", "dateLastCrawled": "2022-01-25T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "In a deep Q-network, what are the advantages/disadvantages of ...", "url": "https://www.quora.com/In-a-deep-Q-network-what-are-the-advantages-disadvantages-of-enumerating-all-possible-action-states-even-if-actions-are-independent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-a-deep-Q-network-what-are-the-advantages-disadvantages-of...", "snippet": "Answer (1 of 3): Q <b>learning</b> requires the creation of a two-dimensional array. The first dimension is the number of possible states, while the second dimension is the number of possible actions. Each state-action pair maintains a Q value, which is the expected discounted long-term reward for selec...", "dateLastCrawled": "2022-01-14T00:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to make a <b>reward</b> function in <b>reinforcement learning</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/189067/how-to-make-a-reward-function-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/189067", "snippet": "One way to view the problem is that the <b>reward</b> function determines the hardness of the problem. For example, traditionally, we might specify a single state to be rewarded: R ( s 1) = 1. R ( s 2.. n) = 0. In this case, the problem to be solved is quite a hard one, <b>compared</b> to, say, R ( s i) = 1 / i 2, where there is a <b>reward</b> gradient over states.", "dateLastCrawled": "2022-01-26T20:28:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>introduction to Q-Learning: Reinforcement Learning</b>", "url": "https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/an-<b>introduction-to-q-learning-reinforcement-learning</b>", "snippet": "In this article, we are going to step into the world of reinforcement <b>learning</b>, another beautiful branch of artificial intelligence, which lets machines learn on their own in a way different from traditional <b>machine</b> <b>learning</b>. Particularly, we will be covering the simplest reinforcement <b>learning</b> algorithm i.e. the Q-<b>Learning</b> algorithm in great detail.", "dateLastCrawled": "2022-01-31T09:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Q-function</b>: input the state-atcion pair, output the Q-value. The letter \u201cQ\u201d is used to represent the quality of taking a given action in a given state. Q-<b>learning</b>. It is used for <b>learning</b> the optimal policy by <b>learning</b> the optimal Q-values for each state-action pair in a Markov Decision Process", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Q-Learning</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/engineering/q-learning", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/engineering/<b>q-learning</b>", "snippet": "Majed Alsadhan, in <b>Machine</b> <b>Learning</b>, Big Data, and IoT for Medical Informatics, 2021. 3.2 Reinforcement <b>learning</b> 3.2.1 Traditional. <b>Q-learning</b> (Watkins and Dayan, 1992) is a simple RL algorithm that given the current state, seeks to find the best action to take in that state. It is an off-policy algorithm because it learns from actions that are random (i.e., outside the policy). The algorithm works in three basic steps: (1) the agent starts in a state and takes an action and receives a ...", "dateLastCrawled": "2022-01-24T01:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Relationship between state (V) and action(Q) value function in ...", "url": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and-action-q-value-function-in-reinforcement-learning-bb9a988c0127", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and...", "snippet": "Value function can be defined as the expected value of an agent in a certain state. There are two types of value functions in RL: State-value and action-value. It is important to understand the\u2026", "dateLastCrawled": "2022-02-03T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Reinforcement Q-<b>Learning</b> from Scratch in Python with OpenAI Gym ...", "url": "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/", "isFamilyFriendly": true, "displayUrl": "https://www.learndatasci.com/tutorials/reinforcement-q-<b>learning</b>-scratch-python-openai-gym", "snippet": "Q-<b>learning</b> is one of the easiest Reinforcement <b>Learning</b> algorithms. The problem with Q-<b>learning</b> however is, once the number of states in the environment are very high, it becomes difficult to implement them with Q table as the size would become very, very large. State of the art techniques uses Deep neural networks instead of the Q-table (Deep Reinforcement <b>Learning</b>). The neural network takes in state information and actions to the input layer and learns to output the right action over the ...", "dateLastCrawled": "2022-02-03T03:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning rate of a Q learning agent</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/33011825/learning-rate-of-a-q-learning-agent", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/33011825", "snippet": "If the <b>learning</b> rate is constant, will <b>Q function</b> converge to the optimal on or <b>learning</b> rate should necessarily decay to guarantee convergence? <b>machine</b>-<b>learning</b> reinforcement-<b>learning</b> q-<b>learning</b>. Share. Follow asked Oct 8 &#39;15 at 9:31. uduck uduck. 119 1 1 silver badge 8 8 bronze badges. 2. 4. With a sufficiently small <b>learning</b> rate you have a convergence guarantee for a convex q <b>learning</b> problem. \u2013 Thomas Jungblut. Oct 8 &#39;15 at 15:27. I assume there is also a dependence on the nature of ...", "dateLastCrawled": "2022-01-24T06:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "On using Huber loss in (Deep) Q-<b>learning</b> | \u30e4\u30ed\u30df\u30eb", "url": "https://jaromiru.wordpress.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://jaromiru.wordpress.com/2017/05/27/on-using-huber-loss-in-deep-q-<b>learning</b>", "snippet": "<b>MACHINE</b> <b>LEARNING</b> &amp; AI. Menu. Let\u2019s make a DQN. Theory; Implementation; Debugging; Full DQN; Double <b>Learning</b> and Prioritized Experience Replay; Let\u2019s make an A3C. Theory ; Implementation; About me; On using Huber loss in (Deep) Q-<b>learning</b>. Posted on May 27, 2017 May 30, 2017 by \u30e4\u30ed\u30df\u30eb. I\u2019ve been recently working on a problem where I put a plain DQN to use. The problem is very simple, deterministic, partially observable and states are quite low-dimensional. The agent however can ...", "dateLastCrawled": "2021-12-26T09:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Why doesn&#39;t <b>Q-learning</b> converge when using function approximation ...", "url": "https://ai.stackexchange.com/questions/11679/why-doesnt-q-learning-converge-when-using-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/11679/why-doesnt-<b>q-learning</b>-converge-when-using...", "snippet": "$\\begingroup$ @nbro The proof doesn&#39;t say that explicitly, but it assumes an exact representation of the <b>Q-function</b> (that is, that exact values are computed and stored for every state/action pair). For infinite state spaces, it&#39;s clear that this exact representation can be infinitely large in the worst case (simple example: let Q(s,a) = sth digit of pi).", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Reinforcement Learning</b> as Heuristic Search <b>Analogy</b> - DataDrivenInvestor", "url": "https://medium.datadriveninvestor.com/reinforcement-learning-as-heuristic-search-analogy-31d92b06dadd", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/<b>reinforcement-learning</b>-as-heuristic-search...", "snippet": "I will not go over all the RL Algorithms, only a subset of those that fit my <b>analogy</b> well, nor will I be giving example code. This post is a purely theoretical outlook and assumes that you can translate the pseudo-code to actual code later. This post will work best if you have some knowledge of basic RL algorithms (TD <b>Learning</b>, Dynamic Programming etc), though I will attempt to go from scratch. Those that have prior knowledge of <b>Reinforcement Learning</b> will benefit the most from this post. On ...", "dateLastCrawled": "2022-01-21T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Applications of <b>Reinforcement Learning</b> in Real World | by garychl ...", "url": "https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/applications-of-<b>reinforcement-learning</b>-in-real-world-1a...", "snippet": "<b>Reinforcement Learning</b> is a very general framework for <b>learning</b> sequential decision making tasks. And Deep <b>Learning</b>, on the other hand, is of course the best set of algorithms we have to learn representations. And combinations of these two different models is the best answer so far we have in terms of <b>learning</b> very good state representations of very challenging tasks that are not just for solving toy domains but actually to solve challenging real world problems.\u201d", "dateLastCrawled": "2022-02-02T20:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Using <b>Keras and Deep Q-Network to Play FlappyBird</b> | Ben Lau", "url": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "isFamilyFriendly": true, "displayUrl": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "snippet": "You just follow the guidiance from the strategy book. Here, <b>Q-function is similar</b> to a strategy guide. Suppose you are in state s and you need to decide whether you take action a or b. If you have this magical Q-function, the answers become really simple \u2013 pick the action with highest Q-value! Here, represents the policy, which you will often see in the ML literature. How do we get the Q-function? That\u2019s where Q-<b>learning</b> is coming from. Let me quickly derive here: Define total future ...", "dateLastCrawled": "2022-01-30T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Learn to Make Decision <b>with Small Data for Autonomous Driving: Deep</b> ...", "url": "https://www.hindawi.com/journals/jat/2020/8495264/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/jat/2020/8495264", "snippet": "GP is a Bayesian nonparametric <b>machine</b> <b>learning</b> framework for regression, classification, and unsupervised <b>learning</b> . A GP ... In addition, the <b>learning</b> method of <b>Q function is similar</b> to that in DQN as well. In our case, we train a deep neural network by DDPG to achieve successful loop trip. It takes about 16 hours and 4000 episodes to achieve a high performance deep neural network. And tens of thousands of data will be updated in the centralized experience replay buffer during training ...", "dateLastCrawled": "2022-01-22T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficient Navigation of Colloidal Robots in an Unknown Environment via ...", "url": "https://onlinelibrary.wiley.com/doi/10.1002/aisy.201900106", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1002/aisy.201900106", "snippet": "In free space navigation (Figure 2a), the navigation strategy derived from the learned optimal <b>Q* function is similar</b> to previous studies 18, 43, 44 and can be summarized approximately as \u03c0 * (s) = {v max, d n \u2208 [d c, \u221e) v max, d n \u2208 [0, d c), \u03b1 n \u2208 [\u2212 \u03b1 c, \u03b1 c] 0, otherwise (3) where d n is the projection of the target-particle vector onto the orientation vector n = (cos\u03b8, sin\u03b8), \u03b1 n is the angle between target-particle distance vector and n, and parameters d c and \u03b1 c are ...", "dateLastCrawled": "2022-01-20T08:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Adapting Soft Actor Critic for Discrete Action Spaces | by Felix ...", "url": "https://towardsdatascience.com/adapting-soft-actor-critic-for-discrete-action-spaces-a20614d4a50a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/adapting-soft-actor-critic-for-discrete-action-spaces-a...", "snippet": "This should accelerate <b>learning</b> in the later stages of training and help with avoiding local optima. Just as before we want to find \u03b8 that optimizes the expected return. To do so in the entropy regularized setting we can simply add an estimate of the entropy to our estimate of the expected return: Entropy Regularized Actor Cost Function. Figure 7: Entropy regularized critic cost functions. How we adapt the Bellman equation for our <b>Q-function is similar</b> to what we have seen in the definition ...", "dateLastCrawled": "2022-02-03T12:06:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep Reinforcement <b>Learning</b> for Agriculture: Principles and Use Cases ...", "url": "https://link.springer.com/chapter/10.1007/978-981-16-5847-1_4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-981-16-5847-1_4", "snippet": "In other words, the Q-function captures the expected total future rewards agent i can receive in state s t by taking action a t. <b>Q-function can be thought of as</b> a table look up, where rows of the table are states s and columns represent actions a.Ultimately, the <b>learning</b> agent i needs to find the best action given current state s.This is called a policy \u03c0(s).Policy captures the <b>learning</b> agent&#39;s behavior at any given time.", "dateLastCrawled": "2022-01-27T09:13:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(q-function)  is like +(workings of a machine learning algorithm)", "+(q-function) is similar to +(workings of a machine learning algorithm)", "+(q-function) can be thought of as +(workings of a machine learning algorithm)", "+(q-function) can be compared to +(workings of a machine learning algorithm)", "machine learning +(q-function AND analogy)", "machine learning +(\"q-function is like\")", "machine learning +(\"q-function is similar\")", "machine learning +(\"just as q-function\")", "machine learning +(\"q-function can be thought of as\")", "machine learning +(\"q-function can be compared to\")"]}