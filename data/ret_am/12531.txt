{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Uncertainty</b> Reduction as a Measure of Cognitive Load in Sentence ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12025", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12025", "snippet": "<b>The amount</b> of <b>information</b> that the sentence&#39;s next word, , gives about the random variable X is defined as the reduction in entropy due to that word2 2 Although alternative definitions are possible, the entropy\u2010reduction measure <b>has</b> the important property that it is additive: The <b>information</b> conveyed by equals the <b>information</b> given by plus the additional <b>information</b> by (Blachman, 1968).", "dateLastCrawled": "2020-06-01T22:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Entropy (information theory</b>) - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Entropy_(information_theory)", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Entropy_(information_theory</b>)", "snippet": "where denotes the sum over the variable&#39;s possible values. The choice of base for , the logarithm, varies for different applications.Base 2 gives the unit of bits (or &quot;shannons&quot;), while base e gives &quot;natural units&quot; nat, and base 10 gives units of &quot;dits&quot;, &quot;bans&quot;, or &quot;hartleys&quot;.An equivalent definition of entropy is the expected value of the self-<b>information</b> of a variable.. The concept of <b>information</b> entropy was introduced by Claude Shannon in his 1948 paper &quot;A Mathematical Theory of ...", "dateLastCrawled": "2022-02-06T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Estimating Transfer Entropy in Continuous Time Between Neural Spike ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8084348/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8084348", "snippet": "Although there <b>has</b> been much recent progress on parametric <b>information</b>-theoretic estimators , such estimators will always inject modelling assumptions <b>into</b> the estimation process. <b>Even</b> in the case that large, general, parametric models are used\u2014as in [ 82 ]\u2014there are no known methods of determining whether such a model is capturing <b>all</b> dependencies present within the data.", "dateLastCrawled": "2022-02-02T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Information</b> theory - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Information_theory", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Information</b>_theory", "snippet": "<b>Information</b> theory is the scientific study of the quantification, storage, and communication of digital <b>information</b>. The field was fundamentally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.: vii The field is at the intersection of probability theory, statistics, computer science, statistical mechanics, <b>information</b> engineering, and electrical engineering. A key measure in <b>information</b> theory is entropy.Entropy quantifies <b>the amount</b> ...", "dateLastCrawled": "2022-02-03T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Evaluating Machine Learning Models</b> \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/content/evaluating-machine-learning-models/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/content/<b>evaluating-machine-learning-models</b>", "snippet": "The beautiful thing about this definition is that it is intimately tied to <b>information</b> theory: log-loss is the <b>cross entropy</b> between the distribution of the true labels and the predictions, and it is very closely related to what\u2019s known as the relative entropy, or Kullback\u2013Leibler divergence. Entropy measures the unpredictability of something. <b>Cross entropy</b> incorporates the entropy of the true distribution, plus the extra unpredictability when one assumes a different distribution than ...", "dateLastCrawled": "2022-01-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "RoBERTa: A <b>Robustly Optimized BERT Pretraining Approach</b> | DeepAI", "url": "https://deepai.org/publication/roberta-a-robustly-optimized-bert-pretraining-approach", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/roberta-a-<b>robustly-optimized-bert-pretraining-approach</b>", "snippet": "The MLM objective is a <b>cross-entropy</b> loss on predicting the masked tokens. BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [M A S K], 10% are left unchanged, and 10% are replaced by a randomly selected vocabulary token. In the original implementation, random masking and replacement is performed once in the beginning and saved for the duration of training, although in practice, data is duplicated so the mask is not always ...", "dateLastCrawled": "2022-01-14T04:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "BayLearn - Machine Learning Symposium - 2021: Accepted Submissions", "url": "https://baylearn-org.github.io/www/submissions", "isFamilyFriendly": true, "displayUrl": "https://baylearn-org.github.io/www/submissions", "snippet": "Prior work <b>has</b> shown that it is possible to learn inter-agent communication protocols using multi-agent reinforcement learning and message-passing network architectures. However, these models use an unconstrained broadcast communication model, in which an agent communicates with <b>all</b> other agents at every step, <b>even</b> when the task does not ...", "dateLastCrawled": "2022-01-24T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Survey of Machine Learning for Big Code and Naturalness | DeepAI", "url": "https://deepai.org/publication/a-survey-of-machine-learning-for-big-code-and-naturalness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-of-machine-learning-for-big-code-and-naturalness", "snippet": "For example, identifiers, <b>even</b> verbose function names that seek to describe their function, carry less <b>information</b> than words <b>like</b> \u201cChristmas\u201d or \u201cset\u201d. In general, statements in code and sentences in text differ in how much background knowledge the reader needs in order to understand them in isolation; an arbitrary statement is far more likely to use domain-specific, <b>even</b> project-specific, names or neologisms than an arbitrary sentence is. Blocks vary greatly in length and semantics ...", "dateLastCrawled": "2021-12-12T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A survey on deep learning and deep reinforcement learning in robotics ...", "url": "https://link.springer.com/article/10.1007/s11370-021-00398-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11370-021-00398-z", "snippet": "<b>After</b> DDPG, several extensions have been suggested, <b>like</b> distributed distributional DDPG (D4PG) (to make it run in a distribution fashion, using N-step returns and prioritized experience replay), multi-agent DDPG (MADDPG) (where multiple agents are coordinated to complete tasks with only local <b>information</b>), and twin delayed deep deterministic (TD3) (uses clipped double Q-learning, a delayed update of the target and policy networks, and smoothing over the target policy to avoid overestimation ...", "dateLastCrawled": "2022-01-29T22:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "172 questions with answers in <b>INFORMATION THEORY</b> | Science topic", "url": "https://www.researchgate.net/topic/Information-Theory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/<b>Information-Theory</b>", "snippet": "3 answers. Jan 20, 2018. Here&#39;s how <b>information theory</b> <b>has</b> been helping us analyze real customer data sets across different domains: a) One of the basic ideas of <b>Information theory</b> is that the ...", "dateLastCrawled": "2022-01-04T19:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Survey of Unsupervised Deep Domain Adaptation", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323662/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8323662", "snippet": "The task loss used is generally a <b>cross-entropy</b> loss, or more specifically the negative log likelihood of a softmax distribution when using a softmax output layer. The exceptions not including a task loss are SimNet [ 187 ] that classify based on distance to prototypes of each class, the work by Sener et al. [ 214 ] that uses k nearest neighbors, and AdaBN [ 141 ] that only adjusts the batch norm layers to the target domain.", "dateLastCrawled": "2022-01-27T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Entropy (information theory</b>) - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Entropy_(information_theory)", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Entropy_(information_theory</b>)", "snippet": "In the view of Jaynes (1957), thermodynamic entropy, as explained by statistical mechanics, should be seen as an application of Shannon&#39;s <b>information</b> theory: the thermodynamic entropy is interpreted as being proportional <b>to the amount</b> of further Shannon <b>information</b> needed to define the detailed microscopic state of the system, <b>that remains</b> uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics, with the constant of proportionality being just ...", "dateLastCrawled": "2022-02-02T08:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>cross entropy approach to design of reliable networks</b> | Request PDF", "url": "https://www.researchgate.net/publication/222952511_A_cross_entropy_approach_to_design_of_reliable_networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/222952511_A_<b>cross_entropy_approach_to_design</b>...", "snippet": "System reliability <b>has</b> been a prominent research topic in the research field of reliability theory, and a sizeable <b>amount</b> of literature <b>has</b> proposed methods for economic analysis of systems. Most ...", "dateLastCrawled": "2021-10-19T04:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Estimating Transfer Entropy in Continuous Time Between Neural Spike ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8084348/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8084348", "snippet": "Although there <b>has</b> been much recent progress on parametric <b>information</b>-theoretic estimators , such estimators will always inject modelling assumptions <b>into</b> the estimation process. <b>Even</b> in the case that large, general, parametric models are used\u2014as in [ 82 ]\u2014there are no known methods of determining whether such a model is capturing <b>all</b> dependencies present within the data.", "dateLastCrawled": "2022-02-02T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Uncertainty</b> Reduction as a Measure of Cognitive Load in Sentence ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12025", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12025", "snippet": "The <b>amount</b> of <b>information</b> that the sentence&#39;s next word, , gives about the random variable X is defined as the reduction in entropy due to that word2 2 Although alternative definitions are possible, the entropy\u2010reduction measure <b>has</b> the important property that it is additive: The <b>information</b> conveyed by equals the <b>information</b> given by plus the additional <b>information</b> by (Blachman, 1968).", "dateLastCrawled": "2020-06-01T22:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Evaluating Machine Learning Models</b> \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/content/evaluating-machine-learning-models/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/content/<b>evaluating-machine-learning-models</b>", "snippet": "The beautiful thing about this definition is that it is intimately tied to <b>information</b> theory: log-loss is the <b>cross entropy</b> between the distribution of the true labels and the predictions, and it is very closely related to what\u2019s known as the relative entropy, or Kullback\u2013Leibler divergence. Entropy measures the unpredictability of something. <b>Cross entropy</b> incorporates the entropy of the true distribution, plus the extra unpredictability when one assumes a different distribution than ...", "dateLastCrawled": "2022-01-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Efficient Test Selection in Active Diagnosis via Entropy ...", "url": "https://www.researchgate.net/publication/228333645_Efficient_Test_Selection_in_Active_Diagnosis_via_Entropy_Approximation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228333645_Efficient_Test_Selection_in_Active...", "snippet": "However, the selection of best possible target probe set from the CPS is proved to be NP-Complete in Natu et al. 10 Several heuristic-based algorithms are proposed in several studies. 1,2,4, 5, 8 ...", "dateLastCrawled": "2021-10-13T09:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "notes-2/Deep Learning.md at master \u00b7 rsantana-isg/notes-2 \u00b7 GitHub", "url": "https://github.com/rsantana-isg/notes-2/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rsantana-isg/notes-2/blob/master/Deep Learning.md", "snippet": "This <b>is similar</b> to plugging the pixels of the image <b>into</b> a char-rnn, but the RNNs run both horizontally and vertically over the image instead of just a 1D sequence of characters. But the ordering of the dimensions, although often arbitrary, can be critical to the training of the model. The sequential nature of this model limits its computational efficiency. For example, its sampling procedure is sequential and non-parallelizable. Additionally, there is no natural latent representation ...", "dateLastCrawled": "2022-01-02T20:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "BayLearn - Machine Learning Symposium - 2021: Accepted Submissions", "url": "https://baylearn-org.github.io/www/submissions", "isFamilyFriendly": true, "displayUrl": "https://baylearn-org.github.io/www/submissions", "snippet": "SkillRank is a way of quantifying skills from a job market&#39;s perspective, and it works by <b>taking</b> <b>into</b> <b>account</b> the frequency and weight of interactions between jobs and skills to determine a rough estimate of the trendiness of skills in the job market. Initial results show that SkillRank provides us a novel and quantitative way to understand skills in the job market.", "dateLastCrawled": "2022-01-24T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Artificial Intelligence - IIT</b> CSE by IGNOU MCA - Issuu", "url": "https://issuu.com/imsf/docs/artificial-intelligence_iit", "isFamilyFriendly": true, "displayUrl": "https://issuu.com/imsf/docs/<b>artificial-intelligence_iit</b>", "snippet": "A state is defined by the specification of the values of <b>all</b> attributes of interest in the world An operator changes one state <b>into</b> the other; it <b>has</b> a precondition which is the value of certain ...", "dateLastCrawled": "2022-01-26T12:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "BayLearn - Machine Learning Symposium - 2021: Accepted Submissions", "url": "https://baylearn-org.github.io/www/submissions.html", "isFamilyFriendly": true, "displayUrl": "https://baylearn-org.github.io/www/submissions.html", "snippet": "These permutations <b>can</b> <b>be thought</b> of as adversarial transformations that expose blind spots in record matching algorithms. Identifying <b>all</b> possible iterations <b>can</b> be computationally intensive, requiring on-the-fly calculations of <b>all</b> likely permutations, and then checking them against a customer name list. In applications with very tight latency requirements this combinatorial explosion <b>can</b> be prohibitive. To mitigate the need to compute these permutations manually, we propose using ...", "dateLastCrawled": "2021-12-23T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Information</b> theory - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Information_theory", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Information</b>_theory", "snippet": "<b>Information</b> theory is the scientific study of the quantification, storage, and communication of digital <b>information</b>. The field was fundamentally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.: vii The field is at the intersection of probability theory, statistics, computer science, statistical mechanics, <b>information</b> engineering, and electrical engineering. A key measure in <b>information</b> theory is entropy.Entropy quantifies the <b>amount</b> ...", "dateLastCrawled": "2022-02-03T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Evaluating Machine Learning Models</b> \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/content/evaluating-machine-learning-models/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/content/<b>evaluating-machine-learning-models</b>", "snippet": "The beautiful thing about this definition is that it is intimately tied to <b>information</b> theory: log-loss is the <b>cross entropy</b> between the distribution of the true labels and the predictions, and it is very closely related to what\u2019s known as the relative entropy, or Kullback\u2013Leibler divergence. Entropy measures the unpredictability of something. <b>Cross entropy</b> incorporates the entropy of the true distribution, plus the extra unpredictability when one assumes a different distribution than ...", "dateLastCrawled": "2022-01-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Survey of Machine Learning for Big Code and Naturalness | DeepAI", "url": "https://deepai.org/publication/a-survey-of-machine-learning-for-big-code-and-naturalness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-of-machine-learning-for-big-code-and-naturalness", "snippet": "One might reasonably ask why it is necessary to handle <b>uncertainty</b> and noise in software development tools, when in many cases the program to be analyzed is known (there is no <b>uncertainty</b> about what the programmer <b>has</b> written) and is deterministic. In fact, there are several interesting motivations for incorporating probabilistic modeling <b>into</b> machine learning methods for software development. First, probabilistic methods offer a principled method for handling <b>uncertainty</b> and fusing multiple ...", "dateLastCrawled": "2021-12-12T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The explanation game: a <b>formal framework for interpretable machine</b> ...", "url": "https://link.springer.com/article/10.1007/s11229-020-02629-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11229-020-02629-9", "snippet": "When those explanations are accurate and simple, <b>Alice</b> <b>can</b> easily be fooled <b>into</b> thinking she <b>has</b> learned some valuable <b>information</b>. In fact, Bob <b>has</b> merely overfit the data. Matters are <b>even</b> worse when we seek to audit algorithms. In this case, eclipsing explanations may actually offer loopholes to bad actors wishing to avoid controversy over questionable decisions. For instance, a myopic focus on accuracy and simplicity would allow (bad) banks to get away with racist loan policies so long ...", "dateLastCrawled": "2021-12-09T10:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "172 questions with answers in <b>INFORMATION THEORY</b> | Science topic", "url": "https://www.researchgate.net/topic/Information-Theory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/<b>Information-Theory</b>", "snippet": "3 answers. Jan 20, 2018. Here&#39;s how <b>information theory</b> <b>has</b> been helping us analyze real customer data sets across different domains: a) One of the basic ideas of <b>Information theory</b> is that the ...", "dateLastCrawled": "2022-01-04T19:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Using multivariate statistical methods to model the electrospray ...", "url": "https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/rcm.4141", "isFamilyFriendly": true, "displayUrl": "https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/rcm.4141", "snippet": "The response factor <b>can</b> then be obtained by fitting a curve to the experimental dependence of signal on time. F X (henceforth referred to as RF) and \u03c3 t are variables in the fitting; t p is defined as the time that divides the area of the peak <b>into</b> halves. A computer program was developed in-house using C-Sharp (C#; Microsoft Corporation) to evaluate the data through a least-squares minimization procedure.", "dateLastCrawled": "2021-12-16T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Soft <b>computing applications in customer segmentation: State</b>-of-art ...", "url": "https://www.researchgate.net/publication/257405081_Soft_computing_applications_in_customer_segmentation_State-of-art_review_and_critique", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/257405081_Soft_computing_applications_in...", "snippet": "The goal of data mining projects is to convert the raw data <b>into</b> useful <b>information</b>. Clustering <b>can</b> also be used to explore differences in attitudes and intentions of the clients. In this study ...", "dateLastCrawled": "2022-01-03T11:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "notes-1/Deep Learning.md at master \u00b7 kirk86/notes-1 \u00b7 <b>GitHub</b>", "url": "https://github.com/kirk86/notes-1/blob/master/Deep%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/kirk86/notes-1/blob/master/Deep Learning.md", "snippet": "In practice this metric does not take <b>into</b> <b>account</b> mode dropping if the number of modes is greater than the number of samples one is visualizing. In fact, the mode dropping problem generally helps visual sample quality as the model <b>can</b> choose to focus on only the most common modes. These common modes correspond, by definition, to more typical samples. Additionally, the generative model is able to allocate more expressive power to the modes it does cover than it would if it attempted to cover ...", "dateLastCrawled": "2021-12-24T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Language Models are Few-Shot Learners \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2005.14165/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2005.14165", "snippet": "K <b>can</b> be any value from 0 to the maximum <b>amount</b> allowed by the model\u2019s context window, which is n c t x = 2048 for <b>all</b> models and typically fits 10 to 100 examples. Larger values of K are usually but not always better, so when a separate development and test set are available, we experiment with a few values of K on the development set and then run the best value on the test set.", "dateLastCrawled": "2022-01-29T11:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>cross entropy approach to design of reliable networks</b> | Request PDF", "url": "https://www.researchgate.net/publication/222952511_A_cross_entropy_approach_to_design_of_reliable_networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/222952511_A_<b>cross_entropy_approach_to_design</b>...", "snippet": "System reliability <b>has</b> been a prominent research topic in the research field of reliability theory, and a sizeable <b>amount</b> of literature <b>has</b> proposed methods for economic analysis of systems. Most ...", "dateLastCrawled": "2021-10-19T04:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Estimating Transfer Entropy in Continuous Time Between Neural Spike ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8084348/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8084348", "snippet": "Although there <b>has</b> been much recent progress on parametric <b>information</b>-theoretic estimators , such estimators will always inject modelling assumptions <b>into</b> the estimation process. <b>Even</b> in the case that large, general, parametric models are used\u2014as in [ 82 ]\u2014there are no known methods of determining whether such a model is capturing <b>all</b> dependencies present within the data.", "dateLastCrawled": "2022-02-02T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 2, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Entropy (information theory</b>) - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Entropy_(information_theory)", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Entropy_(information_theory</b>)", "snippet": "where denotes the sum over the variable&#39;s possible values. The choice of base for , the logarithm, varies for different applications.Base 2 gives the unit of bits (or &quot;shannons&quot;), while base e gives &quot;natural units&quot; nat, and base 10 gives units of &quot;dits&quot;, &quot;bans&quot;, or &quot;hartleys&quot;.An equivalent definition of entropy is the expected value of the self-<b>information</b> of a variable.. The concept of <b>information</b> entropy was introduced by Claude Shannon in his 1948 paper &quot;A Mathematical Theory of ...", "dateLastCrawled": "2022-02-06T23:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Survey of Unsupervised Deep Domain Adaptation", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323662/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8323662", "snippet": "2.1. Transfer Learning. The focus of this survey is domain adaptation. Because domain adaptation <b>can</b> be viewed as a special case of transfer learning [], we first review transfer learning to highlight the role of domain adaptation within this topic.Transfer learning is defined as the learning scenario where a model is trained on a source domain or task and evaluated on a different but related target domain or task, where either the tasks or domains (or both) differ [61, 80, 180, 252].For ...", "dateLastCrawled": "2022-01-27T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Uncertainty</b> Reduction as a Measure of Cognitive Load in Sentence ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12025", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/tops.12025", "snippet": "The <b>amount</b> of <b>information</b> that the sentence&#39;s next word, , gives about the random variable X is defined as the reduction in entropy due to that word2 2 Although alternative definitions are possible, the entropy\u2010reduction measure <b>has</b> the important property that it is additive: The <b>information</b> conveyed by equals the <b>information</b> given by plus the additional <b>information</b> by (Blachman, 1968).", "dateLastCrawled": "2020-06-01T22:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 5, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Information</b> theory - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Information_theory", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Information</b>_theory", "snippet": "<b>Information</b> theory is the scientific study of the quantification, storage, and communication of digital <b>information</b>. The field was fundamentally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.: vii The field is at the intersection of probability theory, statistics, computer science, statistical mechanics, <b>information</b> engineering, and electrical engineering. A key measure in <b>information</b> theory is entropy.Entropy quantifies the <b>amount</b> ...", "dateLastCrawled": "2022-02-03T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>All-Terminal Network Reliability Optimization in Fading Environment</b> via ...", "url": "https://www.researchgate.net/publication/221171002_All-Terminal_Network_Reliability_Optimization_in_Fading_Environment_via_Cross_Entropy_Method", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221171002_<b>All</b>-Terminal_Network_Reliability...", "snippet": "As a result, suggested network topologies become ineffective at presence of obstacles within serviced area. More realistic solution presumes <b>taking</b> <b>into</b> <b>account</b> of obstacles within the serviced ...", "dateLastCrawled": "2021-08-29T05:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Evaluating Machine Learning Models</b> \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/content/evaluating-machine-learning-models/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/content/<b>evaluating-machine-learning-models</b>", "snippet": "The beautiful thing about this definition is that it is intimately tied to <b>information</b> theory: log-loss is the <b>cross entropy</b> between the distribution of the true labels and the predictions, and it is very closely related to what\u2019s known as the relative entropy, or Kullback\u2013Leibler divergence. Entropy measures the unpredictability of something. <b>Cross entropy</b> incorporates the entropy of the true distribution, plus the extra unpredictability when one assumes a different distribution than ...", "dateLastCrawled": "2022-01-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Survey of Machine Learning for Big Code and Naturalness | DeepAI", "url": "https://deepai.org/publication/a-survey-of-machine-learning-for-big-code-and-naturalness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-of-machine-learning-for-big-code-and-naturalness", "snippet": "One might reasonably ask why it is necessary to handle <b>uncertainty</b> and noise in software development tools, when in many cases the program to be analyzed is known (there is no <b>uncertainty</b> about what the programmer <b>has</b> written) and is deterministic. In fact, there are several interesting motivations for incorporating probabilistic modeling <b>into</b> machine learning methods for software development. First, probabilistic methods offer a principled method for handling <b>uncertainty</b> and fusing multiple ...", "dateLastCrawled": "2021-12-12T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "BayLearn - Machine Learning Symposium - 2021: Accepted Submissions", "url": "https://baylearn-org.github.io/www/submissions.html", "isFamilyFriendly": true, "displayUrl": "https://baylearn-org.github.io/www/submissions.html", "snippet": "<b>Compared</b> to the state-of-the-art, we achieve competitive results on memory compression and superior results for compute compression for MobileNet-V2, ResNet50 and ResNeXt-101-32x8d. A deeper analysis of the results obtained by NEMO also shows that both the graph representation and the species-based approach are critical in finding effective configurations for <b>all</b> workloads. Unsupervised Activity Segmentation by Joint Representation Learning and Online Clustering Sateesh Kumar (Retrocausal ...", "dateLastCrawled": "2021-12-23T04:45:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Cross-Entropy</b> Demystified. What is it? Is there any relation to\u2026 | by ...", "url": "https://naokishibuya.medium.com/demystifying-cross-entropy-e80e3ad54a8", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/demystifying-<b>cross-entropy</b>-e80e3ad54a8", "snippet": "However, the <b>machine</b> <b>learning</b> application uses the base e logarithm for implementation convenience. Binary <b>Cross-Entropy</b>. We can use the binary <b>cross-entropy</b> for binary classification where we have yes/no answer. For example, there are only dogs or cats in images. For the binary classifications, the <b>cross-entropy</b> formula contains only two ...", "dateLastCrawled": "2022-01-26T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Why and How to use <b>Cross Entropy</b>. The fundamental reasons for ...", "url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-and-how-to-use-<b>cross-entropy</b>-4e983cbdd873", "snippet": "The fundamental reasons for minimizing binary <b>cross entropy</b> (log loss) with probabilistic classification models . Will Arliss. Sep 26, 2020 \u00b7 7 min read. Introduction. This post discusses why logistic regression necessarily uses a different loss function than linear regression. First, the simple yet inefficient way to solve logistic regression will be presented, then the slightly less simple but much more efficient way will be explained and compared. The simple way. Linear regression is the ...", "dateLastCrawled": "2022-01-31T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Gentle Introduction to Information Entropy - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-is-information-entropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/what-is-information-entropy", "snippet": "Calculating information and entropy is a useful tool in <b>machine</b> <b>learning</b> and is used as the basis for techniques such as feature selection, building decision trees, and, more generally, fitting classification models. As such, a <b>machine</b> <b>learning</b> practitioner requires a strong understanding and intuition for information and entropy. In this post, you will discover a gentle introduction to information entropy. After reading this post, you will know: Information theory is concerned with data ...", "dateLastCrawled": "2022-02-02T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>machine</b> <b>learning</b> - <b>Cross-entropy loss</b> explanation - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/20296", "snippet": "The answer from Neil is correct. However I think its important to point out that while the loss does not depend on the distribution between the incorrect classes (only the distribution between the correct class and the rest), the gradient of this loss function does effect the incorrect classes differently depending on how wrong they are. So when you use cross-ent in <b>machine</b> <b>learning</b> you will change weights differently for [0.1 0.5 0.1 0.1 0.2] and [0.1 0.6 0.1 0.1 0.1].", "dateLastCrawled": "2022-01-27T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "ML Concepts. A.I \u2014 Intelligence showed by machines which is common for humans <b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>.We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Shannon <b>entropy</b> in the context of <b>machine</b> <b>learning</b> and AI | by Frank ...", "url": "https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/shannon-<b>entropy</b>-in-the-context-of-<b>machine</b>-<b>learning</b>-and-ai-24...", "snippet": "Closely related to <b>cross entropy</b>, the KL divergence from q to p, written DKL(p||q), is another similarity measure often used in <b>machine</b> <b>learning</b>. In the language of Bayesian Inference, DKL(p||q ...", "dateLastCrawled": "2022-01-30T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>CS 189/289A</b>: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189", "snippet": "Stanford&#39;s <b>machine</b> <b>learning</b> class provides additional reviews of linear algebra and ... (<b>cross-entropy</b>) functions. The fifth demo gives you sliders so you can understand how softmax works. Lecture 19 (April 6): Heuristics for faster training. Heuristics for avoiding bad local minima. Heuristics to avoid overfitting. Convolutional neural networks. Neurology of retinal ganglion cells in the eye and simple and complex cells in the V1 visual cortex. Read ESL, Sections 11.5 and 11.7. Here is the ...", "dateLastCrawled": "2022-02-02T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Main concepts behind Machine Learning</b> | by Bruno Eidi Nishimoto ...", "url": "https://medium.com/neuronio/main-concepts-behind-machine-learning-22cd81d68a11", "isFamilyFriendly": true, "displayUrl": "https://medium.com/neuronio/<b>main-concepts-behind-machine-learning</b>-22cd81d68a11", "snippet": "<b>Machine</b> <b>Learning</b> is a concept that is currently trending. It is a subarea from Artificial Intelligence and it consists on the fact that the <b>machine</b> can learn by itself without being explicitly ...", "dateLastCrawled": "2022-01-19T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Lecture 4 Fundamentals of deep <b>learning</b> and neural networks", "url": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "snippet": "Deep <b>learning</b>: <b>Machine</b> <b>learning</b> models based on \u201cdeep\u201d neural networks comprising millions (sometimes billions) of parameters organized into hierarchical layers. Features are multiplied and added together repeatedly, with the outputs from one layer of parameters being fed into the next layer -- before a prediction is made. Contrast with linear regression: Agenda for today - More on the structure of neural network models - <b>Machine</b> <b>learning</b> training loop and concept of loss, in the context ...", "dateLastCrawled": "2022-02-02T09:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] A Short Introduction to Entropy, <b>Cross-Entropy</b> and KL-Divergence ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7vhmp7/d_a_short_introduction_to_entropy_crossentropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7vhmp7/d_a_short_introduction_to...", "snippet": "I am having trouble reconciling the concept with the <b>analogy</b>. At 2:35 even if a rainy day was 25% likely, there&#39;s still only two states, rainy and sunny, and therefor only 1 bit of information is needed to convey that, so only one bit of data needs to be sent, even though the 1 bit of data reduces the uncertainty of a rainy day by a factor of 4. I quite don&#39;t get what he means by this being 2 bits of information. I guess where I am stuck is how the uncertainty reduction factor translates to ...", "dateLastCrawled": "2021-08-20T08:03:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Beat the Bookmakers With Tree-Based <b>Machine</b> <b>Learning</b> Algorithms | by ...", "url": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-machine-learning-algorithms-1d349335b54", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-<b>machine</b>...", "snippet": "<b>Cross-entropy is similar</b> to Gini Impurity, but it involves using the concept of entropy from information theory. This article won\u2019t go in depth about it, but essentially, as the cross-entropy ...", "dateLastCrawled": "2022-01-26T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>A Traveler\u2019s Diary on the Road to Machine</b> <b>Learning</b> - Chapter 1 | by ...", "url": "https://medium.com/swlh/a-travelers-diary-on-the-road-to-machine-learning-chapter-1-8850ec5b4243", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>a-travelers-diary-on-the-road-to-machine</b>-<b>learning</b>-chapter-1...", "snippet": "Types of <b>Machine</b> <b>Learning</b> algorithms: ... Sparse categorical <b>cross entropy is similar</b> to categorical cross entropy, only difference is it uses only one value as target. It saves memory as well as ...", "dateLastCrawled": "2021-05-21T04:27:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Deep Learning for Computer Architects</b> | Chen Jeff - Academia.edu", "url": "https://www.academia.edu/40860009/Deep_Learning_for_Computer_Architects", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40860009/<b>Deep_Learning_for_Computer_Architects</b>", "snippet": "This text serves as a primer for computer architects in a new and rapidly evolving \ufb01eld. We review how <b>machine</b> <b>learning</b> has evolved since its inception in the 1960s and track the key developments leading up to the emergence of the powerful deep <b>learning</b> techniques that emerged in the last decade.", "dateLastCrawled": "2022-01-28T02:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(cross-entropy)  is like +(the amount of uncertainty that remains, even after taking into account all of the information that Alice has)", "+(cross-entropy) is similar to +(the amount of uncertainty that remains, even after taking into account all of the information that Alice has)", "+(cross-entropy) can be thought of as +(the amount of uncertainty that remains, even after taking into account all of the information that Alice has)", "+(cross-entropy) can be compared to +(the amount of uncertainty that remains, even after taking into account all of the information that Alice has)", "machine learning +(cross-entropy AND analogy)", "machine learning +(\"cross-entropy is like\")", "machine learning +(\"cross-entropy is similar\")", "machine learning +(\"just as cross-entropy\")", "machine learning +(\"cross-entropy can be thought of as\")", "machine learning +(\"cross-entropy can be compared to\")"]}