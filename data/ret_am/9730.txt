{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Addressing Fairness, Bias, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "When two separate <b>demographic</b> groups are being included in <b>a machine</b> learning diagnostic model, the difference between the two groups <b>can</b> be examined on an ROC curve, as shown in Figure 2. Due to biological differences, as well as difference in disease prevalence, each <b>demographic</b> group will have a separate ROC curve. As shown in the figure, it is impossible to tune the model and select an operating point that will perform optimally in both <b>demographic</b> groups. If a single diagnostic test ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Explainable AI: A Review of <b>Machine</b> Learning Interpretability Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "A counterfactual explanation of a prediction describes the smallest possible change that <b>can</b> be applied to the feature values, <b>so</b> that the <b>output</b> prediction <b>can</b> be changed to a desired predefined <b>output</b>. The goal of this approach was not to shed light on the inner workings of a black-box system or provide insight on its decision-making, but, instead, to identify and reveal which external factors would require changing in order for the desired <b>output</b> to be produced. The authors highlight that ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Group calibration is a byproduct of unconstrained learning</b> | DeepAI", "url": "https://deepai.org/publication/group-calibration-is-a-byproduct-of-unconstrained-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>group-calibration-is-a-byproduct-of-unconstrained-learning</b>", "snippet": "A risk score is <b>calibrated</b> for a group if the risk score obviates the need to solicit group membership for the purpose of predicting an outcome variable of interest. The concept of calibration has a venerable history in statistics and <b>machine</b> learning (Cox, 1958; Murphy and Winkler, 1977; Dawid, 1982; DeGroot and Fienberg, 1983; Platt, 1999; Zadrozny and Elkan, 2001; Niculescu-Mizil and Caruana, 2005).The appearance of calibration as a widely adopted and discussed \u201cfairness criterion ...", "dateLastCrawled": "2021-12-25T14:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>On Fairness and Calibration</b> - ResearchGate", "url": "https://www.researchgate.net/publication/319534462_On_Fairness_and_Calibration", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319534462_<b>On_Fairness_and_Calibration</b>", "snippet": "<b>calibrated</b> classi\ufb01er <b>can</b> perform \u201cworse than random\u201d (see Lemma 3 in Section S2). The only trivial The only trivial classi\ufb01er that satis\ufb01es the calibration condition for a group", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Fairness in <b>machine</b> learning <b>can</b> be categorized according to two dimensions, namely, the task and the type of learning. For the first dimension, there are two tasks in fairness-aware <b>machine</b> learning: discrimination discovery (or assessment) and discrimination removal (or prevention). Discrimination discovery task focuses on assessing and measuring bias in datasets or in predictions made by the MLDM. Discrimination removal focuses on preventing discrimination by manipulating datasets (pre ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Report on Algorithmic Risk Assessment Tools in</b> the U.S. Criminal ...", "url": "https://partnershiponai.org/paper/report-on-machine-learning-in-risk-assessment-tools-in-the-u-s-criminal-justice-system/", "isFamilyFriendly": true, "displayUrl": "https://partnershiponai.org/paper/report-on-<b>machine</b>-learning-in-risk-assessment-tools...", "snippet": "In a simple <b>machine</b> learning prediction model, the tool might simply <b>produce</b> an <b>output</b> <b>like</b> \u201c35% chance of recidivism.\u201d A bootstrapped tool uses many resampled versions of the training datasets to make different predictions, allowing an <b>output</b> <b>like</b>, \u201cIt is 80% likely that this individual\u2019s chance of recidivating is in the 20% \u2013 50% ...", "dateLastCrawled": "2022-02-03T04:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "If <b>so</b>, these revelations might lead to a bigger question: Do sentencing algorithms actually solve the problems they purport to address, or do they simply expose even deeper issues in American society? Part I of this Note provides a condensed history of the use of risk assessments in criminal justice and presents recent issues of racial bias in the application of algorithmic risk assessments at sentencing. Part II explores two different definitions of fairness in the controversy surrounding ...", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Attacking <b>discrimination with smarter machine learning</b> | Hacker News", "url": "https://news.ycombinator.com/item?id=13004790", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=13004790", "snippet": "First, it remedies the main conceptual shortcomings of <b>demographic</b> <b>parity</b> as a fairness notion. Second, it is fully aligned with the central goal of supervised <b>machine</b> learning, that is, to build higher accuracy classifiers.&quot; Your dispute may be with the authors. yummyfajitas on Nov 21, 2016. No, you are misunderstanding things. They aren&#39;t improving accuracy, reducing overfitting, or improving generalizability. If you think I&#39;m wrong, feel free to cite the part of the paper where they claim ...", "dateLastCrawled": "2021-04-23T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Where modern macroeconomics went wrong | Oxford Review of Economic ...", "url": "https://academic.oup.com/oxrep/article/34/1-2/70/4781816", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/oxrep/article/34/1-2/70/4781816", "snippet": "Pseudo-wealth is being created and destroyed all the time, but certain changes\u2014<b>like</b> the creation of new betting markets, e.g. associated with \u2018improvements\u2019 in finance, associated with the creation of markets in derivatives and CDSs\u2014<b>can</b> lead to significant increases in aggregate pseudo-wealth; and certain events, <b>like</b> the collapse of the housing bubble, <b>can</b> lead to its net destruction.", "dateLastCrawled": "2022-01-26T19:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Marketing Strategy and Competitive Positioning</b> (4th Edition) - SILO.PUB", "url": "https://silo.pub/marketing-strategy-and-competitive-positioning-4th-edition.html", "isFamilyFriendly": true, "displayUrl": "https://silo.pub/<b>marketing-strategy-and-competitive-positioning</b>-4th-edition.html", "snippet": "This in turn <b>can</b> lead to a virtuous circle of improvement as happy, motivated staff generate increasingly satisfied customers, <b>so</b> that organisational performance improves, and staff become more satisfied, etc. Similarly, the most effective route to achieving the profit and performance desires of supply chain partners is through market success. Heightened success through partnerships and alliances <b>can</b> serve to bond organisations together, creating more stability and predictability in the ...", "dateLastCrawled": "2022-02-01T21:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>An empirical characterization of fair machine learning</b> for clinical ...", "url": "https://www.sciencedirect.com/science/article/pii/S1532046420302495", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046420302495", "snippet": "A <b>similar</b> phenomenon is observed when considering the impact of unconditional penalties that target <b>demographic</b> <b>parity</b> on metrics that assess equal opportunity in that two criteria appear to coincide in some cases, such as for the model that predicts hospital mortality in the STARR cohort when race and ethnicity is considered the sensitive attribute (Supplementary Figs. D.2) and conflict in others, such when sex is considered to be the sensitive attribute for models that predict prolonged ...", "dateLastCrawled": "2021-10-26T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Addressing Fairness, Bias, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "When two separate <b>demographic</b> groups are being included in a <b>machine</b> learning diagnostic model, the difference between the two groups <b>can</b> be examined on an ROC curve, as shown in Figure 2. Due to biological differences, as well as difference in disease prevalence, each <b>demographic</b> group will have a separate ROC curve. As shown in the figure, it is impossible to tune the model and select an operating point that will perform optimally in both <b>demographic</b> groups. If a single diagnostic test ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Explainable AI: A Review of <b>Machine</b> Learning Interpretability Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "A counterfactual explanation of a prediction describes the smallest possible change that <b>can</b> be applied to the feature values, <b>so</b> that the <b>output</b> prediction <b>can</b> be changed to a desired predefined <b>output</b>. The goal of this approach was not to shed light on the inner workings of a black-box system or provide insight on its decision-making, but, instead, to identify and reveal which external factors would require changing in order for the desired <b>output</b> to be produced. The authors highlight that ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An Empirical Characterization of Fair <b>Machine</b> Learning For Clinical ...", "url": "https://www.arxiv-vanity.com/papers/2007.10306/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2007.10306", "snippet": "The use of <b>machine</b> learning to guide clinical decision making has the potential to worsen existing health disparities. Several recent works frame the problem as that of algorithmic fairness, a framework that has attracted considerable attention and criticism. However, the appropriateness of this framework is unclear due to both ethical as well as technical considerations, the latter of which include trade-offs between measures of fairness and model performance that are not well-understood ...", "dateLastCrawled": "2021-12-09T00:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Group calibration is a byproduct of unconstrained learning</b> | DeepAI", "url": "https://deepai.org/publication/group-calibration-is-a-byproduct-of-unconstrained-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>group-calibration-is-a-byproduct-of-unconstrained-learning</b>", "snippet": "A risk score is <b>calibrated</b> for a group if the risk score obviates the need to solicit group membership for the purpose of predicting an outcome variable of interest. The concept of calibration has a venerable history in statistics and <b>machine</b> learning (Cox, 1958; Murphy and Winkler, 1977; Dawid, 1982; DeGroot and Fienberg, 1983; Platt, 1999; Zadrozny and Elkan, 2001; Niculescu-Mizil and Caruana, 2005).The appearance of calibration as a widely adopted and discussed \u201cfairness criterion ...", "dateLastCrawled": "2021-12-25T14:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Attacking <b>discrimination with smarter machine learning</b> | Hacker News", "url": "https://news.ycombinator.com/item?id=13004790", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=13004790", "snippet": "First, it remedies the main conceptual shortcomings of <b>demographic</b> <b>parity</b> as a fairness notion. Second, it is fully aligned with the central goal of supervised <b>machine</b> learning, that is, to build higher accuracy classifiers.&quot; Your dispute may be with the authors. yummyfajitas on Nov 21, 2016. No, you are misunderstanding things. They aren&#39;t improving accuracy, reducing overfitting, or improving generalizability. If you think I&#39;m wrong, feel free to cite the part of the paper where they claim ...", "dateLastCrawled": "2021-04-23T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Publications</b>", "url": "http://www.stats.ox.ac.uk/~sejdinov/publications/", "isFamilyFriendly": true, "displayUrl": "www.stats.ox.ac.uk/~sejdinov/<b>publications</b>", "snippet": "Deep reinforcement learning is an emerging <b>machine</b>-learning approach that <b>can</b> teach a computer to learn from their actions and rewards <b>similar</b> to the way humans learn from experience. It offers many advantages in automating decision processes to navigate large parameter spaces. This paper proposes an approach to the efficient measurement of quantum devices based on deep reinforcement learning. We focus on double quantum dot devices, demonstrating the fully automatic identification of ...", "dateLastCrawled": "2022-01-29T17:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>The Oxford Handbook of Ethics</b> of AI: An Annotated ... - <b>Ethics in Context</b>", "url": "https://c4ejournal.net/the-oxford-handbook-of-ethics-of-ai-an-annotated-bibliography/", "isFamilyFriendly": true, "displayUrl": "https://c4ejournal.net/<b>the-oxford-handbook-of-ethics</b>-<b>of-ai-an-annotated-bibliography</b>", "snippet": "This book highlights how the pervasive employment of <b>machine</b>-learning technologies that inform <b>so</b>-called \u2018data-driven agency\u2019 threaten privacy, identity, autonomy, non-discrimination, due process and the presumption of innocence. The author argues that smart technologies undermine, reconfigure and overrule the ends of the law in a constitutional democracy, jeopardizing law as an instrument of justice, legal certainty and the public good. However, the author calls on lawyers, computer ...", "dateLastCrawled": "2022-02-02T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Marketing Strategy and Competitive Positioning</b> (4th Edition) - SILO.PUB", "url": "https://silo.pub/marketing-strategy-and-competitive-positioning-4th-edition.html", "isFamilyFriendly": true, "displayUrl": "https://silo.pub/<b>marketing-strategy-and-competitive-positioning</b>-4th-edition.html", "snippet": "This in turn <b>can</b> lead to a virtuous circle of improvement as happy, motivated staff generate increasingly satisfied customers, <b>so</b> that organisational performance improves, and staff become more satisfied, etc. Similarly, the most effective route to achieving the profit and performance desires of supply chain partners is through market success. Heightened success through partnerships and alliances <b>can</b> serve to bond organisations together, creating more stability and predictability in the ...", "dateLastCrawled": "2022-02-01T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What <b>Can</b> You <b>Do to Promote Ethical Behavior in Your</b> Firm? - essayZeus", "url": "https://www.essayzeus.com/2019/12/what-can-you-do-to-promote-ethical-behavior-in-your-firm/", "isFamilyFriendly": true, "displayUrl": "https://www.essayzeus.com/2019/12/what-<b>can</b>-you-<b>do-to-promote-ethical-behavior-in-your</b>-firm", "snippet": "Strategic Plan\u201d presents a <b>similar</b> theme as management has taken a new strategic direction and must make financing decisions that are cost effective, but also preserve financial flexibility going forward. \u201cKelly Solar\u201d concerns a start-up that needs new funds for investment, but already has a significant amount of debt on the books that needs to be renegotiated before new investors will find their investment to be attractive. The case, \u201cJC Penney Company\u201d, presents a large retail ...", "dateLastCrawled": "2022-02-01T06:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Addressing Fairness, Bias, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "When two separate <b>demographic</b> groups are being included in a <b>machine</b> learning diagnostic model, the difference between the two groups <b>can</b> be examined on an ROC curve, as shown in Figure 2. Due to biological differences, as well as difference in disease prevalence, each <b>demographic</b> group will have a separate ROC curve. As shown in the figure, it is impossible to tune the model and select an operating point that will perform optimally in both <b>demographic</b> groups. If a single diagnostic test ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Why fairness cannot be automated: Bridging the gap between EU non ...", "url": "https://www.sciencedirect.com/science/article/pii/S0267364921000406", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0267364921000406", "snippet": "One final question is whether the tests for conditional <b>demographic</b> <b>parity</b> must always be performed over all sets of attributes, and aggregated as discussed, or if they <b>can</b> ever be restricted to a single choice of attribute. According to best practices in statistics, the answer is again contextual. Where, prior to looking at statistical information, convincing information exists that a system is likely to exhibit a bias towards a protected group, this group should be tested for bias in ...", "dateLastCrawled": "2021-12-27T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>On Fairness and Calibration</b> - ResearchGate", "url": "https://www.researchgate.net/publication/319534462_On_Fairness_and_Calibration", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319534462_<b>On_Fairness_and_Calibration</b>", "snippet": "<b>calibrated</b> classi\ufb01er <b>can</b> perform \u201cworse than random\u201d (see Lemma 3 in Section S2). The only trivial The only trivial classi\ufb01er that satis\ufb01es the calibration condition for a group", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Methods of Technology Assessment - Assessing Medical Technologies ...", "url": "https://www.ncbi.nlm.nih.gov/books/NBK217473/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/books/NBK217473", "snippet": "As Chapter 1 indicates, technology assessment offers the essential bridge between basic research and development and prudent practical application of medical technology. We have a substantial body of methods that <b>can</b> be applied to the various tasks of assessment, and their availability makes possible the acceptance, modification, or rejection of new technologies on a largely rational basis. That rationality, however, depends on many factors that go well beyond safety and efficacy, including ...", "dateLastCrawled": "2022-01-24T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>The Oxford Handbook of Ethics</b> of AI: An Annotated ... - <b>Ethics in Context</b>", "url": "https://c4ejournal.net/the-oxford-handbook-of-ethics-of-ai-an-annotated-bibliography/", "isFamilyFriendly": true, "displayUrl": "https://c4ejournal.net/<b>the-oxford-handbook-of-ethics</b>-<b>of-ai-an-annotated-bibliography</b>", "snippet": "The article argues that important issues concerning <b>Machine</b> Learning (ML) decision models <b>can</b> be unveiled without detailed knowledge about the learning algorithm, empowering non-ML experts and stakeholders in debates over if, and how to, include them, for example, in the form of predictive policing. Non-ML experts <b>can</b>, and should, review ML models. We provide a \u2018toolbox\u2019 of questions about three elements of a decision model that <b>can</b> be fruitfully scrutinized by non-ML experts: the ...", "dateLastCrawled": "2022-02-02T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "If <b>so</b>, these revelations might lead to a bigger question: Do sentencing algorithms actually solve the problems they purport to address, or do they simply expose even deeper issues in American society? Part I of this Note provides a condensed history of the use of risk assessments in criminal justice and presents recent issues of racial bias in the application of algorithmic risk assessments at sentencing. Part II explores two different definitions of fairness in the controversy surrounding ...", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fairness Through Awareness</b> | Request PDF", "url": "https://www.researchgate.net/publication/51957869_Fairness_Through_Awareness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/51957869_<b>Fairness_Through_Awareness</b>", "snippet": "Fairness has become a focus of attention in <b>machine</b> learning techniques that have been integrated into the decision-making process, such as loan screening, release decisions, and online search. In ...", "dateLastCrawled": "2021-11-14T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Perception of fairness in algorithmic decisions: Future developers ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666389921002476", "snippet": "The use of algorithmic decision-making has prospects to make decision-making more efficient and reliable. 7, 8 It is no longer a worry, but rather a reality, that algorithms are making non-\u201cobjective\u201d decisions that may reproduce and/or amplify social stereotypes and inequalities. 9 This type of behavior <b>can</b> be witnessed in many domains: gender discrimination has been detected in resume search engines; 10 auto-complete search terms <b>can</b> <b>produce</b> suggested terms that could be viewed as ...", "dateLastCrawled": "2022-01-30T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Where modern macroeconomics went wrong | Oxford Review of Economic ...", "url": "https://academic.oup.com/oxrep/article/34/1-2/70/4781816", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/oxrep/article/34/1-2/70/4781816", "snippet": "For example, matching the variance of <b>output</b> during the 2000s does not generally imply that a model is a good description of <b>output</b> dynamics over the decade. Third, for a given set of moments, there is no well-defined statistic to measure the goodness of fit of a DSGE model or to establish what constitutes an improvement in such a framework. Whether the moments generated by the model satisfactorily match the moments observed in the real world is often determined by an eyeball comparison and ...", "dateLastCrawled": "2022-01-26T19:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GV441</b> Flashcards - <b>Quizlet</b>", "url": "https://quizlet.com/176597982/gv441-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/176597982/<b>gv441</b>-flash-cards", "snippet": "posits that the presence of &quot;correctly <b>calibrated</b>&quot; sub-systems (i.e., financial system, labour market, training system, and inter-firm relations) increases the performance, or the <b>so</b>-called &quot;comparative institutional advantage&quot; of the firm. Taken from the economic concept of comparative advantage in trade, the basic idea is that the institutional structure of particular political economy provides firms with advantages for engaging in specific types of activities. The presence of comparative ...", "dateLastCrawled": "2021-12-24T04:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Addressing Fairness, Bias, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "When two separate <b>demographic</b> groups are being included in a <b>machine</b> learning diagnostic model, the difference between the two groups <b>can</b> be examined on an ROC curve, as shown in Figure 2. Due to biological differences, as well as difference in disease prevalence, each <b>demographic</b> group will have a separate ROC curve. As shown in the figure, it is impossible to tune the model and select an operating point that will perform optimally in both <b>demographic</b> groups. If a single diagnostic test ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Explainable AI: A Review of <b>Machine</b> Learning Interpretability Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "A counterfactual explanation of a prediction describes the smallest possible change that <b>can</b> be applied to the feature values, <b>so</b> that the <b>output</b> prediction <b>can</b> be changed to a desired predefined <b>output</b>. The goal of this approach was not to shed light on the inner workings of a black-box system or provide insight on its decision-making, but, instead, to identify and reveal which external factors would require changing in order for the desired <b>output</b> to be produced. The authors highlight that ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Parity</b>-based Cumulative Fairness-aware Boosting | DeepAI", "url": "https://deepai.org/publication/parity-based-cumulative-fairness-aware-boosting", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>parity</b>-based-cumulative-fairness-aware-boosting", "snippet": "Our contributions are summarized as follows: i) we propose AdaFair, a fairness-aware boosting method, that achieves <b>parity</b> between the protected and non-protected groups (thus achieving fairness) while maintaining good predictive performance for all classes (thus tackling class-imbalance). ii) We define the notion of cumulative fairness for the ensemble for three different <b>parity</b>-based fairness notions: statistical <b>parity</b>, equal opportunity, and disparate mistreatment.", "dateLastCrawled": "2022-02-02T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>An empirical characterization of fair machine learning</b> for clinical ...", "url": "https://www.sciencedirect.com/science/article/pii/S1532046420302495", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046420302495", "snippet": "Some researchers argue that techniques from algorithmic fairness still have a role to play in promoting health equity , particularly if their use is informed by a deeply embedded understanding of the sociotechnical and clinical contexts that surround the use of a predictive model\u2019s <b>output</b>.From this perspective, it is relevant to reason about the trade-offs between properties such as model performance and fairness criteria satisfaction, as long as those properties <b>can</b> be appropriately ...", "dateLastCrawled": "2021-10-26T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Predicting youth at high risk of aging out of foster care using <b>machine</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0145213421001320", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0145213421001320", "snippet": "This study offers initial evidence to suggest that an <b>machine</b> learning approach <b>can</b> provide a fair to good classification of high risk youth, years before they exit foster care without permanency. If this is true, that may mean that the use of such an approach could enhance the efficacy of services provided to improve permanency outcomes and youth transitions into adulthood. With the aid of algorithmic decision-making systems, CWS providers could proactively reach out to youth in foster care ...", "dateLastCrawled": "2021-12-25T12:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An Empirical Characterization of Fair <b>Machine</b> Learning For Clinical ...", "url": "https://www.arxiv-vanity.com/papers/2007.10306/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2007.10306", "snippet": "The use of <b>machine</b> learning to guide clinical decision making has the potential to worsen existing health disparities. Several recent works frame the problem as that of algorithmic fairness, a framework that has attracted considerable attention and criticism. However, the appropriateness of this framework is unclear due to both ethical as well as technical considerations, the latter of which include trade-offs between measures of fairness and model performance that are not well-understood ...", "dateLastCrawled": "2021-12-09T00:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>On Fairness and Calibration</b> - ResearchGate", "url": "https://www.researchgate.net/publication/319534462_On_Fairness_and_Calibration", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319534462_<b>On_Fairness_and_Calibration</b>", "snippet": "<b>calibrated</b> classi\ufb01er <b>can</b> perform \u201cworse than random\u201d (see Lemma 3 in Section S2). The only trivial The only trivial classi\ufb01er that satis\ufb01es the calibration condition for a group", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Equality of Opportunity in Supervised Learning</b>", "url": "https://www.researchgate.net/publication/308980568_Equality_of_Opportunity_in_Supervised_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/308980568_Equality_of_Opportunity_in...", "snippet": "Applying standard <b>machine</b> learning approaches for classification <b>can</b> <b>produce</b> unequal results across different <b>demographic</b> groups. When then used in real-world settings, these inequities <b>can</b> have ...", "dateLastCrawled": "2022-01-15T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "If <b>so</b>, these revelations might lead to a bigger question: Do sentencing algorithms actually solve the problems they purport to address, or do they simply expose even deeper issues in American society? Part I of this Note provides a condensed history of the use of risk assessments in criminal justice and presents recent issues of racial bias in the application of algorithmic risk assessments at sentencing. Part II explores two different definitions of fairness in the controversy surrounding ...", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Marketing Strategy and Competitive Positioning</b> (4th Edition) - SILO.PUB", "url": "https://silo.pub/marketing-strategy-and-competitive-positioning-4th-edition.html", "isFamilyFriendly": true, "displayUrl": "https://silo.pub/<b>marketing-strategy-and-competitive-positioning</b>-4th-edition.html", "snippet": "This in turn <b>can</b> lead to a virtuous circle of improvement as happy, motivated staff generate increasingly satisfied customers, <b>so</b> that organisational performance improves, and staff become more satisfied, etc. Similarly, the most effective route to achieving the profit and performance desires of supply chain partners is through market success. Heightened success through partnerships and alliances <b>can</b> serve to bond organisations together, creating more stability and predictability in the ...", "dateLastCrawled": "2022-02-01T21:45:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Pandas <b>Machine</b> <b>Learning</b> Example", "url": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "snippet": "Regardless of your dataset, <b>demographic</b> <b>parity</b> is a <b>machine</b> <b>learning</b> algorithms. Data Munging It helps us to missing data of wedge form with another. Python with datetime module, i should equal to bring new example <b>machine</b>. Quite possibly the state important part clean the <b>machine</b> <b>learning</b> process is understanding the data you are working with and advantage it relates to reflect task you front to solve. Viewing the corresponding number of dropping down arrow illustrates that are not only all ...", "dateLastCrawled": "2022-01-24T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An example of prediction which complies with <b>Demographic</b> <b>Parity</b> and ...", "url": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-demographic-parity-and-equalizes-group-wise-risks-in-the-context", "isFamilyFriendly": true, "displayUrl": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-<b>demographic</b>...", "snippet": "However, <b>Demographic</b> <b>Parity</b> and EGWR only define fairness on the group level and inspecting the individual level reveals a critical flow of this prediction rule. We have constrained our predictors to those that do not produce Disparate Treatment by prohibiting them from having the sensitive variable as direct input. Nevertheless, enforcing group level fairness constraints (such as DP and EGWR) forces the prediction rule to guess the sensitive attribute corresponding to a given feature vector", "dateLastCrawled": "2022-02-05T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Heidari et al. have written a paper comparing the three criteria \u2013 <b>demographic</b> <b>parity</b>, equality of opportunity, and predictive <b>parity</b> \u2013 to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the <b>analogy</b> is fascinating, it too assumes that we may take what is in the data at face value. In their likening predictive <b>parity</b> to luck egalitarianism, they have to go to especially great lengths, in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Mitigating Unwanted Biases with Adversarial <b>Learning</b> - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/1801.07593/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1801.07593", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concerning <b>demographic</b> groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or income ...", "dateLastCrawled": "2021-10-04T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mitigating Unwanted Biases with Adversarial <b>Learning</b>", "url": "http://www.m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf", "isFamilyFriendly": true, "displayUrl": "www.m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concern-ing <b>demographic</b> groups are in the training data, well-trained models will re\ufb02ect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an ad-versary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or in ...", "dateLastCrawled": "2022-01-23T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "This includes measures such as <b>Demographic</b> <b>Parity</b> / Statistical <b>Parity</b> (Dwork et al., 2012), Equalized Odds Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are trying to test for equivalence. In another survey of fairness definitions, Verma &amp; Rubin (2018) listed 20 definitions of fairness, 13 belonging to ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The <b>measure and mismeasure of fairness: a critical review</b> of fair ...", "url": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness", "snippet": "In case you\u2019re wondering where on earth I\u2019m going with this\u2026 it\u2019s a very stretched <b>analogy</b> I\u2019ve been playing with in my mind. One premise of many models of fairness in <b>machine</b> <b>learning</b> is that you can measure (\u2018prove\u2019) fairness of a <b>machine</b> <b>learning</b> model from within the system \u2013 i.e. from properties of the model itself and perhaps the data it is trained on. Beyond the questions of whether any one model of fairness is better or worse than another, I\u2019m coming to the ...", "dateLastCrawled": "2022-01-30T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Fighting Money Laundering with Statistics and <b>Machine</b> <b>Learning</b>: An ...", "url": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-machine-learning-an-introduction-and-review", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-<b>machine</b>...", "snippet": "Statistics and <b>machine</b> <b>learning</b> have long promised efficient and robust techniques for AML. So far, though, these remain to manifest [Grint2001]. One reason is that the academic literature is small and fragmented [Leite2019, Ngai2011]. With this paper, we aim to do three things. First, we propose a unified terminology to homogenize and associate research methodologies. Second, we review selected, exemplary methods. Third, we present recent <b>machine</b> <b>learning</b> concepts that have the potential to ...", "dateLastCrawled": "2022-01-28T21:12:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(demographic parity)  is like +(a machine that is calibrated so it can produce identical output)", "+(demographic parity) is similar to +(a machine that is calibrated so it can produce identical output)", "+(demographic parity) can be thought of as +(a machine that is calibrated so it can produce identical output)", "+(demographic parity) can be compared to +(a machine that is calibrated so it can produce identical output)", "machine learning +(demographic parity AND analogy)", "machine learning +(\"demographic parity is like\")", "machine learning +(\"demographic parity is similar\")", "machine learning +(\"just as demographic parity\")", "machine learning +(\"demographic parity can be thought of as\")", "machine learning +(\"demographic parity can be compared to\")"]}