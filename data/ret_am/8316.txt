{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Pre-Trained</b> Models: Past, Present and Future - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "snippet": "Large-scale <b>pre-trained</b> models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge <b>model</b> parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of ...", "dateLastCrawled": "2022-01-27T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Lessons Learned from Deploying a <b>PreTrained</b> BERT <b>Model</b> as a Web Site ...", "url": "https://cosmosinyou.medium.com/lessons-learned-from-deploying-a-pretrained-bert-model-as-a-web-site-88a9d967c751", "isFamilyFriendly": true, "displayUrl": "https://cosmosinyou.medium.com/lessons-learned-from-deploying-a-<b>pretrained</b>-bert-<b>model</b>...", "snippet": "This <b>pre-trained</b> <b>model</b> is known as SciBERT [6]. McKillen-Godfried (MG) explores SciBERT\u2019s performance for recommending titles of <b>academic</b> papers based on your phrase-based inquiry. To briefly explain the concept of \u2018training\u2019 in an NLP context, ANNs and ML are based on a form of technology and essentially learns from its environment \u2014 in our case <b>academic</b> papers written by people are the environment and the tech in the form of SciBERT will read many articles to find patterns and ...", "dateLastCrawled": "2022-02-03T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Pre-Trained</b> Models: Past, Present and Future \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2106.07139/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.07139", "snippet": "Large-scale <b>pre-trained</b> models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge <b>model</b> parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of ...", "dateLastCrawled": "2022-01-08T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Boosting the Accuracy of an Image Classifier - <b>Academic</b> Blog", "url": "https://enotebooks.org/2021/12/13/boosting-the-accuracy-of-an-image-classifier/", "isFamilyFriendly": true, "displayUrl": "https://enotebooks.org/2021/12/13/boosting-the-accuracy-of-an-image-classifier", "snippet": "PyTorch makes it easy to load <b>pre-trained</b> models and build on them. Some of the most popular <b>pre-trained</b> models, <b>like</b> ResNet, AlexNet, and VGG, come from the ImageNet Challenge. These <b>pre-trained</b> models allow others to quickly obtain cutting-edge results in computer vision without needing such large amounts of computer power, patience, and time. We actually had great results with DenseNet and decided to use DenseNet161, which gave us very", "dateLastCrawled": "2022-01-16T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "2020b a new sequence to sequence <b>pre trained</b> <b>model</b> which is <b>pre trained</b> ...", "url": "https://www.coursehero.com/file/p5ht2fk9o/2020b-a-new-sequence-to-sequence-pre-trained-model-which-is-pre-trained-by/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/p5ht2fk9o/2020b-a-new-sequence-to-sequence-<b>pre-trained</b>...", "snippet": "BANG is a <b>pre-trained</b> <b>model</b>, which bridges the gap between AR and NAR. MIST is a training method to adopt encoder based models in NAR. One advan- tage of MIST is that it can directly adopt <b>pre-trained</b> models without modification in <b>model</b> architectures, which saves the large computational resources in pre-training and allows it to use existing <b>pre-trained</b> models more flexibly.", "dateLastCrawled": "2022-01-14T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>AI/ML Models 101: What Is</b> a <b>Model</b>? - <b>OspreyData</b>", "url": "https://ospreydata.com/ai-ml-models-101-what-is-a-model/", "isFamilyFriendly": true, "displayUrl": "https://<b>ospreydata</b>.com/<b>ai-ml-models-101-what-is</b>-a-<b>model</b>", "snippet": "<b>AI/ML Models 101: What Is</b> a <b>Model</b>? In AI/ML, a <b>model</b> replicates a decision process to enable automation and understanding. AI/ML models are mathematical algorithms that are \u201ctrained\u201d using data and human <b>expert</b> input to replicate a decision an <b>expert</b> would make when provided that same information. Ideally, the <b>model</b> should also reveal the rationale behind its decision to help interpret the decision process (though frequently that is challenging). Most often, the training processes a ...", "dateLastCrawled": "2022-01-26T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Answered: Which of the following is a CORRECT\u2026 | bartleby", "url": "https://www.bartleby.com/questions-and-answers/which-of-the-following-is-a-correct-description-of-how-transfer-learning-is-applied-in-image-process/9d1d8cad-a74c-4aa3-930c-f47c2c0125de", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bartleby.com</b>/questions-and-answers/which-of-the-following-is-a-correct...", "snippet": "A <b>pre-trained</b> set of weights is used for the top/flat layer section of the network, but custom trained filters are used for the convolutional part of the <b>model</b> C. A <b>pre-trained</b> base <b>model</b> is used to generate predictions for the new image dataset, and those predictions are manually mapped to the new dataset&#39;s target labels D. All <b>model</b> weights ...", "dateLastCrawled": "2022-02-01T11:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "CAN-SIN: A <b>Cross-Layer Heterogeneous Academic Network with Semantic</b> ...", "url": "http://www.apsipa.org/proceedings/2020/pdfs/0001617.pdf", "isFamilyFriendly": true, "displayUrl": "www.apsipa.org/proceedings/2020/pdfs/0001617.pdf", "snippet": "systems <b>like</b> e-commerce (Amazon, Taobao) and social media (Facebook, Twitter, Weibo) platform. In this paper, we focus on <b>academic</b> networks, which is of crucial signicance. By analyzing the <b>academic</b> networks, we can provide paper recommendations to scholars, explore the cooperation mode of different <b>academic</b> teams, and help journals to analyze the conicts of interest between scholars. With the recent trend of graph convolutional networks (GCNs) [ 6], there are several attempts to combine ...", "dateLastCrawled": "2022-01-29T04:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Are There Any <b>Pre-Trained</b> Models <b>for Multi-Class Sentiment Analysis in</b> ...", "url": "https://www.reddit.com/r/LanguageTechnology/comments/ibga1s/are_there_any_pretrained_models_for_multiclass/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/ibga1s/are_there_any_<b>pretrained</b>_<b>models</b>_for_multiclass", "snippet": "Some of the features of mlconjug3 are the following: Easy to use API. Includes <b>pre-trained</b> language models with 99% + accuracy in predicting conjugation class of unknown verbs. Easily train new models or add new languages. Easily integrate MLConjug in your own projects.", "dateLastCrawled": "2021-04-27T11:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "GitHub - deaconspider/Wav2Lip-1: This repository contains the codes of ...", "url": "https://github.com/deaconspider/Wav2Lip-1", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/deaconspider/Wav2Lip-1", "snippet": "Wav2Lip: Accurately Lip-syncing Videos In The Wild. For commercial requests, please contact us at radrabha.m@research.iiit.ac.in or prajwal.k@research.iiit.ac.in.We have an HD <b>model</b> ready that can be used commercially. This code is part of the paper: A Lip Sync <b>Expert</b> Is All You Need for Speech to Lip Generation In the Wild published at ACM Multimedia 2020.", "dateLastCrawled": "2022-01-19T16:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Pre-Trained</b> Models: Past, Present and Future \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2106.07139/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.07139", "snippet": "DALLE is the very first transformer-based text-to-image zero-shot <b>pre-trained</b> <b>model</b> with around 10 billiion parameters. It shows the potential of multi-modal <b>pre-trained</b> models to bridge the gap between text descriptions and image generation, especially the excellent ability in combining different objects, such as \u201can armchair in the shape of an avocado&quot;. CogView improves the numerical precision and training stability by introducing sandwich transformer and sparse attention mechanism, and ...", "dateLastCrawled": "2022-01-08T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Frontiers | Graph Embedding for Scholar Recommendation in <b>Academic</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fphy.2021.768006/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fphy.2021.768006", "snippet": "Then, we extract the authors\u2019 <b>academic</b> attributes by the glove.6B.200d 4 <b>pre-trained</b> word vector, which is obtained from the English corpus using the GloVe <b>model</b> . In order to ensure that the relationship network is not too sparse, we also only select scholars with more than ten friends. Overall, we collect 14,279 scholars and 446,685 co-authorship relations between them. We also extract 6,221 <b>academic</b> attributes and 81,088 textual features for the scholars.", "dateLastCrawled": "2021-12-23T14:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Wu Dao 2.0: China\u2019s Answer To GPT-3. Only Better", "url": "https://analyticsindiamag.com/wu-dao-2-0-chinas-answer-to-gpt-3-only-better/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/wu-dao-2-0-chinas-answer-to-gpt-3-only-better", "snippet": "The Wu Dao 2.0 is a <b>pre-trained</b> AI <b>model</b> that uses 1.75 trillion parameters to simulate conversational speech, writes poems, understand pictures and even generate recipes. The next generation Wu Dao <b>model</b> can also predict the 3D structures of proteins, <b>similar</b> to DeepMind\u2019s AlphaFold and power virtual idols. Recently, China\u2019s first virtual student, Hua Zhibing, was built on Wu Dao 2.0. The language <b>model</b> Wu Dao 2.0 was trained with FastMoE, a Fast Mixture-of-<b>Expert</b> (MoE) training system ...", "dateLastCrawled": "2022-02-03T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "On the relationship between <b>similar</b> requirements and <b>similar</b> software ...", "url": "https://link.springer.com/article/10.1007%2Fs00766-021-00370-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00766-021-00370-4", "snippet": "DW the <b>pre-trained</b> Doc2Vec <b>model</b> available in Gensim data Footnote 3. is used. The <b>model</b> has a vector size of 300, with a minimum frequency set to 2. The <b>model</b> is trained on the English Wikipedia documents resulting in a vocabulary size of 35,556,952. FT we use the <b>pre-trained</b> FT <b>model</b> available in Gensim data. The <b>model</b> has a vector size of ...", "dateLastCrawled": "2022-02-02T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "gensim: <b>Word2Vec</b> <b>Model</b>", "url": "https://radimrehurek.com/gensim_3.8.3/auto_examples/tutorials/run_word2vec.html", "isFamilyFriendly": true, "displayUrl": "https://radimrehurek.com/gensim_3.8.3/auto_examples/tutorials/run_<b>word2vec</b>.html", "snippet": "Continuous-bag-of-words <b>Word2vec</b> is very <b>similar</b> to the skip-gram <b>model</b>. It is also a 1-hidden-layer neural network. The synthetic training task now uses the average of multiple input context words, rather than a single word as in skip-gram, to predict the center word. Again, the projection weights that turn one-hot words into averageable vectors, of the same width as the hidden layer, are interpreted as the word embeddings. <b>Word2Vec</b> Demo\u00b6 To see what <b>Word2Vec</b> can do, let\u2019s download a pre ...", "dateLastCrawled": "2022-01-29T21:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "GitHub - qiyuqianxai/wav2lip", "url": "https://github.com/qiyuqianxai/wav2lip", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/qiyuqianxai/wav2lip", "snippet": "Lip-syncing videos using the <b>pre-trained</b> models (Inference) You can lip-sync any video to any audio: python inference.py --checkpoint_path &lt; ckpt &gt;--face &lt; video.mp 4&gt;--audio &lt; an-audio-source &gt; The result is saved (by default) in results/result_voice.mp4. You can specify it as an argument, <b>similar</b> to several other available options. The audio source can be any file supported by FFMPEG containing audio data: *.wav, *.mp3 or even a video file, from which the code will automatically extract ...", "dateLastCrawled": "2021-11-04T03:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "GitHub - deaconspider/Wav2Lip-1: This repository contains the codes of ...", "url": "https://github.com/deaconspider/Wav2Lip-1", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/deaconspider/Wav2Lip-1", "snippet": "Lip-syncing videos using the <b>pre-trained</b> models (Inference) You can lip-sync any video to any audio: python inference.py --checkpoint_path &lt; ckpt &gt;--face &lt; video.mp 4&gt;--audio &lt; an-audio-source &gt; The result is saved (by default) in results/result_voice.mp4. You can specify it as an argument, <b>similar</b> to several other available options. The audio source can be any file supported by FFMPEG containing audio data: *.wav, *.mp3 or even a video file, from which the code will automatically extract ...", "dateLastCrawled": "2022-01-19T16:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Are There Any <b>Pre-Trained</b> Models <b>for Multi-Class Sentiment Analysis in</b> ...", "url": "https://www.reddit.com/r/LanguageTechnology/comments/ibga1s/are_there_any_pretrained_models_for_multiclass/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/ibga1s/are_there_any_<b>pretrained</b>_<b>models</b>_for_multiclass", "snippet": "It has also been used in several <b>Academic</b> publications, for example: &quot;Generative Grading: ... Information: For my dissertation project I fine-tuned a <b>pre-trained</b> language <b>model</b> on a self-mined dataset of &quot;left&quot; and &quot;right&quot; leaning subreddits to classify comments and subreddit&#39;s. I mined the data over a few months using praw, I used a list of around 20-25 different subreddits taking between 10-20,000 comments from each from within the past year, so the <b>model</b> is quite American election biased ...", "dateLastCrawled": "2021-04-27T11:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Question about transfer learning using the YOLOv5 COCO <b>pre-trained</b> <b>model</b>.", "url": "https://www.reddit.com/r/learnmachinelearning/comments/q0914r/question_about_transfer_learning_using_the_yolov5/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/learnmachinelearning/comments/q0914r/question_about_transfer...", "snippet": "Question about transfer learning using the YOLOv5 COCO <b>pre-trained</b> <b>model</b>. Close. 2. Posted by 3 months ago. Question about transfer learning using the YOLOv5 COCO <b>pre-trained</b> <b>model</b>. I recently used yolov5 to recognize crosswalks, and passed the yolov5s.pt file in using the --weights argument of train.py. After this training on crosswalks, best.pt was outputted. The YAML file for my <b>model</b> only contained the class &quot;crosswalk&quot;, even though yolov5s.pt was trained on COCO, which contains an ...", "dateLastCrawled": "2021-12-21T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Modeling multi-prototype Chinese word representation learning for word ...", "url": "https://link.springer.com/article/10.1007/s40747-021-00482-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40747-021-00482-y", "snippet": "Our method can revise the representations of <b>pre-trained</b> words through <b>similar</b> words or related words in synonym base to achieve a much more accurate word representation for each concept of each word. The final experimental results show that our method outperforms the methods listed previously in word similarity evaluation. A multi-prototype Chinese word representation <b>Model</b>. Overview. Word semantic representation has become an increasingly important problem in the field of natural language ...", "dateLastCrawled": "2022-01-30T11:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Lessons Learned from Deploying a <b>PreTrained</b> BERT <b>Model</b> as a Web Site ...", "url": "https://cosmosinyou.medium.com/lessons-learned-from-deploying-a-pretrained-bert-model-as-a-web-site-88a9d967c751", "isFamilyFriendly": true, "displayUrl": "https://cosmosinyou.medium.com/lessons-learned-from-deploying-a-<b>pretrained</b>-bert-<b>model</b>...", "snippet": "This <b>pre-trained</b> <b>model</b> is known as SciBERT [6]. McKillen-Godfried (MG) explores SciBERT\u2019s performance for recommending titles of <b>academic</b> papers based on your phrase-based inquiry. To briefly explain the concept of \u2018training\u2019 in an NLP context, ANNs and ML are based on a form of technology and essentially learns from its environment \u2014 in our case <b>academic</b> papers written by people are the environment and the tech in the form of SciBERT will read many articles to find patterns and ...", "dateLastCrawled": "2022-02-03T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Pre-trained</b> Models for <b>Natural Language Processing</b>: A Survey \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2003.08271/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2003.08271", "snippet": "Knowledge distillation (KD) [57] is a compression technique in which a small <b>model</b> called student <b>model</b> is trained to reproduce the behaviors of a large <b>model</b> called teacher <b>model</b>. Here the teacher <b>model</b> <b>can</b> be an ensemble of many models and usually well <b>pre-trained</b>. Different to <b>model</b> compression, distillation techniques learn a small student <b>model</b> from a fixed teacher <b>model</b> through some optimization objectives, while compression techniques aiming at searching a sparser architecture.", "dateLastCrawled": "2022-02-02T23:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Pre-trained</b> Models for Natural Language Processing A Survey.pdf ...", "url": "https://www.coursehero.com/file/127027612/Pre-trained-Models-for-Natural-Language-Processing-A-Surveypdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/127027612/<b>Pre-trained</b>-<b>Models</b>-for-Natural-Language...", "snippet": "QIU XP, et al. <b>Pre-trained</b> Models for Natural Language Processing: A Survey March (2020) 3 h 1 h 2 h 3 h 4 h 5 x 1 x 2 x 3 x 4 x 5 (a) Convolutional <b>Model</b> h 1 h 2 h 3 h 4 h 5 x 1 x 2 x 3 x 4 x 5 (b) Recurrent <b>Model</b> h 1 h 2 h 3 h 4 h 5 x 1 x 2 x 3 x 4 x 5 (c) Fully-Connected Self-Attention <b>Model</b> Figure 2: Neural Contextual Encoders bedding of token x t because of the contextual information included in. 2.2 Neural Contextual Encoders Most of the neural contextual encoders <b>can</b> be classified ...", "dateLastCrawled": "2022-01-26T04:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>pre-trained</b>-language-models-part-I.pdf - Language Models Niranjan ...", "url": "https://www.coursehero.com/file/118669752/pre-trained-language-models-part-Ipdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/118669752/<b>pre-trained</b>-language-<b>models</b>-part-Ipdf", "snippet": "View <b>pre-trained</b>-language-models-part-I.pdf from CSE 538 at Stony Brook University. Language Models Niranjan Balasubramanian Slides based on papers referenced herein. An NLP System in the", "dateLastCrawled": "2021-12-30T19:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Leap-Of-<b>Thought</b>: Teaching <b>Pre-Trained</b> Models to Systematically Reason ...", "url": "https://www.aminer.org/pub/5ee3527191e011cb3bff76d9/leap-of-thought-teaching-pre-trained-models-to-systematically-reason-over-implicit", "isFamilyFriendly": true, "displayUrl": "https://www.aminer.org/pub/5ee3527191e011cb3bff76d9/leap-of-<b>thought</b>-teaching-pre...", "snippet": "The authors show that <b>pre-trained</b> LMs <b>can</b> be trained to consistently combine implicit knowledge encoded in their parameters with explicit rules and facts. Users <b>can</b> teach the <b>model</b> facts and rules about the world through natural language statements, and the <b>model</b> will utilize this new information immediately, combining it with the knowledge encoded internally.", "dateLastCrawled": "2022-01-17T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Ten Techniques Learned From fast.ai - FloydHub Blog", "url": "https://blog.floydhub.com/ten-techniques-from-fast-ai/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/ten-techniques-from-fast-ai", "snippet": "Just as using <b>pre-trained</b> models has proven immensely effective in computer vision, it is becoming increasingly clear that natural language processing (NLP) models <b>can</b> benefit from doing the same. In the 4th lesson of fast.ai, Jeremy Howard builds a <b>model</b> to determine if IMDB reviews are positive or negative using transfer learning.", "dateLastCrawled": "2022-01-30T07:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Modeling Public Sentiments About JUUL Flavors on ... - <b>academic</b>.oup.com", "url": "https://academic.oup.com/ntr/article/23/11/1869/6276222", "isFamilyFriendly": true, "displayUrl": "https://<b>academic</b>.oup.com/ntr/article/23/11/1869/6276222", "snippet": "The <b>pre-trained</b> BERT <b>model</b> <b>can</b> be fine-tuned in a supervised fashion using a small amount of labeled training data to perform the particular supervised task. Fine-tuning of BERT is generally performed using a small labeled set for the task in hand, such as sentiment analysis. BERT <b>can</b> be fine-tuned on a small set of sample, due to the heavy pre-training using billions of words that helped the <b>model</b> to learn a strong language representation. This is unlike previous methods which suffered from ...", "dateLastCrawled": "2022-01-27T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[D] Open source <b>pre-trained</b> deep learning <b>model</b> for audio source ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7neui9/d_open_source_pretrained_deep_learning_model_for/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../7neui9/d_open_source_<b>pretrained</b>_deep_learning_<b>model</b>_for", "snippet": "Say I have a music audio file and want to remove everything but the vocal part of the audio (cocktail party problem) Is there good <b>pre-trained</b> deep learning <b>model</b> that I <b>can</b> download and use directly? P.S. I just want to take a youtube video, and remove everything but the voice of the singer. Using Audacity to do so is not good enough.", "dateLastCrawled": "2021-08-12T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Automatic <b>fish species classification in underwater</b> ... - Oxford <b>Academic</b>", "url": "https://academic.oup.com/icesjms/article/75/1/374/3924506", "isFamilyFriendly": true, "displayUrl": "https://<b>academic</b>.oup.com/icesjms/article/75/1/374/3924506", "snippet": "Any <b>pre-trained</b> CNN <b>model</b> <b>can</b> be plugged into the proposed cross-layer pooling framework. We expect better, even more complex models to emerge with time, which <b>can</b> replace the current models to achieve further improvement in the recognition rate. There must be a point of diminishing returns, at which more complexity results in no significant improvement, but at present a larger number of layers does improve the accuracy of classification.", "dateLastCrawled": "2022-02-02T15:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "We Need to Talk About <b>Sentiment</b> Analysis | by Kristof Boghe | The ...", "url": "https://medium.com/swlh/we-need-to-talk-about-sentiment-analysis-9d1f20f2ebfb", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/we-need-to-talk-about-<b>sentiment</b>-analysis-9d1f20f2ebfb", "snippet": "Testing the Pattern <b>model</b>. As you <b>can</b> see, the <b>model</b> does a reasonably good job. Bear in mind that the output represents the average polarity score going from -1 (very negative) to 1 (very positive).", "dateLastCrawled": "2022-01-31T02:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Pre-Trained</b> Models: Past, Present and Future - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "snippet": "As <b>compared</b> to RNNs, Transformer is an encoder-decoder structure that applies a self-attention mechanism, which <b>can</b> <b>model</b> correlations between all words of the input sequence in parallel. Hence, owing to the parallel computation of the self-attention mechanism, Transformer could fully take advantage of advanced computing devices to train large-scale models. In both the encoding and decoding phases of Transformer, the self-attention mechanism of Transformer computes representations for all ...", "dateLastCrawled": "2022-01-27T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Pre-Trained</b> Models: Past, Present and Future \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2106.07139/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.07139", "snippet": "As <b>compared</b> to RNNs, Transformer is an encoder-decoder structure that applies a self-attention mechanism, which <b>can</b> <b>model</b> correlations between all words of the input sequence in parallel. Hence, owing to the parallel computation of the self-attention mechanism, Transformer could fully take advantage of advanced computing devices to train large-scale models. In both the encoding and decoding phases of Transformer, the self-attention mechanism of Transformer computes representations for all ...", "dateLastCrawled": "2022-01-08T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "2020b a new sequence to sequence <b>pre trained</b> <b>model</b> which is <b>pre trained</b> ...", "url": "https://www.coursehero.com/file/p5ht2fk9o/2020b-a-new-sequence-to-sequence-pre-trained-model-which-is-pre-trained-by/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/p5ht2fk9o/2020b-a-new-sequence-to-sequence-<b>pre-trained</b>...", "snippet": "2020b), a new sequence-to-sequence <b>pre-trained</b> <b>model</b>, which is <b>pre-trained</b> by predicting future n-gram based on n-stream self-attention mechanism. For NAR baselines, we compare to NAT (Gu et al. 2017), BANG (Qi et al. 2020a), NAT initialized from MASS and encoder based methods. For BANG, it is a new NAR <b>pre-trained</b> <b>model</b>, which achieves significant improvements <b>compared</b> to previous NAR models.For NAT initialized from MASS, we initialize the parameters of NAT trans-former with the <b>pre-trained</b> ...", "dateLastCrawled": "2022-01-14T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deepening into the suitability of using <b>pre-trained</b> models of ImageNet ...", "url": "https://peerj.com/articles/cs-715/", "isFamilyFriendly": true, "displayUrl": "https://peerj.com/articles/cs-715", "snippet": "A lightweight <b>model</b> that <b>can</b> <b>be compared</b> with the performance of the <b>pre-trained</b> models ResNet50 and Xception has been designed. The <b>model</b> consists of several different components. The first is that the <b>model</b> starts with two convolutional layers sequenced to reduce the size of the input image. The first layer has a filter size of 3 \u00d7 3, and the second one has a filter size of 5 \u00d7 5. Every convolutional layer in the <b>model</b> is followed by a batch normalization layer and a rectified linear ...", "dateLastCrawled": "2022-01-28T19:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Pre-trained</b> Models for Natural Language Processing A Survey.pdf ...", "url": "https://www.coursehero.com/file/127027612/Pre-trained-Models-for-Natural-Language-Processing-A-Surveypdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/127027612/<b>Pre-trained</b>-<b>Models</b>-for-Natural-Language...", "snippet": "QIU XP, et al. <b>Pre-trained</b> Models for Natural Language Processing: A Survey March (2020) 3 h 1 h 2 h 3 h 4 h 5 x 1 x 2 x 3 x 4 x 5 (a) Convolutional <b>Model</b> h 1 h 2 h 3 h 4 h 5 x 1 x 2 x 3 x 4 x 5 (b) Recurrent <b>Model</b> h 1 h 2 h 3 h 4 h 5 x 1 x 2 x 3 x 4 x 5 (c) Fully-Connected Self-Attention <b>Model</b> Figure 2: Neural Contextual Encoders bedding of token x t because of the contextual information included in. 2.2 Neural Contextual Encoders Most of the neural contextual encoders <b>can</b> be classified ...", "dateLastCrawled": "2022-01-26T04:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "ML and NLP Research Highlights of 2021", "url": "https://ruder.io/ml-highlights-2021/", "isFamilyFriendly": true, "displayUrl": "https://ruder.io/ml-highlights-2021", "snippet": "The trained <b>model</b> <b>can</b> then be fine-tuned on different speech tasks (Babu et al., 2021). What happened? 2021 saw the continuation of the development of ever larger <b>pre-trained</b> models. <b>Pre-trained</b> models were applied in many different domains and started to be considered critical for ML research .", "dateLastCrawled": "2022-02-03T04:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "JOURNAL OF IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. V ...", "url": "https://allanchen95.github.io/data/coad.pdf", "isFamilyFriendly": true, "displayUrl": "https://allanchen95.github.io/data/coad.pdf", "snippet": "vision and natural language processing, we propose to train a self-supervised <b>expert</b> linking <b>model</b>, which is \ufb01rst <b>pre-trained</b> by contrastive learning on AMiner data to capture the common representation and matching patterns of experts across AMiner and external sources, and is then \ufb01ne-tuned by adversarial learning on AMiner and the unlabeled external sources to improve the <b>model</b> transferability. Experimental results demonstrate that COAD signi\ufb01cantly outperforms various baselines ...", "dateLastCrawled": "2021-11-18T09:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "COVIDSum:A Linguistically Enriched SciBERT-based Summarization <b>Model</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S1532046422000156", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1532046422000156", "snippet": "Table 6 shows the results of three variations of our proposed summarization <b>model</b> on the test set, from which we <b>can</b> make the following observations: 1) SciBERT-based <b>model</b> performs well on the test set, although the converges of its training loss curve is the slowest among the three models, which indicates that the overfitting issue on the training set occurs when fine-tuning the <b>pre-trained</b> models; 2) when the BERT-based sequence encoder is replaced with domain-specific <b>pre-trained</b> ...", "dateLastCrawled": "2022-01-30T15:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Expert Systems</b> - ResearchGate", "url": "https://www.researchgate.net/journal/Expert-Systems-1468-0394", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/journal/<b>Expert-Systems</b>-1468-0394", "snippet": "<b>Expert Systems</b> is a quarterly journal devoted to all aspects of artificial intelligence and advanced computing. The journal&#39;s readers include knowledge engineers artificial intelligence ...", "dateLastCrawled": "2022-01-23T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Can</b> I sell a product that uses a <b>pre-trained</b> neural network such as ...", "url": "https://www.quora.com/Can-I-sell-a-product-that-uses-a-pre-trained-neural-network-such-as-Keras-Resnet50-Alexnet-Or-will-I-be-sued", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Can</b>-I-sell-a-product-that-uses-a-<b>pre-trained</b>-neural-network-such...", "snippet": "Answer: <b>Pre-trained</b> neural networks are usually considered public knowledge so you\u2019re free to use them anyway you please. However trying to get a patent for a publicly available <b>pre-trained</b> network will be impossible. Also claiming that you\u2019ve invented it may lead to a lawsuit from its creator if...", "dateLastCrawled": "2022-01-19T08:25:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Pre-trained</b> Models - Value <b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> Technology", "url": "https://valueml.com/transfer-learning-approach-pre-trained-models-classifying-imagenet-classes-with-resnet50-in-python/", "isFamilyFriendly": true, "displayUrl": "https://valueml.com/transfer-<b>learning</b>-<b>approach-pre-trained-models-classifying</b>-imagenet...", "snippet": "Transfer <b>Learning</b> enables us to use the <b>pre-trained</b> models from other people by making small relevant changes. Basically, Transfer <b>Learning</b> (TL) is a <b>Machine</b> <b>Learning</b> technique that trains a new <b>model</b> for a particular problem based on the knowledge gained by solving some other problem. For example, the knowledge gained while <b>learning</b> to recognize trucks could be applied to recognize cars.", "dateLastCrawled": "2022-01-21T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Finding the Word <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-word-<b>analogy</b>-from-given-words-using-word2vec...", "snippet": "In the word <b>analogy</b> task, we complete the sentence ... What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the word <b>analogy</b>. In this tutorial, we will be using Word2Vec <b>model</b> and a <b>pre-trained</b> <b>model</b> named \u2018GoogleNews-vectors-negative300.bin\u2018 which is trained on over 50 Billion words by Google. Each word inside the <b>pre-trained</b> dataset is embedded in a 300-dimensional space and the words which are similar in context/meaning are placed closer to each other in ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Should I Learn Machine Learning</b>? | GenUI", "url": "https://www.genui.com/resources/ml-for-developers", "isFamilyFriendly": true, "displayUrl": "https://www.genui.com/resources/ml-for-developers", "snippet": "But it will almost always be best to start with a <b>pre-trained</b> <b>model</b>, from a more general dataset, and then fine-tune it to fit your specific domain. For example, most image recognition models are based on <b>pre-trained</b> models from ImageNet, a dataset of more than 14 million, hand-labeled images divided into over 20,000 classes (like \u201cbicycle\u201d, \u201cstrawberry\u201d, \u201csky\u201d).", "dateLastCrawled": "2022-01-30T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "SE3M: A <b>Model</b> for Software Effort Estimation Using <b>Pre-trained</b> ...", "url": "https://deepai.org/publication/se3m-a-model-for-software-effort-estimation-using-pre-trained-embedding-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/se3m-a-<b>model</b>-for-software-effort-estimation-using-pre...", "snippet": "Idri and Abran (Idri et al., 2016b) also classify a technique by <b>analogy</b> as a <b>machine</b> <b>learning</b> technique. These authors further point out that <b>machine</b> <b>learning</b> models have also gained significant attention for effort estimation purposes, as they can <b>model</b> the complex relationship between effort and software attributes (cost factors), especially when this relationship is not linear, and it does not appear to have any predetermined form. Analog-based reasoning approaches have proven to be ...", "dateLastCrawled": "2021-12-25T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Word2Vec in Gensim Explained for Creating Word Embedding Models ...", "url": "https://machinelearningknowledge.ai/word2vec-in-gensim-explained-for-creating-word-embedding-models-pretrained-and-custom/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>knowledge.ai/word2vec-in-gensim-explained-for-creating-word...", "snippet": "<b>Machine</b> <b>learning</b> and ... This is another way putting that word2vec can draw the <b>analogy</b> that if Man is to Woman then Kind is to Queen! The publicly released <b>model</b> of word2vec by Google consists of 300 features and the <b>model</b> is trained in the Google news dataset. The vocabulary size of the <b>model</b> is around 1.6 billion words. However, this might have taken a huge time for the <b>model</b> to be trained on but they have applied a method of simple subsampling approach to optimize the time. Word2Vec ...", "dateLastCrawled": "2022-02-02T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Using Deep <b>Learning</b> for Image Analogies | by Tomer Amit | Towards Data ...", "url": "https://towardsdatascience.com/using-deep-learning-for-image-analogies-aa2e7d7af337", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-deep-<b>learning</b>-for-image-analogies-aa2e7d7af337", "snippet": "I will use the <b>pre-trained</b> VGG16 image classification <b>model</b>. The <b>model</b> consists of CNN layers stacked one after another, connected by max pooling layers. The input of the network is a 244\u00d7244\u00d73 image (i.e image width and length are 244 pixels, and 3 channels), and after applying all the convolutional layers, we get a 7\u00d77\u00d7512 array. (diagram taken from deeplearning.ai course by Andrew Ng, \u201cConvolutional Neural Networks\u201d) At the end of the network we have an additional flattening layer ...", "dateLastCrawled": "2022-01-19T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Transfer <b>Learning</b>: The Highest Leverage Deep <b>Learning</b> Skill You Can Learn", "url": "https://www.the-analytics.club/transfer-learning", "isFamilyFriendly": true, "displayUrl": "https://www.the-analytics.club/transfer-<b>learning</b>", "snippet": "Transfer <b>learning</b> is a <b>machine</b> <b>learning</b> technique in which a <b>model</b> trained on a specific task is reused as part of the training process for another, different task. Here is a simple <b>analogy</b> to help you understand how transfer <b>learning</b> works: imagine that one person has learned everything there is to know about dogs. In contrast, another person has learned everything about cats. If both people are asked, \u201cWhat\u2019s an animal with four legs, a tail, and barks?\u201d The person who knows all ...", "dateLastCrawled": "2022-01-29T09:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> | Request PDF", "url": "https://www.researchgate.net/publication/349152012_Classifying_and_completing_word_analogies_by_machine_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349152012_Classifying_and_completing_word...", "snippet": "In this paper, we depart from this assumption to adopt a <b>machine</b> <b>learning</b> approach, i.e., <b>learning</b> a substitute of the parallelogram <b>model</b>. To achieve our goal, we first review the formal modeling ...", "dateLastCrawled": "2021-11-11T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Transfer <b>Learning to solve a Classification Problem</b> :: InBlog", "url": "https://inblog.in/Transfer-Learning-to-solve-a-Classification-Problem-9bihoVsKsV", "isFamilyFriendly": true, "displayUrl": "https://inblog.in/Transfer-<b>Learning-to-solve-a-Classification-Problem</b>-9bihoVsKsV", "snippet": "Why we need <b>pre-Trained</b> <b>Model</b>? Transfer <b>Learning</b> via VGG16; Building a <b>Model</b>; Code Walk Through; Result and Evaluation; Introduction: Neural networks are very different type of the <b>model</b> as compared to the Supervised <b>Learning</b>,. The most important things about deep <b>learning</b> <b>model</b> is it is very hard to train. It requires lots of the resources that a small company can\u2019t bear. RAM on a <b>machine</b> is cheap and is available in plenty. You need hundreds of GB\u2019s of RAM to run a super complex ...", "dateLastCrawled": "2021-11-25T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - <b>Merging pretrained models in Word2Vec</b>? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/30482669/merging-pretrained-models-in-word2vec", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/30482669", "snippet": "How do i merge these two huge <b>pre-trained</b> vectors? or how do i train a new <b>model</b> and update vectors on top of another? I see that C based word2vec does not support batch training. I am looking to compute word <b>analogy</b> from these two models. I believe that vectors learned from these two sources will produce pretty good results. <b>machine</b>-<b>learning</b> word2vec. Share. Follow edited May 28 &#39;15 at 14:04. pbu. asked May 27 &#39;15 at 12:37. pbu pbu. 2,706 7 7 gold badges 37 37 silver badges 62 62 bronze ...", "dateLastCrawled": "2022-01-22T22:47:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(pre-trained model)  is like +(academic expert)", "+(pre-trained model) is similar to +(academic expert)", "+(pre-trained model) can be thought of as +(academic expert)", "+(pre-trained model) can be compared to +(academic expert)", "machine learning +(pre-trained model AND analogy)", "machine learning +(\"pre-trained model is like\")", "machine learning +(\"pre-trained model is similar\")", "machine learning +(\"just as pre-trained model\")", "machine learning +(\"pre-trained model can be thought of as\")", "machine learning +(\"pre-trained model can be compared to\")"]}