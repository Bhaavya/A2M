{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "4. <b>Hyperparameter</b> Tuning - Evaluating Machine Learning Models [Book]", "url": "https://www.oreilly.com/library/view/evaluating-machine-learning/9781492048756/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/evaluating-machine-learning/9781492048756/ch04.html", "snippet": "<b>Hyperparameter</b> <b>settings</b> could have a big impact on the prediction accuracy of the trained model. Optimal <b>hyperparameter</b> <b>settings</b> often differ for different datasets. Therefore they should be tuned for each dataset. Since the training process doesn\u2019t set the hyperparameters, there needs to be a meta process that tunes the hyperparameters. This is what we mean by <b>hyperparameter</b> tuning. <b>Hyperparameter</b> tuning is a meta-optimization task. As Figure 4-1 shows, each trial of a particular ...", "dateLastCrawled": "2022-02-01T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Hyperparameter Tuning with callbacks in</b> Keras | by Abhishek Rajbhoj ...", "url": "https://towardsdatascience.com/hyperparameter-tuning-with-callbacks-in-keras-5230f51f29b3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>hyperparameter-tuning-with-callbacks-in</b>-keras-5230f51f29b3", "snippet": "This article showcases a simple approach to tune your hyperparameters by accessing your model weights using callbacks in Keras. Applied Machine Learning is an empirical process where you need to try out different <b>settings</b> of hyperparameters and deduce which <b>settings</b> work best for your application. This technique is popularly known as ...", "dateLastCrawled": "2022-02-02T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Adaptive <b>Hyperparameter</b> Tuning - Determined AI", "url": "https://www.determined.ai/assets/images/resources/AdaptiveHyperparameterTuning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.determined.ai/assets/images/resources/Adaptive<b>Hyperparameter</b>Tuning.pdf", "snippet": "We all know what it\u2019s <b>like</b> to watch a game on <b>TV</b> when one side is winning by such a large margin that the game is \u201cover before it\u2019s over.\u201d While a comeback is possible, it just doesn\u2019t feel worth watching for another half hour to the end, so we grab the remote and flip to another game. Just as we might change the channel in this scenario, Deter-mined\u2019s adaptive <b>hyperparameter</b> search abandons unpromising <b>hyperparameter</b> configurations that are unlikely to outper-form other ...", "dateLastCrawled": "2022-01-30T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Local Search Optimization for HyperParameter Tuning</b> - The SAS Data ...", "url": "https://blogs.sas.com/content/subconsciousmusings/2016/09/20/local-search-optimization-for-hyperparameter-tuning/", "isFamilyFriendly": true, "displayUrl": "https://blogs.sas.com/.../2016/09/20/<b>local-search-optimization-for-hyperparameter-tuning</b>", "snippet": "Many other adjustable <b>settings</b> also affect the quality of the picture \u2013 brightness, contrast, sharpness, color, tint, color temperature, picture mode, and other more advanced picture control options <b>like</b> motion mode and noise reduction. While most people simply connect the <b>TV</b> and use out-of-the-box <b>settings</b>, not having been instructed in the past to adjust the <b>TV</b>, it turns out that modern HDTVs need to be calibrated to the room size and typical lighting, which will vary. Simply reducing ...", "dateLastCrawled": "2022-01-15T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Ant Colony-Based <b>Hyperparameter</b> Optimisation in <b>Total Variation</b> ...", "url": "https://www.researchgate.net/publication/348625402_Ant_Colony-Based_Hyperparameter_Optimisation_in_Total_Variation_Reconstruction_in_X-ray_Computed_Tomography", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348625402_Ant_Colony-Based_<b>Hyperparameter</b>...", "snippet": "Cross-sectional slices of the reconstructed images from the female phantom using different method and <b>hyperparameter</b> <b>settings</b>. The display window is [0-0.07]. \u2026 Different sets of hyperparameters ...", "dateLastCrawled": "2022-01-15T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Cross-Validation</b> and <b>Hyperparameter</b> Tuning: How to Optimise your ...", "url": "https://towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>cross-validation</b>-and-<b>hyperparameter</b>-tuning-how-to...", "snippet": "<b>Hyperparameter</b> Tuning. Unlike model parameters, which are learned during model training and can not be set arbitrarily, hyperparameters are parameters that can be set by the user before training a Machine Learning model. Examples of hyperparameters in a Random Forest are the number of decision trees to have in the forest, the maximum number of features to consider at each split or the maximum depth of the tree. As I mentioned previously, there is no one-size-fits-all solution to finding ...", "dateLastCrawled": "2022-02-02T15:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "4. Model Training Patterns - <b>Machine Learning Design Patterns</b> [Book]", "url": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "snippet": "When you get more data, first train it with the old <b>settings</b>, ... we choose a list of possible values we\u2019d <b>like</b> to try for each <b>hyperparameter</b> we want to optimize. For example, in scikit-learn\u2019s RandomForestRegressor() model, let\u2019s say we want to try the following combination of values for the model\u2019s max_depth and n_estimators hyperparameters: grid_values = {&#39;max_depth&#39;: [5, 10, 100], &#39;n_estimators&#39;: [100, 150, 200]} Using grid search, we\u2019d try every combination of the specified ...", "dateLastCrawled": "2022-01-30T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "r - How to get RMSE value from GA <b>hyperparameter</b> tuning? - Stack Overflow", "url": "https://stackoverflow.com/questions/58808552/how-to-get-rmse-value-from-ga-hyperparameter-tuning", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/58808552", "snippet": "I found R code for <b>hyperparameter</b> tuning using GA. The following is the code, but it does not show the expected results, which will be the prediction accuracy? I have mentioned the output it produc...", "dateLastCrawled": "2021-10-19T08:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Machine learning applications for assessment of dynamic progressive ...", "url": "https://www.sciencedirect.com/science/article/pii/S2352012421011851", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2352012421011851", "snippet": "Default <b>settings</b> are recommended for other parameters, such as booster, base_score, random_state and so on. <b>Hyperparameter</b> optimization of the XGBoost is performed by the bayesian optimization framework and grid search method. The bayesian optimization is based on bayesian inference and gaussian process, and the maximum value of an unknown function can be found. The XGBoost model architecture for the first tree is visualized using a convenient function", "dateLastCrawled": "2022-01-30T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - Is <b>random state</b> a parameter to tune ... - Cross ...", "url": "https://stats.stackexchange.com/questions/263999/is-random-state-a-parameter-to-tune", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/263999/is-<b>random-state</b>-a-parameter", "snippet": "For this reason, an experienced user will select an appropriate value based on his intuition, domain knowledge and the semantic meaning of the <b>hyperparameter</b> (if any). Alternatively, one might use a validation set to perform <b>hyperparameter</b> selection. Here, we try to find an optimal <b>hyperparameter</b> value for the entire population of data by testing different candidate values on a sample of the population (the validation set).", "dateLastCrawled": "2022-01-29T15:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Ant Colony-Based <b>Hyperparameter</b> Optimisation in <b>Total Variation</b> ...", "url": "https://www.mdpi.com/1424-8220/21/2/591/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/21/2/591/htm", "snippet": "In this paper, a computer-aided training method for <b>hyperparameter</b> selection of limited data <b>X-ray computed tomography</b> (XCT) reconstruction was proposed. The proposed method employed the ant colony optimisation (ACO) approach to assist in <b>hyperparameter</b> selection for the adaptive-weighted projection-controlled steepest descent (AwPCSD) algorithm, which is a <b>total-variation</b> (<b>TV</b>) based regularisation algorithm. During the implementation, there was a colony of artificial ants that swarm through ...", "dateLastCrawled": "2022-02-02T13:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Faster <b>Hyperparameter</b> Tuning in the Cloud : Coiled", "url": "https://coiled.io/blog/faster-hyperparameter-tuning-in-the-cloud/", "isFamilyFriendly": true, "displayUrl": "https://coiled.io/blog/faster-<b>hyperparameter</b>-tuning-in-the-cloud", "snippet": "Hyperband \u2013 let\u2019s invest more in lottery tickets <b>similar</b> to those that are winning Optuna \u2013 intelligent, algorithmic discovery of the <b>hyperparameter</b> space Let\u2019s find the best set of hyperparameters for an XGBoost model that predicts the quality of the wine based on 11 descriptive features [Cortez et al., 2009] .", "dateLastCrawled": "2022-01-22T23:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Local Search Optimization for HyperParameter Tuning</b> - The SAS Data ...", "url": "https://blogs.sas.com/content/subconsciousmusings/2016/09/20/local-search-optimization-for-hyperparameter-tuning/", "isFamilyFriendly": true, "displayUrl": "https://blogs.sas.com/.../2016/09/20/<b>local-search-optimization-for-hyperparameter-tuning</b>", "snippet": "Many other adjustable <b>settings</b> also affect the quality of the picture \u2013 brightness, contrast, sharpness, color, tint, color temperature, picture mode, and other more advanced picture control options like motion mode and noise reduction. While most people simply connect the <b>TV</b> and use out-of-the-box <b>settings</b>, not having been instructed in the past to adjust the <b>TV</b>, it turns out that modern HDTVs need to be calibrated to the room size and typical lighting, which will vary. Simply reducing ...", "dateLastCrawled": "2022-01-15T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Ant Colony-Based <b>Hyperparameter</b> Optimisation in <b>Total Variation</b> ...", "url": "https://www.researchgate.net/publication/348625402_Ant_Colony-Based_Hyperparameter_Optimisation_in_Total_Variation_Reconstruction_in_X-ray_Computed_Tomography", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348625402_Ant_Colony-Based_<b>Hyperparameter</b>...", "snippet": "Cross-sectional slices of the reconstructed images from the female phantom using different method and <b>hyperparameter</b> <b>settings</b>. The display window is [0-0.07]. \u2026 Different sets of hyperparameters ...", "dateLastCrawled": "2022-01-15T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Hyperparameter</b> to Optimize - <b>TV</b> Assignment Help", "url": "https://www.tvassignmenthelp.com/questions/decision-tree-hyperparameter-to-optimize-statistic-assignment-help", "isFamilyFriendly": true, "displayUrl": "https://www.<b>tv</b>assignmenthelp.com/questions/<b>decision-tree-hyperparameter-to-optimize</b>...", "snippet": "Another such <b>hyperparameter</b> is &quot;a split must reduce impurity by Y in order to be allowed&quot;. Choose 1 <b>hyperparameter</b> to optimize. Build decision trees with various <b>settings</b> of that <b>hyperparameter</b>, evaluating their accuracy with your validation data.", "dateLastCrawled": "2022-01-13T00:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Basic statistics and <b>hyper-parameter</b> <b>settings</b> for the four datasets ...", "url": "https://researchgate.net/figure/Basic-statistics-and-hyper-parameter-settings-for-the-four-datasets_tbl1_333072348", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Basic-statistics-and-<b>hyper-parameter</b>-<b>settings</b>-for-the...", "snippet": "Download scientific diagram | Basic statistics and <b>hyper-parameter</b> <b>settings</b> for the four datasets. from publication: Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation ...", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Contouring learning rate to optimize neural</b> nets \u2013 O\u2019Reilly", "url": "https://www.oreilly.com/content/contouring-learning-rate-to-optimize-neural-nets/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/content/<b>contouring-learning-rate-to-optimize-neural</b>-nets", "snippet": "Figure 1 shows the comparison of different optimization techniques under <b>similar</b> <b>hyperparameter</b> <b>settings</b>: Figure 1. A comparison of optimization techniques. Source: Alec Radford, used with permission. In this image, the momentum update overshoots the target, but reaches the overall minimum faster. \u201cNAG\u201d is the Nesterov Accelerated Gradient, in which the first step is taken in the direction of the velocity and then a correction is made to the velocity vector based on the new location ...", "dateLastCrawled": "2022-01-01T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Word Embedding: <b>Word2Vec</b> With Genism, NLTK, and t-SNE Visualization ...", "url": "https://medium.com/swlh/word-embedding-word2vec-with-genism-nltk-and-t-sne-visualization-43eae8ab3e2e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/word-embedding-<b>word2vec</b>-with-genism-nltk-and-t-sne...", "snippet": "Word vectors are situated in the vector space to such an extent that words that share regular <b>settings</b> in the corpus are found near each other in the space First \u2014 An input layer, Middle-Hidden ...", "dateLastCrawled": "2022-01-29T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "4. Model Training Patterns - <b>Machine Learning Design Patterns</b> [Book]", "url": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "snippet": "When you get more data, first train it with the old <b>settings</b>, ... By <b>similar</b> task, we\u2019re referring to the problem being solved. To do transfer learning for image classification, for example, it is better to start with a model that has been trained for image classification, rather than object detection. Continuing with the example, let\u2019s say we\u2019re building a binary classifier to determine whether an image of an x-ray contains a broken bone. We only have 200 images of each class: broken ...", "dateLastCrawled": "2022-01-30T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "In deep learning, can I change the weight of loss dynamically? - Stack ...", "url": "https://stackoverflow.com/questions/51527211/in-deep-learning-can-i-change-the-weight-of-loss-dynamically", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/51527211", "snippet": "Call for experts in deep learning. Hey, I am recently working on training images using tensorflow in python for tone mapping. To get the better result, I focused on using perceptual loss introduced from this paper by Justin Johnson.. In my implementation, I made the use of all 3 parts of loss: a feature loss that extracted from vgg16; a L2 pixel-level loss from the transferred image and the ground true image; and the <b>total variation</b> loss.", "dateLastCrawled": "2022-01-19T13:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Hyperparameter Tuning with callbacks in</b> Keras | by Abhishek Rajbhoj ...", "url": "https://towardsdatascience.com/hyperparameter-tuning-with-callbacks-in-keras-5230f51f29b3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>hyperparameter-tuning-with-callbacks-in</b>-keras-5230f51f29b3", "snippet": "This article showcases a simple approach to tune your hyperparameters by accessing your model weights using callbacks in Keras. Applied Machine Learning is an empirical process where you need to try out different <b>settings</b> of hyperparameters and deduce which <b>settings</b> work best for your application. This technique is popularly known as ...", "dateLastCrawled": "2022-02-02T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "4. Model Training Patterns - <b>Machine Learning Design Patterns</b> [Book]", "url": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "snippet": "When you get more data, first train it with the old <b>settings</b>, ... <b>Hyperparameter</b> tuning <b>can</b> <b>be thought</b> of as an outer optimization loop. Nonlinear optimization. The hyperparameters that need to be tuned fall into two groups: those related to model architecture and those related to model training. Model architecture hyperparameters, like the number of layers in your model or the number of neurons per layer, control the mathematical function that underlies the machine learning model ...", "dateLastCrawled": "2022-01-30T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Hyperparameter</b> Tuning Machine Learning - XpCourse", "url": "https://www.xpcourse.com/hyperparameter-tuning-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.xpcourse.com/<b>hyperparameter</b>-tuning-machine-learning", "snippet": "<b>Hyperparameter</b> tuning relies more on experimental results than theory, and thus the best method to determine the optimal <b>settings</b> is to try many different combinations evaluate the performance of each model. However, evaluating each model only on the training set <b>can</b> lead to one of the most fundamental problems in machine learning: overfitting.", "dateLastCrawled": "2022-01-24T04:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Hyperband: A novel <b>bandit-based approach to hyperparameter optimization</b> ...", "url": "https://www.researchgate.net/publication/326109067_Hyperband_A_novel_bandit-based_approach_to_hyperparameter_optimization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/326109067_Hyperband_A_novel_bandit-based...", "snippet": "<b>Hyperparameter</b> tuning <b>can</b> further improve the predictive performance, but unlike neural networks, full-batch training of many models on large datasets <b>can</b> be time consuming. Owing to the discovery ...", "dateLastCrawled": "2022-01-03T10:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning \u2014 Is the emperor wearing clothes</b>? | by Cassie Kozyrkov ...", "url": "https://kozyrkov.medium.com/machine-learning-is-the-emperor-wearing-clothes-928fe406fe09", "isFamilyFriendly": true, "displayUrl": "https://kozyrkov.medium.com/<b>machine-learning-is-the-emperor-wearing-clothes</b>-928fe406fe09", "snippet": "This is followed by an eternity of fiddling with the code <b>settings</b> (don\u2019t let the noble name \u201c<b>hyperparameter</b> tuning\u201d fool you) until voila! A model! One that turns out not to work when you evaluate its performance on new data\u2026 and you\u2019re back to the drawing board, over and over, until finally the heavens open up and your solution stops embarrassing itself. That\u2019s why it\u2019s so important to", "dateLastCrawled": "2022-01-28T00:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Hyperparameter</b> Tuning Deep Learning - The Best Images, Videos ...", "url": "https://discussionsbytopic.com/hyperparameter-tuning-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://discussionsbytopic.com/<b>hyperparameter</b>-tuning-deep-learning", "snippet": "Trending posts and videos related to <b>Hyperparameter</b> Tuning Deep Learning! <b>Hyperparameter</b> Tuning Deep Learning. Images, posts &amp; videos related to &quot;<b>Hyperparameter</b> Tuning Deep Learning&quot; Best ways to learn <b>Hyperparameter</b> Tuning for Deep Learning. I\u2019m pretty new to AI, and I\u2019ve taken a ton of classes on Neural Networks, and I want to work on improving my skills in actually implementing them. I know the equations inside and out now, but I need to gain a good understanding/intuition of what to ", "dateLastCrawled": "2021-05-23T18:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "UofWashington-MachineLearning/clustering_5_lda_BBI.py at master ...", "url": "https://github.com/bdanalytics/UofWashington-MachineLearning/blob/master/clustering_5_lda_BBI.py", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/bdanalytics/UofWashington-MachineLearning/blob/master/clustering_5...", "snippet": "# In the video lectures, we saw that alpha and gamma <b>can</b> <b>be thought</b> of as smoothing parameters when we compute how much each document &quot;likes&quot; a topic (in the case of alpha) or how much each topic &quot;likes&quot; a word (in the case of gamma). In both cases, these parameters serve to reduce the differences across topics or words in terms of these calculated preferences; alpha makes the document preferences &quot;smoother&quot; over topics, and gamma makes the topic preferences &quot;smoother&quot; over words.", "dateLastCrawled": "2022-01-03T15:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Regularization: Ridge, Lasso &amp; Elastic Net Regression - DataCamp", "url": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net", "isFamilyFriendly": true, "displayUrl": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net", "snippet": "As the model complexity, which in the case of linear regression <b>can</b> <b>be thought</b> of as the number of predictors, increases, estimates&#39; variance also increases, but the bias decreases. The unbiased OLS would place us on the right-hand side of the picture, which is far from optimal. That&#39;s why we regularize: to lower the variance at the cost of some bias, thus moving left on the plot, towards the optimum.", "dateLastCrawled": "2022-02-02T03:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Studying and Analysing the Effect of Weight Norm Penalties and Dropout ...", "url": "https://www.ijert.org/research/studying-and-analysing-the-effect-of-weight-norm-penalties-and-dropout-as-regularizers-for-small-convolutional-neural-networks-IJERTV10IS010025.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/research/studying-and-analysing-the-effect-of-weight-norm...", "snippet": "unrivalled results on data that <b>can</b> be structured to have a grid-like topology. This is the reason why convolutional networks are chosen for computer vision applications as the feature images <b>can</b> be reduced to form either a 2D or 3D grid of pixels. The success of these modern gradient-based networks is associated with the fact that this type of network architecture makes use of sparse interactions wherein, each convolution operation is carried by a kernel with a size smaller than the input ...", "dateLastCrawled": "2022-01-30T21:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>can</b> we <b>reduce loss in deep learning</b>? - Quora", "url": "https://www.quora.com/How-can-we-reduce-loss-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-we-<b>reduce-loss-in-deep-learning</b>", "snippet": "Answer: To <b>reduce loss in deep learning</b>: * Hyperparameters are the configuration <b>settings</b> used to tune how the model is trained. * The derivative of (y - y&#39;)2 with respect to the weights and biases tells us how loss changes for a given example * * Simple to compute and convex * So we repe...", "dateLastCrawled": "2022-01-19T22:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Adaptive <b>Hyperparameter</b> Tuning - Determined AI", "url": "https://www.determined.ai/assets/images/resources/AdaptiveHyperparameterTuning.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.determined.ai/assets/images/resources/Adaptive<b>Hyperparameter</b>Tuning.pdf", "snippet": "on <b>TV</b> when one side is winning by such a large margin that the game is \u201cover before it\u2019s over.\u201d While a comeback is possible, it just doesn\u2019t feel worth watching for another half hour to the end, so we grab the remote and flip to another game. Just as we might change the channel in this scenario, Deter-mined\u2019s adaptive <b>hyperparameter</b> search abandons unpromising <b>hyperparameter</b> configurations that are unlikely to outper-form other configurations under consideration. Algorithm ...", "dateLastCrawled": "2022-01-30T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Ant Colony-Based <b>Hyperparameter</b> Optimisation in <b>Total Variation</b> ...", "url": "https://www.mdpi.com/1424-8220/21/2/591/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/21/2/591/htm", "snippet": "In this paper, a computer-aided training method for <b>hyperparameter</b> selection of limited data <b>X-ray computed tomography</b> (XCT) reconstruction was proposed. The proposed method employed the ant colony optimisation (ACO) approach to assist in <b>hyperparameter</b> selection for the adaptive-weighted projection-controlled steepest descent (AwPCSD) algorithm, which is a <b>total-variation</b> (<b>TV</b>) based regularisation algorithm. During the implementation, there was a colony of artificial ants that swarm through ...", "dateLastCrawled": "2022-02-02T13:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Ant Colony-Based <b>Hyperparameter</b> Optimisation in <b>Total Variation</b> ...", "url": "https://www.researchgate.net/publication/348625402_Ant_Colony-Based_Hyperparameter_Optimisation_in_Total_Variation_Reconstruction_in_X-ray_Computed_Tomography", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348625402_Ant_Colony-Based_<b>Hyperparameter</b>...", "snippet": "Cross-sectional slices of the reconstructed images from the female phantom using different method and <b>hyperparameter</b> <b>settings</b>. The display window is [0-0.07]. The display window is [0-0.07].", "dateLastCrawled": "2022-01-15T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Cross-Validation</b> and <b>Hyperparameter</b> Tuning: How to Optimise your ...", "url": "https://towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>cross-validation</b>-and-<b>hyperparameter</b>-tuning-how-to...", "snippet": "<b>Hyperparameter</b> Tuning. Unlike model parameters, which are learned during model training and <b>can</b> not be set arbitrarily, hyperparameters are parameters that <b>can</b> be set by the user before training a Machine Learning model. Examples of hyperparameters in a Random Forest are the number of decision trees to have in the forest, the maximum number of ...", "dateLastCrawled": "2022-02-02T15:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Grid Search</b> for model tuning. A model <b>hyperparameter</b> is a\u2026 | by Rohan ...", "url": "https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>grid-search</b>-for-model-tuning-3319b259367e", "snippet": "A model <b>hyperparameter</b> is a characteristic of a model that is external to the model and whose value cannot be estimated from data. The value of the <b>hyperparameter</b> has to be set before the learning process begins. For example, c in Support Vector Machines, k in k-Nearest Neighbors, the number of hidden layers in Neural Networks. In contrast, a parameter is an internal characteristic of the model and its value <b>can</b> be estimated from data. Example, beta coefficients of linear/logistic regression ...", "dateLastCrawled": "2022-02-02T15:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Stochastic <b>Hyperparameter</b> Optimization through Hypernetworks", "url": "https://www.researchgate.net/publication/323410474_Stochastic_Hyperparameter_Optimization_through_Hypernetworks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/323410474_Stochastic_<b>Hyperparameter</b>...", "snippet": "PDF | Machine learning models are often tuned by nesting optimization of model weights inside the optimization of hyperparameters. We give a method to... | Find, read and cite all the research you ...", "dateLastCrawled": "2021-09-18T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Lesson 4. Advanced Machine Learning</b> | KNIME", "url": "https://www.knime.com/self-paced-course/l2-ds-knime-analytics-platform-for-data-scientists-advanced/lesson4", "isFamilyFriendly": true, "displayUrl": "https://www.knime.com/self-paced-course/l2-ds-knime-analytics-platform-for-data...", "snippet": "They tend to give more accurate and robust results <b>compared</b> to simple models, though they require more <b>settings</b>. The search for the best performing <b>hyperparameter</b> setting <b>can</b> be automated with a parameter optimization loop. This lesson includes exercises. The corresponding data files, solution workflows, and prebuilt, empty exercise workflows with instructions are available in the ...", "dateLastCrawled": "2022-01-29T01:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "4. The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf ...", "url": "https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/ch04.html", "snippet": "The sensitivity of classifier performance to <b>hyperparameter</b> <b>settings</b> depends on the model and the distribution of training data. Logistic regression is relatively robust (or insensitive) to <b>hyperparameter</b> <b>settings</b>. Even so, it is necessary to find and use the right range of hyperparameters. Otherwise, the advantages of one model versus another may be solely due to tuning parameters, and will not reflect the actual behavior of the model or features. Even the best autotuning packages still ...", "dateLastCrawled": "2022-01-29T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Cost-Efficient Online <b>Hyperparameter</b> Optimization", "url": "https://www.arxiv-vanity.com/papers/2101.06590/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2101.06590", "snippet": "Recent work on hyperparameters optimization (HPO) has shown the possibility of training certain hyperparameters together with regular parameters. However, these online HPO algorithms still require running evaluation on a set of validation examples at each training step, steeply increasing the training cost. To decide when to query the validation loss, we model online HPO as a time-varying Bayesian optimization problem, on top of which we propose a novel costly feedback setting to capture the ...", "dateLastCrawled": "2021-09-26T09:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Influence of reconstruction <b>settings</b> in electrical impedance tomography ...", "url": "https://iopscience.iop.org/article/10.1088/1361-6579/ab248e", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1361-6579/ab248e", "snippet": "We therefore chose to further evaluate different <b>settings</b> for these parameters with ts ranging from 0.01\u20130.08 (0.01 step size) and wr ranging from 0.1\u20130.3 (0.05 step size). In addition, the extent of the regularization of the optimization formulation, i.e. the value of the <b>hyperparameter</b>, <b>can</b> be adjusted according to a noise performance ...", "dateLastCrawled": "2021-01-11T19:45:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> 101. The idea of this blog is to cut jargon\u2026 | by ...", "url": "https://medium.com/artificialis/machine-learning-101-5b9d3e4c44b7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>machine</b>-<b>learning</b>-101-5b9d3e4c44b7", "snippet": "<b>Machine</b> <b>Learning</b> is smarter than those spammers, ... In <b>analogy</b>, the test set is ... Things like cross-validation, <b>hyperparameter</b> tuning, over and underfitting, etc\u2026 will be coming up. Hope you ...", "dateLastCrawled": "2022-01-30T14:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The Hitchhiker\u2019s Guide to Optimization in <b>Machine Learning</b> | by Aman ...", "url": "https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-machine-learning-edcf5a104210", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-<b>machine</b>...", "snippet": "NOTE: For the sake of simplicity and better understanding, we\u2018ll restrict the scope of our discussion to supervised <b>machine learning</b> algorithms only. <b>Machine Learning</b> is the ideal culmination of Applied Mathematics and Computer Science, where we train and use data-driven applications to run inferences on the available data. Generally speaking, for an ML task, the type of inference (i.e., the prediction that the model makes) varies on the basis of the problem statement and the type of data ...", "dateLastCrawled": "2022-02-02T19:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Hyperparameters</b> tuning of ensemble model for software effort estimation ...", "url": "https://link.springer.com/article/10.1007/s12652-020-02277-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12652-020-02277-4", "snippet": "Claesen M, De Moor B (2015) <b>Hyperparameter</b> search in <b>machine</b> <b>learning</b>. arXiv:1502.02127. Clarke B, Fokoue E, Zhang H (2009) Principle and theory for data mining and <b>machine</b> <b>learning</b>. Springer, New York. Book Google Scholar Conte SD, Dunsmore HE, Shen VY (2019) Software Engineering Metrics and Models. Benjamin-Cummings Publishing. Co., San Francisco", "dateLastCrawled": "2021-12-25T15:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Evaluation of Model and <b>Hyperparameter</b> Choices in word2vec", "url": "https://west.uni-koblenz.de/assets/theses/evaluation-model-hyperparameter-choices-word2vec.pdf", "isFamilyFriendly": true, "displayUrl": "https://west.uni-koblenz.de/assets/theses/evaluation-model-<b>hyperparameter</b>-choices...", "snippet": "used for the evaluation of the similarity and the <b>analogy</b> task and further breaks down the downstream <b>machine</b> <b>learning</b> tasks used. The identi\ufb01ed best practices are used to evaluate our own experiments to evaluate the effects for some small model and <b>hyperparameter</b> changes for the word2vec algorithm. The experiments", "dateLastCrawled": "2022-02-03T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How Bias and Variance Affect a <b>Machine Learning</b> Model | by Ismael ...", "url": "https://medium.com/swlh/how-bias-and-variance-affect-a-machine-learning-model-6d258d9221db", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/how-bias-and-variance-affect-a-<b>machine-learning</b>-model-6d258d9221db", "snippet": "In <b>machine learning</b>, bias is the algorithm tendency to repeatedly learn the wrong thing by ignoring all the information in the data. Thus, high bias results from the algorithm missing relevant ...", "dateLastCrawled": "2021-12-15T02:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b>-<b>learning</b>-based models to predict shear transfer strength of ...", "url": "https://www.sciencedirect.com/science/article/pii/S0141029621013778", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0141029621013778", "snippet": "Third, a customized model training procedure is carried out including train-test split, feature scaling, feature selection, <b>hyperparameter</b> tuning, and outlier detection to obtain the optimized <b>machine</b>-<b>learning</b>-based models. Fourth, with various performance indicators, the two <b>machine</b>-<b>learning</b>-based models are evaluated by experimental results, and further assessed by the comparison with three common mechanical-based models. Finally, a parameter study on the most influential parameters is ...", "dateLastCrawled": "2022-01-18T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Lecture 4: \\(k\\)-Nearest Neighbours and SVM RBFs \u2014 CPSC 330 Applied ...", "url": "https://ubc-cs.github.io/cpsc330/lectures/04_kNNs-SVM-RBF.html", "isFamilyFriendly": true, "displayUrl": "https://ubc-cs.github.io/cpsc330/lectures/04_kNNs-SVM-RBF.html", "snippet": "What\u2019s the fundamental trade-off in supervised <b>machine</b> <b>learning</b>? What is the golden rule of <b>machine</b> <b>learning</b>? <b>Learning</b> outcomes\u00b6 From this lecture, you will be able to. explain the notion of similarity-based algorithms; broadly describe how \\(k\\)-NNs use distances; discuss the effect of using a small/large value of the <b>hyperparameter</b> \\(k\\) when using the \\(k\\)-NN algorithm; describe the problem of curse of dimensionality; explain the general idea of SVMs with RBF kernel; broadly describe ...", "dateLastCrawled": "2022-01-11T11:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> | Request PDF", "url": "https://www.researchgate.net/publication/349152012_Classifying_and_completing_word_analogies_by_machine_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349152012_Classifying_and_completing_word...", "snippet": "Recent <b>machine</b> <b>learning</b> methods for deriving vector-space embeddings of words (e.g., word2vec) have achieved considerable success in natural language processing. These vector spaces have also been ...", "dateLastCrawled": "2021-11-11T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Explanation of Bias and Variance for <b>Machine</b> <b>Learning</b> &amp; Deep <b>Learning</b> ...", "url": "https://www.reddit.com/r/learnmachinelearning/comments/s5bjsv/explanation_of_bias_and_variance_for_machine/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/learn<b>machinelearning</b>/comments/s5bjsv/explanation_of_bias_and...", "snippet": "This gradient gives us the information we need about the landscape of the function i.e. the steepest direction where we should move in order to minimize the function. A point to keep in mind: gamma the step size (also called the <b>learning</b> rate) is a <b>hyperparameter</b>.-----I have been studying and practicing <b>Machine</b> <b>Learning</b> and Computer Vision for ...", "dateLastCrawled": "2022-01-16T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Data Analyst vs. <b>Data Scientist</b> vs. ML Engineer Job Titles | Towards ...", "url": "https://towardsdatascience.com/data-analyst-vs-data-scientist-2534fc1057c3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/data-analyst-vs-<b>data-scientist</b>-2534fc1057c3", "snippet": "Hopefully, this <b>analogy</b> will help you make more informed choices around your education, job applications, and project staffing. \ud83d\udd35 Data Analyst . The data analyst is capable of taking da t a from the \u201cstarting line\u201d (i.e., pulling data from storage), doing data cleaning and processing, and creating a final product like a dashboard or report. The data analyst may also be responsible for transforming data for use by a <b>data scientist</b>, a hand-off that we\u2019ll explore in a moment. The data ...", "dateLastCrawled": "2022-02-01T15:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Demystifying Differentiation and Optimisers in Neural Network | by ...", "url": "https://medium.com/nerd-for-tech/demystifying-differentiation-and-optimisers-in-neural-network-510c54f693c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/demystifying-differentiation-and-optimisers-in-neural...", "snippet": "Here, we have two hyperparameters, momentum (m) and <b>learning</b> rate (/eta).A <b>hyperparameter is like</b> a knob. If you rotate one knob, the model could learn better or worse. It gives us control over ...", "dateLastCrawled": "2021-12-22T03:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "cufctl.github.io", "url": "https://cufctl.github.io/mlbd/notebooks/supervised-learning.ipynb", "isFamilyFriendly": true, "displayUrl": "https://cufctl.github.io/mlbd/notebooks/supervised-<b>learning</b>.ipynb", "snippet": "A <b>hyperparameter is like</b> a parameter, except we have to set it ourselves; the model cannot learn a hyperparameter on its own. The distance metric is also a hyperparameter; it is a function that we have to choose. Another very important aspect of designing a <b>machine</b> <b>learning</b> system is to pick the best hyperparameter values, or the values for ...", "dateLastCrawled": "2021-12-29T09:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Problem statement - 3 - InternshipGitbook", "url": "https://shahyaseen71.gitbook.io/internshipgitbook/data-science-mini-project-task-3/problem-statement", "isFamilyFriendly": true, "displayUrl": "https://shahyaseen71.gitbook.io/internshipgitbook/data-science-mini-project-task-3/...", "snippet": "In <b>machine</b> <b>learning</b>, we are usually concerned with predictive capabilities: we want models that can help us know the likely outcomes of future scenarios. However, it turns out that model predictions on both the training data used to fit the model, and the testing data , which was not used to fit the model, are important for understanding the workings of the model.", "dateLastCrawled": "2022-01-31T21:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "rnn - How to improve LSTM accuracy on multiclass text classification ...", "url": "https://datascience.stackexchange.com/questions/93074/how-to-improve-lstm-accuracy-on-multiclass-text-classification", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/93074/how-to-improve-lstm-accuracy-on...", "snippet": "50% is quite decent because you have five labels and random guessing model would have achieved only 20% accuracy. So you know your model is <b>learning</b> something. The other thing you want to check out is whether this is suited to be a regression problem more than classification. For e.g, misclassifying a 5 (ground truth) into a 4 is better than ...", "dateLastCrawled": "2022-01-22T15:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "MNIST for Beginners - Deeplearning4j: Open-source, Distributed Deep ...", "url": "https://mgubaidullin.github.io/deeplearning4j-docs/mnist-for-beginners", "isFamilyFriendly": true, "displayUrl": "https://mgubaidullin.github.io/deep<b>learning</b>4j-docs/mnist-for-beginners", "snippet": "It is used to benchmark the performance of <b>machine</b> <b>learning</b> algorithms. Deep <b>learning</b> performs quite well on MNIST, achieving more than 99.7% accuracy. We will use MNIST to train a neural network to look at each image and predict the digit. The first step is to install Deeplearning4j. GET STARTED WITH DEEP <b>LEARNING</b> The MNIST Dataset. The MNIST dataset contains a training set of 60,000 examples, and a test set of 10,000 examples. The training set is used to teach the algorithm to predict the ...", "dateLastCrawled": "2022-01-31T16:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Quickstart with MNIST - Deeplearning4j", "url": "https://deeplearning4j.konduit.ai/v/en-1.0.0-beta6/getting-started/tutorials/quickstart-with-mnist", "isFamilyFriendly": true, "displayUrl": "https://deep<b>learning</b>4j.konduit.ai/v/en-1.0.0-beta6/getting-started/tutorials/quick...", "snippet": "Deeplearning4j. Community Forum ND4J Javadoc DL4J Javadoc. Search\u2026", "dateLastCrawled": "2022-01-27T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Newest &#39;lstm&#39; Questions - Page 4 - Data Science Stack Exchange", "url": "https://datascience.stackexchange.com/questions/tagged/lstm?tab=newest&page=4", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/tagged/lstm?tab=newest&amp;page=4", "snippet": "Q&amp;A for Data science professionals, <b>Machine</b> <b>Learning</b> specialists, and those interested in <b>learning</b> more about the field Stack Exchange Network Stack Exchange network consists of 178 Q&amp;A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.", "dateLastCrawled": "2022-01-19T14:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Classifying Sentiment from Text Reviews | by XuanKhanh Nguyen | Towards ...", "url": "https://towardsdatascience.com/classifying-sentiment-from-text-reviews-a2c65ea468d6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/classifying-sentiment-from-text-reviews-a2c65ea468d6", "snippet": "The process of defining <b>hyperparameter is similar</b> to part 1 (as mentioned in 1B). Second, we tried MLP. The hyperparameters used here control the activation functions, the number of hidden layers, and the number of neurons composing the hidden layers. For the number of hidden layers, the size ranges from 1 to 3, as we learned that for most <b>learning</b> tasks, the number of hidden layers for an MLP model is usually optimized for 1 or 2 hidden layers. For the number of neurons per layer, we used ...", "dateLastCrawled": "2021-12-23T08:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Black-Box <b>Optimization with Local Generative Surrogates</b>", "url": "https://proceedings.neurips.cc/paper/2020/file/a878dbebc902328b41dbf02aa87abb58-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/a878dbebc902328b41dbf02aa87abb58-Paper.pdf", "snippet": "synthetic labeled data for various tasks in <b>machine</b> <b>learning</b> [52, 49, 50, 7]. A common challenge is to \ufb01nd optimal parameters of a simulated system in terms of a given objective function, e.g., to optimize a real-world system\u2019s design or ef\ufb01ciency using the simulator as a proxy, or to calibrate a simulator to generate data that match a real-data distribution. A typical simulator optimization problem can be de\ufb01ned as \ufb01nding = argmin x P R(F(x; )), where Ris an objective we Equal ...", "dateLastCrawled": "2022-02-01T16:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Feature Extraction Methods in Quantitative Structure\u2013Activity ...", "url": "https://www.researchgate.net/publication/340914630_Feature_Extraction_Methods_in_Quantitative_Structure-Activity_Relationship_Modeling_A_Comparative_Study", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340914630_Feature_Extraction_Methods_in...", "snippet": "<b>hyperparameter is similar</b> to that of a deep <b>learning</b>. model. A recti\ufb01ed linear unit (ReLU) activation function . was applied. W e experimented with both Adam and. recti\ufb01ed Adam optimizers. The ...", "dateLastCrawled": "2022-01-17T05:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Declar Custom Parameter Pytorch", "url": "https://groups.google.com/g/vapahzok/c/SgV-9NE5p7U", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/vapahzok/c/SgV-9NE5p7U", "snippet": "All groups and messages ... ...", "dateLastCrawled": "2022-01-22T01:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - Grid search or <b>gradient</b> descent? - Data Science ...", "url": "https://datascience.stackexchange.com/questions/62323/grid-search-or-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/62323/grid-search-or-<b>gradient</b>-descent", "snippet": "A <b>hyperparameter can be thought of as</b> something &quot;structural&quot;, e.g. the number of layers, the number of nodes for each layer (notice that these two determine indirectly also the number of parameters, i.e. how many weights and biases there are in our model), i.e. things that do not change during training. Hyperparameters are not confined to the model itself, they are also applicable to the <b>learning</b> algorithm used (e.g. optimization algorithm, <b>learning</b> rate, etc). A specific set of ...", "dateLastCrawled": "2022-01-21T06:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep ...", "url": "https://www.arxiv-vanity.com/papers/1711.02257/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1711.02257", "snippet": "Deep multitask networks, in which one neural network produces multiple predictive outputs, are more scalable and often better regularized than their single-task counterparts. Such advantages can potentially lead to gains in both speed and performance, but multitask networks are also difficult to train without finding the right balance between tasks. We present a novel gradient normalization (GradNorm) technique which automatically balances the multitask loss function by directly tuning the ...", "dateLastCrawled": "2021-10-12T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A General and Adaptive Robust Loss Function - ResearchGate", "url": "https://www.researchgate.net/publication/338511972_A_General_and_Adaptive_Robust_Loss_Function", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338511972_A_General_and_Adaptive_Robust_Loss...", "snippet": "This paper adopts an adaptive robust loss [13], which learns hyper-parameters independently, and reduces the workload of manual tuning. The function form is not only limited to MSE, but also ...", "dateLastCrawled": "2022-01-28T06:09:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(hyperparameter)  is like +(TV settings)", "+(hyperparameter) is similar to +(TV settings)", "+(hyperparameter) can be thought of as +(TV settings)", "+(hyperparameter) can be compared to +(TV settings)", "machine learning +(hyperparameter AND analogy)", "machine learning +(\"hyperparameter is like\")", "machine learning +(\"hyperparameter is similar\")", "machine learning +(\"just as hyperparameter\")", "machine learning +(\"hyperparameter can be thought of as\")", "machine learning +(\"hyperparameter can be compared to\")"]}