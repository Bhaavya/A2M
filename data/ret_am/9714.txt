{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Decoder: Machine learning</b> | Thoughtworks", "url": "https://www.thoughtworks.com/decoder/machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.thoughtworks.com/<b>decoder/machine-learning</b>", "snippet": "\ufeff<b>Machine</b> <b>learning</b> is a subset of artificial intelligence, where computer algorithms will identify patterns in the data and apply those patterns to make predictions. It is widely used today, and is at the heart of things <b>like</b> Netflix\u2019s recommendation engine, Tesla\u2019s self-driving car and speech to text applications.", "dateLastCrawled": "2022-01-20T04:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Decoder: Online machine learning</b> | Thoughtworks", "url": "https://www.thoughtworks.com/decoder/online-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.thoughtworks.com/<b>decoder/online-machine-learning</b>", "snippet": "Online <b>machine</b> <b>learning</b> is a method for training models using a constant stream of real-time data. The <b>model</b> learns as each new individual data point is streamed in, continuously improving the quality and relevance of its outputs. \ufeff. It\u2019s especially useful in use cases where conditions constantly change.", "dateLastCrawled": "2022-01-12T08:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>a decoder in machine learning? - Quora</b>", "url": "https://www.quora.com/What-is-a-decoder-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>a-decoder-in-machine-learning</b>", "snippet": "Answer: Encoder and <b>decoder</b> aren\u2019t just relevant in <b>machine</b> <b>learning</b> but you might also frequently notice these terms elsewhere. To truly understand the significance of encoder and <b>decoder</b> in the <b>machine</b> <b>learning</b> context, it\u2019s important to understand what they actually mean. Simply put, an encod...", "dateLastCrawled": "2022-01-23T15:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Encoder-<b>Decoder</b> Recurrent Neural Network Models for Neural <b>Machine</b> ...", "url": "https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/encoder-<b>decoder</b>-recurrent-neura", "snippet": "Cho NMT <b>Model</b>. In this section, we will look at the neural <b>machine</b> translation system described by Kyunghyun Cho, et al. in their 2014 paper titled \u201c<b>Learning</b> Phrase Representations using RNN Encoder\u2013<b>Decoder</b> for Statistical <b>Machine</b> Translation.\u201dWe will refer to it as the \u201cCho NMT <b>Model</b>\u201d <b>model</b> for lack of a better name. Importantly, the Cho <b>Model</b> is used only to score candidate translations and is not used directly for translation <b>like</b> the Sutskever <b>model</b> above.", "dateLastCrawled": "2022-02-02T17:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The encoder-<b>decoder</b> <b>model</b> as a dimensionality reduction technique | A ...", "url": "https://ekamperi.github.io/machine%20learning/2021/01/21/encoder-decoder-model.html", "isFamilyFriendly": true, "displayUrl": "https://ekamperi.github.io/<b>machine</b> <b>learning</b>/2021/01/21/encoder-<b>decoder</b>-<b>model</b>.html", "snippet": "An encoder-<b>decoder</b> network is an unsupervised artificial neural <b>model</b> that consists of an encoder component and a <b>decoder</b> one (duh!). The encoder takes the input and transforms it into a compressed encoding, handed over to the <b>decoder</b>. The <b>decoder</b> strives to reconstruct the original representation as close as possible. Ultimately, the goal is to learn a representation (read: encoding) for a dataset. In a sense, we force the AE to memorize the training data by devising some mnemonic rule of ...", "dateLastCrawled": "2022-02-01T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>seq2seq model in Machine Learning - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>seq2seq-model-in-machine-learning</b>", "snippet": "It mainly has two components i.e encoder and <b>decoder</b>, and hence sometimes it is called the Encoder-<b>Decoder</b> Network. ... We make buckets of different sizes <b>like</b> (4, 8) (8, 15), and so on, where 4 is the max input length defined by us and 8 is the max output length defined. My Personal Notes arrow_drop_up. Save. <b>Like</b>. Previous. Python | Tensorflow cos() method. Next . Program to print the given Z Pattern. Recommended Articles. Page : <b>Learning</b> <b>Model</b> Building in Scikit-learn : A Python <b>Machine</b> ...", "dateLastCrawled": "2022-01-25T19:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Encoder-<b>Decoder</b> <b>Seq2Seq</b> Models, Clearly Explained!! | by Kriz Moses ...", "url": "https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/encoder-<b>decoder</b>-<b>seq2seq</b>-<b>models</b>-clearly-explained-c...", "snippet": "The paper Sequence to Sequence <b>Learning</b> with Neural Networks by Ilya Sutskever, et al was one of the pioneer papers introducing the encoder-<b>decoder</b> <b>model</b> for <b>machine</b> translation and in general ...", "dateLastCrawled": "2022-02-03T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Tutorial on Sequential <b>Machine</b> <b>Learning</b>", "url": "https://analyticsindiamag.com/a-tutorial-on-sequential-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/a-tutorial-on-sequential-<b>machine</b>-<b>learning</b>", "snippet": "The input and output domain of the seq2seq <b>model</b> is different <b>like</b> (English-Hindi) and used mostly in <b>machine</b> translation applications. Whereas the Autoencoder is a special case of the seq2seq <b>model</b> where both input and output domain are the same (English-English), it behaves <b>like</b> auto-association means it perfectly recalls or rebuilds the input sequence if we pass a corrupted sequence. Features <b>like</b> this have leveraged autoencoder in many applications <b>like</b> pattern compilation etc.", "dateLastCrawled": "2022-02-02T16:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Develop a Seq2Seq <b>Model for Neural Machine Translation in Keras</b>", "url": "https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/define-encoder-<b>decoder</b>-sequence-sequence-<b>model</b>...", "snippet": "The encoder-<b>decoder</b> <b>model</b> provides a pattern for using recurrent neural networks to address challenging sequence-to-sequence prediction problems, such as <b>machine</b> translation. Encoder-<b>decoder</b> models can be developed in the Keras Python deep <b>learning</b> library and an example of a neural <b>machine</b> translation system developed with this <b>model</b> has been described on the Keras blog, with sample code distributed with the Keras project. In", "dateLastCrawled": "2022-02-03T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Essential Guide to <b>Transformer</b> Models in <b>Machine</b> <b>Learning</b> | HackerNoon", "url": "https://hackernoon.com/essential-guide-to-transformer-models-in-machine-learning-dzz3tk8", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/essential-guide-to-<b>transformer</b>-<b>models</b>-in-<b>machine</b>-<b>learning</b>-dzz3tk8", "snippet": "An Essential Guide to <b>Transformer</b> Models in <b>Machine</b> <b>Learning</b>. <b>Transformer</b> models have become the defacto standard for NLP tasks. But for such a useful <b>model</b>, transformers are still very difficult to understand. It took me multiple readings of the Google research paper first introducing transformers to really understand how transformers work. I\u2019ll try to keep the jargon and the technicality to a minimum, but do keep in mind that this topic is complicated. In a <b>machine</b> translation ...", "dateLastCrawled": "2022-02-02T16:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>seq2seq model in Machine Learning - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>seq2seq-model-in-machine-learning</b>", "snippet": "<b>Decoder</b>: It <b>is similar</b> to the encoder. It takes as input the hidden vector generated by the encoder, its own hidden states, and the current word to produce the next hidden vector and finally predict the next word. Apart from these two, many optimizations have to lead to other components of seq2seq: Attention: The input to the <b>decoder</b> is a single vector that has to store all the information about the context. This becomes a problem with large sequences. Hence the attention mechanism is ...", "dateLastCrawled": "2022-01-25T19:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Graph <b>Representation Learning</b> \u2014 The Encoder-<b>Decoder</b> <b>Model</b> (Part 2) | by ...", "url": "https://towardsdatascience.com/graph-representation-learning-the-encoder-decoder-model-part-2-ed8b505af447", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/graph-<b>representation-learning</b>-the-encoder-<b>decoder</b>-<b>model</b>...", "snippet": "<b>Machine</b> <b>learning</b> on graphs: A <b>model</b> and comprehensive taxonomy. arXiv preprint arXiv:2005.03675. The goal of the encoder is to translate the features of a data point into a low-dimensional representation. The <b>decoder</b> takes as input this representation and tries to reconstruct the original data. The performance of this architecture increases if ...", "dateLastCrawled": "2022-02-02T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Transformer <b>Model</b>", "url": "https://machinelearningmastery.com/the-transformer-model/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/the-transformer-<b>model</b>", "snippet": "The <b>Decoder</b>; Sum Up: The Transformer <b>Model</b>; Comparison to Recurrent and Convolutional Layers; Prerequisites. For this tutorial, we assume that you are already familiar with: The concept of attention; The attention mechanism; The Transfomer attention mechanism; The Transformer Architecture. The Transformer architecture follows an encoder-<b>decoder</b> structure, but does not rely on recurrence and convolutions in order to generate an output. The Encoder-<b>Decoder</b> Structure of the Transformer ...", "dateLastCrawled": "2022-01-27T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "CLIP vs Vision Language Pre-training Vs VisionEncoderDecoder", "url": "https://analyticsindiamag.com/clip-vs-vision-language-pre-training-vs-visionencoderdecoder/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/clip-vs-vision-language-pre-training-vs-visionencoder<b>decoder</b>", "snippet": "After such a Vision-Encoder-Text-<b>Decoder</b> <b>model</b> has been trained or fine-tuned, it can be saved/loaded just like any other <b>model</b>. VLP (Vision Language Pre-training) Mixed-modal frame. Damodaran says that Unified VLP models are typically pre-trained on a large number of image-text pairs with \u201ccreative\u201d self-supervised objectives and loss ...", "dateLastCrawled": "2022-02-01T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Encoder-<b>Decoder</b> Recurrent Neural Network Models for Neural <b>Machine</b> ...", "url": "https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/encoder-<b>decoder</b>-recurrent-neura", "snippet": "Sutskever NMT <b>Model</b>. In this section, we will look at the neural <b>machine</b> translation <b>model</b> developed by Ilya Sutskever, et al. as described in their 2014 paper \u201cSequence to Sequence <b>Learning</b> with Neural Networks\u201c. We will refer to it as the \u201cSutskever NMT <b>Model</b>\u201c, for lack of a better name. This is an important paper as it was one of the first to introduce the Encoder-<b>Decoder</b> <b>model</b> for <b>machine</b> translation and more generally sequence-to-sequence <b>learning</b>.", "dateLastCrawled": "2022-02-02T17:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Time Series</b> Forecasting with Deep <b>Learning</b> and <b>Attention</b> Mechanism | by ...", "url": "https://towardsdatascience.com/time-series-forecasting-with-deep-learning-and-attention-mechanism-2d001fc871fc", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>time-series</b>-forecasting-with-deep-<b>learning</b>-and...", "snippet": "The encoder operation is very <b>similar</b> to the same operation of the Encoder-<b>Decoder</b> <b>model</b>. At each time step, the representation of each input sequence is computed as a function of the hidden state of the previous time step and of the current input. The final hidden state contains all the encoded information from the previous hidden representations and the previous inputs.", "dateLastCrawled": "2022-02-03T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - Auto-Encoder/<b>Decoder</b> - Generic Swapping <b>Model</b> - Data ...", "url": "https://datascience.stackexchange.com/questions/107454/auto-encoder-decoder-generic-swapping-model", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/.../auto-encoder-<b>decoder</b>-generic-swapping-<b>model</b>", "snippet": "Data Science Stack Exchange is a question and answer site for Data science professionals, <b>Machine</b> <b>Learning</b> specialists, and those interested in <b>learning</b> more about the field. It only takes a minute to sign up. Sign up to join this community", "dateLastCrawled": "2022-02-01T00:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Encoders and <b>Decoders for Neural Machine Translation</b> | <b>Pluralsight</b>", "url": "https://www.pluralsight.com/guides/encoders-and-decoders-for-neural-machine-translation", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pluralsight.com</b>/guides/encoders-and-<b>decoders-for-neural-machine-translation</b>", "snippet": "It applies a <b>similar</b> token at the end. The first output word is generated by running the stacked LSTM layers. A SoftMax activation function applies to the last layer. Its job is to introduce non-linearity in the network. Now this word is passed through the remaining layers and the generation sequence is repeated. Multiple factors depend upon improving the accuracy of the encoder-<b>decoder</b> <b>model</b>. The hyper-parameters such as optimizers, cross-entropy loss, <b>learning</b> rate, etc., play an important ...", "dateLastCrawled": "2022-01-27T02:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Image similarity model</b>. Finding top N <b>similar</b> images on a given\u2026 | by ...", "url": "https://medium.com/analytics-vidhya/image-similarity-model-6b89a22e2f1a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>image-similarity-model</b>-6b89a22e2f1a", "snippet": "Splitting up the data is mainly useful for the hyperparameter tuning part of <b>machine</b> <b>learning</b>. As every task of ML/DL plays a key role in <b>model</b> training and to make our <b>model</b> fairly well on test ...", "dateLastCrawled": "2022-01-28T16:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Image <b>Similarity</b> Search in PyTorch | by Aditya Oke - Medium", "url": "https://medium.com/pytorch/image-similarity-search-in-pytorch-1a744cf3469", "isFamilyFriendly": true, "displayUrl": "https://medium.com/pytorch/image-<b>similarity</b>-search-in-pytorch-1a744cf3469", "snippet": "Consider each point as Feature Representation. Recalling our <b>machine</b> <b>learning</b> basics, one way of finding these is using K-Nearest Neighbors ! Where \u201cK\u201d is the number of <b>similar</b> images the user ...", "dateLastCrawled": "2022-02-02T23:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Decoder: Machine learning</b> | Thoughtworks", "url": "https://www.thoughtworks.com/decoder/machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.<b>thought</b>works.com/<b>decoder/machine-learning</b>", "snippet": "<b>Machine</b> <b>learning</b> is highly dependent on the quality of training data. If there are flaws in your data set, the models may learn to make poor decisions or <b>can</b> drive unethical outcomes by exploiting the inherent bias in your data sets. Even more concerning, many current data sets have been shown to contain bias. If these are used to train models ...", "dateLastCrawled": "2022-01-20T04:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Encoder-<b>Decoder</b> <b>Seq2Seq</b> Models, Clearly Explained!! | by Kriz Moses ...", "url": "https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/encoder-<b>decoder</b>-<b>seq2seq</b>-<b>models</b>-clearly-explained-c...", "snippet": "At a very high level, an encoder-<b>decoder</b> <b>model</b> <b>can</b> <b>be thought</b> of as two blocks, the encoder and the <b>decoder</b> connected by a vector which we will refer to as the \u2018context vector\u2019. Image by author", "dateLastCrawled": "2022-02-03T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> Translation(<b>Encoder-Decoder</b> <b>Model</b>)! | by Shreya Srivastava ...", "url": "https://medium.com/analytics-vidhya/machine-translation-encoder-decoder-model-7e4867377161", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>machine</b>-translation-<b>encoder-decoder</b>-<b>model</b>-7e4867377161", "snippet": "<b>Decoder</b> LSTM at training. The initial states (ho, co) of the <b>decoder</b> is set to the final states of the encoder. It <b>can</b> <b>be thought</b> of as that the <b>decoder</b> is trained to generate the output based on ...", "dateLastCrawled": "2022-02-01T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Decoder: Online machine learning</b> | Thoughtworks", "url": "https://www.thoughtworks.com/decoder/online-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.<b>thought</b>works.com/<b>decoder/online-machine-learning</b>", "snippet": "Online <b>machine</b> <b>learning</b> is a method for training models using a constant stream of real-time data. The <b>model</b> learns as each new individual data point is streamed in, continuously improving the quality and relevance of its outputs. It\u2019s especially useful in use cases where conditions constantly change. For example, if you trained a <b>model</b> to ...", "dateLastCrawled": "2022-01-12T08:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The encoder-<b>decoder</b> <b>model</b> as a dimensionality reduction technique | A ...", "url": "https://ekamperi.github.io/machine%20learning/2021/01/21/encoder-decoder-model.html", "isFamilyFriendly": true, "displayUrl": "https://ekamperi.github.io/<b>machine</b> <b>learning</b>/2021/01/21/encoder-<b>decoder</b>-<b>model</b>.html", "snippet": "An encoder-<b>decoder</b> network is an unsupervised artificial neural <b>model</b> that consists of an encoder component and a <b>decoder</b> one (duh!). The encoder takes the input and transforms it into a compressed encoding, handed over to the <b>decoder</b>. The <b>decoder</b> strives to reconstruct the original representation as close as possible. Ultimately, the goal is to learn a representation (read: encoding) for a dataset. In a sense, we force the AE to memorize the training data by devising some mnemonic rule of ...", "dateLastCrawled": "2022-02-01T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>a decoder in machine learning? - Quora</b>", "url": "https://www.quora.com/What-is-a-decoder-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>a-decoder-in-machine-learning</b>", "snippet": "Answer: Encoder and <b>decoder</b> aren\u2019t just relevant in <b>machine</b> <b>learning</b> but you might also frequently notice these terms elsewhere. To truly understand the significance of encoder and <b>decoder</b> in the <b>machine</b> <b>learning</b> context, it\u2019s important to understand what they actually mean. Simply put, an encod...", "dateLastCrawled": "2022-01-23T15:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to Develop a Seq2Seq <b>Model for Neural Machine Translation in Keras</b>", "url": "https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/define-encoder-<b>decoder</b>-sequence-sequence-<b>model</b>...", "snippet": "The encoder-<b>decoder</b> <b>model</b> provides a pattern for using recurrent neural networks to address challenging sequence-to-sequence prediction problems, such as <b>machine</b> translation. Encoder-<b>decoder</b> models <b>can</b> be developed in the Keras Python deep <b>learning</b> library and an example of a neural <b>machine</b> translation system developed with this <b>model</b> has been described on the Keras blog, with sample code distributed with the Keras project. In", "dateLastCrawled": "2022-02-03T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Importance Of <b>Thought</b> Vector In Seq2seq <b>Model</b> | by Aditya Mohanty | Medium", "url": "https://adityaroc.medium.com/importance-of-thought-vector-in-seq2seq-model-407f1abb4da4", "isFamilyFriendly": true, "displayUrl": "https://adityaroc.medium.com/importance-of-<b>thought</b>-vector-in-seq2seq-<b>model</b>-407f1abb4da4", "snippet": "The crux of every seq2seq <b>model</b> is a <b>thought</b> vector which is also known as context vector. Here we would discuss in detail about the importance of <b>thought</b> vector. What Is Encoder-<b>Decoder</b> Architecture: Let us say we are handling a problem of <b>machine</b> transla t ion. Here we would need encoder <b>decoder</b> architecture. The first half of the encoder-<b>decoder</b> architecture is an encoder which would turn the nlp text into a lower dimensional form. This lower dimensional form is known as <b>thought</b> vector ...", "dateLastCrawled": "2022-01-11T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sentiment Analysis of Tweets using BERT - Thinking Neuron", "url": "https://thinkingneuron.com/sentiment-analysis-of-tweets-using-bert/", "isFamilyFriendly": true, "displayUrl": "https://thinkingneuron.com/sentiment-analysis-of-tweets-using-bert", "snippet": "BERT stands for Bidirectional Encoder Representations from Transformers and it is a state-of-the-art <b>machine</b> <b>learning</b> <b>model</b> used for NLP tasks like text classification, sentiment analysis, text summarization, etc. Okay\u2026 so what is Bidirectional? What are Encoder Representations? And what is Transformer??!! Let\u2019s trace it back one step at a time! There is a lot that happened before BERT. Each iteration improved the previous solutions. To put the timeline in perspective. Below is the order ...", "dateLastCrawled": "2022-02-02T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Essential Guide to <b>Transformer</b> Models in <b>Machine</b> <b>Learning</b> | HackerNoon", "url": "https://hackernoon.com/essential-guide-to-transformer-models-in-machine-learning-dzz3tk8", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/essential-guide-to-<b>transformer</b>-<b>models</b>-in-<b>machine</b>-<b>learning</b>-dzz3tk8", "snippet": "An Essential Guide to <b>Transformer</b> Models in <b>Machine</b> <b>Learning</b>. <b>Transformer</b> models have become the defacto standard for NLP tasks. But for such a useful <b>model</b>, transformers are still very difficult to understand. It took me multiple readings of the Google research paper first introducing transformers to really understand how transformers work. I\u2019ll try to keep the jargon and the technicality to a minimum, but do keep in mind that this topic is complicated. In a <b>machine</b> translation ...", "dateLastCrawled": "2022-02-02T16:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Decoder: Online machine learning</b> | Thoughtworks", "url": "https://www.thoughtworks.com/decoder/online-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.thoughtworks.com/<b>decoder/online-machine-learning</b>", "snippet": "The <b>model</b> learns as each new individual data point is streamed in, continuously improving the quality and relevance of its outputs. \ufeff It\u2019s especially useful in use cases where conditions constantly change. For example, if you trained a <b>model</b> to understand customer sentiment about your products using a fixed set of data, it wouldn\u2019t be able to accurately infer meaning from discussions about new products, or from new topics of conversation among customers. Online <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-12T08:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Encoder-<b>Decoder</b> Recurrent Neural Network Models for Neural <b>Machine</b> ...", "url": "https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/encoder-<b>decoder</b>-recurrent-neura", "snippet": "Sutskever NMT <b>Model</b>. In this section, we will look at the neural <b>machine</b> translation <b>model</b> developed by Ilya Sutskever, et al. as described in their 2014 paper \u201cSequence to Sequence <b>Learning</b> with Neural Networks\u201c. We will refer to it as the \u201cSutskever NMT <b>Model</b>\u201c, for lack of a better name. This is an important paper as it was one of the first to introduce the Encoder-<b>Decoder</b> <b>model</b> for <b>machine</b> translation and more generally sequence-to-sequence <b>learning</b>.", "dateLastCrawled": "2022-02-02T17:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "An empirical comparison of neural networks and <b>machine</b> <b>learning</b> ...", "url": "https://www.nature.com/articles/s41598-020-60932-4/", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-020-60932-4", "snippet": "Linear Regression is one of the most basic <b>machine</b> <b>learning</b> methods typically used to <b>model</b> the predictive relationship between the dependent target variable to multiple explanatory variables 27 ...", "dateLastCrawled": "2022-02-02T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Encoder-Decoder Deep Learning Models for Text Summarization</b>", "url": "https://machinelearningmastery.com/encoder-decoder-deep-learning-models-text-summarization/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>encoder-decoder-deep-learning-models</b>-text-summarization", "snippet": "The <b>model</b> shows significant performance gains on the DUC-2004 shared task <b>compared</b> with several strong baselines. IBM <b>Model</b> . This approach was described by Ramesh Nallapati, et al. from IBM Watson in their 2016 paper \u201cAbstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\u201c. The approach is based on the encoder-<b>decoder</b> recurrent neural network with attention, developed for <b>machine</b> translation. Our baseline <b>model</b> corresponds to the neural <b>machine</b> translation <b>model</b> used ...", "dateLastCrawled": "2022-02-02T13:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Graph <b>Representation Learning</b> \u2014 The Encoder-<b>Decoder</b> <b>Model</b> (Part 2) | by ...", "url": "https://towardsdatascience.com/graph-representation-learning-the-encoder-decoder-model-part-2-ed8b505af447", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/graph-<b>representation-learning</b>-the-encoder-<b>decoder</b>-<b>model</b>...", "snippet": "<b>Machine</b> <b>learning</b> on graphs: A <b>model</b> and comprehensive ... Therefore, the <b>learning</b> goal for an encoder-<b>decoder</b> architecture is to minimize information loss during the reconstruction process. In the case of networks, this reconstruction process intends to restore the original graph structure (unsupervised setting) or to achieve a representation to address a specific task, such as a node or edge classification (supervised setting). <b>Compared</b> to existing research works, GraphEDM defines an end-to ...", "dateLastCrawled": "2022-02-02T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning</b> phrase representation using <b>RNN Encoder</b>-<b>Decoder</b> for <b>Machine</b> ...", "url": "https://medium.com/@gautam.karmakar/learning-phrase-representation-using-rnn-encoder-decoder-for-machine-translation-9171cd6a6574", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@gautam.karmakar/<b>learning</b>-phrase-representation-using-<b>rnn-encoder</b>...", "snippet": "Encoder-<b>decoder</b> <b>model</b> has been proved more robust and scalable <b>compared</b> to statistical <b>machine</b> translation <b>model</b> such as CSLM. They have <b>compared</b> RNN and also RNN + CSLM <b>model</b> and <b>compared</b> result ...", "dateLastCrawled": "2022-01-18T19:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine translation</b> with the <b>seq2seq</b> <b>model</b>: Different approaches | by ...", "url": "https://towardsdatascience.com/machine-translation-with-the-seq2seq-model-different-approaches-f078081aaa37", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-translation</b>-with-the-<b>seq2seq</b>-<b>model</b>-different...", "snippet": "T here are two approaches we <b>can</b> take when doing <b>machine translation</b>. We will discuss them in the upcoming sections. Introduction to the <b>seq2seq</b> approach for <b>Machine translation</b> . The <b>seq2seq</b> <b>model</b> also called the encoder-<b>decoder</b> <b>model</b> uses Long Short Term Memory- LSTM for text generation from the training corpus. The <b>seq2seq</b> <b>model</b> is also useful in <b>machine translation</b> applications. What does the <b>seq2seq</b> or encoder-<b>decoder</b> <b>model</b> do in simple words? It predicts a word given in the user input ...", "dateLastCrawled": "2022-02-03T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Essential Guide to <b>Transformer</b> Models in <b>Machine</b> <b>Learning</b> | HackerNoon", "url": "https://hackernoon.com/essential-guide-to-transformer-models-in-machine-learning-dzz3tk8", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/essential-guide-to-<b>transformer</b>-<b>models</b>-in-<b>machine</b>-<b>learning</b>-dzz3tk8", "snippet": "Since, our <b>model</b> contains no recurrence and no convolution, in order for the <b>model</b> to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \u201cpositional encodings\u201d to the input embeddings at the bottoms of both the encoder and <b>decoder</b> stacks (as we will see later). The positional encodings need to have the same dimension, D, as the embeddings have so that the two <b>can</b> be summed.", "dateLastCrawled": "2022-02-02T16:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python - How do Encoder/<b>Decoder</b> models learn in Deep <b>Learning</b>? - Stack ...", "url": "https://stackoverflow.com/questions/50524927/how-do-encoder-decoder-models-learn-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/50524927", "snippet": "After <b>learning</b> a bit about encoder/<b>decoder</b> models in deep <b>learning</b> (mostly in Keras), i still cannot understand where the <b>learning</b> takes place. Does the encoder just create the feature map and then the <b>decoder</b> tries to get as close as possible as the result with BackProp, or does the encoder learn as well when the <b>model</b> is trained?", "dateLastCrawled": "2022-01-08T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep <b>learning</b> - What are the inputs to the first <b>decoder</b> layer in a ...", "url": "https://datascience.stackexchange.com/questions/88981/what-are-the-inputs-to-the-first-decoder-layer-in-a-transformer-model-during-the", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/88981/what-are-the-inputs-to-the-first...", "snippet": "The output of the <b>decoder</b> will <b>be compared</b> against this in the training. The input to the <b>decoder</b> would be &lt;start&gt; I am fine &lt;EOS&gt;. Notice that the input to the <b>decoder</b> is the target sequence shifted one position to the right by the token that signals the beginning of the sentence. The logic of this is that the output at each position should ...", "dateLastCrawled": "2022-01-31T13:52:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Dynamical machine learning volumetric reconstruction of</b> objects ...", "url": "https://www.nature.com/articles/s41377-021-00512-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41377-021-00512-x", "snippet": "The encoder and <b>decoder</b> also utilize separable convolution, in conjunction with residual <b>learning</b>, which is known to improve generalization in deep networks 90.", "dateLastCrawled": "2022-02-02T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "9.6. <b>Encoder-Decoder</b> Architecture \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/<b>encoder-decoder</b>.html", "snippet": "<b>Encoder-Decoder</b> Architecture \u2014 Dive into Deep <b>Learning</b> 0.17.0 documentation. 9.6. <b>Encoder-Decoder</b> Architecture. As we have discussed in Section 9.5, <b>machine</b> translation is a major problem domain for sequence transduction models, whose input and output are both variable-length sequences. To handle this type of inputs and outputs, we can design ...", "dateLastCrawled": "2022-01-30T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Neural networks? <b>Machine learning</b>? Here&#39;s your secret <b>decoder</b> for A.I ...", "url": "https://www.digitaltrends.com/cool-tech/types-of-artificial-intelligence/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.digitaltrends.com</b>/cool-tech/types-of-artificial-intelligence", "snippet": "Reinforcement <b>Learning</b>. Reinforcement <b>learning</b> is another flavor of <b>machine learning</b>. It\u2019s heavily inspired by behaviorist psychology, and is based around the idea that software agent can learn ...", "dateLastCrawled": "2022-01-19T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning</b> to Generate Long-term Future via Hierarchical Prediction", "url": "https://proceedings.mlr.press/v70/villegas17a.html", "isFamilyFriendly": true, "displayUrl": "https://proceedings.mlr.press/v70/villegas17a.html", "snippet": "Our model is built with a combination of LSTM and <b>analogy</b> based encoder-<b>decoder</b> convolutional neural networks, which independently predict the video structure and generate the future frames, respectively. In experiments, our model is evaluated on the Human3.6M and Penn Action datasets on the task of long-term pixel-level video prediction of humans performing actions and demonstrate significantly better results than the state-of-the-art.} } Copy to Clipboard Download. Endnote %0 Conference ...", "dateLastCrawled": "2021-11-21T21:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding and Improving Morphological <b>Learning</b> in the Neural ...", "url": "https://aclanthology.org/I17-1015/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/I17-1015", "snippet": "End-to-end training makes the neural <b>machine</b> translation (NMT) architecture simpler, yet elegant compared to traditional statistical <b>machine</b> translation (SMT). However, little is known about linguistic patterns of morphology, syntax and semantics learned during the training of NMT systems, and more importantly, which parts of the architecture are responsible for <b>learning</b> each of these phenomenon. In this paper we i) analyze how much morphology an NMT <b>decoder</b> learns, and ii) investigate ...", "dateLastCrawled": "2022-01-18T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep Visual <b>Analogy</b>-Making - NeurIPS", "url": "https://proceedings.neurips.cc/paper/2015/file/e07413354875be01a996dc560274708e-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2015/file/e07413354875be01a996dc560274708e-Paper.pdf", "snippet": "analogies), but our model leverages a deep encoder and <b>decoder</b> trainable by backprop. Memisevic and Hinton [19] proposed the Factored Gated Boltzmann <b>Machine</b> for <b>learning</b> to repre-sent transformations between pairs of images. This and related models [25,8,20] use 3-way tensors", "dateLastCrawled": "2021-12-18T19:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The conceptual arithmetics of concepts | by Assaad MOAWAD | DataThings ...", "url": "https://medium.com/datathings/the-conceptual-arithmetics-of-concepts-369df29e4e0f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/datathings/the-conceptual-arithmetics-of-concepts-369df29e4e0f", "snippet": "<b>Machine</b> <b>learning</b> field is an amazing and very fast evolving domain. However, it is still hard to use it in its current state due to its cost and complexity. With time, we will have more and more ...", "dateLastCrawled": "2022-01-04T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Unlocking <b>Drug Discovery</b> With <b>Machine</b> <b>Learning</b> | by Joey Mach | Towards ...", "url": "https://towardsdatascience.com/unlocking-drug-discovery-through-machine-learning-part-1-8b2a64333e07", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/unlocking-<b>drug-discovery</b>-through-<b>machine</b>-<b>learning</b>-part...", "snippet": "Accelerating <b>drug discovery</b> by leveraging <b>machine</b> <b>learning</b> to generate and create retro-synthesis pathways for molecules. Joey Mach . Nov 23, 2019 \u00b7 17 min read. The way we discover drugs is EXTREMELY inefficient. Something needs to be done. Despite all the innovation that is happening in the pharmaceutical industry recently, especially in the cancer research space, there\u2019s still a huge gap for improvement! Our current approach to <b>drug discovery</b> hasn\u2019t changed much since the 1920s. This ...", "dateLastCrawled": "2022-02-02T22:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Dive into Deep <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/", "isFamilyFriendly": true, "displayUrl": "d2l.ai", "snippet": "Dive into Deep <b>Learning</b>. Interactive deep <b>learning</b> book with code, math, and discussions. Implemented with NumPy/MXNet, PyTorch, and TensorFlow. Adopted at 200 universities from 50 countries.", "dateLastCrawled": "2022-01-30T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "Encoder-<b>Decoder</b> Attention: Attention between the input sequence and the output sequence. ... If you are looking for an <b>analogy</b> between self attention and attention, think of z serving the purpose of context vectors and not global alignment weights. The <b>Transformer</b>. \u26a0\ufe0f A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2) (left) The ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "lec11.pdf - CSC321 Neural Networks and <b>Machine</b> <b>Learning</b> Lecture 11 ...", "url": "https://www.coursehero.com/file/102398699/lec11pdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/102398699/lec11pdf", "snippet": "View lec11.pdf from CS 102 at Pacific Northwest College Of Art. CSC321 Neural Networks and <b>Machine</b> <b>Learning</b> Lecture 11 March 25, 2020 Agenda I I I Deep Residual Networks (CNN) Attention", "dateLastCrawled": "2022-02-01T20:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Dynamic modeling <b>for NOx emission sequence prediction of SCR</b> system ...", "url": "https://www.sciencedirect.com/science/article/pii/S0360544219321772", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0360544219321772", "snippet": "After analyzing the structure of the decoder, I consider this trend is caused by the accumulation of errors. <b>Decoder is like</b> a chain, as shown in Fig. 3. The true value at time t is input to the decoder, for predicting the value at time t+1. After that, the prediction value at time t+1 is input to the decoder, for predicting the value at time t+2.", "dateLastCrawled": "2021-12-08T01:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Nuclear imaging and artificial intelligence</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/B9780128202739000117", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/B9780128202739000117", "snippet": "Any <b>machine</b> <b>learning</b> method that inputs features, hand-engineered by a domain expert from raw data, is considered traditional <b>machine</b> <b>learning</b>. As such, models that input structured or tabular data fall into this category. Specifically, models that are not deep neural networks also fall into this category, like support vector machines (SVMs), decision tree-based ensemble methods, and shallow artificial neural networks. Linear regression and logistic regression, borrowed from statistics, are ...", "dateLastCrawled": "2021-10-14T13:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Protobuf Parsing in Python</b> | Datadog", "url": "https://www.datadoghq.com/blog/engineering/protobuf-parsing-in-python/", "isFamilyFriendly": true, "displayUrl": "https://www.datadoghq.com/blog/engineering/<b>protobuf-parsing-in-python</b>", "snippet": "<b>Machine</b> <b>Learning</b>; Real-Time BI; On-Premises Monitoring; Log Analysis &amp; Correlation; Docs About. Contact ... It is designed to be used for inter-<b>machine</b> communication and remote procedure calls (RPC). This can be used in many different situations, including payloads for HTTP APIs. To get started you need to learn a simple language that is used to describe how your data is shaped, but once done a variety of programming languages can be used to easily read and write Protobuf messages - let\u2019s ...", "dateLastCrawled": "2022-02-02T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Data Cleaning</b> | HackerNoon", "url": "https://hackernoon.com/data-cleaning-3c3e37f358dc", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/<b>data-cleaning</b>-3c3e37f358dc", "snippet": "In general, you\u2019ll only want to normalize your data if you\u2019re going to be using a <b>machine</b> <b>learning</b> or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \u201cGaussian\u201d in the name probably assumes normality.)", "dateLastCrawled": "2022-01-29T03:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "AI Supercomputing (part 2): Limitations, Encoder-Decoder, Transformers ...", "url": "https://brunomaga.github.io/AI-Supercomputing-2", "isFamilyFriendly": true, "displayUrl": "https://brunomaga.github.io/AI-Supercomputing-2", "snippet": "The Masked Multi-head Attention component on the <b>decoder is similar</b> to the regular MHA, but replaces the diagonal of the attention mechanism matrix by zeros, to hide next word from the model. Decoding is performed with a word of the output sequence of a time, with previously seen words added to the attention array, and the following words set to zero. Applied to the previous example, the four iterations are: Input of the masked attention mechanism on the decoder for the sentence &quot;Le gros ...", "dateLastCrawled": "2022-01-04T09:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Building a Convolutional VAE in <b>PyTorch</b> | by Ta-Ying Cheng | Towards ...", "url": "https://towardsdatascience.com/building-a-convolutional-vae-in-pytorch-a0f54c947f71", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/building-a-convolutional-vae-in-<b>pytorch</b>-a0f54c947f71", "snippet": "Decoder \u2014 The <b>decoder is similar</b> to the traditional autoencoders, ... How to Use UX to Make <b>Machine</b>-<b>Learning</b> Systems More Effective. Redd Experience Design in Human Friendly [Notes] (Ir)Reproducible <b>Machine</b> <b>Learning</b>: A Case Study. Ceshine Lee. How Cimpress Delivers Cloud Inference for its Image Processing Services. Mike O&#39;Brien in Apache MXNet. Use C# and ML.NET <b>Machine</b> <b>Learning</b> To Predict Taxi Fares In New York. Mark Farragher . And of course, LSTM - Part II. Eniola Alese in ExplainingML ...", "dateLastCrawled": "2022-02-02T03:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>ML-descent: an optimization algorithm for FWI using</b> <b>machine</b> <b>learning</b>", "url": "https://www.researchgate.net/publication/336022842_ML-descent_an_optimization_algorithm_for_FWI_using_machine_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336022842_ML-descent_an_optimization...", "snippet": "The <b>decoder is similar</b> to the enco der, but in a ... Active <b>learning</b> is a <b>machine</b> <b>learning</b> ap- proach to achieving high-accuracy with a small amount of labels by letting the learn- ing algorithm ...", "dateLastCrawled": "2021-08-22T22:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How to use the <b>Transformer</b> for Audio Classification | by Facundo Deza ...", "url": "https://codeburst.io/how-to-use-transformer-for-audio-classification-5f4bc0d0c1f0", "isFamilyFriendly": true, "displayUrl": "https://codeburst.io/how-to-use-<b>transformer</b>-for-audio-classification-5f4bc0d0c1f0", "snippet": "For the <b>decoder is similar</b> but it has two differences: The input of the decoder is masked, this avoids the decoder to see the \u201cfuture\u201d and ; It has two Multi-Head Attention in a row before going to a position-wise fully connected feed-forward network. One of them is for the encoder output and the other one for decoder input. Finally, after the last residual connection and layer normalization, the output of the decoder goes through a linear projection and then a softmax, which gets the ...", "dateLastCrawled": "2022-01-26T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "8.14. <b>Sequence to Sequence</b> \u2014 Dive into Deep <b>Learning</b> 0.7 documentation", "url": "https://classic.d2l.ai/chapter_recurrent-neural-networks/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://classic.d2l.ai/chapter_recurrent-neural-networks/seq2seq.html", "snippet": "In this section we will implement the seq2seq model to train on the <b>machine</b> translation dataset. ... The forward calculation of the <b>decoder is similar</b> to the encoder\u2019s. The only difference is we add a dense layer with the hidden size to be the vocabulary size to output the predicted confidence score for each word. # Save to the d2l package. class Seq2SeqDecoder (d2l. Decoder): def __init__ (self, vocab_size, embed_size, num_hiddens, num_layers, dropout = 0, ** kwargs): super ...", "dateLastCrawled": "2022-01-31T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Frontiers | <b>Learning</b> Semantic Graphics Using Convolutional Encoder ...", "url": "https://www.frontiersin.org/articles/10.3389/fpls.2019.01404/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpls.2019.01404", "snippet": "The <b>decoder is similar</b> in architecture to the encoder but with fewer feature maps for optimized computation and memory requirements. Each block in the decoder is also a repeating structure of up-sampling, followed by multiple 3 \u00d7 3 deconvolution, batch normalization, and nonlinear activation operations. The number of feature maps at each level in the decoder is kept constant except for the output layer where it is equal to the number of target classes. The network contains extended skip ...", "dateLastCrawled": "2022-02-02T20:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Lane Compression: A Lightweight Lossless Compression Method for <b>Machine</b> ...", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3431815", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3431815", "snippet": "The <b>decoder is similar</b> to the encoder but in reverse. Input compressed symbols are buffered in the input buffer. This must be large enough for the worst case compressed symbol. A stop code detector checks for stop code sequences and handles them appropriately. It is important for the decoder&#39;s throughput to quickly compute the size of each variable-length compressed symbol, because the next symbol decode cannot start until the width of the previous symbol is known. Therefore, all possible ...", "dateLastCrawled": "2022-01-18T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Columns Occurrences Graph to Improve Column Prediction in Deep <b>Learning</b> ...", "url": "https://www.mdpi.com/2076-3417/11/24/12116/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2076-3417/11/24/12116/htm", "snippet": "Although our <b>decoder is similar</b> to the base model SyntaxsqlNet, our columns occurrences score, in the encoder, allows the model to include user intentions regarding column prediction from the database. In addition, overall accuracy increased with our model\u2019s greater focus on column prediction. Our model achieved a prominent increase in exact match accuracy. Table 2 shows the experimental results in the form of exact match accuracy compared with the base model and two other methods from [14 ...", "dateLastCrawled": "2022-01-18T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Learning</b> to combine classifiers outputs with the transformer for text ...", "url": "https://content.iospress.com/articles/intelligent-data-analysis/ida200007", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/intelligent-data-analysis/ida200007", "snippet": "The transformer <b>decoder is similar</b>, but it includes a variant that allows computing a language model task with long-term dependencies with context both to the left and to the right of each target word. The modification is called the masked language model, and it consists of a block that randomly masks some words in the input and output in the self-attention layer. The rest of the transformer layer considers the same encoder blocks, that is, the self-attention and feed-forward layer mechanism ...", "dateLastCrawled": "2022-02-02T13:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Hugging Face Pre-trained Models: Find the Best One for Your Task ...", "url": "https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best", "snippet": "When you are working on a <b>Machine</b> <b>learning</b> problem, adapting an existing solution and repurposing it can help you get to a solution much faster. Using existing models, not just aid <b>machine</b> <b>learning</b> engineers or data scientists but also helps companies to save computational costs as it requires less training. There are many companies that provide open source libraries containing pre-trained models and Hugging Face is one of them. Hugging Face first launched its chat platform back in 2017. To ...", "dateLastCrawled": "2022-02-02T20:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Copyright Does Not Exist</b> | Hacker Culture | Microcomputers", "url": "https://www.scribd.com/document/36926355/Copyright-Does-Not-Exist", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/36926355/<b>Copyright-Does-Not-Exist</b>", "snippet": "This <b>machine</b> differed from the mammoth IBM machines that had been used by universities since 1948, ... and speculations about self-referential intelligent systems (self-referential means &quot;<b>learning</b> from mistakes&quot;, or simply: <b>learning</b> ) figured heavily in this philosophy. Parallels were drawn to such varied subjects as paradoxes among the ancient philosophers, Bach&#39;s mathematical play with harmonies, Escher&#39;s mathematically inspired etchings and drawings, and Benoit Mandelbrot &#39;s theories of ...", "dateLastCrawled": "2022-01-05T05:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep <b>learning</b> approaches for neural decoding across architectures and ...", "url": "https://jesselivezey.com/wp-content/uploads/2020/11/neural_decoding_review.pdf", "isFamilyFriendly": true, "displayUrl": "https://jesselivezey.com/wp-content/uploads/2020/11/neural_decoding_review.pdf", "snippet": "A <b>decoder can be thought of as</b> a function approximator, doing either regression or classi cation depending on whether the output is a continuous or categorical variable. Given the great successes of deep <b>learning</b> at <b>learning</b> complex functions across many domains [17{26], it is unsurprising that deep <b>learning</b> has become a popular approach in neuroscience. Here, we review the many uses of deep <b>learning</b> for neural decoding. We emphasize how di erent deep <b>learning</b> architectures can induce biases ...", "dateLastCrawled": "2022-01-01T14:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Lecture <b>6: Unsupervised learning and generative models</b> | CS236781: Deep ...", "url": "https://vistalab-technion.github.io/cs236781/lecture_notes/lecture_06/", "isFamilyFriendly": true, "displayUrl": "https://vistalab-technion.github.io/cs236781/lecture_notes/lecture_06", "snippet": "A <b>decoder can be thought of as</b> a transposed version of the encoder, in which the dimensionality gradually increases toward the output. Though the decoder does not necessarily need to match the same dimensions (in reversed order) of the encoder\u2019s intermediate layers, such symmetric architectures are very frequent. In what follows, we remind the working of a convolutional layer and describe how to formally transpose it.", "dateLastCrawled": "2021-11-30T16:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep <b>learning</b> approaches <b>for neural decoding across architectures</b> and ...", "url": "https://academic.oup.com/bib/article/22/2/1577/6054827", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/bib/article/22/2/1577/6054827", "snippet": "A <b>decoder can be thought of as</b> a function approximator, doing either regression or classification depending on whether the output is a continuous or categorical variable. Given the great successes of deep <b>learning</b> at <b>learning</b> complex functions across many domains [ 17\u201326 ], it is unsurprising that deep <b>learning</b> has become a popular approach in neuroscience.", "dateLastCrawled": "2021-12-22T17:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Attention for Neural Machine Translation (NMT</b>)", "url": "https://www.linkedin.com/pulse/attention-neural-machine-translation-nmt-ajay-taneja", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/attention-neural-<b>machine</b>-translation-nmt-ajay-taneja", "snippet": "The MBR <b>decoder can be thought of as</b> selecting a consensus translation, i.e. for each sentence, the decoder selects the translation that is closest on an average to all the likely translations and ...", "dateLastCrawled": "2022-01-23T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Improved Training of <b>Sparse Coding Variational Autoencoder via Weight</b> ...", "url": "https://deepai.org/publication/improved-training-of-sparse-coding-variational-autoencoder-via-weight-normalization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/improved-training-of-sparse-coding-variational-auto...", "snippet": "The SVAE <b>decoder can be thought of as</b> reparameterized weights with g = 1. Weight normalization has been shown to accelerate model training and encourage disentangled representation <b>learning</b>. We expect having a unit norm constraint on the SVAE decoder to have similar effects. Future work could focus on verifying the effect of normalization on ...", "dateLastCrawled": "2022-01-30T18:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Combining Decoder Design and Neural Adaptation in Brain-<b>Machine</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0896627314007399", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0896627314007399", "snippet": "We believe that this will be possible, and that framing it as a two-learner system may be helpful (e.g., DiGiovanna et al., 2009); (1) the <b>decoder can be thought of as</b> a \u201csurrogate spinal cord,\u201d which effectively reads out cortical neural activity and is learned by the brain (learner 1) via neural adaptation, and (2) the decoder itself can also learn (learner 2) via decoder design and decoder adaptation. In other words, a system where the brain and decoder collaborate to produce more ...", "dateLastCrawled": "2021-10-22T10:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Neural machine translation of Hindi and English</b> - IOS Press", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs179873", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs179873", "snippet": "A <b>Decoder can be thought of as</b> an inverse function to that of an encoder. Decoders work on probability, with the output being decided by the goal of maximizing the probability given the input code i.e. probabilistic decoder model p (x \u2223 z \u2192 = \u03c8 enc (x)), and maximizes the likelihood of an example x conditioned on z \u2192, the learned code for x. The decoder is an two layer sequential LSTM with a global attention mechanism inspired from Bahdanau et al. and Luong et al. . A simple non ...", "dateLastCrawled": "2021-12-30T00:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Lesson <b>2: ConvNets for Semantic Segmentation</b> - Module 5 ... - <b>Coursera</b>", "url": "https://www.coursera.org/lecture/visual-perception-self-driving-cars/lesson-2-convnets-for-semantic-segmentation-ii7Th", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/visual-perception-self-driving-cars/lesson-2-convnets...", "snippet": "The feature <b>decoder can be thought of as</b> a mirror image of the feature extractor. Instead of using the convolution pooling paradigm to downsample the resolution, it uses upsampling layers followed by a convolutional layer to upsample the resolution of the feature map. The upsampling usually using nearest neighbor methods achieves the opposite effect to pooling, but results in an inaccurate feature map. The following convolutional layers are then used to correct the features in the upsampled ...", "dateLastCrawled": "2022-01-19T00:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "rnn - <b>Transformers for embedding sequences as</b> fixed-length vectors ...", "url": "https://stats.stackexchange.com/questions/455992/transformers-for-embedding-sequences-as-fixed-length-vectors", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/455992/<b>transformers-for-embedding-sequences</b>...", "snippet": "If you do this without attention, the output of the <b>decoder can be thought of as</b> a fixed length representation of these sequences. I&#39;ve been calling this idea a recurrent autoencoder, but I haven&#39;t seen it explored by anyone else, yet. This could be very useful for <b>machine</b> <b>learning</b> tasks on sequences, since you can learn from fixed-length vectors. (Especially if you have lots of unlabeled data, but a small amount of labels)", "dateLastCrawled": "2022-01-25T14:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Deep learning approaches for neural decoding: from</b> CNNs to LSTMs and ...", "url": "https://deepai.org/publication/deep-learning-approaches-for-neural-decoding-from-cnns-to-lstms-and-spikes-to-fmri", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>deep-learning-approaches-for-neural-decoding-from</b>-cnns...", "snippet": "In the last decade, deep <b>learning</b> has become the state-of-the-art method in many <b>machine</b> <b>learning</b> tasks ranging from speech recognition to image segmentation. The success of deep networks in other domains has led to a new wave of applications in neuroscience. In this article, we review deep <b>learning</b> approaches to neural decoding. We describe the architectures used for extracting useful features from neural recording modalities ranging from spikes to EEG. Furthermore, we explore how deep ...", "dateLastCrawled": "2021-12-06T21:48:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(decoder)  is like +(machine learning model)", "+(decoder) is similar to +(machine learning model)", "+(decoder) can be thought of as +(machine learning model)", "+(decoder) can be compared to +(machine learning model)", "machine learning +(decoder AND analogy)", "machine learning +(\"decoder is like\")", "machine learning +(\"decoder is similar\")", "machine learning +(\"just as decoder\")", "machine learning +(\"decoder can be thought of as\")", "machine learning +(\"decoder can be compared to\")"]}