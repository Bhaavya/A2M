{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "For example, <b>hinge</b> <b>loss</b> is available as a <b>loss</b> function in Keras. <b>Squared</b> <b>hinge</b>. The <b>squared</b> <b>hinge</b> <b>loss</b> <b>is like</b> the <b>hinge</b> formula displayed above, but then the \\(max()\\) function output is <b>squared</b>. This helps achieving two things: Firstly, it makes the <b>loss</b> <b>value</b> more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "<b>Squared</b> <b>Hinge</b> <b>Loss</b>; Multi-Class Classification <b>Loss</b> Functions Multi-Class Cross-Entropy <b>Loss</b>; Sparse Multiclass Cross-Entropy <b>Loss</b>; Kullback Leibler Divergence <b>Loss</b> ; We will focus on how to choose and implement different <b>loss</b> functions. For more theory on <b>loss</b> functions, see the post: <b>Loss</b> and <b>Loss</b> Functions for Training Deep Learning Neural Networks; Regression <b>Loss</b> Functions. A regression predictive modeling problem involves predicting a real-valued quantity. In this section, we will ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "The main difference <b>between</b> the <b>hinge</b> <b>loss</b> and the cross entropy <b>loss</b> is that the former arises from trying to maximize the margin <b>between</b> our decision boundary and data points - thus attempting to ensure that each point is correctly and confidently classified*, while the latter comes from a maximum likelihood estimate of our model\u2019s parameters. The softmax function, whose scores are used by the cross entropy <b>loss</b>, allows us to interpret our model\u2019s scores as relative probabilities ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "Cross-entropy <b>loss</b> measures the performance of a classification model whose output is a probability <b>value</b> <b>between</b> 0 and 1. Cross-entropy <b>loss</b> increases as the <b>predicted</b> probability diverge from the <b>actual</b> label. So predicting a probability of .012 when the <b>actual</b> observation label is 1 would be bad and result in a high <b>loss</b> <b>value</b>. A perfect model would have a log <b>loss</b> of 0. The graph above shows the range of possible <b>loss</b> values given a true observation. As the <b>predicted</b> probability ...", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Keras Loss Functions - Types and Examples</b> - DataFlair", "url": "https://data-flair.training/blogs/keras-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://data-flair.training/blogs/keras-<b>loss</b>-functions", "snippet": "5. <b>Hinge</b> <b>Loss</b> in Keras. Here <b>loss</b> is defined as, <b>loss</b>=max(1-<b>actual</b>*<b>predicted</b>,0) The <b>actual</b> values are generally -1 or 1. And if it is not, then we convert it to -1 or 1. This <b>loss</b> is available as: keras.losses.<b>Hinge</b>(reduction,name) 6. CosineSimilarity in Keras. Calculate the cosine similarity <b>between</b> the <b>actual</b> and <b>predicted</b> values. The <b>loss</b> ...", "dateLastCrawled": "2022-02-03T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Most Used <b>Loss</b> <b>Functions To Optimize Machine Learning Algorithms</b>", "url": "https://analyticsindiamag.com/most-used-loss-functions-to-optimize-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/most-used-<b>loss</b>-<b>functions-to-optimize-machine-learning</b>...", "snippet": "<b>Hinge</b> <b>Loss</b>; <b>Hinge</b> <b>loss</b> was originally developed for SVM. The simple intuition behind <b>hinge</b> <b>loss</b> is, it works on the difference of sign. For e.g. the target variable has values <b>like</b> -1 and 1 and the model predicts 1 whereas the <b>actual</b> class is -1, the function will impose a higher penalty at this point because it can sense the difference in the ...", "dateLastCrawled": "2022-02-01T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Which <b>loss</b> function is better: <b>hinge</b> <b>loss</b> or softmax? - Quora", "url": "https://www.quora.com/Which-loss-function-is-better-hinge-loss-or-softmax", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Which-<b>loss</b>-function-is-better-<b>hinge</b>-<b>loss</b>-or-softmax", "snippet": "Answer: This is an easy one, <b>hinge</b> <b>loss</b>, since softmax is not a <b>loss</b> function. Softmax is a means for converting a set of values to a \u201cprobability distribution\u201d. We would not traditionally consider this a <b>loss</b> function as much as we would use it in the process of computing the <b>loss</b> with another f...", "dateLastCrawled": "2022-01-12T19:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Why do <b>people prefer Cross Entropy Loss</b> <b>to Hinge Loss in classification</b> ...", "url": "https://www.quora.com/Why-do-people-prefer-Cross-Entropy-Loss-to-Hinge-Loss-in-classification-task", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-do-<b>people-prefer-Cross-Entropy-Loss</b>-to-<b>Hinge</b>-<b>Loss</b>-in...", "snippet": "Answer: I think, this derives heavily from the notion of the nature of classification labels to begin with. See, when you are talking about <b>Hinge</b> <b>Loss</b> - you are running Maximization of Information Metrics in terms of Vectorial dynamics, correct? Assuming, that you are accounting for the Hamming...", "dateLastCrawled": "2022-01-20T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Activation Functions and <b>Loss</b> Functions for neural networks \u2014 How to ...", "url": "https://medium.com/analytics-vidhya/activation-functions-and-loss-functions-for-neural-networks-how-to-pick-the-right-one-542e1dd523e0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/activation-functions-and-<b>loss</b>-functions-for-neural...", "snippet": "where y is the <b>actual</b> label and p is the classifier\u2019s <b>predicted</b> probability distributions for predicting the class j Range: (0,inf) Pros: Similar to Binary Cross Entropy, the continuous nature ...", "dateLastCrawled": "2022-02-03T05:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - Is it ok to define your own cost function for ...", "url": "https://stackoverflow.com/questions/12157881/is-it-ok-to-define-your-own-cost-function-for-logistic-regression", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/12157881", "snippet": "In least-squares models, the cost function is defined as the square of the difference <b>between</b> the <b>predicted</b> <b>value</b> and the <b>actual</b> <b>value</b> as a function of the input. When we do <b>logistic regression</b>, we change the cost function to be a logarithmic function instead of defining it to be the square of the difference <b>between</b> the sigmoid function (the output <b>value</b>) and the <b>actual</b> output.", "dateLastCrawled": "2022-01-23T06:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "The main difference <b>between</b> the <b>hinge</b> <b>loss</b> and the cross entropy <b>loss</b> is that the former arises from trying to maximize the margin <b>between</b> our decision boundary and data points - thus attempting to ensure that each point is correctly and confidently classified*, while the latter comes from a maximum likelihood estimate of our model\u2019s parameters. The softmax function, whose scores are used by the cross entropy <b>loss</b>, allows us to interpret our model\u2019s scores as relative probabilities ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "For example, <b>hinge</b> <b>loss</b> is available as a <b>loss</b> function in Keras. <b>Squared</b> <b>hinge</b>. The <b>squared</b> <b>hinge</b> <b>loss</b> is like the <b>hinge</b> formula displayed above, but then the \\(max()\\) function output is <b>squared</b>. This helps achieving two things: Firstly, it makes the <b>loss</b> <b>value</b> more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "<b>Squared</b> <b>Hinge</b> <b>Loss</b>; Multi-Class Classification <b>Loss</b> Functions Multi-Class Cross-Entropy <b>Loss</b>; Sparse Multiclass Cross-Entropy <b>Loss</b>; Kullback Leibler Divergence <b>Loss</b> ; We will focus on how to choose and implement different <b>loss</b> functions. For more theory on <b>loss</b> functions, see the post: <b>Loss</b> and <b>Loss</b> Functions for Training Deep Learning Neural Networks; Regression <b>Loss</b> Functions. A regression predictive modeling problem involves predicting a real-valued quantity. In this section, we will ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Doubly Optimized Calibrated Support Vector Machine (DOC-SVM): An ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3490990/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3490990", "snippet": "Calibration is a degree of agreement <b>between</b> <b>predicted</b> probability with <b>actual</b> risk, ... Theorem 5 indicates that <b>minimizing</b> the <b>hinge</b> <b>loss</b> function leads to AUC maximization because implies . The following objective function optimizes the Doubly Optimized Calibrated Support Vector Machine (DOC-SVM), (6) where is the <b>loss</b> for the -th data point ; is the weight parameter; and are the penalty parameters for the <b>hinge</b> <b>loss</b> the <b>squared</b> <b>loss</b>, respectively. DOC-SVM optimizes discrimination and ...", "dateLastCrawled": "2021-09-20T00:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions in Machine Learning | G. Wu", "url": "https://guangyuwu.wordpress.com/2021/02/02/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://guangyuwu.wordpress.com/2021/02/02/<b>loss</b>-functions-in-machine-learning", "snippet": "TensorFlow: tf.keras.losses.binary_crossentropy Note, we add a very small <b>value</b>, epsilon \u03b5 (in this case 1E-7) to the <b>predicted</b> probabilities to avoid ever calculating the log of 0.0. This means that in practice, the best possible <b>loss</b> will be a <b>value</b> very close to zero, but not exactly zero.", "dateLastCrawled": "2022-01-12T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Decoding Advanced Loss Functions in Machine Learning: A Comprehensive Guide</b>", "url": "https://www.dexlabanalytics.com/blog/decoding-advanced-loss-functions-in-machine-learning-a-comprehensive-guide", "isFamilyFriendly": true, "displayUrl": "https://www.dexlabanalytics.com/blog/<b>decoding-advanced-loss-functions-in</b>-machine...", "snippet": "Cross-entropy will calculate a score that summarizes the average difference <b>between</b> the <b>actual</b> and <b>predicted</b> probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy <b>value</b> is 0. <b>Hinge</b> <b>Loss</b>. The <b>hinge</b> <b>loss</b> function is popular with Support Vector Machines (SVMs). These are used for training the classifiers,", "dateLastCrawled": "2022-01-12T10:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An overview of the <b>Gradient Descent</b> algorithm | by Nishit Jain ...", "url": "https://towardsdatascience.com/an-overview-of-the-gradient-descent-algorithm-8645c9e4de1e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/an-overview-of-the-<b>gradient-descent</b>-algorithm-8645c9e4de1e", "snippet": "L2 <b>Loss</b> / Mean <b>Squared</b> <b>Error</b>; Root Mean <b>Squared</b> <b>Error</b>; Classification Losses: Log <b>Loss</b> (Cross-Entropy <b>Loss</b>) SVM <b>Loss</b> (<b>Hinge</b> <b>Loss</b>) Learning Rate: This is the hyperparameter that determines the steps the <b>gradient descent</b> algorithm takes. <b>Gradient Descent</b> is too sensitive to the learning rate. If it is too big, the algorithm may bypass the local minimum and overshoot. If it too small, it might increase the total computation time to a very large extent. We will see the effect of the learning ...", "dateLastCrawled": "2022-02-01T20:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Neural networks are trained using stochastic gradient descent and require that you choose a <b>loss</b> function when designing and configuring your model. There are many <b>loss</b> functions to choose from and it can be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when training a neural network. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Metrics to Evaluate Classification and Regression Algorithms | by ...", "url": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and-regression-algorithms-1554f1e00a75", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and...", "snippet": "MSE is calculated as the average of the <b>squared</b> differences <b>between</b> the <b>predicted</b> <b>and actual</b> values. The result is always positive regardless of the sign of the <b>predicted</b> <b>and actual</b> values and a ...", "dateLastCrawled": "2022-01-17T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Classification vs. Regression Algorithms in Machine Learning M", "url": "https://www.projectpro.io/article/classification-vs-regression-in-machine-learning/545", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/classification-vs-regression-in-machine-learning/545", "snippet": "It tries to measure the difference in entropy (randomness) <b>between</b> the <b>predicted</b> class and the <b>actual</b> class. Categorical Cross-Entropy: This <b>loss</b> is used for multi-class classification problems. One hot-encoded input is required to feed into the <b>loss</b> function. This function works well with many classification problems and is most used in the industry. Get confident to build end-to-end projects. Access to a curated library of 120+ end-to-end industry projects with solution code, videos and ...", "dateLastCrawled": "2022-01-27T17:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "The main difference <b>between</b> the <b>hinge</b> <b>loss</b> and the cross entropy <b>loss</b> is that the former arises from trying to maximize the margin <b>between</b> our decision boundary and data points - thus attempting to ensure that each point is correctly and confidently classified*, while the latter comes from a maximum likelihood estimate of our model\u2019s parameters. The softmax function, whose scores are used by the cross entropy <b>loss</b>, allows us to interpret our model\u2019s scores as relative probabilities ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Picking <b>Loss</b> Functions - A comparison <b>between</b> MSE, Cross Entropy, and ...", "url": "https://rohanvarma.me/Loss-Functions/?source=post_page---------------------------", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions/?source=post_page---------------------------", "snippet": "Interpreting the cross-entropy <b>loss</b> as <b>minimizing</b> the KL divergence <b>between</b> 2 distributions is interesting if we consider how we <b>can</b> extend cross-entropy to different scenarios. For example, a lot of datasets are only partially labelled or have noisy (i.e. occasionally incorrect) labels. If we could probabilistically assign labels to the unlabelled portion of a dataset, or interpret the incorrect labels as being sampled from a probabalistic noise distribution, we <b>can</b> still apply the idea of ...", "dateLastCrawled": "2020-05-27T14:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>loss</b>-functions-and-optimization-algorithms...", "snippet": "One of the most widely used <b>loss function</b> is mean square <b>error</b>, which calculates the square of difference <b>between</b> <b>actual</b> <b>value</b> and <b>predicted</b> <b>value</b>. Different <b>loss</b> functions are used to deal with ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reductions</b> \u2013 Page 2 \u2013 Machine Learning (Theory)", "url": "https://hunch.net/?cat=12&paged=2", "isFamilyFriendly": true, "displayUrl": "https://hunch.net/?cat=12&amp;paged=2", "snippet": "For log <b>loss</b> and <b>squared</b> <b>loss</b>, any other threshold is inconsistent. Since the optimal predictor for <b>hinge</b> <b>loss</b> always takes <b>value</b> 0 or 1, there is some freedom in how we convert, but a reasonable approach is to also threshold at 0.5. Now, we want to analyze the stability of predictions.", "dateLastCrawled": "2021-12-22T07:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Concepts</b> | Machine Learning - Michael Clark", "url": "https://m-clark.github.io/introduction-to-machine-learning/concepts.html", "isFamilyFriendly": true, "displayUrl": "https://m-clark.github.io/introduction-to-machine-learning/<b>concepts</b>.html", "snippet": "<b>Concepts</b>. Given a set of predictor variables \\(X\\) and some target \\(y\\), we look for some function, \\(f(X)\\), to make predictions of y from those input variables.We also need a function to penalize errors in prediction, i.e. a <b>loss</b> function.With a chosen <b>loss</b> function, we then find the model which will minimize <b>loss</b>, generally speaking.", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Robust Truncated Hinge Loss Support Vector Machines</b>", "url": "https://www.researchgate.net/publication/4742783_Robust_Truncated_Hinge_Loss_Support_Vector_Machines", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/4742783_<b>Robust_Truncated_Hinge_Loss_Support</b>...", "snippet": "First, we introduce a definition of the least squares type of difference of convex <b>loss</b> (LS-DC) and show that the most commonly used losses in the SVM community are LS-DC <b>loss</b> or <b>can</b> be ...", "dateLastCrawled": "2021-10-22T03:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Categorical cross entropy vs binary \u2014 tensorflow: softmax_cross_entropy", "url": "https://inneanvand.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy-pf12475u5r", "isFamilyFriendly": true, "displayUrl": "https://inneanvand.com/.../<b>loss</b>-functions/categorical-crossentropy-pf12475u5r", "snippet": "It will calculate the average difference <b>between</b> the <b>actual</b> and <b>predicted</b> probability distributions for all classes in the problem Binary cross-entropy is used to compute the cross-entropy <b>between</b> the true labels and <b>predicted</b> outputs. It&#39;s used when two-class problems arise like cat and dog classification [1 or 0]. Below is an example of Binary Cross-Entropy <b>Loss</b> calculation: ## Binary Corss Entropy Calculation import tensorflow as tf #input lables", "dateLastCrawled": "2022-01-12T03:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "quantitative analysis of different <b>loss</b> functions COPY", "url": "https://www.researchgate.net/profile/Rajkumar-Soundrapandiyan-2/publication/329976215_Targeted_style_transfer_using_cycle_consistent_generative_adversarial_networks_with_quantitative_analysis_of_different_loss_functions1/links/5daa614e92851c577eb84b26/Targeted-style-transfer-using-cycle-consistent-generative-adversarial-networks-with-quantitative-analysis-of-different-loss-functions1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Rajkumar-Soundrapandiyan-2/publication/329976215...", "snippet": "<b>Squared</b> <b>Error</b>, Binary Cross Entropy <b>Error</b>, and L1 <b>loss</b> functions. In this paper, our network is trained for image-to-image translation where the style or content of the Target image is changed by ...", "dateLastCrawled": "2021-10-21T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding binary <b>cross-entropy</b> / log <b>loss</b>: a visual explanation ...", "url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-binary-<b>cross-entropy</b>-log-<b>loss</b>-a-visual...", "snippet": "Binary <b>Cross-Entropy</b> / Log <b>Loss</b>. where y is the label (1 for green points and 0 for red points) and p(y) is the <b>predicted</b> probability of the point being green for all N points.. Reading this formula, it tells you that, for each green point (y=1), it adds log(p(y)) to the <b>loss</b>, that is, the log probability of it being green.Conversely, it adds log(1-p(y)), that is, the log probability of it being red, for each red point (y=0).Not necessarily difficult, sure, but no so intuitive too\u2026", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. Cross-entropy is commonly used in machine learning as a <b>loss</b> function. Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference <b>between</b> two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy <b>between</b> two probability distributions, whereas cross-entropy <b>can</b> <b>be thought</b> to calculate the total entropy <b>between</b> the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "The main difference <b>between</b> the <b>hinge</b> <b>loss</b> and the cross entropy <b>loss</b> is that the former arises from trying to maximize the margin <b>between</b> our decision boundary and data points - thus attempting to ensure that each point is correctly and confidently classified*, while the latter comes from a maximum likelihood estimate of our model\u2019s parameters. The softmax function, whose scores are used by the cross entropy <b>loss</b>, allows us to interpret our model\u2019s scores as relative probabilities ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Which <b>loss</b> function is better: <b>hinge</b> <b>loss</b> or softmax? - Quora", "url": "https://www.quora.com/Which-loss-function-is-better-hinge-loss-or-softmax", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Which-<b>loss</b>-function-is-better-<b>hinge</b>-<b>loss</b>-or-softmax", "snippet": "Answer: This is an easy one, <b>hinge</b> <b>loss</b>, since softmax is not a <b>loss</b> function. Softmax is a means for converting a set of values to a \u201cprobability distribution\u201d. We would not traditionally consider this a <b>loss</b> function as much as we would use it in the process of computing the <b>loss</b> with another f...", "dateLastCrawled": "2022-01-12T19:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be <b>predicted</b> that lies <b>between</b> (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Understanding Loss Functions in Machine Learning</b> | Engineering ...", "url": "https://www.section.io/engineering-education/understanding-loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.section.io/engineering-education/<b>understanding-loss-functions-in-machine</b>...", "snippet": "On taking a closer look at the formulas, one <b>can</b> observe that if the difference <b>between</b> the <b>predicted</b> and the <b>actual</b> <b>value</b> is high, L2 <b>loss</b> magnifies the effect when <b>compared</b> to L1. Since L2 succumbs to outliers, L1 <b>loss</b> function is the more robust <b>loss</b> function. L1 <b>loss</b> is less stable than L2 <b>loss</b>. Since L1 <b>loss</b> deals with the difference in distances, a small horizontal change <b>can</b> lead to the regression line jumping a large amount. Such an effect taking place across multiple iterations ...", "dateLastCrawled": "2022-02-01T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions in Machine Learning | G. Wu", "url": "https://guangyuwu.wordpress.com/2021/02/02/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://guangyuwu.wordpress.com/2021/02/02/<b>loss</b>-functions-in-machine-learning", "snippet": "TensorFlow: tf.keras.losses.binary_crossentropy Note, we add a very small <b>value</b>, epsilon \u03b5 (in this case 1E-7) to the <b>predicted</b> probabilities to avoid ever calculating the log of 0.0. This means that in practice, the best possible <b>loss</b> will be a <b>value</b> very close to zero, but not exactly zero.", "dateLastCrawled": "2022-01-12T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "machine learning - What are the impacts of choosing different <b>loss</b> ...", "url": "https://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing-different-loss-functions-in-classification-to-a", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing...", "snippet": "Following the least squares vs. logistic regression example in PRML, I added the <b>hinge</b> <b>loss</b> for comparison. As shown in the figure, <b>hinge</b> <b>loss</b> and logistic regression / cross entropy / log-likelihood / softplus have very close results, because their objective functions are close (figure below), while MSE is generally more sensitive to outliers.", "dateLastCrawled": "2022-01-22T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Types of Loss Function</b> - OpenGenus IQ: Computing Expertise", "url": "https://iq.opengenus.org/types-of-loss-function/", "isFamilyFriendly": true, "displayUrl": "https://iq.opengenus.org/<b>types-of-loss-function</b>", "snippet": "<b>Loss</b> function is an important part in artificial neural networks, which is used to measure the inconsistency <b>between</b> <b>predicted</b> <b>value</b> (^y) <b>and actual</b> label (y). It is a non-negative <b>value</b>, where the robustness of model increases along with the decrease of the <b>value</b> of <b>loss</b> function.", "dateLastCrawled": "2022-01-28T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Most Common <b>Loss</b> Functions in Machine Learning | by Sparsh Gupta ...", "url": "https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/most-common-<b>loss</b>-functions-in-machine-learning-c7212a99dae0", "snippet": "1. Binary Cross-Entropy <b>Loss</b> / Log <b>Loss</b>. This is the most common <b>Loss function</b> used in Classification problems. The cross-entropy <b>loss</b> decreases as the <b>predicted</b> probability converges to the <b>actual</b> label. It measures the performance of a classification model whose <b>predicted</b> output is a probability <b>value</b> <b>between</b> 0 and 1.", "dateLastCrawled": "2022-02-03T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "The distinction is the difference <b>between</b> <b>predicted</b> <b>and actual</b> probability. This adds data about information <b>loss</b> in the model training. The farther away the <b>predicted</b> probability distribution is ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Dummies guide to <b>Cost Functions in Machine Learning [with Animation</b> ...", "url": "https://machinelearningknowledge.ai/cost-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningknowledge.ai/<b>cost-functions-in-machine-learning</b>", "snippet": "To work with <b>hinge</b> <b>loss</b>, the binary classification output should be denoted with +1 or -1. SVM predicts a classification score h(y) where y is the <b>actual</b> output. Then <b>hinge</b> <b>loss</b> for a particular data D is given as-<b>Hinge</b>_<b>Loss</b>(D) = max(0,1-y*h(y)) Then <b>hinge</b> <b>loss</b> cost function for the entire N data set is given by. <b>Hinge</b>_<b>Loss</b>_Cost = Sum of <b>Hinge</b> ...", "dateLastCrawled": "2022-02-02T20:29:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the following stages occur: Stage 1: Underfitting stage \u2013 high train and high test errors (or low ...", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, <b>squared</b> <b>hinge</b> <b>loss</b> function (as against <b>hinge</b> <b>loss</b> function) and l2 penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "However, in <b>machine</b> <b>learning</b> methodology, <b>squared</b> <b>loss</b> will be minimized with respect to ... <b>Squared</b> <b>loss</b> (for regression) <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A study on L2-<b>loss (Squared Hinge-Loss) multiclass SVM</b> | Request PDF", "url": "https://www.researchgate.net/publication/235884495_A_study_on_L2-loss_Squared_Hinge-Loss_multiclass_SVM", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235884495_A_study_on_L2-<b>loss</b>_<b>Squared</b>_<b>Hinge</b>...", "snippet": "Taking the <b>analogy</b> to classification task, it has been previously studied [13] that using the <b>squared</b> <b>hinge</b> <b>loss</b> in SVM would yield better accuracy when \u03bb is large. In this case, underfitting ...", "dateLastCrawled": "2021-12-14T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Metrics to Evaluate Classification and Regression Algorithms | by ...", "url": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and-regression-algorithms-1554f1e00a75", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and...", "snippet": "<b>Hinge</b> <b>loss</b>. cross-entropy <b>loss</b> / log <b>loss</b>. likelihood <b>loss</b>. MSE / Quadratic <b>loss</b> / L2 <b>loss</b>: Mean <b>Squared</b> Error, or MSE <b>loss</b> is the default <b>loss</b> to use for regression problems. Mathematically, it ...", "dateLastCrawled": "2022-01-17T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Scikit Learn - Stochastic Gradient Descent</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_stochastic_gradient_descent</b>.htm", "snippet": "The default value is \u2018<b>hinge</b>\u2019 which will give us a linear SVM. The other options which can be used are \u2212. log \u2212 This <b>loss</b> will give us logistic regression i.e. a probabilistic classifier. modified_huber \u2212 a smooth <b>loss</b> that brings tolerance to outliers along with probability estimates. <b>squared</b>_<b>hinge</b> \u2212 similar to \u2018<b>hinge</b>\u2019 <b>loss</b> but ...", "dateLastCrawled": "2022-02-03T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "3.3. Metrics and scoring: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "<b>hinge</b>_<b>loss</b> (y_true, pred_decision, *[, ...]) Average <b>hinge</b> <b>loss</b> (non-regularized). ... \u201cThe Matthews correlation coefficient is used in <b>machine</b> <b>learning</b> as a measure of the quality of binary (two-class) classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Models 1.1 Support vector <b>machine</b> 1.1.1 Principle 1.1.2 Kernel 1.1.3 Soft margin SVM 1.1.4 <b>Hinge</b> <b>loss</b> view 1.1.5 Multi-class SVM 1.1.6 Extensions 1.2 Tree-based models 1.2.1 Decision tree 1.2.2 Random forest 1.2.3 Gradient boosted decision trees 1.2.4 Tools 1.3 EM Principle 1.4 MaxEnt 1.4.1 Entropy 1.5 Model selection 1.5.1 Under-fitting / Over-fitting 1.5.2 Model ensemble, sklearn 2.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "50 Data Scientist Interview Questions (ANSWERED with PDF) To Crack Next ...", "url": "https://www.mlstack.cafe/blog/data-scientist-interview-questions", "isFamilyFriendly": true, "displayUrl": "https://www.mlstack.cafe/blog/data-scientist-interview-questions", "snippet": "Companies need data scientists. They need people who are able to take large amounts of data and make it usable. The national average salary for a Data Scientist in the United States is $117,212. Data Scientist roles in Australia were typically advertised between $110k and $140k in the last 3 months. Follow along and learn the 50 most common and advanced Data Scientist Interview Questions and Answers (PDF download ready) you must know before your next <b>Machine</b> <b>Learning</b> and Data Science interview.", "dateLastCrawled": "2022-02-03T06:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "We\u2019re then using <b>machine</b> <b>learning</b> for ... The <b>squared hinge loss is like</b> the hinge formula displayed above, but then the \\(max()\\) function output is squared. This helps achieving two things: Firstly, it makes the loss value more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add to the loss more significantly than smaller errors. Note that simiarly, this may also mean that you\u2019ll need to inspect your dataset for the presence of such outliers first. Secondly ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "\u635f\u5931\u51fd\u6570 - \u7b97\u6cd5\u6742\u8d27\u94fa - bjmsong.github.io", "url": "https://bjmsong.github.io/2020/02/21/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/", "isFamilyFriendly": true, "displayUrl": "https://bjmsong.github.io/2020/02/21/\u635f\u5931\u51fd\u6570", "snippet": "the training data is fed into the <b>machine</b> <b>learning</b> model; Loss : compare between some actual targets and predicted targets; the lower the loss, the more the set of targets and the set of predictions resemble each other; the more they resemble each other, the better the <b>machine</b> <b>learning</b> model performs. Backward pass", "dateLastCrawled": "2021-12-27T11:43:00.0000000Z", "language": "zh_chs", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(squared hinge loss)  is like +(minimizing error between predicted and actual value)", "+(squared hinge loss) is similar to +(minimizing error between predicted and actual value)", "+(squared hinge loss) can be thought of as +(minimizing error between predicted and actual value)", "+(squared hinge loss) can be compared to +(minimizing error between predicted and actual value)", "machine learning +(squared hinge loss AND analogy)", "machine learning +(\"squared hinge loss is like\")", "machine learning +(\"squared hinge loss is similar\")", "machine learning +(\"just as squared hinge loss\")", "machine learning +(\"squared hinge loss can be thought of as\")", "machine learning +(\"squared hinge loss can be compared to\")"]}