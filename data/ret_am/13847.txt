{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov</b> Chain Overview: Characteristics &amp; Applications", "url": "https://www.latentview.com/blog/markov-chains-what-are-they-and-where-do-they-matter/", "isFamilyFriendly": true, "displayUrl": "https://www.latentview.com/blog/<b>markov</b>-chains-what-are-they-and-where-do-they-matter", "snippet": "Named after a Russian Mathematician Andrey <b>Markov</b>, <b>Markov</b> Chains deal with a sequence of possible events in which the probability of the <b>next</b> event <b>depends</b> <b>only</b> <b>on the current</b> <b>state</b> of the event. The underlying base idea of a <b>Markov</b> Chain is a \u201c<b>Markov</b> <b>Property</b>\u201d aka \u201cMemoryless <b>property</b>\u201d that states that as long as we know the <b>current</b> <b>state</b> of the process, any more information about the <b>past</b> states would <b>not</b> be helpful in predicting the probability of future <b>state</b> of the process. It ...", "dateLastCrawled": "2022-02-02T19:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What are the assumptions of the <b>Markov</b> chain process? - Quora", "url": "https://www.quora.com/What-are-the-assumptions-of-the-Markov-chain-process", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-the-<b>assumptions</b>-of-the-<b>Markov</b>-chain-process", "snippet": "Answer (1 of 2): I don&#39;t remember much in the topic and class at master and above in statistics and probability. If you want to learn it, you should take 2+ probability classes in bachelor at higher division, at least. As for the process, it is based on the probability from one event to another, ...", "dateLastCrawled": "2022-01-17T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reinforcement Learning : <b>Markov-Decision</b> Process (Part 1) | by ...", "url": "https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-reinforcement-learning-<b>markov-decision</b>...", "snippet": "<b>Markov</b> <b>Property</b>. S[t] denotes the <b>current</b> <b>state</b> of the agent and s[t+1] denotes the <b>next</b> <b>state</b>. What this equation means is that the transition from <b>state</b> S[t] to S[t+1] is entirely independent of the <b>past</b>. So, the RHS of the Equation means the same as LHS if the system has a <b>Markov</b> <b>Property</b>. Intuitively meaning that our <b>current</b> <b>state</b> already ...", "dateLastCrawled": "2022-02-02T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Markov decision process</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Markov_decision_process", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Markov_decision_process</b>", "snippet": "A <b>Markov decision process</b> is a 4-tuple (,,,), where: is a set of states called the <b>state</b> space,; is a set of actions called the action space (alternatively, is the set of actions available from <b>state</b> ), (, \u2032) = (+ = \u2032 =, =) is the probability that action in <b>state</b> at time will lead to <b>state</b> \u2032 at time +,(, \u2032) is the immediate reward (or expected immediate reward) received after transitioning from <b>state</b> to <b>state</b> \u2032, due to action The <b>state</b> and action spaces may be finite or infinite ...", "dateLastCrawled": "2022-02-07T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Markov</b> Decision Process - Reinforcement Learning", "url": "https://www.linkedin.com/pulse/markov-decision-process-reinforcement-learning-farrukh-akhtar", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/<b>markov</b>-decision-process-reinforcement-learning-farrukh...", "snippet": "A MDP is a discrete time-<b>state</b> transition system. MDP is actually a reinforcement learning <b>task</b> and it satisfies all the requirements of a <b>Markov</b> <b>property</b>.", "dateLastCrawled": "2022-01-03T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An <b>introduction to part-of-speech tagging and the Hidden Markov</b> Model", "url": "https://www.freecodecamp.org/news/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24/", "isFamilyFriendly": true, "displayUrl": "https://www.freecodecamp.org/news/an-<b>introduction-to-part-of-speech</b>-tagging-and-the...", "snippet": "The <b>Markov</b> <b>property</b> suggests that the distribution for a random variable in the future <b>depends</b> solely <b>only</b> on its distribution in the <b>current</b> <b>state</b>, and none of the previous states have any impact on the future states. For a much more detailed explanation of the working of <b>Markov</b> chains, refer to this link. Also, have a look at the following example just to see how probability of the <b>current</b> <b>state</b> can be computed using the formula above, taking into account the Markovian <b>Property</b>. Apply the ...", "dateLastCrawled": "2022-02-02T18:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Formalization of a <b>Reinforcement Learning</b> Problem | Towards Data Science", "url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/drl-02-formalization-of-a-<b>reinforcement-learning</b>...", "snippet": "The <b>Markov</b> <b>property</b> states that the future <b>depends</b> <b>only</b> on the present and <b>not</b> on the <b>past</b>. The <b>Markov</b> process consists of a sequence of states that strictly obey the <b>Markov</b> <b>property</b>. When an RL problem satisfies the <b>Markov</b> <b>property</b>, i.e., the future <b>depends</b> <b>only</b> <b>on the current</b> <b>state</b> s and action a, but <b>not</b> on the <b>past</b>, it is formulated as a <b>Markov</b> Decision Process (MDP). For the moment, we could consider that an MDP consists basically of five-tuple &lt;S,A,R,p,\u03b3&gt; where the symbols mean: S ...", "dateLastCrawled": "2022-02-02T17:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "POS Tagging Hidden <b>Markov</b> Models (<b>HMM</b>) Viterbi algorithm in NLP maths ...", "url": "https://medium.com/data-science-in-your-pocket/pos-tagging-using-hidden-markov-models-hmm-viterbi-algorithm-in-nlp-mathematics-explained-d43ca89347c4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-in-your-pocket/pos-tagging-using-hidden-<b>markov</b>-models...", "snippet": "A <b>Markov</b> chain makes a very strong <b>assumption</b> that if we want to predict the future in the sequence, all that matters is the <b>current</b> <b>state</b>. All the states before the <b>current</b> <b>state</b> have no impact ...", "dateLastCrawled": "2022-02-03T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why are <b>Markov</b>&#39;s assumptions needed in learning problems? Is it just ...", "url": "https://www.quora.com/Why-are-Markovs-assumptions-needed-in-learning-problems-Is-it-just-for-modelling-purposes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-are-<b>Markov</b>s-<b>assumptions</b>-needed-in-learning-problems-Is-it...", "snippet": "Answer: Without <b>Markov</b>\u2019s assumptions, we would get a system that is much too complex for practical use. The amount of data and computation time needed would be impractical for any but the most basic systems. Without the <b>Markov</b> <b>assumption</b>, the value of a variable <b>depends</b> on every single other var...", "dateLastCrawled": "2022-01-15T11:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Problem Formulation and the <b>Markov</b> Decision Process", "url": "https://www.inf.ed.ac.uk/teaching/courses/rl/slides12/3_MDPs.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.inf.ed.ac.uk/teaching/courses/rl/slides12/3_MDPs.pdf", "snippet": "of a future event given any <b>past</b> events and <b>current</b> <b>state</b>, is independent of <b>past</b> states and <b>depends</b> <b>only</b> on present \u2022 The conditional probabilities are transition probabilities, \u2022 These are stationary if time invariant, called p ij, 27/01/2012 5 . <b>Markov</b> Chains \u2022 Looking forward in time, n-<b>step</b> transition probabilities, p ij (n) \u2022 One can write a transition matrix, \u2022 A stochastic process is a finite-<b>state</b> <b>Markov</b> chain if it has, \u2013Finite number of states \u2013Markovian <b>property</b> ...", "dateLastCrawled": "2021-12-22T22:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Process</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/mathematics/markov-process", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/mathematics/<b>markov-process</b>", "snippet": "The <b>Markov</b> <b>property</b> (3.3) leads to the matrix equation ... The difficulty is that the model is essentially non-Markovian: the probability distribution of the position of the <b>next</b> carbon atom <b>depends</b> <b>not</b> <b>only</b> on the previous one or two, but on all previous positions. It can formally be treated as a <b>Markov process</b> by adding an infinity of variables to take the whole history into account, but that does <b>not</b> help in solving the problem. Exercise. The transition matrix (5.5) for the random walk ...", "dateLastCrawled": "2022-02-02T05:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov</b> Chain Overview: Characteristics &amp; Applications", "url": "https://www.latentview.com/blog/markov-chains-what-are-they-and-where-do-they-matter/", "isFamilyFriendly": true, "displayUrl": "https://www.latentview.com/blog/<b>markov</b>-chains-what-are-they-and-where-do-they-matter", "snippet": "Named after a Russian Mathematician Andrey <b>Markov</b>, <b>Markov</b> Chains deal with a sequence of possible events in which the probability of the <b>next</b> event <b>depends</b> <b>only</b> <b>on the current</b> <b>state</b> of the event. The underlying base idea of a <b>Markov</b> Chain is a \u201c<b>Markov</b> <b>Property</b>\u201d aka \u201cMemoryless <b>property</b>\u201d that states that as long as we know the <b>current</b> <b>state</b> of the process, any more information about the <b>past</b> states would <b>not</b> be helpful in predicting the probability of future <b>state</b> of the process. It ...", "dateLastCrawled": "2022-02-02T19:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Markov model</b> - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Markov_model", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Markov_model</b>", "snippet": "In probability theory, a <b>Markov model</b> is a stochastic model used to model randomly changing systems. [1] It is assumed that future states depend <b>only</b> <b>on the current</b> <b>state</b>, <b>not</b> on the events that occurred before it (that is, it assumes the <b>Markov</b> <b>property</b>).Generally, this <b>assumption</b> enables reasoning and computation with the model that would otherwise be intractable.For this reason, in the fields of predictive modelling and probabilistic forecasting, it is desirable for a given model to ...", "dateLastCrawled": "2021-03-06T09:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Markov decision process</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Markov_decision_process", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Markov_decision_process</b>", "snippet": "A <b>Markov decision process</b> is a 4-tuple (,,,), where: is a set of states called the <b>state</b> space,; is a set of actions called the action space (alternatively, is the set of actions available from <b>state</b> ), (, \u2032) = (+ = \u2032 =, =) is the probability that action in <b>state</b> at time will lead to <b>state</b> \u2032 at time +,(, \u2032) is the immediate reward (or expected immediate reward) received after transitioning from <b>state</b> to <b>state</b> \u2032, due to action The <b>state</b> and action spaces may be finite or infinite ...", "dateLastCrawled": "2022-02-07T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Markov chain</b> - Infogalactic: the planetary knowledge core", "url": "https://infogalactic.com/info/Markov_chain", "isFamilyFriendly": true, "displayUrl": "https://infogalactic.com/info/<b>Markov_chain</b>", "snippet": "A <b>Markov chain</b> (discrete-time <b>Markov chain</b> or DTMC), named after Andrey <b>Markov</b>, is a random process that undergoes transitions from one <b>state</b> to another on a <b>state</b> space.It must possess a <b>property</b> that is usually characterized as &quot;memorylessness&quot;: the probability distribution of the <b>next</b> <b>state</b> <b>depends</b> <b>only</b> <b>on the current</b> <b>state</b> and <b>not</b> on the sequence of events that preceded it.This specific kind of &quot;memorylessness&quot; is called the <b>Markov</b> <b>property</b>.<b>Markov</b> chains have many applications as ...", "dateLastCrawled": "2021-05-10T02:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Why are <b>Markov</b>&#39;s assumptions needed in learning problems? Is it just ...", "url": "https://www.quora.com/Why-are-Markovs-assumptions-needed-in-learning-problems-Is-it-just-for-modelling-purposes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-are-<b>Markov</b>s-<b>assumptions</b>-needed-in-learning-problems-Is-it...", "snippet": "Answer: Without <b>Markov</b>\u2019s assumptions, we would get a system that is much too complex for practical use. The amount of data and computation time needed would be impractical for any but the most basic systems. Without the <b>Markov</b> <b>assumption</b>, the value of a variable <b>depends</b> on every single other var...", "dateLastCrawled": "2022-01-15T11:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Markov</b> decision process - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Markov_decision_process", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Markov</b>_decision_process", "snippet": "A <b>Markov</b> chain or <b>Markov</b> process is a stochastic model describing a sequence of possible events in which the probability of each event <b>depends</b> <b>only</b> on the <b>state</b> attained in the previous event. A countably infinite sequence, in which the chain moves <b>state</b> at discrete time <b>steps</b>, gives a discrete-time <b>Markov</b> chain (DTMC). A continuous-time process is called a continuous-time <b>Markov</b> chain (CTMC).", "dateLastCrawled": "2022-01-03T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Futures Studies and <b>Markov</b> Chain Hybrid Dynamic Multiplicative ...", "url": "https://www.researchgate.net/publication/266738882_Futures_Studies_and_Markov_Chain_Hybrid_Dynamic_Multiplicative_Model_for_Probability_Impact_Analysis", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/266738882_Futures_Studies_and_<b>Markov</b>_Chain...", "snippet": "The <b>Markov</b> <b>property</b> states that the conditional probability distribution for the system at the <b>next</b> <b>step</b> (and in fact at all future <b>steps</b>) given its <b>current</b> <b>state</b> <b>depends</b> <b>only</b> <b>on the current</b> stat ...", "dateLastCrawled": "2021-12-14T22:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What are the properties of a <b>Markov</b> chain? - Quora", "url": "https://www.quora.com/What-are-the-properties-of-a-Markov-chain", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-the-properties-of-a-<b>Markov</b>-chain", "snippet": "Answer (1 of 3): The defining <b>property</b> is that, given the <b>current</b> <b>state</b>, the future is conditionally independent of the <b>past</b>. That can be paraphrased as &quot;if you know the <b>current</b> <b>state</b>, any additional information about the <b>past</b> will <b>not</b> change your predictions about the future.&quot; In explicit fo...", "dateLastCrawled": "2022-01-15T08:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>A probabilistic approach toward earthquake hazard assessment</b> using two ...", "url": "https://link.springer.com/article/10.1007/s11069-014-1438-3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11069-014-1438-3", "snippet": "The Markovian <b>property</b> states that in the process accompanying a series of <b>past</b> events, the <b>next</b> event is <b>only</b> depend on <b>current</b> event and <b>not</b> on the previous ones. Many researchers have discussed the application of <b>Markov</b> chains for seismic hazard assessment. Following paragraph enlists the literature related to different stochastic modeling of earthquake process. This statistical approach is applied for the estimation of seismic hazards of the Northeastern India.", "dateLastCrawled": "2022-01-11T17:43:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov</b> Chain Overview: Characteristics &amp; Applications", "url": "https://www.latentview.com/blog/markov-chains-what-are-they-and-where-do-they-matter/", "isFamilyFriendly": true, "displayUrl": "https://www.latentview.com/blog/<b>markov</b>-chains-what-are-they-and-where-do-they-matter", "snippet": "Named after a Russian Mathematician Andrey <b>Markov</b>, <b>Markov</b> Chains deal with a sequence of possible events in which the probability of the <b>next</b> event <b>depends</b> <b>only</b> <b>on the current</b> <b>state</b> of the event. The underlying base idea of a <b>Markov</b> Chain is a \u201c<b>Markov</b> <b>Property</b>\u201d aka \u201cMemoryless <b>property</b>\u201d that states that as long as we know the <b>current</b> <b>state</b> of the process, any more information about the <b>past</b> states would <b>not</b> be helpful in predicting the probability of future <b>state</b> of the process. It ...", "dateLastCrawled": "2022-02-02T19:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An <b>introduction to part-of-speech tagging and the Hidden Markov</b> Model", "url": "https://www.freecodecamp.org/news/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24/", "isFamilyFriendly": true, "displayUrl": "https://www.freecodecamp.org/news/an-<b>introduction-to-part-of-speech</b>-tagging-and-the...", "snippet": "The <b>Markov</b> <b>property</b> suggests that the distribution for a random variable in the future <b>depends</b> solely <b>only</b> on its distribution in the <b>current</b> <b>state</b>, and none of the previous states have any impact on the future states. For a much more detailed explanation of the working of <b>Markov</b> chains, refer to this link. Also, have a look at the following example just to see how probability of the <b>current</b> <b>state</b> <b>can</b> be computed using the formula above, taking into account the Markovian <b>Property</b>. Apply the ...", "dateLastCrawled": "2022-02-02T18:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Problem Formulation and the <b>Markov</b> Decision Process", "url": "https://www.inf.ed.ac.uk/teaching/courses/rl/slides12/3_MDPs.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.inf.ed.ac.uk/teaching/courses/rl/slides12/3_MDPs.pdf", "snippet": "of a future event given any <b>past</b> events and <b>current</b> <b>state</b>, is independent of <b>past</b> states and <b>depends</b> <b>only</b> on present \u2022 The conditional probabilities are transition probabilities, \u2022 These are stationary if time invariant, called p ij, 27/01/2012 5 . <b>Markov</b> Chains \u2022 Looking forward in time, n-<b>step</b> transition probabilities, p ij (n) \u2022 One <b>can</b> write a transition matrix, \u2022 A stochastic process is a finite-<b>state</b> <b>Markov</b> chain if it has, \u2013Finite number of states \u2013Markovian <b>property</b> ...", "dateLastCrawled": "2021-12-22T22:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>What are the assumptions made by hidden Markov</b> models? - Quora", "url": "https://www.quora.com/What-are-the-assumptions-made-by-hidden-Markov-models", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-are-the-assumptions-made-by-hidden-Markov</b>-models", "snippet": "Answer: <b>Assumptions made by hidden Markov</b> Models Hidden <b>Markov</b> Models Fundamentals Abstract How <b>can</b> we apply machine learning to data that is represented as a sequence of observations over time? For instance, we might be interested in discovering the sequence of words that someone spoke based ...", "dateLastCrawled": "2022-01-19T05:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Progress and challenges in the automated construction of <b>Markov</b> <b>state</b> ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2766407/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2766407", "snippet": "<b>Markov</b> <b>state</b> models ... As a result, the model satisfies the <b>Markov</b> <b>property</b>\u2014the identity of the <b>next</b> <b>state</b> <b>depends</b> <b>only</b> on the identity of the <b>current</b> <b>state</b> and <b>not</b> any of the previous states. MSMs are better able to capture the stochastic nature of processes such as protein folding than traditional analysis techniques, allowing more quantitative comparisons with and predictions of experimental observables. Thus, they will allow researchers to move beyond the traditional view of MD ...", "dateLastCrawled": "2021-08-06T21:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Language Modeling</b>", "url": "https://lena-voita.github.io/nlp_course/language_modeling.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>language_modeling</b>.html", "snippet": "<b>Markov</b> <b>Property</b> (Independence <b>Assumption</b>) ... At each <b>step</b>, the <b>current</b> <b>state</b> contains information about previous tokens and it is used to predict the <b>next</b> token. In training, you feed the training examples. At inference, you feed as context the tokens your model generated; this usually happens until the _eos_ token is generated. \u2022 Multiple layers: feed the states from one RNN to the <b>next</b> one. To get a better text representation, you <b>can</b> stack multiple layers. In this case, inputs for the ...", "dateLastCrawled": "2022-01-29T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Markov chain</b> - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Markov_chain", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Markov_chain</b>", "snippet": "The <b>Markov</b> <b>property</b> states that the conditional probability distribution for the system at the <b>next</b> <b>step</b> (and in fact at all future <b>steps</b>) <b>depends</b> <b>only</b> <b>on the current</b> <b>state</b> of the system, and <b>not</b> additionally on the <b>state</b> of the system at previous <b>steps</b>. Since the system changes randomly, it is generally impossible to predict with certainty the <b>state</b> of a <b>Markov chain</b> at a given point in the future. [22] However, the statistical properties of the system&#39;s future <b>can</b> be predicted. [22] In ...", "dateLastCrawled": "2021-03-25T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Top 50 Artificial Intelligence Questions</b> and Answers (2022) - javatpoint", "url": "https://www.javatpoint.com/artificial-intelligence-interview-questions", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/artificial-intelligence-interview-questions", "snippet": "In the hidden <b>markov</b> model, hidden defines a <b>property</b> that it assumes that the <b>state</b> of a process generated at a particular time is hidden from the observer, and <b>Markov</b> defines that it assumes that the process satisfies the <b>Markov</b> <b>property</b>. The HMM models are mostly used for temporal data. The HMM is used in various applications such as reinforcement learning, temporal pattern recognition, etc. 18) What is Strong AI, and how is it different from the Weak AI? Strong AI: Strong AI is about ...", "dateLastCrawled": "2022-01-31T12:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Markov</b> chain - HandWiki", "url": "https://handwiki.org/wiki/Markov_chain", "isFamilyFriendly": true, "displayUrl": "https://handwiki.org/wiki/<b>Markov</b>_chain", "snippet": "A <b>Markov</b> chain or <b>Markov</b> process is a stochastic model describing a sequence of possible events in which the probability of each event <b>depends</b> <b>only</b> on the <b>state</b> attained in the previous event. A countably infinite sequence, in which the chain moves <b>state</b> at discrete time <b>steps</b>, gives a discrete-time <b>Markov</b> chain (DTMC). A continuous-time process is called a continuous-time <b>Markov</b> chain (CTMC). It is named after the Russia n mathematician Andrey <b>Markov</b>.. <b>Markov</b> chains have many applications ...", "dateLastCrawled": "2021-12-15T12:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Markov_chain</b> : definition of <b>Markov_chain</b> and synonyms of <b>Markov_chain</b> ...", "url": "http://dictionary.sensagent.com/Markov_chain/en-en/", "isFamilyFriendly": true, "displayUrl": "dictionary.sensagent.com/<b>Markov_chain</b>/en-en", "snippet": "A <b>Markov chain</b>, named after Andrey <b>Markov</b>, is a mathematical system that undergoes transitions from one <b>state</b> to another, between a finite or countable number of possible states.It is a random process characterized as memoryless: the <b>next</b> <b>state</b> <b>depends</b> <b>only</b> <b>on the current</b> <b>state</b> and <b>not</b> on the sequence of events that preceded it.This specific kind of &quot;memorylessness&quot; is called the <b>Markov</b> <b>property</b>.<b>Markov</b> chains have many applications as statistical models of real-world processes.", "dateLastCrawled": "2021-12-04T00:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov</b> Chain Overview: Characteristics &amp; Applications", "url": "https://www.latentview.com/blog/markov-chains-what-are-they-and-where-do-they-matter/", "isFamilyFriendly": true, "displayUrl": "https://www.latentview.com/blog/<b>markov</b>-chains-what-are-they-and-where-do-they-matter", "snippet": "Named after a Russian Mathematician Andrey <b>Markov</b>, <b>Markov</b> Chains deal with a sequence of possible events in which the probability of the <b>next</b> event <b>depends</b> <b>only</b> <b>on the current</b> <b>state</b> of the event. The underlying base idea of a <b>Markov</b> Chain is a \u201c<b>Markov</b> <b>Property</b>\u201d aka \u201cMemoryless <b>property</b>\u201d that states that as long as we know the <b>current</b> <b>state</b> of the process, any more information about the <b>past</b> states would <b>not</b> be helpful in predicting the probability of future <b>state</b> of the process. It ...", "dateLastCrawled": "2022-02-02T19:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Reinforcement Learning : <b>Markov-Decision</b> Process (Part 1) | by ...", "url": "https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-reinforcement-learning-<b>markov-decision</b>...", "snippet": "<b>Markov</b> <b>Property</b>. S[t] denotes the <b>current</b> <b>state</b> of the agent and s[t+1] denotes the <b>next</b> <b>state</b>. What this equation means is that the transition from <b>state</b> S[t] to S[t+1] is entirely independent of the <b>past</b>. So, the RHS of the Equation means the same as LHS if the system has a <b>Markov</b> <b>Property</b>. Intuitively meaning that our <b>current</b> <b>state</b> already ...", "dateLastCrawled": "2022-02-02T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Markov</b> Decision Process - Reinforcement Learning", "url": "https://www.linkedin.com/pulse/markov-decision-process-reinforcement-learning-farrukh-akhtar", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/<b>markov</b>-decision-process-reinforcement-learning-farrukh...", "snippet": "A MDP is a discrete time-<b>state</b> transition system. MDP is actually a reinforcement learning <b>task</b> and it satisfies all the requirements of a <b>Markov</b> <b>property</b>.", "dateLastCrawled": "2022-01-03T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Markov decision process</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Markov_decision_process", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Markov_decision_process</b>", "snippet": "A <b>Markov decision process</b> is a 4-tuple (,,,), where: is a set of states called the <b>state</b> space,; is a set of actions called the action space (alternatively, is the set of actions available from <b>state</b> ), (, \u2032) = (+ = \u2032 =, =) is the probability that action in <b>state</b> at time will lead to <b>state</b> \u2032 at time +,(, \u2032) is the immediate reward (or expected immediate reward) received after transitioning from <b>state</b> to <b>state</b> \u2032, due to action The <b>state</b> and action spaces may be finite or infinite ...", "dateLastCrawled": "2022-02-07T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Language Modeling</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/language_modeling.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/<b>language_modeling</b>.html", "snippet": "<b>Markov</b> <b>Property</b> (Independence <b>Assumption</b>) ... At each <b>step</b>, the <b>current</b> <b>state</b> contains information about previous tokens and it is used to predict the <b>next</b> token. In training, you feed the training examples. At inference, you feed as context the tokens your model generated; this usually happens until the _eos_ token is generated. \u2022 Multiple layers: feed the states from one RNN to the <b>next</b> one. To get a better text representation, you <b>can</b> stack multiple layers. In this case, inputs for the ...", "dateLastCrawled": "2022-01-29T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Markov</b> Decision Processes in Artificial Intelligence: MDPs, beyond MDPs ...", "url": "https://www.researchgate.net/publication/297831446_Markov_Decision_Processes_in_Artificial_Intelligence_MDPs_beyond_MDPs_and_applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/297831446_<b>Markov</b>_Decision_Processes_in...", "snippet": "<b>Markov</b> decision processes are defined as controlled stochastic processes satisfying the <b>Markov</b> <b>property</b> in the sense that the probability of reaching the <b>next</b> <b>state</b> of the environment from the ...", "dateLastCrawled": "2022-01-30T02:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Top 50 Artificial Intelligence Questions</b> and Answers (2022) - javatpoint", "url": "https://www.javatpoint.com/artificial-intelligence-interview-questions", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/artificial-intelligence-interview-questions", "snippet": "In the hidden <b>markov</b> model, hidden defines a <b>property</b> that it assumes that the <b>state</b> of a process generated at a particular time is hidden from the observer, and <b>Markov</b> defines that it assumes that the process satisfies the <b>Markov</b> <b>property</b>. The HMM models are mostly used for temporal data. The HMM is used in various applications such as reinforcement learning, temporal pattern recognition, etc. 18) What is Strong AI, and how is it different from the Weak AI? Strong AI: Strong AI is about ...", "dateLastCrawled": "2022-01-31T12:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What are the properties of a <b>Markov</b> chain? - Quora", "url": "https://www.quora.com/What-are-the-properties-of-a-Markov-chain", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-the-properties-of-a-<b>Markov</b>-chain", "snippet": "Answer (1 of 3): The defining <b>property</b> is that, given the <b>current</b> <b>state</b>, the future is conditionally independent of the <b>past</b>. That <b>can</b> be paraphrased as &quot;if you know the <b>current</b> <b>state</b>, any additional information about the <b>past</b> will <b>not</b> change your predictions about the future.&quot; In explicit fo...", "dateLastCrawled": "2022-01-15T08:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Markov</b> Processes | The Reinforcement Learning Workshop", "url": "https://subscription.packtpub.com/book/data/9781800200456/2/ch02lvl1sec10/markov-processes", "isFamilyFriendly": true, "displayUrl": "https://subscription.packtpub.com/book/data/9781800200456/2/ch02lvl1sec10/<b>markov</b>-processes", "snippet": "By looking <b>only</b> at the <b>current</b> <b>state</b>, it is <b>not</b> possible to decide with certainty which of the two future states will be the <b>next</b> one, as we cannot infer the ball&#39;s direction, whether it is going toward the top of the screen or the bottom. We need to know the history, that is, which of the two previous states was the actual previous <b>state</b>, in order to understand what the <b>next</b> <b>state</b> will be. In this case, the future <b>state</b> is <b>not</b> independent of the <b>past</b>: Note. Notice that the arrow is <b>not</b> ...", "dateLastCrawled": "2022-01-03T15:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why <b>do we need Markov Chain? - Quora</b>", "url": "https://www.quora.com/Why-do-we-need-Markov-Chain", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-<b>do-we-need-Markov-Chain</b>", "snippet": "Answer (1 of 3): Mathematics is interconnected. There is very, very little that <b>can</b> <b>only</b> be resolved via one particular path. That said, the key idea behind <b>Markov</b> Chains is simple: <b>can</b> you completely ignore history beyond a certain point? Or, more accurately: if I completely account for recent ...", "dateLastCrawled": "2022-01-03T05:22:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CPSC 540: <b>Machine</b> <b>Learning</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Courses/540-W20/L18.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Courses/540-W20/L18.pdf", "snippet": "Digression: Local <b>Markov</b> <b>Property</b> and <b>Markov</b> Blanket Approximate inference methods often useconditional p(x j jx j), where x k j means \\x i for all iexcept xkj&quot;: xk1;x 2;:::;xk j 1;x k j+1;:::;x k d. In UGMs, the conditional simpli es due toconditional independence, p(x jjx j) = p(x j jx nei( )); thislocal <b>Markov</b> propertymeans conditional only depends on neighbours. We say that theneighbours of x j are its \\<b>Markov</b> blnkaet&quot;. Iterated Conditional Mode Gibbs Sampling Digression: Local <b>Markov</b> ...", "dateLastCrawled": "2021-11-12T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Memorylessness and Markov Property</b> - LinkedIn", "url": "https://www.linkedin.com/pulse/memorylessness-markov-property-sreenath-s", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/memorylessness-<b>markov</b>-<b>property</b>-sreenath-s", "snippet": "Memorylessness is the <b>property</b> of a probability distribution by virtue of which it is independent of the events occurred in past. We usually say, a process begins at time t=0 and continues till ...", "dateLastCrawled": "2021-04-29T03:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Hidden Markov Model</b>. Elaborated with examples | Towards Data Science", "url": "https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>markov</b>-and-<b>hidden-markov-model</b>-3eec42298d75", "snippet": "<b>Markov</b> Model as a Finite State <b>Machine</b> from Fig.9. data \u2014Image by Author. The Viterbi algorithm is a dynamic programming algorithm similar to the forward procedure which is often used to find maximum likelihood. Instead of tracking the total probability of generating the observations, it tracks the maximum probability and the corresponding state sequence. Consider the sequence of emotions : H,H,G,G,G,H for 6 consecutive days. Using the Viterbi algorithm we will find out the more likelihood ...", "dateLastCrawled": "2022-01-30T23:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Colleen M. Farrelly</b> - cours.polymtl.ca", "url": "https://cours.polymtl.ca/mth6301/mth8302/Farrelly-Machine_Learning_by_Analogy.pdf", "isFamilyFriendly": true, "displayUrl": "https://cours.polymtl.ca/mth6301/mth8302/Farrelly-<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>.pdf", "snippet": "<b>property</b>\u2014may require unreasonably wide networks). ... geometry, and <b>Markov</b> chains. Useful in combination with other <b>machine</b> <b>learning</b> methods to provide extra insight (ex. spectral clustering). 39 K-means algorithm with weighting and dimension reduction components of similarity measure. Simplify balls of string to warm colors and cool colors before untangling. Can be reformulated as a graph clustering problem. Partition subcomponents of a graph based on flow equations. www.simplepastimes ...", "dateLastCrawled": "2021-12-14T17:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>MCMC</b> Intuition for Everyone. Easy? I tried. | by ... - Towards Data Science", "url": "https://towardsdatascience.com/mcmc-intuition-for-everyone-5ae79fff22b1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>mcmc</b>-intuition-for-everyone-5ae79fff22b1", "snippet": "But, before Jumping onto <b>Markov</b> Chains let us learn a little bit about <b>Markov</b> <b>Property</b>. Suppose you have a system of M possible states, and you are hopping from one state to another. Don\u2019t get confused yet. A concrete example of a system is the weather which jumps from hot to cold to moderate states. Or another system could be the stock market which jumps from Bear to Bull to stagnant states. <b>Markov</b> <b>Property</b> says that given a process which is at a state Xn at a particular point of time ...", "dateLastCrawled": "2022-02-03T01:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to explain &#39;<b>Markov</b> <b>Property</b>&#39; to a student, 11 years old - Quora", "url": "https://www.quora.com/How-can-you-explain-Markov-Property-to-a-student-11-years-old", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-can-you-explain-<b>Markov</b>-<b>Property</b>-to-a-student-11-years-old", "snippet": "Answer (1 of 3): This is going to be tough. I have not interacted with a 11 year old student in many years. The last time when I did interact was when I was 11 years old myself. I will hence try to explain here <b>Markov</b> <b>property</b> in words which would resonate with a much younger version of me. Let&#39;...", "dateLastCrawled": "2022-01-07T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Do recurrent neural networks have the <b>Markov</b> <b>property</b>? - Quora", "url": "https://www.quora.com/Do-recurrent-neural-networks-have-the-Markov-property", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Do-recurrent-neural-networks-have-the-<b>Markov</b>-<b>property</b>", "snippet": "Answer (1 of 2): Definitely!* The <b>Markov</b> <b>property</b> exactly defines the <b>property</b> of being \u201cmemoryless\u201d: the conditional probability distribution of the next state, conditioned on both the past states and the current state, is equal to the conditional probability of the next state given the current...", "dateLastCrawled": "2022-01-15T00:06:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Neural Networks | Abdelrahman Elogeel&#39;s Blog", "url": "https://elogeel.wordpress.com/category/artificial-intelligence/machine-learning/neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://elogeel.wordpress.com/category/artificial-intelligence/<b>machine</b>-<b>learning</b>/neural...", "snippet": "<b>Learning</b> Rate is variable that controls how big a step the gradient descent takes downhill. ... present, the future does not depend on the past. A process with this property is called Markov process. The term strong <b>Markov property is similar</b> to this, except that the meaning of \u201cpresent\u201d is defined in terms of a certain type of random variable, which might be specified in terms of the outcomes of the stochastic process itself, known as a stopping time. A hidden Markov model (HMM) is a ...", "dateLastCrawled": "2021-12-10T12:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> | <b>Abdelrahman Elogeel&#39;s Blog</b>", "url": "https://elogeel.wordpress.com/category/artificial-intelligence/machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://elogeel.wordpress.com/category/artificial-intelligence/<b>machine</b>-<b>learning</b>", "snippet": "<b>Machine</b> <b>learning</b> is related to artificial intelligence (Russell and Norvig 1995) because an intelligent system should be able to adapt to changes in its environment. Data mining is the name coined in the business world for the application of <b>machine</b> <b>learning</b> algorithms to large amounts of data (Weiss and Indurkhya 1998). In computer science, it is also called knowledge discovery in databases (KDD). Chapter\u2019s Important Keywords: <b>Machine</b> <b>Learning</b>. Data Mining. Descriptive Model. Predictive ...", "dateLastCrawled": "2022-01-23T10:23:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(markov property)  is like +(assumption that the next step in a task depends only on the current state, not on past steps)", "+(markov property) is similar to +(assumption that the next step in a task depends only on the current state, not on past steps)", "+(markov property) can be thought of as +(assumption that the next step in a task depends only on the current state, not on past steps)", "+(markov property) can be compared to +(assumption that the next step in a task depends only on the current state, not on past steps)", "machine learning +(markov property AND analogy)", "machine learning +(\"markov property is like\")", "machine learning +(\"markov property is similar\")", "machine learning +(\"just as markov property\")", "machine learning +(\"markov property can be thought of as\")", "machine learning +(\"markov property can be compared to\")"]}