{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Language Attrition: <b>Can You Forget Your</b> Native Language?", "url": "https://www.scienceabc.com/humans/can-you-forget-your-first-language.html", "isFamilyFriendly": true, "displayUrl": "https://www.scienceabc.com/humans/<b>can-you-forget-your</b>-first-language.html", "snippet": "As an adult, the chances of <b>forgetting</b> your native language are much less than for a child. Language attrition is the formal term for the gradual reduction or <b>loss</b> of linguistic abilities in an individual. In most cases, the language that is affected is the native or first language (<b>L1</b>). The lack of contact with the first language due to a change in environment results in a person\u2019s gradual <b>loss</b> in proficiency.", "dateLastCrawled": "2022-02-02T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>About learning and forgetting English as second language</b>", "url": "https://www.researchgate.net/post/About-learning-and-forgetting-English-as-second-language", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>About-learning-and-forgetting-English-as-second-language</b>", "snippet": "Moreover, <b>L1</b> attrition is not a process that leads to entire <b>loss</b> of <b>L1</b> knowledge but rather as a convergence towards an L2 whereby attriters take up L2 structures in some features of grammar ...", "dateLastCrawled": "2022-01-25T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Forgetting</b> your native language: consequences to career prospects ...", "url": "https://languageattrition.org/workshop2018/", "isFamilyFriendly": true, "displayUrl": "https://languageattrition.org/workshop2018", "snippet": "Sometimes it may then seem <b>like</b> a good idea to use the <b>L1</b> (e.g. when a patient or client is a native/proficient speaker of that language). The assumption is that, if you can do <b>something</b> in the L2, you can also do it in the <b>L1</b> \u2013 at worst, you may have to learn the appropriate vocabulary. Other common problems, for example the lack of appropriate style or interactive norms, are simply not on the radar, or taken to be an entirely \u2018cultural\u2019 and not a \u2018linguistic\u2019 problem.", "dateLastCrawled": "2022-01-17T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(DOC) <b>the effect of L2 on L1</b> | Mohammad Motamedi - Academia.edu", "url": "https://www.academia.edu/8840375/the_effect_of_L2_on_L1", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/8840375", "snippet": "The study on language attrition has been classified into the following categories: <b>L1</b> <b>loss</b> in <b>L1</b> environment: Dialect <b>loss</b> <b>L1</b> <b>loss</b> in L2 environment: Immigrant L2 <b>loss</b> in <b>L1</b> environment: Foreign language attrition L2 <b>loss</b> in L2 environment: Language reversion in elderly people Hansen (2001a) remarked that &amp;quota; language attrition has been studied for two reasons; First of all, researchers have taken interest in knowing attrition processes and then, it has got considerable pedagogical ...", "dateLastCrawled": "2022-01-13T00:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Second-language attrition</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Second-language_attrition", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Second-language_attrition</b>", "snippet": "Hansen quotes Loftus &amp; Loftus (1976) to describe <b>forgetting</b>: &quot;[\u2026] much <b>like</b> being unable to find <b>something</b> that we have misplaced somewhere&quot; (1999: 10). Cohen states, evidence for knowing that a learner is not able to &quot;find&quot; <b>something</b>, is the use of the so-called progressive retrieval (1986). Thereby, the learner is unable to express <b>something</b> that is in his mind and consequently uses an incorrect form. He eventually remembers the correct one (Cohen 1986; Olshtain 1989). Time is considered ...", "dateLastCrawled": "2022-01-23T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Now you speak it, now you don&#39;t: <b>L1</b> <b>attrition in international adoption</b> ...", "url": "https://www.academia.edu/4751888/Now_you_speak_it_now_you_dont_L1_attrition_in_international_adoption", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/4751888/Now_you_speak_it_now_you_dont_<b>L1</b>_attrition_in...", "snippet": "Attrition The <b>loss</b> of language skills: <b>Forgetting</b> a language learned at school L2 attrition <b>Forgetting</b> one\u2019s mothertongue <b>L1</b> attrition \u2193 impossible? Study of attrition gives insight on How human brain works L2 acquisition Factors leading to <b>L1</b> attrition L2 environment No or limited exposure to <b>L1</b> Level of education Age of onset of attrition (+ length of time since) Language attitude, motivation Use Context of international adoption Children adopted abroad are usually old enough to ...", "dateLastCrawled": "2022-01-19T01:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "neural networks - Matlab - Single Layer Perceptron - output? - Cross ...", "url": "https://stats.stackexchange.com/questions/76865/matlab-single-layer-perceptron-output", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/76865/matlab-single-layer-perceptron-output", "snippet": "If on the other hand you&#39;re using a squared <b>loss</b> (which it seems <b>like</b> you are, but is normally not the right choice for classification) the update will look <b>like</b> $$ w_\\text{new} = w_\\text{old} + \\eta\\sum_{j}[d(j) - \\sigma(x(j))][1 - \\sigma(x(j))]\\sigma(x(j))x(j). $$ The reason your weights are diverging is probably due to <b>forgetting</b> the parenthesis when translating the above, you have . w(new) = w + step/N * sum_all(d(j) - o(j) * o(j) * (1 - o(j)) * x(j)) which should be. w(new) = w + step/N ...", "dateLastCrawled": "2022-01-30T08:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>L1</b> attrition and the mental lexicon", "url": "https://hal.archives-ouvertes.fr/hal-00981094/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/hal-00981094/document", "snippet": "<b>something</b> that all bilinguals experience to some degree. The ensuing change which can be ... <b>loss</b> or interference. A certain amount of flexibility may even be an intrinsic characteristic of open-class systems such as the lexicon. An interesting perspective on the effects of <b>loss</b> in the lexical system are the computer simulations of vocabulary <b>loss</b> provided by Meara (2004). His models are relatively small and loosely interlocking systems of 2,500 items, each of which is connected to two other ...", "dateLastCrawled": "2022-01-06T19:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[<b>L2 Review] I keep forgetting what I covered</b> : CFA", "url": "https://www.reddit.com/r/CFA/comments/n3ne0n/l2_review_i_keep_forgetting_what_i_covered/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/CFA/comments/n3ne0n/<b>l2_review_i_keep_forgetting_what_i_covered</b>", "snippet": "It was <b>something</b> along the lines of, \u201cthe outcome isn\u2019t the goal. The goal is to study every day and go through the process, even when you\u2019re tired and don\u2019t feel <b>like</b> it.\u201d I\u2019m definitely doing it a disservice by paraphrasing but basically the goal should be to put yourself through studying every day. I\u2019ve applied this to weight <b>loss</b>. My goal isn\u2019t to lose 40 pounds. My goal is to eat right every day and get 20 minutes outside. Already down 10 pounds in the last month and a ...", "dateLastCrawled": "2021-05-03T04:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are the <b>Symptoms of Damage to the Hippocampus</b>?", "url": "https://www.thehealthboard.com/what-are-the-symptoms-of-damage-to-the-hippocampus.htm", "isFamilyFriendly": true, "displayUrl": "https://www.thehealthboard.com/what-are-the-<b>symptoms-of-damage-to-the-hippocampus</b>.htm", "snippet": "Subscribe to our newsletter and learn <b>something</b> new every day. Wikibuy Review: A Free Tool ... any of these functions may be impaired. One symptom <b>of damage to the hippocampus</b> is amnesia, the <b>loss</b> of some portion of the memory. Hippocampus damage can also lead to poor impulse control , hyperactivity, and difficulty with spatial navigation or memory. <b>Damage to the hippocampus</b> may result in amnesia. <b>Damage to the hippocampus</b> may occur in a number of different ways. One common way this area of ...", "dateLastCrawled": "2022-02-02T23:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Language Attrition: <b>Can You Forget Your</b> Native Language?", "url": "https://www.scienceabc.com/humans/can-you-forget-your-first-language.html", "isFamilyFriendly": true, "displayUrl": "https://www.scienceabc.com/humans/<b>can-you-forget-your</b>-first-language.html", "snippet": "As an adult, the chances of <b>forgetting</b> your native language are much less than for a child. Language attrition is the formal term for the gradual reduction or <b>loss</b> of linguistic abilities in an individual. In most cases, the language that is affected is the native or first language (<b>L1</b>). The lack of contact with the first language due to a change in environment results in a person\u2019s gradual <b>loss</b> in proficiency. A person who undergoes language attrition is commonly a bilingual or a ...", "dateLastCrawled": "2022-02-02T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>About learning and forgetting English as second language</b>", "url": "https://www.researchgate.net/post/About-learning-and-forgetting-English-as-second-language", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>About-learning-and-forgetting-English-as-second-language</b>", "snippet": "Moreover, <b>L1</b> attrition is not a process that leads to entire <b>loss</b> of <b>L1</b> knowledge but rather as a convergence towards an L2 whereby attriters take up L2 structures in some features of grammar ...", "dateLastCrawled": "2022-01-25T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Review of <b>the Litreture, Vocabulary Attrition among EFL</b> ... - Academia.edu", "url": "https://www.academia.edu/983156/Review_of_the_Litreture_Vocabulary_Attrition_among_EFL_Persian_Learners_in_Three_Recent_Decades", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/983156/Review_of_<b>the_Litreture_Vocabulary_Attrition_among_EFL</b>...", "snippet": "Language attrition is preferred in comparison to language <b>loss</b> whereby many psycholinguistics claimed that <b>something</b> which is committed to long term memory cannot be ever removed (Weltens and Grendel, 1993). The taxonomical framework is proposed by Van Els (1986, as cited in Kopke and Schimd, 2004) which has been classified into the following categories: 1. <b>L1</b> <b>loss</b> in <b>L1</b> environment: Dialect <b>loss</b> 2. <b>L1</b> <b>loss</b> in L2 environment: Immigrant 3. L2 <b>loss</b> in <b>L1</b> environment: Foreign language attrition ...", "dateLastCrawled": "2021-11-18T13:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Validation of the Threshold Hypothesis in Computer Aided</b> English ...", "url": "https://scialert.net/fulltext/?doi=jas.2013.5716.5722", "isFamilyFriendly": true, "displayUrl": "https://scialert.net/fulltext/?doi=jas.2013.5716.5722", "snippet": "L2 <b>loss</b> in an <b>L1</b> setting is usually observed in individuals who have lost the ability to use an L2 that was perhaps studied at school in their <b>L1</b> setting. Finally, L2 <b>loss</b> in an L2 environment is most commonly observed among immigrant communities without formal training in or immediate access to their L2 who lose that L2 as they age and revert to their <b>L1</b>. This study mainly concentrates on the <b>forgetting</b> of L2 (English) learners in an <b>L1</b> (Chinese mandarin) language environment.", "dateLastCrawled": "2022-01-19T17:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Defining language attrition</b> - ResearchGate", "url": "https://www.researchgate.net/publication/282851959_Defining_language_attrition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/282851959", "snippet": "Recent studies have extended this debate to first language <b>loss</b> (<b>L1</b> attrition). This article gives an overview of the first event\u2010related brain potential (ERP) studies on <b>L1</b> attrition and L2 ...", "dateLastCrawled": "2022-01-28T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 5, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Second-language attrition</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Second-language_attrition", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Second-language_attrition</b>", "snippet": "Hansen quotes Loftus &amp; Loftus (1976) to describe <b>forgetting</b>: &quot;[\u2026] much like being unable to find <b>something</b> that we have misplaced somewhere&quot; (1999: 10). Cohen states, evidence for knowing that a learner is not able to &quot;find&quot; <b>something</b>, is the use of the so-called progressive retrieval (1986). Thereby, the learner is unable to express <b>something</b> that is in his mind and consequently uses an incorrect form. He eventually remembers the correct one (Cohen 1986; Olshtain 1989). Time is considered ...", "dateLastCrawled": "2022-01-23T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Chapter 1", "url": "http://www.robwaring.org/classes/thesis/2004/Language_Attrition.doc", "isFamilyFriendly": true, "displayUrl": "www.robwaring.org/classes/thesis/2004/Language_Attrition.doc", "snippet": "The abbreviation is L2 <b>similar</b> to <b>L1</b> and this is the common term. Generally, the L2 refers to another language after the <b>L1</b>, regardless of whether it is the second, third, fourth, or fifth language. For instance, a child\u2019s native language is Japanese, and in his life, he acquired English speaking ability. This means that his <b>L1</b> is Japanese, and the L2 is English. 1.3 Differences between <b>L1</b> and L2. To distinguish the differences between <b>L1</b> and L2, some background information will be needed ...", "dateLastCrawled": "2021-08-26T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Psych 461_S15: Work on your form(ant)! Changes in <b>L1</b> during L2 acquisition", "url": "https://psych461.blogspot.com/2015/04/work-on-your-formant-changes-in-l1.html", "isFamilyFriendly": true, "displayUrl": "https://psych461.blogspot.com/2015/04/work-on-your-formant-changes-in-<b>l1</b>.html", "snippet": "Ortega talks about <b>something</b> <b>similar</b>, saying that because the <b>L1</b> speakers have had mass exposure to the language, they are more set in their pronunciation patterns of the language. Ortega agrees with Chang that fossilization is a misconception by describing cases in which L2 learners are perceived to be accentless, and so have worked past the <b>L1</b> filter when learning L2. Chang&#39;s work takes it a step further by describing the <b>L1</b> as a changing entity, showing that L2 acquisition has bi ...", "dateLastCrawled": "2022-01-06T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python - <b>Overfitting</b> in neural network - Cross Validated", "url": "https://stats.stackexchange.com/questions/292700/overfitting-in-neural-network", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/292700", "snippet": "<b>Overfitting</b> is <b>something</b> that happens gradually, so it is sometimes hard to say. Also, whether a model is &quot;good&quot; or not depends a lot on context. If you need 99% accuracy for your model to be used in production then the values are not &quot;good&quot;. However, the values you show for train and test <b>loss</b>, accuracy do not indicate a problem with <b>overfitting</b> to me. It is normal to see a slight drop in performance between training values and test values. Not only that, but is often acceptable to have a ...", "dateLastCrawled": "2022-02-01T10:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Post <b>L1</b> Thoughts/Feelings : CFA", "url": "https://www.reddit.com/r/CFA/comments/lms0ft/post_l1_thoughtsfeelings/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/CFA/comments/lms0ft/post_<b>l1</b>_thoughtsfeelings", "snippet": "Post <b>L1</b> Thoughts/Feelings. General information. Just took <b>L1</b> two days ago and have had some time to reflect. I went in with really good mock scores (73, 78, and 75 on two Kaplan CBT and one CFAI six hour paper based) and was feeling confident. As more time has gone by am starting to feel quite deflated - I immediately recognized around 28-30 ...", "dateLastCrawled": "2022-01-23T14:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About learning and forgetting English as second language</b>", "url": "https://www.researchgate.net/post/About-learning-and-forgetting-English-as-second-language", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>About-learning-and-forgetting-English-as-second-language</b>", "snippet": "In a bilingual situation language <b>loss</b> <b>can</b> be demonstrated as <b>L1</b> or L2 <b>loss</b>. Some researchers view attrition in the light of a decline in individual\u2019s abilities, commonly measured dramatically ...", "dateLastCrawled": "2022-01-25T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Validation of the Threshold Hypothesis in Computer Aided</b> English ...", "url": "https://scialert.net/fulltext/?doi=jas.2013.5716.5722", "isFamilyFriendly": true, "displayUrl": "https://scialert.net/fulltext/?doi=jas.2013.5716.5722", "snippet": "As illustrated in Table 1, <b>L1</b> <b>loss</b> in an <b>L1</b> environment <b>can</b> be found among people with dementia or those with aphasia in the situation of native language. <b>L1</b> <b>loss</b> in an L2 setting <b>can</b> be found among immigrants who lose their first language in the new environment. L2 <b>loss</b> in an <b>L1</b> setting is usually observed in individuals who have lost the ability to use an L2 that was perhaps studied at school in their <b>L1</b> setting. Finally, L2 <b>loss</b> in an L2 environment is most commonly observed among ...", "dateLastCrawled": "2022-01-19T17:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Psych 461_S15: Work on your form(ant)! Changes in <b>L1</b> during L2 acquisition", "url": "https://psych461.blogspot.com/2015/04/work-on-your-formant-changes-in-l1.html", "isFamilyFriendly": true, "displayUrl": "https://psych461.blogspot.com/2015/04/work-on-your-formant-changes-in-<b>l1</b>.html", "snippet": "Since writing a blog post about <b>L1</b> attrition (language <b>forgetting</b>), I have been intrigued by the <b>thought</b> of L2 influencing and shaping the course of <b>L1</b>. We haven&#39;t had much chance to talk about L2-<b>L1</b> influences, so I decided to do more research. I found an interesting study in phonetic science, which I was prompted to use because we have also not had a chance to talk in depth about acquisition of sounds in a language. In his study, Charles B. Chang looked at the influence of L2 (Korean) on ...", "dateLastCrawled": "2022-01-06T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(DOC) <b>the effect of L2 on L1</b> | Mohammad Motamedi - Academia.edu", "url": "https://www.academia.edu/8840375/the_effect_of_L2_on_L1", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/8840375", "snippet": "The study on language attrition has been classified into the following categories: <b>L1</b> <b>loss</b> in <b>L1</b> environment: Dialect <b>loss</b> <b>L1</b> <b>loss</b> in L2 environment: Immigrant L2 <b>loss</b> in <b>L1</b> environment: Foreign language attrition L2 <b>loss</b> in L2 environment: Language reversion in elderly people Hansen (2001a) remarked that &amp;quota; language attrition has been studied for two reasons; First of all, researchers have taken interest in knowing attrition processes and then, it has got considerable pedagogical ...", "dateLastCrawled": "2022-01-13T00:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Language Attrition: The next phase</b> - ResearchGate", "url": "https://www.researchgate.net/publication/257945468_Language_Attrition_The_next_phase", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/257945468_<b>Language_Attrition_The_next_phase</b>", "snippet": "whether an individual <b>can</b> really forget an <b>L1</b> or L2 once learned, and how and why this \u2018<b>loss</b>\u2019 might proceed, is an intriguing one. One could even go so far as to say that most people", "dateLastCrawled": "2021-12-03T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Post <b>L1</b> Thoughts/Feelings : CFA", "url": "https://www.reddit.com/r/CFA/comments/lms0ft/post_l1_thoughtsfeelings/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/CFA/comments/lms0ft/post_<b>l1</b>_<b>thoughts</b>feelings", "snippet": "Post <b>L1</b> Thoughts/Feelings. Just took <b>L1</b> two days ago and have had some time to reflect. I went in with really good mock scores (73, 78, and 75 on two Kaplan CBT and one CFAI six hour paper based) and was feeling confident. As more time has gone by am starting to feel quite deflated - I immediately recognized around 28-30 questions I didn&#39;t know ...", "dateLastCrawled": "2022-01-23T14:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>AP Psychology Ch7/8: Memory and Cognition</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/174408562/ap-psychology-ch78-memory-and-cognition-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/174408562/<b>ap-psychology-ch78-memory-and-cognition</b>-flash-cards", "snippet": "<b>Forgetting</b> <b>can</b> be measured by asking people to recall, recognize, or relearn information. Different methods of measuring retention often produce different estimates of <b>forgetting</b>. Recognition measures tend to yield higher estimates of retention than recall measures. Assess ineffective encoding and decay as potential causes of <b>forgetting</b>. Some <b>forgetting</b>, including pseudoforgetting , is caused by ineffective encoding of information, which is usually due to lack of attention. Decay theory ...", "dateLastCrawled": "2019-11-06T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>PSYC2016 notes.docx - MEMORY L1 Explain what cognitive</b> psychology is ...", "url": "https://www.coursehero.com/file/94661833/PSYC2016-notesdocx/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/94661833/PSYC2016-notesdocx", "snippet": "<b>MEMORY L1 Explain what cognitive</b> psychology is and what makes it distinct to other forms of psychology-Cognitive psychology: internal processes involved in making sense of the environment, and deciding what action might be appropriate Cannot be observed draws on metaphors and methods from other domains Describe the frameworks and metaphors used by cognitive psychologists from the 1950\u2019s up to now-1950\u2019s- 80\u2019s \u2013 information processing Computer metaphor: mind is a symbol processing ...", "dateLastCrawled": "2021-10-04T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "If you forget <b>something</b> immediately after thinking it, is the <b>thought</b> ...", "url": "https://www.quora.com/If-you-forget-something-immediately-after-thinking-it-is-the-thought-still-imprinted-on-your-brain-and-it-simply-cant-be-accessed-for-the-time-being-or-has-the-thought-left-the-brain-permanently", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/If-you-forget-<b>something</b>-immediately-after-thinking-it-is-the...", "snippet": "Answer (1 of 6): I was sitting with my friends in Dubai an evening, my mother language is not English, one friend asked me what was name of a particular small bird in English? I tried very hard upon my brain to recall my memory but could not recall the name of bird. Same night I had a dream. I wa...", "dateLastCrawled": "2022-01-21T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[<b>L2 Review] I keep forgetting what I covered</b> : CFA", "url": "https://www.reddit.com/r/CFA/comments/n3ne0n/l2_review_i_keep_forgetting_what_i_covered/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/CFA/comments/n3ne0n/<b>l2_review_i_keep_forgetting_what_i_covered</b>", "snippet": "I took <b>level 1</b> in February and was one of the kids that failed while touching the MPS. Obviously it was a dark time lol but he released a video explaining all of it, sure, whatever, bad timing. But then he started going into goal-creating/reaching. It was <b>something</b> along the lines of, \u201cthe outcome isn\u2019t the goal. The goal is to study every day and go through the process, even when you\u2019re tired and don\u2019t feel like it.\u201d I\u2019m definitely doing it a disservice by paraphrasing but ...", "dateLastCrawled": "2021-05-03T04:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About learning and forgetting English as second language</b>", "url": "https://www.researchgate.net/post/About-learning-and-forgetting-English-as-second-language", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>About-learning-and-forgetting-English-as-second-language</b>", "snippet": "In a bilingual situation language <b>loss</b> <b>can</b> be demonstrated as <b>L1</b> or L2 <b>loss</b>. Some researchers view attrition in the light of a decline in individual\u2019s abilities, commonly measured dramatically ...", "dateLastCrawled": "2022-01-25T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Validation of the Threshold Hypothesis in Computer Aided</b> English ...", "url": "https://scialert.net/fulltext/?doi=jas.2013.5716.5722", "isFamilyFriendly": true, "displayUrl": "https://scialert.net/fulltext/?doi=jas.2013.5716.5722", "snippet": "<b>L1</b> <b>loss</b> in an L2 setting <b>can</b> be found among immigrants who lose their first language in the new environment. L2 <b>loss</b> in an <b>L1</b> setting is usually observed in individuals who have lost the ability to use an L2 that was perhaps studied at school in their <b>L1</b> setting. Finally, L2 <b>loss</b> in an L2 environment is most commonly observed among immigrant communities without formal training in or immediate access to their L2 who lose that L2 as they age and revert to their <b>L1</b>. This study mainly ...", "dateLastCrawled": "2022-01-19T17:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Selective Mutism Arising from First Language Attrition, Successfully ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4620318/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4620318", "snippet": "Language <b>loss</b> covers a variety of linguistic phenomena: it <b>can</b> be the result ... This last phenomenon is referred to as &quot;first-language (<b>L1</b>) attrition&quot;. This term <b>can</b> be described as a process of dealing with &quot;the non-pathological decrease of the mother tongue that had previously been acquired by an individual&quot;. 2. In other words, attrition refers to the change in linguistic behavior due to a frequent use of the L2 language that becomes dominant making the <b>L1</b> language subject to L2 influence ...", "dateLastCrawled": "2021-09-09T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Defining language attrition</b> - ResearchGate", "url": "https://www.researchgate.net/publication/282851959_Defining_language_attrition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/282851959", "snippet": "<b>compared</b> his <b>L1</b> skills to riding a . ... longer do <b>something</b> which s/he had . previously been able to do, and this . <b>loss</b> of pro ciency is not caused by a . deterioration of the brain due to age ...", "dateLastCrawled": "2022-01-28T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Chapter 1", "url": "http://www.robwaring.org/classes/thesis/2004/Language_Attrition.doc", "isFamilyFriendly": true, "displayUrl": "www.robwaring.org/classes/thesis/2004/Language_Attrition.doc", "snippet": "Once <b>L1</b> learners stop using a language, they forget some of what they have previously learned, such as letters, idioms, fluency, grammar, and so on, slowly. So, lack of using <b>L1</b> <b>can</b> lead <b>to forgetting</b> it, but if people use their L2 more than their <b>L1</b>, the L2 <b>can</b> take the role of the <b>L1</b>. This hypothesis also applies to some other potential reasons.", "dateLastCrawled": "2021-08-26T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[Title] Language attrition and multilingualism", "url": "https://www.let.rug.nl/languageattrition/Papers/SchmidEAL2012.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.let.rug.nl/languageattrition/Papers/SchmidEAL2012.pdf", "snippet": "environment for a long time <b>can</b> sometimes show signs of &#39;<b>forgetting</b>&#39; their <b>L1</b>. When they attempt to speak it, they may have difficulties in remembering certain words, they may make what appear to be lexical or grammatical &#39;mistakes&#39;, or they may speak their native language with a foreign accent. This is usually ascribed to a scenario where the . picture represented above has reversed: for these speakers, it is the second language (the language of the country to which they emigrated, and ...", "dateLastCrawled": "2021-12-12T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Review of recent advances in dealing with data size challenges in Deep ...", "url": "https://suneeta-mall.github.io/2021/12/31/data-in-deep-learning.html", "isFamilyFriendly": true, "displayUrl": "https://suneeta-mall.github.io/2021/12/31/data-in-deep-learning.html", "snippet": "When you sort your dataset descending by <b>loss</b> you are guaranteed to find <b>something</b> unexpected, strange, and helpful. Personally, I have found <b>loss</b> a very good measure to find poorly labeled samples. So, the natural question would be \u201cshould we explore how we <b>can</b> use the <b>loss</b> as a measure to prune the dataset?\u201d. It\u2019s not until NeurIPS 2021, 21 that this was properly investigated. This Standford study looked into the initial <b>loss</b> gradient norm of individual training examples, averaged ...", "dateLastCrawled": "2022-02-01T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "l6 - retrieval and <b>forgetting</b> p11 Flashcards | Quizlet", "url": "https://quizlet.com/654112676/l6-retrieval-and-forgetting-p11-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/654112676/l6-retrieval-and-<b>forgetting</b>-p11-flash-cards", "snippet": "- The older memory has less <b>forgetting</b> still to suffer from <b>compared</b> to the younger - Retention of knowledge is determined by how successfully we initially learn it - Memory trace has a half-life so the better the initial learning, the better the memory. mechanisms of <b>forgetting</b>. decay and interference. decay. <b>loss</b> of memory due to the passage of time, during which the memory trace is not used gradual weakening of memory over time may be particularly important in working memory and is the ...", "dateLastCrawled": "2021-12-19T15:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What should I do when my <b>neural network</b> doesn&#39;t learn? - Cross Validated", "url": "https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/352036", "snippet": "<b>Loss</b> functions are not measured on the correct scale (for example, cross-entropy <b>loss</b> <b>can</b> be expressed in terms of probability or logits) ... <b>Forgetting</b> to scale the testing data; Scaling the testing data using the statistics of the test partition instead of the train partition; <b>Forgetting</b> to un-scale the predictions (e.g. pixel values are in [0,1] instead of [0, 255]). Here&#39;s an example of a question where the problem appears to be one of model configuration or hyperparameter choice, but ...", "dateLastCrawled": "2022-01-30T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Forgetting</b> my stick | Bob&#39;s Spinal Blog", "url": "https://rtclay.com/2012/05/26/forgetting-my-stick/", "isFamilyFriendly": true, "displayUrl": "https://rtclay.com/2012/05/26/<b>forgetting</b>-my-stick", "snippet": "Well I am standing on the drive but <b>something</b> is missing. I have a free hand, an odd feeling, back to the house to get my stick! Ten years ago, I returned home after almost six months in hospital following my accident. At that stage of my recovery, I was able to walk short distances with a pair of crutches. So <b>forgetting</b> my stick was not an option. In fact, leaving the house without help was not possible. Our home had a high step and steep drive, so I needed help to get out the front door ...", "dateLastCrawled": "2022-01-26T23:40:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Regularization \u2014 Understanding <b>L1</b> and L2 regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-<b>l1</b>-and-l2...", "snippet": "Understanding what regularization is and why it is required for <b>machine</b> <b>learning</b> and diving deep to clarify the importance of <b>L1</b> and L2 regularization in Deep <b>learning</b>.", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, the ... Mean Absolute Error, <b>L1</b> <b>Loss</b> (used by PerceptiLabs\u2019 Regression component): sums the absolute differences between the predictions and ground truth, and finds the average. <b>Loss</b> functions are used in a variety of use cases. The following table shows common image processing use cases where you might apply these, and other <b>loss</b> functions: Image Source: PerceptiLabs <b>Loss</b> in PL. Configuring a <b>loss</b> ...", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "is known as <b>L1</b>-norm, while the latter is known as the L2-norm. Keep in mind that L2-norm is more sensitive than <b>L1</b>-norm to large-valued outliers. Ridge and LASSO regularizations are based on L2-norm and <b>L1</b>-norm, respectively, while Elastic Net regularization is based on the mix of two. 2.6 What does a <b>machine</b> <b>learning</b> <b>learning</b>-curve measure ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "regression - Why <b>L1</b> norm for sparse models - Cross Validated", "url": "https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/45643", "snippet": "Show activity on this post. With a sparse model, we think of a model where many of the weights are 0. Let us therefore reason about how <b>L1</b>-regularization is more likely to create 0-weights. Consider a model consisting of the weights . With <b>L1</b> regularization, you penalize the model by a <b>loss</b> function = .", "dateLastCrawled": "2022-01-26T23:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - <b>L1</b>-norm vs l2-norm as cost function when ...", "url": "https://stackoverflow.com/questions/43301036/l1-norm-vs-l2-norm-as-cost-function-when-standardizing", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43301036", "snippet": "I feel that the l2-norm will penalize less the model than the <b>l1</b>-norm since squaring a number that is between 0 and 1 will always result in a lower number. So my question is, is it ok to use the l2-norm when both the input and the output are standardized? <b>machine</b>-<b>learning</b> statistics gradient-descent. Share. Follow edited Apr 10 &#39;17 at 12:08. jeremie. asked Apr 8 &#39;17 at 22:34. jeremie jeremie. 807 8 8 silver badges 15 15 bronze badges. Add a comment | 1 Answer Active Oldest Votes. 1 It does ...", "dateLastCrawled": "2022-01-24T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0888613X21000141", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0888613X21000141", "snippet": "The <b>machine</b> <b>learning</b> approaches outperform state of the art approaches on Google, BATS and DiffVec datasets. As far as we know, neither <b>analogy</b> classification nor <b>analogy</b> completion have been investigated in the same way as we have proposed in this paper, namely <b>learning</b> a model, instead of starting from the parallelogram model. The paper is structured as follows. Section 2 recalls the postulates characterizing analogical proportions and identifies a rigorous method for enlarging a set of ...", "dateLastCrawled": "2021-11-13T01:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Gentle Introduction to Pix2Pix Generative</b> Adversarial Network", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-pix2pix-generative</b>...", "snippet": "In <b>analogy</b> to automatic language translation, we define automatic image-to-image translation as the task of translating one possible representation of a scene into another, given sufficient training data. \u2014 Image-to-Image Translation with Conditional Adversarial Networks, 2016. It is a challenging problem that typically requires the development of a specialized model and hand-crafted <b>loss</b> function for the type of translation task being performed. Classical approaches use per-pixel ...", "dateLastCrawled": "2022-02-02T13:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Deep learning</b> - lectures.alex.balgavy.eu", "url": "https://lectures.alex.balgavy.eu/ml-notes/deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://lectures.alex.balgavy.eu/ml-notes/<b>deep-learning</b>", "snippet": "<b>Deep learning</b> <b>Deep learning</b> systems (autodiff engines) Tensors. To scale up backpropagation, want to move from operations on scalars to tensors. Tensor: generalisation of vectors/matrices to higher dimensions. e.g. a 2-tensor has two dimensions, a 4-tensor has 4 dimensions. You can represent data as a tensor. e.g. an RGB image is a 3-tensor of the red, green, and blue values for each pixel. Functions on tensors. Functions have inputs and outputs, all of which are tensors. They implement ...", "dateLastCrawled": "2021-12-15T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Denoising Seismic Records with Image Translation Networks</b> | CSEG RECORDER", "url": "https://csegrecorder.com/articles/view/denoising-seismic-records-with-image-translation-networks", "isFamilyFriendly": true, "displayUrl": "https://csegrecorder.com/articles/view/<b>denoising-seismic-records-with-image</b>...", "snippet": "The pix2pix network is a generative <b>machine</b> <b>learning</b> algorithm. Based on Alec Radford, et. al\u2019s DCGAN [6] architecture, ... the <b>L1 loss is similar</b> to the L2 loss: except the second-degree norm is replaced with the first-degree norm: The alternative denoising strategies tested against the image translation network included total-variation filtering, bilateral filtering, and wavelet transform filtering. Figure 3. Results of various image denoising techniques on synthetic data. Upper left ...", "dateLastCrawled": "2022-01-12T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A deep <b>learning</b> framework for constitutive modeling based on temporal ...", "url": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "snippet": "These two features meet the requirement for sequence modeling in <b>machine</b> <b>learning</b>. Therefore, the nonlinear constitutive models may be classified as sequence modeling from the viewpoint of <b>machine</b> <b>learning</b>. Concrete material and steel material both exhibit significant ultra-long-term memory effects and many model-driven constitutive relationships were developed to simulate stress-strain curves of materials , , , , with ultra-long-term memory effect. For steel material, the traditional ...", "dateLastCrawled": "2022-01-20T12:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(l1 loss)  is like +(forgetting something)", "+(l1 loss) is similar to +(forgetting something)", "+(l1 loss) can be thought of as +(forgetting something)", "+(l1 loss) can be compared to +(forgetting something)", "machine learning +(l1 loss AND analogy)", "machine learning +(\"l1 loss is like\")", "machine learning +(\"l1 loss is similar\")", "machine learning +(\"just as l1 loss\")", "machine learning +(\"l1 loss can be thought of as\")", "machine learning +(\"l1 loss can be compared to\")"]}