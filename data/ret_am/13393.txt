{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is Dimensionality <b>Reduction</b>? Overview, and Popular Techniques", "url": "https://www.simplilearn.com/what-is-dimensionality-reduction-article", "isFamilyFriendly": true, "displayUrl": "https://www.simplilearn.com/what-is-<b>dimension</b>ality-<b>reduction</b>-article", "snippet": "By <b>reducing</b> <b>the number</b> of input <b>features</b>, thereby <b>reducing</b> <b>the number</b> <b>of dimensions</b> in the feature space. Hence, \u201cdimensionality <b>reduction</b>.\u201d To make a long story short, dimensionality <b>reduction</b> means <b>reducing</b> your feature set\u2019s <b>dimension</b>. Why Dimensionality <b>Reduction</b> is Important. Dimensionality <b>reduction</b> brings many advantages to your machine learning <b>data</b>, including: Fewer <b>features</b> mean less complexity; You will need less storage space because you have fewer <b>data</b>; Fewer <b>features</b> ...", "dateLastCrawled": "2022-02-02T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Top <b>10 Dimensionality Reduction Techniques For Machine Learning</b> ...", "url": "https://www.upgrad.com/blog/top-dimensionality-reduction-techniques-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/top-<b>dimensionality-reduction-techniques-for-machine-learning</b>", "snippet": "Principal Component Analysis (PCA): It is a method of <b>reducing</b> the dimensionality of a <b>data</b> set by transforming it into a new coordinate system such that the greatest variance in the <b>data</b> is explained by the first coordinate and the second greatest variance is explained by the second coordinate, and so on. 2. Factor Analysis: It is a statistical technique for extracting independent variables (also called factors) from a <b>data</b> set. The purpose is to simplify or reduce <b>the number</b> of variables ...", "dateLastCrawled": "2022-02-02T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Applied Dimensionality Reduction</b> \u2014 3 Techniques using Python \u2013 LearnDataSci", "url": "https://www.learndatasci.com/tutorials/applied-dimensionality-reduction-techniques-using-python/", "isFamilyFriendly": true, "displayUrl": "https://www.learn<b>data</b>sci.com/tutorials/<b>applied-dimensionality-reduction</b>-techniques...", "snippet": "Dimensionality <b>reduction</b> reduces <b>the number</b> <b>of dimensions</b> (also called <b>features</b> and attributes) of a dataset. It is <b>used</b> to remove redundancy and help both <b>data</b> scientists and machines extract useful patterns. The goal of this article. In the first part of this article, we&#39;ll discuss some dimensionality <b>reduction</b> theory and introduce various algorithms for <b>reducing</b> <b>dimensions</b> in various types of datasets. The second part of this article walks you through a case study, where we get our hands ...", "dateLastCrawled": "2022-02-02T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>What Is Dimension Reduction In Data Science</b>? - KDnuggets", "url": "https://www.kdnuggets.com/2019/01/dimension-reduction-data-science.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2019/01/<b>dimension</b>-<b>reduction</b>-<b>data</b>-science.html", "snippet": "The large <b>number</b> of <b>features</b> make the <b>data</b> set sparse. Furthermore, it takes a much larger space to store a <b>data</b> set with a large <b>number</b> of <b>features</b>. Moreover, it can get very difficult to analyse and visualize a <b>data</b> set with a large <b>number</b> <b>of dimensions</b>. <b>Dimension</b> <b>reduction</b> can reduce the time that is required to train our machine learning model and it can also benefit in eliminating over-fitting. This article outlines the techniques which we can follow to compress our <b>data</b> set onto a new ...", "dateLastCrawled": "2022-01-23T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Introduction to Dimensionality <b>Reduction</b> - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/dimensionality-reduction/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>dimension</b>ality-<b>reduction</b>", "snippet": "The higher <b>the number</b> of <b>features</b>, the harder it gets to visualize the training set and then work on it. Sometimes, most of these <b>features</b> are correlated, and hence redundant. This is where dimensionality <b>reduction</b> algorithms come into play. Dimensionality <b>reduction</b> is the process of <b>reducing</b> <b>the number</b> of random variables under consideration, by obtaining a set of principal variables. It can be divided into feature selection and feature extraction.", "dateLastCrawled": "2022-02-02T19:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Dimensionality Reduction</b> in <b>Data</b> Mining | by Uditha Maduranga | Towards ...", "url": "https://towardsdatascience.com/dimensionality-reduction-in-data-mining-f08c734b3001", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>dimensionality-reduction</b>-in-<b>data</b>-mining-f08c734b3001", "snippet": "Then <b>dimensionality reduction</b> is that you reduce those <b>features</b> of attributes of <b>data</b> by combining or merging them in such a way that it will not loose much of the significant characteristics of the original dataset. One of the major problem that occurs with high dimensional <b>data</b> is widely known as the \u201cCurse of <b>Dimensionality</b>\u201d. This pushes us to reduce the <b>dimensions</b> of our <b>data</b> if we want to use them for analysis.", "dateLastCrawled": "2022-01-30T10:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Dimensionality</b> <b>Reduction</b> toolbox in python | by ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/dimensionality-reduction-toolbox-in-python-9a18995927cd", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>dimensionality</b>-<b>reduction</b>-toolbox-in-python-9a18995927cd", "snippet": "<b>Dimension a lity</b> <b>reduction</b> is the process of <b>reducing</b> the total <b>number</b> of <b>features</b> in our feature set using strategies <b>like</b> feature selection or feature extraction. For example, a base that contains the characteristics of a car will be difficult to view as they are numerous. One could imagine merging the mileage and the age of the car to form the characteristic wear, provided that they are correlated. There are a multitude of algorithms for the <b>reduction</b> of <b>dimensionality</b>, there are mainly ...", "dateLastCrawled": "2022-01-31T22:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "4.2 Dimensionality <b>reduction</b> techniques: Visualizing complex <b>data</b> sets ...", "url": "https://compgenomr.github.io/book/dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html", "isFamilyFriendly": true, "displayUrl": "https://compgenomr.github.io/book/<b>dimension</b>ality-<b>reduction</b>-techniques-visualizing...", "snippet": "In statistics, <b>dimension</b> <b>reduction</b> techniques are a set of processes for <b>reducing</b> <b>the number</b> of random variables by obtaining a set of principal variables. For example, in the context of a gene expression matrix across different patient samples, this might mean getting a set of new variables that cover the variation in sets of genes. This way samples can be represented by a couple of principal variables instead of thousands of genes. This is useful for visualization, clustering and ...", "dateLastCrawled": "2022-02-01T20:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Dimensional <b>Reduction</b> with <b>LDA</b>. Maybe you are a <b>data</b> scientist given ...", "url": "https://medium.com/swlh/dimensional-reduction-with-lda-86db2a71430d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>dimension</b>al-<b>reduction</b>-with-<b>lda</b>-86db2a71430d", "snippet": "Imagine graphing a <b>data</b> set with 100 <b>features</b>! Another issue with higher <b>dimension</b> <b>data</b> is that as <b>the number</b> of <b>features</b> grows, the <b>data</b> points required to properly populate the <b>features</b> grows ...", "dateLastCrawled": "2022-01-28T05:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is dimensionality <b>reduction</b>? What is the difference between ...", "url": "https://datascience.stackexchange.com/questions/130/what-is-dimensionality-reduction-what-is-the-difference-between-feature-selecti", "isFamilyFriendly": true, "displayUrl": "https://<b>data</b>science.stackexchange.com/questions/130", "snippet": "What is dimensionality <b>reduction</b>: If you think of <b>data</b> in a matrix, where rows are instances and columns are attributes (or <b>features</b>), then dimensionality <b>reduction</b> is mapping this <b>data</b> matrix to a new matrix with fewer columns. For visualization, if you think of each matrix-column (attribute) as a <b>dimension</b> in feature space, then dimensionality <b>reduction</b> is projection of instances from the higher dimensional space (more columns) to a lower dimensional sub-space (fewer columns).", "dateLastCrawled": "2022-02-01T08:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Dimensionality Reduction</b> in <b>Data</b> Mining | by Uditha Maduranga | Towards ...", "url": "https://towardsdatascience.com/dimensionality-reduction-in-data-mining-f08c734b3001", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>dimensionality-reduction</b>-in-<b>data</b>-mining-f08c734b3001", "snippet": "Then <b>dimensionality reduction</b> is that you reduce those <b>features</b> of attributes of <b>data</b> by combining or merging them in such a way that it will not loose much of the significant characteristics of the original dataset. One of the major problem that occurs with high dimensional <b>data</b> is widely known as the \u201cCurse of <b>Dimensionality</b>\u201d. This pushes us to reduce the <b>dimensions</b> of our <b>data</b> if we want to use them for analysis.", "dateLastCrawled": "2022-01-30T10:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Top <b>10 Dimensionality Reduction Techniques For Machine Learning</b> ...", "url": "https://www.upgrad.com/blog/top-dimensionality-reduction-techniques-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/top-<b>dimensionality-reduction-techniques-for-machine-learning</b>", "snippet": "Dimensionality <b>reduction</b> is a technique <b>used</b> in <b>data</b> mining to map high-dimensional <b>data</b> into a low-dimensional representation in order to visualize <b>data</b> and find patterns that are otherwise not apparent using traditional methods. It is often <b>used</b> in conjunction with clustering techniques or classification techniques to project the <b>data</b> into a lower dimensional space to facilitate visualizing the <b>data</b> and finding patterns.", "dateLastCrawled": "2022-02-02T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Applied Dimensionality Reduction</b> \u2014 3 Techniques using Python \u2013 LearnDataSci", "url": "https://www.learndatasci.com/tutorials/applied-dimensionality-reduction-techniques-using-python/", "isFamilyFriendly": true, "displayUrl": "https://www.learn<b>data</b>sci.com/tutorials/<b>applied-dimensionality-reduction</b>-techniques...", "snippet": "Dimensionality <b>reduction</b> reduces <b>the number</b> <b>of dimensions</b> (also called <b>features</b> and attributes) of a dataset. It is <b>used</b> to remove redundancy and help both <b>data</b> scientists and machines extract useful patterns. The goal of this article. In the first part of this article, we&#39;ll discuss some dimensionality <b>reduction</b> theory and introduce various algorithms for <b>reducing</b> <b>dimensions</b> in various types of datasets. The second part of this article walks you through a case study, where we get our hands ...", "dateLastCrawled": "2022-02-02T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>The Multidimensional Scaling (MDS) algorithm for dimensionality reduction</b>", "url": "https://medium.datadriveninvestor.com/the-multidimensional-scaling-mds-algorithm-for-dimensionality-reduction-9211f7fa5345", "isFamilyFriendly": true, "displayUrl": "https://medium.<b>data</b>driveninvestor.com/<b>the-multidimensional-scaling-mds-algorithm-for</b>...", "snippet": "Dimensionality <b>reduction</b> or <b>dimension</b> <b>reduction</b> is the process of <b>reducing</b> <b>the number</b> of random variables under consideration by obtaining a set of principal variables [2]. The aim of <b>dimension</b> <b>reduction</b> procedures is to summarize the original p-dimensional <b>data</b> space in the form of lower k-dimensional components subspace. To achieve this goal, statistical and mathematical theory provided many approaches like Principal component analysis (PCA), Linear discriminant analysis (LDA), Factor ...", "dateLastCrawled": "2022-02-01T08:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "4.2 Dimensionality <b>reduction</b> techniques: Visualizing complex <b>data</b> sets ...", "url": "https://compgenomr.github.io/book/dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html", "isFamilyFriendly": true, "displayUrl": "https://compgenomr.github.io/book/<b>dimension</b>ality-<b>reduction</b>-techniques-visualizing...", "snippet": "As with other <b>dimension</b> <b>reduction</b> methods, you can choose how many lower <b>dimensions</b> you need. The main difference of t-SNE, as mentiones above, is that it tries to preserve the local structure of the <b>data</b>. This kind of local structure embedding is missing in the MDS algorithm, which also has a <b>similar</b> goal. MDS tries to optimize the distances as a whole, whereas t-SNE optimizes the distances with the local structure in mind. This is defined by the \u201cperplexity\u201d parameter in the arguments ...", "dateLastCrawled": "2022-02-01T20:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is dimensionality <b>reduction</b>? What is the difference between ...", "url": "https://datascience.stackexchange.com/questions/130/what-is-dimensionality-reduction-what-is-the-difference-between-feature-selecti", "isFamilyFriendly": true, "displayUrl": "https://<b>data</b>science.stackexchange.com/questions/130", "snippet": "What is dimensionality <b>reduction</b>: If you think of <b>data</b> in a matrix, where rows are instances and columns are attributes (or <b>features</b>), then dimensionality <b>reduction</b> is mapping this <b>data</b> matrix to a new matrix with fewer columns. For visualization, if you think of each matrix-column (attribute) as a <b>dimension</b> in feature space, then dimensionality <b>reduction</b> is projection of instances from the higher dimensional space (more columns) to a lower dimensional sub-space (fewer columns).", "dateLastCrawled": "2022-02-01T08:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Clustering and Dimensionality Reduction: Understanding the \u201cMagic</b> ...", "url": "https://www.imperva.com/blog/clustering-and-dimensionality-reduction-understanding-the-magic-behind-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.imperva.com/blog/<b>clustering-and-dimensionality-reduction-understanding-the</b>...", "snippet": "PCA guarantees finding the best linear transformation that reduces <b>the number</b> <b>of dimensions</b> with a minimum loss of information. Sometimes the information that was lost is regarded as noise \u2013 information that does not <b>represent</b> the phenomena we are trying to model, but is rather a side effect of some usually unknown processes. PCA process can be visualized as follows (Figure 3): Figure 3: PCA process visualized Following the process in the example, we might be content with just PC1 \u2013 one ...", "dateLastCrawled": "2022-02-02T22:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "3 New <b>Techniques for Data-Dimensionality Reduction in Machine Learning</b> ...", "url": "https://thenewstack.io/3-new-techniques-for-data-dimensionality-reduction-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thenewstack.io/3-new-<b>techniques-for-data-dimensionality-reduction-in-machine</b>...", "snippet": "A <b>number</b> of <b>techniques for data-dimensionality reduction</b> are available to estimate how informative each column is and, if needed, to skim it off the dataset. Back in 2015, we identified the seven most commonly <b>used</b> <b>techniques for data-dimensionality reduction</b>, including: Ratio of missing values. Low variance in the column values.", "dateLastCrawled": "2022-02-01T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning Explained: Dimensionality Reduction</b> | R-bloggers", "url": "https://www.r-bloggers.com/2017/07/machine-learning-explained-dimensionality-reduction/", "isFamilyFriendly": true, "displayUrl": "https://www.r-bloggers.com/2017/07/<b>machine-learning-explained-dimensionality-reduction</b>", "snippet": "<b>Features</b> selection as a basic <b>reduction</b>. The most obvious way to reduce dimensionality is to remove some <b>dimensions</b> and to select the more suitable variables for the problem. Here are some ways to select variables: Greedy algorithms which add and remove variables until some criterion is met. For instance, the stepwise regression with forward selection will add at each step the variable which improve the fit in the most significant way; Shrinking and penalization methods, which will add cost ...", "dateLastCrawled": "2022-01-29T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "40 Questions to test a <b>Data Scientist on Dimensionality Reduction</b> ...", "url": "https://quizlet.com/300364379/40-questions-to-test-a-data-scientist-on-dimensionality-reduction-techniques-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/300364379/40-questions-to-test-a-<b>data</b>-scientist-on-<b>dimension</b>ality...", "snippet": "Could the steps performed above <b>represent</b> a dimensionality <b>reduction</b> method? A. True B. False (A) Yes, Because Step 1 could be <b>used</b> <b>to represent</b> the <b>data</b> into 2 lower <b>dimensions</b>. Which of the following techniques would perform better for <b>reducing</b> <b>dimensions</b> of a <b>data</b> set? A. Removing columns which have too many missing values B. Removing columns which have high variance in <b>data</b> C. Removing columns with dissimilar <b>data</b> trends D. None of these (A) If a columns have too many missing values ...", "dateLastCrawled": "2021-11-18T18:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Applied Dimensionality Reduction</b> \u2014 3 Techniques using Python \u2013 LearnDataSci", "url": "https://www.learndatasci.com/tutorials/applied-dimensionality-reduction-techniques-using-python/", "isFamilyFriendly": true, "displayUrl": "https://www.learn<b>data</b>sci.com/tutorials/<b>applied-dimensionality-reduction</b>-techniques...", "snippet": "Dimensionality <b>reduction</b> reduces <b>the number</b> <b>of dimensions</b> (also called <b>features</b> and attributes) of a dataset. It is <b>used</b> to remove redundancy and help both <b>data</b> scientists and machines extract useful patterns. The goal of this article. In the first part of this article, we&#39;ll discuss some dimensionality <b>reduction</b> theory and introduce various algorithms for <b>reducing</b> <b>dimensions</b> in various types of datasets. The second part of this article walks you through a case study, where we get our hands ...", "dateLastCrawled": "2022-02-02T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Dimensionality Reduction and Deep Dive</b> Into ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/deep-dive-into-principal-component-analysis-fc64347c4d20", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>deep-dive-into-principal-component-analysis</b>-fc64347c4d20", "snippet": "All the above cases highlight the fact that for datasets with a large <b>number</b> of <b>features</b> (<b>dimensions</b>), we need to reduce <b>the number</b> of <b>features</b> before training any regressor/classifier to perform predictive tasks. The above drawbacks of high-dimensional <b>data</b> is commonly referred to as the Curse of Dimensionality. <b>Reducing</b> D <b>dimensions</b> to d <b>dimensions</b> Methods of Dimensionality <b>Reduction</b>. The problem of Dimensionality <b>Reduction</b> <b>can</b> be divided into 2 parts: (i) Feature Selection (ii) Feature ...", "dateLastCrawled": "2022-01-26T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Singular Value Decomposition for Dimensionality Reduction in</b> Python", "url": "https://machinelearningmastery.com/singular-value-decomposition-for-dimensionality-reduction-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>singular-value-decomposition-for-dimensionality</b>...", "snippet": "Input variables are also called <b>features</b>. We <b>can</b> consider the columns of <b>data</b> representing <b>dimensions</b> on an n-dimensional feature space and the rows of <b>data</b> as points in that space. This is a useful geometric interpretation of a dataset. In a dataset with k numeric attributes, you <b>can</b> visualize the <b>data</b> as a cloud of points in k-dimensional space \u2026 \u2014 Page 305, <b>Data</b> Mining: Practical Machine Learning Tools and Techniques, 4th edition, 2016. Having a large <b>number</b> <b>of dimensions</b> in the ...", "dateLastCrawled": "2022-02-02T12:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "16 Dimensionality <b>reduction</b> | <b>Tidy Modeling with R</b>", "url": "https://www.tmwr.org/dimensionality.html", "isFamilyFriendly": true, "displayUrl": "https://www.tmwr.org/<b>dimension</b>ality.html", "snippet": "16 Dimensionality <b>reduction</b>. Dimensionality <b>reduction</b> <b>can</b> be a good choice when you suspect there are \u201ctoo many\u201d variables. An excess of variables, usually predictors, <b>can</b> be a problem because it is difficult to understand or visualize <b>data</b> in higher <b>dimensions</b>. For example, in high dimensional biology experiments, one of the first tasks is ...", "dateLastCrawled": "2022-01-28T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Multivariate Data Methods</b> | BIOSCI 220: Quantitative Biology", "url": "https://stats-uoa.github.io/BIOSCI220/multivariate-data-methods.html", "isFamilyFriendly": true, "displayUrl": "https://stats-uoa.github.io/BIOSCI220/<b>multivariate-data-methods</b>.html", "snippet": "<b>Dimension</b> <b>reduction</b>. <b>Reduction</b> <b>of dimensions</b> is needed when there are far too many <b>features</b> in a dataset. Too many <b>features</b> makes it hard to distinguish between the important ones that are relevant to the output and the redundant or not-so important ones. <b>Reducing</b> the <b>dimensions</b> of <b>data</b> is called dimensionality <b>reduction</b>. So the aim is to find the best low-dimensional representation of the variation in a multivariate (lots and lots of variables) <b>data</b> set, but how do we do this? One way is ...", "dateLastCrawled": "2022-02-02T14:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "GitHub - bhagwaann/Decrypting-Dimensionality-<b>Reduction</b>: Seeing the ...", "url": "https://github.com/bhagwaann/Decrypting-Dimensionality-Reduction", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/bhagwaann/Decrypting-<b>Dimension</b>ality-<b>Reduction</b>", "snippet": "<b>Reducing</b> the <b>dimensions</b> of <b>data</b> to 2D or 3D may allow us to plot and visualize it precisely. Dimensionality <b>reduction</b> could be done by both feature selection methods as well as feature engineering methods. Feature selection is the process of identifying and selecting relevant <b>features</b> for your sample. Feature engineering is manually generating new <b>features</b> from existing <b>features</b>, by applying some transformation or performing some operation on them. We <b>can</b> remove <b>features</b> with low variance as ...", "dateLastCrawled": "2022-01-10T10:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Dimensionality <b>Reduction</b> on Multi-Dimensional Transfer Functions for ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3153355/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3153355", "snippet": "Although many <b>features</b> <b>can</b> possibly isolate more areas of interest, if we are certain about which <b>features</b> we would like to use, then the remaining <b>features</b> may be considered less important. The benefit of dimensionality <b>reduction</b> still holds here because multi-channel <b>data</b> has 3 or 4 intensities and adding one more feature increases the <b>dimension</b> to 6 or 8. The difference compared to the previous scenario is, however, that the original domain is much smaller, i.e. 6D in our example. We ...", "dateLastCrawled": "2021-12-08T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Principal Component Analysis (<b>PCA</b>) in Machine Learning\u2014 You will never ...", "url": "https://medium.com/codex/principal-component-analysis-pca-how-it-works-mathematically-d5de4c7138e6", "isFamilyFriendly": true, "displayUrl": "https://medium.com/codex/principal-component-analysis-<b>pca</b>-how-it-works-mathematically...", "snippet": "Principal Component Analysis(<b>PCA</b>) is a popular unsupervised machine learning technique which is <b>used</b> for <b>reducing</b> <b>the number</b> of input variables in the training dataset. This technique comes under\u2026", "dateLastCrawled": "2022-02-02T13:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "When you do PCA (or any dimensionality <b>reduction</b>), what is &quot;<b>the number</b> ...", "url": "https://stats.stackexchange.com/questions/277951/when-you-do-pca-or-any-dimensionality-reduction-what-is-the-number-of-dimens", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/277951", "snippet": "Fundamental question. When you do PCA (or any dimensionality <b>reduction</b>), what is &quot;<b>the number</b> <b>of dimensions</b>&quot;? I always <b>thought</b> that the thing you measure (ie, the variable) is <b>the number</b> <b>of dimensions</b>: eg, if you measure the length, width, height of a box, that&#39;s 3 <b>dimensions</b> (3 variables); if you measure the abundance of 10,000 genes in 200 cells, that&#39;s 10,000 <b>dimensions</b> (not 200 <b>dimensions</b>).", "dateLastCrawled": "2022-02-01T22:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Data Warehousing - Database Questions and Answers</b> | MCQ - Trenovision", "url": "https://trenovision.com/data-warehousing-database-questions-and-answers-mcq/", "isFamilyFriendly": true, "displayUrl": "https://trenovision.com/<b>data-warehousing-database-questions-and-answers</b>-mcq", "snippet": "<b>Reducing</b> <b>the number</b> of attributes to solve the high dimensionality problem is called as _____. A. dimensionality curse. B. dimensionality <b>reduction</b>. C. cleaning. D. Overfitting. Show Answer. Feedback The correct answer is: B. 140. <b>Data</b> that are not of interest to the <b>data</b> mining task is called as _____. A. missing <b>data</b>. B. changing <b>data</b>. C. irrelevant <b>data</b>. D. noisy <b>data</b>. Show Answer. Feedback The correct answer is: C. 141. _____ are effective tools to attack the scalability problem. A ...", "dateLastCrawled": "2022-02-02T19:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Dimensionality Reduction for Data Visualization</b>: PCA vs TSNE vs UMAP vs ...", "url": "https://towardsdatascience.com/dimensionality-reduction-for-data-visualization-pca-vs-tsne-vs-umap-be4aa7b1cb29", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>dimensionality-reduction-for-data-visualization</b>-pca-vs...", "snippet": "Image by Author Implementing t-SNE. One thing to note down is that t-SNE is very computationally expensive, hence it is mentioned in its documentation that : \u201cIt is highly recommended to use another dimensionality <b>reduction</b> method (e.g. PCA for dense <b>data</b> or TruncatedSVD for sparse <b>data</b>) to reduce <b>the number</b> <b>of dimensions</b> to a reasonable amount (e.g. 50) if <b>the number</b> of <b>features</b> is very high.", "dateLastCrawled": "2022-02-02T19:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Applied Dimensionality Reduction</b> \u2014 3 Techniques using Python \u2013 LearnDataSci", "url": "https://www.learndatasci.com/tutorials/applied-dimensionality-reduction-techniques-using-python/", "isFamilyFriendly": true, "displayUrl": "https://www.learn<b>data</b>sci.com/tutorials/<b>applied-dimensionality-reduction</b>-techniques...", "snippet": "Dimensionality <b>reduction</b> reduces <b>the number</b> <b>of dimensions</b> (also called <b>features</b> and attributes) of a dataset. It is <b>used</b> to remove redundancy and help both <b>data</b> scientists and machines extract useful patterns. The goal of this article. In the first part of this article, we&#39;ll discuss some dimensionality <b>reduction</b> theory and introduce various algorithms for <b>reducing</b> <b>dimensions</b> in various types of datasets. The second part of this article walks you through a case study, where we get our hands ...", "dateLastCrawled": "2022-02-02T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Comparison of Two <b>Dimension</b>-<b>Reduction</b> Methods for Network ...", "url": "https://www.academia.edu/70284577/Comparison_of_Two_Dimension_Reduction_Methods_for_Network_Simulation_Models", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/70284577/Comparison_of_Two_<b>Dimension</b>_<b>Reduction</b>_Methods_for...", "snippet": "<b>Reducing</b> the <b>dimension</b> of multivariate responses <b>can</b> reveal the most significant model Key words: correlation analysis; kevin.mills@nist.gov behaviors, allowing subsequent analyses <b>dimension</b> <b>reduction</b>; network simulation; james.filliben@nist.gov to focus on one response per behavior. principal components analysis. This paper investigates two methods for <b>reducing</b> <b>dimension</b> in multivariate <b>data</b> generated from simulation models. One method combines correlation analysis and clustering. The ...", "dateLastCrawled": "2022-02-07T11:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "4.2 Dimensionality <b>reduction</b> techniques: Visualizing complex <b>data</b> sets ...", "url": "https://compgenomr.github.io/book/dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html", "isFamilyFriendly": true, "displayUrl": "https://compgenomr.github.io/book/<b>dimension</b>ality-<b>reduction</b>-techniques-visualizing...", "snippet": "In statistics, <b>dimension</b> <b>reduction</b> techniques are a set of processes for <b>reducing</b> <b>the number</b> of random variables by obtaining a set of principal variables. For example, in the context of a gene expression matrix across different patient samples, this might mean getting a set of new variables that cover the variation in sets of genes. This way samples <b>can</b> be represented by a couple of principal variables instead of thousands of genes. This is useful for visualization, clustering and ...", "dateLastCrawled": "2022-02-01T20:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Dimensionality <b>reduction</b> for bag-of-words models: PCA vs LSA", "url": "http://cs229.stanford.edu/proj2017/final-reports/5163902.pdf", "isFamilyFriendly": true, "displayUrl": "cs229.stanford.edu/proj2017/final-reports/5163902.pdf", "snippet": "performed <b>dimension reduction</b> using both LSA and PCA, and <b>compared</b> the performance of a basic one-vs-rest support vector machine (SVM) using different numbers of <b>features</b> from the two methods. The result was that PCA performed much better than LSA for this problem. 1.1 Previous work The use of singular value decompositions to simplify the feature space in NLP began with the work Deerwester et al. (1990). In Li et al. (2006), genetic algorithms are <b>used</b> to reduce <b>the number</b> of <b>features</b> in ...", "dateLastCrawled": "2022-02-01T11:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "3 New <b>Techniques for Data-Dimensionality Reduction in Machine Learning</b> ...", "url": "https://thenewstack.io/3-new-techniques-for-data-dimensionality-reduction-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thenewstack.io/3-new-<b>techniques-for-data-dimensionality-reduction-in-machine</b>...", "snippet": "A <b>number</b> of <b>techniques for data-dimensionality reduction</b> are available to estimate how informative each column is and, if needed, to skim it off the dataset. Back in 2015, we identified the seven most commonly <b>used</b> <b>techniques for data-dimensionality reduction</b>, including: Ratio of missing values. Low variance in the column values.", "dateLastCrawled": "2022-02-01T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Principal Component Analysis for <b>Dimensionality</b> <b>Reduction</b> | by Lorraine ...", "url": "https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/principal-component-analysis-for-<b>dimensionality</b>...", "snippet": "In machine learning, however, too much <b>data</b> <b>can</b> be a bad thing. At a certain point, more <b>features</b> or <b>dimensions</b> <b>can</b> decrease a model\u2019s accuracy since there is more <b>data</b> that needs to be generalized \u2014 this is known as the curse of <b>dimensionality</b>. <b>Dimensionality</b> <b>reduction</b> is way to reduce t he complexity of a model and avoid overfitting. There are two main categories of <b>dimensionality</b> <b>reduction</b>: feature selection and feature extraction. Via feature selection, we select a subset of the ...", "dateLastCrawled": "2022-02-03T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "16 Dimensionality <b>reduction</b> | <b>Tidy Modeling with R</b>", "url": "https://www.tmwr.org/dimensionality.html", "isFamilyFriendly": true, "displayUrl": "https://www.tmwr.org/<b>dimension</b>ality.html", "snippet": "16 Dimensionality <b>reduction</b>. Dimensionality <b>reduction</b> <b>can</b> be a good choice when you suspect there are \u201ctoo many\u201d variables. An excess of variables, usually predictors, <b>can</b> be a problem because it is difficult to understand or visualize <b>data</b> in higher <b>dimensions</b>. For example, in high dimensional biology experiments, one of the first tasks is ...", "dateLastCrawled": "2022-01-28T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>A comparative dimensionality reduction study in</b> ... - Journal of Big <b>Data</b>", "url": "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-0286-0", "isFamilyFriendly": true, "displayUrl": "https://journalofbig<b>data</b>.springeropen.com/articles/10.1186/s40537-020-0286-0", "snippet": "Telecom Companies logs customer\u2019s actions which generate a huge amount of <b>data</b> that <b>can</b> bring important findings related to customer\u2019s behavior and needs. The main characteristics of such <b>data</b> are the large <b>number</b> of <b>features</b> and the high sparsity that impose challenges to the analytics steps. This paper aims to explore dimensionality <b>reduction</b> on a real telecom dataset and evaluate customers\u2019 clustering in reduced and latent space, <b>compared</b> to original space in order to achieve better ...", "dateLastCrawled": "2022-01-13T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Feature Extraction</b>-Principal Component Analysis | by Mansi Arora | Medium", "url": "https://medium.com/@mansiarora_20448/feature-extraction-principal-component-analysis-a10705b330ce", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@mansiarora_20448/<b>feature-extraction</b>-principal-component-analysis-a...", "snippet": "Thus,Dimensionality <b>Reduction</b> is a process through which we <b>can</b> visualize a high <b>dimension</b> <b>data</b> by <b>reducing</b> the no <b>of dimensions</b>.This is done through two process: Feature selection:It is a process ...", "dateLastCrawled": "2022-01-26T10:08:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Dimension</b> <b>reduction</b> ... and it has been used for conducting research and for deploying <b>machine</b> <b>learning</b> systems into production across more than a dozen areas of computer science and other fields ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. ... K-means algorithm with weighting and <b>dimension</b> <b>reduction</b> components of similarity measure. Simplify balls of string to warm colors and cool colors before untangling. Can be reformulated as a graph clustering problem. Partition subcomponents of a graph based on flow equations. www.simplepastimes.com 40. Multivariate technique similar to mode or density clustering. Find peaks and valleys in data according to an input function on the ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Colleen M. Farrelly</b> - cours.polymtl.ca", "url": "https://cours.polymtl.ca/mth6301/mth8302/Farrelly-Machine_Learning_by_Analogy.pdf", "isFamilyFriendly": true, "displayUrl": "https://cours.polymtl.ca/mth6301/mth8302/Farrelly-<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>.pdf", "snippet": "<b>Dimension</b> <b>reduction</b>/mappingpre-processing Principle component, manifold <b>learning</b>\u2026 Hybrid of neural network methods and tree models. 32 Aggregation of multiple. types of models. Like a small town election. Different people have different views of the politics and care about different issues. Different modeling methods capture different pieces of the data and vote in different pieces. Leverage strengths, minimize weaknesses Diversity of methods to better explore underlying data geometry ...", "dateLastCrawled": "2021-12-14T17:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "4. Dimensionality <b>Reduction</b> Techniques and PCA \u2013 The Unsupervised ...", "url": "https://dev2u.net/2021/10/01/4-dimensionality-reduction-techniques-and-pca-the-unsupervised-learning-workshop/", "isFamilyFriendly": true, "displayUrl": "https://dev2u.net/2021/10/01/4-<b>dimension</b>ality-<b>reduction</b>-techniques-and-pca-the...", "snippet": "Dimensionality <b>reduction</b> techniques have many uses in <b>machine</b> <b>learning</b>, as the ability to extract the useful information of a dataset can provide performance boosts in many <b>machine</b> <b>learning</b> problems. They can be particularly useful in unsupervised as opposed to supervised <b>learning</b> methods because the dataset does not contain any ground truth labels or targets to achieve. In unsupervised <b>learning</b>, the training environment is being used to organize the data in a way that is appropriate for the ...", "dateLastCrawled": "2022-01-26T07:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Dimensionality <b>Reduction</b> Using <b>Factor Analysis</b> | by Chiranjit Majumdar ...", "url": "https://medium.com/@chiranjit7/dimensionality-reduction-using-factor-analysis-8aa754465afc", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@chiranjit7/<b>dimension</b>ality-<b>reduction</b>-using-<b>factor-analysis</b>-8aa754465afc", "snippet": "Dimensionality <b>reduction</b> technique plays a very crucial role to handle this situation. This is a key test to perform while doing feature engineering. <b>Factor analysis</b> will help you to understand ...", "dateLastCrawled": "2022-01-30T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>MACHINE</b> <b>LEARNING</b> (15A05706) - VEMU", "url": "http://vemu.org/uploads/lecture_notes/20_12_2019_484640891.pdf", "isFamilyFriendly": true, "displayUrl": "vemu.org/uploads/lecture_notes/20_12_2019_484640891.pdf", "snippet": "3 Unit-III : (Dimensionality <b>Reduction</b>) 3.1 Introduction 76 3.2 Unit-III notes 72-92 3.3 Solved Problems 3.4 Part A Questions 93 3.5 Part B Questions 94 4 Unit-IV : (Linear Discrimination) 4.1 Introduction 95 4.2 Unit-IV notes 95-110 4.3 Solved Problems 4.4 Part A Questions 111 4.5 Part B Questions 112 5 Unit-V : (Kernel Machines) 5.1 Introduction 113 5.2 Unit-V notes 113-146 5.3 Solved Problems 5.4 Part A Questions 147 5.5 Part B Questions 148 . 2 UNIT-1 1. What Is <b>Machine</b> <b>Learning</b>? <b>Machine</b> ...", "dateLastCrawled": "2022-01-28T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Understanding Machine Learning by Analogy</b> with a Simple Contour Map ...", "url": "https://contemplations.blog/machine-learning-analogy-countour-map/", "isFamilyFriendly": true, "displayUrl": "https://<b>contemplations</b>.blog/<b>machine</b>-<b>learning</b>-<b>analogy</b>-countour-map", "snippet": "The Basis for <b>Machine</b> <b>Learning</b> by <b>Analogy</b>, Using a Contour Map. In this post, we will take a closer look at <b>Machine</b> <b>Learning</b> and its nephew, Deep <b>Learning</b>. There is no \u201c<b>Learning</b>\u201d (in the human sense) in either <b>Machine</b> <b>learning</b> or Deep <b>Learning</b>, there are only quite simple and readily available mathematical procedures which allow us to adapt parameters of many kinds of parameterized systems (or networks), such as a neural network, in such a way that the system (or network), together with ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Machine</b> <b>Learning</b> and Theological Traditions of <b>Analogy</b>", "url": "https://www.researchgate.net/publication/349470559_Machine_Learning_and_Theological_Traditions_of_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349470559_<b>Machine</b>_<b>Learning</b>_and_Theological...", "snippet": "theories of <b>analogy</b> to <b>machine</b> <b>learning</b> has brought us here, since much of it was developed, in the first place, in thinking about the use of shared vocabulary for creature and creator.", "dateLastCrawled": "2021-11-04T23:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Data Mining and <b>Machine</b> <b>Learning</b> in Astronomy - Nicholas M. Ball ...", "url": "https://ned.ipac.caltech.edu/level5/March11/Ball/Ball2.html", "isFamilyFriendly": true, "displayUrl": "https://ned.ipac.caltech.edu/level5/March11/Ball/Ball2.html", "snippet": "In many ways, <b>dimension reduction is similar</b> to classification, in the sense that a larger number of input attributes is reduced to a smaller number of outputs. Many classification schemes in fact directly use PCA. Other dimension reduction methods utilize the same or similar algorithms to those used for the actual data mining: an ANN can perform PCA when set up as an autoencoder, and kernel methods can act as generalizations of PCA. A binary genetic algorithm Section 2.4.4) can be used in ...", "dateLastCrawled": "2022-01-30T22:43:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(dimension reduction)  is like +(reducing the number of dimensions (features) that are used to represent data)", "+(dimension reduction) is similar to +(reducing the number of dimensions (features) that are used to represent data)", "+(dimension reduction) can be thought of as +(reducing the number of dimensions (features) that are used to represent data)", "+(dimension reduction) can be compared to +(reducing the number of dimensions (features) that are used to represent data)", "machine learning +(dimension reduction AND analogy)", "machine learning +(\"dimension reduction is like\")", "machine learning +(\"dimension reduction is similar\")", "machine learning +(\"just as dimension reduction\")", "machine learning +(\"dimension reduction can be thought of as\")", "machine learning +(\"dimension reduction can be compared to\")"]}