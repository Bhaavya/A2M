{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>Sparsity-Based Model of Bounded Rationality</b> * | The Quarterly Journal ...", "url": "https://academic.oup.com/qje/article/129/4/1661/1854039", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/qje/article/129/4/1661/1854039", "snippet": "( r <b>like</b> in the traditional rational actor model). For instance, to choose a, the decision maker should consider not only innovations x 1 in his wealth, and the deviation of GDP from its trend, x 2, but also the impact of interest rate, x 10, demographic trends in China, x 100, recent discoveries in the supply of copper, x 200, and so on.There are n &gt; 10,000 (say) factors that should in principle be taken into account. A sensible agent will \u201cnot think\u201d about most of factors, especially ...", "dateLastCrawled": "2022-01-29T16:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Shopping behavior recognition using a language modeling</b> <b>analogy</b>", "url": "https://www.sciencedirect.com/science/article/pii/S0167865512003844", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167865512003844", "snippet": "Automatic understanding and recognition of human shopping behavior has many potential applications, attracting an increasing interest in the marketing\u2026", "dateLastCrawled": "2021-10-20T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Sparsity-Based Model of Bounded Rationality</b> | Request PDF", "url": "https://www.researchgate.net/publication/228225296_A_Sparsity-Based_Model_of_Bounded_Rationality", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228225296_A_<b>Sparsity-Based_Model_of_Bounded</b>...", "snippet": "A <b>Sparsity-Based Model of Bounded Rationality</b>. December 2011. Quarterly Journal of Economics 129 (4) DOI: 10.2139/ssrn.1781234. Authors: Xavier Gabaix. Harvard University. Request full-text PDF ...", "dateLastCrawled": "2021-12-24T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Shopping behavior recognition using a language modeling <b>analogy</b>", "url": "https://www.researchgate.net/publication/262331810_Shopping_behavior_recognition_using_a_language_modeling_analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/262331810_Shopping_behavior_recognition_using...", "snippet": "<b>Analogy</b> between behavior recognition and speech recognition. 1880 M.C. Popa et al./ Pattern Recognition Letters 34 (2013) 1879\u20131889 2008 ) for behavior interpretation, which require manual user", "dateLastCrawled": "2022-01-10T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Analogical proportion-based methods for recommendation \u2013 First ...", "url": "https://www.sciencedirect.com/science/article/pii/S016501141830887X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S016501141830887X", "snippet": "<b>Analogy</b> making is widely recognized as a powerful kind of common-sense reasoning. This paper primarily addresses the relevance of analogical reasoning for recommender systems, which aim at providing suggestions of interest for end-users. A well-known form of <b>analogy</b> is that of analogical proportions, which are statements of the form \u201c a is to b as c is to d\u201c. Encouraged by good results obtained in classification by analogical proportion-based techniques, we study the potential use of ...", "dateLastCrawled": "2021-11-18T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Daily Mael - 1986 | Daily Reckless", "url": "https://www.dailyreckless.com/daily-mael-1986/", "isFamilyFriendly": true, "displayUrl": "https://www.dailyreckless.com/daily-mael-1986", "snippet": "I <b>like</b> the <b>sparsity</b> of the chorus too. Dark cinematic vibe until that horrendous slappy bass bit comes in. Trundles on with little invention, although rhyming \u2018coupe\u2019 with \u2018blas\u00e9\u2019 is a nice touch. The abrupt change after explosive percussive gunshots is startling and the whole \u2018rhythm train\u2019 section is pretty fab, but then that bloody bass comes back. \u2018Oh no it\u2019s not enough.\u2019 Jogs along on an even keel with no surprises despite its spooky setting. Little substance in the ...", "dateLastCrawled": "2022-01-08T21:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A short-glimpse into the Machine Learning world | by Mauro Krikorian ...", "url": "https://medium.com/southworks/a-short-intro-glimpse-into-the-machine-learning-world-6c4b96452236", "isFamilyFriendly": true, "displayUrl": "https://medium.com/southworks/a-short-intro-glimpse-into-the-machine-learning-world-6c...", "snippet": "Before diving into cool features <b>like</b> self-driving cars, predicting future outcomes for sports betting, autonomous retail stores, improved e-commerce user experience by automatically tagging, orga", "dateLastCrawled": "2021-09-04T11:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Attention Is Preference: A Foundation Derived from Brand Involvement ...", "url": "https://www.r-bloggers.com/2014/09/attention-is-preference-a-foundation-derived-from-brand-involvement-segmentation/", "isFamilyFriendly": true, "displayUrl": "https://www.r-bloggers.com/2014/09/attention-is-preference-a-foundation-derived-from...", "snippet": "<b>Like</b> factor analysis one can vary the number of latent variables until an acceptable solution is found. The NMF package offers a number of criteria, but interpretability must take precedence. In general, we want to see a lot of yellow indicating that we have achieved some degree of simple structure. It would be helpful if each latent features was anchored, that is, a few rows or columns with values near one. This is a restatement of the varimax criteria in factor rotation (see", "dateLastCrawled": "2021-10-06T21:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What <b>is the benefit of clustering data? - Quora</b>", "url": "https://www.quora.com/What-is-the-benefit-of-clustering-data", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>is-the-benefit-of-clustering-data</b>", "snippet": "Answer: Let me draw an <b>analogy</b> here. Each one of us have different groups of friends such as old school/college friends, colleagues, people from the same locality, friends with common interests etc. Our interaction, attitude etc. with them becomes different for each group. Similarly, when we ha...", "dateLastCrawled": "2022-01-23T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is deep learning AI? - Technological Ideas", "url": "https://technologicalidea.quora.com/What-is-deep-learning-AI", "isFamilyFriendly": true, "displayUrl": "https://technologicalidea.quora.com/What-is-deep-learning-AI", "snippet": "<b>Like</b> a child would learn a new language with minimal supervision together with other complex tasks <b>like</b> navigating through the environment. The ability to successfully learn a new task and generalize it to other otherwise unrelated tasks is a very good indicator of intelligence. Deep learning is itself a revolutionary idea from its humble beginnings in the 80&#39;s to it becoming a state-of-the-art learning algorithm today, it has undergone many changes and has achieved a lot. Deep learning has ...", "dateLastCrawled": "2022-01-15T07:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>Sparsity-Based Model of Bounded Rationality</b> * | The Quarterly Journal ...", "url": "https://academic.oup.com/qje/article-abstract/129/4/1661/1854039", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/qje/article-abstract/129/4/1661/1854039", "snippet": "The main advantages of the <b>sparsity</b>-based model relative to <b>similar</b> approaches are the following key points: (i) it predicts actions that are deterministic (in contrast with \u201cnoisy signal\u201d models, say); (ii) it predicts actions that are continuous as a function of the parameters (in contrast with models with fixed costs of attention, say); and (iii) it can be applied in a wide variety of contexts, in particular to any problem that can be expressed as in problem (2). I address some ...", "dateLastCrawled": "2021-12-11T20:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Shopping behavior recognition using a language modeling</b> <b>analogy</b>", "url": "https://www.sciencedirect.com/science/article/pii/S0167865512003844", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167865512003844", "snippet": "Automatic understanding and recognition of human shopping behavior has many potential applications, attracting an increasing interest in the marketing\u2026", "dateLastCrawled": "2021-10-20T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Shopping behavior recognition using a language modeling <b>analogy</b>", "url": "https://www.researchgate.net/publication/262331810_Shopping_behavior_recognition_using_a_language_modeling_analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/262331810_Shopping_behavior_recognition_using...", "snippet": "<b>Analogy</b> between behavior recognition and speech recognition. 1880 M.C. Popa et al./ Pattern Recognition Letters 34 (2013) 1879\u20131889 2008 ) for behavior interpretation, which require manual user", "dateLastCrawled": "2022-01-10T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>SILVERBACK+: scalable association mining via</b> fast list intersection for ...", "url": "https://link.springer.com/article/10.1007/s10115-016-0962-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10115-016-0962-8", "snippet": "There is a clear <b>analogy</b> between \\({\\mathcal {D}}_U\\) and the famous <b>supermarket</b> example of frequent item-set mining. ... <b>Similar</b> to FP-Growth, Eclat employs the divide and conquer strategy to decompose the original search space . It allows frequent item-set discovery via transaction list (tid-list) intersections and is the first algorithm to use column-based, rather than row-based representation of the data. The support of an item-set is determined by intersecting the transaction lists for ...", "dateLastCrawled": "2021-12-06T11:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Deep Learning Isn&#39;t Deep Enough Unless It Copies From the Brain - IEEE ...", "url": "https://spectrum.ieee.org/deep-learning-isnt-deep-enough-unless-it-copies-from-the-brain", "isFamilyFriendly": true, "displayUrl": "https://spectrum.ieee.org/deep-learning-isnt-deep-enough-unless-it-copies-from-the-brain", "snippet": "I made the <b>analogy</b> to figuring out what life is. About 120 years ago, we didn&#39;t understand what life was. There was a concept called \u00e9lan vital, which was a mysterious force that people thought ...", "dateLastCrawled": "2022-01-31T04:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Attention Is Preference: A Foundation Derived from Brand Involvement ...", "url": "https://www.r-bloggers.com/2014/09/attention-is-preference-a-foundation-derived-from-brand-involvement-segmentation/", "isFamilyFriendly": true, "displayUrl": "https://www.r-bloggers.com/2014/09/attention-is-preference-a-foundation-derived-from...", "snippet": "I will draw an <b>analogy</b> with topic modeling in an attempt to explain this approach. Topic modeling starts with a bag of words used in a collection of documents. The assumptions are that the documents cover different topics and that the words used reflect the topics discussed by each document. In our makeup example, we might present a long checklist of brands and products replacing the bag of words in topic modeling. Then, instead of word counts as our intensity measure, we might ask about ...", "dateLastCrawled": "2021-10-06T21:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Recommender system a-introduction</b> - SlideShare", "url": "https://www.slideshare.net/zh3f/recommender-system-aintroduction", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/zh3f/<b>recommender-system-aintroduction</b>", "snippet": "In <b>analogy</b> to the original technique, which was developed in the information retrieval \ufb01eld, they called that factor the inverse user frequency. Herlocker et al. (1999) address the same problem through a variance weighting factor that increases the in\ufb02uence of items that have a high variance in the ratings \u2013 that is, items on which controversial opinions exist. Our basic similarity measure used in the example also does not take into account whether two users have co-rated only a few ...", "dateLastCrawled": "2021-12-23T05:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A short-glimpse into the Machine Learning world | by Mauro Krikorian ...", "url": "https://medium.com/southworks/a-short-intro-glimpse-into-the-machine-learning-world-6c4b96452236", "isFamilyFriendly": true, "displayUrl": "https://medium.com/southworks/a-short-intro-glimpse-into-the-machine-learning-world-6c...", "snippet": "There is an excellent <b>analogy</b> to explain this type of learning paradigm: training a dog. This learning paradigm is like a dog trainer, which teaches the dog how to respond to specific signs, such ...", "dateLastCrawled": "2021-09-04T11:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What <b>is the benefit of clustering data? - Quora</b>", "url": "https://www.quora.com/What-is-the-benefit-of-clustering-data", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>is-the-benefit-of-clustering-data</b>", "snippet": "Answer: Let me draw an <b>analogy</b> here. Each one of us have different groups of friends such as old school/college friends, colleagues, people from the same locality, friends with common interests etc. Our interaction, attitude etc. with them becomes different for each group. Similarly, when we ha...", "dateLastCrawled": "2022-01-23T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is deep learning AI? - Technological Ideas", "url": "https://technologicalidea.quora.com/What-is-deep-learning-AI", "isFamilyFriendly": true, "displayUrl": "https://technologicalidea.quora.com/What-is-deep-learning-AI", "snippet": "In a <b>similar</b> way, deep learning algorithms can automatically translate between languages. This can be powerful for travelers, business people and those in government. Vision for drones and autonomous cars. The way an autonomous vehicle understands the realities of the road and how to respond to them whether it\u2019s a stop sign, a ball in the street or another vehicle is through deep learning algorithms. The more data the algorithms receive, the better they are able to act human-like in their ...", "dateLastCrawled": "2022-01-15T07:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>Sparsity-Based Model of Bounded Rationality</b> * | The Quarterly Journal ...", "url": "https://academic.oup.com/qje/article-abstract/129/4/1661/1854039", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/qje/article-abstract/129/4/1661/1854039", "snippet": "The main advantages of the <b>sparsity</b>-based model relative to similar approaches are the following key points: (i) it predicts actions that are deterministic (in contrast with \u201cnoisy signal\u201d models, say); (ii) it predicts actions that are continuous as a function of the parameters (in contrast with models with fixed costs of attention, say); and (iii) it <b>can</b> be applied in a wide variety of contexts, in particular to any problem that <b>can</b> be expressed as in problem (2). I address some ...", "dateLastCrawled": "2021-12-11T20:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Sparsity-Based Model of Bounded Rationality</b> | Request PDF", "url": "https://www.researchgate.net/publication/228225296_A_Sparsity-Based_Model_of_Bounded_Rationality", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228225296_A_<b>Sparsity-Based_Model_of_Bounded</b>...", "snippet": "One <b>can</b> think of this as bounded rationality that induces excessive shrinkage or <b>sparsity</b> of investors&#39; forecasting models, along the lines of Sims (2003) , Gabaix (2014), and Molavi et al. (2020 ...", "dateLastCrawled": "2021-12-24T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Applications of AI &amp; DS \u2013 Part (b) - Makeup &amp; Breakup", "url": "https://makeupandbreakup.com/2020/11/19/applications-of-ai-ds-part-b/", "isFamilyFriendly": true, "displayUrl": "https://makeupandbreakup.com/2020/11/19/applications-of-ai-ds-part-b", "snippet": "Once we have a set of historical data of sales of <b>supermarket</b> how do we use that data to build a model that <b>can</b> predict the optimum inventory level that <b>supermarket</b> should hold on a particular day. Optimal in the sense either way holding extra inventory or going stock out has a cost component and the stock out has a bigger cost comparison to the carrying extra stock.", "dateLastCrawled": "2022-01-28T02:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Data Mining &amp; Data Warehousing Lecture Notes</b>", "url": "https://www.slideshare.net/fellowbuddy/data-mining-data-warehousing-lecture-notes", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/fellowbuddy/<b>data-mining-data-warehousing-lecture-notes</b>", "snippet": "Using association rule learning, the <b>supermarket</b> <b>can</b> determine which products are frequently bought together and use this information for marketing purposes. This is sometimes referred to as market basket analysis. Clustering \u2013 is the task of discovering groups and structures in the data that are in some way or another &quot;similar&quot;, without using known structures in the data. Classification \u2013 is the task of generalizing known structure to apply to new data. For example, an e-mail program ...", "dateLastCrawled": "2022-01-20T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Probabilistic Topic Models: A focus on graphical model design and ...", "url": "https://www.researchgate.net/publication/264630088_Probabilistic_Topic_Models_A_focus_on_graphical_model_design_and_applications_to_document_and_image_analysis", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/264630088", "snippet": "<b>can</b> <b>be thought</b> of as a mixed-memb ership mo del of group ed data\u2014rather than associate each. group of observations (document) with one component (topic), eac h group exhibits multiple ...", "dateLastCrawled": "2022-01-04T18:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 5, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Stochastic gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Stochastic_gradient_descent</b>", "snippet": "<b>Stochastic gradient descent</b> (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable).It <b>can</b> be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data).Especially in high-dimensional optimization problems this reduces ...", "dateLastCrawled": "2022-02-02T23:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Subsequent Iteration</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/subsequent-iteration", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>subsequent-iteration</b>", "snippet": "The pervasive information architecture of the <b>supermarket</b> has a different use for internal consistency, that is, the capability to provide a coherent in-channel experience that suits the context, goals, and users it is designed for, and external consistency or the capability to support and sustain the same logic across all channels. Designing internal consistency means adopting and maintaining pragmatically correct classification systems\u2014not theoretically sound ones: this is classification ...", "dateLastCrawled": "2021-12-23T12:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Deep Learning Isn&#39;t Deep Enough Unless It Copies From the Brain - IEEE ...", "url": "https://spectrum.ieee.org/deep-learning-isnt-deep-enough-unless-it-copies-from-the-brain", "isFamilyFriendly": true, "displayUrl": "https://spectrum.ieee.org/deep-learning-isnt-deep-enough-unless-it-copies-from-the-brain", "snippet": "I made the <b>analogy</b> to figuring out what life is. About 120 years ago, we didn&#39;t understand what life was. There was a concept called \u00e9lan vital, which was a mysterious force that people <b>thought</b> ...", "dateLastCrawled": "2022-01-31T04:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>ACCT2118 - Industrial Project - Skill Audit</b>", "url": "https://www.slideshare.net/Tr4nL3Huy/acct2118-industrial-project-skill-audit", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/Tr4nL3Huy/<b>acct2118-industrial-project-skill-audit</b>", "snippet": "The Pareto principle (a.k.a. the 80-20 rule, the law of the vital few and the principle of factor <b>sparsity</b>) states that, for many events, 80% of the effects comes from 20% of the causes. &lt;br /&gt;80/20 Pareto&lt;br /&gt; 26. Pareto effect or Pareto\u2019s law:&lt;br /&gt;a small proportion of causes produce a large proportion of results. Thus frequently a vital ...", "dateLastCrawled": "2022-01-23T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "20 Notes on Data Science for Business by Foster Provost and Tom Fawcett ...", "url": "https://daaronr.github.io/metrics_discussion/n-ds4bs.html", "isFamilyFriendly": true, "displayUrl": "https://daaronr.github.io/metrics_discussion/n-ds4bs.html", "snippet": "A useful <b>analogy</b> from (Melania, 2019): ... Hopefully, the analyst has <b>thought</b> this through carefully. It is surprising how often one finds that an analyst has not, and is simply reporting some measure he learned about in a class in school. expected value computation provides a framework that is extremely useful in organizing thinking about data-analytic problems. Specifically, it decomposes data-analytic thinking into (i) the structure of the problem, (ii) the elements of the analysis that ...", "dateLastCrawled": "2021-12-30T20:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Shopping behavior recognition using a language modeling</b> <b>analogy</b>", "url": "https://www.sciencedirect.com/science/article/pii/S0167865512003844", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167865512003844", "snippet": "Automatic understanding and recognition of human shopping behavior has many potential applications, attracting an increasing interest in the marketing\u2026", "dateLastCrawled": "2021-10-20T05:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Shopping behavior recognition using a language modeling <b>analogy</b>", "url": "https://www.researchgate.net/publication/262331810_Shopping_behavior_recognition_using_a_language_modeling_analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/262331810_Shopping_behavior_recognition_using...", "snippet": "For example, Kinect <b>can</b> identify if a shopper in a store is trying on an item, returning it to the shelf, or proceeding with the item to check out (Popa et al., 2013). Although there is sometimes ...", "dateLastCrawled": "2022-01-10T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Sparsity-Based Model of Bounded Rationality</b> * | The Quarterly Journal ...", "url": "https://academic.oup.com/qje/article-abstract/129/4/1661/1854039", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/qje/article-abstract/129/4/1661/1854039", "snippet": "The main advantages of the <b>sparsity</b>-based model relative to similar approaches are the following key points: (i) it predicts actions that are deterministic (in contrast with \u201cnoisy signal\u201d models, say); (ii) it predicts actions that are continuous as a function of the parameters (in contrast with models with fixed costs of attention, say); and (iii) it <b>can</b> be applied in a wide variety of contexts, in particular to any problem that <b>can</b> be expressed as in problem (2). I address some ...", "dateLastCrawled": "2021-12-11T20:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A <b>Sparsity-Based Model of Bounded Rationality</b> | Request PDF", "url": "https://www.researchgate.net/publication/228225296_A_Sparsity-Based_Model_of_Bounded_Rationality", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228225296_A_<b>Sparsity-Based_Model_of_Bounded</b>...", "snippet": "One <b>can</b> think of this as bounded rationality that induces excessive shrinkage or <b>sparsity</b> of investors&#39; forecasting models, along the lines of Sims (2003) , Gabaix (2014), and Molavi et al. (2020 ...", "dateLastCrawled": "2021-12-24T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Differentiable Pattern Set Mining", "url": "https://dl.acm.org/doi/pdf/10.1145/3447548.3467348", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/pdf/10.1145/3447548.3467348", "snippet": "patterns. For training, optimizing a data-<b>sparsity</b> aware reconstruc-tion loss, continuous versions of the weights are learned in small, noisy steps. This formulation provides a link between the discrete search space and continuous optimization, thus allowing for a gradi- ent based strategy to discover sets of high-quality and noise-robust patterns. Through extensive experiments on both synthetic and real world data, we show that BinaPs discovers high quality and noise robust patterns, and ...", "dateLastCrawled": "2021-12-18T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep Learning Isn&#39;t Deep Enough Unless It Copies From the Brain - IEEE ...", "url": "https://spectrum.ieee.org/deep-learning-isnt-deep-enough-unless-it-copies-from-the-brain", "isFamilyFriendly": true, "displayUrl": "https://spectrum.ieee.org/deep-learning-isnt-deep-enough-unless-it-copies-from-the-brain", "snippet": "<b>Compared</b> with traditional memories, such as SRAM and DRAM, PCM and RRAM are already slower to program and wear out after fewer programming cycles. Fortunately, for inference, weights don&#39;t need to ...", "dateLastCrawled": "2022-01-31T04:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 6, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Stochastic gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Stochastic_gradient_descent</b>", "snippet": "<b>Stochastic gradient descent</b> (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable).It <b>can</b> be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data).Especially in high-dimensional optimization problems this reduces ...", "dateLastCrawled": "2022-02-02T23:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Recommender system a-introduction</b> - SlideShare", "url": "https://www.slideshare.net/zh3f/recommender-system-aintroduction", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/zh3f/<b>recommender-system-aintroduction</b>", "snippet": "A further discussion of costs and bene\ufb01ts of implicit ratings <b>can</b> be found in Nichols (1998). 2.3.2 Data <b>sparsity</b> and the cold-start problem In the rating matrices used in the previous examples, ratings existed for all but one user\u2013item combination. In real-world applications, of course, the rating matrices tend to be very sparse, as customers typically provide ratings for (or have bought) only a small fraction of the catalog items. In general, the challenge in that context is thus to ...", "dateLastCrawled": "2021-12-23T05:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Which machine learning algorithms are optimal for</b> boolean features? - Quora", "url": "https://www.quora.com/Which-machine-learning-algorithms-are-optimal-for-boolean-features", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Which-machine-learning-algorithms-are-optimal-for</b>-boolean-features", "snippet": "Answer (1 of 5): Boosted decision trees work very well for categorical (in this case, 2-categorical) features. The information gain criterion [1] works especially ...", "dateLastCrawled": "2022-01-28T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why do we use loss <b>functions in machine learning instead</b> of simply ...", "url": "https://www.quora.com/Why-do-we-use-loss-functions-in-machine-learning-instead-of-simply-optimizing-for-accuracy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-do-we-use-loss-<b>functions-in-machine-learning-instead</b>-of...", "snippet": "Answer (1 of 5): You want to get the machine to learn, not you to teach the machine. So it has to have a way of measuring the \u201cwrong\u201d that it needs to minimize or optimize away. The alternative could be to measure the \u201cright\u201d but that makes you the intelligence teaching, not the machine learnin...", "dateLastCrawled": "2022-01-22T22:33:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Sparsity</b> is an essential feature of many contemporary data problems. Remote sensing, various forms of automated screening and other high throughput measurement devices collect a large amount of ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Regularization \u2014 Understanding L1 and L2 regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2...", "snippet": "The <b>sparsity</b> feature used in L1 regularization has been used extensively as a feature selection mechanism in <b>machine</b> <b>learning</b>. Feature selection is a mechanism which inherently simplifies a ...", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "An E\ufb03cient Sparse Metric <b>Learning</b> in High ... - <b>Machine</b> <b>Learning</b>", "url": "http://machinelearning.org/archive/icml2009/papers/46.pdf", "isFamilyFriendly": true, "displayUrl": "<b>machinelearning</b>.org/archive/icml2009/papers/46.pdf", "snippet": "This <b>sparsity</b> prior of <b>learning</b> distance metric serves to regularize the com-plexity of the distance model especially in the \u201cless example number p and high dimension d\u201d setting. Theoretically, by <b>analogy</b> to the covariance estimation problem, we \ufb01nd the proposed distance <b>learning</b> algorithm has a consistent result at rate O!&quot;# m2 logd $% n &amp; to the target distance matrix with at most m nonzeros per row. Moreover, from the imple-mentation perspective, this! 1-penalized log-determinant ...", "dateLastCrawled": "2021-11-19T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Discovering governing equations from data</b> by sparse identification of ...", "url": "https://www.pnas.org/content/pnas/113/15/3932.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/<b>pnas</b>/113/15/3932.full.pdf", "snippet": "examples. In this work, we combin e <b>sparsity</b>-promoting techniques and <b>machine</b> <b>learning</b> with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only as-sumption about the structureof the model is that there are onlya few important terms that govern the dy namics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to ...", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> in Medical Imaging", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4220564/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4220564", "snippet": "SUPERVISED <b>LEARNING</b>. In <b>machine</b> <b>learning</b>, one often seeks to predict an output variable y based on a vector x of input variables. To accomplish this, it is assumed that the input and output approximately obey a functional relationship y=f (x), called the predictive model, as shown in Figure 1.In supervised <b>learning</b>, the predictive model is discovered with the benefit of training data consisting of examples for which both x and y are known. We will denote these available pairs of examples as ...", "dateLastCrawled": "2022-02-03T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "What are the <b>basic concepts in machine learning</b>? I found that the best way to discover and get a handle on the <b>basic concepts in machine learning</b> is to review the introduction chapters to <b>machine learning</b> textbooks and to watch the videos from the first model in online courses. Pedro Domingos is a lecturer and professor on <b>machine learning</b> at the University of Washing and", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Dynamical <b>machine</b> <b>learning</b> volumetric reconstruction of objects ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8027224/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8027224", "snippet": "The sequence index in the angle of illumination plays the role of discrete time in the dynamical system <b>analogy</b>. Thus, the imaging problem turns into a problem of nonlinear system identification, which also suggests dynamical <b>learning</b> as a better fit to regularize the reconstructions. We devised a Recurrent Neural Network (RNN) architecture with a novel Separable-Convolution Gated Recurrent Unit (SC-GRU) as the fundamental building block. Through a comprehensive comparison of several ...", "dateLastCrawled": "2022-01-08T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Dynamical machine learning volumetric reconstruction of</b> objects ...", "url": "https://www.nature.com/articles/s41377-021-00512-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41377-021-00512-x", "snippet": "Recently, thanks to a ground-breaking observation from 2010 that <b>sparsity</b> can be learnt by a deep neural network 48, the idea of using <b>machine</b> <b>learning</b> to approximate solutions to inverse problems ...", "dateLastCrawled": "2022-02-02T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "regression - Why L1 norm for sparse models - Cross Validated", "url": "https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/45643", "snippet": "There are many norms that lead to <b>sparsity</b> (e.g., as you mentioned, any Lp norm with p &lt;= 1). In general, any norm with a sharp corner at zero induces <b>sparsity</b>. So, going back to the original question - the L1 norm induces <b>sparsity</b> by having a discontinuous gradient at zero (and any other penalty with this property will do so too). $\\endgroup$", "dateLastCrawled": "2022-01-26T23:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "The result is a <b>learning</b> model that may result in generally better word embeddings. GloVe, is a new global log-bilinear regression model for the unsupervised <b>learning</b> of word representations that outperforms other models on word <b>analogy</b>, word similarity, and named entity recognition tasks. \u2014 GloVe: Global Vectors for Word Representation, 2014.", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Learning Neural Representations for Network Anomaly Detection</b>", "url": "https://www.researchgate.net/publication/325797465_Learning_Neural_Representations_for_Network_Anomaly_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/325797465_<b>Learning</b>_Neural_Representations_for...", "snippet": "Many <b>machine</b> <b>learning</b> algorithms have been. Manuscript received December 22, 2017; revised March 13, 2018. This. work is funded by Vietnam International Education De velopment (VIED) and. by ...", "dateLastCrawled": "2021-12-06T22:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Self-representation based dual-graph regularized <b>feature selection</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation ... Her current research interests include pattern recognition and <b>machine</b> <b>learning</b>. Licheng Jiao (SM\u05f389) received the B.S. degree from Shanghai Jiaotong University, Shanghai, China, in 1982, the M.S. and Ph.D. degrees from Xi\u05f3an Jiaotong University, Xi\u05f3an, China, in 1984 and 1990, respectively. From 1990 to 1991, he was a postdoctoral Fellow in the National Key Laboratory for Radar Signal ...", "dateLastCrawled": "2021-11-22T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Self-representation based dual-graph regularized feature selection ...", "url": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "snippet": "<b>machine</b> <b>learning</b> and computer vision \ufb01elds [41]. <b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation [41]. Taking into account of manifold <b>learning</b> and feature selection, and inspired by the self-representation property and the idea of dual-regularization <b>learning</b> [44,45], we propose a novel feature selection algorithm for clustering, named self-representation based dual-graph regularized feature selection clustering (DFSC). This algorithm ...", "dateLastCrawled": "2022-02-02T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Unsupervised feature selection</b> by <b>regularized self-representation</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation. With the above considerations, in this paper we propose a simple yet very effective <b>unsupervised feature selection</b> method by exploiting the self-representation ability of features. The feature matrix is represented over itself to find the representative feature components. The representation residual is minimized by L 2, 1-norm loss to reduce the effect of outlier samples. Different from the ...", "dateLastCrawled": "2022-01-24T12:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Talk <b>Archive</b> - Research on Algorithms and Incentives in Networks", "url": "https://rain.stanford.edu/schedule/archive.shtml", "isFamilyFriendly": true, "displayUrl": "https://rain.stanford.edu/schedule/<b>archive</b>.shtml", "snippet": "McFowland\u2019s research interests\u2014which lie at the intersection of Information Systems, <b>Machine</b> <b>Learning</b>, and Public Policy\u2014include the development of computationally efficient algorithms for large-scale statistical <b>machine</b> <b>learning</b> and \u201cbig data\u201d analytics. More specifically, his research seeks to demonstrate that many real-world problems faced by organizations, and society more broadly, can be reduced to the tasks of anomalous pattern detection and discovery. As a data and ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Talks - <b>sites.google.com</b>", "url": "https://sites.google.com/view/dssseminarseries/talks", "isFamilyFriendly": true, "displayUrl": "https://<b>sites.google.com</b>/view/dssseminarseries/talks", "snippet": "Abstracts &amp; Bios for upcoming talks", "dateLastCrawled": "2022-01-27T14:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Sparse representations for text categorization</b>", "url": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text_categorization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text...", "snippet": "<b>Machine</b> <b>learning</b> for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is ...", "dateLastCrawled": "2021-12-10T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Continual Learning via Neural Pruning</b> | DeepAI", "url": "https://deepai.org/publication/continual-learning-via-neural-pruning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>continual-learning-via-neural-pruning</b>", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more efficient use of resources in machines with memory constraints.", "dateLastCrawled": "2021-12-30T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Sparse Representations for Text Categorization</b> | Dimitri Kanevsky ...", "url": "https://www.academia.edu/2738730/Sparse_Representations_for_Text_Categorization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2738730/<b>Sparse_Representations_for_Text_Categorization</b>", "snippet": "A Comparative Study of <b>Machine</b> <b>Learning</b> Methods for Verbal Autopsy Text Classification. By Eric S Atwell and Samuel Danso. CSC435 book proposal. By Russell Frith. Higher-Order Smoothing: A Novel Semantic Smoothing Method for Text Classification. By Murat C Ganiz, Mitat Poyraz, and Zeynep Kilimci. INFORMATION RETRIEVAL. By febi k. Introduction to information retrieval. By Valeria Mesi. Download pdf. \u00d7 Close Log In. Log In with Facebook Log In with Google. Sign Up with Apple. or. Email ...", "dateLastCrawled": "2021-10-13T23:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Non-negative data-<b>driven mapping of structural connections</b> with ...", "url": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "snippet": "For ICA, <b>sparsity can be thought of as</b> a proxy for independence. 3.5. In-vivo data decompositions. For real data, we decomposed group-average tractography matrices, using independent component analysis (ICA) and non-negative matrix factorisation (NMF), with a range of model orders K. ICA was initialised with regular PCA, in which the first 500 components were retained (explaining 97% of the total variance). ICA was applied to the reduced dataset using the FastICA algorithm (Hyv\u00e4rinen and ...", "dateLastCrawled": "2021-10-11T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Continual <b>Learning</b> via Neural Pruning \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1903.04476/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1903.04476", "snippet": "We introduce Continual <b>Learning</b> via Neural Pruning (CLNP), a new method aimed at lifelong <b>learning</b> in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the ...", "dateLastCrawled": "2021-11-07T10:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Continual <b>Learning</b> via Neural Pruning", "url": "https://openreview.net/pdf?id=Hyl_XXYLIB", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=Hyl_XXYLIB", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much at-tention from the <b>machine</b> <b>learning</b> community in recent years. The main obstacle for effective continual <b>learning</b> is the problem of cata-strophic forgetting: machines trained on new problems forget about", "dateLastCrawled": "2022-01-05T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Abstract - arXiv.org e-Print archive", "url": "https://arxiv.org/pdf/1903.04476", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1903.04476", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more ef\ufb01cient use of resources in machines with memory constraints. There is also great interest in continual <b>learning</b> from a more long term ...", "dateLastCrawled": "2021-10-25T14:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Introduction to compressed sensing</b>", "url": "https://www.researchgate.net/publication/220043734_Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220043734_<b>Introduction_to_compressed_sensing</b>", "snippet": "systems control, clustering, and <b>machine</b> <b>learning</b> [14, 15, 58, 61, 89, 193, 217, 240, 244]. Low-dimensional manifolds hav e also been prop osed as approximate mod-", "dateLastCrawled": "2022-01-14T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Introduction to compressed sensing</b> | Marco Duarte - Academia.edu", "url": "https://www.academia.edu/1443164/Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/1443164/<b>Introduction_to_compressed_sensing</b>", "snippet": "<b>Introduction to Compressed Sensing</b> For any x \u2208 \u03a3k , we can associate a k-face of C n with the support and sign pattern of x. One can show that the number of k-faces of AC n is precisely the number of index sets of size k for which signals supported on them can be recovered by (1.12) with B (y) = {z : Az = y}.", "dateLastCrawled": "2022-01-21T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Compressed Sensing : Theory and Applications</b> | Kutyniok, Gitta Eldar ...", "url": "https://b-ok.africa/book/2086657/84a688", "isFamilyFriendly": true, "displayUrl": "https://b-ok.africa/book/2086657/84a688", "snippet": "You can write a book review and share your experiences. Other readers will always be interested in your opinion of the books you&#39;ve read. Whether you&#39;ve loved the book or not, if you give your honest and detailed thoughts then people will find new books that are right for them.", "dateLastCrawled": "2021-12-26T07:22:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparsity)  is like +(supermarket analogy)", "+(sparsity) is similar to +(supermarket analogy)", "+(sparsity) can be thought of as +(supermarket analogy)", "+(sparsity) can be compared to +(supermarket analogy)", "machine learning +(sparsity AND analogy)", "machine learning +(\"sparsity is like\")", "machine learning +(\"sparsity is similar\")", "machine learning +(\"just as sparsity\")", "machine learning +(\"sparsity can be thought of as\")", "machine learning +(\"sparsity can be compared to\")"]}