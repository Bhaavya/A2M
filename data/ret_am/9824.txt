{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Embeddings | Machine Learning Crash Course | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/<b>embedding</b>", "snippet": "An <b>embedding</b> is a relatively low-dimensional <b>space</b> into which you can translate <b>high-dimensional</b> vectors. Embeddings make it easier to do machine learning on large inputs <b>like</b> sparse vectors representing words. Ideally, an <b>embedding</b> captures some of the semantics of the input by placing semantically similar inputs close together in the <b>embedding</b> <b>space</b>.", "dateLastCrawled": "2022-01-30T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "machine learning - What is the difference between <b>latent</b> and <b>embedding</b> ...", "url": "https://ai.stackexchange.com/questions/11285/what-is-the-difference-between-latent-and-embedding-spaces", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/11285/what-is-the-difference-between-<b>latent</b>-and...", "snippet": "The expression &quot;<b>embedding</b> <b>space</b>&quot; refers to a vector <b>space</b> that represents an original <b>space</b> of inputs (e.g. images or words). For example, in the case of &quot;word embeddings&quot;, which are vector representations of words. It can also refer to a <b>latent</b> <b>space</b> because a <b>latent</b> <b>space</b> can also be a <b>space</b> of vectors. However, an <b>embedding</b> <b>space</b> is not necessarily an hidden <b>space</b>. It is just another (vector) representation of another <b>space</b>.", "dateLastCrawled": "2022-02-03T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Attribute-based Explanation of Non-Linear Embeddings of High ...", "url": "https://ieeevis.b-cdn.net/vis_2021/pdfs/v-full-1440.pdf", "isFamilyFriendly": true, "displayUrl": "https://ieeevis.b-cdn.net/vis_2021/pdfs/v-full-1440.pdf", "snippet": "this raw data plot is an <b>embedding</b> of the <b>high-dimensional</b> data using methods <b>like</b> linear projections (e.g. PCA) or non-linear techniques <b>like</b> multi-dimensional scaling (MDS) or t-distributed stochastic neigh-bor <b>embedding</b> (t-SNE). Linear techniques have the advantage that the resulting axes still have meaning, but often they cannot uncover complex structures in <b>high-dimensional</b> <b>space</b>. Non-linear projections often nicely reveal complex structure in <b>high-dimensional</b> <b>space</b>, but no longer offer ...", "dateLastCrawled": "2022-01-24T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Neural Network Embeddings Explained | by Will Koehrsen | Towards Data ...", "url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-network-<b>embeddings</b>-explained-4d028e6f0526", "snippet": "Embeddings. An <b>embedding</b> is a mapping of a discrete \u2014 categorical \u2014 variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed <b>space</b>.. Neural network embeddings have 3 primary purposes:", "dateLastCrawled": "2022-02-03T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Data Mining Lecture 5: <b>Embedding</b> Data", "url": "http://comp6237.ecs.soton.ac.uk/lectures/pdf/05_embedding_data_jh.pdf", "isFamilyFriendly": true, "displayUrl": "comp6237.ecs.soton.ac.uk/lectures/pdf/05_<b>embedding</b>_data_jh.pdf", "snippet": "I Optimise the positions of points in lower dimensional <b>space</b> so their Euclidean distances are <b>like</b> the distances between the <b>high dimensional</b> points I Can use any distance measure in the high D <b>space</b> 12/33. <b>Embedding</b> Data - Multidimensional Scaling There are two main sorts of multidimensional scaling: I Metric MDS - Tries to match distances I non-metric MDS - tries to match rankings Only requires distances between items as input Unlike PCA and SOM, there is no explicit mapping Both metric ...", "dateLastCrawled": "2021-11-02T11:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Word <b>Embedding</b> and Vector <b>Space</b> Models | by Jiaqi (Karen) Fang ...", "url": "https://medium.com/analytics-vidhya/word-embedding-and-vector-space-models-11c9b76f58e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/word-<b>embedding</b>-and-vector-<b>space</b>-models-11c9b76f58e", "snippet": "When we have a representation of our words in a <b>high dimensional</b> <b>space</b>, we could use an algorithm <b>like</b> PCA to get a representation on a vector <b>space</b> with fewer dimensions. If we want to visualize ...", "dateLastCrawled": "2022-01-31T03:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[D] Difference between representation vs. latent vs. <b>embedding</b> <b>space</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/ofivs2/d_difference_between_representation_vs_latent_vs/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/ofivs2/d_difference_between...", "snippet": "dogs_<b>like</b>_me. \u00b7 12h. Latent = unobserved variable, usually in a generative model. <b>embedding</b> = some notion of &quot;similarity&quot; is meaningful. probably also <b>high dimensional</b>, dense, and continuous. representation = intermediate transformation of the raw input.", "dateLastCrawled": "2021-12-18T01:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What do <b>we intuitively mean by embedding</b> a manifold into a higher ...", "url": "https://www.quora.com/What-do-we-intuitively-mean-by-embedding-a-manifold-into-a-higher-dimensional-space-Can-you-give-some-examples", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-do-<b>we-intuitively-mean-by-embedding</b>-a-manifold-into-a...", "snippet": "Answer (1 of 4): Sure. Here&#39;s an example: The circle is a 1-dimensional manifold, and here it is embedded in the plane, which is the 2-dimensional Euclidean <b>space</b>. Here&#39;s a circle embedded in 3-<b>space</b>: of course it is pictured here on a flat screen, but you can use your imagination to imagine a...", "dateLastCrawled": "2022-01-20T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "machine learning - How do I encode time in <b>high dimensional</b> <b>space</b> ...", "url": "https://datascience.stackexchange.com/questions/67486/how-do-i-encode-time-in-high-dimensional-space", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/.../how-do-i-encode-time-in-<b>high-dimensional</b>-<b>space</b>", "snippet": "I have a dataset of form text, text, category, category, time, text and I would <b>like</b> to apply the attention mechanism to it. This requires that all inputs be in the same vector <b>space</b>. I am using a particular encoding method (from BERT) for the text-type data and I can build a custom trainable <b>embedding</b> for the category features. However, I don&#39;t have a good way of <b>embedding</b> time data.", "dateLastCrawled": "2022-01-18T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "t-SNE Implementations with more flexible similarity metrics in the ...", "url": "https://m-lin-dm.github.io/t-SNE/", "isFamilyFriendly": true, "displayUrl": "https://m-lin-dm.github.io/t-SNE", "snippet": "Higher `fan` causes thinner tails of the similarity function in the <b>embedding</b> <b>space</b> and less dense clusters. Swiss roll dataset. For all comparisons below I use the 3D \u201cswiss roll\u201d dataset and reduce it into 2D. As t-SNE is an iterative process, I also use the exact same initial condition: points are drawn from a bivariate normal distribution in 2D. tSNE_simple.m. This modified t-SNE skips the computation of neighborhood sizes \u03c3, and instead uses the same neighborhood size for each ...", "dateLastCrawled": "2022-01-22T22:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Embeddings | Machine Learning Crash Course | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/<b>embedding</b>", "snippet": "An <b>embedding</b> is a relatively low-dimensional <b>space</b> into which you can translate <b>high-dimensional</b> vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an <b>embedding</b> captures some of the semantics of the input by placing semantically <b>similar</b> inputs close together in the <b>embedding</b> <b>space</b>.", "dateLastCrawled": "2022-01-30T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "machine learning - What is the difference between <b>latent</b> and <b>embedding</b> ...", "url": "https://ai.stackexchange.com/questions/11285/what-is-the-difference-between-latent-and-embedding-spaces", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/11285/what-is-the-difference-between-<b>latent</b>-and...", "snippet": "The expression &quot;<b>embedding</b> <b>space</b>&quot; refers to a vector <b>space</b> that represents an original <b>space</b> of inputs (e.g. images or words). For example, in the case of &quot;word embeddings&quot;, which are vector representations of words. It can also refer to a <b>latent</b> <b>space</b> because a <b>latent</b> <b>space</b> can also be a <b>space</b> of vectors. However, an <b>embedding</b> <b>space</b> is not necessarily an hidden <b>space</b>. It is just another (vector) representation of another <b>space</b>.", "dateLastCrawled": "2022-02-03T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) DISCOVERING ROBUST EMBEDDINGS IN (DIS)SIMILARITY <b>SPACE</b> FOR HIGH ...", "url": "https://www.academia.edu/12852821/DISCOVERING_ROBUST_EMBEDDINGS_IN_DIS_SIMILARITY_SPACE_FOR_HIGH_DIMENSIONAL_LINGUISTIC_FEATURES", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/12852821/DISCOVERING_ROBUST_<b>EMBEDDING</b>S_IN_DIS_<b>SIMILAR</b>ITY...", "snippet": "DISCOVERING ROBUST EMBEDDINGS IN (DIS)SIMILARITY <b>SPACE</b> FOR <b>HIGH-DIMENSIONAL</b> LINGUISTIC FEATURES. Computational Intelligence, 2012. Makoto Miwa. Junichi Tsujii. Tingting Mu. Sophia Ananiadou. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package. Translate PDF . Related Papers. Unfolding Kernel embeddings of graphs: Enhancing class separation ...", "dateLastCrawled": "2022-01-31T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Embedding</b> in Machine Learning | Cathy\u2019s Notes", "url": "https://cathyqian.github.io/2020/08/27/Embedding_In_Machine_Learning.html", "isFamilyFriendly": true, "displayUrl": "https://cathyqian.github.io/2020/08/27/<b>Embedding</b>_In_Machine_Learning.html", "snippet": "<b>Embedding</b> in the context of deep learning is to map <b>high-dimensional</b> vectors or categorical variables to relatively low-dimensional <b>space</b> so that <b>similar</b> items are close to each other. It can be applied to any <b>high dimensional</b>, sparse or categorical features, i.e., IP addresses from ad impressions, raw pixels in a video, audio data from speech ...", "dateLastCrawled": "2022-01-22T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Data Mining Lecture 5: <b>Embedding</b> Data", "url": "http://comp6237.ecs.soton.ac.uk/lectures/pdf/05_embedding_data_jh.pdf", "isFamilyFriendly": true, "displayUrl": "comp6237.ecs.soton.ac.uk/lectures/pdf/05_<b>embedding</b>_data_jh.pdf", "snippet": "I Start with data in a <b>high dimensional</b> <b>space</b> and a set of corresponding points in a lower dimensional <b>space</b> I Optimise the positions of points in lower dimensional <b>space</b> so their Euclidean distances are like the distances between the <b>high dimensional</b> points I Can use any distance measure in the high D <b>space</b> 12/33. <b>Embedding</b> Data - Multidimensional Scaling There are two main sorts of multidimensional scaling: I Metric MDS - Tries to match distances I non-metric MDS - tries to match rankings ...", "dateLastCrawled": "2021-11-02T11:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "UNDERSTANDING GRAPH <b>EMBEDDING</b> METHODS AND THEIR APPLICATIONS", "url": "https://epubs.siam.org/doi/pdf/10.1137/20M1386062", "isFamilyFriendly": true, "displayUrl": "https://epubs.siam.org/doi/pdf/10.1137/20M1386062", "snippet": "verting <b>high-dimensional</b> sparse graphs into low-dimensional, dense, and continuous vector spaces, preserving maximally the graph structure properties. Another type of emerging uncertainty estimation. The main goal of graph <b>embedding</b> methods is to pack every node&#39;s properties into a vector with a smaller dimension; hence, node similarity in the original standard metrics. The nonlinear and highly informative graph embeddings generated in the latent <b>space</b> can be conveniently used to address ...", "dateLastCrawled": "2022-01-30T09:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Word <b>Embedding</b> and Vector <b>Space</b> Models | by Jiaqi (Karen) Fang ...", "url": "https://medium.com/analytics-vidhya/word-embedding-and-vector-space-models-11c9b76f58e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/word-<b>embedding</b>-and-vector-<b>space</b>-models-11c9b76f58e", "snippet": "When we have a representation of our words in a <b>high dimensional</b> <b>space</b>, we could use an algorithm like PCA to get a representation on a vector <b>space</b> with fewer dimensions. If we want to visualize ...", "dateLastCrawled": "2022-01-31T03:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Neural Network Embeddings Explained | by Will Koehrsen | Towards Data ...", "url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-network-<b>embeddings</b>-explained-4d028e6f0526", "snippet": "Embeddings. An <b>embedding</b> is a mapping of a discrete \u2014 categorical \u2014 variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed <b>space</b>.. Neural network embeddings have 3 primary purposes:", "dateLastCrawled": "2022-02-03T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "hash - <b>Embedding</b> <b>high dimensional</b> vectors into low dimensional <b>space</b> ...", "url": "https://cs.stackexchange.com/questions/60916/embedding-high-dimensional-vectors-into-low-dimensional-space-preserving-similar", "isFamilyFriendly": true, "displayUrl": "https://cs.stackexchange.com/questions/60916/<b>embedding</b>-<b>high-dimensional</b>-vectors-into...", "snippet": "What I want to do is to embed these vectors into a <b>space</b> such as $\\vec{b}_{i} \\in [0, 255... Stack Exchange Network Stack Exchange network consists of 178 Q&amp;A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.", "dateLastCrawled": "2022-01-19T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Discovering robust embeddings in (Dis)similarity <b>space</b> for high ...", "url": "https://www.researchgate.net/publication/237146000_Discovering_robust_embeddings_in_Dissimilarity_space_for_high-dimensional_linguistic_features", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/237146000_Discovering_robust_<b>embeddings</b>_in...", "snippet": "The main idea is to repel points from different classes that are close by in the input <b>high dimensional</b> <b>space</b>. The proposed methodology is generic and can be applied to any graph-based method for ...", "dateLastCrawled": "2021-08-10T22:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Hessian Eigenmaps: new locally linear <b>embedding</b> techniques for high ...", "url": "http://web.mit.edu/6.454/www/www_fall_2003/esuddert/donoho_hessian03.pdf", "isFamilyFriendly": true, "displayUrl": "<b>web.mit.edu</b>/6.454/www/www_fall_2003/esuddert/donoho_hessian03.pdf", "snippet": "new locally linear <b>embedding</b> techniques for <b>high-dimensional</b> data ... <b>embedding</b> <b>space</b> Rn obeys d&lt;n. We speak of the image M= \u03c8(\u0398) as the manifold, although of course from the viewpoint of manifold theory it is actually the very special case of a single coordinate patch. The vector \u03b8can <b>be thought</b> of as some control parameters underlying a measuring device, and the manifold as the enumeration m= \u03c8(\u03b8) of all possible measurements as the parameters vary. Thus the mapping \u03c8associates ...", "dateLastCrawled": "2021-09-21T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Thought</b> Chart: Tracking Dynamic EEG Brain Connectivity with ...", "url": "https://creativecoding.soe.ucsc.edu/pdfs/Xing_ThoughtChart_BIH_2016.pdf", "isFamilyFriendly": true, "displayUrl": "https://creativecoding.soe.ucsc.edu/pdfs/Xing_<b>Thought</b>Chart_BIH_2016.pdf", "snippet": "EEG activities <b>can</b> then be represented as a trajectory or <b>thought</b> chart in this <b>space</b>. Our framework \ufb01rst applied graph dissimilarity <b>space</b> <b>embedding</b> to the temporal EEG connectomes of 20 healthy volunteers, both at rest and during an emotion regulation task (ERT), followed by local neighborhood reconstruction then nonlinear dimensionality reduction (NDR) in order to reconstruct and embed the learned manifold in a lower-dimensional Euclidean <b>space</b>. We showed that resting and ERT <b>thought</b> ...", "dateLastCrawled": "2021-11-08T19:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Series on Theories: High Dimensional Data 101</b> | by Kate Wall | The ...", "url": "https://medium.com/swlh/series-on-theories-high-dimensional-data-101-81cab8e0bea6", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>series-on-theories-high-dimensional-data-101</b>-81cab8e0bea6", "snippet": "In four dimensions, this shared <b>space</b> decreases to 30.8%. The shared <b>space</b> <b>can</b> be computed in n-dimensions with the following formula. We see in the figure to the right that as dimension count ...", "dateLastCrawled": "2021-05-30T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Demystifying Spectral <b>Embedding</b>. A Dimensionality Reduction Technique ...", "url": "https://medium.com/mlearning-ai/demystifying-spectral-embedding-b2368bba580", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mlearning-ai/demystifying-spectral-<b>embedding</b>-b2368bba580", "snippet": "Spectral <b>Embedding</b> is a technique used for non ... the data lies in a low-dimensional manifold in a <b>high-dimensional</b> <b>space</b> . Now, this very statement, may lead many readers to stumble, since the ...", "dateLastCrawled": "2022-01-31T13:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Introduction to <b>Word Embeddings</b>. What is a word <b>embedding</b>? | by Hunter ...", "url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-<b>word-embeddings</b>-4cf857b12edc", "snippet": "If we <b>thought</b> that one-hot encoding was <b>high dimensional</b>, then co-occurrence is <b>high dimensional</b> squared. That\u2019s a lot of data to store in memory. Neural Probabilistic Model. Now, we <b>can</b> start to get into some neural networks. A neural probabilistic model learns an <b>embedding</b> by achieving some task like modeling or classification and is what the rest of these embeddings are more or less based on. Typically, you clean your text and create one-hot encoded vectors. Then, you define your ...", "dateLastCrawled": "2022-01-30T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[D] Difference between representation vs. latent vs. <b>embedding</b> <b>space</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/ofivs2/d_difference_between_representation_vs_latent_vs/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/ofivs2/d_difference_between...", "snippet": "Latent = unobserved variable, usually in a generative model. <b>embedding</b> = some notion of &quot;similarity&quot; is meaningful. probably also <b>high dimensional</b>, dense, and continuous. representation = intermediate transformation of the raw input. Could be as simple as a one-hot encoding, or as downstream as a classifier&#39;s output logits vector.", "dateLastCrawled": "2021-12-18T01:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Inferring disease subtypes from clusters in explanation <b>space</b> ...", "url": "https://www.nature.com/articles/s41598-020-68858-7", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-020-68858-7", "snippet": "This <b>space</b> of feature contributions (the explanation <b>space</b>) <b>can</b> <b>be thought</b> of as a new feature <b>space</b> in its own right and <b>can</b> itself serve as a basis for classification. In contrast to the ...", "dateLastCrawled": "2022-01-31T22:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Use <b>Image Embeddings for Object Localization</b> | AgileThought", "url": "https://agilethought.com/blogs/use-image-embeddings-object-localization/", "isFamilyFriendly": true, "displayUrl": "https://agile<b>thought</b>.com/blogs/use-image-<b>embeddings</b>-object-localization", "snippet": "One added bonus of <b>embedding</b> our input images into vector <b>space</b> is that we <b>can</b> visualize them using t-SNE\u2014an algorithm that learns to project data in <b>high dimensional</b> <b>space</b> into lower dimensional <b>space</b> while preserving global and local distances (see Figure 4). Figure 4: Plot of Pascal VOC images in vector <b>space</b> using image embeddings from ...", "dateLastCrawled": "2022-01-28T18:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "data visualization - How to project <b>high dimensional</b> <b>space</b> into a two ...", "url": "https://stats.stackexchange.com/questions/63589/how-to-project-high-dimensional-space-into-a-two-dimensional-plane", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/63589", "snippet": "How to project <b>high dimensional</b> <b>space</b> into a two-dimensional plane? Ask Question Asked 8 years, 6 months ago. Active 8 years, 6 ... my first <b>thought</b> would have been multidimensional scaling on the distances themselves (which is related to PCA), but since you have the locations and not just the distances, by my understanding, PCA should work for that. $\\endgroup$ \u2013 Glen_b. Jul 7 &#39;13 at 23:14. 1 $\\begingroup$ @Glen_b, The key point is not that MDS is for distances input and PCA is for ...", "dateLastCrawled": "2022-01-19T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the intuition behind the fact that we <b>can</b>&#39;t embed every ...", "url": "https://www.quora.com/What-is-the-intuition-behind-the-fact-that-we-cant-embed-every-manifold-in-any-higher-dimensional-space", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-intuition-behind-the-fact-that-we-<b>can</b>t-embed-every...", "snippet": "Answer (1 of 2): Edit: With the new question after the first answer, below: Your original n-dimensional manifold <b>can</b> be bent in at least n fundamental ways (one for each dimension.) Each way of bending it <b>can</b> block an <b>embedding</b> (topologists say it <b>can</b> create an obstruction to an <b>embedding</b>) unless...", "dateLastCrawled": "2022-01-06T13:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "pca - What is <b>embedding</b>? (in <b>the context of dimensionality reduction</b> ...", "url": "https://stats.stackexchange.com/questions/487545/what-is-embedding-in-the-context-of-dimensionality-reduction", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/487545/what-is-<b>embedding</b>-in-the-context-of...", "snippet": "An <b>embedding</b> is a relatively low-dimensional <b>space</b> [subspace] into which you <b>can</b> translate <b>high-dimensional</b> vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an <b>embedding</b> captures some of the semantics of the input by placing semantically similar inputs close together in the <b>embedding</b> <b>space</b> [projection]. An <b>embedding</b> <b>can</b> be learned and reused across models [mapping]. Depending on context, it&#39;s best that you make it ...", "dateLastCrawled": "2022-01-23T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Embedding</b> in Machine Learning | Cathy\u2019s Notes", "url": "https://cathyqian.github.io/2020/08/27/Embedding_In_Machine_Learning.html", "isFamilyFriendly": true, "displayUrl": "https://cathyqian.github.io/2020/08/27/<b>Embedding</b>_In_Machine_Learning.html", "snippet": "What is <b>Embedding</b>? <b>Embedding</b> in the context of deep learning is to map <b>high-dimensional</b> vectors or categorical variables to relatively low-dimensional <b>space</b> so that similar items are close to each other. It <b>can</b> be applied to any <b>high dimensional</b>, sparse or categorical features, i.e., IP addresses from ad impressions, raw pixels in a video, audio data from speech, texts from a job description.", "dateLastCrawled": "2022-01-22T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep Recursive <b>Embedding</b> for <b>High-Dimensional</b> Data | IEEE Journals ...", "url": "https://ieeexplore.ieee.org/document/9585419", "isFamilyFriendly": true, "displayUrl": "https://ieeexplore.ieee.org/document/9585419", "snippet": "<b>Embedding</b> <b>high-dimensional</b> data onto a low-dimensional manifold is of both theoretical and practical value. In this paper, we propose to combine deep neural networks (DNN) with mathematics-guided <b>embedding</b> rules for <b>high-dimensional</b> data <b>embedding</b>. We introduce a generic deep <b>embedding</b> network (DEN) framework, which is able to learn a parametric mapping from <b>high-dimensional</b> <b>space</b> to low-dimensional <b>space</b>, guided by well-established objectives such as Kullback-Leibler (KL) divergence ...", "dateLastCrawled": "2021-10-31T19:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "[2111.00622] Deep Recursive <b>Embedding</b> for <b>High-Dimensional</b> Data", "url": "https://arxiv.org/abs/2111.00622", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2111.00622", "snippet": "<b>Embedding</b> <b>high-dimensional</b> data onto a low-dimensional manifold is of both theoretical and practical value. In this paper, we propose to combine deep neural networks (DNN) with mathematics-guided <b>embedding</b> rules for <b>high-dimensional</b> data <b>embedding</b>. We introduce a generic deep <b>embedding</b> network (DEN) framework, which is able to learn a parametric mapping from <b>high-dimensional</b> <b>space</b> to low-dimensional <b>space</b>, guided by well-established objectives such as Kullback-Leibler (KL) divergence ...", "dateLastCrawled": "2021-12-18T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Zero-<b>Shot Object Detection via Learning an Embedding from Semantic</b> ...", "url": "https://www.ijcai.org/Proceedings/2020/0126.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/2020/0126.pdf", "snippet": "classes are usually semantically related in a <b>high dimensional</b> <b>space</b>, called semantic <b>space</b>. There are three types of <b>em-bedding</b> <b>space</b> for ZSL models: learning a joint <b>embedding</b> <b>space</b> between visual <b>space</b> and semantic <b>space</b> [Ba et al., 2015], learning an <b>embedding</b> from visual <b>space</b> to semantic <b>space</b> [Frome et al., 2013], and <b>learning an embedding from semantic</b> <b>space</b> to visual <b>space</b> [Zhang et al., 2016]. Taking semantic <b>space</b> or a joint <b>embedding</b> as the <b>embedding</b> <b>space</b> means that visual ...", "dateLastCrawled": "2021-12-08T17:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "GeodesicEmbedding (GE): A <b>High-Dimensional</b> <b>Embedding</b> Approach for Fast ...", "url": "https://deepai.org/publication/geodesicembedding-ge-a-high-dimensional-embedding-approach-for-fast-geodesic-distance-queries", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/geodesic<b>embedding</b>-ge-a-<b>high-dimensional</b>-<b>embedding</b>...", "snippet": "08/31/21 - In this paper, we develop a novel method for fast geodesic distance queries. The key idea is to embed the mesh into a high-dimensi...", "dateLastCrawled": "2022-01-23T11:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Deep hierarchical embedding for simultaneous modeling</b> of GPCR proteins ...", "url": "https://www.nature.com/articles/s41598-021-88623-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-88623-8", "snippet": "Distances in the low-dimensional <b>embedding</b> <b>space</b> have long been utilized for effective query search in the <b>high dimensional</b> data. In this setting, similarity between data points is defined in ...", "dateLastCrawled": "2022-01-07T05:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Guide to Multidimensional Scaling in Python with Scikit-Learn", "url": "https://stackabuse.com/guide-to-multidimensional-scaling-in-python-with-scikit-learn/", "isFamilyFriendly": true, "displayUrl": "https://stackabuse.com/guide-to-multidimensional-scaling-in-python-with-scikit-learn", "snippet": "MDS is a non-linear technique for <b>embedding</b> data in a lower-dimensional <b>space</b>. It maps points residing in a higher-dimensional <b>space</b> to a lower-dimensional <b>space</b> while preserving the distances between those points as much as possible. Because of this, the pairwise distances between points in the lower-dimensional <b>space</b> are matched closely to their actual distances. The following figure is an example of a possible mapping of points from 3D to 2D and 1D <b>space</b>. The pairwise distances of the ...", "dateLastCrawled": "2022-02-02T20:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Cross-modal semantic autoencoder with <b>embedding</b> consensus | Scientific ...", "url": "https://www.nature.com/articles/s41598-021-92750-7", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-92750-7", "snippet": "Two encoders \\({P_v},{P_t}\\) project image and text data into low-dimensional <b>space</b> A, and two decoders reproject A back <b>to high-dimensional</b> data. Full size image <b>Embedding</b> consensus", "dateLastCrawled": "2022-01-26T15:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Visualization of Multidimensional Datasets Using</b> t-SNE in Python", "url": "https://pyshark.com/visualization-of-multidimensional-datasets-using-t-sne-in-python/", "isFamilyFriendly": true, "displayUrl": "https://pyshark.com/<b>visualization-of-multidimensional-datasets-using</b>-t-sne-in-python", "snippet": "Create a low dimensional <b>space</b> that replicates the properties of the probability distribution from Step 1 as close as possible. Step 1 : Conditional probability in <b>high dimensional</b> <b>space</b>. Depending on the statistical knowledge of the reader it <b>can</b> be easy or difficult to understand.", "dateLastCrawled": "2022-02-02T13:10:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Finding the Word <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-word-<b>analogy</b>-from-given-words-using-word2vec...", "snippet": "In the word <b>analogy</b> task, ... What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the word <b>analogy</b>. In this tutorial, we will be using Word2Vec model and a pre-trained model named \u2018GoogleNews-vectors-negative300.bin\u2018 which is trained on over 50 Billion words by Google. Each word inside the pre-trained dataset is embedded in a 300-dimensional <b>space</b> and the words which are similar in context/meaning are placed closer to each other in the <b>space</b>. Methodology to ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "snippet": "A suitable representation is therefore essential for the success of <b>analogy</b>-based <b>learning</b> to rank. Therefore, we propose a method for analogical <b>embedding</b>, i.e., for <b>embedding</b> the data in a target <b>space</b> such that, in this <b>space</b>, the aforementioned <b>analogy</b> assumption is as valid and strongly pronounced as possible. This is accomplished by means of a neural network with a quadruple Siamese structure, which is trained on a suitably designed set of examples in the form of quadruples of objects ...", "dateLastCrawled": "2022-01-17T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://homepages.uni-paderborn.de/ahmadim/IDA%202021.pdf", "isFamilyFriendly": true, "displayUrl": "https://homepages.uni-paderborn.de/ahmadim/IDA 2021.pdf", "snippet": "7 Intelligent Systems and <b>Machine</b> <b>Learning</b> <b>Embedding</b> By ignoring irrelevant or noisy features, the performance can often be improved Common feature selection techniques tailored for the case of <b>analogy</b>-based <b>learning</b> to rank. <b>Analogy</b>-based <b>learning</b> to rank (able2rank) 8 Intelligent Systems and <b>Machine</b> <b>Learning</b> Extension to feature vectors Degree of <b>analogy</b>. Analogical <b>Embedding</b> 9 Intelligent Systems and <b>Machine</b> <b>Learning</b> Positive example: preferences on both sides are coherent Negative ...", "dateLastCrawled": "2022-01-06T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evaluating vector-<b>space</b> models of <b>analogy</b>", "url": "https://cocosci.princeton.edu/papers/vector_space_analogy_cogsci2017_final.pdf", "isFamilyFriendly": true, "displayUrl": "https://cocosci.princeton.edu/papers/vector_<b>space</b>_<b>analogy</b>_cogsci2017_final.pdf", "snippet": "Recent <b>machine</b> <b>learning</b> methods for deriving vector-<b>space</b> embeddings of words (e.g., word2vec) have achieved considerable success in natural language processing. These vector spaces have also been shown to exhibit a surprising ca-pacity to capture verbal analogies, with similar results for nat-ural images, giving new life to a classic model of analogies as parallelograms that was \ufb01rst proposed by cognitive scientists. We evaluate the parallelogram model of <b>analogy</b> as applied to modern word ...", "dateLastCrawled": "2021-09-24T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "snippet": "With the emergence of word <b>embedding</b> models, a lot of progress has been made in NLP, essentially assuming that a word <b>analogy</b> like m a n: k i n g:: w o m a n: q u e e n is an instance of a parallelogram within the underlying vector <b>space</b>. In this paper, we depart from this assumption to adopt a <b>machine</b> <b>learning</b> approach, i.e., <b>learning</b> a substitute of the parallelogram model. To achieve our goal, we first review the formal modeling of analogical proportions, highlighting the properties which ...", "dateLastCrawled": "2021-11-13T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Graph <b>Embedding</b> for Deep <b>Learning</b> | by Flawnson Tong | Towards Data Science", "url": "https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/overview-of-deep-<b>learning</b>-on-graph-<b>embeddings</b>-4305c10ad4a4", "snippet": "Using an <b>analogy</b> with word2vec, if a document is made of sentences (which is then made of words), then a graph is made of sub-graphs ... Graph <b>embedding</b> techniques take graphs and embed them in a lower dimensional continuous latent <b>space</b> before passing that representation through a <b>machine</b> <b>learning</b> model. Walk <b>embedding</b> methods perform graph traversals with the goal of preserving structure and features and aggregates these traversals which can then be passed through a recurrent neural ...", "dateLastCrawled": "2022-02-01T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-word2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, word <b>embedding</b> is used to map words into vectors of real numbers. There are various word <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce word embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector <b>space</b>, with each unique word in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting ...", "url": "https://www.researchgate.net/figure/In-the-word-embedding-space-the-analogy-pairs-exhibit-interesting-algebraic_fig1_319370400", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/In-the-word-<b>embedding</b>-<b>space</b>-the-<b>analogy</b>-pairs...", "snippet": "Download scientific diagram | In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting algebraic relationships. from publication: Visual Exploration of Semantic Relationships in Neural ...", "dateLastCrawled": "2021-12-21T11:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-<b>embeddings</b>-in-nlp", "snippet": "Word <b>Embedding</b> or Word Vector is a numeric vector input that represents a word in a lower-dimensional <b>space</b>. It allows words with similar meaning to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features. Features: Anything that relates words to one another. Eg: Age, Sports, Fitness, Employed etc. Each word vector has values corresponding to these features. Goal of Word Embeddings. To reduce dimensionality; To use a ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Zero-shot <b>learning</b> via discriminative representation extraction ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "snippet": "The pioneer work in ZSL can be traced to Larochelle et al. , where it verified that when test images belong to some classes that are not available at training stage, a <b>machine</b> <b>learning</b> system can still figure out what a test image is. Due to the importance of zero-shot <b>learning</b>, the number of proposed approaches has increased steadily recently.The number of new zero-shot <b>learning</b> approaches proposed every year was increasing.", "dateLastCrawled": "2021-10-30T07:08:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A self-supervised domain-general <b>learning</b> framework for human ventral ...", "url": "https://www.nature.com/articles/s41467-022-28091-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-022-28091-4", "snippet": "On this view, the <b>embedding space can be thought of as</b> a high-fidelity perceptual interface, with useful visual primitives over which separate conceptual representational systems can operate.", "dateLastCrawled": "2022-01-25T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Spectral Af\ufb01ne-Kernel Embeddings</b> - NSF", "url": "https://par.nsf.gov/servlets/purl/10039348", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10039348", "snippet": "Since <b>machine</b> <b>learn-ing</b> algorithms struggle with high dimensions (an issue known as the curse of dimensionality in this context), one typically needs to map these data points from their high-dimensional space into a lower dimensional space without signi\ufb01cant distortion. Mapping data (living in RD with D\u02db1 but sampling a manifold of low in-trinsic dimensionality d \u02ddD) into a low-dimensional <b>embedding space can be thought of as</b> a preliminary feature extraction step in <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-29T21:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting affinity ties in a surname network", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "snippet": "<b>Machine</b> <b>learning</b>-based approaches for knowledge graph completion To cover the broadest possible range of methods and architectures in the evaluation, we identified representative methods of different model families, taking care that these methods achieve state-of-the-art performances in knowledge graph completion and have open-source implementations that favor the reproducibility of the reported results.", "dateLastCrawled": "2021-09-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(embedding space)  is like +(high-dimensional space)", "+(embedding space) is similar to +(high-dimensional space)", "+(embedding space) can be thought of as +(high-dimensional space)", "+(embedding space) can be compared to +(high-dimensional space)", "machine learning +(embedding space AND analogy)", "machine learning +(\"embedding space is like\")", "machine learning +(\"embedding space is similar\")", "machine learning +(\"just as embedding space\")", "machine learning +(\"embedding space can be thought of as\")", "machine learning +(\"embedding space can be compared to\")"]}