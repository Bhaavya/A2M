{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Fix <b>Bias in Machine Learning Algorithms</b>? - Yields.io", "url": "https://www.yields.io/blog/how-to-fix-bias-in-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.yields.io/blog/how-to-fix-<b>bias-in-machine-learning-algorithms</b>", "snippet": "How to handle <b>bias in machine learning algorithms</b> using <b>equalized</b> <b>odds</b> in practice? We could now define a simple procedure to solve bias in ML models. Determine the protected attributes of the datasetTrain the model on the full datasetWith the mathematical definition of \u201c<b>equalized</b> <b>odds</b>\u201d, you are able to measure the biasCorrect the model output algorithmically, if needed, after the model has been trained This systematic approach will allow you to quantify the degree of bias and you will ...", "dateLastCrawled": "2022-01-25T05:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Fairness</b> | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/fairness", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>fairness</b>", "snippet": "<b>Equalized</b> <b>odds</b> is formally defined in &quot;Equality of Opportunity in Supervised <b>Learning</b>&quot; as follows: &quot;predictor \u0176 satisfies <b>equalized</b> <b>odds</b> with respect to protected attribute A and outcome Y if \u0176 and A are independent, conditional on Y.&quot; Note: Contrast <b>equalized</b> <b>odds</b> with the more relaxed equality of opportunity metric. experimenter&#39;s bias", "dateLastCrawled": "2022-02-02T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fairness in Machine Learning</b> \u2014 Labelia (ex Substra Foundation)", "url": "https://www.labelia.org/en/blog/fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.labelia.org/en/blog/<b>fairness-in-machine-learning</b>", "snippet": "<b>Equalized</b> <b>Odds</b>. Let\u2019s take a binary parameter (0/1), G (ex: gender), that we want to protect from <b>algorithm</b> discrimination. To simplify, let\u2019s consider only G as a parameter. We\u2019re dealing with two groups: one is considered protected (ex: male group) the other one unprotected (female group) Then we consider the result of a binary classifier. One of the output classes is considered a positive outcome (ex: getting hired) and the other one a negative outcome (ex: not getting hired). In ...", "dateLastCrawled": "2022-01-30T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Fairness in <b>ML 2: Equal opportunity and odds</b>", "url": "https://www2.cs.duke.edu/courses/fall18/compsci590.1/lectures/FairML2.pdf", "isFamilyFriendly": true, "displayUrl": "https://www2.cs.duke.edu/courses/fall18/compsci590.1/lectures/FairML2.pdf", "snippet": "\u2022 Equal <b>odds</b>/opportunity \u2013 Different groups may be treated unequally \u2013 Maybe due to the problem \u2013 Maybe due to bias in the dataset \u2022 While demographic parity seems <b>like</b> a good fairness goal for the society, \u2026 Equal <b>odds</b>/opportunity seems to be measuring whether an <b>algorithm</b> is fair (independent of other factors <b>like</b> input data). 22", "dateLastCrawled": "2022-01-28T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Addressing Fairness in <b>Machine</b> <b>Learning</b> Predictions: Strategic Best ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3389631", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3389631", "snippet": "One exception is those algorithms that use <b>equalized</b> <b>odds</b> as a fairness criterion which can decrease disparity in behavior. However, they cannot be used in many practical settings. We propose a new class of fair <b>machine</b> <b>learning</b> algorithms that alleviate disparity in prediction results, disparity in behavior of prediction subjects, and does not need to account for the sensitive variable explicitly. Our <b>algorithm</b> also complies with the notion of equal treatment and explainable AI, and can be ...", "dateLastCrawled": "2022-01-15T12:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Equality of Opportunity in Supervised <b>Learning</b>", "url": "https://home.ttic.edu/~nati/Publications/HardtPriceSrebro2016.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Publications/HardtPriceSrebro2016.pdf", "snippet": "As <b>machine</b> <b>learning</b> increasingly a\ufb00ects decisions in domains protected by anti-discrimination law, there is much interest in algorithmically measuring and ensuring fairness in <b>machine</b> <b>learning</b>. In domains such as advertising, credit, employment, education, and criminal justice, <b>machine</b> <b>learning</b> could help obtain more accurate predictions, but its e\ufb00ect on existing biases is not well understood. Although reliance on data and quantitative measures can help quantify and eliminate existing ...", "dateLastCrawled": "2022-01-31T14:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Differentially Private Fair <b>Learning</b> - Proceedings of <b>Machine</b> <b>Learning</b> ...", "url": "http://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "snippet": "Our \ufb01rst <b>algorithm</b> is a private implementation of the <b>equalized</b> <b>odds</b> post-processing approach of (Hardt et al.,2016). This <b>algorithm</b> is appeal-ingly simple, but must be able to use protected group membership explicitly at test time, which can be viewed as a form of \u201cdisparate treatment\u201d. Our second <b>algorithm</b> is a differentially private version of the oracle-ef\ufb01cient in-processing ap-proach of (Agarwal et al.,2018) which is more complex but need not have access to protected group ...", "dateLastCrawled": "2022-01-22T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "In this case, the biases of humans are not mitigated by the <b>machine learning</b> <b>algorithm</b>. In fact, they are reproduced in the classifications that are made. Why does this happen? Recidivism scores such as those made by the Northpointe software are based on prior arrests, age of first police contact, parents\u2019 incarceration record. This information is shaped by biases in the world (such as from cultural values and nationalism) and injustices more generally (such as racial prejudices).", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Trade-Offs between Fairness and Privacy in <b>Machine</b> <b>Learning</b>", "url": "https://crcs.seas.harvard.edu/files/crcs/files/ai4sg-21_paper_23.pdf", "isFamilyFriendly": true, "displayUrl": "https://crcs.seas.harvard.edu/files/crcs/files/ai4sg-21_paper_23.pdf", "snippet": "because these decision making systems are typically <b>machine</b> <b>learning</b> models, and are trained on potentially sensitive data, we would not <b>like</b> to inadvertently leak information about people in the training data, and would <b>like</b> to protect their privacy. These concerns have received a lot of attention in the re-search community in the last few years [Chaudhuri et al., 2011; Corbett-Davies and Goel, 2018; Dwork et al., 2006, 2014, 2012; Kleinberg et al., 2017; Feldman et al., 2015]. However ...", "dateLastCrawled": "2021-09-30T12:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Fairness in <b>ML 2: Equal opportunity and odds</b>", "url": "https://www2.cs.duke.edu/courses/fall18/compsci590.1/lectures/FairML2.pdf", "isFamilyFriendly": true, "displayUrl": "https://www2.cs.duke.edu/courses/fall18/compsci590.1/lectures/FairML2.pdf", "snippet": "\u2022 We will look at a <b>similar</b> result later in the course due to Kleinberg, Mullainathan and Raghavan (2016) 15. Outline \u2022 Observational measure of fairness \u2013 Issues with Disparate Impact \u2013 Equal opportunity and <b>Equalized</b> <b>odds</b> \u2013 Positive Rate Parity \u2013 Tradeoff \u2022 Achieving <b>Equalized</b> <b>Odds</b> \u2013 Binary Classifier 16. <b>Equalized</b> <b>Odds</b> R satisfies <b>equalized</b> <b>odds</b> if R is conditionally independent of A given Y. \u2022 Derived Classifier: A new classifier \u2019. that only depends on C, A (and Y ...", "dateLastCrawled": "2022-01-28T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Fairness</b> | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/fairness", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>fairness</b>", "snippet": "<b>Equalized</b> <b>odds</b> is formally defined in &quot;Equality of Opportunity in Supervised <b>Learning</b>&quot; as follows: &quot;predictor \u0176 satisfies <b>equalized</b> <b>odds</b> with respect to protected attribute A and outcome Y if \u0176 and A are independent, conditional on Y.&quot; Note: Contrast <b>equalized</b> <b>odds</b> with the more relaxed equality of opportunity metric. experimenter&#39;s bias", "dateLastCrawled": "2022-02-02T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Removing Unfair Bias in <b>Machine</b> <b>Learning</b>", "url": "https://community.ibm.com/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=618c7917-8d2b-ca78-5ecc-2286486b9c69&forceDialog=0", "isFamilyFriendly": true, "displayUrl": "https://community.ibm.com/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=...", "snippet": "<b>Equalized</b> <b>Odds</b> Postprocessing (Hardt et al., NIPS 2016) Reweighing (Kamiran and Calders, KIS 2012) Reject Option Classification (Kamiran et al., ICDM 2012) Prejudice Remover Regularizer (Kamishima et al., ECML PKDD 2012) Calibrated <b>Equalized</b> <b>Odds</b> Postprocessing (Pleiss et al., NIPS 2017) <b>Learning</b> Fair Representations (Zemel et al., ICML 2013)", "dateLastCrawled": "2022-01-29T04:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Review on Fairness in <b>Machine</b> <b>Learning</b> | ACM Computing Surveys", "url": "https://dl.acm.org/doi/10.1145/3494672", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3494672", "snippet": "Additionally, there is also evidence for incompatibility between <b>equalized</b> accuracy and <b>equalized</b> <b>odds</b>, as in the COMPAS criminal justice use case [6, 20]. We will hereby demonstrate some intuition for trade-offs between measures, and we additionally refer the reader to Appendices B.1 and B.2 for further details on the trade-offs between fairness measures and measures impossibility results.", "dateLastCrawled": "2022-02-07T09:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2,", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Algorithmic Decision Making with Conditional Fairness", "url": "https://kunkuang.github.io/papers//KDD20-ConditionalFairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://kunkuang.github.io/papers//KDD20-ConditionalFairness.pdf", "snippet": "require the <b>algorithm</b> should treat different groups of individuals equally. The most commonly used group fairness notions include demographic parity[11], equal opportunity[15], <b>equalized</b> <b>odds</b>[15] and calibration[20]. These fairness notions are easy to understand and implement in real <b>machine</b> <b>learning</b> problems. However, they", "dateLastCrawled": "2021-10-08T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Trade-Offs between Fairness and Privacy in <b>Machine</b> <b>Learning</b>", "url": "https://crcs.seas.harvard.edu/files/crcs/files/ai4sg-21_paper_23.pdf", "isFamilyFriendly": true, "displayUrl": "https://crcs.seas.harvard.edu/files/crcs/files/ai4sg-21_paper_23.pdf", "snippet": "differential privacy and fairness can be at <b>odds</b> with each other when we consider a <b>learning</b> <b>algorithm</b> with non-trivial accuracy. In particular, we consider a simple binary classi\ufb01-cation setting where the <b>learning</b> <b>algorithm</b> is given full ac-cess to the underlying distribution, and show that even un-", "dateLastCrawled": "2021-09-30T12:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Fix <b>Bias in Machine Learning Algorithms</b>? - Yields.io", "url": "https://www.yields.io/blog/how-to-fix-bias-in-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.yields.io/blog/how-to-fix-<b>bias-in-machine-learning-algorithms</b>", "snippet": "Another <b>similar</b> but more recent example was reported when the Apple credit card limits suddenly showed gender bias since married couples discovered that the husband\u2019s limit was 20 times higher when compared to his wife\u2019s. Common types of <b>bias in machine learning algorithms</b> In <b>machine</b> <b>learning</b>, we have different types of bias which can be organized in many different ways. We stick to the ones from Wikipedia: Pre-existing bias in algorithms is a consequence of underlying social and ...", "dateLastCrawled": "2022-01-25T05:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "GitHub - rtikyani/CS634_IBM_Fairness_Pipeline: Detecting and mitigating ...", "url": "https://github.com/rtikyani/CS634_IBM_Fairness_Pipeline", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rtikyani/CS634_IBM_Fairness_Pipeline", "snippet": "This <b>algorithm</b> should be used if the <b>learning</b> <b>algorithm</b> can be modified. There are two bias mitigation algorithms, adversarial debiasing, and prejudice remover. The post-processing treats the learned model as a black box. This <b>algorithm</b> should be the only <b>algorithm</b> used if the training data or <b>learning</b> <b>algorithm</b> cannot be modified. There are three bias mitigation algorithms: <b>equalized</b> <b>odds</b> postprocessing, calibrated <b>equalized</b> <b>odds</b> postprocessing, and reject option classification.", "dateLastCrawled": "2022-01-04T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>Learning</b> \u2013 The Results Are Not the only Thing that Matters ...", "url": "https://link.springer.com/chapter/10.1007/978-3-030-50423-6_46", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-50423-6_46", "snippet": "Recent works considering algorithms to ensure ML fairness applied at the training time include: where the problem of <b>learning</b> a non-discriminatory predictor from a finite training set is studied to preserve \u201c<b>equalized</b> <b>odds</b>\u201d fairness, [21, 30] where a flexible mechanism to design fair classifiers by leveraging a novel intuitive measure of the decision boundary (un)fairness is introduced, and that addresses the problem of reducing the fair classification to a sequence of cost-sensitive ...", "dateLastCrawled": "2021-12-22T00:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Review on Fairness in <b>Machine</b> <b>Learning</b> | ACM Computing Surveys", "url": "https://dl.acm.org/doi/10.1145/3494672", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3494672", "snippet": "<b>Equalized</b> <b>odds</b>: This measure was designed by Hardt et al. to ... However, such mechanisms are tightly coupled with the <b>machine</b> <b>algorithm</b> itself. Hence, we see that the selection of the method depends on the availability of the ground truth, the availability of the sensitive attributes at test time, and on the desired definition of fairness, which <b>can</b> also vary from one application to another. Several preliminary attempts were made to understand which methods are best for use. The study in ...", "dateLastCrawled": "2022-02-07T09:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Responsibility Cannot Be Delegated to an <b>Algorithm</b> \u2014 <b>MACHINE</b> <b>LEARNING</b> ...", "url": "https://www.machinelearningforscience.de/en/responsibility-cannot-be-delegated-to-an-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinelearning</b>forscience.de/en/responsibility-<b>can</b>not-be-delegated-to-an...", "snippet": "Another fairness concept is \u201c<b>equalized</b> <b>odds</b>\u201d or \u201c<b>equalized</b> opportunity\u201d, meaning everyone has the same chances. If, for instance, we stick to the case of university admissions, that would mean if you have equal ability you should \u2013 no matter if you\u2019re a woman or a man, or black or white \u2013 be admitted to this course of study. The big problem is: You <b>can</b>\u2019t meet all the conceptions of fairness simultaneously. You need to decide in favor of one or the other.", "dateLastCrawled": "2021-11-29T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Survey on Bias and <b>Fairness in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/a-survey-on-bias-and-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-on-bias-and-<b>fairness-in-machine-learning</b>", "snippet": "(<b>Equalized</b> <b>Odds</b>). A predictor \u0176 ... <b>algorithm</b> <b>can</b> only treat the learned model as a black box without any ability to modify the training data or <b>learning</b> <b>algorithm</b>, then only post-processing <b>can</b> be used where data is labeled by some black-box model and then relabeled as a function only of the original labels (bellamy2018ai; berk2017convex). Examples of some existing work and their categorization into these types is shown in Table 3. These methods are not just limited to general <b>machine</b> ...", "dateLastCrawled": "2022-01-22T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Accuracy and Fairness Trade-o s in <b>Machine</b> <b>Learning</b>: A Stochastic Multi ...", "url": "https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/tech-papers/20/20T_016.pdf", "isFamilyFriendly": true, "displayUrl": "https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/...", "snippet": "disadvantageous groups. In the literature of fair <b>machine</b> <b>learning</b>, several prevailing criteria for fairness include dis-parate impact (Barocas &amp; Selbst,2016) (also called demo-graphic parity (Calders et al.,2009)), <b>equalized</b> <b>odds</b> (Hardt etal.,2016), anditsspecialcaseof equalopportunity (Hardt et al.,2016), corresponding to different aspects of ...", "dateLastCrawled": "2022-01-28T20:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "In this case, the biases of humans are not mitigated by the <b>machine learning</b> <b>algorithm</b>. In fact, they are reproduced in the classifications that are made. Why does this happen? Recidivism scores such as those made by the Northpointe software are based on prior arrests, age of first police contact, parents\u2019 incarceration record. This information is shaped by biases in the world (such as from cultural values and nationalism) and injustices more generally (such as racial prejudices). This ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Non-Discriminatory <b>Machine</b> <b>Learning</b> through Convex Fairness Criteria ...", "url": "https://www.researchgate.net/publication/330302454_Non-Discriminatory_Machine_Learning_through_Convex_Fairness_Criteria", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/330302454_Non-Discriminatory_<b>Machine</b>_<b>Learning</b>...", "snippet": "From a technical view, most relevant prior works formulated the fair classification problem as a constrained optimization problem, e.g., constrained to statistical parity [75,55, 32, 13], or ...", "dateLastCrawled": "2022-01-29T21:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>Impossibility Theorem</b> of <b>Machine</b> Fairness \u2013 A Causal Perspective ...", "url": "https://deepai.org/publication/the-impossibility-theorem-of-machine-fairness-a-causal-perspective", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>impossibility-theorem</b>-of-<b>machine</b>-fairness-a-causal...", "snippet": "The goal of the <b>learning</b> <b>algorithm</b> <b>can</b> then <b>be thought</b> of as manipulating the past data and making it consistent with a notion of fairness in order to make fair predictions in the future. This is a way to ensure fairness as classification models usually assume that data is independent and identically drawn from the same distribution. Conceptually, this <b>can</b> <b>be thought</b> of as introducing a correction to the labels in the dataset.", "dateLastCrawled": "2022-01-27T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fairness in Machine Learning: How</b> <b>Can</b> a <b>Model Trained on Aerial Imagery</b> ...", "url": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "isFamilyFriendly": true, "displayUrl": "https://opendri.org/wp-content/uploads/2020/05/CatherineInness_-Fairness_in_ML_for_DRM.pdf", "snippet": "only source of information for a supervised <b>machine</b> <b>learning</b> model. If the labels contain bias, even unintentional bias, then the <b>algorithm</b> will propagate it. Increased label accuracy <b>can</b> be achieved with measures such as: 1. Improving the digitization of local building records 2. Increasing diversity in the people selected to manually label ...", "dateLastCrawled": "2021-12-05T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "AI/ML Ethics - What Will it Take to Trust the Model - SMARTER Risk ...", "url": "https://www.smarterriskmanagement.com/ai-ml-ethics-what-will-it-take-to-trust-the-model/", "isFamilyFriendly": true, "displayUrl": "https://www.smarterriskmanagement.com/ai-ml-ethics-what-will-it-take-to-trust-the-model", "snippet": "For a \u201csafe\u201d AI to exist, there needs to be some sort of human oversight into the way a <b>machine</b>-<b>learning</b> <b>algorithm</b> shifts over time. If, for example, a model that previously took a minute or two to run is suddenly taking ten minutes, or something like the probability of default changes by several points for a majority of the cases, a person needs to take a look at what changed to correct that behavior as well as document the review and the actions taken. Without this oversight, a <b>machine</b> ...", "dateLastCrawled": "2021-12-12T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Multi-Objective Automatic Machine Learning with AutoxgboostMC</b>", "url": "https://www.researchgate.net/publication/335462866_Multi-Objective_Automatic_Machine_Learning_with_AutoxgboostMC", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335462866_Multi-Objective_Automatic_<b>Machine</b>...", "snippet": "local search or racing as well as genetic <b>algorithm</b> based approaches <b>can</b> be used to. <b>Multi-Objective Automatic Machine Learning with AutoxgboostMC</b> 3. optimize <b>machine</b> <b>learning</b> pipelines, given ...", "dateLastCrawled": "2021-11-05T05:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Fix <b>Bias in Machine Learning Algorithms</b>? - Yields.io", "url": "https://www.yields.io/blog/how-to-fix-bias-in-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.yields.io/blog/how-to-fix-<b>bias-in-machine-learning-algorithms</b>", "snippet": "How to handle <b>bias in machine learning algorithms</b> using <b>equalized</b> <b>odds</b> in practice? We could now define a simple procedure to solve bias in ML models. Determine the protected attributes of the datasetTrain the model on the full datasetWith the mathematical definition of \u201c<b>equalized</b> <b>odds</b>\u201d, you are able to measure the biasCorrect the model output algorithmically, if needed, after the model has been trained This systematic approach will allow you to quantify the degree of bias and you will ...", "dateLastCrawled": "2022-01-25T05:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Responsibility Cannot Be Delegated to an <b>Algorithm</b> \u2014 <b>MACHINE</b> <b>LEARNING</b> ...", "url": "https://www.machinelearningforscience.de/en/responsibility-cannot-be-delegated-to-an-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinelearning</b>forscience.de/en/responsibility-<b>can</b>not-be-delegated-to-an...", "snippet": "Another fairness concept is \u201c<b>equalized</b> <b>odds</b>\u201d or \u201c<b>equalized</b> opportunity\u201d, meaning everyone has the same chances. If, for instance, we stick to the case of university admissions, that would mean if you have equal ability you should \u2013 no matter if you\u2019re a woman or a man, or black or white \u2013 be admitted to this course of study. The big problem is: You <b>can</b>\u2019t meet all the conceptions of fairness simultaneously. You need to decide in favor of one or the other.", "dateLastCrawled": "2021-11-29T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "with <b>Machine</b> <b>Learning</b>", "url": "https://aisecure.github.io/TEACHING/CS562/Fall2021_files/student_pres/1118JiaweiZhang.pdf", "isFamilyFriendly": true, "displayUrl": "https://aisecure.github.io/TEACHING/CS562/Fall2021_files/student_pres/1118JiaweiZhang.pdf", "snippet": "<b>Equalized</b> <b>odds</b> (not included in paper) Consider the previous case about getting admitted to college: Race Gender GPA Publication Predictor Admitted (Y = 1) Department Same Probability for male and female. Not Admitted (Y = 0) De\ufb01nition 3: (<b>Equalized</b> <b>odds</b>) A predictor H : X \u2192 Y satis\ufb01es this de\ufb01nition if the subjects in the protected and", "dateLastCrawled": "2022-01-11T17:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "In this case, the biases of humans are not mitigated by the <b>machine learning</b> <b>algorithm</b>. In fact, they are reproduced in the classifications that are made. Why does this happen? Recidivism scores such as those made by the Northpointe software are based on prior arrests, age of first police contact, parents\u2019 incarceration record. This information is shaped by biases in the world (such as from cultural values and nationalism) and injustices more generally (such as racial prejudices). This ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Tutorial on Fairness of <b>Machine</b> <b>Learning</b> in Recommender Systems", "url": "https://fairness-tutorial.github.io/files/Tutorial_on_Fairness_in_Recommendation.pdf", "isFamilyFriendly": true, "displayUrl": "https://fairness-tutorial.github.io/files/Tutorial_on_Fairness_in_Recommendation.pdf", "snippet": "vised <b>learning</b> usually implies constraints such as <b>equalized</b> <b>odds</b> [32, 60] and demographic parity [11]. Individual fairness requires that similar treatment should be received by each similar individu-als, which is hard to define precisely due to the lack of agreement on task-specific similarity metrics for individuals [20]. There are", "dateLastCrawled": "2022-01-29T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Differentially Private Fair <b>Learning</b> - Proceedings of <b>Machine</b> <b>Learning</b> ...", "url": "http://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "snippet": "Our \ufb01rst <b>algorithm</b> is a private implementation of the <b>equalized</b> <b>odds</b> post-processing approach of (Hardt et al.,2016). This <b>algorithm</b> is appeal-ingly simple, but must be able to use protected group membership explicitly at test time, which <b>can</b> be viewed as a form of \u201cdisparate treatment\u201d. Our second <b>algorithm</b> is a differentially private version of the oracle-ef\ufb01cient in-processing ap-proach of (Agarwal et al.,2018) which is more complex but need not have access to protected group ...", "dateLastCrawled": "2022-01-22T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Enforcing fairness in logistic regression <b>algorithm</b>", "url": "https://www.researchgate.net/publication/344855305_Enforcing_fairness_in_logistic_regression_algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344855305_Enforcing_fairness_in_logistic...", "snippet": "modify the results of the <b>learning</b> <b>algorithm</b> [4]. Adj ... <b>equalized</b> <b>odds</b> with out sacrificing both predi ctive accuracy . and disparate impact. Constraint logistic regression <b>can</b> be lear ned using ...", "dateLastCrawled": "2021-09-12T18:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "AI Fairness -A Brief Introduction to AI Fairness 360 | by ...", "url": "https://transformernlp.medium.com/ai-fairness-a-brief-introduction-to-ai-fairness-360-b2e39c96ca49", "isFamilyFriendly": true, "displayUrl": "https://transformernlp.medium.com/ai-fairness-a-brief-introduction-to-ai-fairness-360...", "snippet": "The choice among <b>algorithm</b> categories <b>can</b> partially be made based on the user persona\u2019s ability to intervene at different parts of a <b>machine</b> <b>learning</b> pipeline. If the user is allowed to modify the training data, then pre-processing <b>can</b> be used. If the user is allowed to change the <b>learning</b> <b>algorithm</b>, then in-processing <b>can</b> be used. If the user <b>can</b> only treat the learned model as a black box without any ability to modify the training data or <b>learning</b> <b>algorithm</b>, then only post-processing <b>can</b> ...", "dateLastCrawled": "2022-01-18T14:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>Learning</b> \u2013 The Results Are Not the only Thing that Matters ...", "url": "https://link.springer.com/chapter/10.1007/978-3-030-50423-6_46", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-50423-6_46", "snippet": "Recently it has come to attention that skilfully crafted inputs <b>can</b> affect artificial intelligence algorithms to sway the classification results in the fashion tailored to the adversary needs [].This new disturbance in the proliferation of <b>Machine</b> <b>Learning</b> has been a subject riveting attention of the researches very recently, and at the time of writing this paper a variety of vulnerabilities have been uncovered [].With the recent spike of interest in the field of securing ML algorithms, a ...", "dateLastCrawled": "2021-12-22T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Fairness in Machine Learning</b> \u2014 Labelia (ex Substra Foundation)", "url": "https://www.labelia.org/en/blog/fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.labelia.org/en/blog/<b>fairness-in-machine-learning</b>", "snippet": "Although these biases are often unintentional, the consequences of their presence in <b>machine</b> <b>learning</b> systems <b>can</b> be significant. Depending on how the <b>machine</b> <b>learning</b> systems are used, such biases could result in lower customer service experiences, reduced sales and revenue, unfair or possibly illegal actions, and potentially dangerous conditions.", "dateLastCrawled": "2022-01-30T18:51:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A <b>machine learning</b> technique that iteratively combines a set of simple and not very accurate classifiers ... Contrast with <b>equalized</b> <b>odds</b> and equality of opportunity, which permit classification results in aggregate to depend on sensitive attributes, but do not permit classification results for certain specified ground-truth labels to depend on sensitive attributes. See &quot;Attacking discrimination with smarter <b>machine learning</b>&quot; for a visualization exploring the tradeoffs when optimizing for ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Residual Unfairness in Fair <b>Machine</b> <b>Learning</b> from Prejudiced Data", "url": "http://proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/kallus18a/kallus18a.pdf", "snippet": "chine <b>learning</b> has considered some subcomponents of the overall problem we study of <b>learning</b> fair policies from bi-ased datasets. Hardt et al. (2016) formalize the criteria of equal opportunity and <b>equalized</b> <b>odds</b>. Lum &amp; Isaac (2016) show that a predictive policing algorithm for drug enforce-ment in Oakland, trained on police records, will ...", "dateLastCrawled": "2022-01-31T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "We embed the evaluation of AI fairness within the best practices of <b>machine</b> <b>learning</b> development and operations such as version control, ... This includes measures such as Demographic Parity / Statistical Parity (Dwork et al., 2012), <b>Equalized</b> <b>Odds</b> Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Fighting Money Laundering with Statistics and <b>Machine</b> <b>Learning</b>: An ...", "url": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-machine-learning-an-introduction-and-review", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-<b>machine</b>...", "snippet": "Third, we present recent <b>machine</b> <b>learning</b> concepts that have the potential to improve AML. The remainder of the paper is organized as follows. Section 2 introduces AML in banks. Section 3 presents our terminology. Sections 4 and 5 review the literature on client risk profiling and suspicious behavior flagging, respectively. Section 6 provides a discussion on future research directions and section 7 concludes the paper. 2 Anti-Money Laundering in Banks. The international framework for AML is ...", "dateLastCrawled": "2022-01-28T21:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Survey on Bias and Fairness in <b>Machine</b> <b>Learning</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1908.09635/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.09635", "snippet": "With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in <b>machine</b> <b>learning</b>, natural language ...", "dateLastCrawled": "2021-11-15T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Mitigating Unwanted Biases with Adversarial Learning</b>", "url": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_162.pdf", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concern- ing demographic groups are in the training data, well-trained models will re\ufb02ect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an ad-versary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or in ...", "dateLastCrawled": "2021-12-17T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Papers on fairness in <b>machine</b> <b>learning</b>, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about fairness as it might apply to an ML process at hand, common language \u2013 and common sense \u2013 will do just fine. If, after analyzing your use case, you judge that the more technical results are relevant to the process in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On Predicting Recidivism: Epistemic Risk, Tradeoffs, and Values in ...", "url": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on-predicting-recidivism-epistemic-risk-tradeoffs-and-values-in-machine-learning/7E541FA03E78C3141A65EA99A0CA6E9A", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/on...", "snippet": "This paper examines the role of value judgments in the design of <b>machine</b>-<b>learning</b> (ML) systems generally and in recidivism-prediction algorithms specifically. Drawing on work on inductive and epistemic risk, the paper argues that ML systems are value laden in ways similar to human decision making, because the development and design of ML systems requires human decisions that involve tradeoffs that reflect values. In many cases, these decisions have significant\u2014and, in some cases, disparate ...", "dateLastCrawled": "2022-01-26T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Measuring discrimination in algorithmic <b>decision making</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10618-017-0506-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10618-017-0506-1", "snippet": "A <b>machine</b> <b>learning</b> algorithm is a procedure used for producing a predictive model from historical data. A model is a collection of decision rules used for <b>decision making</b> for new incoming data. The model would take personal characteristics as inputs (for example, income, credit history, employment status), and produce a prediction (for example, credit risk level). Fig. 1. A typical <b>machine</b> <b>learning</b> setting. Full size image. <b>Learning</b> algorithms as such cannot discriminate, because they are ...", "dateLastCrawled": "2022-01-29T20:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(equalized odds)  is like +(machine learning algorithm)", "+(equalized odds) is similar to +(machine learning algorithm)", "+(equalized odds) can be thought of as +(machine learning algorithm)", "+(equalized odds) can be compared to +(machine learning algorithm)", "machine learning +(equalized odds AND analogy)", "machine learning +(\"equalized odds is like\")", "machine learning +(\"equalized odds is similar\")", "machine learning +(\"just as equalized odds\")", "machine learning +(\"equalized odds can be thought of as\")", "machine learning +(\"equalized odds can be compared to\")"]}