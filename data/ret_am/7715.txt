{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Empirical Risk Minimization under Random Censorship: Theory</b> and ...", "url": "https://deepai.org/publication/empirical-risk-minimization-under-random-censorship-theory-and-practice", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>empirical-risk-minimization-under-random-censorship</b>...", "snippet": "As the distribution of (X, Y) is unknown in practice, the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> paradigm (<b>ERM</b> in abbreviated form, see e.g. Gy\u00f6rfi et al. ) suggests considering solutions \u02c6 f n of the <b>minimization</b> problem, also referred to as least squares regression, min f \u2208 F \u02c6 R n (f), where \u02c6 R n (f) is a statistical estimate of the <b>risk</b> R P (f) computed from a training sample D n = {(X 1, Y 1), \u2026, (X n, Y n)} of independent copies of (X, Y). In general the <b>empirical</b> version. \u02c6 R n (f) = 1 ...", "dateLastCrawled": "2022-01-23T17:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Data Science Interview Questions (30 days</b> of Interview Preparation) # ...", "url": "https://inblog.in/Data-Science-Interview-Questions-30-days-of-Interview-Preparation-Day-17-Ozxv3yYzps", "isFamilyFriendly": true, "displayUrl": "https://inblog.in/<b>Data-Science-Interview-Questions-30-days</b>-of-Interview-Preparation...", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>): ... ULMFiT achieves the state-of-the-art result using novel techniques <b>like</b>: 1. Discriminative fine-<b>tuning</b>. 2. Slanted triangular <b>learning</b> rates. 3. Gradual unfreezing. Discriminative Fine-<b>Tuning</b> . Different layers of a neural network capture different types of information so they should be fine tuned to varying extents. Instead of using the same <b>learning</b> rates for all layers of the <b>model</b>, discriminative fine-<b>tuning</b> allows us to tune each layer with ...", "dateLastCrawled": "2021-11-25T12:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Empirical risk minimization</b>: probabilistic complexity and stepsize ...", "url": "https://link.springer.com/article/10.1007%2Fs10589-019-00080-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10589-019-00080-2", "snippet": "<b>Empirical risk minimization</b> (<b>ERM</b>) is one of the most powerful tools in applied statistics, and is regarded as the canonical approach to regression analysis. In the context of <b>machine</b> <b>learning</b> and big data analytics, various important problems such as support vector machines, (regularized) linear regression, and logistics regression can be cast as <b>ERM</b> problems, see for e.g. [ 17 ].", "dateLastCrawled": "2022-01-25T18:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>DATA SCIENCE INTERVIEW PREPARATION DAY 17</b>", "url": "https://gamakaai.com/wp-content/uploads/2021/04/GamakaAI_DAY_17_InterviewQuestions.pdf", "isFamilyFriendly": true, "displayUrl": "https://gamakaai.com/wp-content/uploads/2021/04/GamakaAI_DAY_17_InterviewQuestions.pdf", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>): It is a principle in statistical <b>learning</b> theory which defines a family of <b>learning</b> algorithms and is used to give theoretical bounds on their performance. The idea is that we don\u2019t know exactly how well an algorithm will work in practice (the true &quot;<b>risk</b>&quot;) because we don&#39;t know the true distribution of data that the algorithm will work on, but as an alternative we can measure its performance on a known set of training data. We assumed that our samples ...", "dateLastCrawled": "2021-12-01T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Optimization</b> - mlstory.org", "url": "https://mlstory.org/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>optimization</b>.html", "snippet": "Then we turned to <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) where we instead rely on numerical methods to discover good decision rules when we don\u2019t have such a probability <b>model</b>. In this chapter, we take a closer look at how to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems effectively. We focus on the core <b>optimization</b> methods commonly used to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems and on the mathematical tools used to analyze their running times. Our main subject will be gradient descent ...", "dateLastCrawled": "2022-01-30T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Learning as Optimization: Linear Regression</b>", "url": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec4_slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec4_slides.pdf", "snippet": "<b>Machine</b> <b>Learning</b> (CS771A) <b>Learning as Optimization: Linear Regression</b> 3 . <b>Learning</b> as Optimization To nd the best f, we minimize the <b>empirical</b> <b>risk</b> w.r.t. f.<b>Empirical</b> <b>Risk</b> <b>Minimization</b>(<b>ERM</b>) f^ = arg min f L emp(f) = arg min f XN n=1 \u2018(y n;f(x n)) We also want f to be \\simple&quot;. To do so, we add a \\regularizer&quot; R(f) f^ = arg min f XN n=1 \u2018(y n;f(x n)) + R(f) The regularizer R(f) is a measure of complexity of our <b>model</b> f This is called Regularized (<b>Empirical</b>) <b>Risk</b> <b>Minimization</b> We want both ...", "dateLastCrawled": "2022-02-03T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Practical <b>Issues in Machine Learning OverfittingOverfittingand</b> <b>Model</b> ...", "url": "https://www.cs.cmu.edu/~epxing/Class/10701-10s/Lecture/lecture8.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~epxing/Class/10701-10s/Lecture/lecture8.pdf", "snippet": "Cost of <b>model</b> (log prior) Bayesian viewpoint: prior probability of f \u2261 cost is small if fis highly probable, cost is large if fis improbable <b>ERM</b> (<b>empirical</b> <b>risk</b> <b>minimization</b>) over a restricted class F, e.g. linear classifiers, \u2261uniform prior on f \u0454F, zero probability for other predictors", "dateLastCrawled": "2021-05-27T01:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "4 \u2013 The Overfitting Iceberg \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie ...", "url": "https://blog.ml.cmu.edu/2020/08/31/4-overfitting/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/4-overfitting", "snippet": "As stated in the original paper, the predictor h n is commonly chosen from some function class H such as logistic regression, using <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>). By changing the <b>model</b> complexity, the capacity of the function class H also changes. It is possible to control the bias-variance tradeoff by selecting our models to balance ...", "dateLastCrawled": "2022-01-25T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>From predictive to prescriptive analytics: A data-driven</b> multi-item ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167923620300956", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167923620300956", "snippet": "The proposed approach uses sample demand data as input, and <b>machine</b> (and deep) <b>learning</b> methods with <b>empirical</b> <b>risk</b> <b>minimization</b> principle to find order quantities. A heuristic is developed using hierarchies of the retail products to perform multi-item inventory optimization when a capacity constraint is active. The proposed approach is tested on a real-world dataset of retail products. The results from the proposed method are compared with data-driven max-min and <b>empirical</b> inventory ...", "dateLastCrawled": "2022-01-11T17:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> and Dynamical Systems - Approximation, Optimization ...", "url": "http://www.smartchair.org/f_/JSML2020/8a6e0/F4/Machine%20Learning%20and%20Dynamical%20Systems%20-%20Qianxiao%20Li.pdf", "isFamilyFriendly": true, "displayUrl": "www.smartchair.org/f_/JSML2020/8a6e0/F4/<b>Machine</b> <b>Learning</b> and Dynamical Systems...", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) F^ arg min F2H 1 N XN i=1 (F(xi); yi |{z} F (xi)) ( is aloss function) Population <b>risk</b> <b>minimization</b> (PRM) ~F arg min F2H E (x;y)\u02d8 (F(x);y) ( is theinput-output distribution) We want to solve PRM, but we often can only perform <b>ERM</b>. The gap between ^F and ~F is the problem ofgeneralization. 8 49. <b>Empirical</b> and Population <b>Risk</b> <b>Minimization</b> <b>Learning</b>/approximation involves picking out the \u201cbest\u201d F 2Hso that it makes <b>similar</b> predictions to F <b>Empirical</b> <b>risk</b> ...", "dateLastCrawled": "2022-01-05T19:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Optimization</b> - mlstory.org", "url": "https://mlstory.org/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>optimization</b>.html", "snippet": "Then we turned to <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) where we instead rely on numerical methods to discover good decision rules when we don\u2019t have such a probability <b>model</b>. In this chapter, we take a closer look at how to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems effectively. We focus on the core <b>optimization</b> methods commonly used to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems and on the mathematical tools used to analyze their running times. Our main subject will be gradient descent ...", "dateLastCrawled": "2022-01-30T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>siamcse21</b> - GitHub Pages", "url": "https://amirgholami.github.io/siamcse21/", "isFamilyFriendly": true, "displayUrl": "https://amirgholami.github.io/<b>siamcse21</b>", "snippet": "Training is often formulated as the solution of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) problems, which are optimization programs whose complexity scales with the number of elements in the dataset. In such settings, the implementation cost of second-order methods is prohibitive, and, hence, resort to stochastic methods is inevitable. Stochastic Newton-type methods (<b>similar</b> to their deterministic versions) are superior to first-order methods in a local neighborhood of the optimal solution, but ...", "dateLastCrawled": "2021-12-17T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Private Empirical Risk Minimization Beyond the</b> Worst Case: The Effect ...", "url": "https://www.researchgate.net/publication/268525500_Private_Empirical_Risk_Minimization_Beyond_the_Worst_Case_The_Effect_of_the_Constraint_Set_Geometry", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/268525500_Private_<b>Empirical</b>_<b>Risk</b>_<b>Minimization</b>...", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard technique in <b>machine</b> <b>learning</b>, where a <b>model</b> is selected by minimizing a loss function over constraint set. When the training dataset consists of ...", "dateLastCrawled": "2021-10-26T05:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Extracurricular <b>Learning</b>: Knowledge Transfer Beyond <b>Empirical</b> Distribution", "url": "https://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular_Learning_Knowledge_Transfer_Beyond_Empirical_Distribution_CVPRW_2021_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular...", "snippet": "major accuracy improvements compared to the <b>empirical</b> <b>risk</b> <b>minimization</b>-based training for various recent neural network architectures: ... <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>): min \u03b8 1 n X i l(f\u03b8(xi),yi) (2) In KD [28], a student <b>model</b> f\u03b8 is encouraged to match the output of a teacher \u03c4 on the training set: min \u03b8 1 n X i l(f\u03b8(xi),\u03c4(xi)) (3) \u03c4 in (3) can be a single more powerful <b>model</b> or an ensemble of several models. In the original KD [28] an average of losses in (2) and (3) is used ...", "dateLastCrawled": "2022-01-17T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Learning</b> to Branch - Proceedings of <b>Machine</b> <b>Learning</b> Research | The ...", "url": "http://proceedings.mlr.press/v80/balcan18a/balcan18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/balcan18a/balcan18a.pdf", "snippet": "training set, and thus performs <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>). We prove that the adaptive nature of our algorithm is necessary: performing <b>ERM</b> over a data-independent dis- cretization of the parameter space can be disastrous. In particular, for any discretization of the parameter space, we provide an in\ufb01nite family of distributions over MILP in-stances such that every point in the discretization results in a B&amp;B tree with exponential size in expectation, but there exist in\ufb01nitely ...", "dateLastCrawled": "2022-01-31T15:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Our NeurIPS 2021 Reading List", "url": "https://www.borealisai.com/en/blog/our-neurips-2021-reading-list/", "isFamilyFriendly": true, "displayUrl": "https://www.borealisai.com/en/blog/our-neurips-2021-reading-list", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is commonly used to train models in <b>machine</b> <b>learning</b>, but in practice, distribution shifts (or domain shifts) can cause problems for these models and result in suboptimal performance. Previously researchers have studied <b>similar</b> problems in several related areas such as domain adaptation, domain generalization, and meta-<b>learning</b>. In this work, the authors combine ideas from meta-<b>learning</b> and domain adaptation and propose a generic framework termed Adaptive ...", "dateLastCrawled": "2022-01-11T16:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "4 \u2013 The Overfitting Iceberg \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie ...", "url": "https://blog.ml.cmu.edu/2020/08/31/4-overfitting/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/4-overfitting", "snippet": "As stated in the original paper, the predictor h n is commonly chosen from some function class H such as logistic regression, using <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>). By changing the <b>model</b> complexity, the capacity of the function class H also changes. It is possible to control the bias-variance tradeoff by selecting our models to balance ...", "dateLastCrawled": "2022-01-25T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Lecture Notes for EC500: Optimization for <b>Machine</b> <b>Learning</b>", "url": "https://optmlclass.github.io/notes/optforml_notes.pdf", "isFamilyFriendly": true, "displayUrl": "https://optmlclass.github.io/notes/optforml_notes.pdf", "snippet": "<b>machine</b> <b>learning</b> work\ufb02ow consists of two main choices: 1.Choose some kind of <b>model</b> to explain the data. In supervised <b>learning</b> in which z = (x;y), typically we pick some function fand use the <b>model</b> y \u02c7f(x;w) where w is the parameter of the <b>model</b>. We will let Wbe the set of acceptable values for w. 2.Fit the <b>model</b> to the data. That is, using ...", "dateLastCrawled": "2022-01-28T09:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> Under Fairness Constraints | Request PDF", "url": "https://www.researchgate.net/publication/333044604_Empirical_Risk_Minimization_Under_Fairness_Constraints", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333044604_<b>Empirical</b>_<b>Risk</b>_<b>Minimization</b>_Under...", "snippet": "Donini et al. (2018) presented a comprehensive approach based on <b>empirical</b> <b>risk</b> <b>minimization</b>, which incorporates a fairness constraint into the <b>learning</b> problem. It encourages the conditional <b>risk</b> ...", "dateLastCrawled": "2021-12-20T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Chapter 10 Supervised Learning</b> | Introduction to Data Science", "url": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "snippet": "We now present the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) approach to supervised <b>learning</b>, a.k.a. M-estimation in the statistical literature. Remark. We do not discuss purely algorithmic approaches such as K-nearest neighbour and kernel smoothing due to space constraints. For a broader review of supervised <b>learning</b>, see the Bibliographic Notes. Example 10.1 (Spam classification) Consider the problem of predicting if a mail is spam or not based on its attributes: length, number of exclamation ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Gradient Starvation</b> - GitHub Pages", "url": "https://mohammadpz.github.io/GradientStarvation.html", "isFamilyFriendly": true, "displayUrl": "https://mohammadpz.github.io/<b>GradientStarvation</b>.html", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b> - vanilla cross-entropy) performs well on the training set (91 %) but very bad on the test set (24 %). It is expected because the color feature is reversed in the test set, and we cannot expect the poor <b>model</b> to ignore the color! On the other hand, Invariant <b>Risk</b> <b>Minimization</b> (IRM) achieves good performance on the test set (67 %). However, SD also performs well (68 %) without requiring access to multiple training environments, unlike IRM. How does SD learn to ...", "dateLastCrawled": "2022-01-29T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding <b>Machine</b> <b>Learning</b>: From Theory To Algorithms 1107057132 ...", "url": "https://ebin.pub/understanding-machine-learning-from-theory-to-algorithms-1107057132-9781107057135.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/understanding-<b>machine</b>-<b>learning</b>-from-theory-to-algorithms-1107057132...", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length (MDL) <b>learning</b> rules, which show \u201chow <b>a machine</b> <b>can</b> learn.\u201d We quantify the amount of data needed for <b>learning</b> using the <b>ERM</b>, SRM, and MDL rules and show how <b>learning</b> might fail by deriving a \u201cno-free-lunch\u201d theorem. We also discuss how much computation time is required for <b>learning</b>. In the second part of the book we describe various <b>learning</b> algorithms. For some of ...", "dateLastCrawled": "2021-12-31T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Lester</b> Mackey: Research - Stanford University", "url": "https://web.stanford.edu/~lmackey/research.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/~lmackey/research.html", "snippet": "Recent work in <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) approximates the expensive refitting with a single Newton step warm-started from the full training set optimizer. While this <b>can</b> greatly reduce runtime, several open questions remain including whether these approximations lead to faithful <b>model</b> selection and whether they are suitable for non-smooth objectives. We address these questions with three main contributions: (i) we provide uniform non-asymptotic, deterministic <b>model</b> assessment ...", "dateLastCrawled": "2022-02-02T17:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The use of <b>vicinal-risk minimization for training decision trees</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S1568494615001507", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1568494615001507", "snippet": "In general, training in <b>machine</b> <b>learning</b> is ill-posed and <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) does not necessarily produce best generalization over an unseen test set, a problem which is exacerbated by small datasets; it is this \u2018small data\u2019 scenario we explicitly address in this paper. The deficiencies of <b>ERM</b> are illustrated in Fig. 1 for the trivial case of classifying linearly separable patterns with a plane. Reduction of the ER to zero <b>can</b> be achieved by any of the infinite number of ...", "dateLastCrawled": "2022-01-14T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>A Bayesian perspective of statistical machine learning</b> for big data ...", "url": "https://link.springer.com/article/10.1007/s00180-020-00970-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00180-020-00970-8", "snippet": "The <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) principle discussed in Sect. 4 is often implemented using optimization. The <b>risk</b> function corresponds to the loss function that is appropriate for the problem. Many <b>machine</b> <b>learning</b> problems are convex optimization problems and numerical optimization techniques are used in these problems, see e.g., Nocedal and Wright", "dateLastCrawled": "2021-12-27T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Chapter 10 Supervised Learning</b> | R (BGU course)", "url": "http://www.john-ros.com/Rcourse/supervised.html", "isFamilyFriendly": true, "displayUrl": "www.john-ros.com/Rcourse/supervised.html", "snippet": "10.1.3 Unbiased <b>Risk</b> Estimation. The fundamental problem of overfitting, is that the <b>empirical</b> <b>risk</b>, \\(R_n(\\hat f)\\), is downward biased to the population <b>risk</b>, \\(R(\\hat f)\\).We <b>can</b> remove this bias in two ways: (a) purely algorithmic resampling approaches, and (b) theory driven estimators.. Train-Validate-Test: The simplest form of algorithmic validation is to split the data.A train set to train/estimate/learn \\(\\hat f\\).A validation set to compute the out-of-sample expected loss, \\(R(\\hat ...", "dateLastCrawled": "2022-02-03T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding <b>Deep Learning</b> (Still) Requires Rethinking Generalization ...", "url": "https://cacm.acm.org/magazines/2021/3/250713-understanding-deep-learning-still-requires-rethinking-generalization/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2021/3/250713", "snippet": "Regularization <b>can</b> <b>be thought</b> of as the operational counterpart of a notion of <b>model</b> complexity. When the complexity of a <b>model</b> is very high, regularization introduces algorithmic tweaks intended to reward models of lower complexity. Regularization is a popular technique to make optimization problems &quot;well posed&quot;: when an infinite number of solutions agree with the data, regularization breaks ties in favor of the solution with lowest complexity.", "dateLastCrawled": "2022-02-01T17:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Severely Theoretical | <b>Machine</b> <b>learning</b>, computational neuroscience ...", "url": "https://severelytheoretical.wordpress.com/", "isFamilyFriendly": true, "displayUrl": "https://severelytheoretical.wordpress.com", "snippet": "We could, of course, do the old, boring <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), your grandmother\u2019s dumb method. This would simply lump the training data from all environments into one single giant training set and minimize the loss on that, with the hope that whatever features are more or less invariant across the environments will automatically emerge out of this optimization. Mathematically, <b>ERM</b> in this setting corresponds to solving the following well-known optimization problem (assuming the ...", "dateLastCrawled": "2022-02-03T13:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Extracurricular <b>Learning</b>: Knowledge Transfer Beyond <b>Empirical</b> Distribution", "url": "https://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular_Learning_Knowledge_Transfer_Beyond_Empirical_Distribution_CVPRW_2021_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular...", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>): min \u03b8 1 n X i l(f\u03b8(xi),yi) (2) In KD [28], a student <b>model</b> f\u03b8 is encouraged to match the output of a teacher \u03c4 on the training set: min \u03b8 1 n X i l(f\u03b8(xi),\u03c4(xi)) (3) \u03c4 in (3) <b>can</b> be a single more powerful <b>model</b> or an ensemble of several models. In the original KD [28] an average of losses in (2) and (3 ...", "dateLastCrawled": "2022-01-17T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which <b>can</b> result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "[2010.14672v1] Why Does MAML <b>Outperform ERM? An Optimization Perspective</b>", "url": "https://arxiv.org/abs/2010.14672v1", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2010.14672v1", "snippet": "<b>Model</b>-Agnostic Meta-<b>Learning</b> (MAML) has demonstrated widespread success in training models that <b>can</b> quickly adapt to new tasks via one or few stochastic gradient descent steps. However, the MAML objective is significantly more difficult to optimize <b>compared</b> to standard <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), and little is understood about how much MAML improves over <b>ERM</b> in terms of the fast adaptability of their solutions in various scenarios. We analytically address this issue in a linear ...", "dateLastCrawled": "2020-10-29T09:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Empirical risk minimization</b>: probabilistic complexity and stepsize ...", "url": "https://link.springer.com/article/10.1007%2Fs10589-019-00080-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10589-019-00080-2", "snippet": "<b>Empirical risk minimization</b> (<b>ERM</b>) is one of the most powerful tools in applied statistics, and is regarded as the canonical approach to regression analysis. In the context of <b>machine</b> <b>learning</b> and big data analytics, various important problems such as support vector machines, (regularized) linear regression, and logistics regression <b>can</b> be cast as <b>ERM</b> problems, see for e.g. . In an <b>ERM</b> problem, a training set with m instances, \\(\\{(\\mathbf {a}_i,b_i)\\}_{i=1}^m\\), is given, where \\(\\mathbf {a ...", "dateLastCrawled": "2022-01-25T18:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Fundamental Limits of Ridge-Regularized <b>Empirical</b> <b>Risk</b> <b>Minimization</b> in ...", "url": "https://web.ece.ucsb.edu/Faculty/selected_pubs/Pedarsani-mp/ERM-AISTATS21.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.ece.ucsb.edu/Faculty/selected_pubs/Pedarsani-mp/<b>ERM</b>-AISTATS21.pdf", "snippet": "<b>machine</b> <b>learning</b>. <b>ERM</b> methods are often e\ufb03cient in implementation, but \ufb01rst one needs to make cer-tainchoices: suchas,chooseanappropriatelossfunc-tion and regularization function, and tune the regu-larizationparameter. Classicalstatisticshavecomple-mented the practice of <b>ERM</b> with an elegant theory regardingoptimalsuchchoices,aswellas ...", "dateLastCrawled": "2021-08-30T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Optimization</b> - mlstory.org", "url": "https://mlstory.org/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>optimization</b>.html", "snippet": "Then we turned to <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) where we instead rely on numerical methods to discover good decision rules when we don\u2019t have such a probability <b>model</b>. In this chapter, we take a closer look at how to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems effectively. We focus on the core <b>optimization</b> methods commonly used to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems and on the mathematical tools used to analyze their running times. Our main subject will be gradient descent ...", "dateLastCrawled": "2022-01-30T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "4 \u2013 The Overfitting Iceberg \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie ...", "url": "https://blog.ml.cmu.edu/2020/08/31/4-overfitting/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/4-overfitting", "snippet": "As stated in the original paper, the predictor h n is commonly chosen from some function class H such as logistic regression, using <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>). By changing the <b>model</b> complexity, the capacity of the function class H also changes. It is possible to control the bias-variance tradeoff by selecting our models to balance ...", "dateLastCrawled": "2022-01-25T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning</b> to Branch - Proceedings of <b>Machine</b> <b>Learning</b> Research | The ...", "url": "http://proceedings.mlr.press/v80/balcan18a/balcan18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/bal<b>can</b>18a/bal<b>can</b>18a.pdf", "snippet": "training set, and thus performs <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>). We prove that the adaptive nature of our algorithm is necessary: performing <b>ERM</b> over a data-independent dis- cretization of the parameter space <b>can</b> be disastrous. In particular, for any discretization of the parameter space, we provide an in\ufb01nite family of distributions over MILP in-stances such that every point in the discretization results in a B&amp;B tree with exponential size in expectation, but there exist in\ufb01nitely ...", "dateLastCrawled": "2022-01-31T15:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Chapter 10 Supervised Learning</b> | Introduction to Data Science", "url": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "snippet": "We now present the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) approach to supervised <b>learning</b>, a.k.a. M-estimation in the statistical literature. Remark. We do not discuss purely algorithmic approaches such as K-nearest neighbour and kernel smoothing due to space constraints. For a broader review of supervised <b>learning</b>, see the Bibliographic Notes. Example 10.1 (Spam classification) Consider the problem of predicting if a mail is spam or not based on its attributes: length, number of exclamation ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Workshop on Socially Responsible <b>Machine</b> <b>Learning</b>", "url": "https://icml.cc/virtual/2021/workshop/8347", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2021/workshop/8347", "snippet": "Despite the success of large-scale <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) at achieving high accuracy across a variety of <b>machine</b> <b>learning</b> tasks, fair <b>ERM</b> is hindered by the incompatibility of fairness constraints with stochastic optimization. In this paper, we propose the fair <b>empirical</b> <b>risk</b> <b>minimization</b> via exponential R\u00e9nyi mutual information (FERMI) framework. FERMI is built on a stochastic estimator for exponential R\u00e9nyi mutual information (ERMI), an information divergence measuring the ...", "dateLastCrawled": "2021-12-20T14:01:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Basics of <b>Machine</b> <b>Learning</b>", "url": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_<b>learning</b>.pdf", "snippet": "This is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) COMPSCI 527 \u2014 Computer Vision Basics of <b>Machine</b> <b>Learning</b> 15/26. Loss and <b>Risk</b> <b>Machine</b> <b>Learning</b> and the Statistical <b>Risk</b> <b>ERM</b>: w^ 2argmin w2R m L T(w) In <b>machine</b> <b>learning</b>, we go much farther: We also want h to do well on previously unseen inputs To relate past and future data, assume that all data comes from the same joint probability distribution p(x;y) p is called the generative data model or just model The goal of <b>machine</b> <b>learning</b> is to ...", "dateLastCrawled": "2021-11-06T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Computational and Statistical <b>Learning</b> Theory", "url": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) / Sample Average Approximation (SAA): Collect sample z1UYU zm ... SGD for <b>Machine</b> <b>Learning</b> Initialize S 4 L r At iteration t: Draw T \u00e7\u00e1U \u00e71\u00de If U \u00e7 S \u00e7 \u00e1\u00f6 T \u00e7 O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00e7\u00f6 T \u00e7 else: S \u00e7 &gt; 5 Z S \u00e7 Return S % \u00cd L 5 \u00cd \u00c3 \u00cd S \u00e7 \u00e7 @ 5 Draw T 5\u00e1U 5 \u00e1\u00e5\u00e1 T \u00e0 \u00e1U \u00e0 1\u00de Initialize S 4 L r At iteration t: Pick E \u00d0 s\u00e5I at random If U \u00dc S \u00e7 \u00e1\u00f6 T \u00dc O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00dc\u00f6 T \u00dc else: S \u00e7 &gt; 5 Z S \u00e7 S \u00e7 &gt; 5 Z ...", "dateLastCrawled": "2022-01-26T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, <b>empirical</b> <b>risk</b>, motivation for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Further Reading, Supplementary: Jan 12: Consistency of <b>ERM</b>, Sufficient condition for <b>ERM</b> as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Stratified <b>Sampling Meets Machine Learning</b>", "url": "http://proceedings.mlr.press/v48/liberty16.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v48/liberty16.pdf", "snippet": "3. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard ap-proach in <b>machine</b> <b>learning</b> in which the chosen model is the minimizer of the <b>empirical</b> <b>risk</b>. The <b>empirical</b> <b>risk</b> R emp(p) is de\ufb01ned as an average loss of the model over the training set Q. Here Qis a query log containing a ran-", "dateLastCrawled": "2021-10-13T10:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lecture 1: Reinforcement <b>Learning</b>: What and Why?", "url": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "snippet": "<b>machine</b> <b>learning</b> and is referred to as <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). 3 Challenges of reinforcement <b>learning</b> Consider the cart pole balancing problem, where a cart carrying an unactuated pole \ufb02oats on a straight horizontal track. The cart is actuated by a torque applied either to the right or the left direction. Seeherefor a real cart ...", "dateLastCrawled": "2021-09-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Statistical <b>Learning</b> Theory and the C-Loss cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the <b>Risk</b> functional as L(.) is called the Loss function, and minimize it w.r.t. w achieving the best possible loss. But we can not do this ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2006.09461] Robust <b>Compressed Sensing using Generative Models</b> - arXiv", "url": "https://arxiv.org/abs/2006.09461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2006.09461", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-06-27T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Strati\ufb01ed Sampling meets <b>Machine</b> <b>Learning</b>", "url": "https://edoliberty.github.io/papers/lls15.pdf", "isFamilyFriendly": true, "displayUrl": "https://edoliberty.github.io/papers/lls15.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) approach. Their so-lution of the optimization problem however does not carry over. Chaudhuri et al. [10] also use the query log to iden- tify outlier records. Those are indexed separately and not sampled. While their approach mainly focuses on query ex-ecution speed, one can distill a sampling scheme from it. The work of Joshi and Jermaine [11] is closely related to 2Neyman allocation is also known as Neyman optimal allo-cation. ours. They generate a large ...", "dateLastCrawled": "2021-12-21T16:05:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ToyotaTechnologicalInstituteatChicago UniversityofTexasatAustin surbhi ...", "url": "https://arxiv.org/pdf/2005.07652.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2005.07652.pdf", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, \u02c6h \u2208 RERM U(S) ,argmin h\u2208H 1 m Xm i=1 sup z\u2208U(x) 1 [h(z) 6= y]. In this paper, we provide necessary and su\ufb03cient conditions on perturbation sets U ...", "dateLastCrawled": "2021-07-27T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Efficiently Learning Adversarially Robust Halfspaces with</b> Noise | DeepAI", "url": "https://deepai.org/publication/efficiently-learning-adversarially-robust-halfspaces-with-noise", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>efficiently-learning-adversarially-robust-halfspaces</b>...", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, ^ h \u2208 R E R M U ( S ) \u225c argmin h \u2208 H 1 m m \u2211 i = 1 sup z \u2208 U ( x ) 1 [ h ( z ) \u2260 y ] .", "dateLastCrawled": "2021-12-05T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficiently <b>Learning</b> Adversarially Robust Halfspaces with Noise", "url": "http://proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "snippet": "remains a major challenge in <b>machine</b> <b>learning</b>. A line of work has shown that predictors learned by deep neural networks are not robust to adversarial examples (Szegedy et al.,2014;Biggio et al.,2013;Goodfellow et al.,2015). This has led to a long line of research studying different aspects of robustness to adversarial examples. In this paper, we consider the problem of distribution-independent <b>learning</b> of halfspaces that are robust to ad-versarial examples at test time, also referred to as ...", "dateLastCrawled": "2021-11-21T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(empirical risk minimization (erm))  is like +(tuning a machine learning model)", "+(empirical risk minimization (erm)) is similar to +(tuning a machine learning model)", "+(empirical risk minimization (erm)) can be thought of as +(tuning a machine learning model)", "+(empirical risk minimization (erm)) can be compared to +(tuning a machine learning model)", "machine learning +(empirical risk minimization (erm) AND analogy)", "machine learning +(\"empirical risk minimization (erm) is like\")", "machine learning +(\"empirical risk minimization (erm) is similar\")", "machine learning +(\"just as empirical risk minimization (erm)\")", "machine learning +(\"empirical risk minimization (erm) can be thought of as\")", "machine learning +(\"empirical risk minimization (erm) can be compared to\")"]}