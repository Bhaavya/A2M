{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Research</b> - Jian @ WatVis", "url": "https://www.jeffjianzhao.com/research/", "isFamilyFriendly": true, "displayUrl": "https://www.jeffjianzhao.com/<b>research</b>", "snippet": "Recently, they have been further evolved into an advanced approach called <b>multi-head</b> <b>self-attention</b> networks, which can encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether <b>multi-head</b> attention. Meanwhile, the increased model complexity prevents users from easily understanding ...", "dateLastCrawled": "2022-02-03T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Appendix to 2020 CS379C Class Discussion Notes", "url": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "snippet": "If you are interested in following up, the earlier <b>book</b>, Figments of Reality, ... consisting of a masked <b>multi-head</b> <b>self-attention</b> layer and a point-wise, fully connected layer \u2013 so that, for example, if the source is the third character of the first pattern and the sink is the register of the second operand of the comparator, the model should expect that the output register contains a 1 (or some Boolean equivalent) if the current input character matches the third character of the first ...", "dateLastCrawled": "2022-01-31T03:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Deep Attentive End-<b>to-End Continuous Breath Sensing from Speech</b>", "url": "https://www.researchgate.net/publication/344843177_Deep_Attentive_End-to-End_Continuous_Breath_Sensing_from_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344843177_Deep_Attentive_End-to-End...", "snippet": "also indicated by the success of models that utilise <b>multi-head</b>. <b>self-attention</b> 34]. They further quantify this attention distribu-tion variability by using the Jensen-Shannon divergence. In all ...", "dateLastCrawled": "2021-10-16T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Artificial Intelligence in Education: 21st International Conference ...", "url": "https://dokumen.pub/artificial-intelligence-in-education-21st-international-conference-aied-2020-ifrane-morocco-july-610-2020-proceedings-part-ii-1st-ed-9783030522391-9783030522407.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/artificial-intelligence-in-education-21st-international-conference...", "snippet": "The resulting embeddings were processed by a <b>multi-head</b> attention layer that consists of a <b>selfattention</b> distributed across a number of heads. Attention computes the compatibility function of a query Q given a set of corresponding key-value pairs (K-V). These relationships modeled by <b>self-attention</b> do not necessarily correspond to those typically understood in natural language (e.g., syntactic structure, coreferences etc.), but are rather some latent dependencies that arise from the text. A ...", "dateLastCrawled": "2022-01-29T07:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Speech and Computer | springerprofessional.de", "url": "https://www.springerprofessional.de/speech-and-computer/19684028", "isFamilyFriendly": true, "displayUrl": "https://www.springerprofessional.de/speech-and-computer/19684028", "snippet": "This <b>book</b> constitutes the proceedings of the 23rd International Conference on Speech and Computer, SPECOM 2021, held in St. Petersburg, Russia, in September 2021.* The 74 papers presented were carefully reviewed and selected from 163 submissions. The papers present current research in the area of computer speech processing including audio signal processing, automatic speech recognition, speaker recognition, computational paralinguistics, speech synthesis, sign language and multimodal ...", "dateLastCrawled": "2021-12-28T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) AN OVERVIEW ON <b>THE HISTORY OF RUSSIAN LEXICOGRAPHY</b> - Academia.edu", "url": "https://www.academia.edu/40428464/AN_OVERVIEW_ON_THE_HISTORY_OF_RUSSIAN_LEXICOGRAPHY", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40428464/AN_OVERVIEW_ON_<b>THE_HISTORY_OF_RUSSIAN_LEXICOGRAPHY</b>", "snippet": "The dictionaries prepared between the 11-17th centuries are the first period of Russian lexicography and have the names of \u201cGlossary\u201d, \u201cAlfavit\u201d, \u201cAzbukovnik\u201d, \u201cLexicon\u201d. These dictionaries include the translation of an unfamiliar word or words that", "dateLastCrawled": "2022-01-16T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Text Understanding with the Attention Sum</b> Reader Network | Request PDF", "url": "https://www.researchgate.net/publication/306093209_Text_Understanding_with_the_Attention_Sum_Reader_Network", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/306093209_Text_Understanding_with_the...", "snippet": "Hoang, Wiseman, and Rush (2018) combined the Attention-Sum Reader (Kadlec et al. 2016) with a multi-task objective to track entities in the context, further improving performance to 59.23%. Both ...", "dateLastCrawled": "2022-01-06T22:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "INTERSPEECH 2019 - <b>Schedule</b>", "url": "https://www.interspeech2019.org/program/schedule/", "isFamilyFriendly": true, "displayUrl": "https://www.interspeech2019.org/program/<b>schedule</b>", "snippet": "Comparative Analysis of Think-<b>aloud</b> Methods for Everyday Activities in the Context of Cognitive Robotics Poster; 11 00 \u201313 00 Moritz Meier (University of Bremen), Celeste Mason (University of Bremen), Felix Putze (University of Bremen), Tanja Schultz (Universit\u00e4t Bremen) RadioTalk: a large-scale corpus of talk radio transcripts Poster; 11 00 \u201313 00 Doug Beeferman (MIT Media Lab), William Brannon (MIT Media Lab), Deb Roy (MIT Media Lab) Qualitative evaluation of ASR adaptation in a ...", "dateLastCrawled": "2022-02-03T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>IberSPEECH 2018 Proceedings</b> | Speech Synthesis | Cognitive Science - Scribd", "url": "https://www.scribd.com/document/395775589/IberSPEECH-2018-Proceedings", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/395775589/<b>IberSPEECH-2018-Proceedings</b>", "snippet": "Finally, we would <b>like</b> to thank all those whose effort made possible this conference, including the members of the organizing committee, the local organizing committee, the ALBAYZIN committee, the scientific reviewer committee, the authors, the conference attendees, the supporting institutions, and so many <b>people</b> who gave their best to achieve a succesful con-ference.", "dateLastCrawled": "2021-12-01T11:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Multimedia for Accessible Human Computer Interfaces [1&amp;nbsp;ed ...", "url": "https://dokumen.pub/multimedia-for-accessible-human-computer-interfaces-1nbsped-3030707156-9783030707156.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/multimedia-for-accessible-human-computer-interfaces-1nbsped...", "snippet": "3.2 Use Case #2: Auto Screen Scrolling When the user is <b>reading</b> an ebook, his or her head and eyes will move consistently on the screen. The proposed LSTM method can capture this consistency and anticipate that the user may want to read the next lines of the ebook. In this case, we show an example of navigating content on the screen using POGs. The content here can be PDF files such as a paper or <b>a book</b>, a website, an email, etc. Let ls be the height of a screen, the top left corner of the ...", "dateLastCrawled": "2022-01-21T15:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Research</b> - Jian @ WatVis", "url": "https://www.jeffjianzhao.com/research/", "isFamilyFriendly": true, "displayUrl": "https://www.jeffjianzhao.com/<b>research</b>", "snippet": "Recently, they have been further evolved into an advanced approach called <b>multi-head</b> <b>self-attention</b> networks, which can encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether <b>multi-head</b> attention. Meanwhile, the increased model complexity prevents users from easily understanding ...", "dateLastCrawled": "2022-02-03T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Deep Attentive End-<b>to-End Continuous Breath Sensing from Speech</b>", "url": "https://www.researchgate.net/publication/344843177_Deep_Attentive_End-to-End_Continuous_Breath_Sensing_from_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344843177_Deep_Attentive_End-to-End...", "snippet": "also indicated by the success of models that utilise <b>multi-head</b>. <b>self-attention</b> [34]. They further quantify this attention distribu- tion variability by using the Jensen-Shannon divergence. In all ...", "dateLastCrawled": "2021-10-16T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Artificial Intelligence in Education: 21st International Conference ...", "url": "https://dokumen.pub/artificial-intelligence-in-education-21st-international-conference-aied-2020-ifrane-morocco-july-610-2020-proceedings-part-ii-1st-ed-9783030522391-9783030522407.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/artificial-intelligence-in-education-21st-international-conference...", "snippet": "The resulting embeddings were processed by a <b>multi-head</b> attention layer that consists of a <b>selfattention</b> distributed across a number of heads. Attention computes the compatibility function of a query Q given a set of corresponding key-value pairs (K-V). These relationships modeled by <b>self-attention</b> do not necessarily correspond to those typically understood in natural language (e.g., syntactic structure, coreferences etc.), but are rather some latent dependencies that arise from the text. A ...", "dateLastCrawled": "2022-01-29T07:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Speech and Computer | springerprofessional.de", "url": "https://www.springerprofessional.de/speech-and-computer/19684028", "isFamilyFriendly": true, "displayUrl": "https://www.springerprofessional.de/speech-and-computer/19684028", "snippet": "This <b>book</b> constitutes the proceedings of the 23rd International Conference on Speech and Computer, SPECOM 2021, held in St. Petersburg, Russia, in September 2021.* The 74 papers presented were carefully reviewed and selected from 163 submissions. The papers present current research in the area of computer speech processing including audio signal processing, automatic speech recognition, speaker recognition, computational paralinguistics, speech synthesis, sign language and multimodal ...", "dateLastCrawled": "2021-12-28T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Appendix to 2020 CS379C Class Discussion Notes", "url": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "snippet": "A <b>similar</b> arrangement is used by L 2 to select a subroutine corresponding to a compiled sequence of L 1 primitives. These subroutines constitute the L 2 action space and correspond to the L 2 primitives. Both L 1 and L 2 networks employ state representations corresponding to compressed summaries of the contents of working memory as the means of selecting actions for execution. This approach is analogous to the way in which the basal ganglia construct and gate modulated summaries of ...", "dateLastCrawled": "2022-01-31T03:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Text Understanding with the Attention Sum</b> Reader Network | Request PDF", "url": "https://www.researchgate.net/publication/306093209_Text_Understanding_with_the_Attention_Sum_Reader_Network", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/306093209_Text_Understanding_with_the...", "snippet": "<b>Similar</b> to how humans read passages and give answers, we propose a three-stage mechanism called \u201dVerification for an Elaborate Span\u201d (V4ES): 1) sketchy <b>reading</b> that the model briefly browses ...", "dateLastCrawled": "2022-01-06T22:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Papers | The Web Conference - 2021", "url": "https://www2021.thewebconf.org/program/papers/", "isFamilyFriendly": true, "displayUrl": "https://www2021.thewebconf.org/program/papers", "snippet": "Specifically, ATON consists of a feature embedding module and a customized <b>self-attention</b> learning module, which are optimized by a triplet deviation-based loss function. We obtain an optimal attention-guided embedding space with expanded high-level information and rich semantics, and thus outlying behaviors of the queried outlier can be better ...", "dateLastCrawled": "2022-01-30T19:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>IberSPEECH 2018 Proceedings</b> | Speech Synthesis | Cognitive Science - Scribd", "url": "https://www.scribd.com/document/395775589/IberSPEECH-2018-Proceedings", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/395775589/<b>IberSPEECH-2018-Proceedings</b>", "snippet": "xxv Towards expressive prosody generation in TTS for <b>reading</b> <b>aloud</b> applications 40 Monica Dominguez, Alicia Burga, Mireia Farr\u00fas, Leo Wanner Performance evaluation of front- and back-end techniques for ASV spoofing de- tection systems based on deep features 45 Alejandro Gomez-Alanis, Antonio M. Peinado, Jos\u00e9 Andr\u00e9s Gonz\u00e1lez L\u00f3pez, Angel M. Gomez", "dateLastCrawled": "2021-12-01T11:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) AN OVERVIEW ON <b>THE HISTORY OF RUSSIAN LEXICOGRAPHY</b> - Academia.edu", "url": "https://www.academia.edu/40428464/AN_OVERVIEW_ON_THE_HISTORY_OF_RUSSIAN_LEXICOGRAPHY", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40428464/AN_OVERVIEW_ON_<b>THE_HISTORY_OF_RUSSIAN_LEXICOGRAPHY</b>", "snippet": "The dictionaries prepared between the 11-17th centuries are the first period of Russian lexicography and have the names of \u201cGlossary\u201d, \u201cAlfavit\u201d, \u201cAzbukovnik\u201d, \u201cLexicon\u201d. These dictionaries include the translation of an unfamiliar word or words that", "dateLastCrawled": "2022-01-16T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Multimedia for Accessible Human Computer Interfaces [1&amp;nbsp;ed ...", "url": "https://dokumen.pub/multimedia-for-accessible-human-computer-interfaces-1nbsped-3030707156-9783030707156.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/multimedia-for-accessible-human-computer-interfaces-1nbsped...", "snippet": "3.2 Use Case #2: Auto Screen Scrolling When the user is <b>reading</b> an ebook, his or her head and eyes will move consistently on the screen. The proposed LSTM method can capture this consistency and anticipate that the user may want to read the next lines of the ebook. In this case, we show an example of navigating content on the screen using POGs. The content here can be PDF files such as a paper or <b>a book</b>, a website, an email, etc. Let ls be the height of a screen, the top left corner of the ...", "dateLastCrawled": "2022-01-21T15:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Research</b> - Jian @ WatVis", "url": "https://www.jeffjianzhao.com/research/", "isFamilyFriendly": true, "displayUrl": "https://www.jeffjianzhao.com/<b>research</b>", "snippet": "Recently, they have been further evolved into an advanced approach called <b>multi-head</b> <b>self-attention</b> networks, which <b>can</b> encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether <b>multi-head</b> attention. Meanwhile, the increased model complexity prevents users from easily understanding ...", "dateLastCrawled": "2022-02-03T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Multi-Document Summarization of Evaluative Text</b>. | Request PDF", "url": "https://www.researchgate.net/publication/220947042_Multi-Document_Summarization_of_Evaluative_Text", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220947042_Multi-Document_Summarization_of...", "snippet": "By introducing an improved <b>multi-head</b> <b>self-attention</b> mechanism in the model coding stage, the training model enables the correct summary syntax and semantic information to obtain higher weight ...", "dateLastCrawled": "2021-12-18T03:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Appendix to 2020 CS379C Class Discussion Notes", "url": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "snippet": "Suppose that the source (&quot;from&quot; register) and sink (&quot;to&quot; register) of the last register transfer is available in some form, perhaps in the first attentional subunit of a transformer decoder stack \u2013 consisting of a masked <b>multi-head</b> <b>self-attention</b> layer and a point-wise, fully connected layer \u2013 so that, for example, if the source is the third character of the first pattern and the sink is the register of the second operand of the comparator, the model should expect that the output ...", "dateLastCrawled": "2022-01-31T03:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Analysis of Images, Social Networks and Texts: 8th International ...", "url": "https://dokumen.pub/analysis-of-images-social-networks-and-texts-8th-international-conference-aist-2019-kazan-russia-july-1719-2019-revised-selected-papers-1st-ed-2019-978-3-030-37333-7-978-3-030-37334-4.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/analysis-of-images-social-networks-and-texts-8th-international...", "snippet": "In this talk, I briefly describe standard attention in sequence to sequence models, as well as the Transformer architecture with <b>multi-head</b> <b>self-attention</b>. Then, we evaluate the contribution made by individual attention heads to the overall performance of the Transformer and analyse the roles played by them. I show that the most important and con\ufb01dent heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a ...", "dateLastCrawled": "2022-01-18T23:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Papers | The Web Conference - 2021", "url": "https://www2021.thewebconf.org/program/papers/", "isFamilyFriendly": true, "displayUrl": "https://www2021.thewebconf.org/program/papers", "snippet": "At the same time, governments are <b>thought</b> to be controlled by &quot;puppet masters,&quot; as democratically elected officials serve as a fake showroom of democracy. In this paper, we provide an empirical exploratory analysis of the QAnon community on Voat.co, a Reddit-esque news aggregator, which has recently captured the interest of the press for its toxicity and for providing a platform to QAnon followers. More precisely, we analyze a large dataset from /v/GreatAwakening, the most popular QAnon ...", "dateLastCrawled": "2022-01-30T19:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Image Processing <b>Group</b> | Signal Theory and Communications Department", "url": "https://imatge.upc.edu/web/biblio/export/bibtex", "isFamilyFriendly": true, "displayUrl": "https://imatge.upc.edu/web/biblio/export/bibtex", "snippet": "Using a camera with a depth sensor, the heads of the <b>people</b> in the range of view <b>can</b> be detected and modeled. This allows to\\ determine the orientation of the head which is used to estimate the direction of vision. A tracking by detection algorithm\\ allows to compute the trajectory of each user. The attention at each advertising point is estimated based on the trajectories and head orientations of the individuals in the area of interest.", "dateLastCrawled": "2021-12-31T18:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "ICT Systems Security and Privacy Protection: 35th IFIP TC 11 ...", "url": "https://ebin.pub/ict-systems-security-and-privacy-protection-35th-ifip-tc-11-international-conference-sec-2020-maribor-slovenia-september-2123-2020-proceedings-1st-ed-9783030582005-9783030582012.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/ict-systems-security-and-privacy-protection-35th-ifip-tc-11...", "snippet": "Nowadays, the \ufb01rst Scan Phase <b>can</b> be skipped by attackers, since targets <b>can</b> be found using scanner services like Shodan [23] or Censys [7]. This kind of specialized search engine is designed to scan and gather information about devices and systems accessible from the Internet. With a simple query, we <b>can</b> obtain thousands of IP addresses with the open SSH port. The SSHCure solves the scenario by allowing attackers to enter the Bruteforce Phase without passing through the Scan Phase. The ...", "dateLastCrawled": "2022-01-07T14:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) AN OVERVIEW ON <b>THE HISTORY OF RUSSIAN LEXICOGRAPHY</b> - Academia.edu", "url": "https://www.academia.edu/40428464/AN_OVERVIEW_ON_THE_HISTORY_OF_RUSSIAN_LEXICOGRAPHY", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40428464/AN_OVERVIEW_ON_<b>THE_HISTORY_OF_RUSSIAN_LEXICOGRAPHY</b>", "snippet": "The dictionaries prepared between the 11-17th centuries are the first period of Russian lexicography and have the names of \u201cGlossary\u201d, \u201cAlfavit\u201d, \u201cAzbukovnik\u201d, \u201cLexicon\u201d. These dictionaries include the translation of an unfamiliar word or words that", "dateLastCrawled": "2022-01-16T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "WWW &#39;21: Proceedings of the Web Conference 2021", "url": "https://www2021.thewebconf.org/wp-content/uploads/2021/07/toc_main.html", "isFamilyFriendly": true, "displayUrl": "https://www2021.thewebconf.org/wp-content/uploads/2021/07/toc_main.html", "snippet": "We <b>group</b> users into advantaged and disadvantaged groups according to their level of activity, and conduct experiments to show that current recommender systems will behave unfairly between two groups of users. Specifically, the advantaged users (active) who only account for a small proportion in data enjoy much higher recommendation quality than those disadvantaged users (inactive). Such bias <b>can</b> also affect the overall performance since the disadvantaged users are the majority. To solve this ...", "dateLastCrawled": "2022-02-03T09:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Multimedia for Accessible Human Computer Interfaces [1&amp;nbsp;ed ...", "url": "https://dokumen.pub/multimedia-for-accessible-human-computer-interfaces-1nbsped-3030707156-9783030707156.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/multimedia-for-accessible-human-computer-interfaces-1nbsped...", "snippet": "The content here <b>can</b> be PDF files such as a paper or <b>a book</b>, a website, an email, etc. Let ls be the height of a screen, the top left corner of the screen (0, 0) be the screen origin, and (x, y) be the detected POG in the screen coordinate system, where x is the row on the screen and y is the column. If y is not in the range of [ls /3, 2ls /3], the device will move the content automatically:", "dateLastCrawled": "2022-01-21T15:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Research</b> - Jian @ WatVis", "url": "https://www.jeffjianzhao.com/research/", "isFamilyFriendly": true, "displayUrl": "https://www.jeffjianzhao.com/<b>research</b>", "snippet": "Recently, they have been further evolved into an advanced approach called <b>multi-head</b> <b>self-attention</b> networks, which <b>can</b> encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether <b>multi-head</b> attention. Meanwhile, the increased model complexity prevents users from easily understanding ...", "dateLastCrawled": "2022-02-03T00:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Multi-Document Summarization of Evaluative Text</b>. | Request PDF", "url": "https://www.researchgate.net/publication/220947042_Multi-Document_Summarization_of_Evaluative_Text", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220947042_Multi-Document_Summarization_of...", "snippet": "By introducing an improved <b>multi-head</b> <b>self-attention</b> mechanism in the model coding stage, the training model enables the correct summary syntax and semantic information to obtain higher weight ...", "dateLastCrawled": "2021-12-18T03:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Automatic Summarization of Bug Reports</b> | Request PDF", "url": "https://www.researchgate.net/publication/262056000_Automatic_Summarization_of_Bug_Reports", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/262056000_<b>Automatic_Summarization_of_Bug_Reports</b>", "snippet": "By introducing an improved <b>multi-head</b> <b>self-attention</b> mechanism in the model coding stage, the training model enables the correct summary syntax and semantic information to obtain higher weight ...", "dateLastCrawled": "2021-11-14T17:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Appendix to 2020 CS379C Class Discussion Notes", "url": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "snippet": "If you are interested in following up, the earlier <b>book</b>, Figments of Reality, ... in some form, perhaps in the first attentional subunit of a transformer decoder stack \u2013 consisting of a masked <b>multi-head</b> <b>self-attention</b> layer and a point-wise, fully connected layer \u2013 so that, for example, if the source is the third character of the first pattern and the sink is the register of the second operand of the comparator, the model should expect that the output register contains a 1 (or some ...", "dateLastCrawled": "2022-01-31T03:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Artificial Intelligence in Education: 21st International Conference ...", "url": "https://dokumen.pub/artificial-intelligence-in-education-21st-international-conference-aied-2020-ifrane-morocco-july-610-2020-proceedings-part-ii-1st-ed-9783030522391-9783030522407.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/artificial-intelligence-in-education-21st-international-conference...", "snippet": "The resulting embeddings were processed by a <b>multi-head</b> attention layer that consists of a <b>selfattention</b> distributed across a number of heads. Attention computes the compatibility function of a query Q given a set of corresponding key-value pairs (K-V). These relationships modeled by <b>self-attention</b> do not necessarily correspond to those typically understood in natural language (e.g., syntactic structure, coreferences etc.), but are rather some latent dependencies that arise from the text. A ...", "dateLastCrawled": "2022-01-29T07:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Speech and Computer | springerprofessional.de", "url": "https://www.springerprofessional.de/speech-and-computer/19684028", "isFamilyFriendly": true, "displayUrl": "https://www.springerprofessional.de/speech-and-computer/19684028", "snippet": "This <b>book</b> constitutes the proceedings of the 23rd International Conference on Speech and Computer, SPECOM 2021, held in St. Petersburg, Russia, in September 2021.* The 74 papers presented were carefully reviewed and selected from 163 submissions. The papers present current research in the area of computer speech processing including audio signal processing, automatic speech recognition, speaker recognition, computational paralinguistics, speech synthesis, sign language and multimodal ...", "dateLastCrawled": "2021-12-28T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Papers | The Web Conference - 2021", "url": "https://www2021.thewebconf.org/program/papers/", "isFamilyFriendly": true, "displayUrl": "https://www2021.thewebconf.org/program/papers", "snippet": "Specifically, ATON consists of a feature embedding module and a customized <b>self-attention</b> learning module, which are optimized by a triplet deviation-based loss function. We obtain an optimal attention-guided embedding space with expanded high-level information and rich semantics, and thus outlying behaviors of the queried outlier <b>can</b> be better ...", "dateLastCrawled": "2022-01-30T19:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Multimedia for Accessible Human Computer Interfaces [1&amp;nbsp;ed ...", "url": "https://dokumen.pub/multimedia-for-accessible-human-computer-interfaces-1nbsped-3030707156-9783030707156.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/multimedia-for-accessible-human-computer-interfaces-1nbsped...", "snippet": "<b>Compared</b> to voice-based interfaces, the performance of GCIs will not change significantly in a noisy environment; and <b>compared</b> to hand-based or gesture-based interfaces, GCIs are convenient to use among <b>people</b> with physical impairments. GCIs have a wide range of applications and <b>can</b> improve user experiences in computer assisted driving, gaming, marketing, medicine, among other areas [20, 27, 33, 39]. After a device detects the point of gaze (POG), or point of regard, a user\u2019s attention is ...", "dateLastCrawled": "2022-01-21T15:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) AN OVERVIEW ON <b>THE HISTORY OF RUSSIAN LEXICOGRAPHY</b> - Academia.edu", "url": "https://www.academia.edu/40428464/AN_OVERVIEW_ON_THE_HISTORY_OF_RUSSIAN_LEXICOGRAPHY", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40428464/AN_OVERVIEW_ON_<b>THE_HISTORY_OF_RUSSIAN_LEXICOGRAPHY</b>", "snippet": "The dictionaries prepared between the 11-17th centuries are the first period of Russian lexicography and have the names of \u201cGlossary\u201d, \u201cAlfavit\u201d, \u201cAzbukovnik\u201d, \u201cLexicon\u201d. These dictionaries include the translation of an unfamiliar word or words that", "dateLastCrawled": "2022-01-16T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>IberSPEECH 2018 Proceedings</b> | Speech Synthesis | Cognitive Science - Scribd", "url": "https://www.scribd.com/document/395775589/IberSPEECH-2018-Proceedings", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/395775589/<b>IberSPEECH-2018-Proceedings</b>", "snippet": "As of Economy and Competitiveness and the European Social we <b>can</b> see in the second system the representation is able to Fund through the project TIN2017-85854-C4-1-R, by Gobierno cluster the examples from the same person, whereas in the first de Arag\u00f3n/FEDER (research <b>group</b> T36 17R) and by Nuance method is not able to cluster together examples from the same Communications, Inc.", "dateLastCrawled": "2021-12-01T11:33:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "10.5. <b>Multi-Head Attention</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_attention-mechanisms/multihead-attention.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_attention-mechanisms/<b>multihead-attention</b>.html", "snippet": "This design is called <b>multi-head attention</b>, where each of the \\(h\\) attention pooling outputs is a head [Vaswani et al., 2017]. Using fully-connected layers to perform learnable linear transformations, Fig. 10.5.1 describes <b>multi-head attention</b>.", "dateLastCrawled": "2022-02-02T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "The masked <b>multi-head</b> attention segment, which performs <b>multi-head</b> <b>self-attention</b> on the outputs, but does so in a masked way, so that positions depend on the past only. The <b>multi-head</b> attention segment , which performs <b>multi-head</b> <b>self-attention</b> on a combination of the ( encoded ) inputs and the outputs, so that the model learns to correlate encoded inputs with desired outputs.", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The first is a <b>multi-head</b> <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "Refer also to <b>self-attention</b> and <b>multi-head</b> <b>self-attention</b>, which are the building blocks of Transformers. B. bag of words. #language. A representation of the words in a phrase or passage, irrespective of order. For example, bag of words represents the following three phrases identically: the dog jumps; jumps the dog; dog jumps the; Each word is mapped to an index in a sparse vector, where the vector has an index for every word in the vocabulary. For example, the phrase the dog jumps is ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "From our article about GPT: \u201cThe input is then served to a masked <b>multi-head</b> attention segment, which computes <b>self-attention</b> in a unidirectional way.Here, the residual is added and the result is layer normalized.\u201d Indeed, GPT (which uses the Transformer decoder segment autoregressively during pretraining) and the original Transformer (which performs Seq2Seq), apply a mask in one of the attention modules \u2013 the masked <b>multi-head</b> <b>self-attention</b> subsegment in the decoder segment.. For any ...", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Capturing Multi-Resolution Context by Dilated <b>Self-Attention</b>", "url": "https://www.merl.com/publications/docs/TR2021-036.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.merl.com/publications/docs/TR2021-036.pdf", "snippet": "to <b>machine</b> translation or language modeling, where close-by words are more likely to have a dependent relationship, while only a few distant words or word groups are relevant to trace the semantic con-text and syntax of a sentence [15]. This hypothesis is investigated in this work by combining re-stricted (or time-restricted) <b>self-attention</b> with a dilation mechanism, whereby a high <b>self-attention</b> resolution for neighboring frames and a lower <b>self-attention</b> resolution for distant information ...", "dateLastCrawled": "2021-12-02T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "CoAtNet: how to perfectly combine CNNs and Transformers | by Leonardo ...", "url": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e187ecbf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/codex/coatnet-how-to-perfectly-combine-cnns-and-transformer-9632e...", "snippet": "The <b>multi-head</b> attention block computes <b>self-attention</b> several times with different weight matrices and then concatenates the results together, which are resized to the embedding dimension using ...", "dateLastCrawled": "2022-01-26T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "If you are looking for an <b>analogy</b> between <b>self attention</b> and attention, think of z serving the purpose of context vectors and not global alignment weights. The <b>Transformer</b> . \u26a0\ufe0f A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2) (left) The <b>Transformer</b> architecture. Source: paper. (right) An abstracted version of the same for better ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "5.3. Underfitting and Overfitting \u2014 Dive into Deep <b>Learning</b> 0.17.0 ...", "url": "http://preview.d2l.ai/d2l-en/master/chapter_machine-learning-fundamentals/underfit-overfit.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai/d2l-en/master/chapter_<b>machine</b>-<b>learning</b>-fundamentals/underfit-overfit.html", "snippet": "The noise term \\(\\epsilon\\) obeys a normal distribution with a mean of 0 and a standard deviation of 0.1. For optimization, we typically want to avoid very large values of gradients or losses. This is why the features are rescaled from \\(x^i\\) to \\(\\frac{x^i}{i!}\\).It allows us to avoid very large values for large exponents \\(i\\).We will synthesize 100 samples each for the training set and test set.", "dateLastCrawled": "2021-10-08T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "9.5. <b>Machine Translation</b> and the Dataset \u2014 Dive into Deep <b>Learning</b> 0.17 ...", "url": "https://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/<b>machine-translation</b>-and-dataset.html", "snippet": "<b>Machine Translation</b> and the Dataset \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation. 9.5. <b>Machine Translation</b> and the Dataset. We have used RNNs to design language models, which are key to natural language processing. Another flagship benchmark is <b>machine translation</b>, a central problem domain for sequence transduction models that transform ...", "dateLastCrawled": "2022-01-29T21:57:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(multi-head self-attention)  is like +(a group of people reading a book aloud)", "+(multi-head self-attention) is similar to +(a group of people reading a book aloud)", "+(multi-head self-attention) can be thought of as +(a group of people reading a book aloud)", "+(multi-head self-attention) can be compared to +(a group of people reading a book aloud)", "machine learning +(multi-head self-attention AND analogy)", "machine learning +(\"multi-head self-attention is like\")", "machine learning +(\"multi-head self-attention is similar\")", "machine learning +(\"just as multi-head self-attention\")", "machine learning +(\"multi-head self-attention can be thought of as\")", "machine learning +(\"multi-head self-attention can be compared to\")"]}