{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Value</b>-based Methods in Deep <b>Reinforcement Learning</b> | by Barak Or ...", "url": "https://towardsdatascience.com/value-based-methods-in-deep-reinforcement-learning-d40ca1086e1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>value</b>-based-methods-in-deep-<b>reinforcement-learning</b>-d40...", "snippet": "To promise optimal <b>value</b>: <b>state-action</b> pairs are represented discretely, and all actions are repeatedly sampled in all states. Q-<b>Learning</b> . Q <b>learning</b> in an off-policy method learns the <b>value</b> of taking action in a state and <b>learning</b> Q <b>value</b> and choosing how to act in the world. We define <b>state-action</b> <b>value</b> <b>function</b>: an expected return when starting in s, performing a, and following pi. Represented in a tabulated form. According to Q <b>learning</b>, the agent uses any policy to estimate Q that ...", "dateLastCrawled": "2022-01-29T11:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Value</b> <b>Function</b> <b>Approximation</b> \u2014 Prediction Algorithms | by Reuben ...", "url": "https://towardsdatascience.com/value-function-approximation-prediction-algorithms-98722818501b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>value</b>-<b>function</b>-<b>approximation</b>-prediction-<b>algorithms</b>...", "snippet": "<b>Value</b> <b>function</b> <b>approximation</b> tries to build some <b>function</b> to estimate the true <b>value</b> <b>function</b> by creating a compact representation of the <b>value</b> <b>function</b> that uses a smaller amount of parameters: A common practice is using deep <b>learning</b> \u2014 in that case, the weights of the neural network are the vector of weights w that will be used to estimate the <b>value</b> <b>function</b> across the entire state/<b>state-action</b> space.", "dateLastCrawled": "2022-01-30T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Q-<b>learning</b>: a <b>value</b>-based reinforcement <b>learning</b> <b>algorithm</b> | by Dhanoop ...", "url": "https://medium.com/intro-to-artificial-intelligence/q-learning-a-value-based-reinforcement-learning-algorithm-272706d835cf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/q-<b>learning</b>-a-<b>value</b>-based...", "snippet": "As we discussed in the action-<b>value</b> <b>function</b>, the above equation indicates how we compute the Q-<b>value</b> for an action a starting from state s in Q <b>learning</b>. It is the sum of immediate reward using a ...", "dateLastCrawled": "2022-01-29T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning Value Functions</b> \u2013 Ben Haanstra \u2013 Reinforcement <b>Learning</b> for ...", "url": "https://kofzor.github.io/Learning_Value_Functions/", "isFamilyFriendly": true, "displayUrl": "https://kofzor.github.io/<b>Learning_Value_Functions</b>", "snippet": "An action-<b>value</b> <b>function</b> or more commonly known as Q-<b>function</b> is a simple extension of the above that also accounts for actions. It is used to map combinations of states and actions to values. A single combination is often referred to as a <b>state-action</b> pair, and its <b>value</b> as a (policy) action-<b>value</b>. We use to denote the Q-<b>function</b> when following on , and let denote the action-<b>value</b> of a <b>state-action</b> pair . In the literature, it is common to leave out both and . The action-<b>value</b> is then ...", "dateLastCrawled": "2022-01-02T17:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Reinforcement Learning Tutorial</b> - Javatpoint", "url": "https://www.javatpoint.com/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/reinforcement-<b>learning</b>", "snippet": "It learns the <b>value</b> <b>function</b> Q (S, a), which means how good to take action &quot;a&quot; at a particular state &quot;s.&quot; The below flowchart explains the working of Q- <b>learning</b>: <b>State Action</b> Reward <b>State action</b> (SARSA): SARSA stands for <b>State Action</b> Reward <b>State action</b>, which is an on-policy temporal difference <b>learning</b> method. The on-policy control method ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "QMIX: Monotonic <b>Value</b> <b>Function</b> Factorisation for Deep Multi-Agent ...", "url": "https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/rashidicml18.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/rashidicml18.pdf", "snippet": "(VDN), which allow for centralised <b>value</b>-<b>function</b> <b>learning</b> with decentralised execution. Their <b>algorithm</b> decomposes a central <b>state-action</b> <b>value</b> <b>function</b> into a sum of individual agent terms. This corresponds to the use of a degenerate fully disconnected coordination graph. VDN does not make use of additional state information during training ...", "dateLastCrawled": "2022-01-30T04:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Q-<b>learning</b> <b>algorithm in machine learning. |reinforcement learning</b> ...", "url": "https://www.goeduhub.com/11427/learning-algorithm-machine-learning-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.goeduhub.com/11427/<b>learning</b>-<b>algorithm</b>-<b>machine</b>-<b>learning</b>-reinforcement-<b>learning</b>", "snippet": "In this way the Q-Table is been updated and the Q <b>function</b> is maximized. Q-<b>Learning</b> <b>Algorithm</b> in conclusion: Q-<b>learning</b> is a <b>value</b> based reinforcement <b>learning</b> <b>algorithm</b> meaning a off policy reinforcement <b>learning</b> <b>algorithm</b> which is used to find a optimal action selection policy using Q-<b>function</b> and Q-table where our goal is to maximize Q-<b>function</b> by iteratively updating Q-table in bellman equation. Q(<b>state, action</b>) returns the expected future reward of that action at that state. Share With ...", "dateLastCrawled": "2022-02-03T09:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Epsilon-Greedy Q-learning</b> | Baeldung on Computer Science", "url": "https://www.baeldung.com/cs/epsilon-greedy-q-learning", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>epsilon-greedy-q-learning</b>", "snippet": "This is called the action-<b>value</b> <b>function</b> or Q-<b>function</b>. The <b>function</b> approximates the <b>value</b> of selecting a certain action in a certain state. In this case, is the action-<b>value</b> <b>function</b> learned by the <b>algorithm</b>. approximates the optimal action-<b>value</b> <b>function</b> . The output of the <b>algorithm</b> is calculated values. A Q-table for states and actions looks <b>like</b> this: An easy application of Q-<b>learning</b> is pathfinding in a maze, where the possible states and actions are trivial. With Q-<b>learning</b>, we can ...", "dateLastCrawled": "2022-01-30T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What would a <b>state-action</b> <b>value</b> <b>function</b> learn if we placed multiple ...", "url": "https://www.quora.com/What-would-a-state-action-value-function-learn-if-we-placed-multiple-goals-on-the-state-space-and-moved-from-a-starting-point-to-a-goal-and-then-from-goal-to-goal-using-reinforcement-learning-with-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-would-a-<b>state-action</b>-<b>value</b>-<b>function</b>-learn-if-we-placed...", "snippet": "Answer (1 of 2): It depends on how you define the reward <b>function</b>, how far away the goal states are from each other, whether you are using discounting, whether goal states are absorbing, and what are the available actions (can you choose not to move?). If goals are not absorbing, for instance, th...", "dateLastCrawled": "2022-01-24T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Q_Learning_Simple</b> - Artificial Inteligence", "url": "https://leonardoaraujosantos.gitbook.io/artificial-inteligence/artificial_intelligence/reinforcement_learning/qlearning_simple", "isFamilyFriendly": true, "displayUrl": "https://leonardoaraujosantos.gitbook.io/.../reinforcement_<b>learning</b>/<b>qlearning_simple</b>", "snippet": "<b>Machine</b> <b>Learning</b>. Artificial Intelligence. OpenAI Gym. Tree Search. Markov Decision process. Reinforcement <b>Learning</b>. <b>Q_Learning_Simple</b>. Deep Q <b>Learning</b>. Deep Reinforcement <b>Learning</b> . Natural Language Processing. Appendix. Powered By GitBook. <b>Q_Learning_Simple</b>. Introduction. Q_<b>Learning</b> is a model free reinforcement <b>learning</b> technique. Here we are interested on finding through experiences with the environment the action-<b>value</b> <b>function</b> Q. When the Q <b>function</b> is found we can achieve optimal ...", "dateLastCrawled": "2022-01-30T06:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Ch:13: Deep Reinforcement <b>learning</b> \u2014 Deep Q-<b>learning</b> and Policy ...", "url": "https://medium.com/deep-math-machine-learning-ai/ch-13-deep-reinforcement-learning-deep-q-learning-and-policy-gradients-towards-agi-a2a0b611617e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/deep-math-<b>machine</b>-<b>learning</b>-ai/ch-13-deep-reinforcement-<b>learning</b>...", "snippet": "Q-<b>learning</b> estimates the <b>state-action</b> <b>value</b> <b>function</b>(Q_SA) for a target policy that deterministically selects the action of highest <b>value</b>. Here we have this table Q of size of SxA.", "dateLastCrawled": "2022-02-02T20:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Function</b> Approximation \u2014 <b>Machine</b> <b>Learning</b> for Scientists", "url": "https://ml-lectures.org/docs/reinforcement_learning/ml_reinforcement-learning-5.html", "isFamilyFriendly": true, "displayUrl": "https://ml-lectures.org/docs/reinforcement_<b>learning</b>/ml_reinforcement-<b>learning</b>-5.html", "snippet": "<b>Machine</b> <b>Learning</b> for Scientists. Lecture Introduction Structuring Data without Neural Networks Principle Component Analysis ... even if we could store all the values, the probability of visiting all <b>state-action</b> pairs with the above algorithms becomes increasingly unlikely, in other words most states will never be visited during training. Ideally, we should thus identify states that are \u2018<b>similar</b>\u2019, assign them \u2018<b>similar</b>\u2019 <b>value</b>, and choose \u2018<b>similar</b>\u2019 actions when in these states ...", "dateLastCrawled": "2022-01-26T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Lecture 10: Q-Learning, Function</b> Approximation, Temporal Difference ...", "url": "http://katselis.web.engr.illinois.edu/ECE586/Lecture10.pdf", "isFamilyFriendly": true, "displayUrl": "katselis.web.engr.illinois.edu/ECE586/Lecture10.pdf", "snippet": "from other <b>machine</b> <b>learning</b> paradigms are summarized below: 1 ... The basic <b>learning</b> <b>algorithm</b> in this class is Q-<b>learning</b>. The aim of Q-<b>learning</b> is to approximate the optimal action-<b>value</b> <b>function</b> Qby generating a sequence fQ^ kg k 0 of such functions. The underlying idea is that if Q^ kis \u201cclose\u201d to Qfor some k, then the corresponding greedy policy with respect to Q^ kwill be close to the optimal policy which is greedy with respect to Q. 10.1 Q-<b>function</b> and Q-<b>learning</b> The Q-<b>learning</b> ...", "dateLastCrawled": "2022-02-02T05:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is a <b>function</b> <b>value</b>?", "url": "https://philosophy-question.com/library/lecture/read/64368-what-is-a-function-value", "isFamilyFriendly": true, "displayUrl": "https://philosophy-question.com/library/lecture/read/64368-what-is-a-<b>function</b>-<b>value</b>", "snippet": "Almost all reinforcement <b>learning</b> algorithms are based on estimating <b>value</b> functions--functions of states (or of <b>state-action</b> pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).", "dateLastCrawled": "2021-11-02T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Addressing <b>Value</b> Estimation Errors in Reinforcement <b>Learning</b> with a ...", "url": "https://deepai.org/publication/addressing-value-estimation-errors-in-reinforcement-learning-with-a-state-action-return-distribution-function", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/addressing-<b>value</b>-estimation-errors-in-reinforcement...", "snippet": "In current reinforcement <b>learning</b> (RL) methods, <b>function</b> approximation errors are known to lead to the overestimated or underestimated <b>state-action</b> values Q, which further lead to suboptimal policies. We show that the <b>learning</b> of a <b>state-action</b> return distribution <b>function</b> can be used to improve the estimation accuracy of the Q-<b>value</b>. We combine the distributional return <b>function</b> within the maximum entropy RL framework in order to develop what we call the Distributional Soft Actor-Critic ...", "dateLastCrawled": "2022-01-08T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> Models - Javatpoint", "url": "https://www.javatpoint.com/machine-learning-models", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>machine</b>-<b>learning</b>-models", "snippet": "A <b>machine</b> <b>learning</b> model <b>is similar</b> to computer software designed to recognize patterns or behaviors based on previous experience or data. The <b>learning</b> <b>algorithm</b> discovers patterns within the training data, and it outputs an ML model which captures these patterns and makes predictions on new data. Let&#39;s understand an example of the ML model where we are creating an app to recognize the user&#39;s emotions based on facial expressions. So, creating such an app is possible by <b>Machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-02-02T20:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "QMIX: Monotonic <b>Value</b> <b>Function</b> Factorisation for Deep Multi-Agent ...", "url": "https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/rashidicml18.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/rashidicml18.pdf", "snippet": "(VDN), which allow for centralised <b>value</b>-<b>function</b> <b>learning</b> with decentralised execution. Their <b>algorithm</b> decomposes a central <b>state-action</b> <b>value</b> <b>function</b> into a sum of individual agent terms. This corresponds to the use of a degenerate fully disconnected coordination graph. VDN does not make use of additional state information during training ...", "dateLastCrawled": "2022-01-30T04:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "CBR for State <b>Value Function Approximation in Reinforcement Learning</b> ...", "url": "https://www.researchgate.net/publication/221203542_CBR_for_State_Value_Function_Approximation_in_Reinforcement_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221203542_CBR_for_State_<b>Value</b>_<b>Function</b>...", "snippet": "For continuous state and action spaces, the use of <b>function</b> approximators is a necessity and a commonly used type of RL algorithms for these continuous spaces is the actor-critic <b>algorithm</b>, in ...", "dateLastCrawled": "2021-10-01T06:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Reinforcement <b>Learning</b> - What&#39;s the formula for the <b>value</b> <b>function</b> ...", "url": "https://datascience.stackexchange.com/questions/25678/reinforcement-learning-whats-the-formula-for-the-value-function", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/25678", "snippet": "The usual formula that I encounter about the <b>value</b> <b>function</b> V (s) is: V ( s) = R ( s) + m a x a \u2208 A \u2211 s \u2032 \u2208 S T ( s, a, s \u2032) V ( s \u2032) where S is the set of states, A the set of actions, T the transition model. T ( s, a, s \u2032) = P ( s t + 1 = s \u2032 | s t = s, a t = a) and R the reward <b>function</b>. Since I&#39;m working on a model-based ...", "dateLastCrawled": "2022-01-20T12:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Using Q-<b>Learning</b> to solve the CartPole balancing problem | by Jose ...", "url": "https://medium.com/@flomay/using-q-learning-to-solve-the-cartpole-balancing-problem-c0a7f47d3f9d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@flomay/using-q-<b>learning</b>-to-solve-the-cartpole-balancing-problem-c0...", "snippet": "Q-<b>learning</b> is an <b>algorithm</b> that r e lies on updating its action-<b>value</b> functions. This means that with Q-<b>learning</b>, every pair of state and action have an assigned <b>value</b>. By consulting this <b>function</b> ...", "dateLastCrawled": "2022-01-29T22:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement <b>Learning</b>: Introduction to Policy Gradients | by Cheng Xi ...", "url": "https://medium.com/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/reinforcement-<b>learning</b>-introduction-to-policy...", "snippet": "In (5), we <b>can</b> replace the <b>state-action</b> <b>value</b> <b>function</b> with G\u209c, the cumulative discounted reward at timestep t. Then, we <b>can</b> simplify the equation using the fact d/dx[ln(x)] = 1/x.", "dateLastCrawled": "2022-01-28T08:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Reinforcement learning applied to airline</b> revenue management | SpringerLink", "url": "https://link.springer.com/article/10.1057/s41272-020-00228-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1057/s41272-020-00228-4", "snippet": "In RL, the optimal policy <b>can</b> be extracted from the <b>state-action</b> <b>value</b> <b>function</b> \\(Q^{*}(s,a)\\), which <b>can</b> <b>be thought</b> of as the revenue to go from state s, given the agent takes an exploratory action a, then acting following the optimal policy until termination. The DP for the <b>state-action</b> <b>function</b> and its relation to the <b>value</b> <b>function</b> is given below. Once the <b>state-action</b> <b>value</b> <b>function</b> has been determined, the optimal policy is easily determined by", "dateLastCrawled": "2022-01-22T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b>- Reinforcement <b>Learning</b>: The Q <b>Learning</b> <b>Algorithm</b> with ...", "url": "https://www.i2tutorials.com/machine-learning-tutorial/machine-learning-reinforcement-learning-the-q-learning-algorithm-with-an-illustrative-example/", "isFamilyFriendly": true, "displayUrl": "https://www.i2tutorials.com/<b>machine</b>-<b>learning</b>-tutorial/<b>machine</b>-<b>learning</b>-reinforcement...", "snippet": "The learner\u2019s hypothesis is represented in this method by a big table with a single entry for each <b>state-action</b> pair. The <b>value</b> for (s, a)-the learner\u2019s present hypothesis about the real but unknown <b>value</b> Q is stored in the database entry for the pair (s, a) (s, a). Initially, the table <b>can</b> be populated with random values (though it is easier to understand the <b>algorithm</b> if one assumes initial values of zero). The agent repeatedly observes its present state s, picks an action a, performs ...", "dateLastCrawled": "2022-01-24T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What would a <b>state-action</b> <b>value</b> <b>function</b> learn if we placed multiple ...", "url": "https://www.quora.com/What-would-a-state-action-value-function-learn-if-we-placed-multiple-goals-on-the-state-space-and-moved-from-a-starting-point-to-a-goal-and-then-from-goal-to-goal-using-reinforcement-learning-with-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-would-a-<b>state-action</b>-<b>value</b>-<b>function</b>-learn-if-we-placed...", "snippet": "Answer (1 of 2): It depends on how you define the reward <b>function</b>, how far away the goal states are from each other, whether you are using discounting, whether goal states are absorbing, and what are the available actions (<b>can</b> you choose not to move?). If goals are not absorbing, for instance, th...", "dateLastCrawled": "2022-01-24T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Part XIII Reinforcement Learning and Control</b>", "url": "http://cs229.stanford.edu/notes2020spring/cs229-notes12.pdf", "isFamilyFriendly": true, "displayUrl": "cs229.stanford.edu/notes2020spring/cs229-notes12.pdf", "snippet": "The rst <b>algorithm</b>, <b>value</b> iteration, is as follows: <b>Algorithm</b> 1 <b>Value</b> Iteration 1: For each state s, initialize V(s) := 0. 2: for until convergence do 3: For every state, update V(s) := R(s) + max a2A X s0 P sa(s0)V(s0): (4) This <b>algorithm</b> <b>can</b> <b>be thought</b> of as repeatedly trying to update the estimated <b>value</b> <b>function</b> using Bellman Equations (2).", "dateLastCrawled": "2022-02-01T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Policy Gradients: REINFORCE with Baseline | by Cheng Xi Tsou | Nerd For ...", "url": "https://medium.com/nerd-for-tech/policy-gradients-reinforce-with-baseline-6c871a3a068", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/policy-gradients-reinforce-with-baseline-6c871a3a068", "snippet": "A more complex baseline we <b>can</b> use is a state-<b>value</b> <b>function</b>. Since the <b>learning</b> for this <b>algorithm</b> is episodic, we <b>can</b> use a state-<b>value</b> <b>function</b> that leans episodically as well. G\u209c - (S ...", "dateLastCrawled": "2022-02-03T09:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>3.7 Value Functions</b>", "url": "http://www.incompleteideas.net/book/ebook/node34.html", "isFamilyFriendly": true, "displayUrl": "www.incompleteideas.net/book/ebook/node34.html", "snippet": "3.8 Optimal <b>Value</b> Functions Up: 3. The Reinforcement <b>Learning</b> Previous: 3.6 Markov Decision Processes Contents <b>3.7 Value Functions</b>. Almost all reinforcement <b>learning</b> algorithms are based on estimating <b>value</b> functions--functions of states (or of <b>state-action</b> pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of &quot;how good&quot; here is defined in terms of future rewards that <b>can</b> be expected, or, to be ...", "dateLastCrawled": "2022-02-02T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Modern <b>Deep Reinforcement Learning</b> Algorithms | DeepAI", "url": "https://deepai.org/publication/modern-deep-reinforcement-learning-algorithms", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/modern-<b>deep-reinforcement-learning</b>-<b>algorithms</b>", "snippet": "As an object of desire is a strategy, i. e. a <b>function</b> mapping agent\u2019s observations to possible actions, <b>reinforcement learning</b> is considered to be a subfiled of <b>machine</b> <b>learning</b>.But instead of <b>learning</b> from data, as it is established in classical supervised and unsupervised <b>learning</b> problems, the agent learns from experience of interacting with environment. Being more &quot;natural&quot; model of <b>learning</b>, this setting causes new challenges, peculiar only to <b>reinforcement learning</b>, such as ...", "dateLastCrawled": "2022-01-30T15:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - How to get out of &#39;sticky&#39; states? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/11693500/how-to-get-out-of-sticky-states", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/11693500", "snippet": "The agent uses the basic SARSA <b>algorithm</b> with a neural net <b>value</b> <b>function</b> approximator (as discussed in the Sutton book). For policy decisions I&#39;ve tried both e-greedy and softmax. <b>machine</b>-<b>learning</b> neural-network reinforcement-<b>learning</b>. Share. Follow edited Jun 26 &#39;21 at 21:57. desertnaut. 50k 19 19 gold badges 119 119 silver badges 148 148 bronze badges. asked Jul 27 &#39;12 at 18:21. zergylord zergylord. 4,184 4 4 gold badges 35 35 silver badges 60 60 bronze badges. 4. It sounds like the ...", "dateLastCrawled": "2022-01-15T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Can tabular Q-learning converge even if it</b> doesn&#39;t explore all state ...", "url": "https://ai.stackexchange.com/questions/21553/can-tabular-q-learning-converge-even-if-it-doesnt-explore-all-state-action-pair", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/21553/<b>can-tabular-q-learning-converge-even</b>-if...", "snippet": "<b>machine</b>-<b>learning</b> reinforcement-<b>learning</b> q-<b>learning</b> exploration-exploitation-tradeoff. Share. Improve this question . Follow edited Jun 2 &#39;20 at 15:24. nbro \u2666. 31.5k 8 8 gold badges 66 66 silver badges 131 131 bronze badges. asked Jun 1 &#39;20 at 8:40. Tfovid Tfovid. 167 4 4 bronze badges $\\endgroup$ 3 $\\begingroup$ I removed the part about the &quot;self-learned <b>algorithm</b>&quot; or &quot;self-<b>learning</b>&quot;, which are ambiguous terms, in order to avoid more discussions in this comment section. If you have a ...", "dateLastCrawled": "2022-01-18T06:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Value</b>-based Methods in Deep <b>Reinforcement Learning</b> | by Barak Or ...", "url": "https://towardsdatascience.com/value-based-methods-in-deep-reinforcement-learning-d40ca1086e1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>value</b>-based-methods-in-deep-<b>reinforcement-learning</b>-d40...", "snippet": "To promise optimal <b>value</b>: <b>state-action</b> pairs are represented discretely, and all actions are repeatedly sampled in all states. Q-<b>Learning</b> . Q <b>learning</b> in an off-policy method learns the <b>value</b> of taking action in a state and <b>learning</b> Q <b>value</b> and choosing how to act in the world. We define <b>state-action</b> <b>value</b> <b>function</b>: an expected return when starting in s, performing a, and following pi. Represented in a tabulated form. According to Q <b>learning</b>, the agent uses any policy to estimate Q that ...", "dateLastCrawled": "2022-01-29T11:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Reinforcement</b> <b>Learning</b> \u2014 The <b>Value</b> <b>Function</b> | by Jingles (Hong Jing ...", "url": "https://towardsdatascience.com/reinforcement-learning-value-function-57b04e911152", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement</b>-<b>learning</b>-<b>value</b>-<b>function</b>-57b04e911152", "snippet": "The other choice would be to place it at the bottom row. State M should have a higher significance and <b>value</b> as <b>compared</b> to state N because it results in a higher possibility of victory. Therefore, at any given state, we <b>can</b> perform the action that brings us (or the agent) closer to receiving a reward, by picking the state that yields us the highest <b>value</b>. Tic Tac Toe \u2014 Initialise the <b>Value</b> <b>Function</b>. The <b>Value</b> <b>function</b> V(s) for a tic-tac-toe game is the probability of winning for achieving ...", "dateLastCrawled": "2022-02-03T19:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Q-<b>learning</b>: a <b>value</b>-based reinforcement <b>learning</b> <b>algorithm</b> | by Dhanoop ...", "url": "https://medium.com/intro-to-artificial-intelligence/q-learning-a-value-based-reinforcement-learning-algorithm-272706d835cf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/q-<b>learning</b>-a-<b>value</b>-based...", "snippet": "As we discussed in the action-<b>value</b> <b>function</b>, the above equation indicates how we compute the Q-<b>value</b> for an action a starting from state s in Q <b>learning</b>. It is the sum of immediate reward using a ...", "dateLastCrawled": "2022-01-29T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement <b>Learning</b>: Introduction to Policy Gradients | by Cheng Xi ...", "url": "https://medium.com/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/reinforcement-<b>learning</b>-introduction-to-policy...", "snippet": "In (5), we <b>can</b> replace the <b>state-action</b> <b>value</b> <b>function</b> with G\u209c, the cumulative discounted reward at timestep t. Then, we <b>can</b> simplify the equation using the fact d/dx[ln(x)] = 1/x.", "dateLastCrawled": "2022-01-28T08:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Scaling Reward Values for Improved Deep Reinforcement <b>Learning</b>", "url": "https://emuccino.github.io/machine/learning,/reinforcement/learning/2019/02/18/scaling-reward-values-for-improved-deep-reinforcement-learning.html", "isFamilyFriendly": true, "displayUrl": "https://emuccino.github.io/<b>machine</b>/<b>learning</b>,/reinforcement/<b>learning</b>/2019/02/18/scaling...", "snippet": "Deep Reinforcement <b>Learning</b> involves using a neural network as a universal <b>function</b> approximator to learn a <b>value</b> <b>function</b> that maps <b>state-action</b> pairs to their expected future reward given a particular reward <b>function</b>. This <b>can</b> be done many different ways. For example, a Monte Carlo based <b>algorithm</b> will observe total rewards following <b>state-action</b> pairs from a complete episode to make build training data for the neural network. Alternatively, a Temporal Difference approach would use ...", "dateLastCrawled": "2022-01-09T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Batch <b>Value</b>-<b>function</b> Approximation with Only Realizability", "url": "http://proceedings.mlr.press/v139/xie21d.html", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v139/xie21d.html", "snippet": "Our <b>algorithm</b>, BVFT, breaks the hardness conjecture (albeit under a stronger notion of exploratory data) via a tournament procedure that reduces the <b>learning</b> problem to pairwise comparison, and solves the latter with the help of a <b>state-action</b>-space partition constructed from the <b>compared</b> functions. We also discuss how BVFT <b>can</b> be applied to model selection among other extensions and open problems.", "dateLastCrawled": "2021-12-26T11:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Addressing <b>Value</b> Estimation Errors in Reinforcement <b>Learning</b> with a ...", "url": "https://deepai.org/publication/addressing-value-estimation-errors-in-reinforcement-learning-with-a-state-action-return-distribution-function", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/addressing-<b>value</b>-estimation-errors-in-reinforcement...", "snippet": "The overestimations of RL were first found in Q-<b>learning</b> <b>algorithm</b> (watkins1989Q-<b>learning</b>), which is the prototype of most existing <b>value</b>-based RL algorithms (sutton2018reinforcement).For this <b>algorithm</b>, van2016double_DQN demonstrated that any kind of estimation errors <b>can</b> induce an upward bias, irrespective of whether these errors are caused by system noise, <b>function</b> approximation, or any other sources. This overestimation bias is firstly induced by the max operator over all noisy Q ...", "dateLastCrawled": "2022-01-08T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b>: Algorithms, Real-World Applications and Research ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7983091/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7983091", "snippet": "Supervised: Supervised <b>learning</b> is typically the task of <b>machine</b> <b>learning</b> to learn a <b>function</b> that maps an input to an output based on sample input-output pairs [].It uses labeled training data and a collection of training examples to infer a <b>function</b>. Supervised <b>learning</b> is carried out when certain goals are identified to be accomplished from a certain set of inputs [], i.e., a task-driven approach.The most common supervised tasks are \u201cclassification\u201d that separates the data, and ...", "dateLastCrawled": "2022-01-27T01:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Machine</b> <b>Learning</b> in Marketing: Overview, <b>Learning</b> Strategies ...", "url": "https://www.researchgate.net/publication/344000369_Machine_Learning_in_Marketing_Overview_Learning_Strategies_Applications_and_Future_Developments", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344000369_<b>Machine</b>_<b>Learning</b>_in_Marketing...", "snippet": "algorithms include the following: Q-<b>Learning</b>, <b>State-Action</b>-Reward-<b>State-Action</b> (SARSA), and Deep Q Network (DQN). RL is likely to be the form of ML that most approximates with AI .", "dateLastCrawled": "2022-02-01T20:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Reinforcement <b>Learning</b> - What&#39;s the formula for the <b>value</b> <b>function</b> ...", "url": "https://datascience.stackexchange.com/questions/25678/reinforcement-learning-whats-the-formula-for-the-value-function", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/25678", "snippet": "The usual formula that I encounter about the <b>value</b> <b>function</b> V (s) is: V ( s) = R ( s) + m a x a \u2208 A \u2211 s \u2032 \u2208 S T ( s, a, s \u2032) V ( s \u2032) where S is the set of states, A the set of actions, T the transition model. T ( s, a, s \u2032) = P ( s t + 1 = s \u2032 | s t = s, a t = a) and R the reward <b>function</b>. Since I&#39;m working on a model-based ...", "dateLastCrawled": "2022-01-20T12:10:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Relationship between state (V) and action(Q) <b>value</b> <b>function</b> in ...", "url": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and-action-q-value-function-in-reinforcement-learning-bb9a988c0127", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and...", "snippet": "<b>Value</b> <b>function</b> can be defined as the expected <b>value</b> of an agent in a certain state. There are two types of <b>value</b> functions in RL: State-<b>value</b> and action-<b>value</b>. It is important to understand the\u2026", "dateLastCrawled": "2022-02-03T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Value</b>. State-<b>value</b> <b>function</b> v_\u03c0: gives us the <b>value</b> of a state under \u03c0; Action-<b>value</b> <b>function</b> q_\u03c0: gives us the <b>value</b> of an action under \u03c0. q_\u03c0 is referred to as the Q-<b>function</b>, and the output from the <b>function</b> for any given <b>state-action</b> pair is called a Q-<b>value</b>.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Value</b>. State-<b>value</b> <b>function</b> v_\u03c0: gives us the <b>value</b> of a state under \u03c0; Action-<b>value</b> <b>function</b> q_\u03c0: gives us the <b>value</b> of an action under \u03c0. q_\u03c0 is referred to as the Q-<b>function</b>, and the output from the <b>function</b> for any given <b>state-action</b> pair is called a Q-<b>value</b>.", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AI and Reinforcement <b>Learning</b> \u2014 Machines that Learn through Experience ...", "url": "https://www.cantorsparadise.com/ai-and-reinforcement-learning-machines-that-learn-through-experience-e7eea7bb6765", "isFamilyFriendly": true, "displayUrl": "https://www.cantorsparadise.com/ai-and-reinforcement-<b>learning</b>-<b>machines</b>-that-learn...", "snippet": "To align the policy with the updated <b>value</b> <b>function</b>, the algorithm modifies the policy so it would greedily follow the <b>value</b> <b>function</b> (meaning, choosing to perform actions that has the highest <b>value</b>). The algorithm continues by generating a new episode, now under the improved policy, which, in turn, derives a more accurate <b>value</b> estimation and so on. In this process, both the policy and the <b>value</b> <b>function</b> converge to their optimal values, until sufficient accuracy is reached, or when no more ...", "dateLastCrawled": "2022-01-25T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Value</b>-<b>Function</b>-<b>Based Transfer for Reinforcement Learning</b> Using ...", "url": "https://www.researchgate.net/publication/221604435_Value-Function-Based_Transfer_for_Reinforcement_Learning_Using_Structure_Mapping", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221604435_<b>Value</b>-<b>Function</b>-Based_Transfer_for...", "snippet": "chological and computational theory about <b>analogy</b> making, ... the form of a <b>state-action</b> <b>value</b> <b>function</b>, or a q-<b>functio n</b>. A. q-<b>function</b> q: S \u00d7 A 7\u2192 R maps from <b>state-action</b> pairs to. real ...", "dateLastCrawled": "2021-10-16T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine learning for biochemical engineering: A</b> review - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1369703X21001303", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1369703X21001303", "snippet": "<b>Value</b>-based algorithms, typically represented by Q-<b>learning</b>, explicitly learn and optimise the <b>state-action</b> <b>value</b> <b>function</b> and generate the optimal policy by acting greedily with respect to it i.e. choosing the control corresponding to the maximum Q \u03c0 x, u <b>value</b> (<b>state-action</b> <b>value</b>). There are also hybrid algorithms, such as actor-critic methods, which combine policy optimisation methods and <b>value</b>-based methods. Although RL has shown success in game-based control benchmarks, such as AlphaGo", "dateLastCrawled": "2022-01-26T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Reinforcement Learning: Prediction, Control and</b> <b>Value</b> <b>Function</b> ...", "url": "https://deepai.org/publication/reinforcement-learning-prediction-control-and-value-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>reinforcement-learning-prediction-control-and</b>-<b>value</b>...", "snippet": "<b>Reinforcement Learning: Prediction, Control and Value Function Approximation</b>. With the increasing power of computers and the rapid development of self-<b>learning</b> methodologies such as <b>machine</b> <b>learning</b> and artificial intelligence, the problem of constructing an automatic Financial Trading Systems (FTFs) becomes an increasingly attractive research ...", "dateLastCrawled": "2022-01-16T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>SARSA</b> vs Q - <b>learning</b>", "url": "https://tcnguyen.github.io/reinforcement_learning/sarsa_vs_q_learning.html", "isFamilyFriendly": true, "displayUrl": "https://tcnguyen.github.io/reinforcement_<b>learning</b>/<b>sarsa</b>_vs_q_<b>learning</b>.html", "snippet": "<b>SARSA</b> will learn the optimal $\\epsilon$-greedy policy, i.e, the Q-<b>value</b> <b>function</b> will converge to a optimal Q-<b>value</b> <b>function</b> but in the space of $\\epsilon$-greedy policy only (as long as each <b>state action</b> pair will be visited infinitely). We expect that in the limit of $\\epsilon$ decaying to $0$, <b>SARSA</b> will converge to the overall optimal policy. I quote here a paragraph from", "dateLastCrawled": "2022-01-30T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Value</b>-<b>function-based transfer for reinforcement</b> <b>learning</b> using ...", "url": "https://www.academia.edu/2661041/Value_function_based_transfer_for_reinforcement_learning_using_structure_mapping", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2661041/<b>Value</b>_<b>function_based_transfer_for_reinforcement</b>...", "snippet": "Abstract Transfer <b>learning</b> concerns applying knowledge learned in one task (the source) to improve <b>learning</b> another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about <b>analogy</b> making, to . \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset ...", "dateLastCrawled": "2022-01-19T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>learning</b> and AI <b>in marketing \u2013 Connecting computing power to</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167811620300410", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167811620300410", "snippet": "<b>State-Action</b>-Reward-<b>State-Action</b>: 2.2.3: SVD: Singular <b>Value</b> Decomposition: 2.2.2: SVM: Support Vector <b>Machine</b> : 2.2.1: TD: Temporal-Difference: 2.2.3: UGC: User-Generated Content: 3.1: Table 3. Strengths and weaknesses of <b>machine</b> <b>learning</b> methods. Strength \u2022 Ability to handle unstructured data and data of hybrid formats \u2022 Ability to handle large data volume \u2022 Flexible model structure \u2022 Strong predictive performance. Weakness \u2022 Not easy to interpret \u2022 Relationship typically ...", "dateLastCrawled": "2022-01-12T18:25:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(state-action value function)  is like +(machine learning algorithm)", "+(state-action value function) is similar to +(machine learning algorithm)", "+(state-action value function) can be thought of as +(machine learning algorithm)", "+(state-action value function) can be compared to +(machine learning algorithm)", "machine learning +(state-action value function AND analogy)", "machine learning +(\"state-action value function is like\")", "machine learning +(\"state-action value function is similar\")", "machine learning +(\"just as state-action value function\")", "machine learning +(\"state-action value function can be thought of as\")", "machine learning +(\"state-action value function can be compared to\")"]}