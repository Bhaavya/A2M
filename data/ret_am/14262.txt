{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>STEP: Sequence-to-Sequence Transformer Pre-training for</b> Document ...", "url": "https://deepai.org/publication/step-sequence-to-sequence-transformer-pre-training-for-document-summarization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>step-sequence-to-sequence-transformer-pre-training-for</b>...", "snippet": "Based on the above observations, we propose Step (as shorthand for <b>S equence-to-Sequence</b> T ransform E r P re-training), which can be pre-trained on large scale unlabeled documents. Specifically, we design three tasks for seq2seq model pre-training, namely Sentence Reordering (SR), Next Sentence Generation (NSG), and Masked Document Generation (MDG). SR learns to recover a document with randomly shuffled sentences.", "dateLastCrawled": "2022-01-17T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "BART: Denoising <b>Sequence-to-Sequence</b> Pre-training for Natural Language ...", "url": "https://www.arxiv-vanity.com/papers/1910.13461/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1910.13461", "snippet": "We experiment with (1) treating the <b>task</b> as a standard <b>sequence-to-sequence</b> problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as prefix to the target in the decoder, with a loss only on the target part of the sequence. We find the former works better for BART models, and the latter for other models.", "dateLastCrawled": "2022-01-28T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Improving <b>Sequence-to-Sequence</b> Pre-training via Sequence Span Rewriting ...", "url": "https://www.readkong.com/page/improving-sequence-to-sequence-pre-training-via-sequence-3829690", "isFamilyFriendly": true, "displayUrl": "https://www.<b>read</b>kong.com/page/improving-<b>sequence-to-sequence</b>-pre-training-via-sequence...", "snippet": "Specifically, self-supervised training with the sequence span \u2022 First, the <b>task</b> of sequence span rewriting rewriting objective involves three steps: (1) text is closer to the downstream sequence trans- span masking (2) text infilling, and (3) sequence duction tasks since there exists references in span rewriting.", "dateLastCrawled": "2022-01-08T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sequencing events in reading and writing</b>: A Complete Guide for Students ...", "url": "https://literacyideas.com/teaching-sequencing-in-english/", "isFamilyFriendly": true, "displayUrl": "https://literacyideas.com/<b>teaching</b>-sequencing-in-english", "snippet": "Telling It <b>Like</b> It Was. The preparation for this activity works well as a homework as it gives students time to rehearse. However, it also works well after any reading activity to assess a student\u2019s understanding of the sequence of events and their overall comprehension of what they have <b>read</b>.", "dateLastCrawled": "2022-02-03T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-learning architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "10 Printable <b>Sequencing Worksheets</b> [Free]- EduWorksheets", "url": "https://eduworksheets.com/sequencing/", "isFamilyFriendly": true, "displayUrl": "https://eduworksheets.com/sequencing", "snippet": "Sequencing is an essential skill that can help students understand whatever text or story they <b>read</b>. Sequencing is the ability to identify the parts of a story <b>like</b> the beginning, the middle, and the end \u2013 and the ability to retell the story\u2019s events in the order by which they took place. This skill of sequencing events is an essential strategy for comprehension, especially with texts that involve narratives. Students can easily find the meaning in texts if they have the ability to ...", "dateLastCrawled": "2022-02-02T21:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to easily do Handwriting Recognition using Deep Learning", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "Scan, Attend and <b>Read</b>. In this seminal work Scan, Attend and <b>Read</b>(SAR) the authors propose the usage of an attention-based model for end-to-end handwriting recognition. The main contribution of the research is the automatic transcription of text without segmenting into lines as a pre-processing step and thus can scan an entire page and give ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How should I <b>read</b> a deep learning paper? - Quora", "url": "https://www.quora.com/How-should-I-read-a-deep-learning-paper", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-should-I-<b>read</b>-a-deep-learning-paper", "snippet": "Answer: The following is a set of questions you should ask yourself if you are starting <b>to read</b> a Deep Learning paper : 1. Is it the first paper trying to solve a particular problem [examples of particular problems say: sentiment analysis, object detection] you are reading ? [Yes/No] 2. Do you l...", "dateLastCrawled": "2022-01-19T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "BARThez: <b>a Skilled Pretrained French Sequence-to-Sequence</b> Model - arXiv.org", "url": "https://www.readkong.com/page/barthez-a-skilled-pretrained-french-sequence-to-sequence-5482702", "isFamilyFriendly": true, "displayUrl": "https://www.<b>read</b>kong.com/page/barthez-<b>a-skilled-pretrained-french-sequence-to-sequence</b>...", "snippet": "Page topic: &quot;BARThez: <b>a Skilled Pretrained French Sequence-to-Sequence</b> Model - arXiv.org&quot;. Created by: Bradley Salinas. Language: english.", "dateLastCrawled": "2021-09-18T03:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Working at Alef Education</b> | <b>Glassdoor</b>", "url": "https://www.glassdoor.com/Overview/Working-at-Alef-Education-EI_IE2190194.11,25.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.glassdoor.com</b>/Overview/<b>Working-at-Alef-Education</b>-EI_IE2190194.11,25.htm", "snippet": "Interview. First stage was a technical <b>task</b> without any introductory interview or anything. This one is pretty straightforward. A simple NLP problem, where you have to classify an answer to a question into a set of four categories (correct, incorrect, partially correct, contradictory or something <b>like</b> this).", "dateLastCrawled": "2022-01-30T22:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>STEP: Sequence-to-Sequence Transformer Pre-training for</b> Document ...", "url": "https://deepai.org/publication/step-sequence-to-sequence-transformer-pre-training-for-document-summarization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>step-sequence-to-sequence-transformer-pre-training-for</b>...", "snippet": "This <b>task</b> is usually framed as a <b>sequence-to-sequence</b> learning problem Nallapati et al. ; See et al. . In this paper, we adopt the <b>sequence-to-sequence</b> (seq2seq) Transformer Vaswani et al. , which has been demonstrated to be the state-of-the-art for seq2seq modeling Vaswani et al. ; Ott et al. .", "dateLastCrawled": "2022-01-17T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Effective <b>Sequence-to-Sequence</b> Dialogue State Tracking | DeepAI", "url": "https://deepai.org/publication/effective-sequence-to-sequence-dialogue-state-tracking", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/effective-<b>sequence-to-sequence</b>-dialogue-state-tracking", "snippet": "Effective <b>Sequence-to-Sequence</b> Dialogue State Tracking. <b>Sequence-to-sequence</b> models have been applied to a wide variety of NLP tasks, but how to properly use them for dialogue state tracking has not been systematically investigated. In this paper, we study this problem from the perspectives of pre-training objectives as well as the formats of ...", "dateLastCrawled": "2021-12-25T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Teaching</b>-learning cycle: reading and writing connections", "url": "https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/teachingpraccycle.aspx", "isFamilyFriendly": true, "displayUrl": "https://www.education.vic.gov.au/school/teachers/<b>teaching</b>resources/discipline/english/...", "snippet": "The <b>teaching</b> and learning cycle (TLC) involves four key stages which incorporate social support for reading, writing and speaking and listening through varied interactional routines (whole group, small group, pair, individual) to scaffold students\u2019 learning about language and meaning in a variety of texts. These stages are: Building the context or field - understanding the role of texts in our culture and building shared understanding of the topic Modelling the text (or deconstruction ...", "dateLastCrawled": "2022-01-30T19:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "BART: Denoising <b>Sequence-to-Sequence</b> Pre-training for Natural Language ...", "url": "https://www.arxiv-vanity.com/papers/1910.13461/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1910.13461", "snippet": "We experiment with (1) treating the <b>task</b> as a standard <b>sequence-to-sequence</b> problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as prefix to the target in the decoder, with a loss only on the target part of the sequence. We find the former works better for BART models, and the latter for other models.", "dateLastCrawled": "2022-01-28T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Effective <b>Sequence-to-Sequence</b> Dialogue State Tracking", "url": "https://www.researchgate.net/publication/354268639_Effective_Sequence-to-Sequence_Dialogue_State_Tracking", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354268639_Effective_<b>Sequence-to-Sequence</b>...", "snippet": "PDF | <b>Sequence-to-sequence</b> models have been applied to a wide variety of NLP tasks, but how to properly use them for dialogue state tracking has not... | Find, <b>read</b> and cite all the research you ...", "dateLastCrawled": "2021-12-25T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Improving <b>Sequence-to-Sequence</b> Pre-training via Sequence Span Rewriting ...", "url": "https://www.readkong.com/page/improving-sequence-to-sequence-pre-training-via-sequence-3829690", "isFamilyFriendly": true, "displayUrl": "https://www.<b>read</b>kong.com/page/improving-<b>sequence-to-sequence</b>-pre-training-via-sequence...", "snippet": "Specifically, self-supervised training with the sequence span \u2022 First, the <b>task</b> of sequence span rewriting rewriting objective involves three steps: (1) text is closer to the downstream sequence trans- span masking (2) text infilling, and (3) sequence duction tasks since there exists references in span rewriting.", "dateLastCrawled": "2022-01-08T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - mayli10/<b>deep-learning</b>-<b>chatbot</b>: A deep-dive beginner&#39;s walk ...", "url": "https://github.com/mayli10/deep-learning-chatbot", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/mayli10/<b>deep-learning</b>-<b>chatbot</b>", "snippet": "A deep-dive beginner&#39;s walk-through of sentdex&#39;s tutorial for how to build a <b>chatbot</b> with <b>deep learning</b>, Tensorflow, and an NMT <b>sequence-to-sequence</b> model - <b>GitHub</b> - mayli10/<b>deep-learning</b>-<b>chatbot</b>: A deep-dive beginner&#39;s walk-through of sentdex&#39;s tutorial for how to build a <b>chatbot</b> with <b>deep learning</b>, Tensorflow, and an NMT <b>sequence-to-sequence</b> model", "dateLastCrawled": "2022-01-31T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "A common <b>task</b> in NLP and bioinformatics is finding <b>similar</b> strings and sequences. In bioinformatics, the most common algorithm for sequence searching is BLAST. Another option is Locality-Sensitive Hashing (LSH), a popular method for indexing and finding texts at scale. In brief, a document is represented as a BoW vector. The vector is then hashed multiple times using a hash function that encourages \u201ccollisions&#39;&#39; between <b>similar</b> document vectors. Retrieval of documents from a hash bucket ...", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to easily do Handwriting Recognition using Deep Learning", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "Scan, Attend and <b>Read</b>. In this seminal work Scan, Attend and <b>Read</b>(SAR) the authors propose the usage of an attention-based model for end-to-end handwriting recognition. The main contribution of the research is the automatic transcription of text without segmenting into lines as a pre-processing step and thus can scan an entire page and give ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What is transductive learning</b>? - Quora", "url": "https://www.quora.com/What-is-transductive-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-transductive-learning</b>", "snippet": "Answer (1 of 2): The idea behind transductive learning is not to construct a function to learn from a training set - labels for any point from a test set (as in inductive learning). In transductive learning you know the test set beforehand.Then you just pass the information in the training set on...", "dateLastCrawled": "2022-01-19T22:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>STEP: Sequence-to-Sequence Transformer Pre-training for</b> Document ...", "url": "https://deepai.org/publication/step-sequence-to-sequence-transformer-pre-training-for-document-summarization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>step-sequence-to-sequence-transformer-pre-training-for</b>...", "snippet": "We, therefore, propose STEP (as shorthand for <b>Sequence-to-Sequence</b> Transformer Pre-training), which <b>can</b> be trained on large scale unlabeled documents. Specifically, STEP is pre-trained using three different tasks, namely sentence reordering, next sentence generation, and masked document generation. Experiments on two summarization datasets show that all three tasks <b>can</b> improve performance upon a heavily tuned large Seq2Seq Transformer which already includes a strong pre-trained encoder by a ...", "dateLastCrawled": "2022-01-17T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequencing events in reading and writing</b>: A Complete Guide for Students ...", "url": "https://literacyideas.com/teaching-sequencing-in-english/", "isFamilyFriendly": true, "displayUrl": "https://literacyideas.com/<b>teaching</b>-sequencing-in-english", "snippet": "Sequencing is an essential reading skill that students must develop if they are to fully understand all reading material. Luckily, sequencing comes naturally to most children as the concept of chronological order is reinforced from very early on through the practice of the routines of daily life.", "dateLastCrawled": "2022-02-03T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "BART: Denoising <b>Sequence-to-Sequence</b> Pre-training for Natural Language ...", "url": "https://www.researchgate.net/publication/343301801_BART_Denoising_Sequence-to-Sequence_Pre-training_for_Natural_Language_Generation_Translation_and_Comprehension", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/343301801_BART_Denoising_<b>Sequence-to-Sequence</b>...", "snippet": "Experimental results on SegNews demonstrate that our model <b>can</b> outperform several state-of-the-art <b>sequence-to-sequence</b> generation models for this new <b>task</b>. View Show abstract", "dateLastCrawled": "2021-11-30T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "TopNet: Learning from Neural Topic Model to Generate Long Stories | DeepAI", "url": "https://deepai.org/publication/topnet-learning-from-neural-topic-model-to-generate-long-stories", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/topnet-learning-from-neural-topic-model-to-generate...", "snippet": "This <b>task</b> <b>can</b> <b>be thought</b> of the reverse of the text summarization <b>task</b>, and aims to expand the summary text by adding more details. We evaluate our model on the CNN/DailyMail (Hermann et al., 2015) dataset. The experimental results demonstrate that our generated stories perform much better in relevance, fluency and diversity than competitive baselines. Moreover, we show that our framework <b>can</b> not only augment the performance of normal size neural networks, but also has improvement on large ...", "dateLastCrawled": "2022-01-16T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "10 Printable <b>Sequencing Worksheets</b> [Free]- EduWorksheets", "url": "https://eduworksheets.com/sequencing/", "isFamilyFriendly": true, "displayUrl": "https://eduworksheets.com/sequencing", "snippet": "Sequencing is an essential skill that <b>can</b> help students understand whatever text or story they <b>read</b>. Sequencing is the ability to identify the parts of a story like the beginning, the middle, and the end \u2013 and the ability to retell the story\u2019s events in the order by which they took place. This skill of sequencing events is an essential strategy for comprehension, especially with texts that involve narratives. Students <b>can</b> easily find the meaning in texts if they have the ability to ...", "dateLastCrawled": "2022-02-02T21:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sequence-to-Sequence</b>-101/Google-10000-English.txt at master \u00b7 zake7749 ...", "url": "https://github.com/zake7749/Sequence-to-Sequence-101/blob/master/Epoch1-BasicSeq2Seq/dataset/Google-10000-English.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zake7749/<b>Sequence-to-Sequence</b>-101/blob/master/Epoch1-BasicSeq2Seq/...", "snippet": "Cannot retrieve contributors at this time. 9914 lines (9914 sloc) 73.6 KB Raw Blame", "dateLastCrawled": "2021-08-21T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "US20180052664A1 - Method and system for developing, training, and ...", "url": "https://patents.google.com/patent/US20180052664A1/en", "isFamilyFriendly": true, "displayUrl": "https://patents.google.com/patent/US20180052664", "snippet": "The present <b>teaching</b> relates to developing a virtual agent. In one example, a plurality of graphical objects is presented to a user via a bot design programming interface. Each of the plurality of graphical objects represents a module corresponding to an action to be performed by the virtual agent. One or more inputs from the user are received, via the bot design programming interface, for selecting a set of graphical objects from the plurality of graphical objects. The one or more inputs ...", "dateLastCrawled": "2021-12-12T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - mayli10/<b>deep-learning</b>-<b>chatbot</b>: A deep-dive beginner&#39;s walk ...", "url": "https://github.com/mayli10/deep-learning-chatbot", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/mayli10/<b>deep-learning</b>-<b>chatbot</b>", "snippet": "A deep-dive beginner&#39;s walk-through of sentdex&#39;s tutorial for how to build a <b>chatbot</b> with <b>deep learning</b>, Tensorflow, and an NMT <b>sequence-to-sequence</b> model - <b>GitHub</b> - mayli10/<b>deep-learning</b>-<b>chatbot</b>: A deep-dive beginner&#39;s walk-through of sentdex&#39;s tutorial for how to build a <b>chatbot</b> with <b>deep learning</b>, Tensorflow, and an NMT <b>sequence-to-sequence</b> model", "dateLastCrawled": "2022-01-31T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Artificial Intelligence A Modern Approach</b> (4th Edition) | Fahim ...", "url": "https://www.academia.edu/45126798/Artificial_Intelligence_A_Modern_Approach_4th_Edition_", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/45126798/<b>Artificial_Intelligence_A_Modern_Approach</b>_4th_Edition_", "snippet": "Artificial Intelligence (AI) is a big field, and this is a big book. We have tried to explore the full breadth of the field, which encompasses logic, probability, and continuous mathematics; perception, reasoning, learning, and action; fairness,", "dateLastCrawled": "2022-02-02T22:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What is transductive learning</b>? - Quora", "url": "https://www.quora.com/What-is-transductive-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-transductive-learning</b>", "snippet": "Answer (1 of 2): The idea behind transductive learning is not to construct a function to learn from a training set - labels for any point from a test set (as in inductive learning). In transductive learning you know the test set beforehand.Then you just pass the information in the training set on...", "dateLastCrawled": "2022-01-19T22:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Improving <b>Sequence-to-Sequence</b> Pre-training via Sequence Span Rewriting ...", "url": "https://www.readkong.com/page/improving-sequence-to-sequence-pre-training-via-sequence-3829690", "isFamilyFriendly": true, "displayUrl": "https://www.<b>read</b>kong.com/page/improving-<b>sequence-to-sequence</b>-pre-training-via-sequence...", "snippet": "Specifically, self-supervised training with the sequence span \u2022 First, the <b>task</b> of sequence span rewriting rewriting objective involves three steps: (1) text is closer to the downstream sequence trans- span masking (2) text infilling, and (3) sequence duction tasks since there exists references in span rewriting.", "dateLastCrawled": "2022-01-08T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>STEP: Sequence-to-Sequence Transformer Pre-training for</b> Document ...", "url": "https://deepai.org/publication/step-sequence-to-sequence-transformer-pre-training-for-document-summarization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>step-sequence-to-sequence-transformer-pre-training-for</b>...", "snippet": "We evaluate our methods on two summarization datasets (i.e., the CNN/DailyMail and the New York Times datasets). Experiments show that all three tasks we propose <b>can</b> improve upon a heavily tuned large seq2seq Transformer which already includes a strong pre-trained encoder by a large margin. <b>Compared</b> to the best published abstractive models, Step improves the ROUGE-2 by 0.8 on the CNN/DailyMail dataset and by 2.4 on the New York Times dataset using our best performing <b>task</b> for pre-training ...", "dateLastCrawled": "2022-01-17T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "On Sparsifying Encoder Outputs in <b>Sequence-to-Sequence</b> Models - deepai.org", "url": "https://deepai.org/publication/on-sparsifying-encoder-outputs-in-sequence-to-sequence-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-sparsifying-encoder-outputs-in-<b>sequence-to-sequence</b>...", "snippet": "04/24/20 - <b>Sequence-to-sequence</b> models usually transfer all encoder outputs to the decoder for generation. In this work, by contrast, we hypo...", "dateLastCrawled": "2021-11-26T08:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Deep Learning&#39;s Most Important Ideas</b> - A Brief Historical Review", "url": "https://dennybritz.com/blog/deep-learning-most-important-ideas/", "isFamilyFriendly": true, "displayUrl": "https://dennybritz.com/blog/deep-learning-most-important-ideas", "snippet": "<b>Sequence-to-Sequence</b> models with attention (described earlier in this post) worked quite well, but they had a few drawbacks due to their recurrent nature that required sequential computation. They were difficult to parallelize because they processed the input one step at a time. Each time step depends on the previous one. This also made it difficult to scale them to very long sequences. Even with their attention mechanism, they still struggled with modeling complex long-range dependencies ...", "dateLastCrawled": "2022-01-31T19:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "BARThez: <b>a Skilled Pretrained French Sequence-to-Sequence</b> Model - arXiv.org", "url": "https://www.readkong.com/page/barthez-a-skilled-pretrained-french-sequence-to-sequence-5482702", "isFamilyFriendly": true, "displayUrl": "https://www.<b>read</b>kong.com/page/barthez-<b>a-skilled-pretrained-french-sequence-to-sequence</b>...", "snippet": "Page topic: &quot;BARThez: <b>a Skilled Pretrained French Sequence-to-Sequence</b> Model - arXiv.org&quot;. Created by: Bradley Salinas. Language: english.", "dateLastCrawled": "2021-09-18T03:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sequence-to-Sequence</b>-101/Google-10000-English.txt at master \u00b7 zake7749 ...", "url": "https://github.com/zake7749/Sequence-to-Sequence-101/blob/master/Epoch1-BasicSeq2Seq/dataset/Google-10000-English.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zake7749/<b>Sequence-to-Sequence</b>-101/blob/master/Epoch1-BasicSeq2Seq/...", "snippet": "Cannot retrieve contributors at this time. 9914 lines (9914 sloc) 73.6 KB Raw Blame", "dateLastCrawled": "2021-08-21T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Teaching</b>-learning cycle: reading and writing connections", "url": "https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/teachingpraccycle.aspx", "isFamilyFriendly": true, "displayUrl": "https://www.education.vic.gov.au/school/teachers/<b>teaching</b>resources/discipline/english/...", "snippet": "are typically beyond what students <b>can</b> <b>read</b> independently ; are related to the area of study ; provide models of good writing in the focus genre and ; provide clear illustrations of available grammatical choices and how these choices shape the meanings of the texts. Dependent on the year level, the selected text and the <b>teaching</b> focus, whole texts or text extracts <b>can</b> be used. For example, in a focus on narrative texts, a complete narrative might be used to illustrate the main stages of ...", "dateLastCrawled": "2022-01-30T19:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to easily do Handwriting Recognition using Deep Learning", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "Handwriting Text Generation is the <b>task</b> of generating real looking handwritten text and thus <b>can</b> be used to augment the existing datasets. As we know deep learning requires a lot of data to train while obtaining huge corpus of labelled handwriting images for different languages is a cumbersome <b>task</b>. To solve this we <b>can</b> use Generative Adversarial Networks to generate training data. Let&#39;s discuss one such architecture here", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Most importantly, we <b>can</b> <b>read</b> and understand human languages. Additionally, unlike proteins, most human languages include uniform punctuation and stop words, with clearly separable structures such as words, sentences and paragraphs. With proteins, we do not always know whether a sequence of amino-acids is part of a functional unit (e.g. a domain). There is no clear analogy between the building blocks of language and those of proteins. For example, considering protein domains as being ...", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Task oriented</b>, <b>task-oriented</b> individual", "url": "https://stratili-holgy.biz/articles/jobs_for_task-oriented_people60us31123jrm8.html", "isFamilyFriendly": true, "displayUrl": "https://stratili-holgy.biz/articles/jobs_for_<b>task-oriented</b>_people60us31123jrm8.html", "snippet": "It is based on a simple and practical yet very effective <b>sequence-to-sequence</b> approach, ... embedded withing <b>a child</b> and family centered practice, provides a principled approach to assessing and training tasks that the <b>child</b> and family wish to improve or master <b>Task-oriented</b> leaders have several characteristics that help make sure that things get done in a manner that is both proficient and on time every time. These managers usually create clear, easy-to-follow work schedules with. <b>Task</b> ...", "dateLastCrawled": "2021-12-08T15:31:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Read the model framework <b>Encoder-Decoder and Seq2Seq</b> in NLP - easyAI", "url": "https://easyai.tech/en/ai-definition/encoder-decoder-seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://easyai.tech/en/ai-definition/encoder-decoder-seq2seq", "snippet": "Encoder-Decoder This framework is a good illustration of the core ideas of <b>machine</b> <b>learning</b>: ... Seq2Seq (short for <b>Sequence-to-sequence</b>), as literally, enters a sequence and outputs another sequence. The most important aspect of this structure is that the length of the input sequence and the output sequence are variable. For example, the following picture: As shown above: 6 Chinese characters are input, and 3 English words are output. The length of the input and output are different. The ...", "dateLastCrawled": "2022-01-31T17:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Andrew-NG-Notes/andrewng-p-5-sequence-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence...", "snippet": "<b>Machine</b> translation (<b>sequence to sequence</b>): X: text sequence (in one language) Y: text sequence (in other language) Video activity recognition (sequence to one): X: video frames ; Y: label (activity) Name entity recognition (<b>sequence to sequence</b>): X: text sequence; Y: label sequence; Can be used by seach engines to index different type of words inside a text. All of these problems with different input and output (sequence or not) can be addressed as supervised <b>learning</b> with label data X, Y ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(teaching a child to read)", "+(sequence-to-sequence task) is similar to +(teaching a child to read)", "+(sequence-to-sequence task) can be thought of as +(teaching a child to read)", "+(sequence-to-sequence task) can be compared to +(teaching a child to read)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}