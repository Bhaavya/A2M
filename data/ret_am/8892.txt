{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Validity, reliability, and generalizability in qualitative research", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4535087/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4535087", "snippet": "<b>Like</b> quantitative research, the qualitative research aims to seek answers for questions of \u201chow, where, when who and why\u201d with a perspective to build a theory or refute an existing theory. Unlike quantitative research which deals primarily with numerical data and their statistical interpretations under a reductionist, logical and strictly objective paradigm, qualitative research handles nonnumerical information and their phenomenological interpretation, which inextricably tie in with ...", "dateLastCrawled": "2022-02-03T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A high-bias, low-variance introduction to Machine Learning for physicists", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6688775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6688775", "snippet": "Even using a small <b>training</b> set with N train = 10 <b>examples</b>, we find that the <b>model</b> class that generated the data also provides the best fit and the most accurate out-of-sample predictions. That is, the linear <b>model</b> performs the best for data generated from a linear polynomial (the third and tenth order polynomials perform similarly), and the tenth order <b>model</b> performs the best for data generated from a tenth order polynomial. While this may be expected, the results are quite different for ...", "dateLastCrawled": "2022-02-02T11:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Generalization</b> of the Discrete Brier and Ranked Probability Skill ...", "url": "https://www.researchgate.net/publication/249621608_Generalization_of_the_Discrete_Brier_and_Ranked_Probability_Skill_Scores_for_Weighted_Multimodel_Ensemble_Forecasts", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/249621608_<b>Generalization</b>_of_the_Discrete...", "snippet": "The <b>model</b> consists of several modules to develop the probabilistic forecast. Initially, a clear sky <b>model</b> is tuned to incorporate the system and temperature losses and partial shading. The ...", "dateLastCrawled": "2021-12-04T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Exploring Generalization in Deep Learning</b> | Request PDF", "url": "https://www.researchgate.net/publication/317955051_Exploring_Generalization_in_Deep_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/317955051_Exploring_<b>Generalization</b>_in_Deep...", "snippet": "<b>Model</b>-centric AutoEval, also known as <b>model</b> <b>generalization</b> [4,10, 32], predicts the <b>generalization</b> gap (the gap between the <b>training</b> and testing <b>accuracy</b>) of various models. Some works [19,20] aim ...", "dateLastCrawled": "2022-01-25T18:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Chapter 5 <b>Gaussian Process Regression</b> | Surrogates", "url": "https://bookdown.org/rbg/surrogates/chap5.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/rbg/surrogates/chap5.html", "snippet": "Chapter 5 <b>Gaussian Process Regression</b>. Here the goal is humble on theoretical fronts, but fundamental in application. Our aim is to understand the <b>Gaussian process</b> (GP) as a prior over random functions, a posterior over functions given observed data, as a tool for spatial data modeling and surrogate modeling for computer experiments, and simply as a flexible nonparametric regression.", "dateLastCrawled": "2022-02-02T01:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A multilevel scenario based predictive analytics framework to <b>model</b> the ...", "url": "https://www.nature.com/articles/s41598-021-96801-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-96801-x", "snippet": "Using the <b>training</b> data which is a known set of data points, a <b>model</b> is trained to estimate f and using an unknown set of data points known as test data, the performance <b>of the model</b> is evaluated ...", "dateLastCrawled": "2022-02-03T15:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "MultiQA: <b>An Empirical Investigation of Generalization and Transfer</b> in ...", "url": "https://deepai.org/publication/multiqa-an-empirical-investigation-of-generalization-and-transfer-in-reading-comprehension", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/multiqa-<b>an-empirical-investigation-of-generalization</b>...", "snippet": "So far we trained on datasets with 75K <b>examples</b>. To examine <b>generalization</b> as the <b>training</b> set size <b>increases</b>, we evaluate performance <b>as the number</b> of <b>examples</b> from the five large datasets grows. Table 3 shows that <b>generalization</b> <b>improves</b> by 26% on average when increasing the <b>number</b> of <b>examples</b> from 37K to 375K.", "dateLastCrawled": "2021-12-31T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "MultiQA: An Empirical Investigation of <b>Generalization</b> and Transfer in ...", "url": "https://papertohtml.org/paper?id=636904d91d9dd1a641a595d9578ba7640f35aa74", "isFamilyFriendly": true, "displayUrl": "https://papertohtml.org/paper?id=636904d91d9dd1a641a595d9578ba7640f35aa74", "snippet": "The green <b>curve</b> is <b>training</b> on a fixed <b>number</b> of <b>examples</b> from all 5 large datasets without fine-tuning (MULTIQA). In 5 out of 10 curves, pre-<b>training</b> <b>improves</b> performance even given access to all 75K <b>examples</b> from the target dataset. In the other 5, using only the target dataset is better after 30-50K <b>examples</b>. To estimate the savings in ...", "dateLastCrawled": "2021-12-31T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>DropConnect is effective in modeling uncertainty of Bayesian deep</b> ...", "url": "https://www.nature.com/articles/s41598-021-84854-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-84854-x", "snippet": "Due to the high <b>model</b> complexity, DNNs require a huge amount of data to regularize <b>training</b> and prevent the networks from over-fitting the <b>training</b> <b>examples</b>. This reduces their applicability in ...", "dateLastCrawled": "2022-02-02T06:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machine<b>curve</b>.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "The <b>training</b> data is fed into the machine learning <b>model</b> in what is called the forward pass. The origin of this name is really easy: the data is simply fed to the network, which means that it passes through it in a forward fashion. The end result is a set of predictions, one per sample. This means that when my <b>training</b> set consists of 1000 feature vectors (or rows with features) that are accompanied by 1000 targets, I will have 1000 predictions after my forward pass.", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Validity, reliability, and generalizability in qualitative research", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4535087/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4535087", "snippet": "Validity. Validity in qualitative research means \u201cappropriateness\u201d of the tools, processes, and data. Whether the research question is valid for the desired outcome, the choice of methodology is appropriate for answering the research question, the design is valid for the methodology, the sampling and data analysis is appropriate, and finally the results and conclusions are valid for the sample and context.", "dateLastCrawled": "2022-02-03T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Similarity and <b>Generalization</b>: From Noise to Corruption", "url": "https://www.researchgate.net/publication/358261816_Similarity_and_Generalization_From_Noise_to_Corruption", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/358261816_<b>Similar</b>ity_and_<b>Generalization</b>_From...", "snippet": "Left: <b>Examples</b> of input configurations among <b>similar</b> (first 3 lines) and different pairs (last two lines) in the presence of PLN. Center: configuration after <b>similar</b> pair collapse. If this can not ...", "dateLastCrawled": "2022-02-03T19:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Generalization and Generalizability Measures</b> | Benjamin Wah ...", "url": "https://www.academia.edu/2421929/Generalization_and_Generalizability_Measures", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2421929/<b>Generalization_and_Generalizability_Measures</b>", "snippet": "Defines the <b>generalization</b> problem, summarizes various approaches in <b>generalization</b>, identifies the credit assignment problem, and presents the problem and some solutions in measuring generalizability. We discuss anomalies in the ordering of", "dateLastCrawled": "2021-07-19T02:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "MultiQA: An Empirical Investigation of <b>Generalization</b> and Transfer in ...", "url": "https://papertohtml.org/paper?id=636904d91d9dd1a641a595d9578ba7640f35aa74", "isFamilyFriendly": true, "displayUrl": "https://papertohtml.org/paper?id=636904d91d9dd1a641a595d9578ba7640f35aa74", "snippet": "The green <b>curve</b> is <b>training</b> on a fixed <b>number</b> of <b>examples</b> from all 5 large datasets without fine-tuning (MULTIQA). In 5 out of 10 curves, pre-<b>training</b> <b>improves</b> performance even given access to all 75K <b>examples</b> from the target dataset. In the other 5, using only the target dataset is better after 30-50K <b>examples</b>. To estimate the savings in ...", "dateLastCrawled": "2021-12-31T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "MultiQA: <b>An Empirical Investigation of Generalization and Transfer</b> in ...", "url": "https://deepai.org/publication/multiqa-an-empirical-investigation-of-generalization-and-transfer-in-reading-comprehension", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/multiqa-<b>an-empirical-investigation-of-generalization</b>...", "snippet": "So far we trained on datasets with 75K <b>examples</b>. To examine <b>generalization</b> as the <b>training</b> set size <b>increases</b>, we evaluate performance <b>as the number</b> of <b>examples</b> from the five large datasets grows. Table 3 shows that <b>generalization</b> <b>improves</b> by 26% on average when increasing the <b>number</b> of <b>examples</b> from 37K to 375K.", "dateLastCrawled": "2021-12-31T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Deformable Model</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/deformable-model", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>deformable-model</b>", "snippet": "(4) When the <b>number</b> <b>of training</b> data are huge (e.g., thousands), it is infeasible to simply stack all shapes into the data matrix since Sparse Shape Composition could not handle them efficiently. In this case, a dictionary learning technique is employed to learn a compact dictionary, whose size is much smaller than the whole dataset. This compact dictionary highly <b>improves</b> the computational efficiency without sacrificing segmentation <b>accuracy</b>. (5) When the shape data contain thousands of ...", "dateLastCrawled": "2022-01-26T03:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machine<b>curve</b>.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "The <b>training</b> data is fed into the machine learning <b>model</b> in what is called the forward pass. The origin of this name is really easy: the data is simply fed to the network, which means that it passes through it in a forward fashion. The end result is a set of predictions, one per sample. This means that when my <b>training</b> set consists of 1000 feature vectors (or rows with features) that are accompanied by 1000 targets, I will have 1000 predictions after my forward pass.", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "An <b>Tutorial for Regularized Multi-task Learning using</b> the package RMTL", "url": "https://cran.r-project.org/web/packages/RMTL/vignettes/rmtl.html", "isFamilyFriendly": true, "displayUrl": "https://<b>cran.r-project.org</b>/web/packages/RMTL/vignettes/rmtl.html", "snippet": "Cross-validation, <b>Training</b> and Prediction. For all algorithms in RMTL package, \\(\\lambda_1\\) illustrates the strength of relatedness of tasks and high value of \\(\\lambda_1\\) would result in highly <b>similar</b> models. For example, for MTL with low-rank structure (Trace), a large enough \\(\\lambda_1\\) will compress the task space to 1-dimension, thus coefficient vectors of all tasks are proportional.In RMTL, an appropriate \\(\\lambda_1\\) could be estimated using CV based on <b>training</b> data. \\(\\lambda ...", "dateLastCrawled": "2022-02-02T14:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>DropConnect is effective in modeling uncertainty of Bayesian deep</b> ...", "url": "https://www.nature.com/articles/s41598-021-84854-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-84854-x", "snippet": "Due to the high <b>model</b> complexity, DNNs require a huge amount of data to regularize <b>training</b> and prevent the networks from over-fitting the <b>training</b> <b>examples</b>. This reduces their applicability in ...", "dateLastCrawled": "2022-02-02T06:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Discussion on Data Mining - <b>Assignment Den</b>", "url": "https://www.assignmentden.com/discussion-on-data-mining/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>assignmentden</b>.com/discussion-on-data-mining", "snippet": "extended period, the <b>number</b> of dimensions (features) <b>increases</b> in proportion to the <b>number</b> of measurements taken. Traditional data analysis techniques that were developed for low-dimensional data often do not work well for such high-dimensional data due to issues such as curse of dimensionality (to be discussed in Chapter 2). Also, for some data analysis algorithms, the computational complexity <b>increases</b> rapidly as the dimensionality (the <b>number</b> of features) <b>increases</b>. Heterogeneous and ...", "dateLastCrawled": "2022-02-03T12:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Validity, reliability, and generalizability in qualitative research", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4535087/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4535087", "snippet": "This article illustrates with five published studies how qualitative research <b>can</b> impact and reshape the discipline of primary care, spiraling out from clinic-based health screening to community-based disease monitoring, evaluation of out-of-hours triage services to provincial psychiatric care pathways <b>model</b> and finally, national legislation of core measures for children&#39;s healthcare insurance. Fundamental concepts of validity, reliability, and generalizability as applicable to qualitative ...", "dateLastCrawled": "2022-02-03T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>The Shape of Learning Curves: a Review</b>", "url": "https://www.researchgate.net/publication/350253319_The_Shape_of_Learning_Curves_a_Review", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350253319_<b>The_Shape_of_Learning_Curves_a_Review</b>", "snippet": "Learning curves provide insight into the dependence of a learner&#39;s <b>generalization</b> performance on the <b>training</b> set size. This important tool <b>can</b> be used for <b>model</b> selection, to predict the effect ...", "dateLastCrawled": "2021-10-19T17:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A high-bias, low-variance introduction to Machine Learning for physicists", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6688775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6688775", "snippet": "When the amount <b>of training</b> data is limited as it is when N train = 100, one <b>can</b> often get better predictive performance by using a less expressive <b>model</b> (e.g., a lower order polynomial) rather than the more complex <b>model</b> (e.g., the tenthorder polynomial). The simpler <b>model</b> has more \u201cbias\u201d but is less dependent on the particular realization of the <b>training</b> dataset, i.e. less \u201cvariance\u201d. Finally we note that even with ten thousand data points, the <b>model</b>\u2019s performance quickly ...", "dateLastCrawled": "2022-02-02T11:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Optimizing the <b>generalization</b> ability of artificial neural ...", "url": "https://www.academia.edu/63635672/Optimizing_the_generalization_ability_of_artificial_neural_networks_in_ELISA_protocols_by_employing_different_topologies_and_GENETIC_operators", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/63635672/Optimizing_the_<b>generalization</b>_ability_of_artificial...", "snippet": "Optimizing the <b>generalization</b> ability of artificial neural networks in ELISA protocols by employing different topologies and GENETIC operators. 2010. C. Kousoulos. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. Optimizing the <b>generalization</b> ability of artificial neural networks in ELISA protocols by employing different topologies and GENETIC operators . Download ...", "dateLastCrawled": "2022-01-31T08:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Geospatial Data <b>Accuracy</b> Assessment", "url": "https://nepis.epa.gov/Exe/ZyPURL.cgi?Dockey=9101IL6M.TXT", "isFamilyFriendly": true, "displayUrl": "https://nepis.epa.gov/Exe/ZyPURL.cgi?Dockey=9101IL6M.TXT", "snippet": "A positive feature of both <b>examples</b> is that <b>generalization</b> to some population is statistically justified (e.g., the population of all locations conveniently accessible by road or all areas of the map consisting of 3 x 3 homogeneous pixel blocks). Extrapolation to the full map is problematic. In the NLCD assessment, restricting the sample to 3x3 ...", "dateLastCrawled": "2022-02-02T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "6 Learning to Classify Text - Natural Language Toolkit \u2014 NLTK 3.6.2 ...", "url": "https://www.nltk.org/book_1ed/ch06.html", "isFamilyFriendly": true, "displayUrl": "https://www.nltk.org/book_1ed/ch06.html", "snippet": "And since the <b>number</b> of branches <b>increases</b> exponentially as we go down the tree, the amount of repetition <b>can</b> be very large. A related problem is that decision trees are not good at making use of features that are weak predictors of the correct label. Since these features make relatively small incremental improvements, they tend to occur very low in the decision tree. But by the time the decision tree learner has descended far enough to use these features, there is not enough <b>training</b> data ...", "dateLastCrawled": "2022-01-31T19:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A jamming transition from under- to over-parametrization affects ...", "url": "https://www.researchgate.net/publication/336401253_A_jamming_transition_from_under-_to_over-parametrization_affects_generalization_in_deep_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336401253_A_jamming_transition_from_under-_to...", "snippet": "Both promote an asymmetry of the phase space where increasing the <b>number</b> <b>of training</b> <b>examples</b> <b>improves</b> <b>generalization</b> further than increasing the <b>number</b> <b>of training</b> parameters. Our analytical ...", "dateLastCrawled": "2021-12-24T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Exploring <b>Model</b> Robustness with Adaptive Networks and Improved ...", "url": "https://deepai.org/publication/exploring-model-robustness-with-adaptive-networks-and-improved-adversarial-training", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/exploring-<b>model</b>-robustness-with-adaptive-networks-and...", "snippet": "The natural validation <b>accuracy</b> of PGD-7 <b>increases</b> faster than Free-10 at the beginning, while the <b>accuracy</b> at the end <b>of training</b> become close, as shown in figs. 2(e) and 2(b). Free-10 consistently <b>improves</b> robust <b>accuracy</b> against adversarial validation samples, while PGD-7 seems to saturate after the fast increase at the beginning (see figs ...", "dateLastCrawled": "2022-01-26T10:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Introduction to Machine Learning The Wikipedia Guide</b> - Academia.edu", "url": "https://www.academia.edu/41157657/Introduction_to_Machine_Learning_The_Wikipedia_Guide", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/41157657", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-29T19:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Discussion on Data Mining - <b>Assignment Den</b>", "url": "https://www.assignmentden.com/discussion-on-data-mining/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>assignmentden</b>.com/discussion-on-data-mining", "snippet": "<b>number</b> of features) <b>increases</b>. Heterogeneous and Complex Data Traditional data analysis methods often deal with data sets containing attributes of the same type, either continuous or categorical. As the role of data mining in business, science, medicine, and other fields has grown, so has the need for techniques that <b>can</b> handle heterogeneous attributes. Recent years have also seen the emergence of more complex data objects. <b>Examples</b> of such non-traditional types of data include web and ...", "dateLastCrawled": "2022-02-03T12:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Validity, reliability, and generalizability in qualitative research", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4535087/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4535087", "snippet": "This article illustrates with five published studies how qualitative research <b>can</b> impact and reshape the discipline of primary care, spiraling out from clinic-based health screening to community-based disease monitoring, evaluation of out-of-hours triage services to provincial psychiatric care pathways <b>model</b> and finally, national legislation of core measures for children&#39;s healthcare insurance. Fundamental concepts of validity, reliability, and generalizability as applicable to qualitative ...", "dateLastCrawled": "2022-02-03T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A high-bias, low-variance introduction to Machine Learning for physicists", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6688775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6688775", "snippet": "When the amount <b>of training</b> data is limited as it is when N train = 100, one <b>can</b> often get better predictive performance by using a less expressive <b>model</b> (e.g., a lower order polynomial) rather than the more complex <b>model</b> (e.g., the tenthorder polynomial). The simpler <b>model</b> has more \u201cbias\u201d but is less dependent on the particular realization of the <b>training</b> dataset, i.e. less \u201cvariance\u201d. Finally we note that even with ten thousand data points, the <b>model</b>\u2019s performance quickly ...", "dateLastCrawled": "2022-02-02T11:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Over-parameterization: Pitfalls and Opportunities", "url": "https://icml.cc/virtual/2021/workshop/8357", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2021/workshop/8357", "snippet": "Even though the goal of pruning is often to reduce the computational resources consumed during <b>training</b> or inference, it comes as no surprise to theoreticians or practitioners that pruning also <b>improves</b> <b>generalization</b>. In this work, we empirically study pruning&#39;s effect on <b>generalization</b>, focusing on two state-of-the-art pruning algorithms: weight rewinding and learning-rate rewinding. However, each pruning algorithm is actually an aggregation of many design choices: a weight scoring ...", "dateLastCrawled": "2022-02-02T15:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Generalization and Generalizability Measures</b> | Benjamin Wah ...", "url": "https://www.academia.edu/2421929/Generalization_and_Generalizability_Measures", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2421929/<b>Generalization_and_Generalizability_Measures</b>", "snippet": "Defines the <b>generalization</b> problem, summarizes various approaches in <b>generalization</b>, identifies the credit assignment problem, and presents the problem and some solutions in measuring generalizability. We discuss anomalies in the ordering of . \u00d7 Close Log In. Log In with Facebook Log In with Google. Sign Up with Apple. or. Email: Password: Remember me on this computer. or reset ...", "dateLastCrawled": "2021-07-19T02:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "MultiQA: <b>An Empirical Investigation of Generalization and Transfer</b> in ...", "url": "https://deepai.org/publication/multiqa-an-empirical-investigation-of-generalization-and-transfer-in-reading-comprehension", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/multiqa-<b>an-empirical-investigation-of-generalization</b>...", "snippet": "So far we trained on datasets with 75K <b>examples</b>. To examine <b>generalization</b> as the <b>training</b> set size <b>increases</b>, we evaluate performance <b>as the number</b> of <b>examples</b> from the five large datasets grows. Table 3 shows that <b>generalization</b> <b>improves</b> by 26% on average when increasing the <b>number</b> of <b>examples</b> from 37K to 375K.", "dateLastCrawled": "2021-12-31T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Exploring Generalization in Deep Learning</b> | Request PDF", "url": "https://www.researchgate.net/publication/317955051_Exploring_Generalization_in_Deep_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/317955051_Exploring_<b>Generalization</b>_in_Deep...", "snippet": "<b>Model</b>-centric AutoEval, also known as <b>model</b> <b>generalization</b> [4,10, 32], predicts the <b>generalization</b> gap (the gap between the <b>training</b> and testing <b>accuracy</b>) of various models. Some works [19,20] aim ...", "dateLastCrawled": "2022-01-25T18:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Generalization</b> of the Discrete Brier and Ranked Probability Skill ...", "url": "https://www.researchgate.net/publication/249621608_Generalization_of_the_Discrete_Brier_and_Ranked_Probability_Skill_Scores_for_Weighted_Multimodel_Ensemble_Forecasts", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/249621608_<b>Generalization</b>_of_the_Discrete...", "snippet": "Under these conditions, even the addition of an objectively-poor <b>model</b> <b>can</b> improve multi-<b>model</b> skill. It seems that simple ensemble inflation methods cannot yield the same skill improvement. Using ...", "dateLastCrawled": "2021-12-04T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "MultiQA: An Empirical Investigation of <b>Generalization</b> and Transfer in ...", "url": "https://papertohtml.org/paper?id=636904d91d9dd1a641a595d9578ba7640f35aa74", "isFamilyFriendly": true, "displayUrl": "https://papertohtml.org/paper?id=636904d91d9dd1a641a595d9578ba7640f35aa74", "snippet": "The green <b>curve</b> is <b>training</b> on a fixed <b>number</b> of <b>examples</b> from all 5 large datasets without fine-tuning (MULTIQA). In 5 out of 10 curves, pre-<b>training</b> <b>improves</b> performance even given access to all 75K <b>examples</b> from the target dataset. In the other 5, using only the target dataset is better after 30-50K <b>examples</b>. To estimate the savings in ...", "dateLastCrawled": "2021-12-31T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machine<b>curve</b>.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "The <b>training</b> data is fed into the machine learning <b>model</b> in what is called the forward pass. The origin of this name is really easy: the data is simply fed to the network, which means that it passes through it in a forward fashion. The end result is a set of predictions, one per sample. This means that when my <b>training</b> set consists of 1000 feature vectors (or rows with features) that are accompanied by 1000 targets, I will have 1000 predictions after my forward pass.", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Machine <b>learning methods for landslide susceptibility</b> studies: A ...", "url": "https://www.sciencedirect.com/science/article/pii/S0012825220302713", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0012825220302713", "snippet": "<b>Compared</b> with RF, EXT is found to be faster in terms of computational time, and complexity is about three times larger (i.e., the <b>number</b> of leaves with tree depth <b>increases</b>) (Geurts et al., 2006). However, the performance of EXT diminishes when irrelevant attributes are present in the data. This is because attributes in EXT are selected randomly without taking in to account their relationship with the dependent variable. This problem <b>can</b> be easily solved by removing the irrelevant attributes ...", "dateLastCrawled": "2022-01-27T13:28:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding Generelization in <b>Machine</b> <b>Learning</b>", "url": "https://www.asjadk.io/generalization/", "isFamilyFriendly": true, "displayUrl": "https://www.asjadk.io/<b>generalization</b>", "snippet": "In Supervised <b>machine</b> <b>learning</b> we solve problems like image classification where we learn a. Asjad K. Home Research Resources Photography about me. Home Research Resources Photography about me Login Subscribe. Login Subscribe. Understanding Generelization in <b>Machine</b> <b>Learning</b>. Nov 21, 2020 4 min read <b>Machine</b> Intelligence Understanding Generelization in <b>Machine</b> <b>Learning</b> \u201cA computer program is said to learn from experience E with respect to some class of tasks T and performance measureP, if ...", "dateLastCrawled": "2022-01-23T21:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Learning</b> Curves in <b>Machine</b> <b>Learning</b> - ResearchGate", "url": "https://www.researchgate.net/publication/247934703_Learning_Curves_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/247934703_<b>Learning</b>_<b>Curves</b>_in_<b>Machine</b>_<b>Learning</b>", "snippet": "<b>Learning</b> curves provide insight into the dependence of a learner&#39;s <b>generalization</b> performance on the training set size. This important tool can be used for model selection, to predict the effect ...", "dateLastCrawled": "2021-12-15T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "Bayesian, (4) <b>Analogy</b>, and (5) Unsupervised <b>learning</b>. Pedro Domingos proposed these five ML paradigms, and \u00a71.3 explains briefly what each of these five ML paradigms is about. <b>MACHINE</b> <b>LEARNING</b>: A QUANTITATIVE APPROACH 5 2 <b>Machine</b> <b>Learning</b> Fundamentals Illustrated with Regression 2.1 Try to find a publicly available <b>machine</b> <b>learning</b> dataset and apply an end-to-end procedure similar to the one we used with the fuel economy dataset to come up with your own first linear regression <b>machine</b> ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is <b>learning</b>? <b>Machine Learning: Decision Trees</b>", "url": "https://www.csee.umbc.edu/courses/671/fall12/notes/14/14a.pptx.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.csee.umbc.edu/courses/671/fall12/notes/14/14a.pptx.pdf", "snippet": "Major paradigms of <b>machine</b> <b>learning</b> ... \u2022 <b>Analogy</b> \u2013 Determine correspondence between two different representations \u2022 Discovery \u2013 Unsupervised, specific goal not given \u2022 Genetic algorithms \u2013 \u201cEvolutionary\u201d search techniques, based on an <b>analogy</b> to \u201csurvival of the fittest\u201d \u2022 Reinforcement \u2013 Feedback (positive or negative reward) given at the end of a sequence of steps 8 The Classification Problem \u2022 Extrapolate from set of examples to make accurate predictions about ...", "dateLastCrawled": "2021-08-10T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding <b>Deep Learning</b> (Still) Requires Rethinking <b>Generalization</b> ...", "url": "https://cacm.acm.org/magazines/2021/3/250713-understanding-deep-learning-still-requires-rethinking-generalization/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2021/3/250713", "snippet": "An <b>analogy</b> to matrix factorization illustrated the importance of implicit regularization. ... there is renewed interest in seeking to explain <b>generalization</b> in <b>deep learning</b> by characterizing the implicit regularization induced by the <b>learning</b> algorithms. 37, 38, 35, 1. In-depth analysis on memorization of overparameterized models also extends our intuition on overfitting from the traditional U-shaped risk <b>curve</b> to the &quot;double descent&quot; risk <b>curve</b>. Specifically, in the overparameterized ...", "dateLastCrawled": "2022-02-01T17:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "PREDICTION OF RESEARCH TOPICS USING COMBINATION OF <b>MACHINE</b> <b>LEARNING</b> AND ...", "url": "http://www.jatit.org/volumes/Vol49No3/14Vol49No3.pdf", "isFamilyFriendly": true, "displayUrl": "www.jatit.org/volumes/Vol49No3/14Vol49No3.pdf", "snippet": "Extreme <b>Learning</b> <b>Machine</b> and Support Vector <b>Machine</b>. The prediction result is then finally refined by logistic <b>curve</b>. The dataset used in this study is a research report on Bioinformatics from Microsoft Research and NCBI (National Center for Biotechnology Information), over the past 30 years. Experimental result indicates that the combination of <b>machine</b> <b>learning</b> approaches and logistic-<b>curve</b> may improve the prediction accuracy. In addition, the emerging topic of the same dataset can be ...", "dateLastCrawled": "2021-11-21T16:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Convolutional Neural Networks and their components for computer vision</b> ...", "url": "https://www.machinecurve.com/index.php/2018/12/07/convolutional-neural-networks-and-their-components-for-computer-vision/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinecurve</b>.com/index.php/2018/12/07/convolutional-neural-networks-and...", "snippet": "<b>Machine</b> <b>learning</b> (and consequently deep <b>learning</b>) can be used to train computers to see things. We know that <b>machine</b> <b>learning</b> is about feeding examples to machines, after which they derive the patterns in these examples themselves. Consequently, we can see that using <b>machine</b> <b>learning</b> for computer vision equals showing machines enough examples so that they can learn to recognize them on their own, for new data. In deep <b>learning</b>, we use deep neural networks to learn machines to recognize ...", "dateLastCrawled": "2022-01-30T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "04_PAC.pdf - PAC <b>Generalization</b> and SRM <b>Machine</b> <b>Learning</b> A.Y 2021\\/22 ...", "url": "https://www.coursehero.com/file/123933166/04-PACpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/123933166/04-PACpdf", "snippet": "PAC, <b>Generalization</b> and SRM <b>Machine</b> <b>Learning</b>, A.Y. 2021/22, Padova Fabio Aiolli October 6th, 2021 Fabio Aiolli PAC, <b>Generalization</b> and SRM October 6th, 2021 1 / 22 A simple experiment P ( red ) = \u03c0 P ( green ) = 1 - \u03c0 \u03c0 is unknown Pick N marbles (the sample ) from the bin, independently \u03c3 = fraction of red marbles in the sample Fabio Aiolli PAC, <b>Generalization</b> and SRM October 6th, 2021 2 / 22", "dateLastCrawled": "2022-01-05T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "5 <b>Machine Learning</b> Challenges \u2013 Iflexion", "url": "https://www.iflexion.com/blog/machine-learning-challenges", "isFamilyFriendly": true, "displayUrl": "https://www.iflexion.com/blog/<b>machine-learning</b>-challenges", "snippet": "<b>Machine learning</b> has the opposite problem, ... By way of <b>analogy</b>, a traditional carpenter&#39;s first tool in the creation of a table might be a crude axe, while their last tools could include the finest-grade sandpaper and the most delicate of engraving instruments. If the carpenter was to exclusively use either approach, the table would either be destroyed in a blizzard of woodchips in the first hour or else take several years to make. In <b>machine learning</b> models, these parameters (or &#39;limiters ...", "dateLastCrawled": "2022-02-02T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>All Unit MCQ questions of ML</b> \u2013 TheCodingShef", "url": "https://thecodingshef.com/all-unit-mcq-questions-of-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thecodingshef.com/<b>all-unit-mcq-questions-of</b>-<b>machine</b>-<b>learning</b>", "snippet": "A kind of supervised <b>learning</b>; Design of NN as <b>curve</b> fitting problem; Use of multidimensional surface to interpolate the test data; All of these Correct option is D. Application of CBR; Design; Planning; Diagnosis; All of these; Correct option is A. What is/are advantages of CBR? A local approx. is found for each test case; Knowledge is in a form understandable to human; Fast to train; All of these Correct option is D. 112 In k-NN algorithm, given a set of training examples and the value of ...", "dateLastCrawled": "2022-01-30T22:09:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Gateway to Memory: An <b>Introduction to Neural Network Modeling</b> of the ...", "url": "https://epdf.pub/gateway-to-memory-an-introduction-to-neural-network-modeling-of-the-hippocampus-.html", "isFamilyFriendly": true, "displayUrl": "https://epdf.pub/gateway-to-memory-an-<b>introduction-to-neural-network-modeling</b>-of-the...", "snippet": "Widrow and Hoff were engineers, studying <b>machine</b> <b>learning</b> because they wanted to create intelligent computers. They weren\u2019t particularly concerned with whether their <b>learning</b> algorithms bore any meaningful resemblance to <b>learning</b> in the brain\u2014any more than an engineer designing airplanes might care whether the designs capture any features of bird \ufb02ight. However, a decade after Widrow and Hoff developed their neural network <b>learning</b> rule, psychologists realized that some very ...", "dateLastCrawled": "2021-12-08T10:15:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(generalization curve)  is like +(illustrating how the accuracy of the model improves as the number of training examples increases)", "+(generalization curve) is similar to +(illustrating how the accuracy of the model improves as the number of training examples increases)", "+(generalization curve) can be thought of as +(illustrating how the accuracy of the model improves as the number of training examples increases)", "+(generalization curve) can be compared to +(illustrating how the accuracy of the model improves as the number of training examples increases)", "machine learning +(generalization curve AND analogy)", "machine learning +(\"generalization curve is like\")", "machine learning +(\"generalization curve is similar\")", "machine learning +(\"just as generalization curve\")", "machine learning +(\"generalization curve can be thought of as\")", "machine learning +(\"generalization curve can be compared to\")"]}