{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>AUC-ROC Curve - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/auc-roc-curve/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>auc</b>-<b>roc</b>-<b>curve</b>", "snippet": "If you are familiar with some basics of Machine Learning then you must have across some of these metrics <b>like</b> accuracy, precision, recall, <b>auc</b>-<b>roc</b>, etc. Let\u2019s say you are working on a binary classification problem and come up with a model with 95% accuracy, now someone asks you what does that mean you would be quick enough to say out of 100 predictions your model makes, 95 of them are correct. Well lets notch it up a bit, now the underlying metric is recall and you are asked the same ...", "dateLastCrawled": "2022-01-30T04:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "F1 <b>Score</b> vs <b>ROC</b> <b>AUC</b> vs Accuracy vs PR <b>AUC</b>: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-<b>roc</b>-<b>auc</b>-pr-<b>auc</b>", "snippet": "Talk about some of the most common binary classification metrics <b>like</b> F1 <b>score</b>, <b>ROC</b> <b>AUC</b>, PR <b>AUC</b>, ... <b>ROC</b> <b>AUC</b>. <b>AUC</b> means <b>area</b> <b>under</b> the <b>curve</b> so to speak about <b>ROC</b> <b>AUC</b> <b>score</b> we need to define <b>ROC</b> <b>curve</b> first. It is a chart that visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart. Of course, the higher TPR and the lower FPR is for each threshold the better and so classifiers ...", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding the <b>AUC</b>-<b>ROC</b> <b>Curve</b> in Machine Learning Classification", "url": "https://analyticsindiamag.com/understanding-the-auc-roc-curve-in-machine-learning-classification/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>under</b>standing-the-<b>auc</b>-<b>roc</b>-<b>curve</b>-in-machine-learning...", "snippet": "<b>Area</b> <b>Under</b> <b>Curve</b> or <b>AUC</b> is one of the most widely used metrics for model evaluation. It is generally used for binary classification problems. <b>AUC</b> measures the entire two-dimensional <b>area</b> present underneath the entire <b>ROC</b> <b>curve</b>. <b>AUC</b> of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than that of a randomly chosen negative example. The <b>Area</b> <b>Under</b> the <b>Curve</b> provides the ability for a classifier to distinguish between classes and ...", "dateLastCrawled": "2022-02-03T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding <b>AUC</b> - <b>ROC</b> <b>Curve</b> | by Sarang Narkhede | Towards Data Science", "url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>under</b>standing-<b>auc</b>-<b>roc</b>-<b>curve</b>-68b2303cc9c5", "snippet": "<b>AUC</b> - <b>ROC</b> <b>curve</b> is a performance measurement for the classification problems at various threshold settings. <b>ROC</b> is a probability <b>curve</b> and <b>AUC</b> represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the <b>AUC</b>, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By analogy, the Higher the <b>AUC</b>, the better the model is at distinguishing between patients with the disease and no disease.", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Calculating AUC: the area under</b> a <b>ROC</b> <b>Curve</b> | R-bloggers", "url": "https://www.r-bloggers.com/2016/11/calculating-auc-the-area-under-a-roc-curve/", "isFamilyFriendly": true, "displayUrl": "https://www.r-bloggers.com/2016/11/<b>calculating-auc-the-area-under</b>-a-<b>roc</b>-<b>curve</b>", "snippet": "Unlike accuracy, <b>ROC</b> curves are insensitive to class imbalance; the bogus screening test would have an <b>AUC</b> of 0.5, which <b>is like</b> not having a test at all. In this post I\u2019ll work through the geometry exercise of computing the <b>area</b>, and develop a concise vectorized function that uses this approach. Then we\u2019ll look at another way of viewing <b>AUC</b> which leads to a probabilistic interpretation. Yes, it really is the <b>area</b> <b>under</b> the <b>curve</b>. Let\u2019s start with a simple artificial data set: category ...", "dateLastCrawled": "2022-02-02T23:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Measuring Performance: AUC (AUROC</b>) \u2013 Glass Box", "url": "https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/", "isFamilyFriendly": true, "displayUrl": "https://glassboxmedicine.com/2019/02/23/<b>measuring-performance-auc-auroc</b>", "snippet": "The AUROC is calculated as the <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b>. A <b>ROC</b> <b>curve</b> shows the trade-off between true positive rate (TPR) and false positive rate (FPR) across different decision thresholds. For a review of TPRs, FPRs, and decision thresholds, see Measuring Performance: The Confusion Matrix. In plotted <b>ROC</b> curves (e.g. the figure of the previous section), the decision thresholds are implicit. The decision thresholds are not shown as an axis. The AUROC itself is also not explicitly shown; it ...", "dateLastCrawled": "2022-02-03T07:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>Area Under</b> an <b>ROC</b> <b>Curve</b>", "url": "http://gim.unmc.edu/dxtests/RoC3.htm", "isFamilyFriendly": true, "displayUrl": "gim.unmc.edu/dxtests/<b>RoC</b>3.htm", "snippet": "The <b>area under</b> the T4 <b>ROC</b> <b>curve</b> is .86. The T4 would be considered to be &quot;good&quot; at separating hypothyroid from euthyroid patients. <b>ROC</b> curves can also be constructed from clinical prediction rules. The graphs at right come from a study of how clinical findings predict strep throat (Wigton RS, Connor JL, Centor RM. Transportability of a decision rule for the diagnosis of streptococcal pharyngitis. Arch Intern Med. 1986;146:81-83.) In that study, the presence of tonsillar exudate, fever ...", "dateLastCrawled": "2022-01-28T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Area</b> <b>under</b> <b>the ROC</b> <b>curve \u2013 assessing discrimination in logistic</b> ...", "url": "https://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://thestatsgeek.com/2014/05/05/<b>area</b>-<b>under</b>-<b>the-roc</b>-<b>curve</b>-assessing-discrimination...", "snippet": "Thus the <b>area</b> <b>under</b> the <b>curve</b> ranges from 1, corresponding to perfect discrimination, to 0.5, corresponding to a model with no discrimination ability. The <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b> is also sometimes referred to as the c-statistic (c for concordance). The <b>area</b> <b>under</b> the estimated <b>ROC</b> <b>curve</b> (<b>AUC</b>) is reported when we plot <b>the ROC</b> <b>curve</b> in R&#39;s Console.", "dateLastCrawled": "2022-01-29T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>ROC</b> Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>roc</b>-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "The most common metric involves receiver operation characteristics (<b>ROC</b>) analysis, and the <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b> (<b>AUC</b>). \u2014 Page 27, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013. The <b>AUC</b> for <b>the ROC</b> can be calculated in scikit-learn using <b>the roc</b>_<b>auc</b>_score() function. <b>Like</b> <b>the roc</b>_<b>curve</b>() function, the <b>AUC</b> function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the positive class. 1. 2. 3... # calculate <b>roc</b> <b>auc</b>. <b>roc</b>_<b>auc</b> ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Area</b> <b>under</b> <b>curve</b> of <b>ROC</b> vs. overall <b>accuracy</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/68893/area-under-curve-of-roc-vs-overall-accuracy", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/68893", "snippet": "I am a little bit confused about the <b>Area</b> <b>Under</b> <b>Curve</b> (<b>AUC</b>) of <b>ROC</b> and the overall <b>accuracy</b>. Will the <b>AUC</b> be proportional to the overall <b>accuracy</b>? In other words, when we have a larger overall <b>accuracy</b> will we definitely a get larger <b>AUC</b>? Or are they by definition positively correlated? If they are positively correlated, why do we bother reporting both of them in some publications? In real case, I performed some classification task and got the results as follows: classifier A got an <b>accuracy</b> ...", "dateLastCrawled": "2022-01-28T17:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "F1 <b>Score</b> vs <b>ROC</b> <b>AUC</b> vs Accuracy vs PR <b>AUC</b>: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-<b>roc</b>-<b>auc</b>-pr-<b>auc</b>", "snippet": "<b>ROC</b> <b>AUC</b>. <b>AUC</b> means <b>area</b> <b>under</b> the <b>curve</b> so to speak about <b>ROC</b> <b>AUC</b> <b>score</b> we need to define <b>ROC</b> <b>curve</b> first. It is a chart that visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart. Of course, the higher TPR and the lower FPR is for each threshold the better and so classifiers that have curves that are more top-left-side are better. An extensive discussion of <b>ROC</b> <b>Curve</b> and <b>ROC</b> ...", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding <b>the ROC Curve</b> and <b>AUC</b> | by Doug Steen | Towards Data Science", "url": "https://towardsdatascience.com/understanding-the-roc-curve-and-auc-dd4f9a192ecb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>under</b>standing-<b>the-roc-curve</b>-and-<b>auc</b>-dd4f9a192ecb", "snippet": "<b>AUC</b> stands for <b>area</b> <b>under</b> <b>the (ROC) curve</b>. Generally, the higher the <b>AUC</b> score, the better a classifier performs for the given task. Figure 2 shows that for a classifier with no predictive power (i.e., random guessing), <b>AUC</b> = 0.5, and for a perfect classifier, <b>AUC</b> = 1.0. Most classifiers will fall between 0.5 and 1.0, with the rare exception being a classifier performs worse than random guessing (<b>AUC</b> &lt; 0.5). Fig. 2 \u2014 Theoretical <b>ROC</b> curves with <b>AUC</b> scores Why use <b>ROC</b> Curves? One advantage ...", "dateLastCrawled": "2022-02-03T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Understanding AUC (of ROC), sensitivity and specificity values</b>", "url": "https://www.researchgate.net/post/Understanding-AUC-of-ROC-sensitivity-and-specificity-values", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Understanding-AUC-of-ROC-sensitivity</b>-and-specificity...", "snippet": "The <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b> (<b>AUC</b>) is a summary measure of performance, that indicates whether on <b>average</b> a true positive is ranked higher than a false positives. If model A has higher <b>AUC</b> than ...", "dateLastCrawled": "2022-02-02T19:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding the <b>AUC</b>-<b>ROC</b> <b>Curve</b> in Machine Learning Classification", "url": "https://analyticsindiamag.com/understanding-the-auc-roc-curve-in-machine-learning-classification/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>under</b>standing-the-<b>auc</b>-<b>roc</b>-<b>curve</b>-in-machine-learning...", "snippet": "The <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b> gives an idea about the benefit of using the test for the underlying question. <b>AUC</b> - <b>ROC</b> curves are also a performance measurement for the classification problems at various threshold settings. By Victor Dey A critical step after implementing a machine learning algorithm is to find out how effective our model is based on metrics and datasets. Different performance metrics available are used to evaluate the Machine Learning Algorithms. As an example, to ...", "dateLastCrawled": "2022-02-03T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "ImbTreeAUC: An R package for building classification trees using the ...", "url": "https://www.sciencedirect.com/science/article/pii/S2352711021000807", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2352711021000807", "snippet": "A common method is to calculate the <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b> (<b>AUC</b>), ... structure, i.e., with 7 leaves only, on <b>average</b>, while other trees are more complex. The results, in in terms of <b>AUC</b>, accuracy and Kappa, are <b>similar</b> for all the methods. The last case study was the E. coli protein localisation site dataset , which has eight classes. The dataset contains 336 instances and is highly imbalanced. The results indicate that the ImbTreeAUC algorithm can outperform the other methods because it ...", "dateLastCrawled": "2022-02-03T03:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Performance Measures for Multi-Class Problems</b> - Data Science Blog ...", "url": "https://www.datascienceblog.net/post/machine-learning/performance-measures-multi-class-problems/", "isFamilyFriendly": true, "displayUrl": "https://www.datascienceblog.net/post/machine-learning/performance-measures-multi-class...", "snippet": "The <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b> (<b>AUC</b>) is a useful tool for evaluating the quality of class separation for soft classifiers. In the multi-class setting, we can visualize the performance of multi-class models according to their one-vs-all precision-recall curves. The <b>AUC</b> can also be generalized to the multi-class setting. One-vs-all precision-recall curves. As discussed in this Stack Exchange thread, we can visualize the performance of a multi-class model by plotting the performance of K binary ...", "dateLastCrawled": "2022-02-02T18:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Area</b> <b>under</b> <b>the ROC</b> <b>curve \u2013 assessing discrimination in logistic</b> ...", "url": "https://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://thestatsgeek.com/2014/05/05/<b>area</b>-<b>under</b>-<b>the-roc</b>-<b>curve</b>-assessing-discrimination...", "snippet": "I got one question regarding the link between the <b>AUC</b> and the probability of correctly ranking two randomly observations (one from the diseased and one from the non-diseased) that you explained in the section \u201cinterpretation of the <b>area</b> <b>under</b> <b>the roc</b> <b>curve</b>\u201d. The point is that I did not manage to mathematically \u201cdemonstrate\u201d that <b>area</b> <b>under</b> the <b>curve</b> \u201csensitivity vs 1-specificity\u201d <b>is similar</b> to calculating the rate of concordant pairs (p(Xi) &gt; p(Xj)).", "dateLastCrawled": "2022-01-29T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Area</b> <b>under</b> <b>curve</b> of <b>ROC</b> vs. overall <b>accuracy</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/68893/area-under-curve-of-roc-vs-overall-accuracy", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/68893", "snippet": "The <b>area</b> <b>under</b> the <b>curve</b> (<b>AUC</b>) is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative example. It measures the classifiers skill in ranking a set of patterns according to the degree to which they belong to the positive class, but without actually assigning patterns to classes.", "dateLastCrawled": "2022-01-28T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Metrics to Evaluate Classification and Regression Algorithms | by ...", "url": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and-regression-algorithms-1554f1e00a75", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and...", "snippet": "Therefore, there the <b>AUC</b> score is 0.9 as the <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b> ios large. 2) Whereas, if we see the last model, predictions are completely overlapping each other and we get the <b>AUC</b> score of ...", "dateLastCrawled": "2022-01-17T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python 2.7 - Plotting Multiple <b>ROC</b> curves, or an <b>average</b> one from multi ...", "url": "https://stackoverflow.com/questions/21374335/plotting-multiple-roc-curves-or-an-average-one-from-multi-class-labels-multino", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/21374335", "snippet": "<b>ROC</b> <b>curve</b> averaging has been proposed by Hand &amp; Till in 2001. They basically compute <b>the ROC</b> curves for all comparison pairs (4 vs. 5, 4 vs. 6 and 5 vs. 6) and <b>average</b> the result. When you compute <b>the ROC</b> <b>curve</b> with pos_label=4, you implicitly say that the other labels are the negatives (5 and 6). Note that this is slightly different from what ...", "dateLastCrawled": "2022-01-07T20:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>AUC-ROC Curve - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/auc-roc-curve/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>auc</b>-<b>roc</b>-<b>curve</b>", "snippet": "So far so good, now let us assume you evaluated your model using <b>AUC</b>-<b>ROC</b> as a metric and got a value of 0.75 and again I shoot the same question at you what does 0.75 or 75% signify, now you might need to give it a <b>thought</b>, some of you might say there is a 75% chance that model identifies a data point correctly but by now you would have already realized that\u2019s not it. Let us try to get a basic understanding of one the most used performance metrics out there for classification problems.", "dateLastCrawled": "2022-01-30T04:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "F1 <b>Score</b> vs <b>ROC</b> <b>AUC</b> vs Accuracy vs PR <b>AUC</b>: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-<b>roc</b>-<b>auc</b>-pr-<b>auc</b>", "snippet": "<b>ROC</b> <b>AUC</b>. <b>AUC</b> means <b>area</b> <b>under</b> the <b>curve</b> so to speak about <b>ROC</b> <b>AUC</b> <b>score</b> we need to define <b>ROC</b> <b>curve</b> first. It is a chart that visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart. Of course, the higher TPR and the lower FPR is for each threshold the better and so classifiers that have curves that are more top-left-side are better. An extensive discussion of <b>ROC</b> <b>Curve</b> and <b>ROC</b> ...", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>Area Under</b> an <b>ROC</b> <b>Curve</b>", "url": "http://gim.unmc.edu/dxtests/RoC3.htm", "isFamilyFriendly": true, "displayUrl": "gim.unmc.edu/dxtests/<b>RoC</b>3.htm", "snippet": "The <b>area under</b> the T4 <b>ROC</b> <b>curve</b> is .86. The T4 would be considered to be &quot;good&quot; at separating hypothyroid from euthyroid patients. <b>ROC</b> curves <b>can</b> also be constructed from clinical prediction rules. The graphs at right come from a study of how clinical findings predict strep throat (Wigton RS, Connor JL, Centor RM. Transportability of a decision rule for the diagnosis of streptococcal pharyngitis. Arch Intern Med. 1986;146:81-83.) In that study, the presence of tonsillar exudate, fever ...", "dateLastCrawled": "2022-01-28T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Area</b> <b>under</b> <b>the ROC</b> <b>curve \u2013 assessing discrimination in logistic</b> ...", "url": "https://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://thestatsgeek.com/2014/05/05/<b>area</b>-<b>under</b>-<b>the-roc</b>-<b>curve</b>-assessing-discrimination...", "snippet": "Thus the <b>area</b> <b>under</b> the <b>curve</b> ranges from 1, corresponding to perfect discrimination, to 0.5, corresponding to a model with no discrimination ability. The <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b> is also sometimes referred to as the c-statistic (c for concordance). The <b>area</b> <b>under</b> the estimated <b>ROC</b> <b>curve</b> (<b>AUC</b>) is reported when we plot <b>the ROC</b> <b>curve</b> in R&#39;s Console.", "dateLastCrawled": "2022-01-29T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "classification - <b>ROC</b> <b>Area</b> <b>Under</b> <b>Curve</b> (<b>AUC</b>) in SVM - different results ...", "url": "https://stats.stackexchange.com/questions/236846/roc-area-under-curve-auc-in-svm-different-results-between-r-functions", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/236846/<b>roc</b>-<b>area</b>-<b>under</b>-<b>curve</b>-<b>auc</b>-in-svm...", "snippet": "<b>ROC</b> <b>Area</b> <b>Under</b> <b>Curve</b> (<b>AUC</b>) in SVM - different results between R functions. Ask Question Asked 5 years, 4 ... Is this normal? Intuitively I would have <b>thought</b> that testing <b>AUC</b> would be lower than in training because of some level of poor fitting to unseen data. Does anyone have an explanation for the differences between <b>AUC</b> from pROC and the ggplot2 extension plotROC that are both calculated on the testing prediction? I&#39;ve had a look at the documentation for both pROC and plotROC (and the ...", "dateLastCrawled": "2022-02-02T08:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>ROC</b>, <b>and AUC in Data Science? - Quora</b>", "url": "https://www.quora.com/What-is-ROC-and-AUC-in-Data-Science", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>ROC</b>-<b>and-AUC-in-Data-Science</b>", "snippet": "Answer (1 of 3): In a binary classification scenario, <b>AUC</b> (<b>Area</b> <b>Under</b> the <b>Curve</b>) <b>can</b> be interpreted as the probability that a random positive sample will be assigned higher positive posterior probability than to a random negative sample. Put in another way, it give that how likely is that a rando...", "dateLastCrawled": "2022-01-24T03:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "python - Different result with <b>roc_auc_score</b>() and <b>auc</b>() - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/31159157/different-result-with-roc-auc-score-and-auc", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31159157", "snippet": "I <b>thought</b> both were just calculating the <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b>. Might be because of the imbalanced dataset but I could not figure out why. Thanks! python machine-learning scikit-learn. Share. Improve this question. Follow edited Jul 1 &#39;15 at 12:40. gowithefloww. asked Jul 1 &#39;15 at 10:48. gowithefloww gowithefloww. 2,011 2 2 gold badges 18 18 silver badges 28 28 bronze badges. Add a comment | 3 Answers Active Oldest Score. 55 <b>AUC</b> is not always <b>area</b> <b>under</b> the <b>curve</b> of a <b>ROC</b> <b>curve</b>. <b>Area</b> ...", "dateLastCrawled": "2022-02-02T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to interpret 95% <b>confidence interval</b> for <b>Area</b> <b>Under</b> <b>Curve</b> of <b>ROC</b>?", "url": "https://stats.stackexchange.com/questions/165033/how-to-interpret-95-confidence-interval-for-area-under-curve-of-roc", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/165033/how-to-interpret-95-confidence...", "snippet": "The same holds for the <b>AUC</b>, when you compute the <b>AUC</b>, you compute it from a sample, in other words what you compute is an estimate for the true unknown <b>AUC</b>. Similarly you <b>can</b>, for the sample that you have, compute a <b>confidence interval</b> for the true but unknown <b>AUC</b>. If you were able to draw an infinite number of samples, and for each sample obtained compute the <b>confidence interval</b> for the true <b>AUC</b>, then $95\\%$ of these computed intervals would contain the true but unknown <b>AUC</b>.", "dateLastCrawled": "2022-01-24T02:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Different result with <b>roc</b>_<b>auc</b>_score() and <b>auc</b>() - Blogmepost - Learn ...", "url": "https://blogmepost.com/37461/Different-result-with-roc_auc_score-and-auc?show=37462", "isFamilyFriendly": true, "displayUrl": "https://blogmepost.com/37461/Different-result-with-<b>roc</b>_<b>auc</b>_score-and-<b>auc</b>?show=37462", "snippet": "Somebody <b>can</b> explain this difference? I <b>thought</b> both were just calculating the <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b>. Might be because of the imbalanced dataset but I could not figure out why. Thanks! Select the correct answer from above options. machine-learning . artificial-intelligence. data-science. deep-learning. keras. Facebook Twitter LinkedIn Email. 1 Answer. 0 votes . answered 4 hours ago by JackTerrance (1.3m points) Best answer. When we need to check or visualize the performance of the multi ...", "dateLastCrawled": "2022-01-29T08:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>can</b> we find <b>the ROC</b> <b>curve</b> and <b>area</b> <b>under</b> <b>ROC</b> <b>curve</b> for a multi ...", "url": "https://www.quora.com/How-can-we-find-the-ROC-curve-and-area-under-ROC-curve-for-a-multi-instance-multi-label-classification-One-feature-vector-may-correspond-to-one-or-more-labels", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>can</b>-we-find-<b>the-ROC</b>-<b>curve</b>-and-<b>area</b>-<b>under</b>-<b>ROC</b>-<b>curve</b>-for-a...", "snippet": "Answer: Lets say you have 3 labels 1,2,3 and you have data belonging to these 3 labels. Let&#39;s say they were labelled as a(1), b(2), c(3) by your model 1 2 3 A 25 5 5 B 5 15 10 C 5 10 20 This table means that 25 instances which were label 1 were labeled a and so on. The true positives would be ...", "dateLastCrawled": "2022-01-19T07:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "F1 <b>Score</b> vs <b>ROC</b> <b>AUC</b> vs Accuracy vs PR <b>AUC</b>: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-<b>roc</b>-<b>auc</b>-pr-<b>auc</b>", "snippet": "<b>ROC</b> <b>AUC</b>. <b>AUC</b> means <b>area</b> <b>under</b> the <b>curve</b> so to speak about <b>ROC</b> <b>AUC</b> <b>score</b> we need to define <b>ROC</b> <b>curve</b> first. It is a chart that visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart. Of course, the higher TPR and the lower FPR is for each threshold the better and so classifiers that have curves that are more top-left-side are better. An extensive discussion of <b>ROC</b> <b>Curve</b> and <b>ROC</b> ...", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Classification: <b>ROC</b> <b>Curve</b> and <b>AUC</b> | Machine Learning Crash Course ...", "url": "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/classification/<b>roc</b>-and-<b>auc</b>", "snippet": "<b>AUC</b>: <b>Area</b> <b>Under</b> <b>the ROC</b> <b>Curve</b>. <b>AUC</b> stands for &quot;<b>Area</b> <b>under</b> <b>the ROC</b> <b>Curve</b>.&quot; That is, <b>AUC</b> measures the entire two-dimensional <b>area</b> underneath the entire <b>ROC</b> <b>curve</b> (think integral calculus) from (0,0) to (1,1). Figure 5. <b>AUC</b> (<b>Area</b> <b>under</b> <b>the ROC</b> <b>Curve</b>). <b>AUC</b> provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting <b>AUC</b> is as the probability that the model ranks a random positive example more highly than a random negative example. For ...", "dateLastCrawled": "2022-02-02T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding the <b>AUC</b>-<b>ROC</b> <b>Curve</b> in Machine Learning Classification", "url": "https://analyticsindiamag.com/understanding-the-auc-roc-curve-in-machine-learning-classification/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>under</b>standing-the-<b>auc</b>-<b>roc</b>-<b>curve</b>-in-machine-learning...", "snippet": "<b>Area</b> <b>Under</b> <b>Curve</b> or <b>AUC</b> is one of the most widely used metrics for model evaluation. It is generally used for binary classification problems. <b>AUC</b> measures the entire two-dimensional <b>area</b> present underneath the entire <b>ROC</b> <b>curve</b>. <b>AUC</b> of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than that of a randomly chosen negative example. The <b>Area</b> <b>Under</b> the <b>Curve</b> provides the ability for a classifier to distinguish between classes and ...", "dateLastCrawled": "2022-02-03T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Can</b> the <b>AUC</b> (<b>Area</b> <b>Under</b> <b>Curve</b>) metric be less than 0.5? - Quora", "url": "https://www.quora.com/Can-the-AUC-Area-Under-Curve-metric-be-less-than-0-5", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Can</b>-the-<b>AUC</b>-<b>Area</b>-<b>Under</b>-<b>Curve</b>-metric-be-less-than-0-5", "snippet": "Answer: <b>Area Under Receiver Operating Characteristic( AUROC ) can</b> be &lt; 0.5. Lets say you have to predict what customers will purchase from an E-commerce website. Say you design the 3 predictors which do the following respectively : 1. Marks everyone as a Buyer ( equivalent to random-guess/monk...", "dateLastCrawled": "2022-01-23T06:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding <b>the ROC Curve</b> and <b>AUC</b> | by Doug Steen | Towards Data Science", "url": "https://towardsdatascience.com/understanding-the-roc-curve-and-auc-dd4f9a192ecb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>under</b>standing-<b>the-roc-curve</b>-and-<b>auc</b>-dd4f9a192ecb", "snippet": "<b>AUC</b> stands for <b>area</b> <b>under</b> <b>the (ROC) curve</b>. Generally, the higher the <b>AUC</b> score, the better a classifier performs for the given task. Figure 2 shows that for a classifier with no predictive power (i.e., random guessing), <b>AUC</b> = 0.5, and for a perfect classifier, <b>AUC</b> = 1.0. Most classifiers will fall between 0.5 and 1.0, with the rare exception being a classifier performs worse than random guessing (<b>AUC</b> &lt; 0.5). Fig. 2 \u2014 Theoretical <b>ROC</b> curves with <b>AUC</b> scores Why use <b>ROC</b> Curves? One advantage ...", "dateLastCrawled": "2022-02-03T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Measuring Performance: AUC (AUROC</b>) \u2013 Glass Box", "url": "https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/", "isFamilyFriendly": true, "displayUrl": "https://glassboxmedicine.com/2019/02/23/<b>measuring-performance-auc-auroc</b>", "snippet": "The AUROC is calculated as the <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b>. A <b>ROC</b> <b>curve</b> shows the trade-off between true positive rate (TPR) and false positive rate (FPR) across different decision thresholds. For a review of TPRs, FPRs, and decision thresholds, see Measuring Performance: The Confusion Matrix. In plotted <b>ROC</b> curves (e.g. the figure of the previous section), the decision thresholds are implicit. The decision thresholds are not shown as an axis. The AUROC itself is also not explicitly shown; it ...", "dateLastCrawled": "2022-02-03T07:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Area</b> <b>under</b> <b>the ROC</b> <b>curve \u2013 assessing discrimination in logistic</b> ...", "url": "https://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://thestatsgeek.com/2014/05/05/<b>area</b>-<b>under</b>-<b>the-roc</b>-<b>curve</b>-assessing-discrimination...", "snippet": "Although it is not obvious from its definition, the <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b> (<b>AUC</b>) has a somewhat appealing interpretation. It turns out that the <b>AUC</b> is the probability that if you were to take a random pair of observations, one with . and one with , the observation with . has a higher predicted probability than the other. The <b>AUC</b> thus gives the probability that the model correctly ranks such pairs of observations. In the biomedical context of risk prediction modelling, the <b>AUC</b> has been ...", "dateLastCrawled": "2022-01-29T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Calculating AUC: the area under</b> a <b>ROC</b> <b>Curve</b> | R-bloggers", "url": "https://www.r-bloggers.com/2016/11/calculating-auc-the-area-under-a-roc-curve/", "isFamilyFriendly": true, "displayUrl": "https://www.r-bloggers.com/2016/11/<b>calculating-auc-the-area-under</b>-a-<b>roc</b>-<b>curve</b>", "snippet": "<b>Calculating AUC: the area under</b> a <b>ROC</b> <b>Curve</b>. by Bob Horton, Microsoft Senior Data Scientist. <b>Receiver Operating Characteristic</b> (<b>ROC</b>) curves <b>are a</b> popular way to visualize the tradeoffs between sensitivitiy and specificity in a binary classifier. In an earlier post, I described a simple \u201cturtle\u2019s eye view\u201d of these plots: a classifier is ...", "dateLastCrawled": "2022-02-02T23:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to get <b>the ROC curve and AUC for Keras model</b>? - knowledge Transfer", "url": "https://androidkt.com/get-the-roc-curve-and-auc-for-keras-model/", "isFamilyFriendly": true, "displayUrl": "https://androidkt.com/get-<b>the-roc-curve-and-auc-for-keras-model</b>", "snippet": "One way to compare classifiers is to measure the <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b>, whereas a purely random classifier will have a <b>ROC</b> <b>AUC</b> equal to 0.5. Scikit-Learn provides a function to get <b>AUC</b>. <b>auc</b>_score=<b>roc</b>_<b>auc</b>_score (y_val_cat,y_val_cat_prob) #0.8822. <b>AUC</b> is the percentage of this <b>area</b> that is <b>under</b> this <b>ROC</b> <b>curve</b>, ranging between 0~1.", "dateLastCrawled": "2022-02-03T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>can</b> I compare 2 <b>AUC values of different parameters of the same</b> ...", "url": "https://www.researchgate.net/post/How-can-I-compare-2-AUC-values-of-different-parameters-of-the-same-sample-group-ROC-analysis", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/How-<b>can</b>-I-compare-2-<b>AUC</b>-values-of-different...", "snippet": "The <b>average</b> of the two correlations is used along with the <b>average</b> of the areas <b>under</b> the two curves to arrive at an estimated correlation between the two areas. A table that applies when the ...", "dateLastCrawled": "2022-01-26T04:11:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding <b>AUC</b> - <b>ROC</b> <b>Curve</b> | by Sarang Narkhede | Towards Data Science", "url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>under</b>standing-<b>auc</b>-<b>roc</b>-<b>curve</b>-68b2303cc9c5", "snippet": "<b>AUC</b> - <b>ROC</b> <b>curve</b> is a performance measurement for the classification problems at various threshold settings. <b>ROC</b> is a probability <b>curve</b> and <b>AUC</b> represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the <b>AUC</b>, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By <b>analogy</b>, the Higher the <b>AUC</b>, the better the model is at distinguishing between patients with the disease and no disease.", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning</b> <b>Evaluation Metrics</b> - GitHub Pages", "url": "https://kevalnagda.github.io/evaluation-metrics", "isFamilyFriendly": true, "displayUrl": "https://kevalnagda.github.io/<b>evaluation-metrics</b>", "snippet": "<b>AUC</b> calculates the <b>area</b> <b>under</b> <b>the ROC</b> <b>curve</b>, and therefore it is between 0 and 1. One way of interpreting <b>AUC</b> is the probability that the model ranks a random positive example more highly than a random negative example. Youden\u2019s index Youden\u2019s J statistic (also called Youden\u2019s index) is a single statistic that captures the performance of a dichotomous (A partition of a whole into two) diagnostic tests. Youden\u2019s J statistic is J = sensitivity + specificity - 1 The right-hand two ...", "dateLastCrawled": "2021-10-13T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Comparison of <b>machine</b>-<b>learning</b> methodologies for accurate diagnosis of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8128240/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8128240", "snippet": "After completion of training and prediction steps during each iteration, predictive metrics (<b>area</b> <b>under</b> the <b>curve</b> (<b>AUC</b>) and probability of correct classification (PCC)) are calculated based on the respective <b>machine</b>-<b>learning</b> classifier results, and <b>receiver operating characteristic</b> (<b>ROC</b>) plots are generated using R package \u201cPresenceAbsence\u201d . The difference between <b>AUC</b> and PCC means was compared by unpaired Student\u2019s t-tests using R base functions for all <b>machine</b>-<b>learning</b>-based ...", "dateLastCrawled": "2022-01-26T11:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding <b>AUC</b> - <b>RO C</b> <b>Cur ve</b>", "url": "https://48hours.ai/files/AUC.pdf", "isFamilyFriendly": true, "displayUrl": "https://48hours.ai/files/<b>AUC</b>.pdf", "snippet": "In <b>Machine</b> <b>Learning</b>, performance measurement is an essential task. So when it comes to a classification problem, we can count on an <b>AUC</b> - <b>ROC</b> <b>Curve</b>. When we need to check or visualize the performance of the multi - class classification problem, we use <b>AUC</b> (<b>Area</b> <b>Under</b> The <b>Cur ve</b>) <b>ROC</b> (Receiver Operating Character istics) <b>curve</b>. It is one of the most important evaluation metrics for checking any classification model\u2019s performance. It is also written as AUROC (<b>Area</b> <b>Under</b> t he Receiver ...", "dateLastCrawled": "2022-02-01T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is the <b>AUC</b> \u2014 <b>ROC</b> <b>Curve</b>?. <b>AUC</b>-<b>ROC</b> <b>CURVE</b> | CONFUSION MATRIX |\u2026 | by ...", "url": "https://medium.com/computer-architecture-club/what-is-the-auc-roc-curve-47fbdcbf7a4a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/computer-architecture-club/what-is-the-<b>auc</b>-<b>roc</b>-<b>curve</b>-47fbdcbf7a4a", "snippet": "The <b>Area</b> <b>Under</b> the <b>Curve</b> (<b>AUC</b>) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of <b>the ROC</b> <b>curve</b>. <b>AUC</b>-<b>ROC</b> <b>curve</b> is a performance measurement ...", "dateLastCrawled": "2022-01-26T00:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "In <b>machine</b> <b>learning</b>, how model accuracy and <b>ROC</b> <b>AUC</b> (<b>area</b> <b>under</b> the TP ...", "url": "https://www.quora.com/In-machine-learning-how-model-accuracy-and-ROC-AUC-area-under-the-TP-vs-FP-rates-curve-are-related", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-<b>machine</b>-<b>learning</b>-how-model-accuracy-and-<b>ROC</b>-<b>AUC</b>-<b>area</b>-<b>under</b>...", "snippet": "Answer (1 of 2): High accuracy and higher <b>AUC</b> are both good things generally. Before understanding AUROC, first the concept of confusion matrix must be understood ...", "dateLastCrawled": "2022-01-07T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine learning predicts mortality based on analysis</b> of ventilation ...", "url": "https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-021-01506-w", "isFamilyFriendly": true, "displayUrl": "https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-021-01506-w", "snippet": "Predictive performance of RNN-based model was higher with <b>Area</b> <b>Under</b> <b>the Receiver Operating Characteristic</b> (<b>ROC</b>) <b>Curve</b> (<b>AUC</b>) of 0.72 (\u00b1 0.01) and Average Precision (AP) of 0.57 (\u00b1 0.01) in comparison to RF and LR for the overall patient dataset. Higher predictive performance was recorded in the subgroup of patients admitted with respiratory disorders with <b>AUC</b> of 0.75 (\u00b1 0.02) and AP of 0.65 (\u00b1 0.03). Inclusion of function of other organs further improved the performance to <b>AUC</b> of 0.79 ...", "dateLastCrawled": "2022-01-31T04:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Receiver Operating <b>Curve</b> (<b>ROC</b>) and The Scale \u2014 Understanding <b>ROC</b> ...", "url": "https://medium.com/data-science-everyday/the-receiver-operating-curve-roc-and-the-scale-understanding-roc-through-an-analogy-8b8f9d954f84", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-everyday/the-receiver-operating-<b>curve</b>-<b>roc</b>-and-the...", "snippet": "The Receiver Operating <b>Curve</b> (<b>ROC</b>) is used to evaluate and improve classification <b>Machine</b> <b>Learning</b> models. Who does not want an accurate classifier, that place observations where they actually\u2026", "dateLastCrawled": "2021-02-13T00:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "C. <b>Area</b> <b>under</b> <b>the ROC</b> <b>curve</b> D. All of the above Answer : D. 24. Which of the following is a good test dataset characteristic? A. Large enough to yield meaningful results B. Is representative of the dataset as a whole C. Both A and B D. None of the above Answer : C. 25. Which of the following is a disadvantage of decision trees? A. Factor analysis", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>AUC</b> <b>ROC</b> <b>curve</b> - <b>auc</b>: <b>area</b> <b>under</b> <b>the roc</b> <b>curve</b>", "url": "https://haar-t.com/questions/25009284/how-to-plot-roc-curve-in-python5q3cww3348a8y8", "isFamilyFriendly": true, "displayUrl": "https://haar-t.com/questions/25009284/how-to-plot-<b>roc</b>-<b>curve</b>-in-python5q3cww3348a8y8", "snippet": "The function returns the false positive rates for each threshold, true positive rates for each threshold and. <b>ROC</b> <b>curve</b> (<b>Receiver Operating Characteristic</b>) is a commonly used way to visualize the performance of a binary classifier and <b>AUC</b> (<b>Area</b> <b>Under</b> <b>the ROC</b> <b>Curve</b>) is used to summarize its performance in a single number. Most <b>machine</b> <b>learning</b> algorithms have the ability to produce probability scores that tells us the strength in which it thinks a given observation is positive L&#39;aire sous la ...", "dateLastCrawled": "2022-01-25T23:14:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(auc (area under the roc curve))  is like +(average)", "+(auc (area under the roc curve)) is similar to +(average)", "+(auc (area under the roc curve)) can be thought of as +(average)", "+(auc (area under the roc curve)) can be compared to +(average)", "machine learning +(auc (area under the roc curve) AND analogy)", "machine learning +(\"auc (area under the roc curve) is like\")", "machine learning +(\"auc (area under the roc curve) is similar\")", "machine learning +(\"just as auc (area under the roc curve)\")", "machine learning +(\"auc (area under the roc curve) can be thought of as\")", "machine learning +(\"auc (area under the roc curve) can be compared to\")"]}