{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An integer programming formulation to identify the sparse network ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2865861/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2865861", "snippet": "<b>Number</b> of <b>variables</b> to be optimized in the lower level is determined at the upper level as: . Upper level integer programming essentially reduces the <b>number</b> of <b>variables</b> to be optimized in the lower level, and as a consequence the estimation problem remains well posed even with a small <b>number</b> of experimental <b>observations</b>. An additional constraint is imposed on the least square minimization program to ensure that the <b>number</b> of estimated parameters does not exceed experimental data points:", "dateLastCrawled": "2017-01-10T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Neural correlates of sparse coding and dimensionality reduction - PLOS", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006908", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006908", "snippet": "A <b>number</b> of different definitions of <b>sparsity</b> can be found in the literature [8, 9], ... Another approach to address this challenge is to reduce the <b>number</b> of <b>variables</b> required to represent a particular <b>input</b>, stimulus, or task space, a process known as dimensionality reduction. Although responses of individual neurons are often complex and highly nonlinear, a population of neurons might share activity patterns because of individual neurons in the population not being independent of each ...", "dateLastCrawled": "2021-02-18T13:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Structured and Sparse Canonical Correlation Analysis as a Brain-Wide ...", "url": "https://adni.loni.usc.edu/adni-publications/Structured%20and%20Sparse%20Canonical%20Correlation%20Analysis%20as%20a%20Brain-Wide%20Multi-Modal%20Data%20Fusion%20Approach.pdf", "isFamilyFriendly": true, "displayUrl": "https://adni.loni.usc.edu/adni-publications/Structured and Sparse Canonical Correlation...", "snippet": "B. <b>Sparsity</b> Constraint Due to the small <b>number</b> of samples but high-dimensional <b>variables</b>/features in the neuroimaging datasets, (1) faces the over\ufb01tting problem. Therefore, sparse penalties such as l1-norm are imposed on w1 and w2 in the sCCA analysis [10]: max w1,w2 wT 1 X T Yw 2 s.t. wT 1 X T Xw 1= 1, wT 2 Y T Yw 2= 1, w 1 \u2264 c1, w2 \u2264 c2 (2)", "dateLastCrawled": "2022-02-01T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to Sparse Matrices for Machine Learning", "url": "https://machinelearningmastery.com/sparse-matrices-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/sparse-matrices-for-machine-learning", "snippet": "The <b>sparsity</b> of a matrix can be quantified with a score, which is the <b>number</b> of zero values in the matrix divided by the total <b>number</b> of elements in the matrix. <b>sparsity</b> = count zero elements / total elements. 1. <b>sparsity</b> = count zero elements / total elements. Below is an example of a small 3 x 6 sparse matrix.", "dateLastCrawled": "2022-02-02T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Dimensionality Reduction with PCA | by Iprathore | Medium", "url": "https://iprathore71.medium.com/dimensionality-reduction-with-pca-d7ff3838dd30", "isFamilyFriendly": true, "displayUrl": "https://iprathore71.medium.com/dimensionality-reduction-with-pca-d7ff3838dd30", "snippet": "The <b>number</b> <b>of input</b> <b>variables</b> or features for a dataset is referred to as its dimensionality. ... Change of Basis \u2014 different basis vectors can be used to represent the same <b>observations</b>, just <b>like</b> you can represent the weight of a patient in kilograms or pounds. key ideas that will help you connect basis vectors and the idea of dimensionality reduction: using different basis vectors to represent the same points. A vector isn\u2019t tied to the coordinate axes set used to describe it ...", "dateLastCrawled": "2022-01-21T10:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Variable selection in <b>social-environmental data</b>: sparse regression and ...", "url": "https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01183-9", "isFamilyFriendly": true, "displayUrl": "https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01183-9", "snippet": "If the <b>number</b> of clusters is small <b>relative</b> to n, clustered <b>variables</b> can be summarized into a single measure, and models can then be fit using multiple regression models. However, if a large <b>number</b> of clusters are present, a better choice is to use cluster membership to fit group lasso or sparse group lasso models. The sparse group lasso is particularly useful, as it has penalties at both the group and individual level, allowing for <b>sparsity</b> across and within groups", "dateLastCrawled": "2022-01-16T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Variable selection for inferential models with relatively high ...", "url": "https://www.nature.com/articles/s41598-020-64829-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-020-64829-0", "snippet": "Variable selection in inferential modelling is problematic when the <b>number</b> of <b>variables</b> is large <b>relative</b> <b>to the number</b> of data points, especially when multicollinearity is present. A variety of ...", "dateLastCrawled": "2022-02-02T05:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "On the <b>Sparsity</b> of XORs in Approximate Model Counting", "url": "https://link.springer.com/chapter/10.1007%2F978-3-030-51825-7_18", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-51825-7_18", "snippet": "Given a Boolean formula \\(\\varphi \\), the problem of model counting, also referred to as #SAT, is to compute the <b>number</b> of solutions of \\(\\varphi \\).Model counting is a fundamental problem in computer science with a wide range of applications ranging from quantified information flow, reliability of networks, probabilistic programming, Bayesian networks, and others [4, 5, 10, 16, 21, 22, 23].Given the computational intractability of #SAT, attention has been focused on the approximation of # ...", "dateLastCrawled": "2021-12-07T18:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Simultaneous input variable and basis function selection</b> for RBF ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231208004530", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231208004530", "snippet": "However, selecting a subset of <b>variables</b> and estimating their <b>relative</b> importances would be valuable in many real world applications. In the present work, a simultaneous <b>input</b> and basis function selection method for a radial basis function (RBF) network is proposed. The selection is performed by minimizing a constrained optimization problem, in which <b>sparsity</b> of the network is controlled by two continuous valued shrinkage parameters. Each <b>input</b> dimension is weighted and the constraints are ...", "dateLastCrawled": "2021-10-20T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Dimensionality Reduction 101 for Dummies</b> <b>like</b> Me | Evisi0n", "url": "https://evisionindia.wordpress.com/2019/10/01/dimensionality-reduction-101-for-dummies-like-me/", "isFamilyFriendly": true, "displayUrl": "https://evisionindia.wordpress.com/.../01/<b>dimensionality-reduction-101-for-dummies</b>-<b>like</b>-me", "snippet": "They are practical only when applied to a dataset with an already relatively low <b>number</b> <b>of input</b> columns. Figure 1. Accuracies of the best-performing models trained on well-known datasets that were reduced using the 10 selected data dimensionality reduction techniques. Linear Dimensionality Reduction Methods. The most common and well-known dimensionality reduction methods are the ones that apply linear transformations, <b>like</b>. Factor Analysis: This technique is used to reduce a large <b>number</b> of ...", "dateLastCrawled": "2022-01-22T19:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Structured and Sparse Canonical Correlation Analysis as a Brain-Wide ...", "url": "https://adni.loni.usc.edu/adni-publications/Structured%20and%20Sparse%20Canonical%20Correlation%20Analysis%20as%20a%20Brain-Wide%20Multi-Modal%20Data%20Fusion%20Approach.pdf", "isFamilyFriendly": true, "displayUrl": "https://adni.loni.usc.edu/adni-publications/Structured and Sparse Canonical Correlation...", "snippet": "B. <b>Sparsity</b> Constraint Due to the small <b>number</b> of samples but high-dimensional <b>variables</b>/features in the neuroimaging datasets, (1) faces the over\ufb01tting problem. Therefore, sparse penalties such as l1-norm are imposed on w1 and w2 in the sCCA analysis [10]: max w1,w2 wT 1 X T Yw 2 s.t. wT 1 X T Xw 1= 1, wT 2 Y T Yw 2= 1, w 1 \u2264 c1, w2 \u2264 c2 (2)", "dateLastCrawled": "2022-02-01T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Selective Overview of Variable Selection in High Dimensional Feature ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3092303/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3092303", "snippet": "For instance, some prior knowledge may lead us to apply some grouping or transformation of the <b>input</b> <b>variables</b> (see, e.g., Fan and Lv (2008)). Some transformation of the <b>variables</b> may be appropriate if a significant portion of the pairwise correlations are high. In some cases, we may want to enlarge the feature space by adding interactions and higher order terms to reduce the bias of the model. <b>Sparsity</b> can also be viewed in the context of dimensionality reduction by introducing a sparse ...", "dateLastCrawled": "2022-01-19T16:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Applying stability selection to consistently estimate sparse principal ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4528629/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4528629", "snippet": "The <b>sparsity</b> index \u03b2 and the spike index \u03b1 define the <b>sparsity</b>, e.g. the <b>number</b> of truely non-zero coefficients, and the dominance of the signal, e.g. the eigenvalue, of the simulated first PC. Further, p and n denote the <b>number</b> of features and samples of the simulated data sets and nsim denotes the <b>number</b> of simulated data sets", "dateLastCrawled": "2021-12-28T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sparsity</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/sparsity", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>sparsity</b>", "snippet": "Ricardo Rendall, ... Marco Reis, in Computer Aided Chemical Engineering, 2016. 1 Introduction. <b>Sparsity</b> and collinearity are two pervasive characteristics commonly found in industrial and laboratory data sets that affect most data-driven methodologies. One such type of methodologies is the class of regression methods, focused on relating process <b>variables</b> (X) with continuous response <b>variables</b> (Y), for applications such as soft-sensor development, process monitoring, control and optimization.", "dateLastCrawled": "2022-01-25T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Homework #2", "url": "https://courses.cs.washington.edu/courses/csep546/21au/assignments/hw2.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.washington.edu/courses/csep546/21au/assignments/hw2.pdf", "snippet": "[2 points] Compared to L2 norm penalty, explain why a L1 norm penalty is more likely to result in <b>sparsity</b> (a larger <b>number</b> of 0s) in the weight vector. c. [2 points] In at most one sentence each, state one possible upside and one possible downside of using the following regularizer: P i |w i| 0.5 . d. [1 point] True or False: If the step-size for gradient descent is too large, it may not converge. e. [2 points] In your own words, describe why stochastic gradient descent (SGD) works, even ...", "dateLastCrawled": "2022-01-20T03:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Automatic <b>Sparsity</b> Detection Implemented as a Source-to-Source ...", "url": "http://www.fastopt.com/papers/gieringkaminski06.pdf", "isFamilyFriendly": true, "displayUrl": "www.fastopt.com/papers/gieringkaminski06.pdf", "snippet": "Depending on the <b>number</b> of independent and dependent <b>variables</b>, ASD is applied in forward or reverse mode to compute the Jacobian\u2019s <b>sparsity</b>. In the presence of a priori knowledge about the <b>sparsity</b> structure of a Jacobian on a block level, it is most e\ufb03cient to restrict ASD to a subset of all blocks. 2 Transformation Rules In this section we present the rules of transforming the function code into both types of ASD codes, the forward and the reverse one. To keep the notation simple, the ...", "dateLastCrawled": "2021-10-19T02:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "On the <b>Sparsity</b> of XORs in Approximate Model Counting", "url": "https://link.springer.com/chapter/10.1007%2F978-3-030-51825-7_18", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-51825-7_18", "snippet": "Given a Boolean formula \\(\\varphi \\), the problem of model counting, also referred to as #SAT, is to compute the <b>number</b> of solutions of \\(\\varphi \\).Model counting is a fundamental problem in computer science with a wide range of applications ranging from quantified information flow, reliability of networks, probabilistic programming, Bayesian networks, and others [4, 5, 10, 16, 21, 22, 23].Given the computational intractability of #SAT, attention has been focused on the approximation of # ...", "dateLastCrawled": "2021-12-07T18:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Variable selection for inferential models with relatively high ...", "url": "https://www.nature.com/articles/s41598-020-64829-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-020-64829-0", "snippet": "Variable selection in inferential modelling is problematic when the <b>number</b> of <b>variables</b> is large <b>relative</b> <b>to the number</b> of data points, especially when multicollinearity is present. A variety of ...", "dateLastCrawled": "2022-02-02T05:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Dimensionality Reduction 101 for Dummies</b> like Me | Evisi0n", "url": "https://evisionindia.wordpress.com/2019/10/01/dimensionality-reduction-101-for-dummies-like-me/", "isFamilyFriendly": true, "displayUrl": "https://evisionindia.wordpress.com/2019/10/01/<b>dimensionality-reduction-101-for-dummies</b>...", "snippet": "They are practical only when applied to a dataset with an already relatively low <b>number</b> <b>of input</b> columns. Figure 1. Accuracies of the best-performing models trained on well-known datasets that were reduced using the 10 selected data dimensionality reduction techniques. Linear Dimensionality Reduction Methods. The most common and well-known dimensionality reduction methods are the ones that apply linear transformations, like. Factor Analysis: This technique is used to reduce a large <b>number</b> of ...", "dateLastCrawled": "2022-01-22T19:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is there a <b>max number of explanatory variables you can include</b> in a ...", "url": "https://www.researchgate.net/post/Is-there-a-max-number-of-explanatory-variables-you-can-include-in-a-GLMM-based-on-the-N-of-study", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/Is-there-a-<b>max-number-of-explanatory-variables-you</b>...", "snippet": "I&#39;m currently running GLMMs in R and want to know if there is a <b>max number of explanatory variables you can include</b>, based on the N of study. For example, you can have 10 IVs for every 10 subjects.", "dateLastCrawled": "2022-01-29T23:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Neural correlates of sparse coding and dimensionality reduction - PLOS", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006908", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006908", "snippet": "A <b>number</b> of different definitions of <b>sparsity</b> <b>can</b> be found in the literature [8, 9], which <b>can</b> sometimes lead to controversy as to which codes <b>can</b> still be considered sparse . An extreme example is the so-called local code, in which each unique event, or \u201ccontext,\u201d is encoded by a single active neuron, or \u201cgrandmother cell\u201d [ 10 ] (illustrated in the left column of Fig 1A ).", "dateLastCrawled": "2021-02-18T13:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Predicting the Present with Bayesian Structural Time Series</b>", "url": "https://people.ischool.berkeley.edu/~hal/Papers/2013/pred-present-with-bsts.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.ischool.berkeley.edu/~hal/Papers/2013/pred-present-with-bsts.pdf", "snippet": "than the <b>number</b> <b>of observations</b> available to t the model), we induce <b>sparsity</b> by placing a spike-and-slab prior distribution on the regression coe cients. This leads to a posterior distribution with positive mass at zero for sets of regression coe cients, so that simulated values from the posterior distribution have many zeros. We use a Markov chain Monte Carlo (MCMC) sampling algorithm to simulate from the posterior distribution, which smooths our predictions over a large <b>number</b> of ...", "dateLastCrawled": "2022-01-27T18:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Principal component analysis: a review and recent developments", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4792409/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4792409", "snippet": "(a) Principal component analysis as an exploratory tool for data analysis. The standard context for PCA as an exploratory data analysis tool involves a dataset with <b>observations</b> on p numerical <b>variables</b>, for each of n entities or individuals. These data values define p n-dimensional vectors x 1,\u2026,x p or, equivalently, an n\u00d7p data matrix X, whose jth column is the vector x j <b>of observations</b> on the jth variable. We seek a linear combination of the columns of matrix X with maximum variance ...", "dateLastCrawled": "2022-02-02T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Compressed Sensing, Sparsity, and Dimensionality</b> in Neuronal ...", "url": "https://www.researchgate.net/publication/223964445_Compressed_Sensing_Sparsity_and_Dimensionality_in_Neuronal_Information_Processing_and_Data_Analysis", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/223964445_Compressed_Sensing_<b>Sparsity</b>_and...", "snippet": "Here, we used the participation ratio (PR), which finds the <b>number</b> of dimensions needed to explain about 80%-90% of the total variance in a population covariance matrix (D\u0105 browska et al., 2021 ...", "dateLastCrawled": "2022-01-28T05:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Variable selection in <b>social-environmental data</b>: sparse regression and ...", "url": "https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01183-9", "isFamilyFriendly": true, "displayUrl": "https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01183-9", "snippet": "If the <b>number</b> of clusters is small <b>relative</b> to n, clustered <b>variables</b> <b>can</b> be summarized into a single measure, and models <b>can</b> then be fit using multiple regression models. However, if a large <b>number</b> of clusters are present, a better choice is to use cluster membership to fit group lasso or sparse group lasso models. The sparse group lasso is particularly useful, as it has penalties at both the group and individual level, allowing for <b>sparsity</b> across and within groups", "dateLastCrawled": "2022-01-16T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Smarter Ways to Encode Categorical Data for Machine Learning | by Jeff ...", "url": "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine...", "snippet": "Sparse data is a matrix with lots of zeroes <b>relative</b> to other values. If your encoders transform your data so that it becomes sparse, some algorithms may not work well. <b>Sparsity</b> <b>can</b> often be managed by flagging it, but many algorithms don\u2019t work well unless the data is dense. Sparse Digging Into Category Encoders. Without further ado, let\u2019s encode! Ordinal. OrdinalEncoder converts each string value to a whole <b>number</b>. The first unique value in your column becomes 1, the second becomes 2 ...", "dateLastCrawled": "2022-01-30T19:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Sparsity</b>-Promoting Calibration for GRAPPA Accelerated Parallel ...", "url": "https://www.researchgate.net/publication/236180982_Sparsity-Promoting_Calibration_for_GRAPPA_Accelerated_Parallel_MRI_Reconstruction", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/236180982_<b>Sparsity</b>-Promoting_Calibration_for...", "snippet": "<b>relative</b> to the maximum singular value (one <b>can</b> also retain a fixed <b>number</b> of singular values), reducing the gain of the kernel. The nonlinear GRAPPA-operator-based", "dateLastCrawled": "2022-01-31T21:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Statistical Methods for Machine Learning</b> II Murat A. Erdogdu &amp; David ...", "url": "https://duvenaud.github.io/sta414/lecture3.pdf", "isFamilyFriendly": true, "displayUrl": "https://duvenaud.github.io/sta414/lecture3.pdf", "snippet": "variable x given a finite set <b>of observations</b>: \u2022We also assume that the data points are i.i.dabove. Need to determine given \u2022We will focus on the maximum likelihood estimation \u2022Remember the curve fitting example. \u2022Remember, the simplest linear model for regression: Key property: linear function of the parameters . \u2022However, it is also a linear function <b>of input</b> <b>variables</b>. where is a d-dimensional <b>input</b> vector (covariates). Linear Basis Function Models \u2022Remember, the simplest ...", "dateLastCrawled": "2021-09-15T02:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 8, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Kullback\u2013Leibler divergence</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Kullback\u2013Leibler_divergence</b>", "snippet": "Introduction and context. Consider two probability distributions and .Usually, represents the data, the <b>observations</b>, or a measured probability distribution. Distribution represents instead a theory, a model, a description or an approximation of .The <b>Kullback\u2013Leibler divergence</b> is then interpreted as the average difference of the <b>number</b> of bits required for encoding samples of using a code optimized for rather than one optimized for .Note that the roles of and <b>can</b> be reversed in some ...", "dateLastCrawled": "2022-02-07T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Variable Representation</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/variable-representation", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>variable-representation</b>", "snippet": "For a PPCA model with no noise in the observed <b>variables</b> x, we have seen that an <b>input</b> vector x <b>can</b> be projected into a reduced dimensional random vector z by computing the mean value of the posterior for the latent <b>variables</b> using z = E [h] = (W T W) \u2212 1 W T x. Appendix A.2 details the relationships between PCA, PPCA, the singular value decomposition and eigendecompositions, and shows that for mean-centered data the U matrix in (9.3) equals the matrix of eigenvectors \u03a6 of the ...", "dateLastCrawled": "2022-01-02T11:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An integer programming formulation to identify the sparse network ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2865861/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2865861", "snippet": "<b>Number</b> of <b>variables</b> to be optimized in the lower level is determined at the upper level as: . Upper level integer programming essentially reduces the <b>number</b> of <b>variables</b> to be optimized in the lower level, and as a consequence the estimation problem remains well posed even with a small <b>number</b> of experimental <b>observations</b>. An additional constraint is imposed on the least square minimization program to ensure that the <b>number</b> of estimated parameters does not exceed experimental data points:", "dateLastCrawled": "2017-01-10T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sparsity</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/sparsity", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>sparsity</b>", "snippet": "We discussed two tree-<b>sparsity</b>-based algorithms for CS-MRI and <b>compared</b> them with the state-of-the-art algorithms based on standard <b>sparsity</b>. In order to observe the benefit of tree <b>sparsity</b> more clearly, total variation terms were removed in all algorithms. Evaluation results demonstrated the practical improvement of the tree-<b>sparsity</b>-based algorithm on MR images. The results show that the benefit of the presented algorithm is greater than predicted by structured <b>sparsity</b> theory. That is ...", "dateLastCrawled": "2022-01-25T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Neural correlates of sparse coding and dimensionality reduction - PLOS", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006908", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006908", "snippet": "A <b>number</b> of different definitions of <b>sparsity</b> <b>can</b> be found in the literature [8, 9], which <b>can</b> sometimes lead to controversy as to which codes <b>can</b> still be considered sparse . An extreme example is the so-called local code, in which each unique event, or \u201ccontext,\u201d is encoded by a single active neuron, or \u201cgrandmother cell\u201d [ 10 ] (illustrated in the left column of Fig 1A ).", "dateLastCrawled": "2021-02-18T13:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Variable selection in <b>social-environmental data</b>: sparse regression and ...", "url": "https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01183-9", "isFamilyFriendly": true, "displayUrl": "https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01183-9", "snippet": "If the <b>number</b> of clusters is small <b>relative</b> to n, clustered <b>variables</b> <b>can</b> be summarized into a single measure, and models <b>can</b> then be fit using multiple regression models. However, if a large <b>number</b> of clusters are present, a better choice is to use cluster membership to fit group lasso or sparse group lasso models. The sparse group lasso is particularly useful, as it has penalties at both the group and individual level, allowing for <b>sparsity</b> across and within groups", "dateLastCrawled": "2022-01-16T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Choose a <b>Feature Selection</b> Method For Machine Learning", "url": "https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>feature-selection</b>-with-real-and-categorical-data", "snippet": "<b>Feature selection</b> is the process of reducing the <b>number</b> <b>of input</b> <b>variables</b> when developing a predictive model. It is desirable to reduce the <b>number</b> <b>of input</b> <b>variables</b> to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model. Statistical-based <b>feature selection</b> methods involve evaluating the relationship between each <b>input</b> variable and the target variable", "dateLastCrawled": "2022-02-02T18:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>Sparsity</b> Algorithm with Applications to Corporate Credit Rating | DeepAI", "url": "https://deepai.org/publication/a-sparsity-algorithm-with-applications-to-corporate-credit-rating", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-<b>sparsity</b>-algorithm-with-applications-to-corporate...", "snippet": "A counterfactual explanation of a particular &quot;black box&quot; attempts to find the smallest change to the <b>input</b> values that modifies the prediction to a particular output, other than the original one. In this work we formulate the problem of finding a counterfactual explanation as an optimization problem. We propose a new &quot;<b>sparsity</b> algorithm&quot; which solves the optimization problem, while also maximizing the <b>sparsity</b> of the counterfactual explanation. We apply the <b>sparsity</b> algorithm to provide a ...", "dateLastCrawled": "2021-12-19T06:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Sparsity</b> with sign-coherent groups of <b>variables</b> via the cooperative-Lasso", "url": "https://www.math.univ-toulouse.fr/jeunes_mathematiciennes/presentations/charbonnier.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.math.univ-toulouse.fr/jeunes_mathematiciennes/presentations/charbonnier.pdf", "snippet": "<b>number</b> of <b>variables</b> pmuch smaller than <b>number</b> <b>of observations</b> n cooperative-Lasso 2. What do we look for and how <b>can</b> we hope for an answer? Objectives of microarray experiments IDi erential analysis I Gene signatures IClustering of co-expressed genes I Gene regulation networks Hope for an answer ITrue phenomenon is a much smaller dimension I <b>Sparsity</b> ITrue phenomenon follows a certain logic I Structure cooperative-Lasso 3. Motivation (1): Robust gene signature Breast Cancer Data I22 269 ...", "dateLastCrawled": "2021-11-20T17:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Fit linear regression model to high-dimensional data - MATLAB ...", "url": "https://in.mathworks.com/help/stats/fitrlinear.html", "isFamilyFriendly": true, "displayUrl": "https://in.mathworks.com/help/stats/fitrlinear.html", "snippet": "p is the <b>number</b> of predictor <b>variables</b> in X and L is the <b>number</b> of regularization-strength values (for more details, see Lambda). If you specify a p-dimensional vector, then the software optimizes the objective function L times using this process. The software optimizes using Beta as the initial value and the minimum value of Lambda as the regularization strength. The software optimizes again using the resulting estimate from the previous optimization as a warm start, and the next smallest ...", "dateLastCrawled": "2022-01-25T18:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Variable selection for inferential models with relatively high ...", "url": "https://www.nature.com/articles/s41598-020-64829-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-020-64829-0", "snippet": "Variable selection in inferential modelling is problematic when the <b>number</b> of <b>variables</b> is large <b>relative</b> <b>to the number</b> of data points, especially when multicollinearity is present. A variety of ...", "dateLastCrawled": "2022-02-02T05:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "regression - <b>Modelling with more variables than data points</b> - Cross ...", "url": "https://stats.stackexchange.com/questions/223486/modelling-with-more-variables-than-data-points", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/223486/<b>modelling-with-more-variables-than</b>...", "snippet": "They use the law of parsimony, or Ockham&#39;s razor, for which if two models explain the observation which the same precision, it <b>can</b> be wisest to choose the more compact in terms of, for instance, the <b>number</b> of free parameters. One does not really &quot;explain&quot; useful relationship between <b>variables</b> with too involved models.", "dateLastCrawled": "2022-01-28T07:39:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Sparsity</b> is an essential feature of many contemporary data problems. Remote sensing, various forms of automated screening and other high throughput measurement devices collect a large amount of ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Regularization \u2014 Understanding L1 and L2 regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2...", "snippet": "The <b>sparsity</b> feature used in L1 regularization has been used extensively as a feature selection mechanism in <b>machine</b> <b>learning</b>. Feature selection is a mechanism which inherently simplifies a ...", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "An E\ufb03cient Sparse Metric <b>Learning</b> in High ... - <b>Machine</b> <b>Learning</b>", "url": "http://machinelearning.org/archive/icml2009/papers/46.pdf", "isFamilyFriendly": true, "displayUrl": "<b>machinelearning</b>.org/archive/icml2009/papers/46.pdf", "snippet": "This <b>sparsity</b> prior of <b>learning</b> distance metric serves to regularize the com-plexity of the distance model especially in the \u201cless example number p and high dimension d\u201d setting. Theoretically, by <b>analogy</b> to the covariance estimation problem, we \ufb01nd the proposed distance <b>learning</b> algorithm has a consistent result at rate O!&quot;# m2 logd $% n &amp; to the target distance matrix with at most m nonzeros per row. Moreover, from the imple-mentation perspective, this! 1-penalized log-determinant ...", "dateLastCrawled": "2021-11-19T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Discovering governing equations from data</b> by sparse identification of ...", "url": "https://www.pnas.org/content/pnas/113/15/3932.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/<b>pnas</b>/113/15/3932.full.pdf", "snippet": "examples. In this work, we combin e <b>sparsity</b>-promoting techniques and <b>machine</b> <b>learning</b> with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only as-sumption about the structureof the model is that there are onlya few important terms that govern the dy namics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to ...", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> in Medical Imaging", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4220564/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4220564", "snippet": "SUPERVISED <b>LEARNING</b>. In <b>machine</b> <b>learning</b>, one often seeks to predict an output variable y based on a vector x of input variables. To accomplish this, it is assumed that the input and output approximately obey a functional relationship y=f (x), called the predictive model, as shown in Figure 1.In supervised <b>learning</b>, the predictive model is discovered with the benefit of training data consisting of examples for which both x and y are known. We will denote these available pairs of examples as ...", "dateLastCrawled": "2022-02-03T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "What are the <b>basic concepts in machine learning</b>? I found that the best way to discover and get a handle on the <b>basic concepts in machine learning</b> is to review the introduction chapters to <b>machine learning</b> textbooks and to watch the videos from the first model in online courses. Pedro Domingos is a lecturer and professor on <b>machine learning</b> at the University of Washing and", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Dynamical <b>machine</b> <b>learning</b> volumetric reconstruction of objects ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8027224/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8027224", "snippet": "The sequence index in the angle of illumination plays the role of discrete time in the dynamical system <b>analogy</b>. Thus, the imaging problem turns into a problem of nonlinear system identification, which also suggests dynamical <b>learning</b> as a better fit to regularize the reconstructions. We devised a Recurrent Neural Network (RNN) architecture with a novel Separable-Convolution Gated Recurrent Unit (SC-GRU) as the fundamental building block. Through a comprehensive comparison of several ...", "dateLastCrawled": "2022-01-08T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Dynamical machine learning volumetric reconstruction of</b> objects ...", "url": "https://www.nature.com/articles/s41377-021-00512-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41377-021-00512-x", "snippet": "Recently, thanks to a ground-breaking observation from 2010 that <b>sparsity</b> can be learnt by a deep neural network 48, the idea of using <b>machine</b> <b>learning</b> to approximate solutions to inverse problems ...", "dateLastCrawled": "2022-02-02T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "regression - Why L1 norm for sparse models - Cross Validated", "url": "https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/45643", "snippet": "There are many norms that lead to <b>sparsity</b> (e.g., as you mentioned, any Lp norm with p &lt;= 1). In general, any norm with a sharp corner at zero induces <b>sparsity</b>. So, going back to the original question - the L1 norm induces <b>sparsity</b> by having a discontinuous gradient at zero (and any other penalty with this property will do so too). $\\endgroup$", "dateLastCrawled": "2022-01-26T23:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "The result is a <b>learning</b> model that may result in generally better word embeddings. GloVe, is a new global log-bilinear regression model for the unsupervised <b>learning</b> of word representations that outperforms other models on word <b>analogy</b>, word similarity, and named entity recognition tasks. \u2014 GloVe: Global Vectors for Word Representation, 2014.", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Learning Neural Representations for Network Anomaly Detection</b>", "url": "https://www.researchgate.net/publication/325797465_Learning_Neural_Representations_for_Network_Anomaly_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/325797465_<b>Learning</b>_Neural_Representations_for...", "snippet": "Many <b>machine</b> <b>learning</b> algorithms have been. Manuscript received December 22, 2017; revised March 13, 2018. This. work is funded by Vietnam International Education De velopment (VIED) and. by ...", "dateLastCrawled": "2021-12-06T22:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Self-representation based dual-graph regularized <b>feature selection</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation ... Her current research interests include pattern recognition and <b>machine</b> <b>learning</b>. Licheng Jiao (SM\u05f389) received the B.S. degree from Shanghai Jiaotong University, Shanghai, China, in 1982, the M.S. and Ph.D. degrees from Xi\u05f3an Jiaotong University, Xi\u05f3an, China, in 1984 and 1990, respectively. From 1990 to 1991, he was a postdoctoral Fellow in the National Key Laboratory for Radar Signal ...", "dateLastCrawled": "2021-11-22T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Self-representation based dual-graph regularized feature selection ...", "url": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "snippet": "<b>machine</b> <b>learning</b> and computer vision \ufb01elds [41]. <b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation [41]. Taking into account of manifold <b>learning</b> and feature selection, and inspired by the self-representation property and the idea of dual-regularization <b>learning</b> [44,45], we propose a novel feature selection algorithm for clustering, named self-representation based dual-graph regularized feature selection clustering (DFSC). This algorithm ...", "dateLastCrawled": "2022-02-02T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Unsupervised feature selection</b> by <b>regularized self-representation</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation. With the above considerations, in this paper we propose a simple yet very effective <b>unsupervised feature selection</b> method by exploiting the self-representation ability of features. The feature matrix is represented over itself to find the representative feature components. The representation residual is minimized by L 2, 1-norm loss to reduce the effect of outlier samples. Different from the ...", "dateLastCrawled": "2022-01-24T12:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Talk <b>Archive</b> - Research on Algorithms and Incentives in Networks", "url": "https://rain.stanford.edu/schedule/archive.shtml", "isFamilyFriendly": true, "displayUrl": "https://rain.stanford.edu/schedule/<b>archive</b>.shtml", "snippet": "McFowland\u2019s research interests\u2014which lie at the intersection of Information Systems, <b>Machine</b> <b>Learning</b>, and Public Policy\u2014include the development of computationally efficient algorithms for large-scale statistical <b>machine</b> <b>learning</b> and \u201cbig data\u201d analytics. More specifically, his research seeks to demonstrate that many real-world problems faced by organizations, and society more broadly, can be reduced to the tasks of anomalous pattern detection and discovery. As a data and ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Talks - <b>sites.google.com</b>", "url": "https://sites.google.com/view/dssseminarseries/talks", "isFamilyFriendly": true, "displayUrl": "https://<b>sites.google.com</b>/view/dssseminarseries/talks", "snippet": "Abstracts &amp; Bios for upcoming talks", "dateLastCrawled": "2022-01-27T14:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Sparse representations for text categorization</b>", "url": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text_categorization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text...", "snippet": "<b>Machine</b> <b>learning</b> for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is ...", "dateLastCrawled": "2021-12-10T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Continual Learning via Neural Pruning</b> | DeepAI", "url": "https://deepai.org/publication/continual-learning-via-neural-pruning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>continual-learning-via-neural-pruning</b>", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more efficient use of resources in machines with memory constraints.", "dateLastCrawled": "2021-12-30T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Sparse Representations for Text Categorization</b> | Dimitri Kanevsky ...", "url": "https://www.academia.edu/2738730/Sparse_Representations_for_Text_Categorization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2738730/<b>Sparse_Representations_for_Text_Categorization</b>", "snippet": "A Comparative Study of <b>Machine</b> <b>Learning</b> Methods for Verbal Autopsy Text Classification. By Eric S Atwell and Samuel Danso. CSC435 book proposal. By Russell Frith. Higher-Order Smoothing: A Novel Semantic Smoothing Method for Text Classification. By Murat C Ganiz, Mitat Poyraz, and Zeynep Kilimci. INFORMATION RETRIEVAL. By febi k. Introduction to information retrieval. By Valeria Mesi. Download pdf. \u00d7 Close Log In. Log In with Facebook Log In with Google. Sign Up with Apple. or. Email ...", "dateLastCrawled": "2021-10-13T23:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Non-negative data-<b>driven mapping of structural connections</b> with ...", "url": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "snippet": "For ICA, <b>sparsity can be thought of as</b> a proxy for independence. 3.5. In-vivo data decompositions. For real data, we decomposed group-average tractography matrices, using independent component analysis (ICA) and non-negative matrix factorisation (NMF), with a range of model orders K. ICA was initialised with regular PCA, in which the first 500 components were retained (explaining 97% of the total variance). ICA was applied to the reduced dataset using the FastICA algorithm (Hyv\u00e4rinen and ...", "dateLastCrawled": "2021-10-11T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Continual <b>Learning</b> via Neural Pruning \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1903.04476/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1903.04476", "snippet": "We introduce Continual <b>Learning</b> via Neural Pruning (CLNP), a new method aimed at lifelong <b>learning</b> in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the ...", "dateLastCrawled": "2021-11-07T10:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Continual <b>Learning</b> via Neural Pruning", "url": "https://openreview.net/pdf?id=Hyl_XXYLIB", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=Hyl_XXYLIB", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much at-tention from the <b>machine</b> <b>learning</b> community in recent years. The main obstacle for effective continual <b>learning</b> is the problem of cata-strophic forgetting: machines trained on new problems forget about", "dateLastCrawled": "2022-01-05T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Abstract - arXiv.org e-Print archive", "url": "https://arxiv.org/pdf/1903.04476", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1903.04476", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more ef\ufb01cient use of resources in machines with memory constraints. There is also great interest in continual <b>learning</b> from a more long term ...", "dateLastCrawled": "2021-10-25T14:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Introduction to compressed sensing</b>", "url": "https://www.researchgate.net/publication/220043734_Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220043734_<b>Introduction_to_compressed_sensing</b>", "snippet": "systems control, clustering, and <b>machine</b> <b>learning</b> [14, 15, 58, 61, 89, 193, 217, 240, 244]. Low-dimensional manifolds hav e also been prop osed as approximate mod-", "dateLastCrawled": "2022-01-14T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Introduction to compressed sensing</b> | Marco Duarte - Academia.edu", "url": "https://www.academia.edu/1443164/Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/1443164/<b>Introduction_to_compressed_sensing</b>", "snippet": "<b>Introduction to Compressed Sensing</b> For any x \u2208 \u03a3k , we can associate a k-face of C n with the support and sign pattern of x. One can show that the number of k-faces of AC n is precisely the number of index sets of size k for which signals supported on them can be recovered by (1.12) with B (y) = {z : Az = y}.", "dateLastCrawled": "2022-01-21T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Compressed Sensing : Theory and Applications</b> | Kutyniok, Gitta Eldar ...", "url": "https://b-ok.africa/book/2086657/84a688", "isFamilyFriendly": true, "displayUrl": "https://b-ok.africa/book/2086657/84a688", "snippet": "You can write a book review and share your experiences. Other readers will always be interested in your opinion of the books you&#39;ve read. Whether you&#39;ve loved the book or not, if you give your honest and detailed thoughts then people will find new books that are right for them.", "dateLastCrawled": "2021-12-26T07:22:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparsity)  is like +(number of input variables relative to the number of observations)", "+(sparsity) is similar to +(number of input variables relative to the number of observations)", "+(sparsity) can be thought of as +(number of input variables relative to the number of observations)", "+(sparsity) can be compared to +(number of input variables relative to the number of observations)", "machine learning +(sparsity AND analogy)", "machine learning +(\"sparsity is like\")", "machine learning +(\"sparsity is similar\")", "machine learning +(\"just as sparsity\")", "machine learning +(\"sparsity can be thought of as\")", "machine learning +(\"sparsity can be compared to\")"]}