{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Here\u2019s The Truth, GANs <b>Easily Can Fool Intrusion Detection Systems</b>", "url": "https://analyticsindiamag.com/heres-the-truth-gans-easily-can-fool-intrusion-detection-systems/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/heres-the-truth-gans-<b>easily-can-fool-intrusion-detection</b>...", "snippet": "The researchers use the <b>Wasserstein</b> <b>loss</b> hence the architecture called <b>Wasserstein</b> GAN. In the case of IDSGAN, the generator changes some features that are very specific to produce generate adversarial traffic data for an attack on IDS. The discriminator is trained to copy the <b>black</b>-<b>box</b> IDS and help and guide with the generator training. The researchers say in the research", "dateLastCrawled": "2022-01-26T13:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to Develop a <b>Wasserstein Generative Adversarial Network</b> (WGAN) From ...", "url": "https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-code-a-<b>wasserstein</b>-generative-adversarial...", "snippet": "The <b>Wasserstein Generative Adversarial Network</b>, or <b>Wasserstein</b> GAN, is an extension to the generative adversarial network that both improves the stability when training the model and provides a <b>loss</b> function that correlates with the quality of generated images. The development of the WGAN has a dense mathematical motivation, although in practice requires only a few minor modifications to the established standard", "dateLastCrawled": "2022-01-31T23:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Gamifying optimization: a <b>Wasserstein</b> distance-based analysis of human ...", "url": "https://www.researchgate.net/publication/357014084_Gamifying_optimization_a_Wasserstein_distance-based_analysis_of_human_search", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357014084_Gamifying_optimization_a...", "snippet": "The main objective of this paper is to outline a theoretical framework to characterise humans&#39; decision-<b>making</b> strategies under uncertainty, in particular active learning in <b>a black</b>-<b>box</b> ...", "dateLastCrawled": "2022-01-30T06:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Book - NIPS", "url": "https://papers.nips.cc/paper/2015", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2015", "snippet": "Local Expectation Gradients for <b>Black</b> <b>Box</b> Variational Inference Michalis Titsias RC AUEB, ... Learning with a <b>Wasserstein</b> <b>Loss</b> Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, Tomaso A. Poggio; Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks Emily L. Denton, Soumith Chintala, arthur szlam, Rob Fergus; Estimating Jaccard Index with Missing Observations: A Matrix Calibration Approach Wenye Li; On Top-k Selection in Multi-Armed Bandits and Hidden ...", "dateLastCrawled": "2022-02-02T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to <b>Train a Progressive Growing GAN in Keras for Synthesizing Faces</b>", "url": "https://machinelearningmastery.com/how-to-train-a-progressive-growing-gan-in-keras-for-synthesizing-faces/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-<b>train-a-progressive-growing-gan-in-keras</b>-for...", "snippet": "In this case, we will use <b>Wasserstein</b> <b>loss</b> (or WGAN <b>loss</b>) and the Adam version of stochastic gradient descent configured as is specified in the paper. The authors of the paper recommend exploring using both WGAN-GP <b>loss</b> and least squares <b>loss</b> and found that the former performed slightly better. Nevertheless, we will use <b>Wasserstein</b> <b>loss</b> as it greatly simplifies the implementation.", "dateLastCrawled": "2022-02-02T01:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Wasserstein</b> Index Generation Model: Automatic Generation of Time-series ...", "url": "https://www.arxiv-vanity.com/papers/1908.04369/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.04369", "snippet": "<b>Wasserstein</b> Index Generation Model: Automatic Generation of Time-series Index with Application to Economic Policy Uncertainty. Fangzhou Xie 1 Department of Economics, New York University 1 1 Present Mailing Address: 30 River Ct, Apt 2308, Jersey City, NJ, 07310. January 25, 2021. Abstract. I propose a novel method, called the <b>Wasserstein</b> Index Generation model (WIG), to generate public sentiment index automatically. It can be performed off-the-shelf and is especially good at detecting sudden ...", "dateLastCrawled": "2021-07-23T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "On <b>the Latent Space of Wasserstein Auto-Encoders</b> | DeepAI", "url": "https://deepai.org/publication/on-the-latent-space-of-wasserstein-auto-encoders", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-<b>the-latent-space-of-wasserstein-auto-encoders</b>", "snippet": "Only a <b>small</b> fraction of the total volume of the latent space is covered by the deterministic encoder. Hence the decoder is only trained on this <b>small</b> fraction, because under the objective (1) the decoder learns to act only on the encoded training images. While it appears in this 2-dimensional toy example that the quality of decoder samples is ...", "dateLastCrawled": "2021-12-09T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "New submissions for Thu, 3 Jun 21 \u00b7 Issue #72 \u00b7 zoq/arxiv ... - <b>GitHub</b>", "url": "https://github.com/zoq/arxiv-updates/issues/72", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zoq/arxiv-updates/issues/72", "snippet": "We consider a general task called partial <b>Wasserstein</b> covering with the goal of emulating a large dataset (e.g., application dataset) using a <b>small</b> dataset (e.g., development dataset) in terms of the empirical distribution by selecting a <b>small</b> subset from a candidate dataset and adding it to the <b>small</b> dataset. We model this task as a discrete optimization problem with partial <b>Wasserstein</b> divergence as an objective function. Although this problem is NP-hard, we prove that it has the ...", "dateLastCrawled": "2021-09-01T02:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[R] <b>Wasserstein</b> Reinforcement Learning : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/c367tk/r_wasserstein_reinforcement_learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/c367tk/r_<b>wasserstein</b>_reinforcement...", "snippet": "Originally, amplifiers were just about <b>making</b> the guitar louder, but it&#39;s become more than that\u2013it&#39;s about manipulating the character of the audio to make it sound more pleasing or achieve some artistic effect. What makes vacuum tubes so attractive is the peculiar non-linearities they introduce to the incoming signal as a byproduct of their interactions with the rest of the circuit when they&#39;re pushed to the limits of their operating ranges. Or \u201cdistortion\u201d for short. If you push a ...", "dateLastCrawled": "2021-01-31T09:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Decrappification, DeOldification, and Super Resolution \u00b7 fast.ai", "url": "https://www.fast.ai/2019/05/03/decrappify/", "isFamilyFriendly": true, "displayUrl": "https://www.fast.ai/2019/05/03/decrappify", "snippet": "This stability <b>loss</b> measure encourages images with stable features in the face of <b>small</b> amounts of random noise. Noise injection is already part of the process to create training sets at Salk anyways - so this was an easy modification. This stability when combined with information about the preceding and following frames of video significantly reduces flicker and improves the quality of output when processing low resolution movies. See more details about the process in the section below ...", "dateLastCrawled": "2022-02-03T10:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Develop a <b>Wasserstein Generative Adversarial Network</b> (WGAN) From ...", "url": "https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-code-a-<b>wasserstein</b>-generative-adversarial...", "snippet": "The <b>Wasserstein Generative Adversarial Network</b>, or <b>Wasserstein</b> GAN, is an extension to the generative adversarial network that both improves the stability when training the model and provides a <b>loss</b> function that correlates with the quality of generated images. The development of the WGAN has a dense mathematical motivation, although in practice requires only a few minor modifications to the established standard", "dateLastCrawled": "2022-01-31T23:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) A <b>Simulated Annealing based Inexact Oracle for Wasserstein</b> <b>Loss</b> ...", "url": "https://www.researchgate.net/publication/306187169_A_Simulated_Annealing_based_Inexact_Oracle_for_Wasserstein_Loss_Minimization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/306187169_A_Simulated_Annealing_based_Inexact...", "snippet": "Learning under a <b>Wasserstein</b> <b>loss</b> is an emerging research topic. We call collectively the problems formulated under this framework <b>Wasserstein</b> <b>Loss</b> Minimization (WLM).", "dateLastCrawled": "2022-01-19T08:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Sumit Mukherjee, Yixi Xu, Anusua Trivedi, Nabajyoti Patowary, and Juan ...", "url": "https://petsymposium.org/2021/files/papers/issue3/paper16-2021-3-source.pdf", "isFamilyFriendly": true, "displayUrl": "https://petsymposium.org/2021/files/papers/issue3/paper16-2021-3-source.pdf", "snippet": "dropout layers [26] and <b>Wasserstein</b> <b>loss</b> [27]. Unlike these techniques, the privGAN architecture is explicitly designed to maximize the privacy/utility trade-o\ufb00, which is quantitatively demonstrated in Section6. 3 priv(ate)GANs In this section, we will motivate and introduce privGANs. In addition, we will provide the theoretical", "dateLastCrawled": "2021-11-05T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Wasserstein</b> Index Generation Model: Automatic Generation of Time-series ...", "url": "https://www.arxiv-vanity.com/papers/1908.04369/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.04369", "snippet": "<b>Wasserstein</b> Index Generation Model: Automatic Generation of Time-series Index with Application to Economic Policy Uncertainty. Fangzhou Xie 1 Department of Economics, New York University 1 1 Present Mailing Address: 30 River Ct, Apt 2308, Jersey City, NJ, 07310. January 25, 2021. Abstract . I propose a novel method, called the <b>Wasserstein</b> Index Generation model (WIG), to generate public sentiment index automatically. It can be performed off-the-shelf and is especially good at detecting ...", "dateLastCrawled": "2021-07-23T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "On <b>the Latent Space of Wasserstein Auto-Encoders</b> | DeepAI", "url": "https://deepai.org/publication/on-the-latent-space-of-wasserstein-auto-encoders", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-<b>the-latent-space-of-wasserstein-auto-encoders</b>", "snippet": "Only a <b>small</b> fraction of the total volume of the latent space is covered by the deterministic encoder. Hence the decoder is only trained on this <b>small</b> fraction, because under the objective (1) the decoder learns to act only on the encoded training images. While it appears in this 2-dimensional toy example that the quality of decoder samples is ...", "dateLastCrawled": "2021-12-09T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Using <b>Wasserstein-2 regularization to ensure fair decisions</b> with Neural ...", "url": "https://deepai.org/publication/using-wasserstein-2-regularization-to-ensure-fair-decisions-with-neural-network-classifiers", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/using-<b>wasserstein-2-regularization-to-ensure</b>-fair...", "snippet": "<b>Wasserstein</b> type distances and optimal transport techniques have grown popular in the recent years (see in or for theoretical and pratical issues on <b>Wasserstein</b> distance) and are widely used in the machine learning and faire learning literature (see in , or for instance). In very particular we will impose closeness with respect to quadratic transportation cost (i.e <b>Wasserstein</b>-2 distance) of the score used to build an automatic decision rule built with a deep neural network acting as a ...", "dateLastCrawled": "2021-12-22T15:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>WASSERSTEIN</b> AUTO LATENT DIMENSIONALITY AND RANDOM ENCODERS", "url": "https://openreview.net/pdf?id=r157GIJvz", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=r157GIJvz", "snippet": "The objective (1) <b>is similar</b> to that of the VAE and has two terms. The \ufb01rst reconstruction term aligns the encoder-decoder pair so that the encoded images can be accurately reconstructed by the decoder as measured by the cost function c(e.g. l 2 or cross-entropy <b>loss</b>). The second regularization term is different from VAEs: it forces the aggregated posterior Q Z to match the prior distribution P Z rather than asking point-wise posteriors Q(ZjX= x) to match P Z simultaneously for all data ...", "dateLastCrawled": "2021-12-27T10:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "New submissions for Thu, 3 Jun 21 \u00b7 Issue #72 \u00b7 zoq/arxiv ... - <b>GitHub</b>", "url": "https://github.com/zoq/arxiv-updates/issues/72", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zoq/arxiv-updates/issues/72", "snippet": "We consider a general task called partial <b>Wasserstein</b> covering with the goal of emulating a large dataset (e.g., application dataset) using a <b>small</b> dataset (e.g., development dataset) in terms of the empirical distribution by selecting a <b>small</b> subset from a candidate dataset and adding it to the <b>small</b> dataset. We model this task as a discrete optimization problem with partial <b>Wasserstein</b> divergence as an objective function. Although this problem is NP-hard, we prove that it has the ...", "dateLastCrawled": "2021-09-01T02:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[R] <b>Wasserstein</b> Reinforcement Learning : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/c367tk/r_wasserstein_reinforcement_learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/c367tk/r_<b>wasserstein</b>_reinforcement...", "snippet": "Originally, amplifiers were just about <b>making</b> the guitar louder, but it&#39;s become more than that\u2013it&#39;s about manipulating the character of the audio to make it sound more pleasing or achieve some artistic effect. What makes vacuum tubes so attractive is the peculiar non-linearities they introduce to the incoming signal as a byproduct of their interactions with the rest of the circuit when they&#39;re pushed to the limits of their operating ranges. Or \u201cdistortion\u201d for short. If you push a ...", "dateLastCrawled": "2021-01-31T09:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Analysis of Application Examples of Differential Privacy in Deep Learning", "url": "https://www.hindawi.com/journals/cin/2021/4244040/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/cin/2021/4244040", "snippet": "An attack against <b>a black</b>-<b>box</b> model has been defined in which exploits the statistical difference between the model\u2019s predictions on the training set and unseen data. White-<b>box</b> attacks: for any input x , besides the output, the attacker can also obtain the structure and parameters of the model and observe the intermediate calculation steps in the hidden layer.", "dateLastCrawled": "2022-01-29T08:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Develop a <b>Wasserstein Generative Adversarial Network</b> (WGAN) From ...", "url": "https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-code-a-<b>wasserstein</b>-generative-adversarial...", "snippet": "We <b>can</b> implement the <b>Wasserstein</b> <b>loss</b> as a custom function in Keras that calculates the average score for real or fake images. The score is maximizing for real examples and minimizing for fake examples. Given that stochastic gradient descent is a minimization algorithm, we <b>can</b> multiply the class label by the mean score (e.g. -1 for real and 1 for fake which as no effect), which ensures that the <b>loss</b> for real and fake images is minimizing to the network. An efficient implementation of this ...", "dateLastCrawled": "2022-01-31T23:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Wasserstein</b> Variational Inference - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/1805.11284/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1805.11284", "snippet": "This paper introduces <b>Wasserstein</b> variational inference, a new form of approximate Bayesian inference based on optimal transport theory. <b>Wasserstein</b> variational inference uses a new family of divergences that includes both f-divergences and the <b>Wasserstein</b> distance as special cases. The gradients of the <b>Wasserstein</b> variational <b>loss</b> are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that <b>can</b> be used with ...", "dateLastCrawled": "2021-11-15T17:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Gamifying optimization: a <b>Wasserstein</b> distance-based analysis of human ...", "url": "https://www.researchgate.net/publication/357014084_Gamifying_optimization_a_Wasserstein_distance-based_analysis_of_human_search", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357014084_Gamifying_optimization_a...", "snippet": "The main objective of this paper is to outline a theoretical framework to characterise humans&#39; decision-<b>making</b> strategies under uncertainty, in particular active learning in <b>a black</b>-<b>box</b> ...", "dateLastCrawled": "2022-01-30T06:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Wasserstein Barycentric Coordinates: Histogram Regression Using</b> Optimal ...", "url": "https://www.researchgate.net/publication/301583875_Wasserstein_Barycentric_Coordinates_Histogram_Regression_Using_Optimal_Transport", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/301583875_<b>Wasserstein</b>_Barycentric_Coordinates...", "snippet": "This step <b>can</b> <b>be thought</b> of as a &quot;projection&quot; of each node connectivity descriptor in the low dimensional space and it is achieved by learning its optimal barycentric coordinates [5] in that space ...", "dateLastCrawled": "2022-01-21T22:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Anomaly detection with Wasserstein GAN</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1812.02463/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1812.02463", "snippet": "The generative model <b>can</b> <b>be thought</b> of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistinguishable from the genuine articles. Even if this framework <b>can</b> be applied to many kinds of models, the approach is mostly performed with two neural ...", "dateLastCrawled": "2022-02-03T19:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding Attention-Deficit/Hyperactivity Disorder From Childhood ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3724232/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3724232", "snippet": "The disorder is <b>thought</b> to result from a combination of <b>small</b> effects from a number of genes (polygenetic). Some of the candidate genes that have been identified thus far relate to synthesis, packaging, release, detection and recycling of dopamine or catecholamines including the post-synaptic DRD4, dopamine transporter, and SNAP 25 genes; as well as others related to other neurotransmitters such as serotonin. Clearly, more work is necessary in disentangling the relationship of candidate ...", "dateLastCrawled": "2022-02-02T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Many Paths to Equilibrium: GANs Do <b>Not Need to Decrease a Divergence</b> At ...", "url": "https://deepai.org/publication/many-paths-to-equilibrium-gans-do-not-need-to-decrease-a-divergence-at-every-step", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/many-paths-to-equilibrium-gans-do-not-need-to-decrease...", "snippet": "Looking at the 5 worst values (the <b>black</b> dots) in a hyperparameter sweep according to sample diversity and negative <b>Wasserstein</b> distance as estimated by an Independent <b>Wasserstein</b> critic, we see that these metrics are able to capture the two examples of model collapse that we have seen when training DRGAN-NS on CelebA, as shown in Figure 21. For sample diversity, the worst results are computed by the biggest absolute difference to the reference point (test set diversity), while for negative ...", "dateLastCrawled": "2021-12-09T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[R] <b>Wasserstein</b> Reinforcement Learning : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/c367tk/r_wasserstein_reinforcement_learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/c367tk/r_<b>wasserstein</b>_reinforcement...", "snippet": "Originally, amplifiers were just about <b>making</b> the guitar louder, but it&#39;s become more than that\u2013it&#39;s about manipulating the character of the audio to make it sound more pleasing or achieve some artistic effect. What makes vacuum tubes so attractive is the peculiar non-linearities they introduce to the incoming signal as a byproduct of their interactions with the rest of the circuit when they&#39;re pushed to the limits of their operating ranges. Or \u201cdistortion\u201d for short. If you push a ...", "dateLastCrawled": "2021-01-31T09:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Attacks on state-of-the-art <b>face recognition</b> using attentional ...", "url": "https://link.springer.com/article/10.1007/s11042-020-09604-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11042-020-09604-z", "snippet": "Feature estimation is a crucial <b>thought</b> in <b>black</b>-<b>box</b> attack. The generalization of the generator trained in the white-<b>box</b> scenario <b>can</b> not satisfy the performance of the <b>black</b>-<b>box</b> attack. It <b>can</b> not attack networks in <b>black</b>-<b>box</b> well shown in Table 7. Thus we present feature estimation in the <b>black</b>-<b>box</b> scenario which <b>can</b> estimate the feature ...", "dateLastCrawled": "2022-02-02T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - adeshpande3/<b>Machine-Learning-Links-And-Lessons-Learned</b>: List ...", "url": "https://github.com/adeshpande3/Machine-Learning-Links-And-Lessons-Learned", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/adeshpande3/<b>Machine-Learning-Links-And-Lessons-Learned</b>", "snippet": "It&#39;s interesting and worrying that the process in which neural nets and deep learning work is still such <b>a black</b> <b>box</b>. If an algorithm comes to the conclusion that a number is let&#39;s say classified as a 7, we don&#39;t really know how the algorithm came to that result because it&#39;s not hardcoded anywhere. It&#39;s hidden behind millions of gradients and derivatives. So if we wanted to use deep learning in a more important field like medicine, the doctor who is using this technology should be able to ...", "dateLastCrawled": "2022-01-19T16:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Wasserstein-Distance-Based Gaussian Mixture Reduction</b> | Request PDF", "url": "https://www.researchgate.net/publication/327131655_Wasserstein-Distance-Based_Gaussian_Mixture_Reduction", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/327131655_<b>Wasserstein</b>-Distance-Based_Gaussian...", "snippet": "The <b>Wasserstein</b> <b>loss</b> <b>can</b> encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the ...", "dateLastCrawled": "2021-11-12T08:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Wasserstein</b> GAN With Quadratic Transport Cost | Request PDF", "url": "https://www.researchgate.net/publication/339561853_Wasserstein_GAN_With_Quadratic_Transport_Cost", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/339561853_<b>Wasserstein</b>_GAN_With_Quadratic...", "snippet": "The <b>Wasserstein</b> <b>loss</b> <b>can</b> encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the ...", "dateLastCrawled": "2022-01-19T04:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Sumit Mukherjee, Yixi Xu, Anusua Trivedi, Nabajyoti Patowary, and Juan ...", "url": "https://petsymposium.org/2021/files/papers/issue3/paper16-2021-3-source.pdf", "isFamilyFriendly": true, "displayUrl": "https://petsymposium.org/2021/files/papers/issue3/paper16-2021-3-source.pdf", "snippet": "model against several white and <b>black</b>\u2013<b>box</b> attacks on several benchmark datasets, iv) we empirically demonstrate on three common benchmark datasets that synthetic images generated by privGAN lead to negligible <b>loss</b> in downstream performance when <b>compared</b> against non\u2013private GANs. While we have focused on benchmarking privGAN exclusively on", "dateLastCrawled": "2021-11-05T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>WASSERSTEIN</b> ROBUST REINFORCEMENT LEARNING", "url": "https://openreview.net/attachment?id=HyxwZRNtDr&name=original_pdf", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/attachment?id=HyxwZRNtDr&amp;name=original_pdf", "snippet": "a <b>Wasserstein</b> constraint for a correct and convergent solver. Apart from the formulation, we also propose an ef\ufb01cient and scalable solver following a novel zero-order optimisation method that we believe <b>can</b> be useful to numerical optimisation in general. We empirically demonstrate signi\ufb01cant gains <b>compared</b> to standard and robust state-of-the-art algorithms on high-dimensional MuJuCo en-vironments. 1 INTRODUCTION Reinforcement learning (RL) has become a standard tool for solving decision ...", "dateLastCrawled": "2021-12-25T02:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Using <b>Wasserstein-2 regularization to ensure fair decisions</b> with Neural ...", "url": "https://deepai.org/publication/using-wasserstein-2-regularization-to-ensure-fair-decisions-with-neural-network-classifiers", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/using-<b>wasserstein-2-regularization-to-ensure</b>-fair...", "snippet": "<b>Wasserstein</b> type distances and optimal transport techniques have grown popular in the recent years (see in or for theoretical and pratical issues on <b>Wasserstein</b> distance) and are widely used in the machine learning and faire learning literature (see in , or for instance). In very particular we will impose closeness with respect to quadratic transportation cost (i.e <b>Wasserstein</b>-2 distance) of the score used to build an automatic decision rule built with a deep neural network acting as a ...", "dateLastCrawled": "2021-12-22T15:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Wasserstein</b> Index Generation Model: Automatic Generation of Time-series ...", "url": "https://www.arxiv-vanity.com/papers/1908.04369/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.04369", "snippet": "<b>Wasserstein</b> Index Generation Model: Automatic Generation of Time-series Index with Application to Economic Policy Uncertainty. Fangzhou Xie 1 Department of Economics, New York University 1 1 Present Mailing Address: 30 River Ct, Apt 2308, Jersey City, NJ, 07310. January 25, 2021 . Abstract. I propose a novel method, called the <b>Wasserstein</b> Index Generation model (WIG), to generate public sentiment index automatically. It <b>can</b> be performed off-the-shelf and is especially good at detecting ...", "dateLastCrawled": "2021-07-23T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Do GAN <b>Loss</b> Functions Really Matter? \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1811.09567/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1811.09567", "snippet": "Hence, a <b>small</b> Lipschitz constant K leads to a <b>small</b> valid interval [a, b] (Eq. 7), which makes different GAN <b>loss</b> functions degenerate to linear ones with similar behaviors. In practice, the <b>loss</b> function <b>can</b> also degenerate with a relatively large valid interval [ a , b ] as long as the part of the <b>loss</b> function in it has near-constant gradient and <b>can</b> be approximated linearly.", "dateLastCrawled": "2021-12-27T21:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Papers for 2021-10-20 \u00b7 Discussion #9 \u00b7 aryanpandey/Paper_Collection ...", "url": "https://github.com/aryanpandey/Paper_Collection/discussions/9", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/aryanpandey/Paper_Collection/discussions/9", "snippet": "<b>Compared</b> with GSA method, the FTSEM method obtained more complete tree skeletons. And the time cost of the FTSEM method is evaluated using the runtime and time per million points (TPMP). The runtime of FTSEM is from 1.0 s to 13.0 s, and the runtime of GSA is from 6.4 s to 309.3 s. The average value of TPMP is 1.8 s for FTSEM, and 22.3 s for GSA respectively. The experimental results demonstrate that the proposed method is feasible, robust, and fast with a good potential on tree skeleton ...", "dateLastCrawled": "2022-01-31T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>How to Develop a Conditional</b> GAN (cGAN) From Scratch", "url": "https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>how-to-develop-a-conditional</b>-generative-adversarial...", "snippet": "There are two motivations for <b>making</b> use of the class label information in a GAN model. Improve the GAN. Targeted Image Generation. Additional information that is correlated with the input images, such as class labels, <b>can</b> be used to improve the GAN. This improvement may come in the form of more stable training, faster training, and/or generated images that have better quality. Class labels <b>can</b> also be used for the deliberate or targeted generation of images of a given type. A limitation of ...", "dateLastCrawled": "2022-02-03T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - MinghuiChen43/awesome-trustworthy-deep-learning: A curated ...", "url": "https://github.com/MinghuiChen43/awesome-trustworthy-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/MinghuiChen43/awesome-trustworthy-deep-learning", "snippet": "<b>Compared</b> with the results reported in the ObjectNet paper, we observe that around 10-15 % of the performance <b>loss</b> <b>can</b> be recovered, without any test time data augmentation. CEB Improves Model Robustness. Ian Fischer, Alexander A. Alemi. Digest: We demonstrate that the Conditional Entropy Bottleneck (CEB) <b>can</b> improve model robustness. CEB is an easy strategy to implement and works in tandem with data augmentation procedures. We report results of a large scale adversarial robustness study on ...", "dateLastCrawled": "2021-10-19T05:10:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to stabilize GAN training. Understand <b>Wasserstein</b> distance and ...", "url": "https://towardsdatascience.com/wasserstein-distance-gan-began-and-progressively-growing-gan-7e099f38da96", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>wasserstein</b>-distance-gan-began-and-progressively...", "snippet": "<b>Wasserstein</b> <b>loss</b> leads to a higher quality of the gradients to train G. ... Finally, one intuitive way to understand this paper is to make an <b>analogy</b> with the gradients on the history of in-layer activation functions. Specifically, the gradients of sigmoid and tanh activations that disappeared in favor of ReLUs, because of the improved gradients in the whole range of values. BEGAN (Boundary Equilibrium Generative Adversarial Networks 2017) We often see that the discriminator progresses too ...", "dateLastCrawled": "2022-01-25T18:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning Wasserstein Embeddings</b> | DeepAI", "url": "https://deepai.org/publication/learning-wasserstein-embeddings", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-wasserstein-embeddings</b>", "snippet": "The <b>Wasserstein</b> distance received a lot of attention recently in the community of <b>machine</b> <b>learning</b>, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy ...", "dateLastCrawled": "2022-01-05T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Learning</b> <b>Wasserstein</b> Embeddings - ResearchGate", "url": "https://www.researchgate.net/publication/320564581_Learning_Wasserstein_Embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320564581_<b>Learning</b>_<b>Wasserstein</b>_Embeddings", "snippet": "Designed through an <b>analogy</b> with ... Fast dictionary <b>learning</b> with a smoothed <b>wasserstein</b> <b>loss</b>. In AISTA TS, pages 630\u2013638, 2016. [32] F. Santambrogio. Introduction to optimal transport theory ...", "dateLastCrawled": "2021-12-13T04:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "deep <b>learning</b> - How can both generator and discriminator losses ...", "url": "https://datascience.stackexchange.com/questions/32699/how-can-both-generator-and-discriminator-losses-decrease", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/32699", "snippet": "In the widely used <b>analogy</b>: ... despite the WGAN having a different <b>loss</b> function, namely the <b>Wasserstein</b> distance, one should still not expect that the discriminator and generator simultaneously monotonically increase -- generally one of them &quot;wins&quot; the round and receives a lower portion of the <b>loss</b>. $\\endgroup$ \u2013 PSub. Mar 13 &#39;21 at 6:07 $\\begingroup$ @PSub You are completely misunderstanding the question. It&#39;s not a question about the small scale changes of the <b>loss</b> values. OP is asking ...", "dateLastCrawled": "2022-01-28T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Manifold-Valued Image Generation with <b>Wasserstein</b> Generative ...", "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4277/4155", "isFamilyFriendly": true, "displayUrl": "https://ojs.aaai.org/index.php/AAAI/article/download/4277/4155", "snippet": "fundamental <b>machine</b> <b>learning</b> problems. However, few mod-ern generative models, including <b>Wasserstein</b> Generative Ad-versarial Nets (WGANs), are studied on manifold-valued im- ages that are frequently encountered in real-world applica-tions. To \ufb01ll the gap, this paper \ufb01rst formulates the problem of generating manifold-valued images and exploits three typical instances: hue-saturation-value (HSV) color image genera-tion, chromaticity-brightness (CB) color image generation, and diffusion ...", "dateLastCrawled": "2022-01-29T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Advanced <b>Machine</b> <b>Learning</b> - Computer Vision", "url": "https://www.vision.rwth-aachen.de/media/course/SS/2019/advanced-machine-learning/ml2_19-part17-gans-6on1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.vision.rwth-aachen.de/media/course/SS/2019/advanced-<b>machine</b>-<b>learning</b>/ml2...", "snippet": "<b>Analogy</b>: police investigator \u2022Both generator and discriminator are deep networks We can train them with backprop. Image sources: www.bundesbank.de, weclipart.com, Kevin McGuiness 15 Advanced <b>Machine</b> <b>Learning</b> Part 17 \u2013Generative Adversarial Networks Training the Discriminator \u2022Procedure Fix generator weights Train discriminator to distinguish between real and generated images Image credit: Kevin McGuiness 16 Visual Computing Institute | Prof. Dr . Bastian Leibe Advanced <b>Machine</b> <b>Learning</b> ...", "dateLastCrawled": "2021-10-25T15:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[D] Is the <b>Wasserstein</b> distance really what we optimize in WGAN ...", "url": "https://www.reddit.com/r/MachineLearning/comments/ew2lzs/d_is_the_wasserstein_distance_really_what_we/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/ew2lzs/d_is_the_<b>wasserstein</b>_distance...", "snippet": "The &quot;genuine&quot; <b>Wasserstein</b> <b>loss</b> relies on optimal transport, a generalization of sorting to high-dimensional feature spaces. In a nutshell: OT relies on the matrix of distances between samples to define a &quot;least action&quot; matching between any two distributions. Now, unfortunately, in spaces of images, the L2 distance is (essentially) meaningless: natural images should not be compared with each other pixel-wise. As a consequence, the baseline <b>Wasserstein</b> distance between two batches of images is ...", "dateLastCrawled": "2021-09-30T11:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>Tour of Generative Adversarial Network Models</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/tour-of-generative-adversarial-network-models/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>tour-of-generative-adversarial-network-models</b>", "snippet": "By <b>analogy</b> with auto-encoders, we propose Context Encoders \u2013 a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. \u2014 Context Encoders: Feature <b>Learning</b> by Inpainting, 2016. Example of the Context Encoders Encoder-Decoder Model Architecture. Taken from: Context Encoders: Feature <b>Learning</b> by Inpainting. The model is trained with a joint-<b>loss</b> that combines both the adversarial <b>loss</b> of generator and discriminator models ...", "dateLastCrawled": "2022-02-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "SpringerLink - <b>Machine Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-020-05924-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05924-1", "snippet": "for a given set \\({\\mathcal {F}}\\) of distributions, twice differentiable and convex <b>loss</b> \\(\\ell\\), and prediction \\(f_\\theta (x)\\).The set \\({\\mathcal {F}}\\) is the set of distributions on which one would like the estimator to achieve a guaranteed performance bound.. Causal inference can be seen to be a specific instance of distributional robustness, where we take \\({\\mathcal {F}}\\) to be the class of all distributions generated under do-interventions on the predictors X (Meinshausen 2018 ...", "dateLastCrawled": "2022-01-15T22:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Gentle Introduction to Pix2Pix Generative</b> Adversarial Network", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-pix2pix-generative</b>...", "snippet": "Image-to-image translation is the controlled conversion of a given source image to a target image. An example might be the conversion of black and white photographs to color photographs. Image-to-image translation is a challenging problem and often requires specialized models and <b>loss</b> functions for a given translation task or dataset. The Pix2Pix GAN is a general approach for image-to-image translation. It is based", "dateLastCrawled": "2022-02-02T13:26:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(wasserstein loss)  is like +(making a black box small)", "+(wasserstein loss) is similar to +(making a black box small)", "+(wasserstein loss) can be thought of as +(making a black box small)", "+(wasserstein loss) can be compared to +(making a black box small)", "machine learning +(wasserstein loss AND analogy)", "machine learning +(\"wasserstein loss is like\")", "machine learning +(\"wasserstein loss is similar\")", "machine learning +(\"just as wasserstein loss\")", "machine learning +(\"wasserstein loss can be thought of as\")", "machine learning +(\"wasserstein loss can be compared to\")"]}