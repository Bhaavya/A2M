{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "The most popular <b>sequence-to-sequence</b> <b>task</b> is translation: usually, from one natural language to another. In the last couple of years, commercial systems became surprisingly good at <b>machine</b> translation - check out, for example, Google Translate, Yandex Translate, DeepL Translator, Bing Microsoft Translator.Today we will <b>learn</b> about the core part of these systems.", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Making Predictions with Sequences - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/sequence-prediction/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>sequence</b>-prediction", "snippet": "\u2014 Multi-<b>task</b> <b>Sequence to Sequence</b> <b>Learning</b>, 2016. If the input and <b>output</b> sequences are a time series, then the problem may be referred to as multi-step time series forecasting. Multi-Step Time Series Forecasting. Given a time series of observations, predict <b>a sequence</b> of observations for a range of future time steps. Text Summarization. Given a document of text, predict a shorter <b>sequence</b> of text that describes the salient parts of the source document. Program Execution. Given the textual ...", "dateLastCrawled": "2022-02-02T04:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What <b>are classic machine learning algorithms similar to sequence</b> to ...", "url": "https://www.quora.com/What-are-classic-machine-learning-algorithms-similar-to-sequence-to-sequence-models", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>are-classic-machine-learning-algorithms-similar-to-sequence</b>...", "snippet": "Answer (1 of 3): As Mundher Alshabi has mentioned, HMMs are one such model. However, HMMs are fairly simple models, that <b>generate</b> one tag per input token, and they only handle <b>inputs</b> <b>in order</b>. So they can\u2019t be used for tasks <b>like</b> <b>machine</b> translation. An alternate approach is the Noisy channel mo...", "dateLastCrawled": "2021-12-20T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "An <b>introduction to sequence-to-sequence learning</b>. Published: February 19, 2019. Many interesting problems in artificial intelligence can be described in the following way: Map <b>a sequence</b> <b>of inputs</b> $\\mathbf{x}$ to the correct <b>sequence</b> of outputs $\\mathbf{y}$. Speech recognition is one example: the goal is to map an audio signal $\\mathbf{x}$ (<b>a sequence</b> of real-valued audio samples) to the correct text transcript $\\mathbf{y}$ (<b>a sequence</b> of letters). Other examples are <b>machine</b> translation ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Data Preparation for <b>Variable Length Input Sequences</b>", "url": "https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/data-preparation-variable-", "snippet": "I have a question about padding outputs in <b>sequence-to-sequence</b> classification problems. Let\u2019s say X has the shape (100, 50, 10), and y has the shape (100, 50, 3). X consists of 100 time series, 50 time steps per time series, and 10 features per time step. The y has three possible one-hot encoded classes per tilmestep. The samples of X have variable length so the shorter samples are pre-padded with 0 For the y labels corresponding to the pre-padded X time steps, should they be [0, 0, 0 ...", "dateLastCrawled": "2022-02-03T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Is there an implementation of the LSTM in the <b>Sequence to Sequence</b> ...", "url": "https://www.quora.com/Is-there-an-implementation-of-the-LSTM-in-the-Sequence-to-Sequence-Learning-paper", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-an-implementation-of-the-LSTM-in-the-<b>Sequence</b>-to...", "snippet": "Answer (1 of 3): Yes and No. Yes, because there are plenty of RNN and LSTM packages out there. Using Torch, Theano or Caffe, it&#39;s relatively easy to implement the model described in the paper. Specifically, if you want to do Neural <b>Machine</b> Translation, which is the experiment mentioned in the pa...", "dateLastCrawled": "2022-01-14T21:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Keras <b>LSTM Example | Sequence Binary Classification</b> - HackDeploy", "url": "https://www.hackdeploy.com/keras-lstm-example-sequence-binary-classification/", "isFamilyFriendly": true, "displayUrl": "https://www.hackdeploy.com/keras-<b>lstm-example-sequence-binary-classification</b>", "snippet": "<b>A sequence</b> is a set of values where each value corresponds to an observation at a specific point in time. <b>Sequence</b> prediction involves using historical sequential data to predict the next value or values. <b>Machine</b> <b>learning</b> models that successfully deal with sequential data are RNN\u2019s (Recurrent Neural Networks).", "dateLastCrawled": "2022-02-03T06:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How Transformers Work. Transformers are a type of neural\u2026 | by Giuliano ...", "url": "https://towardsdatascience.com/transformers-141e32e69591", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-141e32e69591", "snippet": "Each cell takes as <b>inputs</b> x_t (a word in the case of a sentence to sentence translation), the previous cell state and the <b>output</b> of the previous cell. It manipulates these <b>inputs</b> and based on them, it generates a new cell state, and <b>an output</b>. I won\u2019t go into detail on the mechanics of each cell. If you want to understand how each cell works ...", "dateLastCrawled": "2022-02-02T11:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Andrew-NG-Notes/andrewng-p-5-<b>sequence</b>-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-<b>sequence</b>...", "snippet": "Applications <b>like</b> <b>machine</b> translation <b>inputs</b> and outputs sequences have different lengths in most of the cases. So an alternative ... Word embeddings tend to make the biggest difference when the <b>task</b> you&#39;re <b>trying</b> to carry out has a relatively smaller training set. Also, one of the advantages of using word embeddings is that it reduces the size of the input! 10,000 one hot compared to 300 features vector. Word embeddings have an interesting relationship to the face recognition <b>task</b>: In this ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Have you ever met with a <b>machine</b> <b>learning</b> problem when the <b>sequence</b> of ...", "url": "https://www.quora.com/Have-you-ever-met-with-a-machine-learning-problem-when-the-sequence-of-features-had-an-impact-e-g-time-of-running-the-first-marathon-second-etc-How-do-you-design-such-an-ML-application", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Have-you-ever-met-with-a-<b>machine</b>-<b>learning</b>-problem-when-the...", "snippet": "Answer (1 of 5): A major skill of LSTMs and in general Recurrent Neural Networks is the ability <b>to learn</b> sequences. A major <b>task</b> of Natural Language Processing is Part of Speech (POS) Tagging. Now the major challenge in POS tagging is word sense disambiguation, i.e. the same word having differen...", "dateLastCrawled": "2022-01-11T02:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "The most popular <b>sequence-to-sequence</b> <b>task</b> is translation: usually, from one natural language to another. In the last couple of years, commercial systems became surprisingly good at <b>machine</b> translation - check out, for example, Google Translate, Yandex Translate, DeepL Translator, Bing Microsoft Translator.Today we will <b>learn</b> about the core part of these systems.", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What <b>are classic machine learning algorithms similar to sequence</b> to ...", "url": "https://www.quora.com/What-are-classic-machine-learning-algorithms-similar-to-sequence-to-sequence-models", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>are-classic-machine-learning-algorithms-similar-to-sequence</b>...", "snippet": "Answer (1 of 3): As Mundher Alshabi has mentioned, HMMs are one such model. However, HMMs are fairly simple models, that <b>generate</b> one tag per input token, and they only handle <b>inputs</b> <b>in order</b>. So they can\u2019t be used for tasks like <b>machine</b> translation. An alternate approach is the Noisy channel mo...", "dateLastCrawled": "2021-12-20T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Transformer</b> vs RNN and CNN for Translation <b>Task</b> | by Yacine BENAFFANE ...", "url": "https://medium.com/analytics-vidhya/transformer-vs-rnn-and-cnn-18eeefa3602b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>transformer</b>-vs-rnn-and-cnn-18eeefa3602b", "snippet": "On the other hand, the information in different positions of the input <b>sequence</b> in the CNN has the same effect on the <b>output</b> of the encoder (the <b>Transformer</b> <b>is similar</b>). To solve this problem ...", "dateLastCrawled": "2022-01-29T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Neural Machine translation</b> and the need for Attention Mechanism | by ...", "url": "https://medium.com/analytics-vidhya/neural-machine-translation-and-the-need-for-attention-mechanism-60f9a39da9a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>neural-machine-translation</b>-and-the-need-for...", "snippet": "The <b>task</b> of applying a trained model <b>to generate</b> a translation is called inference or more commonly decoding the <b>sequence</b> in <b>machine</b> translation. We have a trained model, now we can <b>generate</b> ...", "dateLastCrawled": "2022-01-22T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Making Predictions with Sequences - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/sequence-prediction/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>sequence</b>-prediction", "snippet": "\u2014 Multi-<b>task</b> <b>Sequence to Sequence</b> <b>Learning</b>, 2016. If the input and <b>output</b> sequences are a time series, then the problem may be referred to as multi-step time series forecasting. Multi-Step Time Series Forecasting. Given a time series of observations, predict <b>a sequence</b> of observations for a range of future time steps. Text Summarization. Given a document of text, predict a shorter <b>sequence</b> of text that describes the salient parts of the source document. Program Execution. Given the textual ...", "dateLastCrawled": "2022-02-02T04:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sequence</b> Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>sequence</b>-classification-", "snippet": "<b>Sequence</b> classification is a predictive modeling problem where you have some <b>sequence</b> <b>of inputs</b> over space or time and the <b>task</b> is to predict a category for the <b>sequence</b>. What makes this problem difficult is that the sequences can vary in length, be comprised of a very large vocabulary of input symbols and may require the model <b>to learn</b> the long-term", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Data Preparation for <b>Variable Length Input Sequences</b>", "url": "https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/data-preparation-variable-", "snippet": "Deep <b>learning</b> libraries assume a vectorized representation of your data. In the case of variable length <b>sequence</b> prediction problems, this requires that your data be transformed such that each <b>sequence</b> has the same length. This vectorization allows code to efficiently perform the matrix operations in batch for your chosen deep <b>learning</b> algorithms. In this tutorial, you will discover techniques that you can use", "dateLastCrawled": "2022-02-03T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "An Introduction to <b>Attention</b> Mechanisms in Deep <b>Learning</b> | Towards Data ...", "url": "https://towardsdatascience.com/visualization-attention-part-5-2c3c14e60548", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/visualization-<b>attention</b>-part-5-2c3c14e60548", "snippet": "<b>In order</b> to make the decision, in fact, we will start not with images, but we want to talk first about the <b>sequence to sequence</b> models. Here, you can see a visualization of gradients from a CNN type of model that is used to translate from English to German if you now start plotting those gradients. So, this is essentially the visualization technique that we already looked at in image processing. You can now see the gradient with respect to that particular input of the respective <b>output</b>. If ...", "dateLastCrawled": "2022-02-02T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How Transformers Work. Transformers are a type of neural\u2026 | by Giuliano ...", "url": "https://towardsdatascience.com/transformers-141e32e69591", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-141e32e69591", "snippet": "<b>Sequence</b> transduction. The input is represented in green, the model is represented in blue, and the <b>output</b> is represented in purple. GIF from 3. For models to perform <b>sequence</b> transduction, it is necessary to have some sort of memory.For example let\u2019s say that we are translating the following sentence to another language (French):", "dateLastCrawled": "2022-02-02T11:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Andrew-NG-Notes/andrewng-p-5-<b>sequence</b>-models.md at master ...", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-<b>sequence</b>...", "snippet": "<b>Machine</b> translation (<b>sequence to sequence</b>): X: text <b>sequence</b> (in one language) Y: text <b>sequence</b> (in other language) Video activity recognition (<b>sequence</b> to one): X: video frames ; Y: label (activity) Name entity recognition (<b>sequence to sequence</b>): X: text <b>sequence</b>; Y: label <b>sequence</b>; Can be used by seach engines to index different type of words inside a text. All of these problems with different input and <b>output</b> (<b>sequence</b> or not) can be addressed as supervised <b>learning</b> with label data X, Y ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "The most popular <b>sequence-to-sequence</b> <b>task</b> is translation: usually, from one natural language to another. In the last couple of years, commercial systems became surprisingly good at <b>machine</b> translation - check out, for example, Google Translate, Yandex Translate, DeepL Translator, Bing Microsoft Translator.Today we will <b>learn</b> about the core part of these systems.", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequence-to-Sequence Models</b> | Mohit Deshpande\u2019s Blog", "url": "https://mohitd.github.io/2018/06/24/seq2seq-models.html", "isFamilyFriendly": true, "displayUrl": "https://mohitd.github.io/2018/06/24/seq2seq-models.html", "snippet": "We\u2019ve only discussed <b>machine</b> translation as an application of seq2seq models, but they <b>can</b> really be used for any <b>sequence-to-sequence</b> <b>task</b>. A recent trend in designing neural architectures is to have an attention mechanism to focus our network to a specific subsection of our input. These work very well, especially with seq2seq models, however, at a higher computation cost.", "dateLastCrawled": "2021-10-22T06:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Differences between Autoregressive, Autoencoding and Sequence</b>-to ...", "url": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive-autoencoding-and-sequence-to-sequence-models-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2020/12/29/differences-between-autoregressive...", "snippet": "Even though the flow is more vertical than in the example above, you <b>can</b> see that it is in essence an encoder-decoder architecture performing <b>sequence-to-sequence</b> <b>learning</b>: We have N encoder segments that take <b>inputs</b> (in the form of a learned embedding) and encode it into a higher-dimensional intermediate representation (in the case of the original Transformer, it outputs a 512-dimensional state vector ).", "dateLastCrawled": "2022-02-02T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Making Predictions with Sequences - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/sequence-prediction/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>sequence</b>-prediction", "snippet": "\u2014 Multi-<b>task</b> <b>Sequence to Sequence</b> <b>Learning</b>, 2016. If the input and <b>output</b> sequences are a time series, then the problem may be referred to as multi-step time series forecasting. Multi-Step Time Series Forecasting. Given a time series of observations, predict <b>a sequence</b> of observations for a range of future time steps. Text Summarization. Given a document of text, predict a shorter <b>sequence</b> of text that describes the salient parts of the source document. Program Execution. Given the textual ...", "dateLastCrawled": "2022-02-02T04:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "An <b>introduction to sequence-to-sequence learning</b>. Published: February 19, 2019. Many interesting problems in artificial intelligence <b>can</b> be described in the following way: Map <b>a sequence</b> <b>of inputs</b> $\\mathbf{x}$ to the correct <b>sequence</b> of outputs $\\mathbf{y}$. Speech recognition is one example: the goal is to map an audio signal $\\mathbf{x}$ (<b>a sequence</b> of real-valued audio samples) to the correct text transcript $\\mathbf{y}$ (<b>a sequence</b> of letters). Other examples are <b>machine</b> translation ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "is <b>the Sequence to Sequence learning right</b>? \u00b7 Issue #395 \u00b7 keras-team ...", "url": "https://github.com/keras-team/keras/issues/395", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/keras-team/keras/issues/395", "snippet": "Assume we are <b>trying</b> <b>to learn</b> <b>a sequence to sequence</b> map. For this we <b>can</b> use Recurrent and TimeDistributedDense layers. Now assume that the sequences have different lengths. We should pad both input and desired sequences with zeros, right? But how will the objective function handle the padded values? There is no choice to pass a mask to the objective function. Won&#39;t this bias the cost function?", "dateLastCrawled": "2022-01-29T00:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How Transformers Work. Transformers are a type of neural\u2026 | by Giuliano ...", "url": "https://towardsdatascience.com/transformers-141e32e69591", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-141e32e69591", "snippet": "Transformers were developed to s o lve the problem of <b>sequence</b> transduction, or neural <b>machine</b> translation. That means any <b>task</b> that transforms an input <b>sequence</b> to <b>an output</b> <b>sequence</b>. This includes speech recognition, text-to-speech transformation, etc..", "dateLastCrawled": "2022-02-02T11:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Attention</b> in Deep Networks with Keras | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/light-on-math-ml-<b>attention</b>-with-keras-dc8dbc1fad39", "snippet": "Let\u2019s have a look at how <b>a sequence to sequence</b> model might be used for a English-French <b>machine</b> translation <b>task</b>. <b>A sequence to sequence</b> model has two components, an encoder and a decoder . The encoder encodes a source sentence to a concise vector (called the context vector ) , where the decoder takes in the context vector as an input and computes the translation using the encoded representation.", "dateLastCrawled": "2022-01-30T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is there an implementation of the LSTM in the <b>Sequence to Sequence</b> ...", "url": "https://www.quora.com/Is-there-an-implementation-of-the-LSTM-in-the-Sequence-to-Sequence-Learning-paper", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-an-implementation-of-the-LSTM-in-the-<b>Sequence</b>-to...", "snippet": "Answer (1 of 3): Yes and No. Yes, because there are plenty of RNN and LSTM packages out there. Using Torch, Theano or Caffe, it&#39;s relatively easy to implement the model described in the paper. Specifically, if you want to do Neural <b>Machine</b> Translation, which is the experiment mentioned in the pa...", "dateLastCrawled": "2022-01-14T21:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What <b>is the difference between LSTM, RNN and</b> <b>sequence to sequence</b>? - Quora", "url": "https://www.quora.com/What-is-the-difference-between-LSTM-RNN-and-sequence-to-sequence", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>is-the-difference-between-LSTM-RNN-and</b>-<b>sequence-to-sequence</b>", "snippet": "Answer: First, <b>sequence-to-sequence</b> is a problem setting, where your input is <b>a sequence</b> and your <b>output</b> is also <b>a sequence</b>. Typical examples of <b>sequence-to-sequence</b> problems are <b>machine</b> translation, question answering, generating natural language description of videos, automatic summarization, e...", "dateLastCrawled": "2022-01-26T04:06:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What <b>are classic machine learning algorithms similar to sequence</b> to ...", "url": "https://www.quora.com/What-are-classic-machine-learning-algorithms-similar-to-sequence-to-sequence-models", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>are-classic-machine-learning-algorithms-similar-to-sequence</b>...", "snippet": "Answer (1 of 3): As Mundher Alshabi has mentioned, HMMs are one such model. However, HMMs are fairly simple models, that <b>generate</b> one tag per input token, and they only handle <b>inputs</b> <b>in order</b>. So they <b>can</b>\u2019t be used for tasks like <b>machine</b> translation. An alternate approach is the Noisy channel mo...", "dateLastCrawled": "2021-12-20T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "is <b>the Sequence to Sequence learning right</b>? \u00b7 Issue #395 \u00b7 keras-team ...", "url": "https://github.com/keras-team/keras/issues/395", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/keras-team/keras/issues/395", "snippet": "Assume we are <b>trying</b> <b>to learn</b> <b>a sequence to sequence</b> map. For this we <b>can</b> use Recurrent and TimeDistributedDense layers. Now assume that the sequences have different lengths. We should pad both input and desired sequences with zeros, right? But how will the objective function handle the padded values? There is no choice to pass a mask to the objective function. Won&#39;t this bias the cost function?", "dateLastCrawled": "2022-01-29T00:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Order Matters: Sequence to sequence for sets</b> | DeepAI", "url": "https://deepai.org/publication/order-matters-sequence-to-sequence-for-sets", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>order-matters-sequence-to-sequence-for-sets</b>", "snippet": "Sequences have become first class citizens in supervised <b>learning</b> thanks to the resurgence of recurrent neural networks.Many complex tasks that require mapping from or to <b>a sequence</b> of observations <b>can</b> now be formulated with the <b>sequence-to-sequence</b> (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized <b>inputs</b> and/or outputs might not be naturally expressed as sequences.", "dateLastCrawled": "2021-12-18T17:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Neural Machine translation</b> and the need for Attention Mechanism | by ...", "url": "https://medium.com/analytics-vidhya/neural-machine-translation-and-the-need-for-attention-mechanism-60f9a39da9a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>neural-machine-translation</b>-and-the-need-for...", "snippet": "The <b>task</b> of applying a trained model <b>to generate</b> a translation is called inference or more commonly decoding the <b>sequence</b> in <b>machine</b> translation. We have a trained model, now we <b>can</b> <b>generate</b> ...", "dateLastCrawled": "2022-01-22T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sequence</b> Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>sequence</b>-classification-", "snippet": "<b>Sequence</b> classification is a predictive modeling problem where you have some <b>sequence</b> <b>of inputs</b> over space or time and the <b>task</b> is to predict a category for the <b>sequence</b>. What makes this problem difficult is that the sequences <b>can</b> vary in length, be comprised of a very large vocabulary of input symbols and may require the model <b>to learn</b> the long-term", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Andrew-NG-Notes/andrewng-p-5-<b>sequence</b>-models.md at master ...", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-<b>sequence</b>...", "snippet": "<b>Machine</b> translation (<b>sequence to sequence</b>): X: text <b>sequence</b> (in one language) Y: text <b>sequence</b> (in other language) Video activity recognition (<b>sequence</b> to one): X: video frames; Y: label (activity) Name entity recognition (<b>sequence to sequence</b>): X: text <b>sequence</b>; Y: label <b>sequence</b>; <b>Can</b> be used by seach engines to index different type of words inside a text. All of these problems with different input and <b>output</b> (<b>sequence</b> or not) <b>can</b> be addressed as supervised <b>learning</b> with label data X, Y as ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Is there an implementation of the LSTM in the <b>Sequence to Sequence</b> ...", "url": "https://www.quora.com/Is-there-an-implementation-of-the-LSTM-in-the-Sequence-to-Sequence-Learning-paper", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-an-implementation-of-the-LSTM-in-the-<b>Sequence</b>-to...", "snippet": "Answer (1 of 3): Yes and No. Yes, because there are plenty of RNN and LSTM packages out there. Using Torch, Theano or Caffe, it&#39;s relatively easy to implement the model described in the paper. Specifically, if you want to do Neural <b>Machine</b> Translation, which is the experiment mentioned in the pa...", "dateLastCrawled": "2022-01-14T21:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Have you ever met with a <b>machine</b> <b>learning</b> problem when the <b>sequence</b> of ...", "url": "https://www.quora.com/Have-you-ever-met-with-a-machine-learning-problem-when-the-sequence-of-features-had-an-impact-e-g-time-of-running-the-first-marathon-second-etc-How-do-you-design-such-an-ML-application", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Have-you-ever-met-with-a-<b>machine</b>-<b>learning</b>-problem-when-the...", "snippet": "Answer (1 of 5): A major skill of LSTMs and in general Recurrent Neural Networks is the ability <b>to learn</b> sequences. A major <b>task</b> of Natural Language Processing is Part of Speech (POS) Tagging. Now the major challenge in POS tagging is word sense disambiguation, i.e. the same word having differen...", "dateLastCrawled": "2022-01-11T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What <b>are the advantages and disadvantages of BiLSTM</b> and BiGRU - Quora", "url": "https://www.quora.com/What-are-the-advantages-and-disadvantages-of-BiLSTM-and-BiGRU-CRF", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>are-the-advantages-and-disadvantages-of-BiLSTM</b>-and-BiGRU-CRF", "snippet": "Answer: Let us start the discussion. First, let us understand each one in detail CRF: It stands for conditional random field. It is <b>a sequence</b> prediction model. It is a discriminative classifier which models the decision boundary between different classes. In Simple words, it learns weights asso...", "dateLastCrawled": "2022-01-23T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Multi-Step <b>LSTM Time Series Forecasting Models for Power</b> Usage", "url": "https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/how-to-develop-lstm-models-for-multi-step-time...", "snippet": "We <b>can</b> also do this in a way where the number <b>of inputs</b> and outputs are parameterized (e.g. n_input, n_out) so that you <b>can</b> experiment with different values or adapt it for your own problem. Below is a function named to_supervised() that takes a list of weeks (history) and the number of time steps to use as <b>inputs</b> and outputs and returns the data in the overlapping moving window format.", "dateLastCrawled": "2022-02-02T15:32:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Week 3: <b>Sequence to sequence</b> architectures. <b>Sequence to sequence</b> models Language translation for example; Image captioning, caption an image; Picking the most likely model <b>Machine</b> Transation Model Split into a model encoding the sentence; and then a language model. Calculate the probability of an English sentence conditioned on a French sentence.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Week 1 Quiz - Introduction to deep <b>learning</b>. What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI is powering personal devices in our homes and offices, similar to electricity. Through the \u201csmart grid\u201d, AI is delivering a new wave of electricity. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Note: Andrew ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(machine learning task where the machine is trying to learn a sequence of inputs in order to generate an output)", "+(sequence-to-sequence task) is similar to +(machine learning task where the machine is trying to learn a sequence of inputs in order to generate an output)", "+(sequence-to-sequence task) can be thought of as +(machine learning task where the machine is trying to learn a sequence of inputs in order to generate an output)", "+(sequence-to-sequence task) can be compared to +(machine learning task where the machine is trying to learn a sequence of inputs in order to generate an output)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}