{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "3. Harm, Discrimination, and Measurement \u2014 Fairness &amp; Algorithmic ...", "url": "https://afraenkel.github.io/fairness-book/content/03-harms.html", "isFamilyFriendly": true, "displayUrl": "https://afraenkel.github.io/fairness-book/content/03-harms.html", "snippet": "3.2. Bias\u00b6. Bias is a term often associated with discrimination; it is milder in the severity of intent. Those that are <b>biased</b> <b>against</b> an individual exhibit some systematic leaning that causes the person treat this individual differently (in a negative way).. Bias also has a statistical meaning: a statistical estimator is <b>biased</b> if it systematically differs from the population parameter it estimates.. The two meanings are (loosely) connected, if one assumes that a person\u2019s decisions (i.e ...", "dateLastCrawled": "2022-02-03T01:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Algorithmic bias detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/algorithmic-bias-detection-and-mitigation-best-", "snippet": "If left unchecked, <b>biased</b> algorithms can lead to decisions which can have a collective, <b>disparate</b> impact on <b>certain</b> <b>groups</b> of people even without the programmer\u2019s intention to discriminate. The ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Fall 2020 Data Ethics in Algorithmic <b>Decision</b> Making", "url": "https://www.csd.uoc.gr/~hy562/lectures20/lec08_Data_Ethics_20.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.csd.uoc.gr/~hy562/lectures20/lec08_Data_Ethics_20.pdf", "snippet": "\u2022 <b>Disparate</b> <b>treatment</b> (DT) is the illegal practice of treating an entity, such as a creditor or employer, differently based on a protected/sensitive attributes such as race, gender, age, religion, sexual orientation, etc. avoid disparities between different outputs for <b>groups</b> of people with the same (or similar) values of non-sensitive attributes but different values of sensitive ones \u2022 <b>Disparate</b> impact (DI) is the result of systematic <b>disparate</b> <b>treatment</b>, where disproportionate adverse ...", "dateLastCrawled": "2022-01-26T19:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Gender inequalities in the workplace: the effects of organizational ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4584998/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4584998", "snippet": "<b>Human</b> resource policies that are inherently <b>biased</b> <b>against</b> a group of people, regardless of their job-related knowledge, skills, abilities, and performance can be termed institutional discrimination. Institutional discrimination <b>against</b> women can occur in each type of HR policy from the recruitment and selection of an individual into an organization, through his/her role assignments, training, pay, performance evaluations, promotion, and termination. For instance, if women are under ...", "dateLastCrawled": "2022-02-03T00:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Fairness Beyond <b>Disparate</b> <b>Treatment</b> &amp; <b>Disparate</b> Impact: Learning ...", "url": "https://www.researchgate.net/publication/315871857_Fairness_Beyond_Disparate_Treatment_Disparate_Impact_Learning_Classification_without_Disparate_Mistreatment", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315871857_Fairness_Beyond_<b>Disparate</b>_<b>Treatment</b>...", "snippet": "ment, <b>disparate</b> <b>treatment</b> arises when a <b>decision</b> making. system provides di\ufb00erent outputs for <b>groups</b> of people with . the same (or similar) values of non-sensitiv e attributes (or. features) but ...", "dateLastCrawled": "2022-01-26T15:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) A categorical model of cognition and <b>biased</b> <b>decision</b> making ...", "url": "https://www.academia.edu/956936/A_categorical_model_of_cognition_and_biased_decision_making", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/956936/A_categorical_model_of_cognition_and_<b>biased</b>_<b>decision</b>...", "snippet": "A categorical model of cognition and <b>biased</b> <b>decision</b> making. The BE Journal of Theoretical Economics, 2008. Matthew O. Jackson. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package. Translate PDF. Related Papers. Categorical Cognition: A Psychological Model of Categories and Identification In <b>Decision</b> Making . By Matthew O. Jackson. 9 ...", "dateLastCrawled": "2022-01-21T10:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Bias In, Bias Out? Evaluating the Folk Wisdom - researchgate.net", "url": "https://www.researchgate.net/publication/335908601_Bias_In_Bias_Out_Evaluating_the_Folk_Wisdom", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335908601_Bias_In_Bias_Out_Evaluating_the...", "snippet": "PDF | We evaluate the folk wisdom that algorithms trained on data produced by <b>biased</b> <b>human</b> <b>decision</b>-makers necessarily reflect this bias. We consider a... | Find, read and cite all the research ...", "dateLastCrawled": "2021-10-14T14:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Chapter 4 : <b>Discrimination Claims, EEOC Proceedings, and</b> Alternative ...", "url": "https://quizlet.com/115617215/chapter-4-discrimination-claims-eeoc-proceedings-and-alternative-dispute-resolution-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/115617215/chapter-4-<b>discrimination-claims-eeoc-proceedings-and</b>...", "snippet": "Those who are within <b>certain</b> protected classifications may avail themselves of <b>certain</b> protections in the workplace. Three types of discrimination claims. 1. <b>disparate</b> <b>treatment</b> (intentional discrimination) 2. <b>disparate</b> impact (a neutral selection standard, unjustified by a business necessity, disqualifies particular races or <b>groups</b> at a significantly higher rate than it does other races or <b>groups</b>); and 3. retaliation. <b>Disparate</b> <b>treatment</b>. Intentional discrimination. <b>Disparate</b> impact. A ...", "dateLastCrawled": "2019-06-21T16:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "In practice, conditional statistical parity is suitable when there is one or several attributes that justify a possible <b>disparate</b> <b>treatment</b> between different <b>groups</b> in the population. Hence, choosing the legitimate attribute(s) is a very sensitive issue as it has a direct impact on the fairness of the <b>decision</b>-making process. More seriously, conditional statistical parity gives a <b>decision</b> <b>maker</b> a tool to game the system and realize a self-fulfilling prophecy. Therefore, it is recommended to ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Employment and Labor Law Exam</b> 1 Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/511400831/employment-and-labor-law-exam-1-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/511400831/<b>employment-and-labor-law-exam</b>-1-flash-cards", "snippet": "The key element in <b>disparate</b> <b>treatment</b> is discriminatory intent. In this context, that means that: a. the <b>decision</b>-<b>maker</b> made the <b>decision</b> with intent to harm b. the <b>decision</b>-<b>maker</b> made the <b>decision</b> with intent to break the law c. the <b>decision</b>-<b>maker</b> made the <b>decision</b> in whole or in part based on the protected class characteristic of the employee", "dateLastCrawled": "2021-06-19T03:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "3. Harm, Discrimination, and Measurement \u2014 Fairness &amp; Algorithmic ...", "url": "https://afraenkel.github.io/fairness-book/content/03-harms.html", "isFamilyFriendly": true, "displayUrl": "https://afraenkel.github.io/fairness-book/content/03-harms.html", "snippet": "<b>Disparate</b> <b>treatment</b> is the act of treating an individual unfavorably based on one of the protected characteristics listed above. This requires the cause or intent of the action is discriminatory. <b>Disparate</b> <b>treatment</b> is illegal in the US under Title VII of the Civil Rights Act. <b>Disparate</b> impact occurs when an act unfairly affects people identifying with one of the protect <b>groups</b> listed above, even if the intent of the action is necessarily discriminatory. <b>Disparate</b> impact is only illegal in ...", "dateLastCrawled": "2022-02-03T01:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Discrimination</b> in the Age of Algorithms | Journal of Legal Analysis ...", "url": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "snippet": "Concerns about <b>biased</b> data, potentially making out either <b>disparate</b> <b>treatment</b> or <b>disparate</b> impact, are very real, as we have discussed in detail above. But systematic differences across <b>groups</b> in the data\u2014that is, in the distributions of the candidate predictors, or in the outcome variable to be predicted\u2014are not by themselves a proof that use of the data is discriminatory in any legally relevant sense. 49", "dateLastCrawled": "2022-01-28T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "4 <b>Theories of Discrimination</b> | Measuring Racial <b>Discrimination</b> | The ...", "url": "https://www.nap.edu/read/10887/chapter/7", "isFamilyFriendly": true, "displayUrl": "https://www.nap.edu/read/10887/chapter/7", "snippet": "Instead, <b>decision</b> makers look for signals that cannot easily be faked and are correlated with the attributes a <b>decision</b> <b>maker</b> is seeking. Education is a prime example. If an employer checks a job applicant\u2019s education credentials and finds that he or she has a degree from a top-rated college and a 4.0 grade point average, that individual likely has a proven track record of intellectual ability and effort. It is difficult to \u201cfake\u201d this information (short of outright lying about one\u2019s ...", "dateLastCrawled": "2022-02-03T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Enhancing the Accuracy and Fairness of <b>Human</b> <b>Decision</b> Making | DeepAI", "url": "https://deepai.org/publication/enhancing-the-accuracy-and-fairness-of-human-decision-making", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/enhancing-the-accuracy-and-fairness-of-<b>human</b>-<b>decision</b>...", "snippet": "In this paper, we represent (<b>biased</b>) <b>human</b> <b>decision</b> making using threshold decisions rules ... we model a <b>biased</b> <b>human</b> <b>decision</b> <b>maker</b> v who has access to p Y | X, Z using the following threshold <b>decision</b> rule: d v (X, Z) = {1 if p Y = 1 | X, Z \u2265 \u03b8 V, Z 0 otherwise, (5) where \u03b8 V, Z \u2208 [0, 1] are constants that depend on the <b>decision</b> <b>maker</b> and the sensitive attribute, and they represent <b>human</b> <b>decision</b> makers\u2019 biases (or preferences) towards <b>groups</b> of people sharing a <b>certain</b> value of ...", "dateLastCrawled": "2022-01-21T15:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Legal framework: Anti-discrimination regulations in several countries, in particular US, distinguish between two legal frameworks, namely <b>disparate</b> <b>treatment</b> and <b>disparate</b> impact (Barocas &amp; Selbst, 2016). In the <b>disparate</b> <b>treatment</b> framework, a <b>decision</b> is considered unfair if it uses (directly or indirectly) the individual\u2019s sensitive attribute information. In the <b>disparate</b> impact framework, a <b>decision</b> is unfair if it results in an outcome that is disproportionately disadvantageous (or ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Policing and <b>Race: Disparate Treatment, Perceptions, and Policy</b> ...", "url": "https://spssi.onlinelibrary.wiley.com/doi/10.1111/sipr.12019", "isFamilyFriendly": true, "displayUrl": "https://spssi.onlinelibrary.wiley.com/doi/10.1111/sipr.12019", "snippet": "The Presence and Consequences of <b>Disparate</b> or <b>Biased</b> Policing. A robust literature of \u201cracial disparity\u201d policing research clearly documents unequal criminal justice outcomes between racial minorities and Whites. Racial disparities are defined as numerical differences in outcomes between racial <b>groups</b>, such that one group is disproportionately represented compared to others and/or the population. In the context of policing, for example, a clear racial disparity exists in that officers ...", "dateLastCrawled": "2021-12-21T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Chapter Four: Racial, Ethnic, and Gender Disparities in Federal ...", "url": "https://www.ussc.gov/sites/default/files/pdf/research-and-publications/research-projects-and-surveys/miscellaneous/15-year-study/chap4.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ussc.gov/sites/default/files/pdf/research-and-publications/research...", "snippet": "which large <b>groups</b> of offenders became subject to mandatory minimum drug sentences, the gap between African American offenders and other <b>groups</b> began to widen. The gap was greatest in the mid-1990s and has narrowed only slightly since then. <b>Similar</b> gaps or disproportionalities can be", "dateLastCrawled": "2022-02-02T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Employment and Labor Law Exam</b> 1 Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/511400831/employment-and-labor-law-exam-1-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/511400831/<b>employment-and-labor-law-exam</b>-1-flash-cards", "snippet": "The key element in <b>disparate</b> <b>treatment</b> is discriminatory intent. In this context, that means that: a. the <b>decision</b>-<b>maker</b> made the <b>decision</b> with intent to harm b. the <b>decision</b>-<b>maker</b> made the <b>decision</b> with intent to break the law c. the <b>decision</b>-<b>maker</b> made the <b>decision</b> in whole or in part based on the protected class characteristic of the employee", "dateLastCrawled": "2021-06-19T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Algorithmic bias detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/algorithmic-bias-detection-and-mitigation-best-", "snippet": "If left unchecked, <b>biased</b> algorithms can lead to decisions which can have a collective, <b>disparate</b> impact on <b>certain</b> <b>groups</b> of people even without the programmer\u2019s intention to discriminate. The ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Model Fairness &amp; Transparency</b>. A Project on Detecting, Understanding ...", "url": "https://medium.com/sfu-cspmp/model-transparency-fairness-552a747b444", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/sfu-cspmp/model-transparency-fairness-552a747b444", "snippet": "1. Introduction. B ias is a prejudice in favor or <b>against</b> a person, group, or a thing that is considered to be unfair. At present times, Machine Learning and Artificial Intelligence play a major ...", "dateLastCrawled": "2022-01-19T03:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "4 <b>Theories of Discrimination</b> | Measuring Racial <b>Discrimination</b> | The ...", "url": "https://www.nap.edu/read/10887/chapter/7", "isFamilyFriendly": true, "displayUrl": "https://www.nap.edu/read/10887/chapter/7", "snippet": "The <b>decision</b> <b>maker</b> typically views an individual\u2019s own statements about himself or herself as untrustworthy (e.g., \u201cI will work hard on this job\u201d or \u201cI am not a terrorist\u201d) because they <b>can</b> be made as easily by those for whom they are not true as by those for whom they are true. Instead, <b>decision</b> makers look for signals that cannot easily be faked and are correlated with the attributes a <b>decision</b> <b>maker</b> is seeking. Education is a prime example. If an employer checks a job applicant ...", "dateLastCrawled": "2022-02-03T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Chapter 3 Discrimination</b> - Employment and HR - United States", "url": "https://www.mondaq.com/unitedstates/human-resources/164254/chapter-3-discrimination", "isFamilyFriendly": true, "displayUrl": "https://<b>www.mondaq.com</b>/unitedstates/<b>human</b>-resources/164254", "snippet": "<b>Disparate</b> <b>treatment</b> occurs when an employer intentionally discriminates <b>against</b> someone because of a protected characteristic such as race, sex, or age. Examples include outright bias (such as refusing to hire members of a particular religion because of the employer&#39;s dislike for them), preferential <b>treatment</b> (such as favoring members of a particular ethnic group to the disadvantage of non-members), and acting on the basis of stereotypes (such as regarding older job applicants as too out of ...", "dateLastCrawled": "2022-01-21T14:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Human</b> <b>Judgment in algorithmic loops: Individual justice and</b> automated ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/rego.12358", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/rego.12358", "snippet": "In so far as algorithmic systems replace the <b>human</b> <b>decision</b>-<b>maker</b>&#39;s ability to be inconsistent, discriminatory, ... as well as aiming to avoid <b>disparate</b> outcomes between protected <b>groups</b>. In cases where the application of existing law is indeterminate, discretionary <b>decision</b>-making allows different dimensions to be weighed (Schauer 1987), and for broader political, economic, and moral considerations to come into play. 10 10 Although the appropriate role(s) of the latter are subject to debate ...", "dateLastCrawled": "2021-11-23T10:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "HUD&#39;s <b>Implementation of the Fair Housing</b> Act&#39;s <b>Disparate</b> Impact Standard", "url": "https://www.federalregister.gov/documents/2020/09/24/2020-19887/huds-implementation-of-the-fair-housing-acts-disparate-impact-standard", "isFamilyFriendly": true, "displayUrl": "https://<b>www.federalregister.gov</b>/documents/2020/09/24/2020-19887/huds-implementation-of...", "snippet": "Some commenters believed that the Proposed Rule sought to legalize housing discrimination and segregation or seek to reframe <b>disparate</b> impact as classic <b>disparate</b> <b>treatment</b>. Another commenter stated that HUD has failed to ask how the Proposed Rule might increase or decrease housing inequality or segregation. In this world of rapid societal change, the standards for proving <b>disparate</b> impact under the Fair Housing Act should stay the same so that the Act&#39;s remedial purpose <b>can</b> be effectuated ...", "dateLastCrawled": "2022-01-30T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Policing and <b>Race: Disparate Treatment, Perceptions, and Policy</b> ...", "url": "https://spssi.onlinelibrary.wiley.com/doi/full/10.1111/sipr.12019", "isFamilyFriendly": true, "displayUrl": "https://spssi.onlinelibrary.wiley.com/doi/full/10.1111/sipr.12019", "snippet": "In trying to understand the source of this <b>disparate</b> <b>treatment</b>, it highlights what empirical social science knows about racial bias in policing, emphasizing how contemporary forms of racial bias <b>can</b> contribute to racially <b>disparate</b> outcomes. The article makes a distinction between what research reveals about the existence of racial bias in policing and perceptions of racially <b>biased</b> policing. Existing racial bias is not always accurately identified nor perceived, which makes it harder to ...", "dateLastCrawled": "2022-01-30T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fall 2020 Data Ethics in Algorithmic <b>Decision</b> Making", "url": "https://www.csd.uoc.gr/~hy562/lectures20/lec08_Data_Ethics_20.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.csd.uoc.gr/~hy562/lectures20/lec08_Data_Ethics_20.pdf", "snippet": "\u2022 <b>Human</b> <b>decision</b>-making affected by greed, prejudice, fatigue, poor scalability, etc. and hence <b>can</b> be <b>biased</b>! Formal procedures <b>can</b> limit opportunities to exercise prejudicial discretion or fall victim to implicit bias \u2022 High-stakes scenarios = ethical problems! Despite existing legal/regulation efforts, current anti-discrimination laws are not yet well equipped to deal with various issues of discrimination in data analysis [S. Barocas, A.D. Selbst 2016] Fall 2020 8 Discrimination is ...", "dateLastCrawled": "2022-01-26T19:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Discriminated by an algorithm: a systematic review of discrimination ...", "url": "https://link.springer.com/article/10.1007/s40685-020-00134-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40685-020-00134-w", "snippet": "Algorithmic <b>decision</b>-making is becoming increasingly common as a new source of advice in HR recruitment and HR development. While firms implement algorithmic <b>decision</b>-making to save costs as well as increase efficiency and objectivity, algorithmic <b>decision</b>-making might also lead to the unfair <b>treatment</b> of <b>certain</b> <b>groups</b> of people, implicit discrimination, and perceived unfairness. Current knowledge about the threats of unfairness and (implicit) discrimination by algorithmic <b>decision</b>-making ...", "dateLastCrawled": "2022-02-02T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A survey of bias in <b>Machine Learning through the prism of Statistical</b> ...", "url": "https://deepai.org/publication/a-survey-of-bias-in-machine-learning-through-the-prism-of-statistical-parity-for-the-adult-data-set", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-of-bias-in-<b>machine-learning-through-the-prism</b>...", "snippet": "A critical question then recently arised among the population: Do algorithmic decisions convey any type of discrimination <b>against</b> specific <b>groups</b> of population or minorities? In this paper, we show the importance of understanding how a bias <b>can</b> be introduced into automatic decisions. We first present a mathematical framework for the fair learning problem, specifically in the binary classification setting. We then propose to quantify the presence of bias by using the standard <b>Disparate</b> Impact ...", "dateLastCrawled": "2021-12-05T13:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Social Media Screening and Procedural Justice: Towards Fairer Use of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8055055/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8055055", "snippet": "By assuming such a role, they <b>can</b> distance themselves purposefully from <b>certain</b> <b>groups</b> to appeal to an intended audience or create affiliation with <b>certain</b> desired social hierarchies and <b>groups</b>. Depending on the audience, users may remove negative contents or fake the information they post on a social media webpage. For example, they may \u201cfake good\u201d if they expect their parents or employers to view the page, or \u201cfake bad\u201d to impress friends (Davison et al.", "dateLastCrawled": "2022-02-03T05:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "When eliminating bias isn\u2019t fair: Algorithmic reductionism and ...", "url": "https://www.sciencedirect.com/science/article/pii/S0749597818303595", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0749597818303595", "snippet": "With that said, it is likely that, in contexts implicating people\u2019s livelihood (e.g., <b>human</b> resources), respectful <b>treatment</b> <b>can</b> be linked to the perceived accuracy of the <b>decision</b>. Indeed, a procedure that denies an employee a promotion without taking into account all the relevant facts or considering the broader context will likely feel inherently disrespectful. The perceived fairness of <b>decision</b> procedures is especially crucial in the design and implementation of a firm\u2019s processes ...", "dateLastCrawled": "2022-01-30T21:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Policing and <b>Race: Disparate Treatment, Perceptions, and Policy</b> ...", "url": "https://spssi.onlinelibrary.wiley.com/doi/full/10.1111/sipr.12019", "isFamilyFriendly": true, "displayUrl": "https://spssi.onlinelibrary.wiley.com/doi/full/10.1111/sipr.12019", "snippet": "The Presence and Consequences of <b>Disparate</b> or <b>Biased</b> Policing. A robust literature of \u201cracial disparity\u201d policing research clearly documents unequal criminal justice outcomes between racial minorities and Whites. Racial disparities are defined as numerical differences in outcomes between racial <b>groups</b>, such that one group is disproportionately represented <b>compared</b> to others and/or the population. In the context of policing, for example, a clear racial disparity exists in that officers ...", "dateLastCrawled": "2022-01-30T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Human</b> Judgment in algorithmic loops: Individual justice and automated ...", "url": "https://onlinelibrary.wiley.com/doi/10.1111/rego.12358", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/rego.12358", "snippet": "In so far as algorithmic systems replace the <b>human</b> <b>decision</b>-<b>maker</b>&#39;s ability to be inconsistent, discriminatory, ... as well as aiming to avoid <b>disparate</b> outcomes between protected <b>groups</b>. In cases where the application of existing law is indeterminate, discretionary <b>decision</b>-making allows different dimensions to be weighed (Schauer 1987), and for broader political, economic, and moral considerations to come into play. 10 10 Although the appropriate role(s) of the latter are subject to debate ...", "dateLastCrawled": "2022-01-29T07:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Algorithmic bias detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/algorithmic-bias-detection-and-mitigation-best-", "snippet": "If left unchecked, <b>biased</b> algorithms <b>can</b> lead to decisions which <b>can</b> have a collective, <b>disparate</b> impact on <b>certain</b> <b>groups</b> of people even without the programmer\u2019s intention to discriminate. The ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Social Media Screening and Procedural Justice: Towards Fairer Use of ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8055055/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8055055", "snippet": "By assuming such a role, they <b>can</b> distance themselves purposefully from <b>certain</b> <b>groups</b> to appeal to an intended audience or create affiliation with <b>certain</b> desired social hierarchies and <b>groups</b>. Depending on the audience, users may remove negative contents or fake the information they post on a social media webpage. For example, they may \u201cfake good\u201d if they expect their parents or employers to view the page, or \u201cfake bad\u201d to impress friends (Davison et al.", "dateLastCrawled": "2022-02-03T05:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "EQUALITY VS. EQUITY | American Journal of Law and Equality | MIT Press", "url": "https://direct.mit.edu/ajle/article/doi/10.1162/ajle_a_00019/107229/EQUALITY-VS-EQUITY", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/ajle/article/doi/10.1162/ajle_a_00019/107229/EQUALITY-VS-EQUITY", "snippet": "If the barrier is affecting <b>certain</b> <b>groups</b> in a disproportionately negative way, ... Their tastes may not be the same.\u201d 69 This objection might be overcome if the <b>decision</b>-<b>maker</b> works to learn the neighbor\u2019s desires and preferences and <b>can</b> craft the <b>treatment</b> accordingly. To treat others as they want to be treated might be understood as applying the golden rule to the golden rule, correcting the risk of missing the wants and needs of others. 70. Another problem with the \u201csameness ...", "dateLastCrawled": "2022-01-17T13:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Towards Supporting and Documenting Algorithmic Fairness in the Data ...", "url": "https://www.blaseur.com/papers/conpro19-cameraready.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.blaseur.com/papers/conpro19-cameraready.pdf", "snippet": "make better decisions than a <b>human</b> <b>decision</b>-<b>maker</b>. At the same time, because these sorts of decisions <b>can</b> seriously impact someone\u2019s life, there are concerns about whether these systems <b>can</b> make these decisions ethically. At the core of these automated <b>decision</b> systems are pre-dictive models created by data scientists to estimate outcomes related to the task. For example, data scientists trying to auto-mate loan decisions may create a model predicting the pro\ufb01t from granting a particular ...", "dateLastCrawled": "2021-10-24T07:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Zafar et al. (2017) formalized another fairness criterion, namely, <b>disparate</b> mistreatment according to which, a <b>decision</b> is unfair if it results in different misclassification rates for <b>groups</b> of people with different sensitive attribute information. Note that this criterion is currently not supported by a legal framework. Machine learning fairness notions <b>can</b> be classified according to the type of fairness it is evaluating. For instance, if a plaintiff is accusing an employer for ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Towards a Fairness-Aware Scoring System for Algorithmic <b>Decision</b>-Making ...", "url": "https://deepai.org/publication/towards-a-fairness-aware-scoring-system-for-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/towards-a-fairness-aware-scoring-system-for-algorithmic...", "snippet": "Even if in this case, their prediction results <b>can</b> still be <b>biased</b> <b>against</b> people with specific sensitive attribute values. This will lead to the problem of unfair allocation of medical resources among patients from different <b>groups</b>. Lots of literature attests this bias <b>against</b> <b>certain</b> <b>groups</b> (e.g., the minority) does exist in many healthcare delivery fields. For example, several empirical studies show black patients are less likely to be selected for organ transplants, survive cardiac ...", "dateLastCrawled": "2022-01-28T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Performance Information, Racial Bias, and Citizen Evaluations of ...", "url": "https://academic.oup.com/jpart/article/31/3/523/5973590", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jpart/article/31/3/523/5973590", "snippet": "Yet, many of the biases that lead public employees to discriminate <b>against</b> <b>certain</b> <b>groups</b> of citizens are fundamental <b>to human</b> cognition and therefore are also likely to result in citizens discriminating <b>against</b> <b>certain</b> <b>groups</b> of public officials (Nosek, Banaji, and Greenwald 2002). This means that discrimination is likely a two-way street\u2014while public employees discriminate <b>against</b> citizens, citizens <b>can</b> also discriminate <b>against</b> public employees. Nevertheless, the prospect of citizens ...", "dateLastCrawled": "2022-01-28T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Model Fairness &amp; Transparency</b>. A Project on Detecting, Understanding ...", "url": "https://medium.com/sfu-cspmp/model-transparency-fairness-552a747b444", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/sfu-cspmp/model-transparency-fairness-552a747b444", "snippet": "1. Introduction. B ias is a prejudice in favor or <b>against</b> a person, group, or a thing that is considered to be unfair. At present times, Machine Learning and Artificial Intelligence play a major ...", "dateLastCrawled": "2022-01-19T03:26:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>learning</b> and applications in microbiology", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8498514/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8498514", "snippet": "<b>Machine</b> <b>learning</b> has two main <b>learning</b> modes: supervised (also known as predictive) to make future predictions from training data, and unsupervised (descriptive), which is exploratory in nature without training data, defined target or output (Mitchell 1997). Training data are the initial information used to teach supervised ML algorithms in the process of developing a model, from which the model creates and refines its rules required for prediction. Typically, training data comprises a set ...", "dateLastCrawled": "2021-12-06T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b>: Science and Technology, Volume 2, Number 1, March ...", "url": "https://iopscience.iop.org/issue/2632-2153/2/1", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/issue/2632-2153/2/1", "snippet": "<b>Machine</b> <b>learning</b> has been used in high energy physics (HEP) for a long time, primarily at the analysis level with supervised classification. Quantum computing was postulated in the early 1980s as way to perform computations that would not be tractable with a classical computer. With the advent of noisy intermediate-scale quantum computing devices, more quantum algorithms are being developed with the aim at exploiting the capacity of the hardware for <b>machine</b> <b>learning</b> applications. An ...", "dateLastCrawled": "2021-12-16T00:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Realistically Integrating <b>Machine</b> <b>Learning</b> Into Clinical Pra ...", "url": "https://journals.lww.com/anesthesia-analgesia/fulltext/2020/05000/realistically_integrating_machine_learning_into.4.aspx", "isFamilyFriendly": true, "displayUrl": "https://<b>journals.lww.com</b>/.../05000/realistically_integrating_<b>machine</b>_<b>learning</b>_into.4.aspx", "snippet": "<b>Machine</b>-<b>learning</b> models have been created to predict an increasing number of clinical outcomes, such as diagnoses and mortality, with applications including C. difficile infection in the inpatient hospital setting, 2 identifying molecular markers for cancer treatments, 3 and postoperative surgical outcomes. 4 Examples of <b>machine</b> <b>learning</b> include a cardiologist using an automated interpretation of an ECG and a radiologist using an automated detection of a lung nodule in a chest x-ray. In both ...", "dateLastCrawled": "2021-11-22T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning</b>, artificial neural networks and social research ...", "url": "https://link.springer.com/article/10.1007/s11135-020-01037-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11135-020-01037-y", "snippet": "<b>Machine learning</b> (ML), and particularly algorithms based on artificial neural networks (ANNs), constitute a field of research lying at the intersection of different disciplines such as mathematics, statistics, computer science and neuroscience. This approach is characterized by the use of algorithms to extract knowledge from large and heterogeneous data sets. In addition to offering a brief introduction to ANN algorithms-based ML, in this paper we will focus our attention on its possible ...", "dateLastCrawled": "2022-01-27T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Immune System Computes the State of the Body: Crowd Wisdom, <b>Machine</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fimmu.2019.00010/full", "isFamilyFriendly": true, "displayUrl": "https://www.<b>frontiers</b>in.org/articles/10.3389/fimmu.2019.00010", "snippet": "The correlations between the components comprising a set of data can be very subtle and obscure to the human observer, yet such correlations are detectable by <b>machine</b> <b>learning</b> algorithms, and, by <b>analogy</b>, by networks of cells and antibodies in the immune system. As a consequence of exposure to training sets of input, the computer algorithm\u2014and the immune system\u2014can accumulate a bank of learned correlations. These formative correlations can then be used by the computer or the repertoire ...", "dateLastCrawled": "2022-01-30T04:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Protein function in precision medicine: deep understanding with <b>machine</b> ...", "url": "https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.12307", "isFamilyFriendly": true, "displayUrl": "https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.12307", "snippet": "<b>disparate</b> parts may ultimately lead to the same observable effect. In this <b>analogy</b>, we might argue that medicine has so far been often investing into mitigat- ing the inconvenience with lemons and much less into improving and augmenting the protocols for \ufb01nding the individual causes of problems. In his recent State-of-the-Union address, the US Pres-ident Barack Obama announced the Precision Medicine Initiative, making this challenge a national and interna-tional priority. Precision ...", "dateLastCrawled": "2021-12-12T19:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Solution Manual Alpaydin Introduction To <b>Machine</b> <b>Learning</b>", "url": "https://se.rangle.io/solution+manual+alpaydin+introduction+to+machine+learning+pdf", "isFamilyFriendly": true, "displayUrl": "https://se.rangle.io/solution+manual+alpaydin+introduction+to+<b>machine</b>+<b>learning</b>+pdf", "snippet": "With in-depth Python and MATLAB/OCTAVE-based computational exercises and a complete <b>treatment</b> of cutting edge numerical optimization techniques, this is an essential resource for students and an ideal reference for researchers and practitioners working in <b>machine</b> <b>learning</b>, computer science, electrical engineering, signal processing, and numerical optimization. Handbook of Research on Innovations in Database Technologies and Applications One of the currently most active research areas within ...", "dateLastCrawled": "2022-01-11T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Latent bias and the implementation of <b>artificial intelligence</b> in ...", "url": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "snippet": "<b>Artificial intelligence</b> (AI) in general, and <b>machine</b> <b>learning</b> in particular, by all accounts, appear poised to revolutionize medicine. 1\u20133 With a wide spectrum of potential uses across translational research (from bench to bedside to health policy), clinical medicine (including diagnosis, <b>treatment</b>, prediction, and healthcare resource allocation), and public health, every area of medicine will be affected.", "dateLastCrawled": "2022-01-28T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "47 <b>Analogy</b> Examples To Make You As Sharp As A Tack (and then some)", "url": "https://www.greetingcardpoet.com/good-analogy-examples-and-definition/", "isFamilyFriendly": true, "displayUrl": "https://www.greetingcardpoet.com/<b>good-analogy-examples-and-definition</b>", "snippet": "The essence of this literary device is to set up a comparison that highlights similarities between two seemingly <b>disparate</b> items. It is by examining how the two items are alike in some way that leads to a clear understanding. Differences between Similes, Metaphors, and Analogies . While similes, metaphors, and analogies are similar in that they all compare two different things, similes and metaphors are figures of speech. In contrast, an <b>analogy</b> is more akin to a logical argument. A writer ...", "dateLastCrawled": "2022-02-02T03:57:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A primer on AI <b>fairness</b>. What it is and the tradeoffs to be made | by ...", "url": "https://towardsdatascience.com/artificial-intelligence-fairness-and-tradeoffs-ce11ac284b63", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/artificial-intelligence-<b>fairness</b>-and-tradeoffs-ce11ac284b63", "snippet": "A <b>machine</b> <b>learning</b> algorithms value is being able to increase the number of true positives and true negatives, which each have a value attached. Each false positive and false negative is costly. The value assigned to each depends on each context. A false negative is more costly in medical situations while a false positive is costlier in death penalty decisions. Expected value is profits that businesses can expect from using the algorithm. The more accurate the model, the higher the profits.", "dateLastCrawled": "2022-02-02T02:42:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(disparate treatment)  is like +(human decision maker biased against certain groups)", "+(disparate treatment) is similar to +(human decision maker biased against certain groups)", "+(disparate treatment) can be thought of as +(human decision maker biased against certain groups)", "+(disparate treatment) can be compared to +(human decision maker biased against certain groups)", "machine learning +(disparate treatment AND analogy)", "machine learning +(\"disparate treatment is like\")", "machine learning +(\"disparate treatment is similar\")", "machine learning +(\"just as disparate treatment\")", "machine learning +(\"disparate treatment can be thought of as\")", "machine learning +(\"disparate treatment can be compared to\")"]}