{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The Mathematics Behind the <b>Regression</b> Algorithms in <b>Machine</b> <b>Learning</b> ...", "url": "https://juliebutlerhartley.medium.com/the-mathematics-behind-the-regression-algorithms-in-machine-learning-e871ecb85d32", "isFamilyFriendly": true, "displayUrl": "https://juliebutlerhartley.medium.com/the-mathematics-behind-the-<b>regression</b>-<b>algorithms</b>...", "snippet": "Linear <b>regression</b> (or ordinary <b>least</b> <b>squares</b> <b>regression</b>) is the most basic <b>regression</b> <b>algorithm</b>. In addition to its uses in <b>machine</b> <b>learning</b>, it is also frequently seen in statistics. Linear <b>regression</b> is typically used to fit data whose shape roughly corresponds to a polynomial, but it can be used for classification also. The use of an appropriate design matrix can also greatly extend the applications of linear <b>regression</b>.", "dateLastCrawled": "2022-01-31T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "[<b>Machine</b> <b>Learning</b>] <b>Least</b> <b>Squares</b> Estimation - Anthony Tan&#39;s Blogs", "url": "https://anthony-tan.com/ml-least_squares_estimaton/", "isFamilyFriendly": true, "displayUrl": "https://anthony-tan.com/ml-<b>least</b>_<b>squares</b>_estimaton", "snippet": "[<b>Machine</b> <b>Learning</b>] <b>Least</b> <b>Squares</b> Estimation. Prerequisites. Basic concepts of <b>machine</b> <b>learning</b>, <b>like</b> model and <b>regression</b>, training set, and test set; Linear algebra, especially the space, and matrix calculation Linear <b>Regression</b> 1. Linear <b>regression</b> is a good problem to begin our <b>machine</b> <b>learning</b> career not only because of its easy form but also its well-defined <b>machine</b> <b>learning</b> concepts. It\u2019s a good basis for more complicated methods, <b>like</b> nonlinear <b>regression</b>, classification, even deep ...", "dateLastCrawled": "2022-01-28T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Least</b> <b>Squares</b> <b>Regression</b> - msg <b>Machine Learning Catalogue</b>", "url": "https://machinelearningcatalogue.com/algorithm/alg_least-squares-regression.html", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearningcatalogue</b>.com/<b>algorithm</b>/alg_<b>least</b>-<b>squares</b>-<b>regression</b>.html", "snippet": "A catalogue of <b>machine</b> <b>learning</b> methods and use cases. <b>Least</b> <b>Squares</b> <b>Regression</b> is used to model the effect of 1\u2026n predictor variables on a dependent variable.It works by finding the optimal set of coefficients with which to multiply together each predictor variable to obtain an estimation of the dependent variable.", "dateLastCrawled": "2022-02-03T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Types of Machine Learning Algorithm</b>", "url": "https://scientistcafe.com/2017/07/08/machinelearningal", "isFamilyFriendly": true, "displayUrl": "https://scientistcafe.com/2017/07/08/<b>machinelearning</b>al", "snippet": "It is often called the statistical <b>machine</b> <b>learning</b> method. Standard <b>regression</b> models are: Ordinary <b>Least</b> <b>Squares</b> <b>Regression</b>; Logistic <b>Regression</b>; Multivariate Adaptive <b>Regression</b> Splines (MARS) Locally Estimated Scatterplot Smoothing (LOESS) The <b>least</b> <b>squares</b> <b>regression</b> and logistic <b>regression</b> are traditional statistical models. Both of them are highly interpretable. MARS is similar to neural networks and partial <b>least</b> <b>squares</b> (PLS) in the respect that they all use surrogate features ...", "dateLastCrawled": "2022-02-01T13:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning: the Basics</b> - Andrew Gibiansky", "url": "https://andrew.gibiansky.com/blog/machine-learning/machine-learning-the-basics/", "isFamilyFriendly": true, "displayUrl": "https://andrew.gibiansky.com/blog/<b>machine</b>-<b>learning</b>/<b>machine-learning-the-basics</b>", "snippet": "Your First <b>Machine</b> <b>Learning</b> <b>Algorithm</b>: Linear <b>Regression</b>. Although it may not seem <b>like</b> it, linear <b>regression</b> - also known as <b>least</b> <b>squares</b> <b>regression</b> - is a type of supervised <b>machine</b> <b>learning</b> <b>algorithm</b>. Suppose we have some data set \\(X\\) and the responses, \\(Y\\). \\[X = \\begin{bmatrix}1.0 \\\\ 2.5 \\\\ 7.3 \\end{bmatrix},\\; Y = \\begin{bmatrix}3.0 \\\\ 9.3 \\\\ 19.5 \\end{bmatrix}\\] We want to develop the best linear model \\(\\hat{y}\\) of two parameters which predicts the value of \\(y\\) given the ...", "dateLastCrawled": "2022-01-29T01:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "8 Popular <b>Regression Algorithms In Machine Learning</b> Of 2021", "url": "https://www.jigsawacademy.com/popular-regression-algorithms-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.jigsawacademy.com/popular-<b>regression</b>-<b>algorithms</b>-ml", "snippet": "Ridge <b>Regression</b> is another popularly used linear <b>regression</b> <b>algorithm</b> in <b>Machine</b> <b>Learning</b>. If only one independent variable is being used to predict the output, it will be termed as a linear <b>regression</b> ML <b>algorithm</b>. ML experts prefer Ridge <b>regression</b> as it minimizes the loss encountered in linear <b>regression</b> (discussed above). In place of OLS (Ordinary <b>Least</b> <b>Squares</b>), the output values are predicted by a ridge estimator in ridge <b>regression</b>. The above-discussed linear <b>regression</b> uses OLS to ...", "dateLastCrawled": "2022-02-03T06:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Partial <b>Least</b> <b>Squares</b> | Towards Data Science", "url": "https://towardsdatascience.com/partial-least-squares-f4e6714452a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/partial-<b>least</b>-<b>squares</b>-f4e6714452a", "snippet": "Partial <b>Least</b> <b>Squares</b>, as said before, is a variation on Ordinary <b>Least</b> <b>Squares</b> (Linear <b>Regression</b>). Because of this, Partial <b>Least</b> <b>Squares</b> cannot be applied to nonlinear problems. Kernel PLS solves this problem and makes Partial <b>Least</b> <b>Squares</b> available for nonlinear problems. Kernel PLS fits a relationship between input and output variables in a high-dimensional space so that the input data set can be considered linear.", "dateLastCrawled": "2022-02-02T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Regression Analysis in Machine learning</b> - Javatpoint", "url": "https://www.javatpoint.com/regression-analysis-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>regression-analysis-in-machine-learning</b>", "snippet": "<b>Regression Analysis in Machine learning</b>. <b>Regression</b> analysis is a statistical method to model the relationship between a dependent (target) and independent (predictor) variables with one or more independent variables. More specifically, <b>Regression</b> analysis helps us to understand how the value of the dependent variable is changing corresponding ...", "dateLastCrawled": "2022-01-30T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>Learning</b>-<b>Regression</b> Algorithms(Linear <b>regression</b>) | by Mohamed ...", "url": "https://medium.com/nerd-for-tech/machine-learning-regression-algorithms-linear-regression-ea97c92f2d9b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/<b>machine</b>-<b>learning</b>-<b>regression</b>-<b>algorithms</b>-linear...", "snippet": "<b>Machine</b> <b>Learning</b> is about Creating an <b>algorithm</b> for which the computer finds a model to fit the data as best as possible and accurately predict. The <b>machine</b> <b>learning</b> <b>algorithm</b> learns the function\u2026", "dateLastCrawled": "2022-01-26T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to <b>Use Optimization Algorithms to Manually Fit Regression</b> Models", "url": "https://machinelearningmastery.com/optimize-regression-models/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/optimize-<b>regression</b>-models", "snippet": "In the case of linear <b>regression</b>, the coefficients can be found by <b>least</b> <b>squares</b> optimization, which can be solved using linear algebra. In the case of logistic <b>regression</b>, a local search optimization <b>algorithm</b> is commonly used.", "dateLastCrawled": "2022-01-26T03:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The Mathematics Behind the <b>Regression</b> Algorithms in <b>Machine</b> <b>Learning</b> ...", "url": "https://juliebutlerhartley.medium.com/the-mathematics-behind-the-regression-algorithms-in-machine-learning-e871ecb85d32", "isFamilyFriendly": true, "displayUrl": "https://juliebutlerhartley.medium.com/the-mathematics-behind-the-<b>regression</b>-<b>algorithms</b>...", "snippet": "Linear <b>regression</b> (or ordinary <b>least</b> <b>squares</b> <b>regression</b>) is the most basic <b>regression</b> <b>algorithm</b>. In addition to its uses in <b>machine</b> <b>learning</b>, it is also frequently seen in statistics. Linear <b>regression</b> is typically used to fit data whose shape roughly corresponds to a polynomial, but it can be used for classification also. The use of an appropriate design matrix can also greatly extend the applications of linear <b>regression</b>.", "dateLastCrawled": "2022-01-31T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The Most Common <b>Machine</b> <b>Learning</b> <b>Regression</b> Algorithms for Data Science ...", "url": "https://medium.com/swlh/types-of-regression-algorithms-eb792039a554", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/types-of-<b>regression</b>-<b>algorithms</b>-eb792039a554", "snippet": "18. Partial <b>Least</b> <b>Squares</b> <b>Regression</b>. Partial <b>least</b> <b>squares</b> <b>regression</b> (PLS <b>regression</b>) is developed from principal components <b>regression</b>. It works in a <b>similar</b> fashion as it finds a linear ...", "dateLastCrawled": "2022-01-28T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Analysis Of OLS <b>Regression</b> \u2014 <b>Machine</b> <b>Learning</b> Algorithms | by Aswin ...", "url": "https://medium.com/nerd-for-tech/analysis-of-ols-regression-machine-learning-algorithms-783c37d10aa3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/.../analysis-of-ols-<b>regression</b>-<b>machine</b>-<b>learning</b>-<b>algorithms</b>-783c37d10aa3", "snippet": "Analysis Of OLS <b>Regression</b> \u2014 <b>Machine</b> <b>Learning</b> Algorithms. Ordinary <b>Least</b> <b>Squares</b> <b>Regression</b> Explained . Aswin Vijayakumar. Follow. Aug 15 \u00b7 5 min read. OLS <b>Regression</b>. <b>Regression</b> technique is ...", "dateLastCrawled": "2021-12-20T16:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Types of Machine Learning Algorithm</b>", "url": "https://scientistcafe.com/2017/07/08/machinelearningal", "isFamilyFriendly": true, "displayUrl": "https://scientistcafe.com/2017/07/08/<b>machinelearning</b>al", "snippet": "It is often called the statistical <b>machine</b> <b>learning</b> method. Standard <b>regression</b> models are: Ordinary <b>Least</b> <b>Squares</b> <b>Regression</b>; Logistic <b>Regression</b>; Multivariate Adaptive <b>Regression</b> Splines (MARS) Locally Estimated Scatterplot Smoothing (LOESS) The <b>least</b> <b>squares</b> <b>regression</b> and logistic <b>regression</b> are traditional statistical models. Both of them are highly interpretable. MARS <b>is similar</b> to neural networks and partial <b>least</b> <b>squares</b> (PLS) in the respect that they all use surrogate features ...", "dateLastCrawled": "2022-02-01T13:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "8 Popular <b>Regression Algorithms In Machine Learning</b> Of 2021", "url": "https://www.jigsawacademy.com/popular-regression-algorithms-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.jigsawacademy.com/popular-<b>regression</b>-<b>algorithms</b>-ml", "snippet": "Ridge <b>Regression</b> is another popularly used linear <b>regression</b> <b>algorithm</b> in <b>Machine</b> <b>Learning</b>. If only one independent variable is being used to predict the output, it will be termed as a linear <b>regression</b> ML <b>algorithm</b>. ML experts prefer Ridge <b>regression</b> as it minimizes the loss encountered in linear <b>regression</b> (discussed above). In place of OLS (Ordinary <b>Least</b> <b>Squares</b>), the output values are predicted by a ridge estimator in ridge <b>regression</b>. The above-discussed linear <b>regression</b> uses OLS to ...", "dateLastCrawled": "2022-02-03T06:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Least</b> <b>Squares</b> <b>Regression</b> - msg <b>Machine Learning Catalogue</b>", "url": "https://machinelearningcatalogue.com/algorithm/alg_least-squares-regression.html", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearningcatalogue</b>.com/<b>algorithm</b>/alg_<b>least</b>-<b>squares</b>-<b>regression</b>.html", "snippet": "A catalogue of <b>machine</b> <b>learning</b> methods and use cases. <b>Least</b> <b>Squares</b> <b>Regression</b> is used to model the effect of 1\u2026n predictor variables on a dependent variable.It works by finding the optimal set of coefficients with which to multiply together each predictor variable to obtain an estimation of the dependent variable.", "dateLastCrawled": "2022-02-03T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>The Least Squares Regression Method</b> \u2013 How to Find the Line of Best Fit", "url": "https://www.freecodecamp.org/news/the-least-squares-regression-method-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.freecodecamp.org/news/<b>the-least-squares-regression-method</b>-explained", "snippet": "What is <b>the Least Squares Regression method</b> and why use it? <b>Least</b> <b>squares</b> is a method to apply linear <b>regression</b>. It helps us predict results based on an existing set of data as well as clear anomalies in our data. Anomalies are values that are too good, or bad, to be true or that represent rare cases. For example, say we have a list of how many topics future engineers here at freeCodeCamp can solve if they invest 1, 2, or 3 hours continuously. Then we can predict how many topics will be ...", "dateLastCrawled": "2022-02-03T15:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Least Angle Regression (LARS) - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/least-angle-regression-lars/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>least</b>-angle-<b>regression</b>-lars", "snippet": "<b>Regression</b> is a supervised <b>machine</b> <b>learning</b> task that can predict continuous values (real numbers), as compared to classification, that can predict categorical or discrete values. Before we begin, if you are a beginner, I highly recommend this article. <b>Least</b> Angle <b>Regression</b> (LARS) is an <b>algorithm</b> used in <b>regression</b> for high dimensional data (i.e., data with a large number of attributes). <b>Least</b> Angle <b>Regression</b> is somewhat <b>similar</b> to forward stepwise <b>regression</b>. Since it is used with data ...", "dateLastCrawled": "2022-01-26T14:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Regression Analysis in Machine learning</b> - Javatpoint", "url": "https://www.javatpoint.com/regression-analysis-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>regression-analysis-in-machine-learning</b>", "snippet": "<b>Regression Analysis in Machine learning</b>. <b>Regression</b> analysis is a statistical method to model the relationship between a dependent (target) and independent (predictor) variables with one or more independent variables. More specifically, <b>Regression</b> analysis helps us to understand how the value of the dependent variable is changing corresponding ...", "dateLastCrawled": "2022-01-30T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to <b>Use Optimization Algorithms to Manually Fit Regression</b> Models", "url": "https://machinelearningmastery.com/optimize-regression-models/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/optimize-<b>regression</b>-models", "snippet": "In the case of linear <b>regression</b>, the coefficients can be found by <b>least</b> <b>squares</b> optimization, which can be solved using linear algebra. In the case of logistic <b>regression</b>, a local search optimization <b>algorithm</b> is commonly used.", "dateLastCrawled": "2022-01-26T03:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least</b> <b>Squares</b> \u2013 Towards Data Science", "url": "https://towardsdatascience.com/tagged/least-squares", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/tagged/<b>least</b>-<b>squares</b>", "snippet": "The <b>least</b>-square estimation is one of the most widely used techniques used in <b>machine</b> <b>learning</b>, signal processing, and statistics. It is the common way of solving the linear <b>regression</b> used widely to model continuous outcomes. It <b>can</b> be modeled as an MMSE estimator or a Bayes estimator with a quadratic\u2026.", "dateLastCrawled": "2022-02-02T15:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "5.2 <b>Least Squares Linear Regression</b> - GitHub Pages", "url": "https://jermwatt.github.io/machine_learning_refined/notes/5_Linear_regression/5_2_Least.html", "isFamilyFriendly": true, "displayUrl": "https://jermwatt.github.io/<b>machine</b>_<b>learning</b>_refined/notes/5_Linear_<b>regression</b>/5_2...", "snippet": "However the <b>Least</b> <b>Squares</b> cost function for linear <b>regression</b> <b>can</b> mathematically shown to be ... differentiator (autograd - as detailed in Section 3.5) to perform both gradient descent and Newton&#39;s method on our <b>machine</b> <b>learning</b> cost functions, here one <b>can</b> (since this cost function is simple enough) &#39;hard code&#39; the gradient by formally by writing it out &#39;by hand&#39; (using the derivative rules detailed in the Appendix). Doing so one <b>can</b> compute the gradient of the <b>Least</b> <b>Squares</b> cost in closed ...", "dateLastCrawled": "2022-02-02T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Least</b> <b>Squares</b> <b>Regression</b> Principal Component Analysis", "url": "https://upcommons.upc.edu/bitstream/handle/2117/331676/Thesis_def.pdf?sequence=2", "isFamilyFriendly": true, "displayUrl": "https://upcommons.upc.edu/bitstream/handle/2117/331676/Thesis_def.pdf?sequence=2", "snippet": "<b>Least</b> <b>Squares</b> <b>Regression</b> Principal Component Analysis A supervised dimensionality reduction method for <b>machine</b> <b>learning</b> in scienti c applications Author: H ector Pascual Herrero Supervisor: Dr. Xin Yee Dr. Joan Torras June 29, 2020. Abstract Dimension reduction is an important technique in surrogate modeling and <b>machine</b> <b>learning</b>. In this thesis, we present three existing dimension reduction methods in de-tail and then we propose a novel supervised dimension reduction method, \u2018<b>Least</b> <b>Squares</b> ...", "dateLastCrawled": "2022-01-21T08:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Ordinary <b>Least</b> <b>Squares</b> Linear <b>Regression</b>: Flaws, Problems and Pitfalls ...", "url": "https://www.clockbackward.com/2009/06/18/ordinary-least-squares-linear-regression-flaws-problems-and-pitfalls/", "isFamilyFriendly": true, "displayUrl": "https://www.clockbackward.com/2009/06/18/ordinary-<b>least</b>-<b>squares</b>-linear-<b>regression</b>...", "snippet": "<b>Least</b> <b>squares</b> <b>regression</b> is particularly prone to this problem, for as soon as the number of features used exceeds the number of training data points, the <b>least</b> <b>squares</b> solution will not be unique, and hence the <b>least</b> <b>squares</b> <b>algorithm</b> will fail. In practice, as we add a large number of independent variables to our <b>least</b> <b>squares</b> model, the performance of the method will typically erode before this critical point (where the number of features begins to exceed the number of training points) is ...", "dateLastCrawled": "2022-01-25T18:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Types of Machine Learning Algorithms</b> | 4 Fundamental Types", "url": "https://www.educba.com/types-of-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>types-of-machine-learning-algorithms</b>", "snippet": "The <b>least</b>-<b>squares</b> is a strategy for performing direct <b>regression</b>. direct <b>regression</b> is the undertaking of fitting a line through a lot of focuses. There are various potential procedures to do this, and the \u201cordinary <b>least</b> <b>squares</b>\u201d system go like this\u2014 You <b>can</b> draw a line, and after that, for all of the data centers, measure the vertical detachment between the point and the line, and incorporate these up; the fitted line would be the place this aggregate of partitions is as meager as ...", "dateLastCrawled": "2022-02-01T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "8 Popular <b>Regression Algorithms In Machine Learning</b> Of 2021", "url": "https://www.jigsawacademy.com/popular-regression-algorithms-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.jigsawacademy.com/popular-<b>regression</b>-<b>algorithms</b>-ml", "snippet": "Ridge <b>Regression</b> is another popularly used linear <b>regression</b> <b>algorithm</b> in <b>Machine</b> <b>Learning</b>. If only one independent variable is being used to predict the output, it will be termed as a linear <b>regression</b> ML <b>algorithm</b>. ML experts prefer Ridge <b>regression</b> as it minimizes the loss encountered in linear <b>regression</b> (discussed above). In place of OLS (Ordinary <b>Least</b> <b>Squares</b>), the output values are predicted by a ridge estimator in ridge <b>regression</b>. The above-discussed linear <b>regression</b> uses OLS to ...", "dateLastCrawled": "2022-02-03T06:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How <b>does a linear regression qualifies as a machine</b> <b>learning</b> <b>algorithm</b> ...", "url": "https://www.quora.com/How-does-a-linear-regression-qualifies-as-a-machine-learning-algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-<b>does-a-linear-regression-qualifies-as-a-machine</b>-<b>learning</b>...", "snippet": "Answer (1 of 5): <b>Machine</b> <b>learning</b> simply refers to computers <b>learning</b> and making predictions from data. Linear <b>regression</b> does this, as more data improves its predictive ability. Although other methods like SVMs and neural networks are more generally <b>thought</b> <b>of as machine</b> <b>learning</b>, these algorith...", "dateLastCrawled": "2022-01-20T14:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How <b>to Develop LARS Regression Models in Python</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/lars-regression-with-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/lars-<b>regression</b>-with-python", "snippet": "<b>Regression</b> is a modeling task that involves predicting a numeric value given an input. Linear <b>regression</b> is the standard <b>algorithm</b> for <b>regression</b> that assumes a linear relationship between inputs and the target variable. An extension to linear <b>regression</b> involves adding penalties to the loss function during training that encourage simpler models that have smaller coefficient values. These extensions are referred to", "dateLastCrawled": "2022-02-03T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Top 10 Machine Learning Algorithms</b> - Is AI Scary in 2022?", "url": "https://techjury.net/blog/machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://techjury.net/blog/<b>machine-learning-algorithms</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> generate up to $1 billion per year in the pharmaceutical industry. In the next 10 years, ... There are many types of <b>regression</b> \u2013 linear, logistic, polynomial, ordinary <b>least</b> <b>squares</b> <b>regression</b>, and so on. Today we\u2019ll just cover the first 2 types because otherwise this will be better published as a book, rather than an article. As we\u2019ll see in a moment, most of the top 10 algorithms are supervised <b>learning</b> algorithms and are best used with Python. Here comes the ...", "dateLastCrawled": "2022-02-02T02:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>the worst machine learning algorithm</b>? - Quora", "url": "https://www.quora.com/What-is-the-worst-machine-learning-algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-worst-machine-learning-algorithm</b>", "snippet": "Answer (1 of 16): Good question! People are always obsessed with the best model, the best <b>algorithm</b>, etc. Lets start with the most fundamental problem in <b>machine</b> <b>learning</b> - linear <b>regression</b> - and see if we <b>can</b> find the \u2018worst\u2019 linear regressor for the dataset \\left\\{ \\left(\\mathbf{x}_{p},y_{p}\\...", "dateLastCrawled": "2022-01-22T10:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Analysis Of OLS <b>Regression</b> \u2014 <b>Machine</b> <b>Learning</b> Algorithms | by Aswin ...", "url": "https://medium.com/nerd-for-tech/analysis-of-ols-regression-machine-learning-algorithms-783c37d10aa3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/.../analysis-of-ols-<b>regression</b>-<b>machine</b>-<b>learning</b>-<b>algorithms</b>-783c37d10aa3", "snippet": "Analysis Of OLS <b>Regression</b> \u2014 <b>Machine</b> <b>Learning</b> Algorithms. Ordinary <b>Least</b> <b>Squares</b> <b>Regression</b> Explained . Aswin Vijayakumar. Follow. Aug 15 \u00b7 5 min read. OLS <b>Regression</b>. <b>Regression</b> technique is ...", "dateLastCrawled": "2021-12-20T16:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Ordinary <b>Least</b> <b>Squares</b> Linear <b>Regression</b>: Flaws, Problems and Pitfalls ...", "url": "https://www.clockbackward.com/2009/06/18/ordinary-least-squares-linear-regression-flaws-problems-and-pitfalls/", "isFamilyFriendly": true, "displayUrl": "https://www.clockbackward.com/2009/06/18/ordinary-<b>least</b>-<b>squares</b>-linear-<b>regression</b>...", "snippet": "<b>Least</b> <b>squares</b> <b>regression</b> is particularly prone to this problem, for as soon as the number of features used exceeds the number of training data points, the <b>least</b> <b>squares</b> solution will not be unique, and hence the <b>least</b> <b>squares</b> <b>algorithm</b> will fail. In practice, as we add a large number of independent variables to our <b>least</b> <b>squares</b> model, the performance of the method will typically erode before this critical point (where the number of features begins to exceed the number of training points) is ...", "dateLastCrawled": "2022-01-25T18:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Online Learning: Recursive Least Squares and Online</b> PCA | by Pier Paolo ...", "url": "https://towardsdatascience.com/online-learning-recursive-least-squares-and-online-pca-c05bd23106c9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>online-learning-recursive-least-squares-and-online</b>-pca...", "snippet": "Online <b>Learning</b>, is a subset of <b>Machine</b> <b>Learning</b> which emphasizes the fact that data generated from environments <b>can</b> change over time. In fact, traditional <b>Machine</b> <b>Learning</b> models are instead\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Online Learning: Recursive Least Squares and Online</b> PCA. A practical introduction on how to create online <b>Machine</b> <b>Learning</b> models able to ...", "dateLastCrawled": "2022-02-03T04:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "RLS: <b>Learning</b> on the Fly. A simple model that learns on the fly\u2026 | by ...", "url": "https://towardsdatascience.com/recursive-least-squares-learning-on-the-fly-f8bb878eb270", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/recursive-<b>least</b>-<b>squares</b>-<b>learning</b>-on-the-fly-f8bb878eb270", "snippet": "The RLS <b>algorithm</b> is able to estimate the optimum weights according to the <b>least</b>-<b>squares</b> solution without explicitly computing the inverse operation in the pseudo-inverse. This makes it a powerful <b>algorithm</b> for online <b>learning</b> applications, where updates to estimations need to be made on the fly. The model was <b>compared</b> to SGD and it was shown that under the right circumstances this <b>algorithm</b> <b>can</b> severely outperform its more popular counterparts.", "dateLastCrawled": "2022-02-03T11:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b>: Linear <b>Regression</b> and its applications - The Data ...", "url": "https://thedatascienceportal.com/posts/linear-regression-and-its-applications/", "isFamilyFriendly": true, "displayUrl": "https://thedatascienceportal.com/posts/linear-<b>regression</b>-and-its-applications", "snippet": "This way, the <b>machine</b> <b>learning</b> <b>algorithm</b> will see what its output should look like \u2013 hence the name, \u201csupervised\u201d. Traditionally Supervised <b>Machine</b> <b>Learning</b> problem <b>can</b> also be \u2013 Classification \u2013 The output is made up of discrete class intervals. Like in the example above, the labels are {\u201cYes\u201d, \u201cNo\u201d} <b>Regression</b> \u2013 The output is a continuous value. It could be a monetary value in some currency, or maybe the temperature at some point in the week. Say we are trying to find ...", "dateLastCrawled": "2022-01-27T10:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Retargeted Least Squares Regression Algorithm</b>", "url": "https://www.researchgate.net/publication/269181331_Retargeted_Least_Squares_Regression_Algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../269181331_<b>Retargeted_Least_Squares_Regression_Algorithm</b>", "snippet": "<b>Least</b> <b>squares</b> <b>regression</b> (LSR) is an important <b>machine</b> <b>learning</b> method for feature extraction, feature selection, and image classification. For the training samples, there are correlations among ...", "dateLastCrawled": "2021-11-03T12:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Least Angle Regression (LARS) - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/least-angle-regression-lars/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>least</b>-angle-<b>regression</b>-lars", "snippet": "<b>Regression</b> is a supervised <b>machine</b> <b>learning</b> task that <b>can</b> predict continuous values (real numbers), as <b>compared</b> to classification, that <b>can</b> predict categorical or discrete values. Before we begin, if you are a beginner, I highly recommend this article. <b>Least</b> Angle <b>Regression</b> (LARS) is an <b>algorithm</b> used in <b>regression</b> for high dimensional data (i.e., data with a large number of attributes). <b>Least</b> Angle <b>Regression</b> is somewhat similar to forward stepwise <b>regression</b>. Since it is used with data ...", "dateLastCrawled": "2022-01-26T14:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b>-<b>Regression</b> Algorithms(Linear <b>regression</b>) | by Mohamed ...", "url": "https://medium.com/nerd-for-tech/machine-learning-regression-algorithms-linear-regression-ea97c92f2d9b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/<b>machine</b>-<b>learning</b>-<b>regression</b>-<b>algorithms</b>-linear...", "snippet": "<b>Machine</b> <b>Learning</b> is about Creating an <b>algorithm</b> for which the computer finds a model to fit the data as best as possible and accurately predict. The <b>machine</b> <b>learning</b> <b>algorithm</b> learns the function\u2026", "dateLastCrawled": "2022-01-26T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "8 Popular <b>Regression Algorithms In Machine Learning</b> Of 2021", "url": "https://www.jigsawacademy.com/popular-regression-algorithms-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.jigsawacademy.com/popular-<b>regression</b>-<b>algorithms</b>-ml", "snippet": "Ridge <b>Regression</b> is another popularly used linear <b>regression</b> <b>algorithm</b> in <b>Machine</b> <b>Learning</b>. If only one independent variable is being used to predict the output, it will be termed as a linear <b>regression</b> ML <b>algorithm</b>. ML experts prefer Ridge <b>regression</b> as it minimizes the loss encountered in linear <b>regression</b> (discussed above). In place of OLS (Ordinary <b>Least</b> <b>Squares</b>), the output values are predicted by a ridge estimator in ridge <b>regression</b>. The above-discussed linear <b>regression</b> uses OLS to ...", "dateLastCrawled": "2022-02-03T06:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Top 10 Machine Learning Algorithms</b> | Analytics Steps", "url": "https://www.analyticssteps.com/blogs/top-10-machine-learning-algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.analyticssteps.com/blogs/<b>top-10-machine-learning-algorithms</b>", "snippet": "In order to establish the connection between a dependent variable and an independent variable, the ordinary <b>least</b> <b>squares</b> method is like- draw a straight line, later for each data point, calculate the vertical distance amidst the point and the line and summed these up. Ordinary <b>Least</b> Square <b>Regression</b>, Image Source. The fitted line would be the one where the sum of distances is as small as possible. <b>Least</b> <b>squares</b> are referring to the sort of errors metric that are minimized. Linear ...", "dateLastCrawled": "2022-01-28T14:57:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS 189/289A: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189s21/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189s21", "snippet": "LDA vs. logistic <b>regression</b>: advantages and disadvantages. ROC curves. Weighted <b>least</b>-<b>squares</b> <b>regression</b>. <b>Least</b>-<b>squares</b> polynomial <b>regression</b>. Read ISL, Sections 4.4.3, 7.1, 9.3.3; ESL, Section 4.4.1. Optional: here is a fine short discussion of ROC curves\u2014but skip the incoherent question at the top and jump straight to the answer.", "dateLastCrawled": "2022-01-31T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>CS 189/289A</b>: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189", "snippet": "<b>regression</b>: <b>least</b>-<b>squares</b> linear <b>regression</b>, logistic <b>regression</b>, polynomial <b>regression</b>, ridge <b>regression</b>, Lasso; density estimation: maximum likelihood estimation (MLE); dimensionality reduction: principal components analysis (PCA), random projection; and clustering: k-means clustering, hierarchical clustering, spectral graph clustering. Useful Links. Access the <b>CS 189/289A</b> Piazza discussion group. If you want an instructional account, you can get one online. Go to the same link if you ...", "dateLastCrawled": "2022-02-02T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "A difficult <b>regression</b> parameter estimation problem is posed when the data sample is hypothesized to have been generated by more than a single <b>regression</b> model. To find the best-fitting number and ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "LSEbA: <b>least squares regression and estimation by analogy</b> in a semi ...", "url": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "snippet": "In this study, we indicatively applied the ordinary <b>least</b> <b>squares</b> <b>regression</b> and the estimation by <b>analogy</b> technique for the computation of the parametric and non-parametric part, respectively. However, there are lots of other well-known methods that can substitute the abovementioned methods and can be used for evaluation of these components. For example, practitioners may use a robust <b>regression</b> in the computation of the parametric portion of the proposed model in order to have a model less ...", "dateLastCrawled": "2021-12-03T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Big Problem with Linear <b>Regression</b> and How to Solve It | Towards Data ...", "url": "https://towardsdatascience.com/robust-regression-23b633e5d6a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/robust-<b>regression</b>-23b633e5d6a5", "snippet": "Introduction to Robust <b>Regression</b> in <b>Machine</b> <b>Learning</b>. Hussein Abdulrahman . Just now \u00b7 7 min read. The idea behind classic linear <b>regression</b> is simple: draw a \u201cbest-fit\u201d line across the data points that minimizes the mean squared errors: Classic linear <b>regression</b> with ordinary <b>least</b> <b>squares</b>. (Image by author) Looks good. But we don\u2019t always get such clean, well behaved data in real life. Instead, we may get something like this: Same algorithm as above, but now performing poorly due ...", "dateLastCrawled": "2022-02-01T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Machine</b> <b>learning</b> terminology for model building and validation There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear <b>regression</b> with two independent variables is trying to fit the best plane with\u2026", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Trends <b>in artificial intelligence, machine learning, and chemometrics</b> ...", "url": "https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ansa.202000162", "isFamilyFriendly": true, "displayUrl": "https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ansa.202000162", "snippet": "The derived spectra were analyzed for classification and quantification purposes using soft independent modeling of class <b>analogy</b> (SIMCA), artificial neural network (ANN), and partial <b>least</b> <b>squares</b> <b>regression</b> (PLSR). A good classification of tomatoes based on their carotenoid profile of 93% and 100% is shown using SIMCA and ANN, respectively. Besides this result, PLSR and ANN were able to achieve a good quantification of all-", "dateLastCrawled": "2022-02-01T19:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "econometrics - Principle of <b>Analogy</b> and Method of Moments - Cross Validated", "url": "https://stats.stackexchange.com/questions/272803/principle-of-analogy-and-method-of-moments", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/272803/principle-of-<b>analogy</b>-and-method-of...", "snippet": "<b>Least</b> <b>squares</b> estimator in the classical linear <b>regression</b> model is a Method of Moments estimator. The model is. y = X \u03b2 + u. Instead of minimizing the sum of squared residuals, we can obtain the OLS estimator by noting that under the assumptions of the specific model, it holds that (&quot;orhtogonality condition&quot;) E ( X \u2032 u) = 0.", "dateLastCrawled": "2022-01-25T20:40:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bayesian <b>Learning</b> - Rebellion Research", "url": "https://www.rebellionresearch.com/bayesian-learning", "isFamilyFriendly": true, "displayUrl": "https://www.rebellionresearch.com/bayesian-<b>learning</b>", "snippet": "Linear Regression example of <b>machine learning Least Squares Regression can be thought of as</b> a very limited <b>learning</b> algorithm, where the training set consists of a number of x and y data pairs. The task would be trying to predict the y value, and the performance measure would be the sum of the squared differences between the predicted and actual y\u2019s.", "dateLastCrawled": "2022-01-19T02:15:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(least squares regression)  is like +(machine learning algorithm)", "+(least squares regression) is similar to +(machine learning algorithm)", "+(least squares regression) can be thought of as +(machine learning algorithm)", "+(least squares regression) can be compared to +(machine learning algorithm)", "machine learning +(least squares regression AND analogy)", "machine learning +(\"least squares regression is like\")", "machine learning +(\"least squares regression is similar\")", "machine learning +(\"just as least squares regression\")", "machine learning +(\"least squares regression can be thought of as\")", "machine learning +(\"least squares regression can be compared to\")"]}