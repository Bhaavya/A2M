{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Getting Started with NLP Transformers | Engineering Education (EngEd ...", "url": "https://www.section.io/engineering-education/getting-started-with-nlp-transformers/", "isFamilyFriendly": true, "displayUrl": "https://www.section.io/engineering-education/getting-started-with-nlp-transformers", "snippet": "Since the NLP Transformer is built to solve <b>sequence-to-sequence</b> tasks, we must first understand other related models. <b>Sequence-to-sequence</b> models. These models are used in NLP to convert sequences of one type to another. An example can be, language translation where a language <b>like</b> Chinese is translated to English.", "dateLastCrawled": "2022-02-03T15:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>STEP: Sequence-to-Sequence Transformer Pre-training for</b> Document ...", "url": "https://deepai.org/publication/step-sequence-to-sequence-transformer-pre-training-for-document-summarization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>step-sequence-to-sequence-transformer-pre-training-for</b>...", "snippet": "Based on the above observations, we propose Step (as shorthand for <b>S equence-to-Sequence</b> T ransform E r P re-training), which can be pre-trained on large scale unlabeled documents. Specifically, we design three tasks for seq2seq model pre-training, namely Sentence Reordering (SR), Next Sentence Generation (NSG), and Masked Document Generation (MDG). SR learns to recover a document with randomly shuffled sentences.", "dateLastCrawled": "2022-01-17T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequence-to-sequence</b> learning with LSTM neural networks. | Download ...", "url": "https://www.researchgate.net/figure/Sequence-to-sequence-learning-with-LSTM-neural-networks_fig1_313204805", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/<b>Sequence-to-sequence</b>-learning-with-LSTM-neural...", "snippet": "Download scientific diagram | <b>Sequence-to-sequence</b> learning with LSTM neural networks. from publication: A New Chatbot for Customer Service on Social Media | Users are rapidly turning to social ...", "dateLastCrawled": "2021-12-01T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Multimodal Co-learning: Challenges, applications with datasets, recent ...", "url": "https://www.sciencedirect.com/science/article/pii/S1566253521002530", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1566253521002530", "snippet": "<b>Like</b> co-<b>teaching</b>, teacher-<b>student</b> deep semi-supervised learning ... and CMU-MOSEI datasets show better performance than the individual modalities as well as <b>sequence-to-sequence</b> multimodal models. Audio modality is a weaker modality, and both the combinations, i.e., audio at test time and video at test time, are evaluated. The approach to factorizing multimodal data into discriminative and generative factors reconstructs missing modalities. The discriminative factors are common across all ...", "dateLastCrawled": "2022-01-27T22:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Improving <b>Sequence-to-Sequence</b> Pre-training via Sequence Span Rewriting ...", "url": "https://www.readkong.com/page/improving-sequence-to-sequence-pre-training-via-sequence-3829690", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/improving-<b>sequence-to-sequence</b>-pre-training-via-sequence...", "snippet": "Specifically, self-supervised training with the sequence span \u2022 First, the <b>task</b> of sequence span rewriting rewriting objective involves three steps: (1) text is closer to the downstream sequence trans- span masking (2) text infilling, and (3) sequence duction tasks since there exists references in span rewriting.", "dateLastCrawled": "2022-01-08T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Teaching</b>-learning cycle: reading and writing connections", "url": "https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/teachingpraccycle.aspx", "isFamilyFriendly": true, "displayUrl": "https://www.education.vic.gov.au/school/teachers/<b>teaching</b>resources/discipline/english/...", "snippet": "<b>Teaching</b> practices for literacy reading and viewing ... The TLC involves the gradual release of responsibility from teacher to <b>student</b> through a structured sequence of interrelated stages and scaffolded activities. Explicit <b>teaching</b>, used widely in contemporary schooling is used to \u201cstress the value of \u2018explicit\u2019 knowledge of grammar and all textual codes\u201d (Luke, 2014, p.1). The TLC emerged from genre-based approaches to literacy in the late 1980s, where the initial emphasis was on ...", "dateLastCrawled": "2022-01-30T19:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Student Blogs</b> - AppliedAICourse", "url": "https://www.appliedaicourse.com/student-blogs/", "isFamilyFriendly": true, "displayUrl": "https://www.appliedaicourse.com/<b>student-blogs</b>", "snippet": "<b>Sequence-to-sequence</b> learning (Seq2Seq) is all about models that take a sequence as an input and outputs a sequence too. There are many examples and applications of this but today I will focus on one specific application which is a machine language translation. For eg English to Hindi. Th ...", "dateLastCrawled": "2022-02-03T08:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Sequencing events in reading and writing</b>: A Complete Guide for Students ...", "url": "https://literacyideas.com/teaching-sequencing-in-english/", "isFamilyFriendly": true, "displayUrl": "https://literacyideas.com/<b>teaching</b>-sequencing-in-english", "snippet": "Telling It <b>Like</b> It Was. The preparation for this activity works well as a homework as it gives students time to rehearse. However, it also works well after any reading activity to assess <b>a student</b>\u2019s understanding of the sequence of events and their overall comprehension of what they have read.", "dateLastCrawled": "2022-02-03T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is Teacher Forcing for Recurrent Neural Networks?", "url": "https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks", "snippet": "Teacher forcing is a method for quickly and efficiently training recurrent neural network models that use the ground truth from a prior time step as input. It is a network training method critical to the development of deep learning language models used in machine translation, text summarization, and image captioning, among many other applications. In this post, you will discover the teacher", "dateLastCrawled": "2022-02-03T19:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "10 Printable <b>Sequencing Worksheets</b> [Free]- EduWorksheets", "url": "https://eduworksheets.com/sequencing/", "isFamilyFriendly": true, "displayUrl": "https://eduworksheets.com/sequencing", "snippet": "There are several ways to apply the concept of sequencing to literature and reading in <b>teaching</b> strategies <b>like</b> thinking out loud, writing in journals, and more. Here are some examples where you can apply the concept of sequencing and use <b>sequencing worksheets</b> for practice: Language Arts. Apart from using sequencing templates, one great way to help your students organize a sequence of events from a story is with the use of story maps. With these, students learn how signal or transition words ...", "dateLastCrawled": "2022-02-02T21:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Top 3 Benefits and Uses of NLP in Education | Scalr.ai", "url": "https://www.width.ai/post/nlp-in-education", "isFamilyFriendly": true, "displayUrl": "https://www.width.ai/post/nlp-in-education", "snippet": "A question-answering model is created on <b>similar</b> lines as a summarization model by adding a <b>sequence-to-sequence</b> layer to a pre-trained transformer model. For a question, which is the input sequence, it should output another sequence which is the answer. The assemblage of pre-trained base model and <b>sequence-to-sequence</b> layer is fine-tuned using subject-specific question-answer datasets.", "dateLastCrawled": "2022-01-29T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>STEP: Sequence-to-Sequence Transformer Pre-training for</b> Document ...", "url": "https://deepai.org/publication/step-sequence-to-sequence-transformer-pre-training-for-document-summarization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>step-sequence-to-sequence-transformer-pre-training-for</b>...", "snippet": "This <b>task</b> is usually framed as a <b>sequence-to-sequence</b> learning problem Nallapati et al. ; See et al. . In this paper, we adopt the <b>sequence-to-sequence</b> (seq2seq) Transformer Vaswani et al. , which has been demonstrated to be the state-of-the-art for seq2seq modeling Vaswani et al. ; Ott et al. .", "dateLastCrawled": "2022-01-17T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequence-to-sequence</b> learning with LSTM neural networks. | Download ...", "url": "https://www.researchgate.net/figure/Sequence-to-sequence-learning-with-LSTM-neural-networks_fig1_313204805", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/<b>Sequence-to-sequence</b>-learning-with-LSTM-neural...", "snippet": "Download scientific diagram | <b>Sequence-to-sequence</b> learning with LSTM neural networks. from publication: A New Chatbot for Customer Service on Social Media | Users are rapidly turning to social ...", "dateLastCrawled": "2021-12-01T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "From an Artificial Neural Network <b>to Teaching</b>", "url": "http://www.ijello.org/Volume16/IJELLv16p001-017Mughaz6498.pdf", "isFamilyFriendly": true, "displayUrl": "www.ijello.org/Volume16/IJELLv16p001-017Mughaz6498.pdf", "snippet": "with <b>sequence-to-sequence</b> architecture. After this, we combined the <b>sequence-to- sequence</b> architecture model with the attention mechanism that gives a general overview of the input that the network receives. From an Artificial Neural Network <b>to Teaching</b>. 2. Contribution The cost of computer applications is cheaper than that of manual human effort, and the availability of a computer program is much greater than that of humans to perform the same <b>task</b>. Thus, using computer applications, we can ...", "dateLastCrawled": "2021-11-03T17:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Teaching</b>-learning cycle: reading and writing connections", "url": "https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/teachingpraccycle.aspx", "isFamilyFriendly": true, "displayUrl": "https://www.education.vic.gov.au/school/teachers/<b>teaching</b>resources/discipline/english/...", "snippet": "The <b>teaching</b> and learning cycle (TLC) involves four key stages which incorporate social support for reading, writing and speaking and listening through varied interactional routines (whole group, small group, pair, individual) to scaffold students\u2019 learning about language and meaning in a variety of texts. These stages are: Building the context or field - understanding the role of texts in our culture and building shared understanding of the topic Modelling the text (or deconstruction ...", "dateLastCrawled": "2022-01-30T19:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning to Teach with Dynamic Loss Functions</b>", "url": "https://proceedings.neurips.cc/paper/2018/file/8051a3c40561002834e59d566b7430cf-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2018/file/8051a3c40561002834e59d566b7430cf-Paper.pdf", "snippet": "dataset. Towards that end, <b>similar</b> to human <b>teaching</b>, the teacher, a parametric model, dynamically outputs different loss functions that will be used and optimized by its <b>student</b> model at different training stages. We develop an ef\ufb01cient learning method for the teacher model that makes gradient based optimization possible, exempt of the ineffective solutions such as policy optimization. We name our method as \u201c<b>learning to teach with dynamic loss functions</b>\u201d (L2T-DLF for short). Extensive ...", "dateLastCrawled": "2022-01-26T01:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Student Blogs</b> - AppliedAICourse", "url": "https://www.appliedaicourse.com/student-blogs/", "isFamilyFriendly": true, "displayUrl": "https://www.appliedaicourse.com/<b>student-blogs</b>", "snippet": "<b>Sequence-to-sequence</b> learning (Seq2Seq) is all about models that take a sequence as an input and outputs a sequence too. There are many examples and applications of this but today I will focus on one specific application which is a machine language translation. For eg English to Hindi. Th ...", "dateLastCrawled": "2022-02-03T08:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Learning from the Master: Distilling Cross-modal Advanced Knowledge for ...", "url": "http://www.shengfenghe.com/qfy-content/uploads/2021/06/1df767522a7511725a705f90d393ad98.pdf", "isFamilyFriendly": true, "displayUrl": "www.shengfenghe.com/qfy-content/uploads/2021/06/1df767522a7511725a705f90d393ad98.pdf", "snippet": "the <b>task</b>-speci\ufb01c feedback from the <b>student</b>, in which the requirement of the <b>student</b> is implicitly embedded. Mean-while, we involve a couple of \u201ctutor\u201d networks into our system as guidance for emphasizing the fruitful knowledge \ufb02exibly. In addition, we incorporate a curriculum learning design to ensure a better convergence. Extensive experi-ments demonstrate that the proposed network outperforms the state-of-the-art methods on several benchmarks, includ-ing in both word-level and ...", "dateLastCrawled": "2022-01-28T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Learning From the Master: Distilling Cross-Modal Advanced Knowledge for ...", "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Learning_From_the_Master_Distilling_Cross-Modal_Advanced_Knowledge_for_Lip_CVPR_2021_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Learning_From_the_Master...", "snippet": "the <b>task</b>-speci\ufb01c feedback from the <b>student</b>, in which the requirement of the <b>student</b> is implicitly embedded. Mean-while, we involve a couple of \u201ctutor\u201d networks into our system as guidance for emphasizing the fruitful knowledge \ufb02exibly. In addition, we incorporate a curriculum learning design to ensure a better convergence. Extensive experi-ments demonstrate that the proposed network outperforms thestate-of-the-artmethodsonseveralbenchmarks, includ-ing in both word-level and sentence ...", "dateLastCrawled": "2022-02-02T17:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Teacher-Student Curriculum Learning</b> | DeepAI", "url": "https://deepai.org/publication/teacher-student-curriculum-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>teacher-student-curriculum-learning</b>", "snippet": "When the <b>Student</b> masters <b>task</b> 1, its learning curve flattens and the Teacher samples the <b>task</b> less often. At this point <b>Student</b> also starts making progress on <b>task</b> 2, so the Teacher samples more from <b>task</b> 2. This continues until the <b>Student</b> masters all tasks. As all <b>task</b> learning curves flatten in the end, the Teacher returns to uniform sampling of the tasks. The picture above is idealistic, since in practice some unlearning often occurs, i.e. when most of the probability mass is allocated ...", "dateLastCrawled": "2021-12-17T08:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Zero-Shot Cross-Lingual Abstractive Sentence Summarization through ...", "url": "https://aclanthology.org/P19-1305.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/P19-1305.pdf", "snippet": "<b>Teaching</b> Generation and Attention Xiangyu Duan1,2, Mingming Yin1, Min Zhang1,2, Boxing Chen3, ... model these pairs as as a <b>sequence-to-sequence</b> <b>task</b> by encoding the source sentence into vector-ized information and decoding it into the abstrac-tive summary. Regarding the techniques of the bilingual trans-lation, recent years witnessed the method tran-sition from statistical machine translation (SMT) (Koehn et al.,2003) to neural machine translation (NMT). NMT employs the <b>sequence-to-sequence</b> ...", "dateLastCrawled": "2022-01-19T20:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>STEP: Sequence-to-Sequence Transformer Pre-training for</b> Document ...", "url": "https://deepai.org/publication/step-sequence-to-sequence-transformer-pre-training-for-document-summarization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>step-sequence-to-sequence-transformer-pre-training-for</b>...", "snippet": "We, therefore, propose STEP (as shorthand for <b>Sequence-to-Sequence</b> Transformer Pre-training), which <b>can</b> be trained on large scale unlabeled documents. Specifically, STEP is pre-trained using three different tasks, namely sentence reordering, next sentence generation, and masked document generation. Experiments on two summarization datasets show that all three tasks <b>can</b> improve performance upon a heavily tuned large Seq2Seq Transformer which already includes a strong pre-trained encoder by a ...", "dateLastCrawled": "2022-01-17T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequencing events in reading and writing</b>: A Complete Guide for Students ...", "url": "https://literacyideas.com/teaching-sequencing-in-english/", "isFamilyFriendly": true, "displayUrl": "https://literacyideas.com/<b>teaching</b>-sequencing-in-english", "snippet": "The more context clues they <b>can</b> recognize too, the more efficiently they will perform this <b>task</b>. There is no shortcut to the development of any of the key reading comprehension skills \u2013 and sequencing is no exception. First, students must understand what sequencing is. Then, they must understand how to identify it in a variety of text genres ...", "dateLastCrawled": "2022-02-03T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Sequence to Sequence</b> Model Performance for Education Chatbot", "url": "https://www.researchgate.net/publication/338056666_Sequence_to_Sequence_Model_Performance_for_Education_Chatbot", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338056666_<b>Sequence_to_Sequence</b>_Model...", "snippet": "Paper \u2014 <b>S equence to Sequence</b> Model Performance for Education Cha t bot. For the models with word embedding, the input and output vocabulary size are 177. and 245 tokens respectively. As for the ...", "dateLastCrawled": "2022-01-28T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Do you think <b>that you are &quot;teaching&quot; according to the ways</b> or &quot;methods ...", "url": "https://www.researchgate.net/post/Do-you-think-that-you-are-teaching-according-to-the-ways-or-methods-others-taught-you", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/Do-you-think-<b>that-you-are-teaching-according-to</b>-the...", "snippet": "They are well trained for the <b>task</b>. When I started <b>teaching</b>, I did not try to imitate anyone. Rather, I used to place myself as <b>a student</b> and tried to think from their perspective. Now, I am ...", "dateLastCrawled": "2022-02-02T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is Teacher Forcing for Recurrent Neural Networks?", "url": "https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks", "snippet": "Teacher forcing is a method for quickly and efficiently training recurrent neural network models that use the ground truth from a prior time step as input. It is a network training method critical to the development of deep learning language models used in machine translation, text summarization, and image captioning, among many other applications. In this post, you will discover the teacher", "dateLastCrawled": "2022-02-03T19:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "10 Printable <b>Sequencing Worksheets</b> [Free]- EduWorksheets", "url": "https://eduworksheets.com/sequencing/", "isFamilyFriendly": true, "displayUrl": "https://eduworksheets.com/sequencing", "snippet": "There are several ways to apply the concept of sequencing to literature and reading in <b>teaching</b> strategies like thinking out loud, writing in journals, and more. Here are some examples where you <b>can</b> apply the concept of sequencing and use <b>sequencing worksheets</b> for practice: Language Arts. Apart from using sequencing templates, one great way to help your students organize a sequence of events from a story is with the use of story maps. With these, students learn how signal or transition words ...", "dateLastCrawled": "2022-02-02T21:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>What is transductive learning</b>? - Quora", "url": "https://www.quora.com/What-is-transductive-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-transductive-learning</b>", "snippet": "Answer (1 of 2): The idea behind transductive learning is not to construct a function to learn from a training set - labels for any point from a test set (as in inductive learning). In transductive learning you know the test set beforehand.Then you just pass the information in the training set on...", "dateLastCrawled": "2022-01-19T22:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Jason</b> Eisner\u2019s publications", "url": "https://www.cs.jhu.edu/~jason/papers/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>cs.jhu.edu/~jason/papers</b>", "snippet": "Following the success of neural <b>sequence-to-sequence</b> models in the SIGMORPHON 2016 shared <b>task</b>, all but one of the submissions included a neural component. The results show that high performance <b>can</b> be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data. However, different biasing and data augmentation resulted in disjoint sets of inflected forms being predicted correctly, suggesting that there is ...", "dateLastCrawled": "2022-01-31T06:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "DeepEdu - Deep Learning and Education", "url": "https://dyadxmachina.github.io/can-machines-teach/concepts.html", "isFamilyFriendly": true, "displayUrl": "https://dyadxmachina.github.io/<b>can</b>-machines-teach/concepts.html", "snippet": "Lastly, in this paper QuAC : Question Answering in Context (Choi 2018), they present a QuAC dataset for QA <b>task</b> in Context that contains 14K information-seeking QA dialogs such as <b>a student</b> who poses a sequence of freeform question to learn as much as possible about a hidden Wikipedia text or a teacher who answers the questions by providing short excerpts from the text. Inspired by their idea and effort, we imagine that it is might be possible to develop a system that <b>can</b> also allow the ...", "dateLastCrawled": "2022-01-31T15:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>STEP: Sequence-to-Sequence Transformer Pre-training for</b> Document ...", "url": "https://deepai.org/publication/step-sequence-to-sequence-transformer-pre-training-for-document-summarization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>step-sequence-to-sequence-transformer-pre-training-for</b>...", "snippet": "We evaluate our methods on two summarization datasets (i.e., the CNN/DailyMail and the New York Times datasets). Experiments show that all three tasks we propose <b>can</b> improve upon a heavily tuned large seq2seq Transformer which already includes a strong pre-trained encoder by a large margin. <b>Compared</b> to the best published abstractive models, Step improves the ROUGE-2 by 0.8 on the CNN/DailyMail dataset and by 2.4 on the New York Times dataset using our best performing <b>task</b> for pre-training ...", "dateLastCrawled": "2022-01-17T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Sequence to Sequence</b> Model Performance for Education Chatbot", "url": "https://www.researchgate.net/publication/338056666_Sequence_to_Sequence_Model_Performance_for_Education_Chatbot", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338056666_<b>Sequence_to_Sequence</b>_Model...", "snippet": "Paper \u2014 <b>S equence to Sequence</b> Model Performance for Education Cha t bot. For the models with word embedding, the input and output vocabulary size are 177. and 245 tokens respectively. As for the ...", "dateLastCrawled": "2022-01-28T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Teaching</b>-learning cycle: reading and writing connections", "url": "https://www.education.vic.gov.au/school/teachers/teachingresources/discipline/english/literacy/readingviewing/Pages/teachingpraccycle.aspx", "isFamilyFriendly": true, "displayUrl": "https://www.education.vic.gov.au/school/teachers/<b>teaching</b>resources/discipline/english/...", "snippet": "This stage <b>can</b> include explicit <b>teaching</b> about the stages of a text or close examination of particular language features of the text, for example the use of prepositional phrases of place to establish setting in a narrative, or the use of relating verbs in defining technical terms in an explanation. Here, the teacher selects an extract or extracts and uses a linguistic lens for a close reading* to teach about particular structural features as well as language features of the text and the ...", "dateLastCrawled": "2022-01-30T19:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Self-Teaching Networks</b> | DeepAI", "url": "https://deepai.org/publication/self-teaching-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>self-teaching-networks</b>", "snippet": "Comparing Eq (7) to Eq (1), the formulation of the self-<b>teaching</b> network is the same as that of the teacher-<b>student</b> learning, ... 8% for the DMA <b>task</b>. For <b>self-teaching networks</b>, <b>compared</b> to the results of the smaller model, we observed relatively larger improvements after increasing the model size with the relative WER reduction up to 3% \u2013 4%, which indicates that the regularization term plays a larger role in the model training. Notably, the improvement for the Cortana <b>task</b> is comparable ...", "dateLastCrawled": "2022-01-03T03:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Comprehension Task</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/engineering/comprehension-task", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/engineering/<b>comprehension-task</b>", "snippet": "The Unified Language Model, UniLM [163], is designed with a shared multi-layer Transformer network and parameters to be jointly pre-trained on a big dataset using three different types of unsupervised language modelling objectives: unidirectional, bidirectional, and <b>sequence-to-sequence</b>. Its <b>task</b> is to predict a masked word based on its context, i.e., cloze <b>task</b>. This multi-objective pre-training allows UniLM to be fine-tuned not only for NLU tasks, like BERT, but also for TG tasks such as ...", "dateLastCrawled": "2022-01-10T13:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Learning from Different Expert Agents", "url": "https://joel99.github.io/lfd_7648_final.pdf", "isFamilyFriendly": true, "displayUrl": "https://joel99.github.io/lfd_7648_final.pdf", "snippet": "To resolve this, we propose to learn a <b>sequence to sequence</b> alignment module which transforms teacher states s T 1:::s t T to <b>student</b> states s S 1:::s t S. This is a domain translation <b>task</b>, and <b>can</b> be inspired by works in e.g. image translation. A critical design choice to make is whether this alignment is paired or not. Provided arbitrary ...", "dateLastCrawled": "2022-01-18T06:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Developing a <b>Task</b>-Based Dialogue System for English Language Learning", "url": "https://www.researchgate.net/publication/346454816_Developing_a_Task-Based_Dialogue_System_for_English_Language_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/346454816_Developing_a_<b>Task</b>-Based_Dialogue...", "snippet": "This study adopted. <b>task</b>-based language learning as the fundamental curriculum design and used a <b>task</b>-based dialogue. system to provide the interface. Dialogue practice thus <b>can</b> be carried out ...", "dateLastCrawled": "2022-01-24T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "LAK21: LAK21: 11th International Learning Analytics and Knowledge ...", "url": "https://www.solaresearch.org/lak_toc/lak21/", "isFamilyFriendly": true, "displayUrl": "https://www.solaresearch.org/lak_toc/lak21", "snippet": "Indoor-positioning technologies <b>can</b> now capture <b>student</b> spatial behaviours, but relatively little work has focused on giving meaning to <b>student</b> activity traces, transforming low-level x/y coordinates into language that makes sense to teachers. Even less research has investigated if teachers <b>can</b> make sense of that feedback. This paper therefore makes two contributions. (1) Methodologically, we document the use of Epistemic Network Analysis (ENA) as an approach to model and visualise students ...", "dateLastCrawled": "2022-01-31T05:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Mutual-<b>learning sequence-level knowledge distillation for automatic</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231220318129", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231220318129", "snippet": "We <b>can</b> observe the following patterns from Table 2.First, the size of <b>student</b> models and knowledge distillation models are much smaller than the teacher model; for example, for <b>Student</b> A, the number of parameters is only about 20% of Teacher A. Secondly, with the knowledge distillation training, the <b>student</b> models (e.g. <b>Student</b> A + seqKD) <b>can</b> achieve better CER than it training directly without knowledge distillation (e.g. <b>Student</b> A).", "dateLastCrawled": "2021-11-14T18:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What would the ideal university be like? - Quora</b>", "url": "https://www.quora.com/What-would-the-ideal-university-be-like", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-would-the-ideal-university-be</b>-like", "snippet": "Answer (1 of 24): I don&#39;t feel I am the correct person to answer this question but I would like to share my views. My Engineering college was under development when I took admission in it. Being it as Institute of National importance I didn&#39;t though much about these things before stepping inside ...", "dateLastCrawled": "2022-01-19T20:24:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep <b>learning</b> in retrosynthesis planning: datasets, models and tools ...", "url": "https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbab391/6375056", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbab391/6375056", "snippet": "<b>Sequence-to-sequence</b> is a class of end-to-end algorithmic framework, which completes the conversion from <b>sequence to sequence</b> by the structure of encoder-decoder, and is used in some scenarios such as automatic speech recognition and <b>machine</b> translation. As for encoder-decoder, the encoder is responsible for encoding information of input sequence to vector and the vector is reverted to sequence by a decoder. The first template-free retrosynthesis method seq2seq was proposed by Liu", "dateLastCrawled": "2022-01-10T01:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Neural Chinese Word Segmentation as Sequence to Sequence</b> Translation ...", "url": "https://deepai.org/publication/neural-chinese-word-segmentation-as-sequence-to-sequence-translation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>neural-chinese-word-segmentation-as-sequence</b>-to...", "snippet": "We first treat CWS as a <b>sequence-to-sequence</b> translation <b>task</b> and introduce the attention-based encoder-decoder framework into CWS. The encoder-decoder captures the whole bidirectional input information without context window limitations and directly outputs the segmented sequence by simultaneously considering the dependencies of previous outputs and the input information. We let our <b>sequence-to-sequence</b> CWS model simultaneously tackle other NLP tasks, e.g., CSC, in an end-to-end mode, and ...", "dateLastCrawled": "2021-12-22T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(teaching a student)", "+(sequence-to-sequence task) is similar to +(teaching a student)", "+(sequence-to-sequence task) can be thought of as +(teaching a student)", "+(sequence-to-sequence task) can be compared to +(teaching a student)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}