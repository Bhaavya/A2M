{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Learning Preferences with Millions of Parameters by Enforcing <b>Sparsity</b>", "url": "https://www.cs.cmu.edu/~jgc/publication/PublicationPDF/Learning_Preferences_With_Millions_Of_Parameters_By_Enforcing_Sparsity.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~jgc/publication/PublicationPDF/Learning_Preferences_With...", "snippet": "tries of W), when <b>the number</b> <b>of training</b> <b>samples</b> is limited, it can easily lead to over\ufb01tting. Considering the dictionary with the size D = 10;000, we have D2 = 108 free parameters that need to be estimated which is far too many for small corpora. To address the above weakness, we propose to constrain W to be a sparse matrix with many zero entries for the pairs of words which are irrelevant for the preference learning task. If W is a highly sparse matrix, then it consumes 1For the sake of ...", "dateLastCrawled": "2020-12-11T13:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Beyond <b>sparsity</b>: The role of L1-<b>optimizer in pattern classification</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S003132031100361X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S003132031100361X", "snippet": "<b>Sparsity</b> determines a small <b>number</b> support <b>training</b> <b>samples</b> to represent a given test sample, while closeness makes the nonzero representation coefficients concentrate on the homo-class <b>training</b> <b>samples</b>. <b>Sparsity</b> benefits for local reconstruction, while closeness helps for global similarity. By combining <b>sparsity</b> and closeness together, the solution of", "dateLastCrawled": "2022-01-05T17:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is the relation between <b>the number</b> of <b>Support Vectors</b> and <b>training</b> ...", "url": "https://stackoverflow.com/questions/9480605/what-is-the-relation-between-the-number-of-support-vectors-and-training-data-and", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/9480605", "snippet": "Both <b>number</b> of <b>samples</b> and <b>number</b> of attributes may influence <b>the number</b> of <b>support vectors</b>, making model more complex. I believe you use words or even ngrams as attributes, so there are quite many of them, and natural language models are very complex themselves. So, 800 <b>support vectors</b> of 1000 <b>samples</b> seem to be ok. (Also pay attention to @karenu&#39;s comments about C/nu parameters that also have large effect on SVs <b>number</b>).", "dateLastCrawled": "2022-01-23T13:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Combining pruning with structured sparsity learning to compress</b> neural ...", "url": "http://reports.ias.ac.in/report/20282/combining-pruning-with-structured-sparsity-learning-to-compress-neural-networks", "isFamilyFriendly": true, "displayUrl": "reports.ias.ac.in/report/20282/<b>combining-pruning-with-structured-sparsity-learning</b>-to...", "snippet": "Because the <b>smaller</b> weights do not contribute <b>much</b> to inference, after a network is trained, the connections containing weights below a threshold can be pruned. The neurons that have either every incoming connection or every outgoing connection which can be trimmed, is removed. Further, the model is retrained using remaining connections. The procedure is depicted in Figure 1. When a few iterations (3-4 iterations) of the <b>training</b>-pruning are carried out, there are no new neurons that are ...", "dateLastCrawled": "2021-12-06T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>How Principal Component Analysis, PCA Works</b>", "url": "https://dataaspirant.com/principal-component-analysis-pca/", "isFamilyFriendly": true, "displayUrl": "https://dataaspirant.com/<b>principal-component-analysis-pca</b>", "snippet": "The increase in <b>the number</b> <b>of features</b> will not always improve classification accuracy. When enough <b>features</b> are not present in the data, ... then <b>the number</b> <b>of training</b> <b>samples</b> required to cover all the combinations increases phenomenally. In the above figure, it is shown that for two variables, we have eight <b>training</b> <b>samples</b>. So, for three variables, we need 24 <b>samples</b>, and so on. Distance Concentration. Distance concentration can be defined as: The problem of convergence of all pairwise ...", "dateLastCrawled": "2022-02-02T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Statistical Learning with Sparsity The Lasso and Generalizations</b> Pages ...", "url": "https://fliphtml5.com/mofx/mxtu/basic/", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/mofx/mxtu/basic", "snippet": "In thehigh-dimensional setting, in which <b>the number</b> <b>of features</b> p is larger <b>than</b> thesample size, it cannot be used without modi\ufb01cation. When p &gt; N , any linearmodel is over-parametrized, and regularization is needed to achieve a stable\ufb01t. Such high-dimensional models arise in various applications. For example,document classi\ufb01cation problems can involve binary <b>features</b> (presence versus 1For SVMs, it is convenient to code the binary response via the sign function. 2This is not the most ...", "dateLastCrawled": "2022-01-19T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How would <b>features</b> from sparse coding and RBM (with a <b>sparsity</b> ...", "url": "https://www.quora.com/How-would-features-from-sparse-coding-and-RBM-with-a-sparsity-constraint-be-comparable", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-would-<b>features</b>-from-sparse-coding-and-RBM-with-a-<b>sparsity</b>...", "snippet": "Answer: Tl;dr - IMO both learn the same representation, but <b>training</b> sparse RBMs seems to be simpler. In a Restricted Boltzmann Machine, the probability distribution of a unit j the binary hidden layer for a given value of the visible layer \\mathbf{v} is P(h_j | \\mathbf{v}) = \\sigma(b_j + \\sum_i...", "dateLastCrawled": "2022-01-14T01:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Guide to <b>Classification</b> on Imbalanced Datasets | by Matthew Stewart ...", "url": "https://towardsdatascience.com/guide-to-classification-on-imbalanced-datasets-d6653aa5fa23", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/guide-to-<b>classification</b>-on-imbalanced-<b>dataset</b>s-d6653aa5fa23", "snippet": "<b>Like</b>-<b>samples</b> in close proximity are removed in an attempt to increase the <b>sparsity</b> of the data distribution. What does oversampling look <b>like</b>? In shot, the opposite of undersampling. We are artificially adding data points to our <b>dataset</b> to make <b>the number</b> of instances in each class balanced. Oversampling. In the scenario of oversampling, we will oversample from the minority class to help reduce the extent of this imbalance. How do we generate these <b>samples</b>? The most common way is to generate ...", "dateLastCrawled": "2022-02-03T01:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the purpose of <b>sparsity</b> constraint in autoencoder? - Quora", "url": "https://www.quora.com/What-is-the-purpose-of-sparsity-constraint-in-autoencoder", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-purpose-of-<b>sparsity</b>-constraint-in-autoencoder", "snippet": "Answer (1 of 3): To frame the answer in a couple of sentences, I\u2019d say <b>sparsity</b> constraint prevents overcomplete autoencoders (where the dimension of the hidden layer is greater <b>than</b> that of the the input layer) from using the trivial identity mapping (simple copy of the input layer information t...", "dateLastCrawled": "2022-01-19T06:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "An Introduction to Autoencoders: Everything You Need to Know", "url": "https://www.v7labs.com/blog/autoencoders-guide", "isFamilyFriendly": true, "displayUrl": "https://www.v7labs.com/blog/autoencoders-guide", "snippet": "<b>Number</b> of layers: <b>Like</b> all neural networks, an important hyperparameter to tune autoencoders is the depth of the encoder and the decoder. While a higher depth increases model complexity, a lower depth is faster to process. <b>Number</b> of nodes per layer: <b>The number</b> of nodes per layer defines the weights we use per layer. Typically, <b>the number</b> of nodes decreases with each subsequent layer in the autoencoder as the input to each of these layers becomes <b>smaller</b> across the layers. Reconstruction Loss ...", "dateLastCrawled": "2022-02-02T02:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>SPARSITY</b> PROMOTING DIMENSIONALITY REDUCTION FOR CLASSIFICATION OF HIGH ...", "url": "https://www.researchgate.net/profile/Minshan-Cui/publication/261193676_Sparsity_promoting_dimensionality_reduction_for_classification_of_high_dimensional_hyperspectral_images/links/5563f50408ae86c06b695bd1/Sparsity-promoting-dimensionality-reduction-for-classification-of-high-dimensional-hyperspectral-images.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Minshan-Cui/publication/261193676_<b>Sparsity</b>...", "snippet": "the i-th class <b>training</b> sample matrix, c is <b>the number</b> of classes, n i represents <b>the number</b> <b>of training</b> <b>samples</b> from class i, and n is the total <b>number</b> <b>of training</b> <b>samples</b>, n = P c i=1 n i. In ...", "dateLastCrawled": "2022-01-09T09:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Umamahesh Srinivas, Yi Chen, Vishal Monga, Nasser M. Nasrabadi, and ...", "url": "http://thanglong.ece.jhu.edu/Tran/Pub/hsi_class_grsl.pdf", "isFamilyFriendly": true, "displayUrl": "<b>than</b>glong.ece.jhu.edu/Tran/Pub/hsi_class_grsl.pdf", "snippet": "dimensionality <b>is much</b> <b>smaller</b> <b>than</b> <b>the number</b> of spectral bands. An unknown pixel is then expressed as a sparse linear combination of a few <b>training</b> <b>samples</b> from a given dictionary and the underlying sparse representation vector implicitly encodes the class information. To exploit contextual (spatial) correlation, a joint <b>sparsity</b> model is employed in [12], where neighboring pixels are assumed to be represented by linear combinations of a few common <b>training</b> <b>samples</b> in order to enforce ...", "dateLastCrawled": "2021-08-12T05:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is the relation between <b>the number</b> of <b>Support Vectors</b> and <b>training</b> ...", "url": "https://stackoverflow.com/questions/9480605/what-is-the-relation-between-the-number-of-support-vectors-and-training-data-and", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/9480605", "snippet": "<b>The number</b> of SVs is in the worst case equal <b>to the number</b> <b>of training</b> <b>samples</b>, so 800/1000 is not yet the worst case, but it&#39;s still pretty bad. Then again, 1000 <b>training</b> documents is a small <b>training</b> set. You should check what happens when you scale up to 10000s or more documents. If things don&#39;t improve, consider using linear SVMs, trained with LibLinear, for document classification; those scale up <b>much</b> better (model size and classification time are linear in <b>the number</b> <b>of features</b> and ...", "dateLastCrawled": "2022-01-23T13:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Combining pruning with structured sparsity learning to compress</b> neural ...", "url": "http://reports.ias.ac.in/report/20282/combining-pruning-with-structured-sparsity-learning-to-compress-neural-networks", "isFamilyFriendly": true, "displayUrl": "reports.ias.ac.in/report/20282/<b>combining-pruning-with-structured-sparsity-learning</b>-to...", "snippet": "Because the <b>smaller</b> weights do not contribute <b>much</b> to inference, after a network is trained, the connections containing weights below a threshold can be pruned. The neurons that have either every incoming connection or every outgoing connection which can be trimmed, is removed. Further, the model is retrained using remaining connections. The procedure is depicted in Figure 1. When a few iterations (3-4 iterations) of the <b>training</b>-pruning are carried out, there are no new neurons that are ...", "dateLastCrawled": "2021-12-06T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sparsity</b>\u2010aware adaptive block\u2010based compressive sensing - Safavi - 2017 ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-spr.2016.0176", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-spr.2016.0176", "snippet": "Moreover, it could be seen from this figure that the visual quality of the blocks with higher <b>sparsity</b> order in our proposed approach is better <b>than</b> the conventional approach, however, as we assign a lower <b>number</b> of <b>samples</b> for blocks with lower <b>sparsity</b> order, the visual quality of these blocks become lower <b>than</b> conventional approach. In this case, may be it is needed to consider a lower bound for <b>the number</b> of measurements relative to the block size or even the quality of the original ...", "dateLastCrawled": "2022-01-20T01:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Parallel Cross-Sparse Filtering Networks and Its Application on Fault ...", "url": "https://www.hindawi.com/journals/js/2022/9259639/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/js/2022/9259639", "snippet": "The percentage <b>of training</b> <b>samples</b> was 10%. When only two <b>features</b> are used to express the fault condition, the accuracy of the proposed method can reach 97.28%, and the standard disassembly is only 1.62%, which <b>is much</b> better <b>than</b> parallel SF. When the feature dimension is equal to 8, the accuracy can reach 99.83%.", "dateLastCrawled": "2022-02-07T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "COMPARISON OF SUBSAMPLING TECHNIQUES FOR RANDOM SUBSPACE ENSEMBLES", "url": "https://www.eng.utoledo.edu/~gserpen/Publications/ICMLC%202010%20Manuscript.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.eng.utoledo.edu/~gserpen/Publications/ICMLC 2010 Manuscript.pdf", "snippet": "The required <b>number</b> of labeled <b>training</b> <b>samples</b> for supervised classification increases exponentially with the dimensionality [5]. <b>Sparsity</b> of data points in the higher dimensions makes the learning task very difficult if not impossible. High dimensionality also causes scalability problems for machine learning algorithms [3]. Such inability to scale exhibits itself in the form of substantial computational cost, which translates into indefinitely long <b>training</b> periods, or possibly (in the ...", "dateLastCrawled": "2022-01-23T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep Learning Rules of Thumb</b> - Jeff Macaluso", "url": "https://jeffmacaluso.github.io/post/DeepLearningRulesOfThumb/", "isFamilyFriendly": true, "displayUrl": "https://jeffmacaluso.github.io/post/<b>DeepLearningRulesOfThumb</b>", "snippet": "Bayesian methods typically generalize <b>much</b> better when limited <b>training</b> data is available, but they typically suffer from high computational cost when <b>the number</b> <b>of training</b> examples is large. (pg. 133) The most common cost function is the negative log-likelihood. As a result, minimizing the cost function causes maximum likelihood estimation ...", "dateLastCrawled": "2022-01-31T18:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "In <b>Defense of Sparsity Based Face Recognition</b> | Request PDF", "url": "https://www.researchgate.net/publication/261479433_In_Defense_of_Sparsity_Based_Face_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261479433_In_<b>Defense_of_Sparsity_Based_Face</b>...", "snippet": "A prevailing view is that the <b>sparsity based face recognition</b> performs well only when the <b>training</b> images have been carefully controlled and <b>the number</b> of <b>samples</b> per class is sufficiently large ...", "dateLastCrawled": "2022-02-03T12:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Statistical Learning with Sparsity The Lasso and Generalizations</b> Pages ...", "url": "https://fliphtml5.com/mofx/mxtu/basic/", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/mofx/mxtu/basic", "snippet": "In thehigh-dimensional setting, in which <b>the number</b> <b>of features</b> p is larger <b>than</b> thesample size, it cannot be used without modi\ufb01cation. When p &gt; N , any linearmodel is over-parametrized, and regularization is needed to achieve a stable\ufb01t. Such high-dimensional models arise in various applications. For example,document classi\ufb01cation problems can involve binary <b>features</b> (presence versus 1For SVMs, it is convenient to code the binary response via the sign function. 2This is not the most ...", "dateLastCrawled": "2022-01-19T00:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Discriminative extreme learning machine with</b> supervised <b>sparsity</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231217301972", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231217301972", "snippet": "The random indices for selecting <b>training</b> <b>samples</b> are kept the same for all compared algorithms. In general, the classification rate varies with the dimensionality of the subspace and thus the best average performance obtained is reported. <b>The number</b> of nearest neighbors in NPE is set as n train-1 if n train is <b>smaller</b> <b>than</b> 5; otherwise, it is ...", "dateLastCrawled": "2021-11-10T08:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Subnetwork representation learning for discovering network biomarkers ...", "url": "https://www.nature.com/articles/s41598-021-03333-5", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-021-03333-5", "snippet": "Essentially, the problem <b>can</b> <b>be thought</b> of as a clustering node within a PPI network represented in the form of an adjacency matrix (Fig. 1). The <b>sparsity</b> of network representation is useful for ...", "dateLastCrawled": "2022-01-29T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Gentle Introduction to Sparse Matrices for Machine Learning", "url": "https://machinelearningmastery.com/sparse-matrices-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/sparse-matrices-for-machine-learning", "snippet": "An example of a <b>smaller</b> sparse matrix might be a word or term occurrence matrix for words in one book against all known words in English. In both cases, the matrix contained is sparse with many more zero values <b>than</b> data values. The problem with representing these sparse matrices as dense matrices is that memory is required and must be allocated for each 32-bit or even 64-bit zero value in the matrix. This is clearly a waste of memory resources as those zero values do not contain any ...", "dateLastCrawled": "2022-02-02T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is the relation between <b>the number</b> of <b>Support Vectors</b> and <b>training</b> ...", "url": "https://stackoverflow.com/questions/9480605/what-is-the-relation-between-the-number-of-support-vectors-and-training-data-and", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/9480605", "snippet": "Both <b>number</b> of <b>samples</b> and <b>number</b> of attributes may influence <b>the number</b> of <b>support vectors</b>, making model more complex. I believe you use words or even ngrams as attributes, so there are quite many of them, and natural language models are very complex themselves. So, 800 <b>support vectors</b> of 1000 <b>samples</b> seem to be ok. (Also pay attention to @karenu&#39;s comments about C/nu parameters that also have large effect on SVs <b>number</b>).", "dateLastCrawled": "2022-01-23T13:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Beyond <b>sparsity</b>: The role of L1-<b>optimizer in pattern classification</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S003132031100361X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S003132031100361X", "snippet": "L 0-optimizer <b>can</b> achieve <b>sparsity</b> only, ... of Class \u201c0\u201d, noticing that the representation weights of the former, as a whole, <b>is much</b> <b>smaller</b> <b>than</b> that of the later in sense of L 1-norm. This closeness provides very important information for classification. Although the test sample Y <b>can</b> be represented most sparsely by X 1 and X 2, it is far away from these two <b>samples</b>, so Y is not likely to belong to the class of X 1 and X 2. Actually, the physical meaning of minimal L 1-norm of ...", "dateLastCrawled": "2022-01-05T17:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning Structured Sparsity in Deep Neural Networks</b> | Request PDF", "url": "https://www.researchgate.net/publication/306187229_Learning_Structured_Sparsity_in_Deep_Neural_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/306187229_Learning_Structured_<b>Sparsity</b>_in...", "snippet": "While pruning techniques <b>can</b> also be used in <b>training</b> [40, 51], they do not necessarily reduce <b>the number</b> <b>of training</b> variables. Low-precision arithmetic [13,24,31,46] <b>can</b> reduce the cost per ...", "dateLastCrawled": "2022-01-21T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What are L1, L2 <b>and Elastic Net Regularization in neural</b> ... - MachineCurve", "url": "https://www.machinecurve.com/index.php/2020/01/21/what-are-l1-l2-and-elastic-net-regularization-in-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/01/21/what-are-l1-l2-and-elastic-net...", "snippet": "The need for regularization during model <b>training</b>. When you are <b>training</b> a machine learning model, at a high level, you\u2019re learning a function \\(\\hat{y}: f(x) \\) which transforms some input value \\(x\\) (often a vector, so \\(\\textbf{x}\\)) into some output value \\(\\hat{y}\\) (often a scalar value, such as a class when classifying and a real <b>number</b> when regressing).. Contrary to a regular mathematical function, the exact mapping (to \\(y\\)) is not known in advance, but is learnt based on the ...", "dateLastCrawled": "2022-02-02T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Sparsity and Compressed Coding in Sensory Systems</b>", "url": "https://www.researchgate.net/publication/264939334_Sparsity_and_Compressed_Coding_in_Sensory_Systems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/264939334_<b>Sparsity_and_Compressed_Coding_in</b>...", "snippet": "random samplings who se <b>number</b> m <b>is much</b> <b>smaller</b> <b>than</b> <b>the. number</b> n of pixels compo sing the image by findin g the. reconstruction with the smallest <b>number</b> of nonz ero wavenum-ber-space component ...", "dateLastCrawled": "2021-08-08T06:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Paper Review: Dropout: A Simple <b>Way to Prevent Neural Networks from</b> ...", "url": "https://medium.com/paper-club/paper-review-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting-4f25e8f2283a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/paper-club/paper-review-dropout-a-simple-way-to-prevent-neural...", "snippet": "This prevents units from co-adapting too <b>much</b>. During <b>training</b>, dropout <b>samples</b> from an exponential <b>number</b> of different \u201cthinned\u201d networks. At test time, it is easy to approximate the effect ...", "dateLastCrawled": "2022-02-01T12:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Winter Quarter 2019 Stanford University", "url": "https://cs230.stanford.edu/files/cs230exam_win19_soln.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs230.stanford.edu/files/cs230exam_win19_soln.pdf", "snippet": "<b>samples</b>. <b>Training</b> converges, but the <b>training</b> loss is very high. You then decide to train this network on 10,000 examples. Is your approach to xing the problem correct? If yes, explain the most likely results <b>of training</b> with 10,000 examples. If not, give a solution to this problem. Solution: The model is su ering from a bias problem.", "dateLastCrawled": "2022-01-25T23:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Learning Preferences with Millions of Parameters by Enforcing <b>Sparsity</b>", "url": "http://people.stern.nyu.edu/xchen3/images/SparseRanking.pdf", "isFamilyFriendly": true, "displayUrl": "people.stern.nyu.edu/xchen3/images/SparseRanking.pdf", "snippet": "tries of W), when <b>the number</b> <b>of training</b> <b>samples</b> is limited, it <b>can</b> easily lead to over\ufb01tting. Considering the dictionary with the size D = 10;000, we have D2 = 108 free parameters that need to be estimated which is far too many for small corpora. To address the above weakness, we propose to constrain W to be a sparse matrix with many zero entries for the pairs of words which are irrelevant for the preference learning task. If W is a highly sparse matrix, then it consumes 1For the sake of ...", "dateLastCrawled": "2021-11-04T08:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sparsity Preserving Discriminant Projections with Applications</b> to Face ...", "url": "https://www.hindawi.com/journals/mpe/2016/5269236/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2016/5269236", "snippet": "Given a set <b>of training</b> <b>samples</b> , where and is <b>the number</b> <b>of training</b> <b>samples</b>, ... It is worth noting that since the <b>training</b> sample size <b>is much</b> <b>smaller</b> <b>than</b> the feature dimensions for those high-dimensional data, might be singular. This problem <b>can</b> be tackled by projecting the <b>training</b> set onto a PCA subspace spanned by the leading eigenvectors to get and replacing by . Based on the above discussion, the proposed SPDP is summarized in Algorithm 1. Algorithm 1 (<b>Sparsity</b> Preserving ...", "dateLastCrawled": "2022-01-26T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparsity and Robustness in Face Recognition</b> | DeepAI", "url": "https://deepai.org/publication/sparsity-and-robustness-in-face-recognition", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>sparsity-and-robustness-in-face-recognition</b>", "snippet": "Namely, the paper assumes that if we have observed a sufficient <b>number</b> of well-aligned <b>training</b> <b>samples</b> a 1 \u2026 a n of a given subject j, then given a new test image y of the same subject, we <b>can</b> write. y \u2248 [a 1 \u2223 \u22ef \u2223 a n] x \u2250 A j x, (1.1) where x. is a vector of coefficients. This low-dimensional linear approximation is motivated by theoretical results [BJ03, FSB04, Ram02] showing that well-aligned images of a convex, Lambertian object lie near a low-dimensional linear subspace of ...", "dateLastCrawled": "2022-01-18T04:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What are L1, L2 <b>and Elastic Net Regularization in neural</b> ... - MachineCurve", "url": "https://www.machinecurve.com/index.php/2020/01/21/what-are-l1-l2-and-elastic-net-regularization-in-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/01/21/what-are-l1-l2-and-elastic-net...", "snippet": "Lasso does not work that well in a high-dimensional case, i.e. where <b>the number</b> of <b>samples</b> is lower <b>than</b> <b>the number</b> of dimensions (Tripathi, n.d.; Wikipedia, 2011). This is also called the \u201clarge \\(p\\), small \\(n\\) case\u201d or the \u201cshort, fat data problem\u201d, and it\u2019s not good because L1 regularization <b>can</b> only select \\(n\\) variables at most (Duke University, n.d.; Tripathi, n.d.).", "dateLastCrawled": "2022-02-02T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>How Principal Component Analysis, PCA Works</b>", "url": "https://dataaspirant.com/principal-component-analysis-pca/", "isFamilyFriendly": true, "displayUrl": "https://dataaspirant.com/<b>principal-component-analysis-pca</b>", "snippet": "The increase in <b>the number</b> <b>of features</b> will not always improve classification accuracy. When enough <b>features</b> are not present in the data, ... then <b>the number</b> <b>of training</b> <b>samples</b> required to cover all the combinations increases phenomenally. In the above figure, it is shown that for two variables, we have eight <b>training</b> <b>samples</b>. So, for three variables, we need 24 <b>samples</b>, and so on. Distance Concentration. Distance concentration <b>can</b> be defined as: The problem of convergence of all pairwise ...", "dateLastCrawled": "2022-02-02T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Paper Review: Dropout: A Simple <b>Way to Prevent Neural Networks from</b> ...", "url": "https://medium.com/paper-club/paper-review-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting-4f25e8f2283a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/paper-club/paper-review-dropout-a-simple-way-to-prevent-neural...", "snippet": "This prevents units from co-adapting too <b>much</b>. During <b>training</b>, dropout <b>samples</b> from an exponential <b>number</b> of different \u201cthinned\u201d networks. At test time, it is easy to approximate the effect ...", "dateLastCrawled": "2022-02-01T12:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is the purpose of <b>sparsity</b> constraint in autoencoder? - Quora", "url": "https://www.quora.com/What-is-the-purpose-of-sparsity-constraint-in-autoencoder", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-purpose-of-<b>sparsity</b>-constraint-in-autoencoder", "snippet": "Answer (1 of 3): To frame the answer in a couple of sentences, I\u2019d say <b>sparsity</b> constraint prevents overcomplete autoencoders (where the dimension of the hidden layer is greater <b>than</b> that of the the input layer) from using the trivial identity mapping (simple copy of the input layer information t...", "dateLastCrawled": "2022-01-19T06:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "On Regularisation Methods for Analysis of <b>High Dimensional</b> Data ...", "url": "https://link.springer.com/article/10.1007/s40745-019-00209-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40745-019-00209-4", "snippet": "\u201c<b>High dimensional</b>\u201d refers to the situations where <b>the number</b> of covariates or predictors <b>is much</b> larger <b>than</b> <b>the number</b> of data points (i.e., \\(p\\gg n\\)). Such situations happen in many domains nowadays where the rapid development of technological advances helps collect a large <b>number</b> of variables to better understand a given phenomenon of interest. Examples occur in genomics, fMRI data analysis, large-scale healthcare analytics, text/image analysis and astronomy, to name but a few.", "dateLastCrawled": "2022-01-31T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Linear</b> Regression - Machine Learning", "url": "https://datafiction.github.io/docs/ml/LinearModels/Linear-models/", "isFamilyFriendly": true, "displayUrl": "https://datafiction.github.io/docs/ml/<b>Linear</b>Models/<b>Linear</b>-models", "snippet": "On the opposite, coordinate descent compute the path points on a pre-specified grid (here we use the default). Thus it is more efficient if <b>the number</b> of grid points is <b>smaller</b> <b>than</b> <b>the number</b> of kinks in the path. Such a strategy <b>can</b> be interesting if <b>the number</b> <b>of features</b> is really large and there are enough <b>samples</b> to select a large amount ...", "dateLastCrawled": "2022-01-25T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the minimum <b>number</b> of <b>samples</b> needed to use a deep learning ...", "url": "https://www.reddit.com/r/MachineLearning/comments/3rrxim/what_is_the_minimum_number_of_samples_needed_to/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/3rrxim/what_is_the_minimum_<b>number</b>_of_<b>samples</b>_needed_to", "snippet": "There are various factors that might influence <b>the number</b> of <b>samples</b> you need to train a neural network &#39;effectively&#39;. The choice of <b>number</b> of layers, <b>number</b> of hidden units per layer etc. It is impossible to a priori comment on the space you are trying to classify. Some classification problems are easier <b>than</b> others and therefore would require fewer layers/units and therefore parameters. 1. Reply. Share. Report Save Follow. level 2. Op \u00b7 6 yr. ago. I figured the answer would be &quot;it depends ...", "dateLastCrawled": "2022-01-05T02:33:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Sparsity</b> is an essential feature of many contemporary data problems. Remote sensing, various forms of automated screening and other high throughput measurement devices collect a large amount of ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An E\ufb03cient Sparse Metric <b>Learning</b> in High ... - <b>Machine</b> <b>Learning</b>", "url": "http://machinelearning.org/archive/icml2009/papers/46.pdf", "isFamilyFriendly": true, "displayUrl": "<b>machinelearning</b>.org/archive/icml2009/papers/46.pdf", "snippet": "This <b>sparsity</b> prior of <b>learning</b> distance metric serves to regularize the com-plexity of the distance model especially in the \u201cless example number p and high dimension d\u201d setting. Theoretically, by <b>analogy</b> to the covariance estimation problem, we \ufb01nd the proposed distance <b>learning</b> algorithm has a consistent result at rate O!&quot;# m2 logd $% n &amp; to the target distance matrix with at most m nonzeros per row. Moreover, from the imple-mentation perspective, this! 1-penalized log-determinant ...", "dateLastCrawled": "2021-11-19T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Regularization \u2014 Understanding L1 and L2 regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2...", "snippet": "The <b>sparsity</b> feature used in L1 regularization has been used extensively as a feature selection mechanism in <b>machine</b> <b>learning</b>. Feature selection is a mechanism which inherently simplifies a ...", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Dynamical <b>machine</b> <b>learning</b> volumetric reconstruction of objects ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8027224/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8027224", "snippet": "The sequence index in the angle of illumination plays the role of discrete time in the dynamical system <b>analogy</b>. Thus, the imaging problem turns into a problem of nonlinear system identification, which also suggests dynamical <b>learning</b> as a better fit to regularize the reconstructions. We devised a Recurrent Neural Network (RNN) architecture with a novel Separable-Convolution Gated Recurrent Unit (SC-GRU) as the fundamental building block. Through a comprehensive comparison of several ...", "dateLastCrawled": "2022-01-08T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Discovering governing equations from data</b> by sparse identification of ...", "url": "https://www.pnas.org/content/pnas/113/15/3932.full.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/<b>pnas</b>/113/15/3932.full.pdf", "snippet": "examples. In this work, we combin e <b>sparsity</b>-promoting techniques and <b>machine</b> <b>learning</b> with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only as-sumption about the structureof the model is that there are onlya few important terms that govern the dy namics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to ...", "dateLastCrawled": "2022-01-20T20:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "What are the <b>basic concepts in machine learning</b>? I found that the best way to discover and get a handle on the <b>basic concepts in machine learning</b> is to review the introduction chapters to <b>machine learning</b> textbooks and to watch the videos from the first model in online courses. Pedro Domingos is a lecturer and professor on <b>machine learning</b> at the University of Washing and", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>Learning</b> in Medical Imaging", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4220564/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4220564", "snippet": "SUPERVISED <b>LEARNING</b>. In <b>machine</b> <b>learning</b>, one often seeks to predict an output variable y based on a vector x of input variables. To accomplish this, it is assumed that the input and output approximately obey a functional relationship y=f (x), called the predictive model, as shown in Figure 1.In supervised <b>learning</b>, the predictive model is discovered with the benefit of training data consisting of examples for which both x and y are known. We will denote these available pairs of examples as ...", "dateLastCrawled": "2022-02-03T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Dynamical machine learning volumetric reconstruction of</b> objects ...", "url": "https://www.nature.com/articles/s41377-021-00512-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41377-021-00512-x", "snippet": "Recently, thanks to a ground-breaking observation from 2010 that <b>sparsity</b> can be learnt by a deep neural network 48, the idea of using <b>machine</b> <b>learning</b> to approximate solutions to inverse problems ...", "dateLastCrawled": "2022-02-02T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "regression - Why L1 norm for sparse models - Cross Validated", "url": "https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/45643", "snippet": "There are many norms that lead to <b>sparsity</b> (e.g., as you mentioned, any Lp norm with p &lt;= 1). In general, any norm with a sharp corner at zero induces <b>sparsity</b>. So, going back to the original question - the L1 norm induces <b>sparsity</b> by having a discontinuous gradient at zero (and any other penalty with this property will do so too). $\\endgroup$", "dateLastCrawled": "2022-01-26T23:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "The result is a <b>learning</b> model that may result in generally better word embeddings. GloVe, is a new global log-bilinear regression model for the unsupervised <b>learning</b> of word representations that outperforms other models on word <b>analogy</b>, word similarity, and named entity recognition tasks. \u2014 GloVe: Global Vectors for Word Representation, 2014.", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Learning Neural Representations for Network Anomaly Detection</b>", "url": "https://www.researchgate.net/publication/325797465_Learning_Neural_Representations_for_Network_Anomaly_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/325797465_<b>Learning</b>_Neural_Representations_for...", "snippet": "Many <b>machine</b> <b>learning</b> algorithms have been. Manuscript received December 22, 2017; revised March 13, 2018. This. work is funded by Vietnam International Education De velopment (VIED) and. by ...", "dateLastCrawled": "2021-12-06T22:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Self-representation based dual-graph regularized <b>feature selection</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231215010759", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation ... Her current research interests include pattern recognition and <b>machine</b> <b>learning</b>. Licheng Jiao (SM\u05f389) received the B.S. degree from Shanghai Jiaotong University, Shanghai, China, in 1982, the M.S. and Ph.D. degrees from Xi\u05f3an Jiaotong University, Xi\u05f3an, China, in 1984 and 1990, respectively. From 1990 to 1991, he was a postdoctoral Fellow in the National Key Laboratory for Radar Signal ...", "dateLastCrawled": "2021-11-22T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Self-representation based dual-graph regularized feature selection ...", "url": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.xidian.edu.cn/rhshang/files/20160516_172953.pdf", "snippet": "<b>machine</b> <b>learning</b> and computer vision \ufb01elds [41]. <b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation [41]. Taking into account of manifold <b>learning</b> and feature selection, and inspired by the self-representation property and the idea of dual-regularization <b>learning</b> [44,45], we propose a novel feature selection algorithm for clustering, named self-representation based dual-graph regularized feature selection clustering (DFSC). This algorithm ...", "dateLastCrawled": "2022-02-02T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Unsupervised feature selection</b> by <b>regularized self-representation</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320314002970", "snippet": "<b>Just as sparsity</b> leads to sparse representation, self-similarity results in self-representation. With the above considerations, in this paper we propose a simple yet very effective <b>unsupervised feature selection</b> method by exploiting the self-representation ability of features. The feature matrix is represented over itself to find the representative feature components. The representation residual is minimized by L 2, 1-norm loss to reduce the effect of outlier samples. Different from the ...", "dateLastCrawled": "2022-01-24T12:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Talk <b>Archive</b> - Research on Algorithms and Incentives in Networks", "url": "https://rain.stanford.edu/schedule/archive.shtml", "isFamilyFriendly": true, "displayUrl": "https://rain.stanford.edu/schedule/<b>archive</b>.shtml", "snippet": "McFowland\u2019s research interests\u2014which lie at the intersection of Information Systems, <b>Machine</b> <b>Learning</b>, and Public Policy\u2014include the development of computationally efficient algorithms for large-scale statistical <b>machine</b> <b>learning</b> and \u201cbig data\u201d analytics. More specifically, his research seeks to demonstrate that many real-world problems faced by organizations, and society more broadly, can be reduced to the tasks of anomalous pattern detection and discovery. As a data and ...", "dateLastCrawled": "2022-01-20T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Talks - <b>sites.google.com</b>", "url": "https://sites.google.com/view/dssseminarseries/talks", "isFamilyFriendly": true, "displayUrl": "https://<b>sites.google.com</b>/view/dssseminarseries/talks", "snippet": "Abstracts &amp; Bios for upcoming talks", "dateLastCrawled": "2022-01-27T14:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Sparse representations for text categorization</b>", "url": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text_categorization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text...", "snippet": "<b>Machine</b> <b>learning</b> for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is ...", "dateLastCrawled": "2021-12-10T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Continual Learning via Neural Pruning</b> | DeepAI", "url": "https://deepai.org/publication/continual-learning-via-neural-pruning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>continual-learning-via-neural-pruning</b>", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more efficient use of resources in machines with memory constraints.", "dateLastCrawled": "2021-12-30T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Non-negative data-<b>driven mapping of structural connections</b> with ...", "url": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S105381192030759X", "snippet": "For ICA, <b>sparsity can be thought of as</b> a proxy for independence. 3.5. In-vivo data decompositions. For real data, we decomposed group-average tractography matrices, using independent component analysis (ICA) and non-negative matrix factorisation (NMF), with a range of model orders K. ICA was initialised with regular PCA, in which the first 500 components were retained (explaining 97% of the total variance). ICA was applied to the reduced dataset using the FastICA algorithm (Hyv\u00e4rinen and ...", "dateLastCrawled": "2021-10-11T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Sparse Representations for Text Categorization</b> | Dimitri Kanevsky ...", "url": "https://www.academia.edu/2738730/Sparse_Representations_for_Text_Categorization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2738730/<b>Sparse_Representations_for_Text_Categorization</b>", "snippet": "A Comparative Study of <b>Machine</b> <b>Learning</b> Methods for Verbal Autopsy Text Classification. By Eric S Atwell and Samuel Danso. CSC435 book proposal. By Russell Frith. Higher-Order Smoothing: A Novel Semantic Smoothing Method for Text Classification. By Murat C Ganiz, Mitat Poyraz, and Zeynep Kilimci. INFORMATION RETRIEVAL. By febi k. Introduction to information retrieval. By Valeria Mesi. Download pdf. \u00d7 Close Log In. Log In with Facebook Log In with Google. Sign Up with Apple. or. Email ...", "dateLastCrawled": "2021-10-13T23:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Continual <b>Learning</b> via Neural Pruning \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1903.04476/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1903.04476", "snippet": "We introduce Continual <b>Learning</b> via Neural Pruning (CLNP), a new method aimed at lifelong <b>learning</b> in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the ...", "dateLastCrawled": "2021-11-07T10:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Continual <b>Learning</b> via Neural Pruning", "url": "https://openreview.net/pdf?id=Hyl_XXYLIB", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=Hyl_XXYLIB", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much at-tention from the <b>machine</b> <b>learning</b> community in recent years. The main obstacle for effective continual <b>learning</b> is the problem of cata-strophic forgetting: machines trained on new problems forget about", "dateLastCrawled": "2022-01-05T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Abstract - arXiv.org e-Print archive", "url": "https://arxiv.org/pdf/1903.04476", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1903.04476", "snippet": "Continual <b>learning</b>, the ability of models to learn to solve new tasks beyond what has previously been trained, has garnered much attention from the <b>machine</b> <b>learning</b> community in recent years. This is driven in part by the practical advantages promised by continual <b>learning</b> schemes such as improved performance on subsequent tasks as well as a more ef\ufb01cient use of resources in machines with memory constraints. There is also great interest in continual <b>learning</b> from a more long term ...", "dateLastCrawled": "2021-10-25T14:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Introduction to compressed sensing</b>", "url": "https://www.researchgate.net/publication/220043734_Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220043734_<b>Introduction_to_compressed_sensing</b>", "snippet": "systems control, clustering, and <b>machine</b> <b>learning</b> [14, 15, 58, 61, 89, 193, 217, 240, 244]. Low-dimensional manifolds hav e also been prop osed as approximate mod-", "dateLastCrawled": "2022-01-14T18:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Introduction to compressed sensing</b> | Marco Duarte - Academia.edu", "url": "https://www.academia.edu/1443164/Introduction_to_compressed_sensing", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/1443164/<b>Introduction_to_compressed_sensing</b>", "snippet": "<b>Introduction to Compressed Sensing</b> For any x \u2208 \u03a3k , we can associate a k-face of C n with the support and sign pattern of x. One can show that the number of k-faces of AC n is precisely the number of index sets of size k for which signals supported on them can be recovered by (1.12) with B (y) = {z : Az = y}.", "dateLastCrawled": "2022-01-21T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Compressed Sensing : Theory and Applications</b> | Kutyniok, Gitta Eldar ...", "url": "https://b-ok.africa/book/2086657/84a688", "isFamilyFriendly": true, "displayUrl": "https://b-ok.africa/book/2086657/84a688", "snippet": "You can write a book review and share your experiences. Other readers will always be interested in your opinion of the books you&#39;ve read. Whether you&#39;ve loved the book or not, if you give your honest and detailed thoughts then people will find new books that are right for them.", "dateLastCrawled": "2021-12-26T07:22:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(sparsity)  is like +(the number of training samples is much smaller than the number of features)", "+(sparsity) is similar to +(the number of training samples is much smaller than the number of features)", "+(sparsity) can be thought of as +(the number of training samples is much smaller than the number of features)", "+(sparsity) can be compared to +(the number of training samples is much smaller than the number of features)", "machine learning +(sparsity AND analogy)", "machine learning +(\"sparsity is like\")", "machine learning +(\"sparsity is similar\")", "machine learning +(\"just as sparsity\")", "machine learning +(\"sparsity can be thought of as\")", "machine learning +(\"sparsity can be compared to\")"]}