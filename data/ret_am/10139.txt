{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "machine learning - What&#39;s the difference <b>between</b> <b>Error</b>, Risk and <b>Loss</b> ...", "url": "https://datascience.stackexchange.com/questions/35928/whats-the-difference-between-error-risk-and-loss", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/35928", "snippet": "This is why <b>minimizing</b> <b>loss</b> and <b>minimizing</b> empirical risk are roughly the same thing. Summary. When we are training our model, our focus should not be on <b>minimizing</b> errors or <b>loss</b>, but to minimize true risk. But most of the time, we can&#39;t, so we minimize the empirical risk and regularize.", "dateLastCrawled": "2022-02-03T06:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Cost Function &amp; <b>Loss</b> Function. In this article, I wanted to put\u2026 | by ...", "url": "https://nadeemm.medium.com/cost-function-loss-function-c3cab1ddffa4", "isFamilyFriendly": true, "displayUrl": "https://nadeemm.medium.com/cost-function-<b>loss</b>-function-c3cab1ddffa4", "snippet": "\u2018<b>Loss</b>\u2019 in Machine learning helps us understand the difference <b>between</b> the predicted <b>value</b> &amp; the <b>actual</b> <b>value</b>. The Function used to quantify this <b>loss</b> during the training phase in the form of a single real number is known as the \u201c<b>Loss</b> Function\u201d. These are used in t h ose supervised learning algorithms that use optimization techniques ...", "dateLastCrawled": "2022-02-02T15:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "Cross-Entropy <b>loss</b> is also called logarithmic <b>loss</b>, log <b>loss</b>, or logistic <b>loss</b>. Each predicted class probability is compared to the <b>actual</b> class desired output 0 or 1 and a score/<b>loss</b> is calculated that penalizes the probability based on how far it is from the <b>actual</b> expected <b>value</b>. The penalty is logarithmic in nature yielding a large score for large differences close to 1 and small score for small differences tending to 0.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Keras Loss Functions - Types and Examples</b> - DataFlair", "url": "https://data-flair.training/blogs/keras-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://data-flair.training/blogs/keras-<b>loss</b>-functions", "snippet": "Calculate the cosine similarity <b>between</b> the <b>actual</b> and predicted values. The <b>loss</b> equation is: <b>loss</b>=-sum(<b>l2</b>_norm(<b>actual</b>)*<b>l2</b>_norm(predicted)) Available in Keras as: keras.losses.CosineSimilarity(axis,reduction,name) All of these losses are available in Keras.losses module. The below code shows an example of how to use these <b>loss</b> functions in ...", "dateLastCrawled": "2022-02-03T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Cost Function in Machine Learning - Javatpoint", "url": "https://www.javatpoint.com/cost-function-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/cost-function-in-machine-learning", "snippet": "It calculated the difference <b>between</b> the <b>actual</b> values and predicted values and measured how wrong was our model in the <b>prediction</b>. By <b>minimizing</b> the <b>value</b> of the cost function, we can get the optimal solution. Gradient Descent: <b>Minimizing</b> the cost function . As we discussed in the above section, the cost function tells how wrong your model is? And each machine learning model tries to minimize the cost function in order to give the best results. Here comes the role of Gradient descent ...", "dateLastCrawled": "2022-01-29T12:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding <b>Loss</b> Functions in Computer Vision! | by Sowmya ...", "url": "https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/ml-cheat-sheet/winning-at-<b>loss</b>-functions-2-important-<b>loss</b>-functions...", "snippet": "Most of the <b>loss</b> functions discussed in the previous article such as MSE or <b>L2</b> <b>loss</b>, MAE or L1 <b>loss</b>, cross-entropy <b>loss</b>, etc, can be applied <b>between</b> every pair of pixels of the <b>prediction</b> and ...", "dateLastCrawled": "2022-02-03T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "machine learning - What do think about switching from <b>L2</b>-<b>loss</b> to L1 ...", "url": "https://stats.stackexchange.com/questions/284012/what-do-think-about-switching-from-l2-loss-to-l1-loss-when-inner-difference-beca", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/284012/what-do-think-about-switching-from-<b>l2</b>...", "snippet": "As for your idea of using a hybrid <b>between</b> L1 and <b>L2</b> <b>loss</b>, Huber <b>loss</b>, does this, but it&#39;s the other way around, preferring <b>L2</b> <b>loss</b> when the difference is small and L1 <b>loss</b> otherwise. Share. Cite. Improve this answer. Follow edited Jun 7 &#39;17 at 12:03. answered Jun 7 &#39;17 at 11:32. Neil G Neil G. 13.5k 3 3 gold badges 40 40 silver badges 84 84 bronze badges $\\endgroup$ 10. 1 $\\begingroup$ Typically we never need &quot;squared results&quot; - that is just a way to make learning faster as far as I know ...", "dateLastCrawled": "2022-01-22T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Mean <b>Square Error &amp; R2 Score Clearly Explained</b> \u2013 BMC Software | Blogs", "url": "https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/mean-squared-<b>error</b>-r2-and-variance-in-regression-analysis", "snippet": "There is no correct <b>value</b> for MSE. Simply put, the lower the <b>value</b> the better and 0 means the model is perfect. Since there is no correct answer, the MSE\u2019s basic <b>value</b> is in selecting one <b>prediction</b> model over another. Similarly, there is also no correct answer as to what R2 should be. 100% means perfect correlation. Yet, there are models ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Predicting Workplace Productivity Using Employees\u2019 Happiness Index | by ...", "url": "https://towardsdatascience.com/training-regression-models-bfd779a73a60", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/training-<b>regression</b>-models-bfd779a73a60", "snippet": "A Smaller <b>Loss</b> <b>Value</b> If the total difference <b>between</b> the predicted values and the <b>actual</b> ones is relatively small, the total <b>error</b>/<b>loss</b> will be smaller <b>value</b> and thus, signify a good model. A Larger <b>Loss</b> <b>Value</b> If the difference <b>between</b> the <b>actual</b> and predicted values is large, the total <b>error</b>/<b>value</b> of <b>loss</b> function will be relatively larger as well to imply that the model is not trained well.", "dateLastCrawled": "2022-01-31T02:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Does high <b>loss</b> <b>value</b> (will/won&#39;t/<b>usually) backprop to high gradients</b> ...", "url": "https://www.quora.com/Does-high-loss-value-will-wont-usually-backprop-to-high-gradients-for-parameters-cross-entropy-loss-for-example", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Does-high-<b>loss</b>-<b>value</b>-will-wont-usually-backprop-to-high...", "snippet": "Answer: The short answer is \u201cNo, not in general\u201d, but we can make concrete statements about certain special cases. First, it may help to consider a counterexample. Forget backprop for a second, and consider the problem of <b>minimizing</b> a function of one variable, specifically f(x) = \\sqrt{|x|} (plo...", "dateLastCrawled": "2022-01-17T04:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding <b>Loss</b> Functions in Computer Vision! | by Sowmya ...", "url": "https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/ml-cheat-sheet/winning-at-<b>loss</b>-functions-2-important-<b>loss</b>-functions...", "snippet": "Most of the <b>loss</b> functions discussed in the previous article such as MSE or <b>L2</b> <b>loss</b>, MAE or L1 <b>loss</b>, cross-entropy <b>loss</b>, etc, can be applied <b>between</b> every pair of pixels of the <b>prediction</b> and ...", "dateLastCrawled": "2022-02-03T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "machine learning - What do think about switching from <b>L2</b>-<b>loss</b> to L1 ...", "url": "https://stats.stackexchange.com/questions/284012/what-do-think-about-switching-from-l2-loss-to-l1-loss-when-inner-difference-beca", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/284012/what-do-think-about-switching-from-<b>l2</b>...", "snippet": "As for your idea of using a hybrid <b>between</b> L1 and <b>L2</b> <b>loss</b>, Huber <b>loss</b>, does this, but it&#39;s the other way around, preferring <b>L2</b> <b>loss</b> when the difference is small and L1 <b>loss</b> otherwise. Share. Cite. Improve this answer. Follow edited Jun 7 &#39;17 at 12:03. answered Jun 7 &#39;17 at 11:32. Neil G Neil G. 13.5k 3 3 gold badges 40 40 silver badges 84 84 bronze badges $\\endgroup$ 10. 1 $\\begingroup$ Typically we never need &quot;squared results&quot; - that is just a way to make learning faster as far as I know ...", "dateLastCrawled": "2022-01-22T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss Function</b> (Part II): <b>Logistic Regression</b> | by Shuyu Luo | Towards ...", "url": "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/optimization-<b>loss-function</b>-under-the-hood-part-ii-d20a...", "snippet": "The <b>loss function</b> of <b>logistic regression</b> is doing this exactly which is called Logistic <b>Loss</b>. See as below. If y = 1, looking at the plot below on left, when <b>prediction</b> = 1, the cost = 0, when <b>prediction</b> = 0, the learning algorithm is punished by a very large cost. Similarly, if y = 0, the plot on right shows, predicting 0 has no punishment but predicting 1 has a large <b>value</b> of cost.", "dateLastCrawled": "2022-02-02T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An overview of the <b>Gradient Descent</b> algorithm | by Nishit Jain ...", "url": "https://towardsdatascience.com/an-overview-of-the-gradient-descent-algorithm-8645c9e4de1e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/an-overview-of-the-<b>gradient-descent</b>-algorithm-8645c9e4de1e", "snippet": "<b>L2</b> <b>Loss</b> / Mean Squared <b>Error</b>; Root Mean Squared <b>Error</b>; Classification Losses: Log <b>Loss</b> (Cross-Entropy <b>Loss</b>) SVM <b>Loss</b> (Hinge <b>Loss</b>) Learning Rate: This is the hyperparameter that determines the steps the <b>gradient descent</b> algorithm takes. <b>Gradient Descent</b> is too sensitive to the learning rate. If it is too big, the algorithm may bypass the local minimum and overshoot. If it too small, it might increase the total computation time to a very large extent. We will see the effect of the learning ...", "dateLastCrawled": "2022-02-01T20:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "That is: when the <b>actual</b> target meets the <b>prediction</b>, the <b>loss</b> is zero. Negative <b>loss</b> doesn\u2019t exist. When the target != the <b>prediction</b>, the <b>loss</b> <b>value</b> increases. For t = 1, or \\(1\\) is your target, hinge <b>loss</b> looks like this: Let\u2019s now consider three scenarios which can occur, given our target \\(t = 1\\) (Kompella, 2017; Wikipedia, 2011):", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "<b>Minimizing</b> this KL divergence corresponds exactly <b>to minimizing</b> the cross-entropy <b>between</b> the distributions. \u2014 Page 132, Deep Learning, 2016. A benefit of using maximum likelihood as a framework for estimating the model parameters (weights) for neural networks and in machine learning in general is that as the number of examples in the training dataset is increased, the estimate of the model parameters improves. This is called the property of \u201c consistency.\u201d Under appropriate conditions ...", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "The main difference <b>between</b> the hinge <b>loss</b> and the cross entropy <b>loss</b> is that the former arises from trying to maximize the margin <b>between</b> our decision boundary and data points - thus attempting to ensure that each point is correctly and confidently classified*, while the latter comes from a maximum likelihood estimate of our model\u2019s parameters. The softmax function, whose scores are used by the cross entropy <b>loss</b>, allows us to interpret our model\u2019s scores as relative probabilities ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Cost Function in Machine Learning - Javatpoint", "url": "https://www.javatpoint.com/cost-function-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/cost-function-in-machine-learning", "snippet": "It calculated the difference <b>between</b> the <b>actual</b> values and predicted values and measured how wrong was our model in the <b>prediction</b>. By <b>minimizing</b> the <b>value</b> of the cost function, we can get the optimal solution. Gradient Descent: <b>Minimizing</b> the cost function . As we discussed in the above section, the cost function tells how wrong your model is? And each machine learning model tries to minimize the cost function in order to give the best results. Here comes the role of Gradient descent ...", "dateLastCrawled": "2022-01-29T12:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Machine learning (2): understand linear regression and gradient descent ...", "url": "https://developpaper.com/machine-learning-2-understand-linear-regression-and-gradient-descent-and-make-simple-prediction/", "isFamilyFriendly": true, "displayUrl": "https://developpaper.com/machine-learning-2-understand-linear-regression-and-gradient...", "snippet": "<b>Prediction</b> begins with guesswork PressLast articleMachine learning is the process of applying mathematical methods to find laws in data. Since mathematics is the interpretation of the real world, let\u2019s return to the real world and make some comparative imagination. Imagine that there is a white board made of plastic foam in front of us. There [\u2026]", "dateLastCrawled": "2022-01-13T01:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Does high <b>loss</b> <b>value</b> (will/won&#39;t/<b>usually) backprop to high gradients</b> ...", "url": "https://www.quora.com/Does-high-loss-value-will-wont-usually-backprop-to-high-gradients-for-parameters-cross-entropy-loss-for-example", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Does-high-<b>loss</b>-<b>value</b>-will-wont-usually-backprop-to-high...", "snippet": "Answer: The short answer is \u201cNo, not in general\u201d, but we can make concrete statements about certain special cases. First, it may help to consider a counterexample. Forget backprop for a second, and consider the problem of <b>minimizing</b> a function of one variable, specifically f(x) = \\sqrt{|x|} (plo...", "dateLastCrawled": "2022-01-17T04:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding <b>Loss</b> Functions in Computer Vision! | by Sowmya ...", "url": "https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/ml-cheat-sheet/winning-at-<b>loss</b>-functions-2-important-<b>loss</b>-functions...", "snippet": "Most of the <b>loss</b> functions discussed in the previous article such as MSE or <b>L2</b> <b>loss</b>, MAE or L1 <b>loss</b>, cross-entropy <b>loss</b>, etc, <b>can</b> be applied <b>between</b> every pair of pixels of the <b>prediction</b> and ...", "dateLastCrawled": "2022-02-03T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "machine learning - What do think about switching from <b>L2</b>-<b>loss</b> to L1 ...", "url": "https://stats.stackexchange.com/questions/284012/what-do-think-about-switching-from-l2-loss-to-l1-loss-when-inner-difference-beca", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/284012/what-do-think-about-switching-from-<b>l2</b>...", "snippet": "Changing the <b>loss</b>, changes the problem, so you <b>can</b>&#39;t objectively compare using one <b>loss</b> with other. As for your idea of using a hybrid <b>between</b> L1 and <b>L2</b> <b>loss</b>, Huber <b>loss</b>, does this, but it&#39;s the other way around, preferring <b>L2</b> <b>loss</b> when the difference is small and L1 <b>loss</b> otherwise.", "dateLastCrawled": "2022-01-22T03:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "Mean Square <b>Error</b>/Quadratic <b>Loss</b>/<b>L2</b> <b>Loss</b>: averages the squared difference <b>between</b> predictions and ground truth, with a focus on the average magnitudes of errors regardless of direction. Mean Absolute <b>Error</b>, L1 <b>Loss</b> (used by PerceptiLabs\u2019 Regression component): sums the absolute differences <b>between</b> the predictions and ground truth, and finds the average.", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "Interpreting the cross-entropy <b>loss</b> as <b>minimizing</b> the KL divergence <b>between</b> 2 distributions is interesting if we consider how we <b>can</b> extend cross-entropy to different scenarios. For example, a lot of datasets are only partially labelled or have noisy (i.e. occasionally incorrect) labels. If we could probabilistically assign labels to the unlabelled portion of a dataset, or interpret the incorrect labels as being sampled from a probabalistic noise distribution, we <b>can</b> still apply the idea of ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Choosing and Customizing Loss Functions for Image Processing</b>", "url": "https://blog.perceptilabs.com/choosing-and-customizing-loss-functions-for-image-processing/", "isFamilyFriendly": true, "displayUrl": "https://blog.perceptilabs.com/<b>choosing-and-customizing-loss-functions-for-image-processing</b>", "snippet": "Mean Square <b>Error</b>/Quadratic <b>Loss</b>/<b>L2</b> <b>Loss</b>: averages the squared difference <b>between</b> predictions and ground truth, with a focus on the average magnitudes of errors regardless of direction. Mean Absolute <b>Error</b>, L1 <b>Loss</b> (used by PerceptiLabs&#39; Regression component): sums the absolute differences <b>between</b> the predictions and ground truth, and finds the average.", "dateLastCrawled": "2022-01-28T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>loss</b>-functions-and-optimization-algorithms...", "snippet": "One of the most widely used <b>loss function</b> is mean square <b>error</b>, which calculates the square of difference <b>between</b> <b>actual</b> <b>value</b> and predicted <b>value</b>. Different <b>loss</b> functions are used to deal with ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Mean <b>Square Error &amp; R2 Score Clearly Explained</b> \u2013 BMC Software | Blogs", "url": "https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/mean-squared-<b>error</b>-r2-and-variance-in-regression-analysis", "snippet": "The r2 score varies <b>between</b> 0 and 100%. It is closely related to the MSE (see below), but not the same. Wikipedia defines r2 as. \u201d \u2026the proportion of the variance in the dependent variable that is predictable from the independent variable (s).\u201d. Another definition is \u201c (total variance explained by model) / total variance.\u201d.", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "A. The <b>value</b> of the gradient at extrema of a function is always zero B. Depends on the type of problem C. Both A and B D. None of the above Answer : A Explanation: The gradient of a multivariable function at a maximum point will be the zero vector of the function, which is the single greatest <b>value</b> that the function <b>can</b> achieve. 13. Which of ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: Data ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "It is also called the Positive Predictive <b>Value</b> (PPV). Precision <b>can</b> <b>be thought</b> of as a measure of a classifiers exactness. A low precision <b>can</b> also indicate a large number of False Positives. Recall: A measure of a classifiers completeness. Recall is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Mean Squared Error (MSE</b>)", "url": "https://www.probabilitycourse.com/chapter9/9_1_5_mean_squared_error_MSE.php", "isFamilyFriendly": true, "displayUrl": "https://www.probabilitycourse.com/chapter9/<b>9_1_5_mean_squared_error_MSE</b>.php", "snippet": "Let us look at an example to practice the above concepts. This is an example involving jointly normal random variables. Thus, before solving the example, it is useful to remember the properties of jointly normal random variables.", "dateLastCrawled": "2022-02-03T04:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Understanding Loss Functions in Machine Learning</b> | Engineering ...", "url": "https://www.section.io/engineering-education/understanding-loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.section.io/engineering-education/<b>understanding-loss-functions-in-machine</b>...", "snippet": "Huber <b>Loss</b>. A comparison <b>between</b> L1 and <b>L2</b> <b>loss</b> yields the following results: L1 <b>loss</b> is more robust than its counterpart. On taking a closer look at the formulas, one <b>can</b> observe that if the difference <b>between</b> the predicted and the <b>actual</b> <b>value</b> is high, <b>L2</b> <b>loss</b> magnifies the effect when <b>compared</b> to L1. Since <b>L2</b> succumbs to outliers, L1 <b>loss</b> ...", "dateLastCrawled": "2022-02-01T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>L2</b> vs <b>L1 Regularization in Machine Learning</b> | Ridge and Lasso ...", "url": "https://www.analyticssteps.com/blogs/l2-and-l1-regularization-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.analyticssteps.com/blogs/<b>l2</b>-and-l1-regularization-machine-learning", "snippet": "Both L1 and <b>L2</b> <b>can</b> add a penalty to the cost depending upon the model complexity, so at the place of computing the cost by using a <b>loss</b> function, there will be an auxiliary component, known as regularization terms, added in order to panelizing complex models.", "dateLastCrawled": "2022-02-02T11:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding <b>Loss</b> Functions in Computer Vision! | by Sowmya ...", "url": "https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/ml-cheat-sheet/winning-at-<b>loss</b>-functions-2-important-<b>loss</b>-functions...", "snippet": "Most of the <b>loss</b> functions discussed in the previous article such as MSE or <b>L2</b> <b>loss</b>, MAE or L1 <b>loss</b>, cross-entropy <b>loss</b>, etc, <b>can</b> be applied <b>between</b> every pair of pixels of the <b>prediction</b> and ...", "dateLastCrawled": "2022-02-03T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "<b>Minimizing</b> this KL divergence corresponds exactly <b>to minimizing</b> the cross-entropy <b>between</b> the distributions. \u2014 Page 132, Deep Learning, 2016. A benefit of using maximum likelihood as a framework for estimating the model parameters (weights) for neural networks and in machine learning in general is that as the number of examples in the training dataset is increased, the estimate of the model parameters improves. This is called the property of \u201c consistency.\u201d Under appropriate conditions ...", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "Cross-Entropy <b>loss</b> is also called logarithmic <b>loss</b>, log <b>loss</b>, or logistic <b>loss</b>. Each predicted class probability is <b>compared</b> to the <b>actual</b> class desired output 0 or 1 and a score/<b>loss</b> is calculated that penalizes the probability based on how far it is from the <b>actual</b> expected <b>value</b>. The penalty is logarithmic in nature yielding a large score for large differences close to 1 and small score for small differences tending to 0.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "Cross-entropy will calculate a score that summarizes the average difference <b>between</b> the <b>actual</b> and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy <b>value</b> is 0. Cross-entropy <b>can</b> be specified as the <b>loss</b> function in Keras by specifying \u2018binary_crossentropy\u2018 when compiling the model.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "What does it mean? The <b>prediction</b> y of the classifier is based on the <b>value</b> of the input x.Assuming margin to have the default <b>value</b> of 1, if y=-1, then the <b>loss</b> will be maximum of 0 and (1 \u2014 x ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "That is: when the <b>actual</b> target meets the <b>prediction</b>, the <b>loss</b> is zero. Negative <b>loss</b> doesn\u2019t exist. When the target != the <b>prediction</b>, the <b>loss</b> <b>value</b> increases. For t = 1, or \\(1\\) is your target, hinge <b>loss</b> looks like this: Let\u2019s now consider three scenarios which <b>can</b> occur, given our target \\(t = 1\\) (Kompella, 2017; Wikipedia, 2011):", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Cost Function in Machine Learning - Javatpoint", "url": "https://www.javatpoint.com/cost-function-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/cost-function-in-machine-learning", "snippet": "It calculated the difference <b>between</b> the <b>actual</b> values and predicted values and measured how wrong was our model in the <b>prediction</b>. By <b>minimizing</b> the <b>value</b> of the cost function, we <b>can</b> get the optimal solution. Gradient Descent: <b>Minimizing</b> the cost function . As we discussed in the above section, the cost function tells how wrong your model is? And each machine learning model tries to minimize the cost function in order to give the best results. Here comes the role of Gradient descent ...", "dateLastCrawled": "2022-01-29T12:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Estimators, <b>Loss</b> Functions, Optimizers \u2014Core of ML Algorithms | by ...", "url": "https://towardsdatascience.com/estimators-loss-functions-optimizers-core-of-ml-algorithms-d603f6b0161a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/estimators-<b>loss</b>-functions-<b>optimizer</b>s-core-of-ml...", "snippet": "In case of a binary classification each predicted probability is <b>compared</b> to the <b>actual</b> class output <b>value</b> (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected <b>value</b>. Visualization . The graph below shows the range of possible log <b>loss</b> values given a true observation (y= 1). As the predicted probability approaches 1, log <b>loss</b> slowly decreases. As the predicted probability decreases, however, the log <b>loss</b> increases rapidly. Log <b>loss</b> ...", "dateLastCrawled": "2022-02-02T04:20:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Regularization \u2014 Understanding L1 and <b>L2</b> regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-<b>l2</b>...", "snippet": "Understanding what regularization is and why it is required for <b>machine</b> <b>learning</b> and diving deep to clarify the importance of L1 and <b>L2</b> regularization in Deep <b>learning</b>.", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "is known as L1-norm, while the latter is known as the <b>L2</b>-norm. Keep in mind that <b>L2</b>-norm is more sensitive than L1-norm to large-valued outliers. Ridge and LASSO regularizations are based on <b>L2</b>-norm and L1-norm, respectively, while Elastic Net regularization is based on the mix of two. 2.6 What does a <b>machine</b> <b>learning</b> <b>learning</b>-curve measure ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Why <b>Machine Learning Is A Metaphor For</b> Life \u2013 Adit Deshpande ...", "url": "https://adeshpande3.github.io/Why-Machine-Learning-is-a-Metaphor-For-Life", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/Why-<b>Machine-Learning-is-a-Metaphor-For</b>-Life", "snippet": "Well, in <b>machine</b> <b>learning</b>, we use regularizers. The first form (and the most common) of regularization that I first learned about was <b>L2</b> regularization or weight decay. This type of regularization is basically imposing a soft constraint on the cost function. We\u2019re telling the network \u201cHey, we want you to minimize the <b>loss</b> from the training ...", "dateLastCrawled": "2022-01-31T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>machine</b> <b>learning</b> - L1-norm vs <b>l2</b>-norm as cost function when ...", "url": "https://stackoverflow.com/questions/43301036/l1-norm-vs-l2-norm-as-cost-function-when-standardizing", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43301036", "snippet": "The basic idea/motivation is how to penalize deviations. L1-norm does not care much about outliers, while <b>L2</b>-norm penalize these heavily. This is the basic difference and you will find a lot of pros and cons, even on wikipedia. So in regards to your question if it makes sense when the expected deviations are small: sure, it behaves the same.", "dateLastCrawled": "2022-01-24T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bias-Variance Decomposition</b> - mlxtend", "url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/", "isFamilyFriendly": true, "displayUrl": "rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp", "snippet": "<b>Bias variance decomposition</b> of <b>machine</b> <b>learning</b> algorithms for various <b>loss</b> functions. from mlxtend.evaluate import bias_variance_decomp. Overview. Often, researchers use the terms bias and variance or &quot;bias-variance tradeoff&quot; to describe the performance of a model -- i.e., you may stumble upon talks, books, or articles where people say that a model has a high variance or high bias. So, what does that mean? In general, we might say that &quot;high variance&quot; is proportional to overfitting, and ...", "dateLastCrawled": "2022-01-31T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, squared hinge <b>loss</b> function (as against hinge <b>loss</b> function) and <b>l2</b> penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>Gentle Introduction to Pix2Pix Generative</b> Adversarial Network", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-pix2pix-generative</b>...", "snippet": "In <b>analogy</b> to automatic language translation, we define automatic image-to-image translation as the task of translating one possible representation of a scene into another, given sufficient training data. \u2014 Image-to-Image Translation with Conditional Adversarial Networks, 2016. It is a challenging problem that typically requires the development of a specialized model and hand-crafted <b>loss</b> function for the type of translation task being performed. Classical approaches use per-pixel ...", "dateLastCrawled": "2022-02-02T13:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are some <b>examples in everyday life analogous to &#39;overfitting</b>&#39; in ...", "url": "https://www.quora.com/What-are-some-examples-in-everyday-life-analogous-to-overfitting-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-some-<b>examples-in-everyday-life-analogous-to-overfitting</b>...", "snippet": "Answer (1 of 3): Exam overfitting - When you study for an exam, only by practicing questions from previous years&#39; exams. You then discover to your horror that xx% of this year&#39;s questions are new, and you get a much lower score than on your practice ones. If you are a bit older, you can expand th...", "dateLastCrawled": "2022-01-06T06:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[D] Looking for papers on treating regression as classification vs ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7gun87/d_looking_for_papers_on_treating_regression_as/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7gun87/d_looking_for_papers_on...", "snippet": "Doing the <b>L2 loss is like</b> doing maximum likelihood on a gaussian with a fixed variance - so the bad regression here is largely coming from the gaussian being mis-specified. I think the richer question would involve comparing approaches that consider the ordering vs. approaches that don t consider the ordering but where both have flexible enough distributions.", "dateLastCrawled": "2021-01-17T22:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep learning</b> - lectures.alex.balgavy.eu", "url": "https://lectures.alex.balgavy.eu/ml-notes/deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://lectures.alex.balgavy.eu/ml-notes/<b>deep-learning</b>", "snippet": "<b>Deep learning</b> <b>Deep learning</b> systems (autodiff engines) Tensors. To scale up backpropagation, want to move from operations on scalars to tensors. Tensor: generalisation of vectors/matrices to higher dimensions. e.g. a 2-tensor has two dimensions, a 4-tensor has 4 dimensions. You can represent data as a tensor. e.g. an RGB image is a 3-tensor of the red, green, and blue values for each pixel. Functions on tensors. Functions have inputs and outputs, all of which are tensors. They implement ...", "dateLastCrawled": "2021-12-15T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A deep <b>learning</b> framework for constitutive modeling based on temporal ...", "url": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "snippet": "These two features meet the requirement for sequence modeling in <b>machine</b> <b>learning</b>. Therefore, the nonlinear constitutive models may be classified as sequence modeling from the viewpoint of <b>machine</b> <b>learning</b>. Concrete material and steel material both exhibit significant ultra-long-term memory effects and many model-driven constitutive relationships were developed to simulate stress-strain curves of materials , , , , with ultra-long-term memory effect. For steel material, the traditional ...", "dateLastCrawled": "2022-01-20T12:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(l2 loss)  is like +(minimizing error between prediction and actual value)", "+(l2 loss) is similar to +(minimizing error between prediction and actual value)", "+(l2 loss) can be thought of as +(minimizing error between prediction and actual value)", "+(l2 loss) can be compared to +(minimizing error between prediction and actual value)", "machine learning +(l2 loss AND analogy)", "machine learning +(\"l2 loss is like\")", "machine learning +(\"l2 loss is similar\")", "machine learning +(\"just as l2 loss\")", "machine learning +(\"l2 loss can be thought of as\")", "machine learning +(\"l2 loss can be compared to\")"]}