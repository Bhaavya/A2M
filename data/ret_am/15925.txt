{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Machine Learning Algorithms That Engineers Need To Know - Uncodemy", "url": "https://www.uncodemy.com/technology/machine-learning-algorithms-that-engineers-need-to-know/", "isFamilyFriendly": true, "displayUrl": "https://www.uncodemy.com/technology/machine-learning-algorithms-that-engineers-need-to...", "snippet": "Customary <b>Least</b> <b>Squares</b> <b>Regression</b>: If you know measurements, you likely have known about direct relapse previously. The <b>least</b> <b>squares</b> is a strategy for performing <b>straight</b> relapse. You can consider direct relapse the assignment of <b>fitting</b> <b>a straight</b> <b>line</b> <b>through</b> <b>a bunch</b> of focuses. There are various potential methodologies to do this, and the \u201ccommon <b>least</b> <b>squares</b>\u201d technique goes <b>like</b> this \u2014 You can define a boundary, and afterward for every one of the information focuses, measure the ...", "dateLastCrawled": "2022-02-02T23:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Basic concepts used in AI. Traditional Programming: Data and\u2026 | by ...", "url": "https://renjithnano.medium.com/basic-concepts-used-in-ai-4027720e9503", "isFamilyFriendly": true, "displayUrl": "https://renjithnano.medium.com/basic-concepts-used-in-ai-4027720e9503", "snippet": "Ordinary <b>Least</b> <b>Squares</b> <b>Regression</b>: Linear <b>regression</b> is the task of <b>fitting</b> <b>a straight</b> <b>line</b> <b>through</b> a set <b>of points</b>. There are multiple possible strategies to do this, and \u201cordinary <b>least</b> <b>squares</b>\u201d strategy go <b>like</b> this \u2014 You can draw a <b>line</b>, and then for each of the data <b>points</b>, measure the vertical distance between the point and the <b>line</b>, and add these up; the fitted <b>line</b> would be the one where this sum of distances is as small as possible.Linear refers the kind of model you are using ...", "dateLastCrawled": "2021-12-27T14:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Least</b> <b>squares</b> is a method of <b>fitting</b> a <b>regression</b> <b>line</b> which is robust ...", "url": "https://www.quora.com/Least-squares-is-a-method-of-fitting-a-regression-line-which-is-robust-i-e-safe-from-outliers-True-or-False", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Least</b>-<b>squares</b>-is-a-method-of-<b>fitting</b>-a-<b>regression</b>-<b>line</b>-which-is...", "snippet": "Answer (1 of 2): This is false. So it is \u201c<b>least</b> <b>squares</b>\u201d - the square of the residual is what you are looking to minimise. Consider your point with the highest residuals and move it some small amount. Consider how your <b>line</b> of best fit will move. Consider for a given small change how much your g...", "dateLastCrawled": "2022-01-08T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The 10 Algorithms Machine Learning Engineers Need to Know - <b>James Le</b>", "url": "https://jameskle.com/writes/machine-learning", "isFamilyFriendly": true, "displayUrl": "https://jameskle.com/writes/machine-learning", "snippet": "<b>Least</b> <b>squares</b> is a method for performing linear <b>regression</b>. You can think of linear <b>regression</b> as the task of <b>fitting</b> <b>a straight</b> <b>line</b> <b>through</b> a set <b>of points</b>. There are multiple possible strategies to do this, and \u201cordinary <b>least</b> <b>squares</b>\u201d strategy go <b>like</b> this \u2014 You can draw a <b>line</b>, and then for each of the data <b>points</b>, measure the vertical distance between the point and the <b>line</b>, and add these up; the fitted <b>line</b> would be the one where this sum of distances is as small as possible.", "dateLastCrawled": "2022-01-28T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Least Squares Regression for Quadratic Curve Fitting</b> - <b>CodeProject</b>", "url": "https://www.codeproject.com/Articles/63170/Least-Squares-Regression-for-Quadratic-Curve-Fitti", "isFamilyFriendly": true, "displayUrl": "https://<b>www.codeproject.com</b>/Articles/63170/<b>Least-Squares-Regression-for-Quadratic</b>...", "snippet": "for i = 1 to n. We want the values of a, b, and c that minimise the sum of <b>squares</b> of the deviations of y i from a*x i ^2 + bx i + c. Such values will give the best-<b>fitting</b> quadratic equation. Let the sum of the <b>squares</b> of the deviations be: Copy Code. n F (a,b,c) = SUM (a*xi^2 + bxi + c - yi)^2. i=1 dF/da = SUM 2* (a*xi^2+b*xi+c-yi)*xi^2 = 0 ...", "dateLastCrawled": "2022-01-25T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Chapter 7 <b>Least</b> <b>Squares</b> Estimation", "url": "http://igpphome.ucsd.edu/~cathy/Classes/SIO223A/sio223a.chap7.pdf", "isFamilyFriendly": true, "displayUrl": "igpphome.ucsd.edu/~cathy/Classes/SIO223A/sio223a.chap7.pdf", "snippet": "7-2 <b>Least</b> <b>Squares</b> Estimation Version 1.3 Solving for the \u03b2\u02c6 i yields the <b>least</b> <b>squares</b> parameter estimates: \u03b2\u02c6 0 = P x2 i P y i\u2212 P x P x y n P x2 i \u2212 ( P x i)2 \u03b2\u02c6 1 = n P x iy \u2212 x y n P x 2 i \u2212 ( P x i) (5) where the P \u2019s are implicitly taken to be from i = 1 to n in each case.", "dateLastCrawled": "2022-02-02T22:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>10 Algorithms Machine Learning Engineers Need</b> to Know | by ... - Gab41", "url": "https://gab41.lab41.org/the-10-algorithms-machine-learning-engineers-need-to-know-f4bb63f5b2fa", "isFamilyFriendly": true, "displayUrl": "https://gab41.lab41.org/the-<b>10-algorithms-machine-learning-engineers-need</b>-to-know-f4bb...", "snippet": "<b>Least</b> <b>squares</b> is a method for performing linear <b>regression</b>. You can think of linear <b>regression</b> as the task of <b>fitting</b> <b>a straight</b> <b>line</b> <b>through</b> a set <b>of points</b>. There are multiple possible strategies to do this, and \u201cordinary <b>least</b> <b>squares</b>\u201d strategy go <b>like</b> this \u2014 You can draw a <b>line</b>, and then for each of the data <b>points</b>, measure the vertical distance between the point and the <b>line</b>, and add these up; the fitted <b>line</b> would be the one where this sum of distances is as small as possible.", "dateLastCrawled": "2022-01-31T03:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning Algorithms</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/ie/192600668/machine-learning-algorithms-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/ie/192600668/<b>machine-learning-algorithms</b>-flash-cards", "snippet": "<b>Least</b> <b>squares</b> is a method for performing linear <b>regression</b>. You can think of linear <b>regression</b> as the task of <b>fitting</b> <b>a straight</b> <b>line</b> <b>through</b> a set <b>of points</b>. There are multiple possible strategies to do this, and &quot;ordinary <b>least</b> <b>squares</b>&quot; strategy go <b>like</b> this \u2014 You can draw a <b>line</b>, and then for each of the data <b>points</b>, measure the vertical distance between the point and the <b>line</b>, and add these up; the fitted <b>line</b> would be the one where this sum of distances is as small as possible.", "dateLastCrawled": "2020-06-19T04:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "algorithm - 3D <b>Least Squares</b> Plane - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/1400213/3d-least-squares-plane", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/1400213", "snippet": "Note that this is the &quot;ordinary <b>least squares</b>&quot; fit, which is appropriate only when z is expected to be a linear function of x and y. If you are looking more generally for a &quot;best fit plane&quot; in 3-space, you may want to learn about &quot;geometric&quot; <b>least squares</b>. Note also that this will fail if your <b>points</b> are in a <b>line</b>, as your example <b>points</b> are.", "dateLastCrawled": "2022-01-24T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Statistics for Applications Exam 3 Solution", "url": "https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-spring-2015/exams/MIT18_443S15_Exam3_Sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/mathematics/18-443-statistics-for-applications-spring-2015/...", "snippet": "<b>Regression</b> <b>Through</b> the Origin For bivariate data on n cases: {(x. i,y. i),i = 1, 2,...,n}, consider the linear model with zero intercept: Y. i = \u03b2x. i + E. i,i = 1, 2,...,n. where E. i. are 2independent and identically distribution N(0,\u03c3) random variables with \ufb01xed, but unknown variance \u03c3. 2 &gt; 0. When x i = 0, then E[Y i | x i,\u03b2] = 0. (a). Solve for the <b>least</b>-<b>squares</b> <b>line</b> \u2013 Y\u02c6 = \u03b2x.\u02c6 (b). Find the distribution of. \u03b2,\u02c6. the slope of the <b>least</b> <b>squares</b> <b>line</b>. (c). What is the ...", "dateLastCrawled": "2022-02-02T02:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>regression</b> - <b>Fitting</b> <b>a straight</b> <b>line</b>: Total <b>Least</b> <b>Squares</b> or Ordinary ...", "url": "https://stats.stackexchange.com/questions/240142/fitting-a-straight-line-total-least-squares-or-ordinary-least-squares", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/240142/<b>fitting</b>-<b>a-straight</b>-<b>line</b>-total-<b>least</b>...", "snippet": "I want to fit <b>a straight</b> <b>line</b> <b>through</b> a scatter plot of two timeseries to understand the influence sea surface temperatures (x-axis) have on land temperature over a particular region (y-axis). I have calculated the correlation coefficient which isn&#39;t particularly strong (0.16), but I also want to fit <b>a straight</b> <b>line</b> <b>through</b> this data, which is the part I&#39;m not sure about. For TLS (Total <b>Least</b> <b>Squares</b>) I have used", "dateLastCrawled": "2022-02-01T10:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Basic concepts used in AI. Traditional Programming: Data and\u2026 | by ...", "url": "https://renjithnano.medium.com/basic-concepts-used-in-ai-4027720e9503", "isFamilyFriendly": true, "displayUrl": "https://renjithnano.medium.com/basic-concepts-used-in-ai-4027720e9503", "snippet": "Ordinary <b>Least</b> <b>Squares</b> <b>Regression</b>: Linear <b>regression</b> is the task of <b>fitting</b> <b>a straight</b> <b>line</b> <b>through</b> a set <b>of points</b>. There are multiple possible strategies to do this, and \u201cordinary <b>least</b> <b>squares</b>\u201d strategy go like this \u2014 You can draw a <b>line</b>, and then for each of the data <b>points</b>, measure the vertical distance between the point and the <b>line</b>, and add these up; the fitted <b>line</b> would be the one where this sum of distances is as small as possible.Linear refers the kind of model you are using ...", "dateLastCrawled": "2021-12-27T14:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The 10 Algorithms Machine Learning Engineers Need to Know - <b>James Le</b>", "url": "https://jameskle.com/writes/machine-learning", "isFamilyFriendly": true, "displayUrl": "https://jameskle.com/writes/machine-learning", "snippet": "<b>Least</b> <b>squares</b> is a method for performing linear <b>regression</b>. You can think of linear <b>regression</b> as the task of <b>fitting</b> <b>a straight</b> <b>line</b> <b>through</b> a set <b>of points</b>. There are multiple possible strategies to do this, and \u201cordinary <b>least</b> <b>squares</b>\u201d strategy go like this \u2014 You can draw a <b>line</b>, and then for each of the data <b>points</b>, measure the vertical distance between the point and the <b>line</b>, and add these up; the fitted <b>line</b> would be the one where this sum of distances is as small as possible.", "dateLastCrawled": "2022-01-28T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machine Learning Algorithms That Engineers Need To Know - Uncodemy", "url": "https://www.uncodemy.com/technology/machine-learning-algorithms-that-engineers-need-to-know/", "isFamilyFriendly": true, "displayUrl": "https://www.uncodemy.com/technology/machine-learning-algorithms-that-engineers-need-to...", "snippet": "Customary <b>Least</b> <b>Squares</b> <b>Regression</b>: If you know measurements, you likely have known about direct relapse previously. The <b>least</b> <b>squares</b> is a strategy for performing <b>straight</b> relapse. You can consider direct relapse the assignment of <b>fitting</b> <b>a straight</b> <b>line</b> <b>through</b> <b>a bunch</b> of focuses. There are various potential methodologies to do this, and the \u201ccommon <b>least</b> <b>squares</b>\u201d technique goes like this \u2014 You can define a boundary, and afterward for every one of the information focuses, measure the ...", "dateLastCrawled": "2022-02-02T23:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Interesting World of Non-Linear Regressions | by Diego Manfre ...", "url": "https://towardsdatascience.com/the-interesting-world-of-non-linear-regressions-eb0c405fdc97", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-interesting-world-of-non-<b>line</b>ar-<b>regressions</b>-eb0c405...", "snippet": "But it keeps getting harder every time I add more <b>points</b> or when the curve I am looking for differs from <b>a straight</b> <b>line</b>. In this case, a curve <b>fitting</b> process can solve all my problems. I admit it is exciting to enter <b>a bunch</b> <b>of points</b> and find a curve that matches the trend \u201cperfectly\u201d. But how does this work? Why <b>fitting</b> a <b>line</b> is not the same as <b>fitting</b> a strange-shaped curve. Everyone is familiar with linear <b>least</b> <b>squares</b> but, what happens when the expression we are trying to match ...", "dateLastCrawled": "2022-01-29T16:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>10 Algorithms Machine Learning Engineers Need</b> to Know | by ... - Gab41", "url": "https://gab41.lab41.org/the-10-algorithms-machine-learning-engineers-need-to-know-f4bb63f5b2fa", "isFamilyFriendly": true, "displayUrl": "https://gab41.lab41.org/the-<b>10-algorithms-machine-learning-engineers-need</b>-to-know-f4bb...", "snippet": "<b>Least</b> <b>squares</b> is a method for performing linear <b>regression</b>. You can think of linear <b>regression</b> as the task of <b>fitting</b> <b>a straight</b> <b>line</b> <b>through</b> a set <b>of points</b>. There are multiple possible strategies to do this, and \u201cordinary <b>least</b> <b>squares</b>\u201d strategy go like this \u2014 You can draw a <b>line</b>, and then for each of the data <b>points</b>, measure the vertical distance between the point and the <b>line</b>, and add these up; the fitted <b>line</b> would be the one where this sum of distances is as small as possible.", "dateLastCrawled": "2022-01-31T03:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Comparing Quadratic and Linear Regression</b>?", "url": "https://www.researchgate.net/post/Comparing_Quadratic_and_Linear_Regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Comparing_Quadratic_and_Linear_Regression</b>", "snippet": "Finally, note that although a polynomial linear <b>regression</b> can be made to fit better than <b>a straight</b> <b>line</b> linear <b>regression</b>, that may constitute overfitting to the data at hand, which will not ...", "dateLastCrawled": "2022-02-03T05:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning Algorithms</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/ie/192600668/machine-learning-algorithms-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/ie/192600668/<b>machine-learning-algorithms</b>-flash-cards", "snippet": "<b>Least</b> <b>squares</b> is a method for performing linear <b>regression</b>. You can think of linear <b>regression</b> as the task of <b>fitting</b> <b>a straight</b> <b>line</b> <b>through</b> a set <b>of points</b>. There are multiple possible strategies to do this, and &quot;ordinary <b>least</b> <b>squares</b>&quot; strategy go like this \u2014 You can draw a <b>line</b>, and then for each of the data <b>points</b>, measure the vertical distance between the point and the <b>line</b>, and add these up; the fitted <b>line</b> would be the one where this sum of distances is as small as possible.", "dateLastCrawled": "2020-06-19T04:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Linear Regression in Python</b> using numpy + polyfit (with code base)", "url": "https://data36.com/linear-regression-in-python-numpy-polyfit/", "isFamilyFriendly": true, "displayUrl": "https://data36.com/<b>linear-regression-in-python</b>-numpy-polyfit", "snippet": "If you put all the x\u2013y value pairs on a graph, you\u2019ll get <b>a straight</b> <b>line</b>:. The relationship between x and y is linear.. Using the equation of this specific <b>line</b> (y = 2 * x + 5), if you change x by 1, y will always change by 2.And it doesn\u2019t matter what a and b values you use, your graph will always show the same characteristics: it will always be <b>a straight</b> <b>line</b>, only its position and slope change. It also means that x and y will always be in linear relationship.. In the linear ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Statistics Chapter 3 Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/55872170/statistics-chapter-3-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/55872170/statistics-chapter-3-flash-cards", "snippet": "The <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> for any data set passes <b>through</b> the point (x bar, y bar) Distance and standard deviations For an increase of one standard deviation (sx) in the value of the explanatory variable x, the <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> predicts an increase of r standard deviations (rxy) in the response variable y", "dateLastCrawled": "2021-12-20T16:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>The Method of Least Squares</b> | Aleksandr Hovhannisyan", "url": "https://www.aleksandrhovhannisyan.com/blog/the-method-of-least-squares/", "isFamilyFriendly": true, "displayUrl": "https://www.aleksandrhovhannisyan.com/blog/<b>the-method-of-least-squares</b>", "snippet": "But clearly, we <b>can</b> draw a best-fit <b>line</b> that at <b>least</b> gets as close to all of the <b>points</b> as possible: Spoiler : The solution is y = 0.59459459 x + 1.2972973 y = 0.59459459x + 1.2972973 y = 0 . 5 9 4 5 9 4 5 9 x + 1 . 2 9 7 2 9 7 3 .", "dateLastCrawled": "2022-01-04T00:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "7 Classical Assumptions of Ordinary <b>Least</b> <b>Squares</b> (OLS) Linear <b>Regression</b>", "url": "https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/<b>regression</b>/ols-<b>line</b>ar-<b>regression</b>-assumptions", "snippet": "Ordinary <b>Least</b> <b>Squares</b> is the most common estimation method for linear models\u2014and that\u2019s true for a good reason.As long as your model satisfies the OLS assumptions for linear <b>regression</b>, you <b>can</b> rest easy knowing that you\u2019re getting the best possible estimates.. <b>Regression</b> is a powerful analysis that <b>can</b> analyze multiple variables simultaneously to answer complex research questions. However, if you don\u2019t satisfy the OLS assumptions, you might not be able to trust the results.", "dateLastCrawled": "2022-02-03T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Ten Algorithms Machine Learning Engineers Need to Know", "url": "https://resources.experfy.com/bigdata-cloud/the-ten-algorithms-machine-learning-engineers-need-to-know/", "isFamilyFriendly": true, "displayUrl": "https://resources.experfy.com/bigdata-cloud/the-ten-algorithms-machine-learning...", "snippet": "<b>Least</b> <b>squares</b> is a method for performing linear <b>regression</b>. You <b>can</b> think of linear <b>regression</b> as the task of <b>fitting</b> <b>a straight</b> <b>line</b> <b>through</b> a set <b>of points</b>. There are multiple possible strategies to do this, and \u201cordinary <b>least</b> <b>squares</b>\u201d strategy go like this \u2014 You <b>can</b> draw a <b>line</b>, and then for each of the data <b>points</b>, measure the vertical distance between the point and the <b>line</b>, and add these up; the fitted <b>line</b> would be the one where this sum of distances is as small as possible.", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Derivation</b> of the formula for Ordinary <b>Least Squares</b> Linear <b>Regression</b> ...", "url": "https://math.stackexchange.com/questions/131590/derivation-of-the-formula-for-ordinary-least-squares-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/131590", "snippet": "Galton realized that the median weights of daughter seeds from a particular size of mother seed approximately described <b>a straight</b> <b>line</b> with positive slope less than 1.0: &quot;Thus he naturally reached <b>a straight</b> <b>regression</b> <b>line</b>, and the constant variability for all arrays of one character for a given character of a second. It was, perhaps, best ...", "dateLastCrawled": "2022-01-27T14:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lecture 9: Four Ways to Solve <b>Least</b> <b>Squares</b> Problems | Video Lectures ...", "url": "https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/video-lectures/lecture-9-four-ways-to-solve-least-squares-problems/", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/.../video-lectures/lecture-9-four-ways-to-solve-<b>least</b>-<b>squares</b>-problems", "snippet": "That may be the case everybody knows is, where this equation is like expressing <b>a straight</b> <b>line</b> going <b>through</b> the data <b>points</b>. So the famous example of <b>least</b> <b>squares</b> is fit <b>a straight</b> <b>line</b> to the b&#39;s, to b1, b2. We&#39;ve got m measurements. We&#39;ve got m measurements. The physics or the mechanics of the problem is pretty well linear. But of course ...", "dateLastCrawled": "2022-01-29T12:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Machine Learning Algorithms That Engineers Need To Know - Uncodemy", "url": "https://www.uncodemy.com/technology/machine-learning-algorithms-that-engineers-need-to-know/", "isFamilyFriendly": true, "displayUrl": "https://www.uncodemy.com/technology/machine-learning-algorithms-that-engineers-need-to...", "snippet": "Customary <b>Least</b> <b>Squares</b> <b>Regression</b>: If you know measurements, you likely have known about direct relapse previously. The <b>least</b> <b>squares</b> is a strategy for performing <b>straight</b> relapse. You <b>can</b> consider direct relapse the assignment of <b>fitting</b> <b>a straight</b> <b>line</b> <b>through</b> <b>a bunch</b> of focuses. There are various potential methodologies to do this, and the \u201ccommon <b>least</b> <b>squares</b>\u201d technique goes like this \u2014 You <b>can</b> define a boundary, and afterward for every one of the information focuses, measure the ...", "dateLastCrawled": "2022-02-02T23:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "statistics - <b>Straight line through data by eye - least</b> <b>squares</b> ...", "url": "https://math.stackexchange.com/questions/204517/straight-line-through-data-by-eye-least-squares", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/204517", "snippet": "<b>Straight line through data by eye - least</b> <b>squares</b>? [closed] Ask Question Asked 9 years, 3 months ago. Active 9 years, 3 months ago. Viewed 278 times 4 1 $\\begingroup$ Closed. This question is off-topic. It is not currently accepting answers. Want to improve this question? Update the question so it&#39;s on-topic for Mathematics Stack Exchange. Closed 9 years ago. Improve this question I heard an interesting fact a while ago about how people draw a <b>line</b> <b>through</b> a cloud <b>of points</b> on a scatter plot ...", "dateLastCrawled": "2022-01-15T02:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "intercept - <b>Regression through</b> the origin - Cross Validated", "url": "https://stats.stackexchange.com/questions/54794/regression-through-the-origin", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/54794/<b>regression-through</b>-the-origin", "snippet": "I <b>thought</b> it might be instructive to add the derivation of the WLS <b>line</b> <b>through</b> the origin and then my &quot;average of slopes&quot; and gungs OLS are special cases: The model is y i = \u03b2 x i + \u03b5 i, where Var ( \u03b5 i) = w i \u03c3 2. We want to minimize S = \u2211 i w i ( y i \u2212 \u03b2 x i) 2. \u2202 S \u2202 \u03b2 = \u2212 \u2211 i 2 x i. w i ( y i \u2212 \u03b2 x i)", "dateLastCrawled": "2022-01-13T14:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Curve <b>Fitting using Linear and Nonlinear Regression</b> - Statistics By Jim", "url": "https://statisticsbyjim.com/regression/curve-fitting-linear-nonlinear-regression/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/<b>regression</b>/curve-<b>fitting</b>-<b>line</b>ar-non<b>line</b>ar-<b>regression</b>", "snippet": "It\u2019s interesting to me that I <b>can</b> use Linear <b>Regression</b> for curve-<b>fitting</b>. I <b>thought</b> I need to learn Nonlinear <b>Regression</b>. Reply. Jim Frost says. July 12, 2021 at 5:27 pm . Hi Yujin, The naming <b>can</b> be confusing! Nonlinear <b>regression</b> <b>can</b> fit a wider variety of curve shapes but often linear <b>regression</b> <b>can</b> fit your curve. I always recommend starting with linear <b>regression</b> because it\u2019s easier and see if that works for your data. Reply. Julian says. May 20, 2021 at 5:57 am. Hello Jim! very ...", "dateLastCrawled": "2022-02-01T19:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "73 questions with answers in <b>NON-LINEAR REGRESSION ANALYSIS</b> ...", "url": "https://www.researchgate.net/topic/Non-Linear-Regression-Analysis", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/<b>Non-Linear-Regression-Analysis</b>", "snippet": "In Yt = In Yo + t In (1+r). Here, Yt is the variable for which growth is calculated, r is the compound growth rate and In is the natural. logarithm. Now, let In Yo = b1and In (1+r) =b2. Therefore ...", "dateLastCrawled": "2022-01-30T13:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "7 Classical Assumptions of Ordinary <b>Least</b> <b>Squares</b> (OLS) Linear <b>Regression</b>", "url": "https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/<b>regression</b>/ols-<b>line</b>ar-<b>regression</b>-assumptions", "snippet": "Ordinary <b>Least</b> <b>Squares</b> is the most common estimation method for linear models\u2014and that\u2019s true for a good reason.As long as your model satisfies the OLS assumptions for linear <b>regression</b>, you <b>can</b> rest easy knowing that you\u2019re getting the best possible estimates.. <b>Regression</b> is a powerful analysis that <b>can</b> analyze multiple variables simultaneously to answer complex research questions. However, if you don\u2019t satisfy the OLS assumptions, you might not be able to trust the results.", "dateLastCrawled": "2022-02-03T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Chapter 7 <b>Least</b> <b>Squares</b> Estimation", "url": "http://igpphome.ucsd.edu/~cathy/Classes/SIO223A/sio223a.chap7.pdf", "isFamilyFriendly": true, "displayUrl": "igpphome.ucsd.edu/~cathy/Classes/SIO223A/sio223a.chap7.pdf", "snippet": "<b>least</b> <b>squares</b> estimation problem <b>can</b> be solved in closed form, and it is relatively straightforward to derive the statistical properties for the resulting parameter estimates. One very simple example which we will treat in some detail in order to illustrate the more general problem is that of \ufb01tting <b>a straight</b> <b>line</b> to a collection of pairs of observations (x i,y i) where i = 1,2,...,n. We suppose that a reasonable model is of the form y = \u03b2 0 + \u03b2 1x, (1) and weneed a mechanism for ...", "dateLastCrawled": "2022-02-02T22:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Least</b> <b>squares</b> is a method of <b>fitting</b> a <b>regression</b> <b>line</b> which is robust ...", "url": "https://www.quora.com/Least-squares-is-a-method-of-fitting-a-regression-line-which-is-robust-i-e-safe-from-outliers-True-or-False", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Least</b>-<b>squares</b>-is-a-method-of-<b>fitting</b>-a-<b>regression</b>-<b>line</b>-which-is...", "snippet": "Answer (1 of 2): This is false. So it is \u201c<b>least</b> <b>squares</b>\u201d - the square of the residual is what you are looking to minimise. Consider your point with the highest residuals and move it some small amount. Consider how your <b>line</b> of best fit will move. Consider for a given small change how much your g...", "dateLastCrawled": "2022-01-08T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Comparing Quadratic and Linear Regression</b>?", "url": "https://www.researchgate.net/post/Comparing_Quadratic_and_Linear_Regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Comparing_Quadratic_and_Linear_Regression</b>", "snippet": "Finally, note that although a polynomial linear <b>regression</b> <b>can</b> be made to fit better than <b>a straight</b> <b>line</b> linear <b>regression</b>, that may constitute overfitting to the data at hand, which will not ...", "dateLastCrawled": "2022-02-03T05:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding Linear <b>Regression</b> vs. <b>Multiple Regression</b>", "url": "https://www.investopedia.com/ask/answers/060315/what-difference-between-linear-regression-and-multiple-regression.asp", "isFamilyFriendly": true, "displayUrl": "https://<b>www.investopedia.com</b>/ask/answers/060315/what-difference-between-<b>line</b>ar...", "snippet": "The <b>least</b>-<b>squares</b> criterion is a method of measuring the accuracy of a <b>line</b> in depicting the data that was used to generate it. That is, the formula determines the <b>line</b> of best fit. That is, the ...", "dateLastCrawled": "2022-02-03T02:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Choosing the Correct Type of <b>Regression</b> Analysis - Statistics By Jim", "url": "https://statisticsbyjim.com/regression/choosing-regression-analysis/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/<b>regression</b>/choosing-<b>regression</b>-analysis", "snippet": "OLS produces the fitted <b>line</b> that minimizes the sum of the squared differences between the data <b>points</b> and the <b>line</b>. Linear <b>regression</b>, also known as ordinary <b>least</b> <b>squares</b> and linear <b>least</b> <b>squares</b>, is the real workhorse of the <b>regression</b> world.Use linear <b>regression</b> to understand the mean change in a dependent variable given a one-unit change in each independent variable.", "dateLastCrawled": "2022-02-03T01:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "correlation - What is the difference between linear <b>regression</b> on y ...", "url": "https://stats.stackexchange.com/questions/22718/what-is-the-difference-between-linear-regression-on-y-with-x-and-x-with-y", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/22718", "snippet": "(Updated much later) Here&#39;s another way to think about this that approaches the topic <b>through</b> the formulas instead of visually: The formula for the slope of a simple <b>regression</b> <b>line</b> is a consequence of the loss function that has been adopted. If you are using the standard Ordinary <b>Least</b> <b>Squares</b> loss function (noted above), you <b>can</b> derive the formula for the slope that you see in every intro textbook. This formula <b>can</b> be presented in various forms; one of which I call the &#39;intuitive&#39; formula ...", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Advantages and Disadvantages of Linear Regression</b>", "url": "https://iq.opengenus.org/advantages-and-disadvantages-of-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://iq.opengenus.org/<b>advantages-and-disadvantages-of-linear-regression</b>", "snippet": "Linear <b>Regression</b> is a very simple algorithm that <b>can</b> be implemented very easily to give satisfactory results.Furthermore, these models <b>can</b> be trained easily and efficiently even on systems with relatively low computational power when <b>compared</b> to other complex algorithms.Linear <b>regression</b> has a considerably lower time complexity when <b>compared</b> to some of the other machine learning algorithms.The mathematical equations of Linear <b>regression</b> are also fairly easy to understand and interpret.Hence ...", "dateLastCrawled": "2022-02-02T08:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Integrity of the linear regression</b> | Request PDF", "url": "https://www.researchgate.net/publication/261587123_Integrity_of_the_linear_regression", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261587123_<b>Integrity_of_the_linear_regression</b>", "snippet": "The <b>least</b> <b>squares</b> linear <b>regression</b> used for two hundred years is an incomplete solution for <b>fitting</b> a <b>line</b> to a set <b>of points</b>. In most cases it will produce two separate and unrelated solutions ...", "dateLastCrawled": "2021-10-01T16:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Statistics Chapter 3 Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/55872170/statistics-chapter-3-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/55872170/statistics-chapter-3-flash-cards", "snippet": "The <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> for any data set passes <b>through</b> the point (x bar, y bar) Distance and standard deviations For an increase of one standard deviation (sx) in the value of the explanatory variable x, the <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> predicts an increase of r standard deviations (rxy) in the response variable y", "dateLastCrawled": "2021-12-20T16:50:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS 189/289A: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189s21/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189s21", "snippet": "LDA vs. logistic <b>regression</b>: advantages and disadvantages. ROC curves. Weighted <b>least</b>-<b>squares</b> <b>regression</b>. <b>Least</b>-<b>squares</b> polynomial <b>regression</b>. Read ISL, Sections 4.4.3, 7.1, 9.3.3; ESL, Section 4.4.1. Optional: here is a fine short discussion of ROC curves\u2014but skip the incoherent question at the top and jump straight to the answer.", "dateLastCrawled": "2022-01-31T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Alternative data mining/<b>machine</b> <b>learning</b> methods for the analytical ...", "url": "https://pubmed.ncbi.nlm.nih.gov/31229078/", "isFamilyFriendly": true, "displayUrl": "https://<b>pubmed</b>.ncbi.nlm.nih.gov/31229078", "snippet": "The most widely used methods are principal component analysis (PCA), partial <b>least</b> <b>squares</b>-discriminant analysis (PLS-DA), soft independent modelling by class <b>analogy</b> (SIMCA), k-nearest neighbours (kNN), parallel factor analysis (PARAFAC), and multivariate curve resolution-alternating <b>least</b> <b>squares</b> (MCR-ALS). Nevertheless, there are alternative data treatment methods, such as support vector <b>machine</b> (SVM), classification and <b>regression</b> tree (CART) and random forest (RF), that show a great ...", "dateLastCrawled": "2021-03-23T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>CS 189/289A</b>: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189", "snippet": "<b>regression</b>: <b>least</b>-<b>squares</b> linear <b>regression</b>, logistic <b>regression</b>, polynomial <b>regression</b>, ridge <b>regression</b>, Lasso; density estimation: maximum likelihood estimation (MLE); dimensionality reduction: principal components analysis (PCA), random projection; and clustering: k-means clustering, hierarchical clustering, spectral graph clustering. Useful Links. Access the <b>CS 189/289A</b> Piazza discussion group. If you want an instructional account, you can get one online. Go to the same link if you ...", "dateLastCrawled": "2022-02-02T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Neurath&#39;s Speedboat</b>: <b>Least squares as springs</b>", "url": "https://joshualoftus.com/posts/2020-11-23-least-squares-as-springs/", "isFamilyFriendly": true, "displayUrl": "https://joshualoftus.com/posts/2020-11-23-<b>least-squares-as-springs</b>", "snippet": "(This is also called total <b>least</b> <b>squares</b> or a special case of Deming <b>regression</b>.) Model complexity/elasticity: <b>machine</b> <b>learning</b> or AI. We can keep building on this <b>analogy</b> by using it to understand more complex modeling methods with another very simple idea: elasticity of the model object itself. Instead of a rigid body like a line (or ...", "dateLastCrawled": "2022-02-03T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "LSEbA: <b>least squares regression and estimation by analogy</b> in a semi ...", "url": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "snippet": "In this study, we indicatively applied the ordinary <b>least</b> <b>squares</b> <b>regression</b> and the estimation by <b>analogy</b> technique for the computation of the parametric and non-parametric part, respectively. However, there are lots of other well-known methods that can substitute the abovementioned methods and can be used for evaluation of these components. For example, practitioners may use a robust <b>regression</b> in the computation of the parametric portion of the proposed model in order to have a model less ...", "dateLastCrawled": "2021-12-03T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "A difficult <b>regression</b> parameter estimation problem is posed when the data sample is hypothesized to have been generated by more than a single <b>regression</b> model. To find the best-fitting number and ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>Learning</b> Notes - LightQuanta", "url": "https://sanghan.xyz/blog/2015/03/machine_learning_notes/", "isFamilyFriendly": true, "displayUrl": "https://sanghan.xyz/blog/2015/03/<b>machine</b>_<b>learning</b>_notes", "snippet": "2013 Sang Han A collection of notes and useful articles on numerical algorithms. Definitions Markov Chains: A special type of stochastic process. The standard definition of a stochastic process is an ordered collection of random variables: Self-Organized Criticality (SOC): \u201cSelf-Organized\u201d means that from any initial condition, the system tends to move toward a critical state, and stay there, without external control. A system is \u201ccritical\u201d if it is in transition between two phases.", "dateLastCrawled": "2021-04-03T07:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Do we actually take random line in first step of ...", "url": "https://stats.stackexchange.com/questions/556085/do-we-actually-take-random-line-in-first-step-of-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/556085/do-we-actually-take-random-line-in...", "snippet": "As a simple <b>analogy</b> to show the difference between a closed form solution and an algorithm: if I were to give you a mathematical equation, ... (to which the exact solution to linear <b>least</b> <b>squares</b> <b>regression</b> is extremely sensitive). Share. Cite. Improve this answer. Follow answered Dec 17 &#39;21 at 12:54. Roger Vadim Roger Vadim. 1,314 6 6 silver badges 16 16 bronze badges $\\endgroup$ Add a comment | Your Answer Thanks for contributing an answer to Cross Validated! Please be sure to answer the ...", "dateLastCrawled": "2022-02-02T23:48:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bayesian <b>Learning</b> - Rebellion Research", "url": "https://www.rebellionresearch.com/bayesian-learning", "isFamilyFriendly": true, "displayUrl": "https://www.rebellionresearch.com/bayesian-<b>learning</b>", "snippet": "Linear Regression example of <b>machine learning Least Squares Regression can be thought of as</b> a very limited <b>learning</b> algorithm, where the training set consists of a number of x and y data pairs. The task would be trying to predict the y value, and the performance measure would be the sum of the squared differences between the predicted and actual y\u2019s.", "dateLastCrawled": "2022-01-19T02:15:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(least squares regression)  is like +(fitting a straight line through a bunch of points)", "+(least squares regression) is similar to +(fitting a straight line through a bunch of points)", "+(least squares regression) can be thought of as +(fitting a straight line through a bunch of points)", "+(least squares regression) can be compared to +(fitting a straight line through a bunch of points)", "machine learning +(least squares regression AND analogy)", "machine learning +(\"least squares regression is like\")", "machine learning +(\"least squares regression is similar\")", "machine learning +(\"just as least squares regression\")", "machine learning +(\"least squares regression can be thought of as\")", "machine learning +(\"least squares regression can be compared to\")"]}