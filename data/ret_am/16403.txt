{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sequence-to-Sequence</b> Models Can Directly Transcribe Foreign Speech ...", "url": "https://www.researchgate.net/publication/315667124_Sequence-to-Sequence_Models_Can_Directly_Transcribe_Foreign_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315667124_<b>Sequence-to-Sequence</b>_Models_Can...", "snippet": "In addition, we find that making use of the training data in both languages by multi-<b>task</b> training <b>sequence-to-sequence</b> speech translation and recognition models with a shared encoder network can ...", "dateLastCrawled": "2022-01-18T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Team03_Project Report.pdf - Big Data Analytics February 2020 <b>BILINGUAL</b> ...", "url": "https://www.coursehero.com/file/81191489/Team03-Project-Reportpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/81191489/Team03-Project-Reportpdf", "snippet": "The so-called Seq2Seq(<b>Sequence to Sequence</b>) is a method that can generate another sequence by a specific method based on a given sequence.It was firstly proposed in 2014, having first, the two articles describes its main idea, namely Google Brain team &quot;<b>Sequence to Sequence</b> Learning with Neural Networks&quot; and Yoshua Bengio team &quot;Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translation.", "dateLastCrawled": "2022-01-23T20:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What we need to learn if we want to do and not just talk", "url": "https://aclanthology.org/N18-3004.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/N18-3004.pdf", "snippet": "In <b>task</b>-oriented dialog, agents need to gener-ate both uent natural language responses and correct external actions <b>like</b> database queries and updates. We show that methods that achieve state of the art performance on syn-theticdatasets,performpoorlyinrealworlddi-alog tasks. We propose a hybrid model, where nearest neighbor is used to generate uent re-sponses and <b>Sequence-to-Sequence</b> (Seq2Seq) type models ensure dialogue coherency and generate accurate external actions. The hybrid model on an ...", "dateLastCrawled": "2022-01-30T08:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "The general <b>task</b> of <b>sequence to sequence</b> modelling is at the heart of some of the most difficult tasks in NLP, including: Text summarization; Text simplification; Question answering; Chatbots ; Machine translation; This s o rt of technology is right out of science fiction. With such a wide range of exciting applications, it\u2019s easy to see why <b>sequence to sequence</b> modeling is more popular than ever. What\u2019s not easy is actually evaluating these systems. Unfortunately for folks who are just ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Progress in Neural NLP: Modeling, Learning, and Reasoning", "url": "https://www.engineering.org.cn/en/10.1016/j.eng.2019.12.014", "isFamilyFriendly": true, "displayUrl": "https://www.engineering.org.cn/en/10.1016/j.eng.2019.12.014", "snippet": "Many NLP tasks can be formulated as a <b>sequence-to-sequence</b> <b>task</b>, such as MT (i.e., given the source language word sequence, generate the target language word sequence), QA (i.e., given the word sequence of a question, generate the word sequence of an answer), and dialog (i.e., given the word sequence of user input, generate the word sequence of response). 2.2.2. Encoder\u2013decoder framework. Ref. [11] proposed an encoder\u2013decoder framework for <b>sequence-to-sequence</b> modeling. As shown in Fig ...", "dateLastCrawled": "2022-01-31T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Progress in Neural NLP: Modeling, Learning, and Reasoning", "url": "https://www.sciencedirect.com/science/article/pii/S2095809919304928", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2095809919304928", "snippet": "Many NLP tasks can be formulated as a <b>sequence-to-sequence</b> <b>task</b>, such as MT (i.e., given the source language word sequence, generate the target language word sequence), QA (i.e., given the word sequence of a question, generate the word sequence of an answer), and dialog (i.e., given the word sequence of user input, generate the word sequence of response). 2.2.2. Encoder\u2013decoder framework. Ref. proposed an encoder\u2013decoder framework for <b>sequence-to-sequence</b> modeling. As shown in Fig. 5 ...", "dateLastCrawled": "2022-01-30T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Multi-<b>Task</b> Learning for Speaker-Role Adaptation in Neural Conversation ...", "url": "https://aclanthology.org/I17-1061.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/I17-1061.pdf", "snippet": "the performance of <b>bilingual</b> machine translation in the form of multi-<b>task</b> learning. In our models, we share the decoder parameters of a S EQ 2S EQ model and autoencoder to incorporate textual in-formation through multi-<b>task</b> learning. 3 Background 3.1 <b>Task</b> denition The <b>task</b> of response generation is to generate a response given a context. In ...", "dateLastCrawled": "2021-12-30T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Frontiers of Natural Language Processing</b> - SlideShare", "url": "https://www.slideshare.net/SebastianRuder/frontiers-of-natural-language-processing", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/SebastianRuder/<b>frontiers-of-natural-language-processing</b>", "snippet": "Timeline 2001 \u2022 Neural language models 2008 \u2022 Multi-<b>task</b> learning 2013 \u2022 Word embeddings 2013 \u2022 Neural networks for NLP 2014 \u2022 <b>Sequence-to-sequence</b> models 2015 \u2022 Attention 2015 \u2022 Memory-based networks 2018 \u2022 Pretrained language models 3 / 68 6. Timeline 2001 \u2022 Neural language models 2008 \u2022 Multi-<b>task</b> learning 2013 \u2022 Word embeddings 2013 \u2022 Neural networks for NLP 2014 \u2022 <b>Sequence-to-sequence</b> models 2015 \u2022 Attention 2015 \u2022 Memory-based networks 2018 \u2022 Pretrained ...", "dateLastCrawled": "2022-01-21T14:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Andrew-NG-Notes/andrewng-p-5-sequence-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence...", "snippet": "Name entity recognition (<b>sequence to sequence</b>): X: text sequence; Y: label sequence; Can be used by seach engines to index different type of words inside a text. All of these problems with different input and output (sequence or not) can be addressed as supervised learning with label data X, Y as the training set. Notation. In this section we will discuss the notations that we will use through the course. Motivating example: Named entity recognition example: X: &quot;Harry Potter and Hermoine ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - Kyubyong/nlp_tasks: Natural Language Processing Tasks and ...", "url": "https://github.com/Kyubyong/nlp_tasks", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kyubyong/nlp_<b>tasks</b>", "snippet": "One day, I felt <b>like</b> drawing a map of the NLP field where I earn a living. I&#39;m sure I&#39;m not the only <b>person</b> who wants to see at a glance which tasks are in NLP. I did my best to cover as many as possible tasks in NLP, but admittedly this is far from exhaustive purely due to my lack of knowledge. And selected references are biased towards recent deep learning accomplishments. I expect these serve as a starting point when you&#39;re about to dig into the <b>task</b>. I&#39;ll keep updating this repo myself ...", "dateLastCrawled": "2022-01-26T10:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Representation Learning in <b>Sequence to Sequence</b> Tasks: Multi ...", "url": "https://www.researchgate.net/publication/351709227_Representation_Learning_in_Sequence_to_Sequence_Tasks_Multi-filter_Gaussian_Mixture_Autoencoder", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/351709227_Representation_Learning_in_Sequence...", "snippet": "PDF | Heterogeneity of sentences exists in <b>sequence to sequence</b> tasks such as machine translation. Sentences with largely varied meanings or grammatical... | Find, read and cite all the research ...", "dateLastCrawled": "2021-12-08T19:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Progress in Neural NLP: Modeling, Learning, and Reasoning", "url": "https://www.sciencedirect.com/science/article/pii/S2095809919304928", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2095809919304928", "snippet": "<b>Sequence-to-sequence</b> modeling2.2.1. <b>Task</b> of <b>sequence-to-sequence</b> modeling. <b>Sequence-to-sequence</b> modeling attempts to generate one sequence with another sequence as input. Many NLP tasks can be formulated as a <b>sequence-to-sequence</b> <b>task</b>, such as MT (i.e., given the source language word sequence, generate the target language word sequence), QA (i ...", "dateLastCrawled": "2022-01-30T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Team03_Project Report.pdf - Big Data Analytics February 2020 <b>BILINGUAL</b> ...", "url": "https://www.coursehero.com/file/81191489/Team03-Project-Reportpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/81191489/Team03-Project-Reportpdf", "snippet": "The so-called Seq2Seq(<b>Sequence to Sequence</b>) is a method that can generate another sequence by a specific method based on a given sequence.It was firstly proposed in 2014, having first, the two articles describes its main idea, namely Google Brain team &quot;<b>Sequence to Sequence</b> Learning with Neural Networks&quot; and Yoshua Bengio team &quot;Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translation.", "dateLastCrawled": "2022-01-23T20:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "The general <b>task</b> of <b>sequence to sequence</b> modelling is at the heart of some of the most difficult tasks in NLP, including: Text summarization; Text simplification; Question answering; Chatbots ; Machine translation; This s o rt of technology is right out of science fiction. With such a wide range of exciting applications, it\u2019s easy to see why <b>sequence to sequence</b> modeling is more popular than ever. What\u2019s not easy is actually evaluating these systems. Unfortunately for folks who are just ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Progress in Neural NLP: Modeling, Learning, and Reasoning", "url": "https://www.engineering.org.cn/en/10.1016/j.eng.2019.12.014", "isFamilyFriendly": true, "displayUrl": "https://www.engineering.org.cn/en/10.1016/j.eng.2019.12.014", "snippet": "<b>Task</b> of <b>sequence-to-sequence</b> modeling. <b>Sequence-to-sequence</b> modeling attempts to generate one sequence with another sequence as input. Many NLP tasks can be formulated as a <b>sequence-to-sequence</b> <b>task</b>, such as MT (i.e., given the source language word sequence, generate the target language word sequence), QA (i.e., given the word sequence of a question, generate the word sequence of an answer), and dialog (i.e., given the word sequence of user input, generate the word sequence of response). 2.2 ...", "dateLastCrawled": "2022-01-31T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Multi-<b>Task</b> Learning for Speaker-Role Adaptation in Neural Conversation ...", "url": "https://luanyi.github.io/YiLuan_files/Multi_Task_Learning_for_Speaker_Role_Adaptation_in_Neural_Conversation_Models.pdf", "isFamilyFriendly": true, "displayUrl": "https://luanyi.github.io/YiLuan_files/Multi_<b>Task</b>_Learning_for_Speaker_Role_Adaptation...", "snippet": "the performance of <b>bilingual</b> machine translation in the form of multi-<b>task</b> learning. In our models, we share the decoder parameters of a SEQ2SEQ model and autoencoder to incorporate textual in-formation through multi-<b>task</b> learning. 3 Background 3.1 <b>Task</b> de\ufb01nition The <b>task</b> of response generation is to generate a response given a context. In ...", "dateLastCrawled": "2021-09-17T08:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Cross-Lingual, Multi-Speaker Text-To-Speech Synthesis Using Neural ...", "url": "https://www.isca-speech.org/archive/pdfs/interspeech_2019/chen19g_interspeech.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.isca-speech.org/archive/pdfs/interspeech_2019/chen19g_interspeech.pdf", "snippet": "convolutional <b>sequence-to-sequence</b> architecture and incorpo-rated a position-augmented attention to support more than 2400 speakers with LibriSpeech dataset. However, both of them can only synthesize speech for observed speakers during training stage. Some approaches introduce a separate deep network to encode the speaker characteristics and embed the information into spectrogram predictor or vocoder. VoiceLoop [6] proposed a novel memory buffer mechanism to \ufb01t new speakers which are not s", "dateLastCrawled": "2022-01-05T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Frontiers of Natural Language Processing</b> - SlideShare", "url": "https://www.slideshare.net/SebastianRuder/frontiers-of-natural-language-processing", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/SebastianRuder/<b>frontiers-of-natural-language-processing</b>", "snippet": "Timeline 2001 \u2022 Neural language models 2008 \u2022 Multi-<b>task</b> learning 2013 \u2022 Word embeddings 2013 \u2022 Neural networks for NLP 2014 \u2022 <b>Sequence-to-sequence</b> models 2015 \u2022 Attention 2015 \u2022 Memory-based networks 2018 \u2022 Pretrained language models 3 / 68 6. Timeline 2001 \u2022 Neural language models 2008 \u2022 Multi-<b>task</b> learning 2013 \u2022 Word embeddings 2013 \u2022 Neural networks for NLP 2014 \u2022 <b>Sequence-to-sequence</b> models 2015 \u2022 Attention 2015 \u2022 Memory-based networks 2018 \u2022 Pretrained ...", "dateLastCrawled": "2022-01-21T14:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Multi-<b>Task</b> Learning for Speaker-Role Adaptation in Neural Conversation ...", "url": "https://aclanthology.org/I17-1061.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/I17-1061.pdf", "snippet": "the performance of <b>bilingual</b> machine translation in the form of multi-<b>task</b> learning. In our models, we share the decoder parameters of a S EQ 2S EQ model and autoencoder to incorporate textual in-formation through multi-<b>task</b> learning. 3 Background 3.1 <b>Task</b> denition The <b>task</b> of response generation is to generate a response given a context. In ...", "dateLastCrawled": "2021-12-30T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is <b>there an online question generator tool</b>? - Quora", "url": "https://www.quora.com/Is-there-an-online-question-generator-tool", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-<b>there-an-online-question-generator-tool</b>", "snippet": "Answer: Yes, there is <b>an online question generator tool</b> known as CHRONON that solves all question paper related problems. This software has been used by many reputed schools, institutes, universities and coaching institutes as well. It is a tool used for creating different types of questions not ...", "dateLastCrawled": "2022-01-29T20:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sequence-to-Sequence</b> Models <b>Can</b> Directly Transcribe Foreign Speech ...", "url": "https://www.researchgate.net/publication/315667124_Sequence-to-Sequence_Models_Can_Directly_Transcribe_Foreign_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315667124_<b>Sequence-to-Sequence</b>_Models_<b>Can</b>...", "snippet": "In addition, we find that making use of the training data in both languages by multi-<b>task</b> training <b>sequence-to-sequence</b> speech translation and recognition models with a shared encoder network <b>can</b> ...", "dateLastCrawled": "2022-01-18T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) AIML and <b>Sequence-to-Sequence</b> Models to Build Artificial ...", "url": "https://www.researchgate.net/publication/332957827_AIML_and_Sequence-to-Sequence_Models_to_Build_Artificial_Intelligence_Chatbots_Insights_from_a_Comparative_Analysis", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/332957827_AIML_and_<b>Sequence-to-Sequence</b>...", "snippet": "2.2 <b>Sequence-to-Sequence</b> The <b>Sequence- to -sequence</b> model makes use of Recurrent Neural Networks (RNN) to take in a user\u2019s input sentence, and generates an output sentence [9].", "dateLastCrawled": "2022-01-17T20:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Direct speech-to-speech translation with a <b>sequence-to-sequence</b> model ...", "url": "https://deepai.org/publication/direct-speech-to-speech-translation-with-a-sequence-to-sequence-model", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/direct-speech-to-speech-translation-with-a-sequence-to...", "snippet": "The proposed model resembles recent <b>sequence-to-sequence</b> models for voice conversion, the <b>task</b> of recreating an utterance in another <b>person</b>\u2019s voice [22, 23, 24]. For example, [ 23 ] proposes an attention-based model to generate spectrograms in the target voice based on input features (spectrogram concatenated with ASR bottleneck features) from the source voice.", "dateLastCrawled": "2021-12-17T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Progress in Neural NLP: Modeling, Learning, and Reasoning", "url": "https://www.sciencedirect.com/science/article/pii/S2095809919304928", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2095809919304928", "snippet": "Many NLP tasks <b>can</b> be formulated as a <b>sequence-to-sequence</b> <b>task</b>, such as MT (i.e., given the source language word sequence, generate the target language word sequence), QA (i.e., given the word sequence of a question, generate the word sequence of an answer), and dialog (i.e., given the word sequence of user input, generate the word sequence of response). 2.2.2. Encoder\u2013decoder framework. Ref. proposed an encoder\u2013decoder framework for <b>sequence-to-sequence</b> modeling. As shown in Fig. 5 ...", "dateLastCrawled": "2022-01-30T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Progress in Neural NLP: Modeling, Learning, and Reasoning", "url": "https://www.engineering.org.cn/en/10.1016/j.eng.2019.12.014", "isFamilyFriendly": true, "displayUrl": "https://www.engineering.org.cn/en/10.1016/j.eng.2019.12.014", "snippet": "Many NLP tasks <b>can</b> be formulated as a <b>sequence-to-sequence</b> <b>task</b>, such as MT (i.e., given the source language word sequence, generate the target language word sequence), QA (i.e., given the word sequence of a question, generate the word sequence of an answer), and dialog (i.e., given the word sequence of user input, generate the word sequence of response). 2.2.2. Encoder\u2013decoder framework. Ref. [11] proposed an encoder\u2013decoder framework for <b>sequence-to-sequence</b> modeling. As shown in Fig ...", "dateLastCrawled": "2022-01-31T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Proceedings of the 56th Annual Meeting of the Association for ...", "url": "https://aclanthology.org/volumes/P18-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/P18-1", "snippet": "We propose a novel, holistic, extendable framework based on a single <b>sequence-to-sequence</b> (seq2seq) model which <b>can</b> be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing <b>task</b>-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a simplistic Two Stage CopyNet instantiation which emonstrates good scalability: significantly reducing model complexity in terms ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Medical Report Generation Using Deep Learning</b> | by Vinithavn ...", "url": "https://medium.com/analytics-vidhya/medical-report-generation-using-deep-learning-87b50096ead0", "isFamilyFriendly": true, "displayUrl": "https://<b>medium</b>.com/analytics-vidhya/<b>medical-report-generation-using-deep-learning</b>-87b...", "snippet": "After t time steps, the model will generate the final report. 6. Performance Metric. For comparing the generated medical reports and actual reports, we <b>can</b> use the BLEU (<b>Bilingual</b> Evaluation ...", "dateLastCrawled": "2022-02-02T21:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Systematic Study of <b>Inner-Attention-Based Sentence Representations in</b> ...", "url": "https://direct.mit.edu/coli/article/46/2/387/93367/A-Systematic-Study-of-Inner-Attention-Based", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/46/2/387/93367/A-Systematic-Study-of-Inner...", "snippet": "Inasmuch as MT is described as the <b>task</b> of translating a sentence from one language to another, at the recent ... -track on multilingual systems was introduced with the aim of exploiting a third language to improve a <b>bilingual</b> model. Multilingual neural machine translation comes in many flavors with different architectures and ways of sharing parameters (Luong et al. 2016; Zoph and Knight 2016; Lee, Cho, and Hofmann 2017 ...", "dateLastCrawled": "2022-01-02T19:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is a good topic for research in NLP to be completed in 9 months ...", "url": "https://www.quora.com/What-is-a-good-topic-for-research-in-NLP-to-be-completed-in-9-months-for-an-undergraduate", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-a-good-topic-for-research-in-NLP-to-be-completed-in-9...", "snippet": "Answer: Word embeddings (representing words as vectors) or sentence embeddings and topics related to them may be worth considering for a 9 month undergraduate project. * Review of word /sentence embeddings * * To begin with a good up to date and comprehensive review of word embeddings seems ...", "dateLastCrawled": "2022-01-16T16:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "translation from one language to another is called", "url": "https://gruporogel.com/21jm0/translation-from-one-language-to-another-is-called", "isFamilyFriendly": true, "displayUrl": "https://gruporogel.com/21jm0/translation-from-one-language-to-another-is-called", "snippet": "A <b>bilingual</b> <b>person</b> is the one who knows both the languages involved in the conversation and is able to understand as well as translate the same. According to Vinay and Darbelnet, a literal translation <b>can</b> only be applied with languages extremely close in cultural terms. Solved Computer software is commonly used to translate ...", "dateLastCrawled": "2022-01-15T11:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "NER2QUES: combining named entity recognition and <b>sequence to sequence</b> ...", "url": "https://link.springer.com/article/10.1007/s00521-021-06477-7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00521-021-06477-7", "snippet": "NER2QUES was the combining pre-trained language model and <b>sequence-to-sequence</b> model. Specifically, we used BERT to detect NERs in a sentence and then applied a <b>sequence-to-sequence</b> model to automatically generate questions that corresponded to NER\u2019s types. We <b>compared</b> the accuracy of the proposed method to PhoBERT and spaCy on the NER <b>task</b>. Also, we used F1, BLEU, ROUGE, and METEOR to measure the effectiveness of this approach with the rules-based method, T5, and BERT on question ...", "dateLastCrawled": "2022-01-22T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Duplex Sequence-to-Sequence Learning for Reversible Machine Translation</b> ...", "url": "https://deepai.org/publication/duplex-sequence-to-sequence-learning-for-reversible-machine-translation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>duplex-sequence-to-sequence-learning</b>-for-reversible...", "snippet": "A <b>sequence-to-sequence</b> model with parameter \u03b8 is duplex if it satisfies the following: its network has two ends: a source end and a target end; both source and target ends <b>can</b> take input and output sequences; the network defines a forward mapping function f \u2192 \u03b8: V \u2217 s \u2192 V \u2217 t and a reverse mapping function f \u2190 \u03b8: V \u2217 t \u2192 V \u2217 s, where V s, V t are the vocabularies of the source and target domains X, Y, and V \u2217 s, V \u2217 t are all possible sequences; essentially, it ...", "dateLastCrawled": "2021-12-26T10:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sequence-to-Sequence Models Can Directly Translate Foreign Speech</b> ...", "url": "https://www.researchgate.net/publication/319185738_Sequence-to-Sequence_Models_Can_Directly_Translate_Foreign_Speech", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319185738_<b>Sequence-to-Sequence</b>_Models_<b>Can</b>...", "snippet": "<b>Sequence-to-sequence</b> based speech translation has shown very good potential over the traditional cascaded system (Berard et al., 2016;Goldwater et al., 2017; Weiss et al., 2017) with end-to-end ...", "dateLastCrawled": "2021-12-11T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Progress in Neural NLP: Modeling, Learning, and Reasoning", "url": "https://www.engineering.org.cn/en/10.1016/j.eng.2019.12.014", "isFamilyFriendly": true, "displayUrl": "https://www.engineering.org.cn/en/10.1016/j.eng.2019.12.014", "snippet": "<b>Task</b> of <b>sequence-to-sequence</b> modeling. <b>Sequence-to-sequence</b> modeling attempts to generate one sequence with another sequence as input. Many NLP tasks <b>can</b> be formulated as a <b>sequence-to-sequence</b> <b>task</b>, such as MT (i.e., given the source language word sequence, generate the target language word sequence), QA (i.e., given the word sequence of a question, generate the word sequence of an answer), and dialog (i.e., given the word sequence of user input, generate the word sequence of response). 2.2 ...", "dateLastCrawled": "2022-01-31T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Progress in Neural NLP: Modeling, Learning, and Reasoning", "url": "https://www.sciencedirect.com/science/article/pii/S2095809919304928", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2095809919304928", "snippet": "Many NLP tasks <b>can</b> be formulated as a <b>sequence-to-sequence</b> <b>task</b>, such as MT (i.e., given the source language word sequence, generate the target language word sequence), QA (i.e., given the word sequence of a question, generate the word sequence of an answer), and dialog (i.e., given the word sequence of user input, generate the word sequence of response). 2.2.2. Encoder\u2013decoder framework. Ref. proposed an encoder\u2013decoder framework for <b>sequence-to-sequence</b> modeling. As shown in Fig. 5 ...", "dateLastCrawled": "2022-01-30T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Direct speech-to-speech translation with a <b>sequence-to-sequence</b> model ...", "url": "https://deepai.org/publication/direct-speech-to-speech-translation-with-a-sequence-to-sequence-model", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/direct-speech-to-speech-translation-with-a-sequence-to...", "snippet": "The proposed model resembles recent <b>sequence-to-sequence</b> models for voice conversion, the <b>task</b> of recreating an utterance in another <b>person</b>\u2019s voice [22, 23, 24]. For example, [ 23 ] proposes an attention-based model to generate spectrograms in the target voice based on input features (spectrogram concatenated with ASR bottleneck features) from the source voice.", "dateLastCrawled": "2021-12-17T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "The general <b>task</b> of <b>sequence to sequence</b> modelling is at the heart of some of the most difficult tasks in NLP, including: Text summarization; Text simplification; Question answering ; Chatbots; Machine translation; This s o rt of technology is right out of science fiction. With such a wide range of exciting applications, it\u2019s easy to see why <b>sequence to sequence</b> modeling is more popular than ever. What\u2019s not easy is actually evaluating these systems. Unfortunately for folks who are just ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Controlling Text Complexity in Neural Machine Translation", "url": "https://aclanthology.org/D19-1166.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/D19-1166.pdf", "snippet": "to align segments across comparable <b>bilingual</b> articles. The resulting dataset makes it pos- sible to train multi-<b>task</b> <b>sequence-to-sequence</b> models that translate Spanish into English tar-geted at an easier reading grade level than the original Spanish. We show that these multi-<b>task</b> models outperform pipeline approaches that translate and simplify text independently. 1 Introduction Generating text at the right level of complexity <b>can</b> make machine translation (MT) more useful for a wide range ...", "dateLastCrawled": "2021-09-14T13:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Automatic repetition instruction generation for air traffic control ...", "url": "https://www.sciencedirect.com/science/article/pii/S0950705122000685", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705122000685", "snippet": "The proposed framework is implemented using a <b>sequence-to-sequence</b> architecture and is optimized through multi-<b>task</b> learning, including text instruction understanding (TIU) and repetition instruction generation (RIG). In the proposed framework, a shared encoder is designed to learn the representations from the input sequence. Two <b>task</b>-specific attention modules are proposed to extract <b>task</b>-specific features, and different decoders are applied to generate the TIU and RIG outputs. The TIU ...", "dateLastCrawled": "2022-01-24T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Systematic Study of <b>Inner-Attention-Based Sentence Representations in</b> ...", "url": "https://direct.mit.edu/coli/article/46/2/387/93367/A-Systematic-Study-of-Inner-Attention-Based", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/coli/article/46/2/387/93367/A-Systematic-Study-of-Inner...", "snippet": "On a first note, the inclusion of language pairs results in an improved performance when <b>compared</b> to the <b>bilingual</b> baselines, as well as the many-to-one and one-to-many cases. The only exception is the En \u2192 Fr <b>task</b>. Moreover, the addition of monolingual data during training leads to even higher scores, producing the overall best model. The improvements in BLEU range from 1.40 to 4.43 <b>compared</b> to the standard <b>bilingual</b> model.", "dateLastCrawled": "2022-01-02T19:16:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Week 3: <b>Sequence to sequence</b> architectures. <b>Sequence to sequence</b> models Language translation for example; Image captioning, caption an image; Picking the most likely model <b>Machine</b> Transation Model Split into a model encoding the sentence; and then a language model. Calculate the probability of an English sentence conditioned on a French sentence.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Week 1 Quiz - Introduction to deep <b>learning</b>. What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI is powering personal devices in our homes and offices, similar to electricity. Through the \u201csmart grid\u201d, AI is delivering a new wave of electricity. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Note: Andrew ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(bilingual person)", "+(sequence-to-sequence task) is similar to +(bilingual person)", "+(sequence-to-sequence task) can be thought of as +(bilingual person)", "+(sequence-to-sequence task) can be compared to +(bilingual person)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}