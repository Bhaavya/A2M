{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "9.2. Long Short-Term Memory (<b>LSTM</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/lstm.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/<b>lstm</b>.html", "snippet": "9.2.1.1. Input <b>Gate</b>, <b>Forget</b> <b>Gate</b>, and Output <b>Gate</b>\u00b6. Just <b>like</b> in GRUs, the data feeding into the <b>LSTM</b> gates are the input at the current time step and the hidden state of the previous time step, as illustrated in Fig. 9.2.1.They are processed by three fully-connected layers with a sigmoid activation function to compute the values of the input, <b>forget</b>. and output gates.", "dateLastCrawled": "2022-02-02T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Bird\u2019s-Eye View <b>Of Artificial Intelligence, Machine Learning, Neural</b> ...", "url": "https://medium.com/geekculture/birds-eye-view-of-artificial-intelligence-machine-learning-neural-networks-language-part-2-a53d93495de1", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/birds-eye-view-of-artificial-intelligence-<b>machine</b>...", "snippet": "<b>Forget</b> <b>Gate</b>: <b>Information</b> from the previous hidden state and current input are passed through this <b>gate</b> with a sigmoid function. The closer the value is to 0 the more it gets forgotten. The closer ...", "dateLastCrawled": "2021-05-17T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Long Short Term Memory(LSTM) and <b>Gated Recurrent</b> Units(GRU) | by ...", "url": "https://prvnk10.medium.com/long-short-term-memory-lstm-and-gated-recurrent-units-gru-240d8a62db9", "isFamilyFriendly": true, "displayUrl": "https://prvnk10.medium.com/long-short-term-memory-lstm-and-<b>gated-recurrent</b>-units-gru...", "snippet": "The whiteboard <b>analogy</b>. The <b>analogy</b> to the above-described scenario is this whiteboard <b>analogy</b>. So, we have a whiteboard(of fixed width and height) and if we start writing <b>information</b> on it(say every 10\u201315 seconds we write some <b>information</b> on it), then after a while this whiteboard would become so messy and it would become very difficult to figure out what was the <b>information</b> that we wrote at time step 1 when we are at time step 100.", "dateLastCrawled": "2022-01-30T07:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>3 Deep Learning Algorithms in under 5</b> minutes \u2014 Part 2 (Deep Sequential ...", "url": "https://towardsdatascience.com/3-deep-learning-algorithms-in-under-5-minutes-part-2-deep-sequential-models-b84e3a29d9a8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>3-deep-learning-algorithms-in-under-5</b>-minutes-part-2...", "snippet": "Enter simple RNNs! This is the crux of a RNN. It takes some input at time t \u2014 x(t) (new word from last kid) and a state from time t-1 \u2014 h(t-1) (previous words of the message) as inputs and produce an output \u2014 y(t) (previous message + new word from last kid + your new word). Once you train a RNN, you <b>can</b> (but generally you won\u2019t) keep predicting forever, because the prediction of time t (i.e. y(t)) becomes the input at t+1 (i.e. y(t)=x(t+1)).Here\u2019s what an RNN looks <b>like</b> in real world.", "dateLastCrawled": "2022-01-29T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Top 10 Deep <b>Learning</b> Algorithms in <b>Machine</b> <b>Learning</b> [2022]", "url": "https://www.projectpro.io/article/deep-learning-algorithms/443", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/deep-<b>learning</b>-<b>algorithms</b>/443", "snippet": "Before we move on to the list of deep <b>learning</b> algorithms in <b>machine</b> <b>learning</b>, let\u2019s understand the structure and working of deep <b>learning</b> algorithms with the popular MNIST dataset.The human brain is a network of billions of neurons that help in representing a tremendous amount of knowledge. Deep <b>Learning</b> also uses the same <b>analogy</b> of a brain neuron for processing the <b>information</b> and recognizing them. Let\u2019s understand this with an example.", "dateLastCrawled": "2022-02-02T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mathematical understanding of RNN and its variants - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/mathematical-understanding-of-rnn-and-its-variants/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/mathematical-understanding-of-rnn-and-its-variants", "snippet": "Today, one <b>can</b> see a lot of such AI-powered applications <b>like</b> the fight against human trafficking, healthcare adviser, self-driving cars, Intrusion detection and prevention, object tracking and counting, face detection and recognition, disease prediction and virtual assistance for human help. This particular post talks about RNN, its variants (LSTM, GRU) and mathematics behind it. RNN is a type of neural network which accepts variable-length input and produces variable-length output. It is ...", "dateLastCrawled": "2022-01-29T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Leveraging workforce analytics using deep learning</b> | by Snigdha ...", "url": "https://towardsdatascience.com/leveraging-workforce-analytics-using-deep-learning-b13427f674bf", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>leveraging-workforce-analytics-using-deep-learning</b>-b...", "snippet": "We have earlier applied <b>machine</b> <b>learning</b> linear models <b>like</b> ARIMA, for the reason that they are very effective for many problems and well understood. But linear methods <b>like</b> these suffer few limitations, such as: -Missing data is generally not supported. - Linear methods assume linear relationships in the data excluding more complex joint distributions. - Most of the linear models focus on uni-variate data. (except for few) whereas most of the real world problems have multiple inputs ...", "dateLastCrawled": "2022-01-25T17:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to pass multiple vectors to a RNN/LSTM network and get output as a ...", "url": "https://www.reddit.com/r/deeplearning/comments/s990a9/how_to_pass_multiple_vectors_to_a_rnnlstm_network/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/deep<b>learning</b>/comments/s990a9/how_to_pass_multiple_vectors_to...", "snippet": "And of course, SVD has also been used for ranking web pages <b>like</b> Google&#39;s Pagerank <b>algorithm</b> does so you <b>can</b> find your favorite recipe with a quick search.-----I have been studying and practicing <b>Machine</b> <b>Learning</b> and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing ...", "dateLastCrawled": "2022-01-21T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What <b>machine learning algorithms come under supervised learning</b>, and ...", "url": "https://www.quora.com/What-machine-learning-algorithms-come-under-supervised-learning-and-what-comes-under-unsupervised-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>machine-learning-algorithms-come-under-supervised-learning</b>...", "snippet": "Answer (1 of 2): Thanks for the A2A. Supervised <b>Learning</b> Algorithms: * Analytical <b>learning</b> * Artificial neural network * Backpropagation * Boosting (meta-<b>algorithm</b>) * Bayesian statistics * Case-based reasoning * Decision tree <b>learning</b> * Inductive logic programming * Gaussian process re...", "dateLastCrawled": "2022-01-16T15:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Could someone suggest a good article that explains the implementation ...", "url": "https://www.reddit.com/r/deeplearning/comments/s9dykl/could_someone_suggest_a_good_article_that/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/deep<b>learning</b>/comments/s9dykl/could_someone_suggest_a_good...", "snippet": "And of course, SVD has also been used for ranking web pages <b>like</b> Google&#39;s Pagerank <b>algorithm</b> does so you <b>can</b> find your favorite recipe with a quick search.-----I have been studying and practicing <b>Machine</b> <b>Learning</b> and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing ...", "dateLastCrawled": "2022-01-21T16:03:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "9.2. Long Short-Term Memory (<b>LSTM</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/lstm.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/<b>lstm</b>.html", "snippet": "9.2.1.1. Input <b>Gate</b>, <b>Forget</b> <b>Gate</b>, and Output <b>Gate</b>\u00b6. Just like in GRUs, the data feeding into the <b>LSTM</b> gates are the input at the current time step and the hidden state of the previous time step, as illustrated in Fig. 9.2.1.They are processed by three fully-connected layers with a sigmoid activation function to compute the values of the input, <b>forget</b>. and output gates.", "dateLastCrawled": "2022-02-02T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Misnomers and Confusing Terms in Machine Learning</b>", "url": "https://product.hubspot.com/blog/misnomers-and-confusing-terms-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://product.hubspot.com/blog/<b>misnomers-and-confusing-terms-in-machine-learning</b>", "snippet": "A common LSTM layer is composed of three gates: <b>information</b> gets into the cell thanks to the input <b>gate</b>, stays in the cell thanks to the <b>forget</b> <b>gate</b>, and exits the cell thanks to the output <b>gate</b>. The most confusing term is the <b>forget</b> <b>gate</b>. If one follows the above logic, when an element of the activated vector is \\(1\\) it means that the component will be completely retained. So the action the <b>gate</b> controls is to remember, not to <b>forget</b>.", "dateLastCrawled": "2022-02-03T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Long Short Term Memory(LSTM) and <b>Gated Recurrent</b> Units(GRU) | by ...", "url": "https://prvnk10.medium.com/long-short-term-memory-lstm-and-gated-recurrent-units-gru-240d8a62db9", "isFamilyFriendly": true, "displayUrl": "https://prvnk10.medium.com/long-short-term-memory-lstm-and-<b>gated-recurrent</b>-units-gru...", "snippet": "<b>Forget</b> <b>Gate</b> (ft) is again some values between 0 to 1. This decides what fraction of s(t-1) ... s1 is already computed and o1 we <b>can</b> compute in a <b>similar</b> way as the i1. And we keep computing the subsequent parameters like this and when we have the h5 output then we <b>can</b> compute the final output as. O(Vh5 + c) where O is the softmax function. <b>Gated Recurrent</b> Units: <b>Gated Recurrent</b> Unit is exactly the same as the LSTM except for one minor change and this change is when we need to combine/sum up ...", "dateLastCrawled": "2022-01-30T07:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Bird\u2019s-Eye View <b>Of Artificial Intelligence, Machine Learning, Neural</b> ...", "url": "https://medium.com/geekculture/birds-eye-view-of-artificial-intelligence-machine-learning-neural-networks-language-part-2-a53d93495de1", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/birds-eye-view-of-artificial-intelligence-<b>machine</b>...", "snippet": "<b>Forget</b> <b>Gate</b>: <b>Information</b> from the previous hidden state and current input are passed through this <b>gate</b> with a sigmoid function. The closer the value is to 0 the more it gets forgotten. The closer ...", "dateLastCrawled": "2021-05-17T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Top 10 Deep <b>Learning</b> Algorithms in <b>Machine</b> <b>Learning</b> [2022]", "url": "https://www.projectpro.io/article/deep-learning-algorithms/443", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/deep-<b>learning</b>-<b>algorithms</b>/443", "snippet": "This is the reason you <b>can</b> <b>forget</b> the answers that you have written in a certification exam after a couple of days, but if you have binge-watched a sitcom on Netflix, you might remember the dialogues and scenes for a long period of time :D . New Projects . Build a Multi-Class Classification Model in Python on Saturn Cloud View Project. Streaming Data Pipeline using Spark, HBase and Phoenix View Project. Build Portfolio Optimization <b>Machine</b> <b>Learning</b> Models in R View Project. Hands-On Approach ...", "dateLastCrawled": "2022-02-02T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>What Is Deep Learning</b>? - YakBots - YakBots.com - <b>Information</b> About ...", "url": "https://yakbots.com/what-is-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://yakbots.com/<b>what-is-deep-learning</b>", "snippet": "In this <b>analogy</b>, the purpose of the person is to become as creative as they <b>can</b> be. We <b>can</b> think of data as knowledge the person <b>can</b> gain and Deep <b>Learning</b> as the brain of the person. <b>Similar</b> to how the brain needs <b>information</b> to be creative, deep <b>learning</b> needs data to be efficient. The more <b>information</b> a brain has, the more creative it <b>can</b> be ...", "dateLastCrawled": "2021-12-29T14:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Combine both forms of memories into 4 gates- <b>forget</b> <b>gate</b>, remember <b>gate</b>, learn <b>gate</b> and use <b>gate</b> - these dates are used to update both long and short term memories. About all gates: using example of NatGeo science and nature show Learn <b>Gate</b> Joins the short term memory and the event; and forgets the un-important part--&gt; ignore factor; <b>Forget</b> <b>Gate</b> The <b>forget</b> factor; Remember <b>Gate</b> Combines the long term memory from the <b>forget</b> <b>gate</b> and short term memory from the learn <b>gate</b> and SIMPLY ADD THEM ...", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Leveraging workforce analytics using deep learning</b> | by Snigdha ...", "url": "https://towardsdatascience.com/leveraging-workforce-analytics-using-deep-learning-b13427f674bf", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>leveraging-workforce-analytics-using-deep-learning</b>-b...", "snippet": "This type of data cannot be modeled with regular <b>machine</b> <b>learning</b> models, but need a specialized set of models that <b>can</b> take care of this aspect of the data. This article is a run through of all the predictive time series models that I had applied in my analysis to predict workforce requirements. This article is NOT about all the various different analytical tasks that we <b>can</b> perform on workforce related data. It is only about the <b>machine</b> <b>learning</b> and deep <b>learning</b> models that I have applied ...", "dateLastCrawled": "2022-01-25T17:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) E-mail classification with <b>machine</b> <b>learning</b> and word embeddings ...", "url": "https://www.researchgate.net/publication/342316103_E-mail_classification_with_machine_learning_and_word_embeddings_for_improved_customer_support", "isFamilyFriendly": true, "displayUrl": "https://www.research<b>gate</b>.net/publication/342316103_E-mail_classification_with_<b>machine</b>...", "snippet": "Abstract and Figures. Classifying e-mails into distinct labels <b>can</b> have a great impact on customer support. By using <b>machine</b> <b>learning</b> to label e-mails, the system <b>can</b> set up queues containing e ...", "dateLastCrawled": "2021-10-04T00:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to run Tensorflow in Intel GPU? : deeplearning", "url": "https://www.reddit.com/r/deeplearning/comments/rykjio/how_to_run_tensorflow_in_intel_gpu/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/deep<b>learning</b>/comments/rykjio/how_to_run_tensorflow_in_intel_gpu", "snippet": "\ud83c\udfe5\ud83e\uddb4 Time for (somewhat of) an <b>analogy</b>: You <b>can</b> think of SVD as an x-ray of a matrix. It provides us with simpler pieces that constitute the matrix and by looking at these simpler pieces i.e. simpler matrices we <b>can</b> say a lot about the matrix in question. Things like its (pseudo-) inverse, rank, null-space, range among other things <b>can</b> be easily determined the same way a doctor <b>can</b> say a lot with an x-ray scan of your body.", "dateLastCrawled": "2022-01-20T16:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Misnomers and Confusing Terms in Machine Learning</b>", "url": "https://product.hubspot.com/blog/misnomers-and-confusing-terms-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://product.hubspot.com/blog/<b>misnomers-and-confusing-terms-in-machine-learning</b>", "snippet": "A common LSTM layer is composed of three gates: <b>information</b> gets into the cell thanks to the input <b>gate</b>, stays in the cell thanks to the <b>forget</b> <b>gate</b>, and exits the cell thanks to the output <b>gate</b>. The most confusing term is the <b>forget</b> <b>gate</b>. If one follows the above logic, when an element of the activated vector is \\(1\\) it means that the component will be completely retained. So the action the <b>gate</b> controls is to remember, not to <b>forget</b>.", "dateLastCrawled": "2022-02-03T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "These blocks <b>can</b> <b>be thought</b> of as a differentiable version of the memory chips in a digital computer. Each one contains one or more recurrently connected memory cells and three multiplicative units \u2013 the input, output and <b>forget</b> gates \u2013 that provide continuous analogues of write, read and reset operations for the cells. \u2026 The net <b>can</b> only interact with the cells via the gates. \u2014 Alex Graves, et al., Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network ...", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Frontiers | The Role of <b>Machine</b> <b>Learning</b> in Knowledge-Based Response ...", "url": "https://internal-journal.frontiersin.org/articles/10.3389/fonc.2018.00266/full", "isFamilyFriendly": true, "displayUrl": "https://internal-journal.frontiersin.org/articles/10.3389/fonc.2018.00266/full", "snippet": "The proposed KBR-ART framework <b>can</b> <b>be thought</b> of as being comprised of four stages, as depicted in Figure 1. ... An LSTM unit consists of 1 cellstate h (t) and 3 gates: <b>forget</b> <b>gate</b> F (t), input <b>gate</b> I (t) , and output <b>gate</b> O (t), with x (t) as input and y (t) as final (prediction) output. In practice, there are several choices for activation functions \u03c3 i, such as sigmoid, ReLu, eLu, Leaky ReLU function, etc., whose effectiveness usually depends on the nature of the dataset and the problem ...", "dateLastCrawled": "2021-11-26T05:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Guide to <b>Deep Learning</b> Layers - ADG Efficiency", "url": "https://adgefficiency.com/guide-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://adgefficiency.com/guide-<b>deep-learning</b>", "snippet": "The <b>forget</b> <b>gate</b> acts like a DELETE, allowing the LSTM to remove <b>information</b> that isn\u2019t useful. The input <b>gate</b> acts like a POST, where the LSTM <b>can</b> choose <b>information</b> to remember. The output <b>gate</b> acts like a GET, where the LSTM chooses what to send back to a user request for <b>information</b>.", "dateLastCrawled": "2022-01-29T00:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Tianyu Lu - Computation and Biology", "url": "https://tianyu-lu.github.io/igem/protein/ml/2019/11/01/Optimizing-Plastic-Degrading-Proteins.html", "isFamilyFriendly": true, "displayUrl": "https://tianyu-lu.github.io/igem/protein/ml/2019/11/01/Optimizing-Plastic-Degrading...", "snippet": "The result of (1), $\\tilde{c}^{}$, <b>can</b> be interpreted as the data to remember for the current cell. Equations (2), (3), and (4) output vectors which <b>can</b> <b>be thought</b> of as gates, where (2) is the update <b>gate</b>, (3) is the <b>forget</b> <b>gate</b>, and (4) is the output <b>gate</b>. Since $\\sigma(z) \\in (0, 1) \\forall z \\in \\mathbb{R}$, the gamma scalars <b>can</b> <b>be thought</b> ...", "dateLastCrawled": "2022-02-03T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Bird\u2019s-Eye View <b>Of Artificial Intelligence, Machine Learning, Neural</b> ...", "url": "https://medium.com/geekculture/birds-eye-view-of-artificial-intelligence-machine-learning-neural-networks-language-part-2-a53d93495de1", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/birds-eye-view-of-artificial-intelligence-<b>machine</b>...", "snippet": "<b>Forget</b> <b>Gate</b>: <b>Information</b> from the previous hidden state and current input are passed through this <b>gate</b> with a sigmoid function. The closer the value is to 0 the more it gets forgotten. The closer ...", "dateLastCrawled": "2021-05-17T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The 10 Neural Network Architectures <b>Machine Learning</b> Researchers Need ...", "url": "https://data-notes.co/a-gentle-introduction-to-neural-networks-for-machine-learning-d5f3f8987786", "isFamilyFriendly": true, "displayUrl": "https://data-notes.co/a-gentle-introduction-to-neural-networks-for-<b>machine-learning</b>-d5...", "snippet": "A <b>machine learning</b> <b>algorithm</b> then takes these examples and produces a program that does the job. The program produced by the <b>learning</b> <b>algorithm</b> may look very different from a typical hand-written program. It may contain millions of numbers. If we do it right, the program works for new cases as well as the ones we trained it on. If the data changes the program <b>can</b> change too by training on the new data. You should note that massive amounts of computation are now cheaper than paying someone to ...", "dateLastCrawled": "2022-01-30T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Which GPU for deep <b>learning</b> : deeplearning", "url": "https://www.reddit.com/r/deeplearning/comments/l6tf3s/which_gpu_for_deep_learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/deep<b>learning</b>/comments/l6tf3s/which_gpu_for_deep_<b>learning</b>", "snippet": "\ud83c\udfe5\ud83e\uddb4 Time for (somewhat of) an <b>analogy</b>: You <b>can</b> think of SVD as an x-ray of a matrix. It provides us with simpler pieces that constitute the matrix and by looking at these simpler pieces i.e. simpler matrices we <b>can</b> say a lot about the matrix in question. Things like its (pseudo-) inverse, rank, null-space, range among other things <b>can</b> be easily determined the same way a doctor <b>can</b> say a lot with an x-ray scan of your body.", "dateLastCrawled": "2022-01-25T07:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Alan&#39;s Blog \u2013 Math, <b>Machine</b> <b>Learning</b>, and other Life Thoughts", "url": "https://achungweb.wordpress.com/", "isFamilyFriendly": true, "displayUrl": "https://achungweb.wordpress.com", "snippet": "Researchers originally tried to construct programs that <b>can</b> remember past <b>information</b> by building a long chain of neural units and having each unit take output from the previous unit, along with new <b>information</b>, as input, thus allowing the use of past-processed <b>information</b>. In other words, the nth unit would take the output from the (n-1)th unit, multiplied by some hidden matrix H, as well as new data X_n, multiplied by some weight matrix W, as input. Theoretically, this <b>information</b> would ...", "dateLastCrawled": "2022-01-19T07:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Shall I consider switching to neuroscience or continue as a <b>machine</b> ...", "url": "https://www.quora.com/Shall-I-consider-switching-to-neuroscience-or-continue-as-a-machine-learning-engineer-I-am-thinking-too-much-about-neuroscience-but-I-am-not-sure-about-it-yet-What-are-my-chances-future-career-in-neuroscience-for", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Shall-I-consider-switching-to-neuroscience-or-continue-as-a...", "snippet": "Answer (1 of 6): If your interest is in algorithms, I suggest sticking with <b>machine</b> <b>learning</b> and/or computer science. Neuroscience is not focused on economically useful algorithms or technology, beyond data analysis techniques and experimental methods internal to the field. Even computational neu...", "dateLastCrawled": "2022-01-19T11:30:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Long Short Term Memory(LSTM) and <b>Gated Recurrent</b> Units(GRU) | by ...", "url": "https://prvnk10.medium.com/long-short-term-memory-lstm-and-gated-recurrent-units-gru-240d8a62db9", "isFamilyFriendly": true, "displayUrl": "https://prvnk10.medium.com/long-short-term-memory-lstm-and-<b>gated-recurrent</b>-units-gru...", "snippet": "The whiteboard <b>analogy</b>. The <b>analogy</b> to the above-described scenario is this whiteboard <b>analogy</b>. So, we have a whiteboard(of fixed width and height) and if we start writing <b>information</b> on it(say every 10\u201315 seconds we write some <b>information</b> on it), then after a while this whiteboard would become so messy and it would become very difficult to figure out what was the <b>information</b> that we wrote at time step 1 when we are at time step 100.", "dateLastCrawled": "2022-01-30T07:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Python RNN: Recurrent Neural Networks for Time Series Forecasting | by ...", "url": "https://towardsdatascience.com/temporal-loops-intro-to-recurrent-neural-networks-for-time-series-forecasting-in-python-b0398963dc1f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/temporal-loops-intro-to-recurrent-neural-networks-for...", "snippet": "The cell acts like a sieve that determines how much incoming <b>information</b> is captured and how much of it is retained. The model <b>can</b> decide whether it opens an input <b>gate</b> to store <b>information</b>, reject and delete it from long-term memory (<b>forget</b> <b>gate</b>), or passes the <b>information</b> on to the next layer (output <b>gate</b>). The RNN makes these decisions based ...", "dateLastCrawled": "2022-02-02T15:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>machine</b> <b>learning</b> - Why does the <b>transformer</b> do better than RNN and LSTM ...", "url": "https://ai.stackexchange.com/questions/20075/why-does-the-transformer-do-better-than-rnn-and-lstm-in-long-range-context-depen", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/20075/why-does-the-<b>transformer</b>-do-better-than...", "snippet": "The way in which RNN is able to store <b>information</b> from the past is to loop in its architecture, which automatically keeps <b>information</b> from the past stored. Second: sltm / gru is a component of regulating the flow of <b>information</b> referred to as the <b>gate</b> and GRU has 2 gates, namely reset <b>gate</b> and <b>gate</b> update.", "dateLastCrawled": "2022-01-29T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "assessment id-188", "url": "https://www.nptel.ac.in/content/storage2/courses/downloads/106106184/Week_11_Assignment_11.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.nptel.ac.in/content/storage2/courses/downloads/106106184/Week_11...", "snippet": "Selective <b>Forget</b> - The Whiteboard <b>Analogy</b> Long Short Term Memory(LSTM) and Cated Recurrent Units(GRLls) HOW LSTMs avoid the problem Of vanishing gradients HOW LSTMs avoid the problem Of vanishing gradients (Contd.) Lecture Material for Week Il Quiz : Assignment Il Week Il Feedback Deep <b>Learning</b> Week 12 DOWNLOAD VIDEOS Announcements About the Course Ask a Question Progress Mentor Assignment 11 The due date for submitting this assignment has passed. As per our records you have not submitted ...", "dateLastCrawled": "2022-01-04T11:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Top 10 Deep <b>Learning</b> Algorithms in <b>Machine</b> <b>Learning</b> [2022]", "url": "https://www.projectpro.io/article/deep-learning-algorithms/443", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/deep-<b>learning</b>-<b>algorithms</b>/443", "snippet": "This is the reason you <b>can</b> <b>forget</b> the answers that you have written in a certification exam after a couple of days, but if you have binge-watched a sitcom on Netflix, you might remember the dialogues and scenes for a long period of time :D . New Projects . Build a Multi-Class Classification Model in Python on Saturn Cloud View Project. Streaming Data Pipeline using Spark, HBase and Phoenix View Project. Build Portfolio Optimization <b>Machine</b> <b>Learning</b> Models in R View Project. Hands-On Approach ...", "dateLastCrawled": "2022-02-02T17:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) How deep the <b>machine</b> <b>learning</b> <b>can</b> be", "url": "https://www.researchgate.net/publication/341148934_How_deep_the_machine_learning_can_be", "isFamilyFriendly": true, "displayUrl": "https://www.research<b>gate</b>.net/publication/341148934_How_deep_the_<b>machine</b>_<b>learning</b>_<b>can</b>_be", "snippet": "PDF | Today we live in the age of artificial intelligence and <b>machine</b> <b>learning</b>; from small startups to HW or SW giants, everyone wants to build <b>machine</b>... | Find, read and cite all the research ...", "dateLastCrawled": "2021-12-12T17:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Leveraging workforce analytics using deep learning</b> | by Snigdha ...", "url": "https://towardsdatascience.com/leveraging-workforce-analytics-using-deep-learning-b13427f674bf", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>leveraging-workforce-analytics-using-deep-learning</b>-b...", "snippet": "This type of data cannot be modeled with regular <b>machine</b> <b>learning</b> models, but need a specialized set of models that <b>can</b> take care of this aspect of the data. This article is a run through of all the predictive time series models that I had applied in my analysis to predict workforce requirements. This article is NOT about all the various different analytical tasks that we <b>can</b> perform on workforce related data. It is only about the <b>machine</b> <b>learning</b> and deep <b>learning</b> models that I have applied ...", "dateLastCrawled": "2022-01-25T17:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b> with Flax - From Zero to Hero | New player in the game ...", "url": "https://www.reddit.com/r/deeplearning/comments/s9ctro/machine_learning_with_flax_from_zero_to_hero_new/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/deep<b>learning</b>/comments/s9ctro/<b>machine</b>_<b>learning</b>_with_flax_from...", "snippet": "I have been studying and practicing <b>Machine</b> <b>Learning</b> and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it <b>can</b> be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which <b>can</b> be of interest to existing practitioners and fresh enthusiasts alike. The posts will cover topics like ...", "dateLastCrawled": "2022-01-21T15:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Could someone suggest a good article that explains the implementation ...", "url": "https://www.reddit.com/r/deeplearning/comments/s9dykl/could_someone_suggest_a_good_article_that/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/deep<b>learning</b>/comments/s9dykl/could_someone_suggest_a_good...", "snippet": "\ud83c\udfe5\ud83e\uddb4 Time for (somewhat of) an <b>analogy</b>: You <b>can</b> think of SVD as an x-ray of a matrix. It provides us with simpler pieces that constitute the matrix and by looking at these simpler pieces i.e. simpler matrices we <b>can</b> say a lot about the matrix in question. Things like its (pseudo-) inverse, rank, null-space, range among other things <b>can</b> be easily determined the same way a doctor <b>can</b> say a lot with an x-ray scan of your body.", "dateLastCrawled": "2022-01-21T16:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are some best book for Computer Architecture and Organization for ...", "url": "https://www.quora.com/What-are-some-best-book-for-Computer-Architecture-and-Organization-for-GATE", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-some-best-book-for-Computer-Architecture-and...", "snippet": "Answer: For <b>GATE</b> you <b>can</b> follow any of the following books from cover to cover: Source: Best Books for <b>GATE</b> CSE", "dateLastCrawled": "2022-01-20T21:15:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning: Text Generation, A Summary</b> \u2013 Alan&#39;s Blog", "url": "https://achungweb.wordpress.com/2017/04/14/machine-learning-text-generation-a-summary/", "isFamilyFriendly": true, "displayUrl": "https://achungweb.wordpress.com/2017/04/14/<b>machine-learning-text-generation-a-summary</b>", "snippet": "The <b>Forget</b> <b>Gate</b>. The <b>Forget</b> <b>Gate</b> is used to eliminate some of the information that currently exists in the cell state; for example, if we are processing language, we might want to <b>forget</b> the gender of the previous noun, since that information has no benefit to processing the next word.", "dateLastCrawled": "2022-01-20T17:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "deep <b>learning</b> - How is the LSTM RNN <b>forget</b> <b>gate</b> calculated? - Data ...", "url": "https://datascience.stackexchange.com/questions/32217/how-is-the-lstm-rnn-forget-gate-calculated", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/32217", "snippet": "You need to look at first term of the next step: C t = f t \u2299 C t \u2212 1 + i t \u2299 C \u00af t. The vector f t that is the output from the <b>forget</b> <b>gate</b>, is used as element-wise multiply against the previous cell state C t \u2212 1. It is this stage where individual elements of C are &quot;remembered&quot; or &quot;forgotten&quot;. Due to the sigmoid function, the vector f ...", "dateLastCrawled": "2022-02-02T10:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "9.2. Long Short-Term Memory (<b>LSTM</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/lstm.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/<b>lstm</b>.html", "snippet": "9.2.1.1. Input <b>Gate</b>, <b>Forget</b> <b>Gate</b>, and Output <b>Gate</b>\u00b6. Just like in GRUs, the data feeding into the <b>LSTM</b> gates are the input at the current time step and the hidden state of the previous time step, as illustrated in Fig. 9.2.1.They are processed by three fully-connected layers with a sigmoid activation function to compute the values of the input, <b>forget</b>. and output gates.", "dateLastCrawled": "2022-02-02T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Bird\u2019s-Eye View <b>Of Artificial Intelligence, Machine Learning, Neural</b> ...", "url": "https://medium.com/geekculture/birds-eye-view-of-artificial-intelligence-machine-learning-neural-networks-language-part-2-a53d93495de1", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/birds-eye-view-of-artificial-intelligence-<b>machine</b>...", "snippet": "The <b>analogy</b> is like someone carefully taking ... The gates in the system are the Input <b>Gate</b>, <b>Forget</b> <b>Gate</b> and Output <b>Gate</b>. The part of the cell known as the cell state acts as a memory that keeps ...", "dateLastCrawled": "2021-05-17T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Machine</b> <b>Learning</b> and Theological Traditions of <b>Analogy</b>", "url": "https://www.researchgate.net/publication/349470559_Machine_Learning_and_Theological_Traditions_of_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.research<b>gate</b>.net/publication/349470559_<b>Machine</b>_<b>Learning</b>_and_Theological...", "snippet": "theories of <b>analogy</b> to <b>machine</b> <b>learning</b> has brought us here, since much of it was developed, in the first place, in thinking about the use of shared vocabulary for creature and creator.", "dateLastCrawled": "2021-11-04T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Long Short Term Memory(LSTM) and <b>Gated Recurrent</b> Units(GRU) | by ...", "url": "https://prvnk10.medium.com/long-short-term-memory-lstm-and-gated-recurrent-units-gru-240d8a62db9", "isFamilyFriendly": true, "displayUrl": "https://prvnk10.medium.com/long-short-term-memory-lstm-and-<b>gated-recurrent</b>-units-gru...", "snippet": "<b>Forget</b> <b>Gate</b> (ft) is again some values between 0 to 1. This decides what fraction of s(t-1) to retain in the final computation of st . And this ft is again a standard recipe, its a function of some inputs which happens to be xt and previous intermediate state ( h(t-1) ) in this case and is also a function of some parameter which are Uf , Wf , bf , in this case also we learn these parameters from the data.", "dateLastCrawled": "2022-01-30T07:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "<b>Forget</b> <b>Gate</b> The <b>forget</b> factor; Remember <b>Gate</b> Combines the long term memory from the <b>forget</b> <b>gate</b> and short term memory from the learn <b>gate</b> and SIMPLY ADD THEM; and generate the new long term memory. Use <b>Gate</b> Takes whatever is useful from both long term and short term memories; and generate the new short term memory.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Deep <b>Learning</b> : Intro to LSTM (Long Short Term Memory) | by HIMANSHU ...", "url": "https://medium.com/@himanshunpatel01/deep-learning-intro-to-lstm-long-short-term-memory-ce504dc6e585", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@himanshunpatel01/deep-<b>learning</b>-intro-to-lstm-long-short-term...", "snippet": "A simple <b>machine</b> <b>learning</b> model or an Artificial Neural Network may learn to predict the stock prices based on a number of features: the volume of the stock, the opening value etc. While the price ...", "dateLastCrawled": "2022-01-27T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does Francois Chollet say in his book &#39;Deep <b>Learning</b>&#39; that LSTM ...", "url": "https://www.quora.com/Why-does-Francois-Chollet-say-in-his-book-Deep-Learning-that-LSTM-neural-networks-are-not-helpful-for-sentiment-analysis-problems-I-mean-there-are-plenty-of-studies-about-sentiment-analysis-and-LSTM-models", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-does-Francois-Chollet-say-in-his-book-Deep-<b>Learning</b>-that...", "snippet": "Answer (1 of 2): I have not read the book yet (I got a copy from Manning recently) so I cannot say that the line (or any paraphrase of it) appears in the book or not. A blanket statement like \u201cX don\u2019t work for Y\u201d is totally wrong in <b>Machine</b> <b>Learning</b> (a lot of papers just get written modifying met...", "dateLastCrawled": "2022-01-14T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>What is an intuitive explanation of working of</b> LSTM? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-of-working-of-LSTM", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-an-intuitive-explanation-of-working-of</b>-LSTM", "snippet": "Answer (1 of 4): LSTM can be thought as a person who has very good memory but don\u2019t care the insignificant details in the past if they are not useful at the future ...", "dateLastCrawled": "2022-01-13T08:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Frontiers | <b>Machine</b> <b>Learning</b> and Metaheuristic Methods for Renewable ...", "url": "https://www.frontiersin.org/articles/10.3389/fceng.2021.665415/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fceng.2021.665415", "snippet": "The <b>forget gate is similar</b> to the update gate in the GRU. The input gate takes the same inputs as the forget gate and processes them into sigmoid and tanh functions. The sigmoid function decides what information should be updated, and the tanh pounds the information between \u22121 and 1 to regulate the flow of information. The outputs of the sigmoid and tanh functions are then multiplied by each other to generate the output of an input gate. Afterward, the input gate outputs and the forget ...", "dateLastCrawled": "2021-12-11T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> and Metaheuristic Methods for Renewable Power ...", "url": "https://www.researchgate.net/publication/351110482_Machine_Learning_and_Metaheuristic_Methods_for_Renewable_Power_Forecasting_A_Recent_Review", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/351110482_<b>Machine</b>_<b>Learning</b>_and_Metaheuristic...", "snippet": "I. ABSTRACT-<b>Machine</b> <b>learning</b> (ML) is increasingly touching all fields in our life and making a huge impact on their improvement. In this paper, we present an overview of several used methods in ML ...", "dateLastCrawled": "2022-01-07T01:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Artificial intelligence-based approach for atrial fibrillation ...", "url": "https://www.sciencedirect.com/science/article/pii/S1746809421008673", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1746809421008673", "snippet": "The <b>machine</b> <b>learning</b>\u2013based methods are limited in their ability to process the data in raw form as those need feature extractor for that purpose, which can be applied to the classifier for the detection and classification of the input pattern . A deep <b>learning</b> approach overcomes this problem since it allows the model to be fed with the raw data for detection and classification.", "dateLastCrawled": "2021-12-06T14:32:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(forget gate)  is like +(analogy for how a machine learning algorithm can forget information)", "+(forget gate) is similar to +(analogy for how a machine learning algorithm can forget information)", "+(forget gate) can be thought of as +(analogy for how a machine learning algorithm can forget information)", "+(forget gate) can be compared to +(analogy for how a machine learning algorithm can forget information)", "machine learning +(forget gate AND analogy)", "machine learning +(\"forget gate is like\")", "machine learning +(\"forget gate is similar\")", "machine learning +(\"just as forget gate\")", "machine learning +(\"forget gate can be thought of as\")", "machine learning +(\"forget gate can be compared to\")"]}