{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Common <b>Loss</b> functions in machine learning | by Ravindra Parmar ...", "url": "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/common-<b>loss</b>-functions-in-machine-learning-46af0ffc4d23", "snippet": "Common <b>Loss</b> functions in machine learning. Machines learn by means of a <b>loss function</b>. It\u2019s a method of evaluating how well specific algorithm models the given data. If predictions deviates too much from <b>actual</b> results, <b>loss function</b> would cough up a very large number. Gradually, with the help of some optimization function, <b>loss function</b> ...", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "<b>Squared</b> hinge <b>loss</b> fits perfect for YES OR NO kind of decision problems, where probability deviation is not the concern. 3. Multi-class Classification <b>Loss</b> Functions. Multi-class classification is the predictive models in which the data points are assigned to more than two classes. <b>Each</b> class is assigned a unique value from 0 to (Number_of_classes \u2013 1). It is highly recommended for image or text classification problems, where a single paper can have multiple topics. Multi-class Cross ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Top Data Science Interview Questions and Answers | by Dr. Dataman ...", "url": "https://medium.com/dataman-in-ai/top-data-science-interview-questions-and-answers-b1395fc2e1e4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/dataman-in-ai/top-data-science-interview-questions-and-answers-b...", "snippet": "A good way to remember \u201c<b>L1</b>\u201d is \u201c1\u201d looks <b>like</b> the symbol for the absolute value. RIDGE \u2014 L2: It adds the sum of the <b>squared</b> value of coefficients to the <b>loss</b> function SSE. How do we ...", "dateLastCrawled": "2022-01-29T12:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "3.3. Metrics and scoring: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "The hinge_<b>loss</b> function computes the <b>average</b> <b>distance</b> <b>between</b> the model and the data using hinge <b>loss</b>, a one-sided metric that considers only <b>prediction</b> errors. (Hinge <b>loss</b> is used in maximal margin classifiers such as support vector machines.) If the labels are encoded with +1 and -1, \\(y\\): is the true value, and \\(w\\) is the predicted decisions as output by decision_function, then the hinge <b>loss</b> is defined as: \\[L_\\text{Hinge}(y, w) = \\max\\left\\{1 - wy, 0\\right\\} = \\left|1 - wy\\right ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "From Linear <b>Regression</b> to Ridge <b>Regression</b>, the Lasso, and the Elastic ...", "url": "https://towardsdatascience.com/from-linear-regression-to-ridge-regression-the-lasso-and-the-elastic-net-4eaecaf5f7e6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/from-linear-<b>regression</b>-to-ridge-<b>regression</b>-the-lasso...", "snippet": "The lines indicate the <b>distance</b> from our <b>prediction</b> to the <b>actual</b> observed data. The sum of the squares of these distances defines our cost in least squares. Image Citation: This image is used with permission and appears as Figure 3.1 in Elements of Statistical Learning, II edition. Typically, this is defined over an example as the <b>loss</b> function. For the whole training data, we work with the cost function, which is the <b>average</b> of losses over <b>each</b> training example. To find the minimum of the ...", "dateLastCrawled": "2022-02-02T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Neural networks are trained using stochastic gradient descent and require that you choose a <b>loss</b> function when designing and configuring your model. There are many <b>loss</b> functions to choose from and it can be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when training a neural network. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "29 Data Science Interview Questions And Answers", "url": "https://www.naukri.com/blog/29-interview-questions-and-answers-for-data-science/", "isFamilyFriendly": true, "displayUrl": "https://www.naukri.com/blog/29-interview-questions-and-answers-for-data-science", "snippet": "Within the sum of squares (WSS), it is defined as the sum of the <b>squared</b> <b>distance</b> <b>between</b> <b>each</b> member of the cluster and its centroid. 10. \u2018People who bought this also bought\u2026\u2019 recommendations seen on Amazon are a result of which algorithm? The recommendation engine is achieved through collaborative filtering. This filter helps to explain the user behavior and their purchase history in terms of selection, ratings etc. The engine then predicts the interest of the person based on the ...", "dateLastCrawled": "2022-02-01T21:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How <b>do Regression Trees Work? | DataDrivenInvestor</b>", "url": "https://www.datadriveninvestor.com/2020/04/13/how-do-regression-trees-work/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>datadriveninvestor</b>.com/2020/04/13/how-do-regression-trees-work", "snippet": "To investigate this problem we run a clinical trial with different dosages and measure how effective <b>each</b> dosage is. In the end we would <b>like</b> to accurately predict the efficiency of the drug at a certain dosage level. Linear Regression . If we plot the results of the clinical trial in some hypothetical scenario, the data points may look similar to the graph below. Plot A. The data points in the plot above (Plot A) indicates that there is some positive correlation <b>between</b> drug dosage and drug ...", "dateLastCrawled": "2022-02-03T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Loss</b> <b>L1</b> Keras [JPT59W]", "url": "https://request.to.it/L1_Loss_Keras.html", "isFamilyFriendly": true, "displayUrl": "https://request.to.it/<b>L1</b>_<b>Loss</b>_Keras.html", "snippet": "Mean Absolute error, also known as <b>L1</b> Error, is defined as the <b>average</b> of the absolute differences <b>between</b> the <b>actual</b> value and the predicted value. #Softmax for multiclass probability ]) le = LabelEncoder() model. A regularizer that applies a <b>L1</b> regularization penalty. 4 of Keras (at least with Tensorflow backend). The focal <b>loss</b> of y_pred w. TensorFlow is a brilliant tool, with lots of power and flexibility. Note: When you add a regularization function to a model, you might need to tweak ...", "dateLastCrawled": "2022-01-29T05:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "GitHub - jeevu94/ml-notes: Personal notes on ML", "url": "https://github.com/jeevu94/ml-notes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jeevu94/ml-notes", "snippet": "Sparse Autoencoder constraint the reconstruction of autoencoder by imposing a constraint in its <b>loss</b>, e.g. using sparsity <b>loss</b> or <b>L1</b> regularization. Sparsity <b>loss</b> is more involved: Compute the <b>average</b> activation of <b>each</b> neuron in the coding layer, over the whole training batch. e.g. we feed 500 inputs into the network, then measure how many ...", "dateLastCrawled": "2022-01-20T20:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "From Linear <b>Regression</b> to Ridge <b>Regression</b>, the Lasso, and the Elastic ...", "url": "https://towardsdatascience.com/from-linear-regression-to-ridge-regression-the-lasso-and-the-elastic-net-4eaecaf5f7e6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/from-linear-<b>regression</b>-to-ridge-<b>regression</b>-the-lasso...", "snippet": "The lines indicate the <b>distance</b> from our <b>prediction</b> to the <b>actual</b> observed data. The sum of the squares of these distances defines our cost in least squares. Image Citation: This image is used with permission and appears as Figure 3.1 in Elements of Statistical Learning, II edition. Typically, this is defined over an example as the <b>loss</b> function. For the whole training data, we work with the cost function, which is the <b>average</b> of losses over <b>each</b> training example. To find the minimum of the ...", "dateLastCrawled": "2022-02-02T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Deep Learning with Dynamically Weighted <b>Loss</b> Function for Sensor-Based ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7038523/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7038523", "snippet": "The first probability distribution is the <b>actual</b> class where the known class label has a probability of 1.0 and there is a probability of 0.0 for all other class labels. Subsequently, the second probability distribution is the predicted probability for <b>each</b> class. The CE <b>loss</b> function for binary classification can be represented mathematically as follows: C E (y, p) = \u2212 (y \u2217 l o g (p) + (1 \u2212 y) \u2217 l o g (1 \u2212 p)) (12) where p is the deep learning model probabilistic output that ...", "dateLastCrawled": "2022-01-25T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Implementing <b>loss</b> functions | Machine Learning Using TensorFlow Cookbook", "url": "https://subscription.packtpub.com/book/data/9781800208865/2/ch02lvl1sec14/implementing-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://subscription.packtpub.com/.../2/ch02lvl1sec14/implementing-<b>loss</b>-functions", "snippet": "The <b>loss function</b> compares the target to the <b>prediction</b> (it measures the <b>distance</b> <b>between</b> the model outputs and the target truth values) and provides a numerical quantification <b>between</b> the two. Getting ready . We will first start a computational graph and load matplotlib, a Python plotting package, as follows: import matplotlib.pyplot as plt import TensorFlow as tf Copy. Now that we are ready to plot, let&#39;s proceed to the recipe without further ado. How to do it... First, we will talk about ...", "dateLastCrawled": "2022-01-21T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "3.3. Model <b>Evaluation</b> - Scikit-learn - W3cubDocs", "url": "https://docs.w3cub.com/scikit_learn/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://docs.w3cub.com/scikit_learn/modules/model_<b>evaluation</b>.html", "snippet": "The hinge_<b>loss</b> function computes the <b>average</b> <b>distance</b> <b>between</b> the model and the data using hinge <b>loss</b>, a one-sided metric that considers only <b>prediction</b> errors. (Hinge <b>loss</b> is used in maximal margin classifiers such as support vector machines.) If the labels are encoded with +1 and -1, \\(y\\): is the true value, and \\(w\\) is the predicted decisions as output by decision_function, then the hinge <b>loss</b> is defined as: \\[L_\\text{Hinge}(y, w) = \\max\\left\\{1 - wy, 0\\right\\} = \\left|1 - wy\\right ...", "dateLastCrawled": "2022-01-28T16:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Top Data Science Interview Questions and Answers | by Dr. Dataman ...", "url": "https://medium.com/dataman-in-ai/top-data-science-interview-questions-and-answers-b1395fc2e1e4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/dataman-in-ai/top-data-science-interview-questions-and-answers-b...", "snippet": "A good way to remember \u201c<b>L1</b>\u201d is \u201c1\u201d looks like the symbol for the absolute value. RIDGE \u2014 L2: It adds the sum of the <b>squared</b> value of coefficients to the <b>loss</b> function SSE. How do we ...", "dateLastCrawled": "2022-01-29T12:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Neural networks are trained using stochastic gradient descent and require that you choose a <b>loss</b> function when designing and configuring your model. There are many <b>loss</b> functions to choose from and it can be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when training a neural network. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "29 Data Science Interview Questions And Answers", "url": "https://www.naukri.com/blog/29-interview-questions-and-answers-for-data-science/", "isFamilyFriendly": true, "displayUrl": "https://www.naukri.com/blog/29-interview-questions-and-answers-for-data-science", "snippet": "Within the sum of squares (WSS), it is defined as the sum of the <b>squared</b> <b>distance</b> <b>between</b> <b>each</b> member of the cluster and its centroid. 10. \u2018People who bought this also bought\u2026\u2019 recommendations seen on Amazon are a result of which algorithm? The recommendation engine is achieved through collaborative filtering. This filter helps to explain the user behavior and their purchase history in terms of selection, ratings etc. The engine then predicts the interest of the person based on the ...", "dateLastCrawled": "2022-02-01T21:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How <b>do Regression Trees Work? | DataDrivenInvestor</b>", "url": "https://www.datadriveninvestor.com/2020/04/13/how-do-regression-trees-work/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>datadriveninvestor</b>.com/2020/04/13/how-do-regression-trees-work", "snippet": "The <b>average</b> dosage <b>between</b> those two patients is 3 mg. We draw a vertical line at the point 3 to indicate a split in our data. The two dots which are highlighted red represent the two smallest dosages. The red dotted line splits the data into two parts. The next step is to calculate the <b>average</b> efficiency of the observations on the left and right hand side of the red dotted line. On the left hand side (less than 3 mg), there is only one observation, which results in an <b>average</b> of 0%. On the ...", "dateLastCrawled": "2022-02-03T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>Mean Squared Error, Mean Absolute Error</b>, Root Mean <b>Squared</b> ...", "url": "https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared", "isFamilyFriendly": true, "displayUrl": "https://www.studytonight.com/post/what-is-<b>mean-squared-error-mean-absolute-error</b>-root...", "snippet": "4. R <b>Squared</b>. It is also known as the coefficient of determination.This metric gives an indication of how good a model fits a given dataset. It indicates how close the regression line (i.e the predicted values plotted) is to the <b>actual</b> data values. The R <b>squared</b> value lies <b>between</b> 0 and 1 where 0 indicates that this model doesn&#39;t fit the given data and 1 indicates that the model fits perfectly to the dataset provided.. import numpy as np X = np.random.randn(100) y = np.random.randn(60) # y ...", "dateLastCrawled": "2022-02-03T03:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Develop k-<b>Nearest Neighbors in Python From Scratch</b>", "url": "https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python", "snippet": "When a <b>prediction</b> is required, the k-most <b>similar</b> records to a new record from the training dataset are then located. From these neighbors, a summarized <b>prediction</b> is made. Similarity <b>between</b> records can be measured many different ways. A problem or data-specific method can be used. Generally, with tabular data, a good starting point is the Euclidean <b>distance</b>. Once the neighbors are discovered, the summary <b>prediction</b> can be made by returning the most common <b>outcome</b> or taking the <b>average</b>. As ...", "dateLastCrawled": "2022-02-02T10:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: Data ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "Precision <b>can</b> <b>be thought</b> of as a measure of a classifiers exactness. A low precision <b>can</b> also indicate a large number of False Positives. Recall: A measure of a classifiers completeness. Recall is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called Sensitivity or the True Positive Rate. Recall <b>can</b> be ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "4. Regression and <b>Prediction</b> - <b>Practical Statistics for Data Scientists</b> ...", "url": "https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/ch04.html", "snippet": "Nowhere is the nexus <b>between</b> statistics and data science stronger than in the realm of <b>prediction</b>\u2014specifically the <b>prediction</b> of an <b>outcome</b> (target) variable based on the values of other \u201cpredictor\u201d variables. Another important connection is in the area of anomaly detection, where regression diagnostics originally intended for data analysis and improving the regression model <b>can</b> be used to detect unusual records. The antecedents of correlation and linear regression date back over a ...", "dateLastCrawled": "2022-02-03T02:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Support Vector Machine</b> \u2014 Introduction to Machine Learning Algorithms ...", "url": "https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>support-vector-machine</b>-introduction-to-machine-learning...", "snippet": "Hinge <b>loss</b> function (function on left <b>can</b> be represented as a function on the right) The cost is 0 if the predicted value <b>and the actual</b> value are of the same sign. If they are not, we then calculate the <b>loss</b> value. We also add a regularization parameter the cost function. The objective of the regularization parameter is to balance the margin maximization and <b>loss</b>. After adding the regularization parameter, the cost functions looks as below. <b>Loss</b> function for SVM. Now that we have the <b>loss</b> ...", "dateLastCrawled": "2022-02-03T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. Cross-entropy is commonly used in machine learning as a <b>loss</b> function. Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference <b>between</b> two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy <b>between</b> two probability distributions, whereas cross-entropy <b>can</b> <b>be thought</b> to calculate the total entropy <b>between</b> the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "3.3. Model evaluation: quantifying the quality of predictions \u2014 scikit ...", "url": "https://thomasjpfan.github.io/scikit-learn-website/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://thomasjpfan.github.io/scikit-learn-website/modules/model_evaluation.html", "snippet": "The <b>actual</b> <b>outcome</b> has to be 1 or 0 (true or false), while the predicted probability of the <b>actual</b> <b>outcome</b> <b>can</b> be a value <b>between</b> 0 and 1. The brier score <b>loss</b> is also <b>between</b> 0 to 1 and the lower the score (the mean square difference is smaller), the more accurate the <b>prediction</b> is. It <b>can</b> <b>be thought</b> of as a measure of the \u201ccalibration\u201d of ...", "dateLastCrawled": "2021-12-12T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fathoming the Deep in <b>Deep Learning \u2013 A Practical Approach</b> \u2013 thoughtful ...", "url": "https://avantlive.wordpress.com/2019/04/29/fathoming-the-deep-in-deep-learning-a-practical-approach/", "isFamilyFriendly": true, "displayUrl": "https://avantlive.wordpress.com/2019/04/29/fathoming-the-deep-in-deep-learning-a...", "snippet": "Widely used in regression, the resultant fitting line for data points should be a line which minimizes the sum of <b>distance</b> of <b>each</b> point to the fitted regression line. Square difference <b>between</b> <b>actual</b> and predicted is named \u2018residual\u2019 and the target of <b>loss</b> function is to minimize the residual sum of squares in DL. Half Mean <b>Squared</b> Error", "dateLastCrawled": "2022-01-21T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "CS231n Convolutional Neural Networks for Visual Recognition", "url": "https://cs231n.github.io/linear-classify/", "isFamilyFriendly": true, "displayUrl": "https://cs231n.github.io/linear-classify", "snippet": "Compute the multiclass svm <b>loss</b> for a single example (x,y) - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick) - y is an integer giving index of correct class (e.g. <b>between</b> 0 and 9 in CIFAR-10) - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10) &quot;&quot;&quot; delta = 1.0 # see notes about delta later in this section scores = W. dot (x) # scores becomes of size 10 x 1, the scores for <b>each</b> class correct ...", "dateLastCrawled": "2022-02-02T19:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Prediction Modeling Using EHR Data</b>: Challenges, Strategies ...", "url": "https://journals.lww.com/lww-medicalcare/Fulltext/2010/06001/Prediction_Modeling_Using_EHR_Data__Challenges,.17.aspx", "isFamilyFriendly": true, "displayUrl": "https://<b>journals.lww.com</b>/lww-medicalcare/Fulltext/2010/06001/<b>Prediction</b>_Modeling_Using...", "snippet": "The latter <b>can</b> <b>be thought</b> of as an interaction <b>between</b> whether the value was obtained and the value itself. This type of variable is \u201cobserved\u201d for all subjects, and takes a value of 0 if the laboratory was never measured. For example, as candidate variables in the <b>prediction</b> model we would include an indicator that hemoglobin A1c was ordered and the value of A1c among people who had it ordered. Machine Learning Classification Techniques. We compared SVM, Boosting, and logistic ...", "dateLastCrawled": "2022-01-19T13:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Develop k-<b>Nearest Neighbors in Python From Scratch</b>", "url": "https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python", "snippet": "Similarity <b>between</b> records <b>can</b> be measured many different ways. A problem or data-specific method <b>can</b> be used. Generally, with tabular data, a good starting point is the Euclidean <b>distance</b>. Once the neighbors are discovered, the summary <b>prediction</b> <b>can</b> be made by returning the most common <b>outcome</b> or taking the <b>average</b>. As such, KNN <b>can</b> be used ...", "dateLastCrawled": "2022-02-02T10:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Theoretical <b>interview questions</b>", "url": "https://ds-interviews.org/theory.html", "isFamilyFriendly": true, "displayUrl": "https://ds-interviews.org/theory.html", "snippet": "In order to rate similar experiences with a higher weight, we <b>can</b> introduce a similarity <b>between</b> users that we use as a multiplier for <b>each</b> rating. Also, as users have an individual profile, one user may have an <b>average</b> rating much larger than another user, so we use normalization techniques (e.g. centering or Z-score normalization) to remove the users\u2019 biases.", "dateLastCrawled": "2022-02-02T23:39:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "<b>Squared</b> hinge <b>loss</b> fits perfect for YES OR NO kind of decision problems, where probability deviation is not the concern. 3. Multi-class Classification <b>Loss</b> Functions. Multi-class classification is the predictive models in which the data points are assigned to more than two classes. <b>Each</b> class is assigned a unique value from 0 to (Number_of_classes \u2013 1). It is highly recommended for image or text classification problems, where a single paper <b>can</b> have multiple topics. Multi-class Cross ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "From Linear <b>Regression</b> to Ridge <b>Regression</b>, the Lasso, and the Elastic ...", "url": "https://towardsdatascience.com/from-linear-regression-to-ridge-regression-the-lasso-and-the-elastic-net-4eaecaf5f7e6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/from-linear-<b>regression</b>-to-ridge-<b>regression</b>-the-lasso...", "snippet": "The lines indicate the <b>distance</b> from our <b>prediction</b> to the <b>actual</b> observed data. The sum of the squares of these distances defines our cost in least squares. Image Citation: This image is used with permission and appears as Figure 3.1 in Elements of Statistical Learning, II edition. Typically, this is defined over an example as the <b>loss</b> function. For the whole training data, we work with the cost function, which is the <b>average</b> of losses over <b>each</b> training example. To find the minimum of the ...", "dateLastCrawled": "2022-02-02T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Implementing <b>loss</b> functions | Machine Learning Using TensorFlow Cookbook", "url": "https://subscription.packtpub.com/book/data/9781800208865/2/ch02lvl1sec14/implementing-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://subscription.packtpub.com/.../2/ch02lvl1sec14/implementing-<b>loss</b>-functions", "snippet": "The <b>loss function</b> compares the target to the <b>prediction</b> (it measures the <b>distance</b> <b>between</b> the model outputs and the target truth values) and provides a numerical quantification <b>between</b> the two. Getting ready . We will first start a computational graph and load matplotlib, a Python plotting package, as follows: import matplotlib.pyplot as plt import TensorFlow as tf Copy. Now that we are ready to plot, let&#39;s proceed to the recipe without further ado. How to do it... First, we will talk about ...", "dateLastCrawled": "2022-01-21T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Common <b>Loss</b> functions in machine learning | by Ravindra Parmar ...", "url": "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/common-<b>loss</b>-functions-in-machine-learning-46af0ffc4d23", "snippet": "Common <b>Loss</b> functions in machine learning. Machines learn by means of a <b>loss function</b>. It\u2019s a method of evaluating how well specific algorithm models the given data. If predictions deviates too much from <b>actual</b> results, <b>loss function</b> would cough up a very large number. Gradually, with the help of some optimization function, <b>loss function</b> ...", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "Cross-entropy will calculate a score that summarizes the <b>average</b> difference <b>between</b> the <b>actual</b> and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0. Cross-entropy <b>can</b> be specified as the <b>loss</b> function in Keras by specifying \u2018binary_crossentropy\u2018 when compiling the model.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: Data ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "The difference <b>between</b> the <b>L1</b>(Lasso) and L2(Ridge) is just that L2(Ridge) is the sum of the square of the weights, while <b>L1</b>(Lasso) is just the sum of the absolute weights in MSE or another <b>loss</b> function. As follows: The difference <b>between</b> their properties <b>can</b> be promptly summarized as follows: Solution uniqueness is a simpler case but requires a bit of imagination. First, this picture below: 4. How would you validate a model you created to generate a predictive model of a quantitative ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Neural networks are trained using stochastic gradient descent and require that you choose a <b>loss</b> function when designing and configuring your model. There are many <b>loss</b> functions to choose from and it <b>can</b> be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when training a neural network. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "3.3. Metrics and scoring: quantifying the quality of predictions ...", "url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/model_evaluation.html", "snippet": "The hinge_<b>loss</b> function computes the <b>average</b> <b>distance</b> <b>between</b> the model and the data using hinge <b>loss</b>, a one-sided metric that considers only <b>prediction</b> errors. (Hinge <b>loss</b> is used in maximal margin classifiers such as support vector machines.) If the labels are encoded with +1 and -1, \\(y\\): is the true value, and \\(w\\) is the predicted decisions as output by decision_function, then the hinge <b>loss</b> is defined as: \\[L_\\text{Hinge}(y, w) = \\max\\left\\{1 - wy, 0\\right\\} = \\left|1 - wy\\right ...", "dateLastCrawled": "2022-02-03T06:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "4. Regression and <b>Prediction</b> - <b>Practical Statistics for Data Scientists</b> ...", "url": "https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/ch04.html", "snippet": "Nowhere is the nexus <b>between</b> statistics and data science stronger than in the realm of <b>prediction</b>\u2014specifically the <b>prediction</b> of an <b>outcome</b> (target) variable based on the values of other \u201cpredictor\u201d variables. Another important connection is in the area of anomaly detection, where regression diagnostics originally intended for data analysis and improving the regression model <b>can</b> be used to detect unusual records. The antecedents of correlation and linear regression date back over a ...", "dateLastCrawled": "2022-02-03T02:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "Variance is the <b>average</b> degree to which <b>each</b> point differs from the mean i.e. the <b>average</b> of all data points. We <b>can</b> relate Standard deviation and Variance because it is the square root of Variance. 14. A data set is given to you and it has missing values which spread along 1 standard deviation from the mean. How much of the data would remain untouched? It is given that the data is spread across mean that is the data is spread across an <b>average</b>. So, we <b>can</b> presume that it is a normal ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Regularization \u2014 Understanding <b>L1</b> and L2 regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-<b>l1</b>-and-l2...", "snippet": "Understanding what regularization is and why it is required for <b>machine</b> <b>learning</b> and diving deep to clarify the importance of <b>L1</b> and L2 regularization in Deep <b>learning</b>.", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, the ... Mean Absolute Error, <b>L1</b> <b>Loss</b> (used by PerceptiLabs\u2019 Regression component): sums the absolute differences between the predictions and ground truth, and finds the average. <b>Loss</b> functions are used in a variety of use cases. The following table shows common image processing use cases where you might apply these, and other <b>loss</b> functions: Image Source: PerceptiLabs <b>Loss</b> in PL. Configuring a <b>loss</b> ...", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "is known as <b>L1</b>-norm, while the latter is known as the L2-norm. Keep in mind that L2-norm is more sensitive than <b>L1</b>-norm to large-valued outliers. Ridge and LASSO regularizations are based on L2-norm and <b>L1</b>-norm, respectively, while Elastic Net regularization is based on the mix of two. 2.6 What does a <b>machine</b> <b>learning</b> <b>learning</b>-curve measure ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "regression - Why <b>L1</b> norm for sparse models - Cross Validated", "url": "https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/45643", "snippet": "Show activity on this post. With a sparse model, we think of a model where many of the weights are 0. Let us therefore reason about how <b>L1</b>-regularization is more likely to create 0-weights. Consider a model consisting of the weights . With <b>L1</b> regularization, you penalize the model by a <b>loss</b> function = .", "dateLastCrawled": "2022-01-26T23:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - <b>L1</b>-norm vs l2-norm as cost function when ...", "url": "https://stackoverflow.com/questions/43301036/l1-norm-vs-l2-norm-as-cost-function-when-standardizing", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43301036", "snippet": "I feel that the l2-norm will penalize less the model than the <b>l1</b>-norm since squaring a number that is between 0 and 1 will always result in a lower number. So my question is, is it ok to use the l2-norm when both the input and the output are standardized? <b>machine</b>-<b>learning</b> statistics gradient-descent. Share. Follow edited Apr 10 &#39;17 at 12:08. jeremie. asked Apr 8 &#39;17 at 22:34. jeremie jeremie. 807 8 8 silver badges 15 15 bronze badges. Add a comment | 1 Answer Active Oldest Votes. 1 It does ...", "dateLastCrawled": "2022-01-24T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0888613X21000141", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0888613X21000141", "snippet": "The <b>machine</b> <b>learning</b> approaches outperform state of the art approaches on Google, BATS and DiffVec datasets. As far as we know, neither <b>analogy</b> classification nor <b>analogy</b> completion have been investigated in the same way as we have proposed in this paper, namely <b>learning</b> a model, instead of starting from the parallelogram model. The paper is structured as follows. Section 2 recalls the postulates characterizing analogical proportions and identifies a rigorous method for enlarging a set of ...", "dateLastCrawled": "2021-11-13T01:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Gentle Introduction to Pix2Pix Generative</b> Adversarial Network", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-pix2pix-generative</b>...", "snippet": "In <b>analogy</b> to automatic language translation, we define automatic image-to-image translation as the task of translating one possible representation of a scene into another, given sufficient training data. \u2014 Image-to-Image Translation with Conditional Adversarial Networks, 2016. It is a challenging problem that typically requires the development of a specialized model and hand-crafted <b>loss</b> function for the type of translation task being performed. Classical approaches use per-pixel ...", "dateLastCrawled": "2022-02-02T13:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Deep learning</b> - lectures.alex.balgavy.eu", "url": "https://lectures.alex.balgavy.eu/ml-notes/deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://lectures.alex.balgavy.eu/ml-notes/<b>deep-learning</b>", "snippet": "<b>Deep learning</b> <b>Deep learning</b> systems (autodiff engines) Tensors. To scale up backpropagation, want to move from operations on scalars to tensors. Tensor: generalisation of vectors/matrices to higher dimensions. e.g. a 2-tensor has two dimensions, a 4-tensor has 4 dimensions. You can represent data as a tensor. e.g. an RGB image is a 3-tensor of the red, green, and blue values for each pixel. Functions on tensors. Functions have inputs and outputs, all of which are tensors. They implement ...", "dateLastCrawled": "2021-12-15T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Denoising Seismic Records with Image Translation Networks</b> | CSEG RECORDER", "url": "https://csegrecorder.com/articles/view/denoising-seismic-records-with-image-translation-networks", "isFamilyFriendly": true, "displayUrl": "https://csegrecorder.com/articles/view/<b>denoising-seismic-records-with-image</b>...", "snippet": "The pix2pix network is a generative <b>machine</b> <b>learning</b> algorithm. Based on Alec Radford, et. al\u2019s DCGAN [6] architecture, ... the <b>L1 loss is similar</b> to the L2 loss: except the second-degree norm is replaced with the first-degree norm: The alternative denoising strategies tested against the image translation network included total-variation filtering, bilateral filtering, and wavelet transform filtering. Figure 3. Results of various image denoising techniques on synthetic data. Upper left ...", "dateLastCrawled": "2022-01-12T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A deep <b>learning</b> framework for constitutive modeling based on temporal ...", "url": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "snippet": "These two features meet the requirement for sequence modeling in <b>machine</b> <b>learning</b>. Therefore, the nonlinear constitutive models may be classified as sequence modeling from the viewpoint of <b>machine</b> <b>learning</b>. Concrete material and steel material both exhibit significant ultra-long-term memory effects and many model-driven constitutive relationships were developed to simulate stress-strain curves of materials , , , , with ultra-long-term memory effect. For steel material, the traditional ...", "dateLastCrawled": "2022-01-20T12:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(l1 loss)  is like +(average distance between each prediction and the actual outcome, squared)", "+(l1 loss) is similar to +(average distance between each prediction and the actual outcome, squared)", "+(l1 loss) can be thought of as +(average distance between each prediction and the actual outcome, squared)", "+(l1 loss) can be compared to +(average distance between each prediction and the actual outcome, squared)", "machine learning +(l1 loss AND analogy)", "machine learning +(\"l1 loss is like\")", "machine learning +(\"l1 loss is similar\")", "machine learning +(\"just as l1 loss\")", "machine learning +(\"l1 loss can be thought of as\")", "machine learning +(\"l1 loss can be compared to\")"]}