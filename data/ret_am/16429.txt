{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Perceptual learning in object recognition</b>: object specificity and <b>size</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0042698999001340", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0042698999001340", "snippet": "<b>Size</b> <b>invariance</b> implies that learning may be happening later in cortex than is required by the task alone. For example, improved recognition could in principle result from an increased signal-to-noise ratio in V1 neurons that encode a specific image. Such learning would not show <b>size</b> <b>invariance</b>, however, because large and small images of the same objects are encoded by different populations of V1 neurons. Hence, our results do not support the idea that learning always happens as early as ...", "dateLastCrawled": "2021-12-28T16:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Position, rotation, scale, and <b>orientation</b> invariant object tracking ...", "url": "https://www.deepdyve.com/lp/spie/position-rotation-scale-and-orientation-invariant-object-tracking-from-2f0yN70LFR", "isFamilyFriendly": true, "displayUrl": "https://www.<b>deepdyve</b>.com/lp/spie/position-rotation-scale-and-<b>orientation</b>-invariant...", "snippet": "A method of tracking objects in video sequences despite any kind of perspective distortion is demonstrated. Moving objects are initially segmented from the scene using a background subtraction method to minimize the search area of the filter. A variation on the Maximum Average Correlation Height (MACH) filter is used to create <b>invariance</b> to <b>orientation</b> while giving high tolerance to background clutter and noise. A log r-\u0952 mapping is employed to give <b>invariance</b> to in-plane rotation and ...", "dateLastCrawled": "2020-11-15T04:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Visual Pattern Recognition in Drosophila Is Invariant for Retinal Position", "url": "https://www.science.org/doi/10.1126/science.1099839", "isFamilyFriendly": true, "displayUrl": "https://www.science.org/doi/10.1126/science.1099839", "snippet": "<b>Like</b> translation <b>invariance</b>, edge <b>orientation</b> is a further basic property that is shared between the visual systems of Drosophila and larger animals. Finally, flies also evaluate relational cues such as {A above B} versus {B above A}. So far this fascinating ability has been demonstrated only for two colors (blue and green) and two edge orientations (+45\u00b0 and \u201345\u00b0). The negative outcome of the experiment with two horizontally arranged oblique bars in the flight simulator cannot be ...", "dateLastCrawled": "2022-01-30T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Capsule Networks", "url": "https://cedar.buffalo.edu/~srihari/CSE676/9.12%20CapsuleNets.pdf", "isFamilyFriendly": true, "displayUrl": "https://cedar.buffalo.edu/~srihari/CSE676/9.12 CapsuleNets.pdf", "snippet": "Pooling and <b>Invariance</b> \u2022Pooling is supposed to obtain positional, orientational, proportional or rotational <b>invariance</b>. \u2022But it is a very crude approach \u2022In reality it removes all sorts of positional <b>invariance</b> Every input value changed, but only half the output values have changed because maxpoolis only sensitive to max value in neighborhood not exact value Original input Translated input. Deep Learning Srihari Example of CNN Limitation \u2022CNN to recognize faces extracts features from ...", "dateLastCrawled": "2022-01-27T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Histograms of Oriented Gradients", "url": "https://courses.cs.duke.edu/compsci527/fall15/notes/hog.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/compsci527/fall15/notes/hog.pdf", "snippet": "The actual <b>size</b> of the window\u2014using multiples of 64 is a mere convenience\u2014re\ufb02ects the assumption that if the image of the person is signi\ufb01cantly smaller than that then resolution might be insuf\ufb01cient to detect it reliably, so it is not even worth trying. The person could of course occupy a bigger part of the image. Because of this, one analyzes not only the original image, but also those in a pyramid of images: A 128 by 64 <b>rectangle</b> at a coarser level of the pyramid corresponds to ...", "dateLastCrawled": "2022-01-28T01:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Scale invariant feature transform</b>", "url": "https://www.slideshare.net/MohammadAsgharBarech/scale-invariant-feature-transform-62714139", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/MohammadAsgharBarech/<b>scale-invariant-feature-transform</b>-62714139", "snippet": "This ensures rotation <b>invariance</b>. The <b>size</b> of the &quot;<b>orientation</b> collection region&quot; around the keypoint depends on it&#39;s scale. The bigger the scale, the bigger the collection region. The details Now for the little details about collecting orientations. 17. 17 Gradient magnitudes and orientations are calculated using these formulae: The magnitude and <b>orientation</b> is calculated for all pixels around the keypoint. Then, A histogram is created for this. In this histogram, the 360 degrees of ...", "dateLastCrawled": "2022-02-01T16:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Electronics | Free Full-Text | SR-SYBA: A Scale and Rotation Invariant ...", "url": "https://www.mdpi.com/2079-9292/9/5/810/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2079-9292/9/5/810/htm", "snippet": "Based on this <b>orientation</b> representation, all feature regions are rotated to the same reference <b>orientation</b> to provide rotation <b>invariance</b>. The original SYBA descriptor is then applied to the scale and <b>orientation</b> normalized feature regions for description and matching. Experiment results show that SR-SYBA greatly improves SYBA for image matching applications with scaling and rotation variations. SR-SYBA obtains comparable or better performance in terms of matching rate compared to the ...", "dateLastCrawled": "2021-10-23T20:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Invariant foreground occupation ratio for scale adaptive mean shift ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-cvi.2014.0150", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-cvi.2014.0150", "snippet": "With its property of scale <b>invariance</b>, an iterative approximation approach is employed to estimate the scale of the foreground in the current image. The scale value is modified by a weighting function, and it is adjusted along the two axes with respect to the width and the height of the target. The scale estimation algorithm is then employed in the mean shift tracker to obtain the ability of scale adaptation for tracking. Experimental results show that, using the authors method for object ...", "dateLastCrawled": "2021-12-17T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "c++ - Template Matching for Coins with OpenCV - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/32692010/template-matching-for-coins-with-opencv", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/32692010", "snippet": "This methods does not directly support scale or <b>orientation</b> <b>invariance</b>. But it is possible to overcome that by scaling candidates to a reference <b>size</b> and by testing against several rotated templates. A detailed example of this technique is shown to detect pressence and location of 50c coins. The same procedure can be applied to the other coins. Two programs will be built. One to create templates from the big image template for the 50c coin. And another one which will take as input those ...", "dateLastCrawled": "2022-01-23T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "computer vision - OpenCV <b>how to calculate SIFT descriptor</b> at a given ...", "url": "https://stackoverflow.com/questions/39263646/opencv-how-to-calculate-sift-descriptor-at-a-given-pixel", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/39263646", "snippet": "Original paper by David G. Lowe (Distinctive Image Features from Scale-Invariant Keypoints) says, &quot;In order to achieve <b>orientation</b> <b>invariance</b>, the coordinates of the descriptor and the gradient orientations are rotated relative to the Keypoint <b>orientation</b>&quot;. while the scale information is used for selecting the level of Gaussian blur for the image during the calculation of descriptor.", "dateLastCrawled": "2022-01-19T00:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Size</b> and position <b>invariance</b> in the visual system", "url": "https://cavlab.net/MyRWResources/MyPDFs/PDF1971-79/Cavanagh1978.pdf", "isFamilyFriendly": true, "displayUrl": "https://cavlab.net/MyRWResources/MyPDFs/PDF1971-79/Cavanagh1978.pdf", "snippet": "<b>size</b> <b>invariance</b> is thus additionally complicated compared to a feature detection approach, as the type of analysis based on the connectedness of the elements in a pattern no longer applies. Since there is no a priori pattern unity in the Fourier domain, recognition, even without <b>size</b> <b>invariance</b>, requires comparison against a known standard. The transform of the pattern being searched for is matched against the transform of the input. This template approach, often called matched filtering ...", "dateLastCrawled": "2021-11-27T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "FUNCTIONAL <b>SIZE</b> <b>INVARIANCE</b> IS NOT PROVIDED BY THE CORTICAL ...", "url": "https://cavlab.net/MyRWResources/MyPDFs/PDF1982/Cavanagh1982.pdf", "isFamilyFriendly": true, "displayUrl": "https://cavlab.net/MyRWResources/MyPDFs/PDF1982/Cavanagh1982.pdf", "snippet": "Functional <b>size</b> <b>invariance</b> 1411 \u2018A Ah <b>ORIENTATION</b> 2 d t .3 - .2 - , 1 t c 0 90 180 270 <b>ORIENTATION</b> Fig. 1. (a) An upright triangle at four equidistant points about the visual origin (fovea): all four triangles", "dateLastCrawled": "2021-08-29T17:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Size-distance invariance: kinetic invariance is different</b> from ...", "url": "https://www.researchgate.net/publication/21534299_Size-distance_invariance_kinetic_invariance_is_different_from_static_invariance", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/21534299_<b>Size</b>-distance_<b>invariance</b>_kinetic...", "snippet": "A proposed kinetic <b>invariance</b> hypothesis asserts that a changing proximal stimulus <b>size</b> (an expanding or contracting solid visual angle) produces a constant perceived <b>size</b> and a changing perceived ...", "dateLastCrawled": "2021-11-03T00:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Rotation Invariance &amp; Equivariance</b> - Home", "url": "https://kobiso.github.io/research/research-rotation/", "isFamilyFriendly": true, "displayUrl": "https://kobiso.github.io/research/research-rotation", "snippet": "Figure: An ARF is a filter of the <b>size</b> W X W X N, and viewed as N-directional points on a W X W grid. The form of the ARF enables it to effectively define relative rotations, e.g., the head rotation of a bird about its body. An ARF actively rotates during convolution; thus it acts as a virtual filter bank containing the canonical filter itself and its multiple unmaterialised rotated versions. In this example, the location and <b>orientation</b> of birds in different postures are captured by the ARF ...", "dateLastCrawled": "2022-01-31T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Perceptual learning in object recognition</b>: object specificity and <b>size</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0042698999001340", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0042698999001340", "snippet": "<b>Size</b> <b>invariance</b> implies that learning may be happening later in cortex than is required by the task alone. For example, improved recognition could in principle result from an increased signal-to-noise ratio in V1 neurons that encode a specific image. Such learning would not show <b>size</b> <b>invariance</b>, however, because large and small images of the same objects are encoded by different populations of V1 neurons. Hence, our results do not support the idea that learning always happens as early as ...", "dateLastCrawled": "2021-12-28T16:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "CS664 Computer Vision 6. Features - cs.cornell.edu", "url": "https://www.cs.cornell.edu/courses/cs664/2008sp/handouts/cs664-6-features.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs664/2008sp/handouts/cs664-6-features.pdf", "snippet": "varying degrees of <b>invariance</b> ... SIFT <b>Orientation</b> <b>Invariance</b> Determine <b>orientation</b> explicitly and normalize to canonical <b>orientation</b> Other alternative is detector that is itself invariant to <b>orientation</b> \u2013 But processing of image for such a detector removes more information \u2013 Recall discussion of image transformations Location and scale invariant detectors \u2013 In practice affine invariant because use extrema \u2013 Rotation or linear \u201cinsensitive\u201d descriptors. 16 SIFT <b>Orientation</b> ...", "dateLastCrawled": "2021-12-10T02:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Capsule Networks", "url": "https://cedar.buffalo.edu/~srihari/CSE676/9.12%20CapsuleNets.pdf", "isFamilyFriendly": true, "displayUrl": "https://cedar.buffalo.edu/~srihari/CSE676/9.12 CapsuleNets.pdf", "snippet": "Pooling and <b>Invariance</b> \u2022Pooling is supposed to obtain positional, orientational, proportional or rotational <b>invariance</b>. \u2022But it is a very crude approach \u2022In reality it removes all sorts of positional <b>invariance</b> Every input value changed, but only half the output values have changed because maxpoolis only sensitive to max value in neighborhood not exact value Original input Translated input. Deep Learning Srihari Example of CNN Limitation \u2022CNN to recognize faces extracts features from ...", "dateLastCrawled": "2022-01-27T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Technical Communications Primer: Gestalt Theory and</b> Visual ... - TechWhirl", "url": "https://techwhirl.com/technical-communications-primer-gestalt-theory-and-visual-design/", "isFamilyFriendly": true, "displayUrl": "https://techwhirl.com/<b>technical-communications-primer-gestalt-theory-and</b>-visual-design", "snippet": "<b>Invariance</b>. A visual image is invariant if we continue to interpret it as the same object even if we change its <b>orientation</b> (by rotation), its <b>size</b> (by magnification), or its position (by relocation). However, images are not equally recognizable in all orientations, sizes, and positions. For example, text is easily recognizable as text, no ...", "dateLastCrawled": "2021-12-05T15:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Visual Pattern Recognition in Drosophila Is Invariant for Retinal ...", "url": "https://science.sciencemag.org/content/305/5686/1020.full", "isFamilyFriendly": true, "displayUrl": "https://science.sciencemag.org/content/305/5686/1020", "snippet": "Like translation <b>invariance</b>, edge <b>orientation</b> is a further basic property that is shared between the visual systems of Drosophila and larger animals. Finally, flies also evaluate relational cues such as {A above B} versus {B above A}. So far this fascinating ability has been demonstrated only for two colors (blue and green) and two edge orientations (+45\u00b0 and \u201345\u00b0). The negative outcome of the experiment with two horizontally arranged oblique bars in the flight simulator cannot be ...", "dateLastCrawled": "2021-06-11T21:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "c++ - Template Matching for Coins with OpenCV - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/32692010/template-matching-for-coins-with-opencv", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/32692010", "snippet": "This methods does not directly support scale or <b>orientation</b> <b>invariance</b>. But it is possible to overcome that by scaling candidates to a reference <b>size</b> and by testing against several rotated templates. A detailed example of this technique is shown to detect pressence and location of 50c coins. The same procedure can be applied to the other coins. Two programs will be built. One to create templates from the big image template for the 50c coin. And another one which will take as input those ...", "dateLastCrawled": "2022-01-23T03:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Form features provide a cue to the angular velocity of rotating objects", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3895155/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3895155", "snippet": "After all, the <b>size</b> <b>invariance</b> of angular velocity <b>can</b> potentially serve as the basis for a kind of perceptual constancy (Fernandez, &amp; Farell, 2007; Hochberg, 1978: MacEvoy, &amp; Paradiso, 2001; Olkkonen, Witzel, Hansen, &amp; Gegenfurtner, 2010) that would allow the perceived speed of rotation to hold constant across changes in <b>size</b>. Such changes in <b>size</b> could occur independently from any rotational motion, for example, due to changes in viewing distance. One likely limiting factor in constructing ...", "dateLastCrawled": "2021-10-16T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Oxford Research Encyclopedia of Neuroscience", "url": "https://depts.washington.edu/shapelab/research/journals/Review2018.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>depts.washington.edu</b>/shapelab/research/journals/Review2018.pdf", "snippet": "\u201c<b>invariance</b>\u201d <b>can</b> <b>be thought</b> of as the brain\u2019s solution to an inverse inference problem in which the physical factors that gave rise to the retinal image are estimated. While the processes of perception and recognition seem fast and effortless, it is a challenging computational problem that involves a substantial proportion of the primate brain. Keywords: ventral pathway, macaque visual cortex, invariant object recognition, temporal pathway, \u201cwhat\u201d, pathway, rhesus monkey ...", "dateLastCrawled": "2021-11-20T23:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Dissociable Perceptual Effects of Visual Adaptation", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2703777/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2703777", "snippet": "Absolute curvature <b>can</b> <b>be thought</b> of as the local <b>orientation</b> gradient present on the retina, whereas shape is scale-invariant, and defined in the present experiments by the unit-less aspect ratio of the imaginary <b>rectangle</b> into which the curves are inscribed. By this definition, changing the scale of the adaptor stimulus, while holding the aspect ratio constant, would affect the absolute but not relative curvature. Some theoretical and neurophysiological work suggests that the brain&#39;s ...", "dateLastCrawled": "2017-01-02T01:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Computer vision -- SIFT <b>feature</b> extraction and retrieval", "url": "https://programming.vip/docs/computer-vision-sift-feature-extraction-and-retrieval.html", "isFamilyFriendly": true, "displayUrl": "https://programming.vip/docs/computer-vision-sift-<b>feature</b>-extraction-and-retrieval.html", "snippet": "This is because SIFT algorithm has scale and rotation <b>invariance</b>. Even if the two images are different in <b>size</b> and angle, the matching results will not be affected. When matching, the Euclidean distance of key <b>feature</b> vector is used as the similarity measure of key points in two images. Take a key point in image 1, and find out the first two keys that are closest to the Euclidean distance in image 2. In these two keys, if the nearest distance divided by the next nearest distance is less than ...", "dateLastCrawled": "2022-01-30T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Comparison of mechanisms for contrast-<b>invariance</b> of <b>orientation</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306452217300726", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306452217300726", "snippet": "Since noise is <b>thought</b> to have an effect on contrast-<b>invariance</b> (Anderson et al., 2000b, Hansel and Van Vreeswijk, 2002, Miller and Troyer, 2002) and the spiny stellate cell control response was sought without mechanisms such as synaptic noise, synaptic depression and inhibition that potentially contribute to contrast-invariant <b>orientation</b> tuning, then adding cortical synaptic noise was excluded. Similarly, adding more thalamocortical neurons for our control response was excluded because ...", "dateLastCrawled": "2021-11-26T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Technical Communications Primer: Gestalt Theory and</b> Visual ... - TechWhirl", "url": "https://techwhirl.com/technical-communications-primer-gestalt-theory-and-visual-design/", "isFamilyFriendly": true, "displayUrl": "https://techwhirl.com/<b>technical-communications-primer-gestalt-theory-and</b>-visual-design", "snippet": "<b>Invariance</b>. A visual image is invariant if we continue to interpret it as the same object even if we change its <b>orientation</b> (by rotation), its <b>size</b> (by magnification), or its position (by relocation). However, images are not equally recognizable in all orientations, sizes, and positions. For example, text is easily recognizable as text, no ...", "dateLastCrawled": "2021-12-05T15:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Scale Invariant <b>Feature Transform with Irregular Orientation Histogram</b> ...", "url": "https://www.researchgate.net/publication/47863805_Scale_Invariant_Feature_Transform_with_Irregular_Orientation_Histogram_Binning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/47863805_Scale_Invariant_Feature_Transform...", "snippet": "However, perfect scale <b>invariance</b> <b>can</b> not be achieved in practice because of sampling artefacts, noise in the image data, and the fact that the computational effort limits the number of analyzed ...", "dateLastCrawled": "2021-10-01T05:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Pattern Matching (specific question</b>) - OpenCV Q&amp;A Forum", "url": "https://answers.opencv.org/question/58650/pattern-matching-specific-question/", "isFamilyFriendly": true, "displayUrl": "https://answers.opencv.org/question/58650/<b>pattern-matching-specific-question</b>", "snippet": "You <b>can</b> see I build it artificially from a <b>rectangle</b> and a semicircle, and that it <b>can</b> have any <b>orientation</b>. So what I want to do is to , once I have this &quot;model&quot; find objects in my camera image that resemble this model but vary in <b>size</b> and <b>orientation</b>. Any idea on how to do this? Any pointers, advice or ideas would be greatly appreciated. Hi there! Please sign in help. faq tags users badges. This forum is disabled, please visit https://forum.opencv.org. ALL UNANSWERED. Ask Your Question 0 ...", "dateLastCrawled": "2021-12-16T21:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Insect Bio-inspired Neural Network Provides New Evidence on How Simple ...", "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005333", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005333", "snippet": "Indeed, honeybees <b>can</b> be trained to discriminate by an impressive range of visual cues; symmetry [1\u20133], arrangements of edges [4\u20136], <b>size</b> [7, 8], pattern disruption and edge <b>orientation</b> [10\u201312]. These abilities are all the more impressive since trained bees are able to apply these same learnt cues to patterns which may have little or no resemblance to the original training patterns, so long as they fall into the same class of e.g. plane of symmetry, or edge <b>orientation</b>.", "dateLastCrawled": "2020-05-07T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Sensation and Perception Exam 2 Quizzes</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/234339273/sensation-and-perception-exam-2-quizzes-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/234339273/<b>sensation-and-perception-exam-2-quizzes</b>-flash-cards", "snippet": "The principle of _____ <b>can</b> account for grouping of stimuli that share <b>orientation</b>, shape, and/or <b>size</b>. Similarity. In a scene, the objects in the foreground are best described as _____, where as the image making up the background is best described as the _____. Figure, ground. Border ownership means that when figure-ground segregation occurs, the border between the figure and background _____. Is perceived to be associated with the figure. In one reversible figure/ground study, Gibson and ...", "dateLastCrawled": "2020-07-05T07:30:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Size</b> and position <b>invariance</b> in the visual system", "url": "https://cavlab.net/MyRWResources/MyPDFs/PDF1971-79/Cavanagh1978.pdf", "isFamilyFriendly": true, "displayUrl": "https://cavlab.net/MyRWResources/MyPDFs/PDF1971-79/Cavanagh1978.pdf", "snippet": "<b>size</b> <b>invariance</b> is thus additionally complicated <b>compared</b> to a feature detection approach, as the type of analysis based on the connectedness of the elements in a pattern no longer applies. Since there is no a priori pattern unity in the Fourier domain, recognition, even without <b>size</b> <b>invariance</b>, requires comparison against a known standard. The transform of the pattern being searched for is matched against the transform of the input. This template approach, often called matched filtering ...", "dateLastCrawled": "2021-11-27T10:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Size-distance invariance: kinetic invariance is different</b> from ...", "url": "https://www.researchgate.net/publication/21534299_Size-distance_invariance_kinetic_invariance_is_different_from_static_invariance", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/21534299_<b>Size</b>-distance_<b>invariance</b>_kinetic...", "snippet": "A proposed kinetic <b>invariance</b> hypothesis asserts that a changing proximal stimulus <b>size</b> (an expanding or contracting solid visual angle) produces a constant perceived <b>size</b> and a changing perceived ...", "dateLastCrawled": "2021-11-03T00:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Position, rotation, scale, and <b>orientation</b> invariant object tracking ...", "url": "https://www.deepdyve.com/lp/spie/position-rotation-scale-and-orientation-invariant-object-tracking-from-2f0yN70LFR", "isFamilyFriendly": true, "displayUrl": "https://www.<b>deepdyve</b>.com/lp/spie/position-rotation-scale-and-<b>orientation</b>-invariant...", "snippet": "A method of tracking objects in video sequences despite any kind of perspective distortion is demonstrated. Moving objects are initially segmented from the scene using a background subtraction method to minimize the search area of the filter. A variation on the Maximum Average Correlation Height (MACH) filter is used to create <b>invariance</b> to <b>orientation</b> while giving high tolerance to background clutter and noise. A log r-\u0952 mapping is employed to give <b>invariance</b> to in-plane rotation and ...", "dateLastCrawled": "2020-11-15T04:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Form features provide a cue to the angular velocity of rotating objects", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3895155/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3895155", "snippet": "After all, the <b>size</b> <b>invariance</b> of angular velocity <b>can</b> potentially serve as the basis for a kind of perceptual constancy (Fernandez, &amp; Farell, 2007; Hochberg, 1978: MacEvoy, &amp; Paradiso, 2001; Olkkonen, Witzel, Hansen, &amp; Gegenfurtner, 2010) that would allow the perceived speed of rotation to hold constant across changes in <b>size</b>. Such changes in <b>size</b> could occur independently from any rotational motion, for example, due to changes in viewing distance. One likely limiting factor in constructing ...", "dateLastCrawled": "2021-10-16T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Rotation invariant feature lines transform for</b> image matching", "url": "https://www.spiedigitallibrary.org/journals/Journal-of-Electronic-Imaging/volume-23/issue-05/053002/Rotation-invariant-feature-lines-transform-for-image-matching/10.1117/1.JEI.23.5.053002.full", "isFamilyFriendly": true, "displayUrl": "https://www.spiedigitallibrary.org/journals/Journal-of-Electronic-Imaging/volume-23/...", "snippet": "First, lines are always strong features, i.e., they are insensitive to noise; second, lines contain <b>size</b> and <b>orientation</b> information, and the line description steps may be simpler; third, the matched number of the feature lines <b>can</b> be fewer <b>compared</b> with the method based on key points, but the feature lines <b>can</b> be more effective because they are composed of numerous points. What we do is describing each line and its neighborhood by a rotation and scaling invariant feature vector. If we match ...", "dateLastCrawled": "2022-01-27T09:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Capsule Networks", "url": "https://cedar.buffalo.edu/~srihari/CSE676/9.12%20CapsuleNets.pdf", "isFamilyFriendly": true, "displayUrl": "https://cedar.buffalo.edu/~srihari/CSE676/9.12 CapsuleNets.pdf", "snippet": "Pooling and <b>Invariance</b> \u2022Pooling is supposed to obtain positional, orientational, proportional or rotational <b>invariance</b>. \u2022But it is a very crude approach \u2022In reality it removes all sorts of positional <b>invariance</b> Every input value changed, but only half the output values have changed because maxpoolis only sensitive to max value in neighborhood not exact value Original input Translated input. Deep Learning Srihari Example of CNN Limitation \u2022CNN to recognize faces extracts features from ...", "dateLastCrawled": "2022-01-27T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Rotation Invariance &amp; Equivariance</b> - Home", "url": "https://kobiso.github.io/research/research-rotation/", "isFamilyFriendly": true, "displayUrl": "https://kobiso.github.io/research/research-rotation", "snippet": "Propose rotatable bounding box (RBox) and detector (DRBox) which <b>can</b> handle the situation where the <b>orientation</b> angles of the objects are arbitrary. The training of DRBox forces the detection networks to learn the correct <b>orientation</b> angle of the objects, so that the rotation invariant property <b>can</b> be achieved.", "dateLastCrawled": "2022-01-31T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Shape Descriptor/Feature Extraction Techniques", "url": "https://www.math.uci.edu/icamp/summer/research_11/park/shape_descriptors_survey_part2.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.math.uci.edu/icamp/summer/research_11/park/shape_descriptors_survey_part2.pdf", "snippet": "\u2022 occultation <b>invariance</b>: when some parts of a shape are occulted by other objects, the feature of the remaining part must not change <b>compared</b> to the original shape. \u2022 statistically independent: two features must be statistically independent. This represents compactness of the representation.", "dateLastCrawled": "2022-02-02T20:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Vision Algorithms for Mobile Robotics Lecture 01 Introduction", "url": "https://rpg.ifi.uzh.ch/docs/teaching/2021/06_feature_detection_2.pdf", "isFamilyFriendly": true, "displayUrl": "https://rpg.ifi.uzh.ch/docs/teaching/2021/06_feature_detection_2.pdf", "snippet": "Example of gradient histogram with 8 <b>orientation</b> bins. Each vote is weighted by the gradient magnitude. 0. 2\u03c0 . HOG Descriptor: (1D vector) 0. 2\u03c0. 0. 2\u03c0. 0. 2\u03c0 \u2026 Feature Descriptor <b>Invariance</b> \u2022he ideal feature T descriptor should be invariant to: \u2022eometric changes: g rotation, scale, view point \u2022 photometric changes: illumination \u2022st feature methods Mo are designed to be invariant to \u2022 translation, 2D \u2022 2D rotation, \u2022 Scale \u2022ome of them <b>can</b> also handle S \u2022iew V -point ...", "dateLastCrawled": "2022-01-18T22:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Histogram of Oriented</b> Gradients - Wai Yan Kyaw \u2013 Medium", "url": "https://waiyankyawmc.medium.com/histogram-of-oriented-gradients-90567ea6490a", "isFamilyFriendly": true, "displayUrl": "https://waiyankyawmc.medium.com/<b>histogram-of-oriented</b>-gradients-90567ea6490a", "snippet": "But, the author stated that increasing the number of <b>orientation</b> bins up to 9 <b>can</b> improve performance. You <b>can</b> also use \u2018signed\u2019 (0\u2013180) or \u2018unsigned\u2019 (0\u2013360) degree values for your histogram. For human detection, 3 x 3 cells blocks of 6 x 6 pixel cells performs best. You <b>can</b> see comparisons of different cells blocks and pixels <b>size</b> ...", "dateLastCrawled": "2022-02-03T20:10:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[2109.12926v1] ML4ML: Automated <b>Invariance</b> Testing for <b>Machine</b> <b>Learning</b> ...", "url": "https://arxiv.org/abs/2109.12926v1", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2109.12926v1", "snippet": "In <b>machine</b> <b>learning</b> workflows, determining <b>invariance</b> qualities of a model is a common testing procedure. In this paper, we propose an automatic testing framework that is applicable to a variety of <b>invariance</b> qualities. We draw an <b>analogy</b> between <b>invariance</b> testing and medical image analysis and propose to use variance matrices as ``imagery&#39;&#39; testing data. This enables us to employ <b>machine</b> <b>learning</b> techniques for analysing such ``imagery&#39;&#39; testing data automatically, hence facilitating ML4ML ...", "dateLastCrawled": "2022-01-20T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "ML4ML: Automated <b>Invariance</b> Testing for <b>Machine</b> <b>Learning</b> Models | DeepAI", "url": "https://deepai.org/publication/ml4ml-automated-invariance-testing-for-machine-learning-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/ml4ml-automated-<b>invariance</b>-testing-for-<b>machine</b>-<b>learning</b>...", "snippet": "In <b>machine</b> <b>learning</b> workflows, determining <b>invariance</b> qualities of a model is a common testing procedure. In this paper, we propose an automatic testing framework that is applicable to a variety of <b>invariance</b> qualities. We draw an <b>analogy</b> between <b>invariance</b> testing and medical image analysis and propose to use variance matrices as \u201cimagery\u201d testing data. This enables us to employ <b>machine</b> <b>learning</b> techniques for analysing such \u201cimagery\u201d testing data automatically, hence facilitating ...", "dateLastCrawled": "2022-01-26T21:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Bhattacharyya Kernels And <b>Machine</b> <b>Learning</b> on Sets of Data | by Michael ...", "url": "https://towardsdatascience.com/bhattacharyya-kernels-and-machine-learning-on-sets-of-data-bf94a22097f7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/bhattacharyya-kernels-and-<b>machine</b>-<b>learning</b>-on-sets-of...", "snippet": "Using <b>invariance</b> theory for <b>learning</b> on graph and relational data. towardsdatascience.com. Set Attention Models for Time Series Classification . A deep <b>learning</b> algorithm for real world time series data. towardsdatascience.com. But in this article, I am going to review how <b>learning</b> on sets can be done using Kernel methods. Kernel Methods. Most recent generation o f data scientists may have never used any kernel methods. Yet they were very widely used around 20 years ago. They are all based ...", "dateLastCrawled": "2022-01-20T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Machine</b> <b>Learning</b>, <b>Machine</b> Vision, and the Brain | Christian ...", "url": "https://www.academia.edu/8040540/Machine_Learning_Machine_Vision_and_the_Brain", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/8040540/<b>Machine</b>_<b>Learning</b>_<b>Machine</b>_Vision_and_the_Brain", "snippet": "In fact, the <b>invariance</b> of the view-tuned neurons to image-plane trans- formation and to changes in illumination has been tested experimentally by Logothetis, Pauls, and Poggio (1995) who report an average rotation <b>invariance</b> over 30 degrees, translation <b>invariance</b> over 2 degrees, and <b>size</b> <b>invariance</b> of up to 1 octave around the training view. These recent data put in sharp focus and in quantitative terms the question of the circuitry underlying the properties of the view-tuned cells. The ...", "dateLastCrawled": "2022-01-26T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Complete Guide To Invariant Theory - History Computer", "url": "https://history-computer.com/the-complete-guide-to-invariant-theory/", "isFamilyFriendly": true, "displayUrl": "https://history-computer.com/the-complete-guide-to-invariant-theory", "snippet": "The invariant theory is critical in artificial intelligence and <b>machine</b> <b>learning</b>. The concept helps <b>machine</b> <b>learning</b> experts understand and apply concepts differently. For instance, in particle physics, invariant theory helps these experts understand that all processes are Lorentz-invariant and can be permutation-invariant considering the particles can be identical. This identity enables these experts to distinguish the particles. Similarly, <b>machine</b> <b>learning</b> experts use invariant theory to ...", "dateLastCrawled": "2022-01-12T03:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Size\u00e2 Extensive Molecular <b>Machine</b> <b>Learning</b> with Global Representations", "url": "https://chemistry-europe.onlinelibrary.wiley.com/doi/pdf/10.1002/syst.201900052", "isFamilyFriendly": true, "displayUrl": "https://chemistry-europe.onlinelibrary.wiley.com/doi/pdf/10.1002/syst.201900052", "snippet": "<b>Size</b>-Extensive Molecular <b>Machine</b> <b>Learning</b> with Global Representations** Hyunwook Jung+,[a, b] Sina Stocker+,[a] Christian Kunkel,[a] Harald Oberhofer,[a] Byungchan Han,[b] Karsten Reuter,[a] and Johannes T. Margraf*[a] <b>Machine</b> <b>learning</b> (ML) models are increasingly used in combi-nation with electronic structure calculations to predict molec-ular properties at a much lower computational cost in high-throughput settings. Such ML models require representations thatencode themolecular structure ...", "dateLastCrawled": "2022-01-30T02:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Design and analysis of <b>machine</b> <b>learning</b> exchange-correlation ...", "url": "https://www.researchgate.net/publication/333731741_Design_and_analysis_of_machine_learning_exchange-correlation_functionals_via_rotationally_invariant_convolutional_descriptors", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333731741_Design_and_analysis_of_<b>machine</b>...", "snippet": "<b>machine</b> <b>learning</b> regression models is a promising framework for xc functional design, although challenges remain in obtaining training data and generating models consistent with pseudopotentials ...", "dateLastCrawled": "2021-10-23T19:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Integrating <b>Machine Learning</b> with Physics-Based Modeling | DeepAI", "url": "https://deepai.org/publication/integrating-machine-learning-with-physics-based-modeling", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/integrating-<b>machine-learning</b>-with-physics-based-modeling", "snippet": "Instead, data generation and training is an interactive process: Data is generated and labeled on the fly as model training proceeds. In <b>analogy</b> with multi-scale modeling, we refer to the former class of problems \u201csequential <b>machine learning</b>\u201d problems and the latter kind \u201cconcurrent <b>machine learning</b>\u201d problems.", "dateLastCrawled": "2022-01-27T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Orthogonalization - Adjust one knob to adjust one parameter, to solve one problem - The TV knob <b>analogy</b> and the car <b>analogy</b>. Chain of assumptions in <b>Machine</b> <b>Learning</b> and different knobs to say improve performance on train/dev set. Andrew Ng does not recommend Early stopping, as it is a knob that affects multiple thing at once. Setting up your goal", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(size invariance)  is like +(rectangle orientation invariance)", "+(size invariance) is similar to +(rectangle orientation invariance)", "+(size invariance) can be thought of as +(rectangle orientation invariance)", "+(size invariance) can be compared to +(rectangle orientation invariance)", "machine learning +(size invariance AND analogy)", "machine learning +(\"size invariance is like\")", "machine learning +(\"size invariance is similar\")", "machine learning +(\"just as size invariance\")", "machine learning +(\"size invariance can be thought of as\")", "machine learning +(\"size invariance can be compared to\")"]}