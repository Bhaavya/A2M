{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Understanding Cross-entropy</b> for Machine Learning", "url": "https://rubikscode.net/2021/08/10/understanding-cross-entropy/", "isFamilyFriendly": true, "displayUrl": "https://rubikscode.net/2021/08/10/<b>understanding-cross-entropy</b>", "snippet": "This means that <b>Cross-entropy</b> can be defined as the number of bits we need to encode information from y using the wrong encoding tool y\u2019. Mathematically, this can be written <b>like</b> this: The other way to write this expression is using expectation: H (y, y\u2019) represents expectation using y and the encoding size using y\u2019.", "dateLastCrawled": "2021-12-19T11:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross-Entropy Loss</b> Function. A loss function used in most\u2026 | by Kiprono ...", "url": "https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>cross-entropy-loss</b>-function-f38c4ec8643e", "snippet": "The <b>concept</b> of <b>cross-entropy</b> traces back into the field of Information Theory where Claude Shannon introduced the <b>concept</b> <b>of entropy</b> in 1948. Before diving into <b>Cross-Entropy</b> cost function, let us introduce <b>entropy</b> . <b>Entropy</b>. <b>Entropy</b> of a random variable X is the level of uncertainty inherent in the variables possible outcome. For p(x) \u2014 probability distribution and a random variable X, <b>entropy</b> is defined as follows. Equation 1: Definition <b>of Entropy</b>. Note log is calculated to base 2 ...", "dateLastCrawled": "2022-02-02T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Cross-entropy</b> for classification. Binary, multi-class and multi-label ...", "url": "https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>cross-entropy</b>-for-classification-d98e7f974451", "snippet": "Binary classification \u2014 we use binary <b>cross-entropy</b> \u2014 a specific case of <b>cross-entropy</b> where our target is 0 or 1. It can be computed with the <b>cross-entropy</b> formula if we convert the target to a one-hot vector <b>like</b> [0,1] or [1,0] and the predictions respectively. We can compute it even without this conversion, with the simplified formula.", "dateLastCrawled": "2022-02-02T19:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning Series Day 2 (Logistic Regression</b>) - Becoming <b>Human</b>", "url": "https://becominghuman.ai/machine-learning-series-day-2-logistic-regression-144af00f6ff5", "isFamilyFriendly": true, "displayUrl": "https://becoming<b>human</b>.ai/<b>machine-learning-series-day-2-logistic-regression</b>-144af00f6ff5", "snippet": "<b>Cross-Entropy</b>: \u201c<b>Cross-entropy</b> loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. <b>Cross-entropy</b> loss increases as the predicted probability diverge from the actual label.\u201d <b>Concept</b>: First of all, despite its name, a Logistic Regression is not a Regression problem but a Classification problem. Intuitively, you can think of the question: \u201cHow to Make $1M in 10 years?\u201d In our scenario, there are two routes: finish ...", "dateLastCrawled": "2022-01-21T10:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Entropy</b> - Meaning, Definition <b>Of Entropy</b>, Formula, Thermodynamic Relation", "url": "https://byjus.com/jee/entropy/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/jee/<b>entropy</b>", "snippet": "The <b>concept</b> <b>of entropy</b> basically talks about the spontaneous changes that occur in the everyday phenomenon or the tendency of the universe towards disorder. <b>Entropy</b> Definition Properties <b>of Entropy</b> <b>Entropy</b> Change <b>Entropy</b> and Thermodynamics. Apart from being just a scientific <b>concept</b> <b>entropy</b> is often described as a measurable physical property that is most commonly associated with uncertainty. However, the term is also used in different fields ranging from classical thermodynamics ...", "dateLastCrawled": "2022-02-03T00:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>cross entropy</b>? - MathsGee Q&amp;A", "url": "https://mathsgee.com/qna/504/write-down-the-partial-fractions-of-dfrac-x-3-3-x-1-x-2-x-2", "isFamilyFriendly": true, "displayUrl": "https://mathsgee.com/qna/504/write-down-the-partial-fractions-of-dfrac-x-3-3-x-1-x-2-x-2", "snippet": "What is <b>cross entropy</b>? Join the MathsGee Q&amp;A community and get study support for success - MathsGee Q&amp;A provides answers to subject-specific questions for improved outcomes. Join the South African Franchises Q&amp;A community and get support for business success - South African Franchises Q&amp;A provides answers to franchise-specific questions for improved outcomes. Quality Learning Support For All . Learning Starts With a Question . Toggle navigation. MathsGee Q&amp;A. Email or Username ; Password ...", "dateLastCrawled": "2021-10-28T01:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Entropy</b> <b>and Redundancy in Human Communication</b> - SLT info", "url": "https://www.sltinfo.com/entropy-and-redundancy-in-human-communication/", "isFamilyFriendly": true, "displayUrl": "https://www.sltinfo.com/<b>entropy</b>-<b>and-redundancy-in-human-communication</b>", "snippet": "However, the terms have come to mean something <b>like</b> the following when applied to <b>human</b> communication: <b>entropy</b>: refers to messages which convey highly unpredictable information to the receiver. redundancy: refers to messages which convey highly predictable information to the receiver. The proportions <b>of entropy</b> and redundancy in messages will ...", "dateLastCrawled": "2022-02-03T18:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is the <b>right cross-entropy loss formula in deep learning</b>? - Quora", "url": "https://www.quora.com/What-is-the-right-cross-entropy-loss-formula-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-<b>right-cross-entropy-loss-formula-in-deep-learning</b>", "snippet": "Answer (1 of 2): This question is often asked if one does not fully understand what <b>cross-entropy</b> means. So instead of answering question directly which already has been done, this answer will provide you with \u201cwhy\u201d part of the whole <b>concept</b>. As such, it will take a longer form. For that reason i...", "dateLastCrawled": "2022-01-12T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python - How do I apply the binary <b>cross-entropy</b> element-wise and then ...", "url": "https://stackoverflow.com/questions/52552910/how-do-i-apply-the-binary-cross-entropy-element-wise-and-then-sum-all-these-loss", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/52552910", "snippet": "The solution suggested in this answer may actually not be what you (reader) are looking for.. If A and B are NxM, where M &gt; 1, then binary_<b>crossentropy</b>(A, B) will not compute the binary <b>cross-entropy</b> element-wise, but binary_<b>crossentropy</b>(A, B) returns an array of shape Nx1, where binary_<b>crossentropy</b>(A, B)[i] correspond to the average binary <b>cross-entropy</b> between A[i] and B[i] (i.e. it computes the binary <b>cross-entropy</b> between A[i][j] and B[i][j], for all j, then it computes the average of ...", "dateLastCrawled": "2022-01-27T08:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Lecture 1: <b>Entropy</b> and mutual information", "url": "http://www.ece.tufts.edu/ee/194NIT/lect01.pdf", "isFamilyFriendly": true, "displayUrl": "www.ece.tufts.edu/ee/194NIT/lect01.pdf", "snippet": "The <b>entropy</b> measures the expected uncertainty in X. We also say that H(X) is approximately equal to how much information we learn on average from one instance of the random variable X. Note that the base of the algorithm is not important since changing the base only changes the value of the <b>entropy</b> by a multiplicative constant. Hb(X) = \u2212 P xp(x)logbp(x) = logb(a)[P xp(x)logap(x)] = logb(a)Ha(X). Customarily, we use the base 2 for the calculation <b>of entropy</b>. 2.1 Example Suppose you have a ...", "dateLastCrawled": "2022-02-03T06:39:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Understanding Cross-entropy</b> for Machine Learning", "url": "https://rubikscode.net/2021/08/10/understanding-cross-entropy/", "isFamilyFriendly": true, "displayUrl": "https://rubikscode.net/2021/08/10/<b>understanding-cross-entropy</b>", "snippet": "In this article, we covered a somewhat convoluted topic of <b>cross-entropy</b>. We explored the nature <b>of entropy</b>, how we extended that <b>concept</b> into <b>cross-entropy</b> and what KL divergence is. Apart from that, we were able to witness that binary <b>cross-entropy</b> is very <b>similar</b> to regular <b>cross-entropy</b>.", "dateLastCrawled": "2021-12-19T11:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross-entropy</b> for classification. Binary, multi-class and multi-label ...", "url": "https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>cross-entropy</b>-for-classification-d98e7f974451", "snippet": "Binary classification \u2014 we use binary <b>cross-entropy</b> \u2014 a specific case of <b>cross-entropy</b> where our target is 0 or 1. It can be computed with the <b>cross-entropy</b> formula if we convert the target to a one-hot vector like [0,1] or [1,0] and the predictions respectively. We can compute it even without this conversion, with the simplified formula.", "dateLastCrawled": "2022-02-02T19:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Dynamic <b>Cross-Entropy</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0165027016302552", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0165027016302552", "snippet": "Dynamic <b>Cross-Entropy</b> (DCE) quantifies the degree of regularity of EEG signals in selected frequency bands. \u2022 DCE analysis can be used to analyze the transition from order to chaotic behavior in complex nonlinear systems. \u2022 The presence of bifurcations is consistent with transitions into less ordered states and chaos. \u2022 The transition to irregular dynamics appears to follow a <b>similar</b> path in case of logistic equation and in the brain. Abstract. Background. Complexity measures for time ...", "dateLastCrawled": "2021-12-14T21:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Essential <b>Math for Data Science: Information Theory</b> - KDnuggets", "url": "https://www.kdnuggets.com/2021/01/essential-math-data-science-information-theory.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2021/01/essential-math-<b>data-science-information-theory</b>.html", "snippet": "<b>Cross Entropy</b> The <b>concept</b> <b>of entropy</b> can be used to compare two probability distributions: this is called the ... which <b>is similar</b> to Q(x). However, in the right panel, P(x) and Q(x) are different. This results in a larger <b>cross entropy</b>, because probabilities associated with a large quantity of information have a small weight, while probabilities associated with a small quantity of information have large weights. The <b>cross entropy</b> can\u2019t be smaller than the <b>entropy</b>. Still in the right panel ...", "dateLastCrawled": "2022-01-23T03:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>The Relationship Between Perplexity And Entropy</b> In NLP", "url": "https://www.topbots.com/perplexity-and-entropy-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/perplexity-and-<b>entropy</b>-in-nlp", "snippet": "Crucially, this tells us we can estimate the <b>cross-entropy</b> H(L,M) by just measuring log M(s) for a random sample of sentences (the first line) or a sufficiently large chunk of text (the second line). The <b>Cross-Entropy</b> is Bounded by the True <b>Entropy</b> of the Language. The <b>cross-entropy</b> has a nice property that H(L) \u2264 H(L,M). Omitting the limit ...", "dateLastCrawled": "2022-02-03T15:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 5, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Entropy</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Entropy", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Entropy</b>", "snippet": "<b>Entropy</b> is a scientific <b>concept</b> as well as a measurable physical property that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the <b>concept</b> are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory.It has found far-ranging applications in chemistry and physics, in biological systems and their relation to ...", "dateLastCrawled": "2022-01-27T19:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Abundance of Entropies. I have been guilty of talking about\u2026 | by ...", "url": "https://towardsdatascience.com/the-abundance-of-entropies-6cd613139c82?source=post_internal_links---------4----------------------------", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-abundance-of-entropies-6cd613139c82?source=post...", "snippet": "While thermodynamics gave us the <b>concept</b> <b>of entropy</b>, it does not give a detailed physical picture <b>of entropy</b>, in terms of positions and velocities of molecules, for instance. Statistical mechanics does give a detailed mechanical meaning to <b>entropy</b> in particular cases. - John R Pierce, An Introduction to Information Theory. Here we come to the understanding that an increase in <b>entropy</b> means a decrease in order or an increase in uncertainty. where K is Boltzmann\u2019s constant and Pi is the ...", "dateLastCrawled": "2022-01-31T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Visual Information Theory</b> -- colah&#39;s blog", "url": "http://colah.github.io/posts/2015-09-Visual-Information/", "isFamilyFriendly": true, "displayUrl": "colah.github.io/posts/2015-09-Visual-Information", "snippet": "Note that this notation for <b>cross-entropy</b> is non-standard. The normal notation is \\(H(p,q)\\). This notation is horrible for two reasons. Firstly, the exact same notation is also used for joint <b>entropy</b>. Secondly, it makes it seem like <b>cross-entropy</b> is symmetric. This is ridiculous, and I\u2019ll be writing \\(H_q(p)\\) instead.\u21a9. Also non-standard ...", "dateLastCrawled": "2022-02-01T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ML | Gini Impurity and <b>Entropy</b> in Decision Tree - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/gini-impurity-and-entropy-in-decision-tree-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/gini-impurity-and-<b>entropy</b>-in-decision-tree-ml", "snippet": "To build the decision tree in an efficient way we use the <b>concept</b> <b>of Entropy</b>. To learn more about the Decision Tree click here. In this article, we will be more focused on the difference between Gini Impurity and <b>Entropy</b>. <b>Entropy</b>: As discussed above <b>entropy</b> helps us to build an appropriate decision tree for selecting the best splitter. <b>Entropy</b> can be defined as a measure of the purity of the sub split. <b>Entropy</b> always lies between 0 to 1. The <b>entropy</b> of any split can be calculated by this ...", "dateLastCrawled": "2022-02-02T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "When is it <b>recommended to use squared error instead of cross entropy</b> ...", "url": "https://www.quora.com/When-is-it-recommended-to-use-squared-error-instead-of-cross-entropy-for-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/When-is-it-<b>recommended-to-use-squared-error-instead-of-cross</b>...", "snippet": "Answer: I will try to explain neuron saturation problem. Why we encounter with neuron saturation in classification problems ? And how <b>cross-entropy</b> cost function ...", "dateLastCrawled": "2022-01-13T06:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Understanding Cross-entropy</b> for Machine Learning", "url": "https://rubikscode.net/2021/08/10/understanding-cross-entropy/", "isFamilyFriendly": true, "displayUrl": "https://rubikscode.net/2021/08/10/<b>understanding-cross-entropy</b>", "snippet": "You know what Einstein said \u201cIf you <b>can</b>\u2019t explain it simply, you don\u2019t understand it well enough.\u201c, so my anxiety kicked in as well.However, after a couple of iterations, I could explain the <b>concept</b> to my friend and even grasp it better myself.. I really was happy that I managed to pull it off. Jokingly she said \u201cYou should write a blog post about it.\u201d, which I <b>thought</b> is actually a good idea. So, in this article, you will learn what <b>Cross-entropy</b> is and how we use it in machine ...", "dateLastCrawled": "2021-12-19T11:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding <b>Entropy</b>: the Golden Measurement of Machine Learning | by ...", "url": "https://towardsdatascience.com/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>entropy</b>-the-golden-measurement-of-machine...", "snippet": "One of the favorite loss functions of neural networks is <b>cross-entropy</b>. Be it categorical, sparse, or binary <b>cross-entropy</b>, the metric is one of the default go-to loss functions for high-performing neural nets. It <b>can</b> also be used for the optimization of almost any classification algorithm, like logistic regression. Like other applications <b>of entropy</b>, such as joint <b>entropy</b> and conditional <b>entropy</b>, <b>cross-entropy</b> is one of many flavors of a rigid definition <b>of entropy</b> fitted for a unique ...", "dateLastCrawled": "2022-02-02T21:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Developing the Concepts of Information and <b>Entropy</b> From Scratch | by ...", "url": "https://towardsdatascience.com/developing-the-concepts-of-information-and-entropy-from-scratch-11faca089d44", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/developing-the-<b>concepts</b>-of-information-and-<b>entropy</b>-from...", "snippet": "From <b>entropy</b>, the next step is to develop the <b>concept</b> of <b>cross-entropy</b>, which is basically <b>entropy</b> (average information) when the bit representation for the events is derived from a different probability distribution than the actual, true probability distribution. <b>Cross-entropy</b> is commonly used in machine learning algorithms. However, this topic is more complicated and will be explained in another post. In this article we came up with a mathematical way to represent information and ...", "dateLastCrawled": "2022-01-23T19:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>The Relationship Between Perplexity And Entropy</b> In NLP", "url": "https://www.topbots.com/perplexity-and-entropy-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/perplexity-and-<b>entropy</b>-in-nlp", "snippet": "Crucially, this tells us we <b>can</b> estimate the <b>cross-entropy</b> H(L,M) by just measuring log M(s) for a random sample of sentences (the first line) or a sufficiently large chunk of text (the second line). The <b>Cross-Entropy</b> is Bounded by the True <b>Entropy</b> of the Language. The <b>cross-entropy</b> has a nice property that H(L) \u2264 H(L,M). Omitting the limit ...", "dateLastCrawled": "2022-02-03T15:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Entropy</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Entropy", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Entropy</b>", "snippet": "<b>Entropy</b> is a scientific <b>concept</b> as well as a measurable physical property that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the <b>concept</b> are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory.It has found far-ranging applications in chemistry and physics, in biological systems and their relation to ...", "dateLastCrawled": "2022-02-03T06:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Can Free Will Really Be a Scientific Idea</b>? | Mind Matters", "url": "https://mindmatters.ai/2019/09/can-free-will-really-be-a-scientific-idea/", "isFamilyFriendly": true, "displayUrl": "https://mindmatters.ai/2019/09/<b>can-free-will-really-be-a-scientific-idea</b>", "snippet": "A related <b>concept</b> is <b>cross-entropy</b>. <b>Cross entropy</b> is also the expectation of surprisal but it uses a different probability distribution for the expectation. An important distinction between <b>cross-entropy</b> and regular <b>entropy</b> is that <b>cross-entropy</b> is never lower than regular <b>entropy</b>. How does the <b>concept</b> <b>of entropy</b> relate to free will? If an entity has a fixed probability assignment, then it will have different <b>entropy</b> characteristics than an entity that does not have a fixed assignment, i.e ...", "dateLastCrawled": "2022-01-24T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Cross entropy</b>, dissimilarity measures, and characterizations of ...", "url": "https://www.researchgate.net/publication/3084227_Cross_entropy_dissimilarity_measures_and_characterizations_of_Quadratic_Entropy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/3084227_<b>Cross_entropy</b>_dissimilarity_measures...", "snippet": "For example, in categorization tasks, the commonly used <b>cross-entropy</b> loss (also known as log-loss), quantifies the <b>cross-entropy</b> between an empirical distribution of features (e.g., by fixing the ...", "dateLastCrawled": "2021-12-13T14:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What <b>is the difference between entropy and atrophy</b>? - Quora", "url": "https://www.quora.com/What-is-the-difference-between-entropy-and-atrophy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>is-the-difference-between-entropy-and-atrophy</b>", "snippet": "Answer: <b>Entropy</b> is a thermodynamic measurement which accounts for loss of order and is needed in some advanced scientific calculations. Atrophy is completely unrelated and usually associated to degradation of the <b>human</b> body such as a muscle that is wasting away or a bone disease which degrades bo...", "dateLastCrawled": "2022-01-11T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Entropy</b> | Free Full-Text | <b>Approximate Entropy and Sample Entropy</b>: A ...", "url": "https://www.mdpi.com/1099-4300/21/6/541/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/21/6/541/htm", "snippet": "The <b>concept</b> <b>of entropy</b> in information theory did not originate following any of those ideas, and the choice of the word <b>entropy</b> for its fundamental definition was either a good joke or an extraordinary intuition (if we believe the story, Shannon <b>thought</b> about naming his new <b>concept</b> \u201cinformation\u201d or \u201cuncertainty\u201d, but those names had already been used extensively prior to his formulation. Then John von Neuman suggested him to use the name <b>entropy</b> because in the statistical mechanics ...", "dateLastCrawled": "2022-01-28T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Concepts and Applications of Information Theory to</b> Immuno-Oncology ...", "url": "https://www.cell.com/trends/cancer/fulltext/S2405-8033(20)30340-X", "isFamilyFriendly": true, "displayUrl": "https://www.cell.com/trends/<b>can</b>cer/fulltext/S2405-8033(20)30340-X", "snippet": "The <b>concept</b> of mutual information does not diverge far from \u2018simple\u2019 <b>entropy</b> changes; the measure of information about x gained by observing y <b>can</b> be reflected through a reduction in <b>entropy</b>. Notably, mutual information is an intrinsically flexible measure that <b>can</b> be extended to heterogeneous variables and mixed variable types (e.g., spatially defined variables).", "dateLastCrawled": "2022-01-30T19:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Cross-entropy</b> for classification. Binary, multi-class and multi-label ...", "url": "https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>cross-entropy</b>-for-classification-d98e7f974451", "snippet": "Binary <b>cross-entropy</b> is another special case of <b>cross-entropy</b> \u2014 used if our target is either 0 or 1. In a neural network, you typically achieve this prediction by sigmoid activation. The target is not a probability vector. We <b>can</b> still use <b>cross-entropy</b> with a little trick. We want to predict whether the image contains a panda or not.", "dateLastCrawled": "2022-02-02T19:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) A <b>Cross Entropy</b> test allows quantitative statistical comparison ...", "url": "https://www.researchgate.net/publication/356891438_A_Cross_Entropy_test_allows_quantitative_statistical_comparison_of_t-SNE_and_UMAP_representations", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356891438_A_<b>Cross_Entropy</b>_test_allows...", "snippet": "<b>Cross entropy</b> captures, as a single statistic, both changes in the relative frequency of cells of different phenotype classes and also the shift of phenotype of cells within phenotype classes.", "dateLastCrawled": "2021-12-19T14:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Cross-Entropy Loss</b> Function. A loss function used in most\u2026 | by Kiprono ...", "url": "https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>cross-entropy-loss</b>-function-f38c4ec8643e", "snippet": "The <b>concept</b> of <b>cross-entropy</b> traces back into the field of Information Theory where Claude Shannon introduced the <b>concept</b> <b>of entropy</b> in 1948. Before diving into <b>Cross-Entropy</b> cost function, let us introduce <b>entropy</b> . <b>Entropy</b> . <b>Entropy</b> of a random variable X is the level of uncertainty inherent in the variables possible outcome. For p(x) \u2014 probability distribution and a random variable X, <b>entropy</b> is defined as follows. Equation 1: Definition <b>of Entropy</b>. Note log is calculated to base 2 ...", "dateLastCrawled": "2022-02-02T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Dynamic <b>Cross-Entropy</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0165027016302552", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0165027016302552", "snippet": "Dynamic <b>Cross-Entropy</b> (DCE) quantifies the degree of regularity of EEG signals in selected frequency bands. \u2022 DCE analysis <b>can</b> be used to analyze the transition from order to chaotic behavior in complex nonlinear systems. \u2022 The presence of bifurcations is consistent with transitions into less ordered states and chaos. \u2022 The transition to irregular dynamics appears to follow a similar path in case of logistic equation and in the brain. Abstract. Background. Complexity measures for time ...", "dateLastCrawled": "2021-12-14T21:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Entropy</b> | Free Full-Text | (<b>Multiscale) Cross-Entropy Methods: A Review</b> ...", "url": "https://www.mdpi.com/1099-4300/22/1/45/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/22/1/45/htm", "snippet": "<b>Cross-entropy</b> was introduced in 1996 to quantify the degree of asynchronism between two time series. In 2009, a multiscale <b>cross-entropy</b> measure was proposed to analyze the dynamical characteristics of the coupling behavior between two sequences on multiple scales. Since their introductions, many improvements and other methods have been developed. In this review we offer a state-of-the-art on <b>cross-entropy</b> measures and their multiscale approaches.", "dateLastCrawled": "2022-01-26T17:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "On the improvement of reinforcement active learning with the ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6583946/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6583946", "snippet": "<b>Cross-entropy</b> is an important <b>concept</b> in Shannon\u2019s information theory that is mainly used to measure the difference information between two probability distributions. The intuition is that we want to increase the similarity of the label prediction probability distribution output by the model to the probability distribution of the real label. This method has been applied in many fields of machine learning. Inspired by this idea, we design our loss function as follows:", "dateLastCrawled": "2021-11-02T12:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Essential <b>Math for Data Science: Information Theory</b> - KDnuggets", "url": "https://www.kdnuggets.com/2021/01/essential-math-data-science-information-theory.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2021/01/essential-math-<b>data-science-information-theory</b>.html", "snippet": "The <b>concept</b> <b>of entropy</b> <b>can</b> be used to compare two probability distributions: this is called the <b>cross entropy</b> between two distributions, which measures how much they differ. The idea is to calculate the information associated with the probabilities of a distribution Q(x) , but instead of weighting according to Q(x) as with the <b>entropy</b>, you weight according to the other distribution P(x) .", "dateLastCrawled": "2022-01-23T03:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Understanding Multiscale Entropy</b> - Sapien Labs | Neuroscience | <b>Human</b> ...", "url": "https://sapienlabs.org/neurotech/understanding-multiscale-entropy/", "isFamilyFriendly": true, "displayUrl": "https://sapienlabs.org/neurotech/<b>understanding-multiscale-entropy</b>", "snippet": "The curve <b>of entropy</b> vs time scale may yield a peak which indicates a time scale at which there is maximal <b>entropy</b> and may therefore be of greater relevance. Indeed Escudero et al. [2] showed that MSE could find significant differences between Alzheimer\u2019s patients and control subjects even on large time scales at 10 electrodes, and that EEG activity is less complex in Alzheimer\u2019s patients <b>compared</b> to controls. Although their study used only 11 controls and Alzheimer\u2019s patients.", "dateLastCrawled": "2022-01-30T04:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ML | Gini Impurity and <b>Entropy</b> in Decision Tree - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/gini-impurity-and-entropy-in-decision-tree-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/gini-impurity-and-<b>entropy</b>-in-decision-tree-ml", "snippet": "<b>Entropy</b> <b>can</b> be defined as a measure of the purity of the sub split. <b>Entropy</b> always lies between 0 to 1. The <b>entropy</b> of any split <b>can</b> be calculated by this formula. The algorithm calculates the <b>entropy</b> of each feature after every split and as the splitting continues on, it selects the best feature and starts splitting according to it. For a detailed calculation <b>of entropy</b> with an example, you <b>can</b> refer to this article. Gini Impurity: The internal working of Gini impurity is also somewhat ...", "dateLastCrawled": "2022-02-02T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Natural Language Processing: Are there any tools that <b>can</b> measure the ...", "url": "https://www.quora.com/Natural-Language-Processing-Are-there-any-tools-that-can-measure-the-cross-entropy-of-2-corpora-of-short-text-snippets", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Natural-Language-Processing-Are-there-any-tools-that-<b>can</b>-measure...", "snippet": "Answer (1 of 2): Reliably measuring semantic similarity between two pieces of text is an open problem in natural language processing. As an illustration consider that as recently as this year, there was a shared task as part of the SemEval (Semantic Evaluation) workshop designed to elicit ideas o...", "dateLastCrawled": "2022-01-17T18:36:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why and How to use <b>Cross Entropy</b>. The fundamental reasons for ...", "url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-and-how-to-use-<b>cross-entropy</b>-4e983cbdd873", "snippet": "The fundamental reasons for minimizing binary <b>cross entropy</b> (log loss) with probabilistic classification models . Will Arliss. Sep 26, 2020 \u00b7 7 min read. Introduction. This post discusses why logistic regression necessarily uses a different loss function than linear regression. First, the simple yet inefficient way to solve logistic regression will be presented, then the slightly less simple but much more efficient way will be explained and compared. The simple way. Linear regression is the ...", "dateLastCrawled": "2022-01-31T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross-Entropy</b> Demystified. What is it? Is there any relation to\u2026 | by ...", "url": "https://naokishibuya.medium.com/demystifying-cross-entropy-e80e3ad54a8", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/demystifying-<b>cross-entropy</b>-e80e3ad54a8", "snippet": "However, the <b>machine</b> <b>learning</b> application uses the base e logarithm for implementation convenience. Binary <b>Cross-Entropy</b>. We can use the binary <b>cross-entropy</b> for binary classification where we have yes/no answer. For example, there are only dogs or cats in images. For the binary classifications, the <b>cross-entropy</b> formula contains only two ...", "dateLastCrawled": "2022-01-26T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>machine</b> <b>learning</b> - <b>Cross-entropy loss</b> explanation - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/20296", "snippet": "The answer from Neil is correct. However I think its important to point out that while the loss does not depend on the distribution between the incorrect classes (only the distribution between the correct class and the rest), the gradient of this loss function does effect the incorrect classes differently depending on how wrong they are. So when you use cross-ent in <b>machine</b> <b>learning</b> you will change weights differently for [0.1 0.5 0.1 0.1 0.2] and [0.1 0.6 0.1 0.1 0.1].", "dateLastCrawled": "2022-01-27T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to Information Entropy - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-is-information-entropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/what-is-information-entropy", "snippet": "Calculating information and entropy is a useful tool in <b>machine</b> <b>learning</b> and is used as the basis for techniques such as feature selection, building decision trees, and, more generally, fitting classification models. As such, a <b>machine</b> <b>learning</b> practitioner requires a strong understanding and intuition for information and entropy. In this post, you will discover a gentle introduction to information entropy. After reading this post, you will know: Information theory is concerned with data ...", "dateLastCrawled": "2022-02-02T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Shannon <b>entropy</b> in the context of <b>machine</b> <b>learning</b> and AI | by Frank ...", "url": "https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/shannon-<b>entropy</b>-in-the-context-of-<b>machine</b>-<b>learning</b>-and-ai-24...", "snippet": "Closely related to <b>cross entropy</b>, the KL divergence from q to p, written DKL(p||q), is another similarity measure often used in <b>machine</b> <b>learning</b>. In the language of Bayesian Inference, DKL(p||q ...", "dateLastCrawled": "2022-01-30T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Softmax and <b>Cross-entropy for multi-class classification</b>. - AppliedAICourse", "url": "https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/3384/softmax-and-cross-entropy-for-multi-class-classification/8/module-8-neural-networks-computer-vision-and-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.appliedaicourse.com/lecture/11/applied-<b>machine</b>-<b>learning</b>-online-course/3384/...", "snippet": "Home Courses Applied <b>Machine</b> <b>Learning</b> Online Course Softmax and <b>Cross-entropy for multi-class classification</b>. Softmax and <b>Cross-entropy for multi-class classification</b>. Instructor: Applied AI Course Duration: 25 mins . Close. This content is restricted. Please Login. Prev. Next. Gradient Checking and clipping . How to train a Deep MLP? Deep <b>Learning</b>:Neural Networks. 1.1 History of Neural networks and Deep <b>Learning</b>. 25 min. 1.2 How Biological Neurons work? 8 min. 1.3 Growth of biological ...", "dateLastCrawled": "2022-02-02T02:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Neural Network</b> Training", "url": "https://cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.2-Training.pdf", "isFamilyFriendly": true, "displayUrl": "https://cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.2-Training.pdf", "snippet": "<b>Machine</b> <b>Learning</b> Srihari Definitions of Gradient and Hessian \u2022First derivative of a scalar function E(w)with respect to a vector w=[w 1,w 2]T is a vector called the Gradient of E(w) \u2022Second derivative of E(w) is a matrix called the Hessian 2 \u2207E(w)= d dw E(w)= \u2202E \u2202w 1", "dateLastCrawled": "2022-02-03T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[D] A Short Introduction to Entropy, <b>Cross-Entropy</b> and KL-Divergence ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7vhmp7/d_a_short_introduction_to_entropy_crossentropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7vhmp7/d_a_short_introduction_to...", "snippet": "I am having trouble reconciling the concept with the <b>analogy</b>. At 2:35 even if a rainy day was 25% likely, there&#39;s still only two states, rainy and sunny, and therefor only 1 bit of information is needed to convey that, so only one bit of data needs to be sent, even though the 1 bit of data reduces the uncertainty of a rainy day by a factor of 4. I quite don&#39;t get what he means by this being 2 bits of information. I guess where I am stuck is how the uncertainty reduction factor translates to ...", "dateLastCrawled": "2021-08-20T08:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How <b>to Split Your Dataset</b> the Right Way - <b>Machine</b> <b>Learning</b> Compass", "url": "https://machinelearningcompass.com/dataset_optimization/split_data_the_right_way/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>compass.com/dataset_optimization/split_data_the_right_way", "snippet": "Here you can search for any <b>machine</b> <b>learning</b> related term and find exactly what you were looking for. If there is a topic that I have not covered yet, please write me about it (you can find my contact details here)!I would love to hear which topic you want to see covered next!Btw, you can also use keyboard shortcuts to open and close the search window. \ud83d\ude0e", "dateLastCrawled": "2022-01-31T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Analogy</b> between Neural network and naive bayes - Cross Validated", "url": "https://stats.stackexchange.com/questions/219687/analogy-between-neural-network-and-naive-bayes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/219687", "snippet": "I am trying to understand the <b>analogy</b> between a single layer neural network and naive Bayes classifier. Particularly, I want to know if, in a neural network, the variables are independent given the class and if the bias term represents the prior probability. (As far as I understand, if I take the log of the naive bayes expression, it seems to give a term similar to the weighted sum +bias in neural network as worked out here", "dateLastCrawled": "2022-01-26T15:11:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Beat the Bookmakers With Tree-Based <b>Machine</b> <b>Learning</b> Algorithms | by ...", "url": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-machine-learning-algorithms-1d349335b54", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-<b>machine</b>...", "snippet": "<b>Cross-entropy is similar</b> to Gini Impurity, but it involves using the concept of entropy from information theory. This article won\u2019t go in depth about it, but essentially, as the cross-entropy ...", "dateLastCrawled": "2022-01-26T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>A Traveler\u2019s Diary on the Road to Machine</b> <b>Learning</b> - Chapter 1 | by ...", "url": "https://medium.com/swlh/a-travelers-diary-on-the-road-to-machine-learning-chapter-1-8850ec5b4243", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>a-travelers-diary-on-the-road-to-machine</b>-<b>learning</b>-chapter-1...", "snippet": "Types of <b>Machine</b> <b>Learning</b> algorithms: ... Sparse categorical <b>cross entropy is similar</b> to categorical cross entropy, only difference is it uses only one value as target. It saves memory as well as ...", "dateLastCrawled": "2021-05-21T04:27:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Deep Learning for Computer Architects</b> | Chen Jeff - Academia.edu", "url": "https://www.academia.edu/40860009/Deep_Learning_for_Computer_Architects", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40860009/<b>Deep_Learning_for_Computer_Architects</b>", "snippet": "This text serves as a primer for computer architects in a new and rapidly evolving \ufb01eld. We review how <b>machine</b> <b>learning</b> has evolved since its inception in the 1960s and track the key developments leading up to the emergence of the powerful deep <b>learning</b> techniques that emerged in the last decade.", "dateLastCrawled": "2022-01-28T02:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(cross-entropy)  is like +(human concept of entropy)", "+(cross-entropy) is similar to +(human concept of entropy)", "+(cross-entropy) can be thought of as +(human concept of entropy)", "+(cross-entropy) can be compared to +(human concept of entropy)", "machine learning +(cross-entropy AND analogy)", "machine learning +(\"cross-entropy is like\")", "machine learning +(\"cross-entropy is similar\")", "machine learning +(\"just as cross-entropy\")", "machine learning +(\"cross-entropy can be thought of as\")", "machine learning +(\"cross-entropy can be compared to\")"]}