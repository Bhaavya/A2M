{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "notes/jurafsky-slp-ch11-transfer.md at master \u00b7 makrai/notes \u00b7 GitHub", "url": "https://github.com/makrai/notes/blob/master/jurafsky-slp-ch11-transfer.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/makrai/notes/blob/master/jurafsky-slp-ch11-transfer.md", "snippet": "without syntactic parse is to use a standard <b>self-attention</b> <b>layer</b> to g i j = SelfATTN(h i: j ) (11.20) eg named entity recognition (NER). Given a scheme for representing spans and set of named entity types, a span-based approach to NER is a straightforward classification problem; assign a label y, from the set of valid NER labels, to span in S(x).", "dateLastCrawled": "2022-01-21T19:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Finding Strong Gravitational Lenses Through <b>Self-Attention</b> | DeepAI", "url": "https://deepai.org/publication/finding-strong-gravitational-lenses-through-self-attention", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/finding-strong-gravitational-<b>lens</b>es-through-<b>self-attention</b>", "snippet": "Then each attention <b>layer</b> is <b>called</b> a head and applies <b>self-attention</b> to one part of the divided input. 3.2.1 Positional Encoding. Suppose we pass the input directly to the attention layers. In that case, the input order or the positional information is lost as transformer models are permutation invariant. So in order to preserve the information regarding the order of features, we use positional encoding, and the lack of positional encoding will lower the performance of a transformer model ...", "dateLastCrawled": "2022-01-11T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A review on the attention mechanism of <b>deep learning</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "snippet": "applied the <b>self-attention</b> mechanism to the computer vision task to solve this problem, <b>called</b> non-local attention, as shown in Fig. 13. They proposed the non-local module that got attention masks by calculating the correlation matrix between each spatial point in the feature map, then the attention guided dense contextual information to aggregate. However, this method <b>also</b> has the following problems: 1) Only the positional attention module is involved, not the commonly used channel ...", "dateLastCrawled": "2022-02-02T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Kernel <b>Self-Attention</b> for Weakly-supervised Image Classification using ...", "url": "https://www.readkong.com/page/kernel-self-attention-for-weakly-supervised-image-5842360", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/kernel-<b>self-attention</b>-for-weakly-supervised-image-5842360", "snippet": "In order to model dependencies between the instances, their representations pass trough the <b>self-attention</b> <b>layer</b> and then aggregate using AbMILP operator. The obtained fixed-size vector goes trough the Fully Connected (FC) classification <b>layer</b>. Kernels in <b>self-attention</b>. In order to indicate to which followed by 1 \u00d7 1 convolution as those were the best fea- extent one instance attends on synthesizing the other one, ture extractor in [38], and for the &quot;Messidor&quot; dataset we the <b>self-attention</b> ...", "dateLastCrawled": "2022-01-29T18:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "The decoder <b>also</b> consists of several layers, where each <b>layer</b> consists of two <b>self-attention</b> sublayers and a fully-connected sublayer. The diagram <b>also</b> shows what sort of connections are created between inputs and the sublayers. The <b>self-attention</b> sublayer looks at all the words at a given time, where the fully-connected sublayer processes words individually.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Optical Computing and Transformer Networks</b> | by Optalysys - Medium", "url": "https://medium.com/optalysys/optical-computing-and-transformer-networks-8145b10f4857", "isFamilyFriendly": true, "displayUrl": "https://medium.com/optalysys/<b>optical-computing-and-transformer-networks</b>-8145b10f4857", "snippet": "<b>Self-attention</b> consists of one or more attention heads. Each attention head is a triplet of matrices (Q,K,V) with the same size, <b>called</b> the query, key, and value weights.", "dateLastCrawled": "2021-10-05T05:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Hugging Face Pre-trained Models: Find the Best One for Your Task ...", "url": "https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best", "snippet": "The decoder is similar in structure to the encoder except that it includes a standard attention mechanism after each <b>self-attention</b> <b>layer</b> that attends to the output of the encoder. It <b>also</b> uses a form of autoregressive or causal <b>self-attention</b>, which allows the model to attend to past outputs. The T5 model was trained on unlabeled data which was generated using a cleaner version of common crawl, Colossal Clean Crawled Corpus(C4). With the help of a text-to-text transformer and a new pre ...", "dateLastCrawled": "2022-02-02T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "FMAGAN:Fusing multiple attention and generative adversarial network to ...", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs211680", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs211680", "snippet": "At the same time, it should be noted that the introduction of <b>self-attention</b> greatly improved the calculated amount of the model, but it is undeniable that <b>self-attention</b> <b>also</b> improves the performance of the model. In order to achieve a better balance between model training speed and model performance, we added two CSAM modules into FMAGAN and achieved the best results in various indicators. The training time of the model increases significantly with the addition of CSAM modules. For this ...", "dateLastCrawled": "2022-02-03T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Back-Propagation <b>Algorithm</b> I. Definitions, Concepts, Algorithms with ...", "url": "https://medium.com/computronium/back-propagation-algorithm-85c65e6fc359", "isFamilyFriendly": true, "displayUrl": "https://medium.com/computronium/back-propagation-<b>algorithm</b>-85c65e6fc359", "snippet": "Normally, when we use a neural network we input some vector x and the network produces an output y. The input vector goes through each hidden <b>layer</b>, one by one, until the output <b>layer</b>. Flow in this\u2026", "dateLastCrawled": "2022-01-23T21:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Single Image Depth Estimation Trained via Depth From Defocus Cues", "url": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Gur_Single_Image_Depth_Estimation_Trained_via_Depth_From_Defocus_Cues_CVPR_2019_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Gur_Single_Image_Depth...", "snippet": "icated CUDA kernel. This <b>layer</b> is then used as part of a novel architecture, combining the successful ASPP archi-tecture [5, 9]. To improve the ASPP block, we add dense connections [16], followed by <b>self-attention</b> [42]. We evaluate our method on all relevant benchmarks we were able to obtain. These include the \ufb02ower light\ufb01eld", "dateLastCrawled": "2022-02-01T21:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "notes/jurafsky-slp-ch11-transfer.md at master \u00b7 makrai/notes \u00b7 GitHub", "url": "https://github.com/makrai/notes/blob/master/jurafsky-slp-ch11-transfer.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/makrai/notes/blob/master/jurafsky-slp-ch11-transfer.md", "snippet": "without syntactic parse is to use a standard <b>self-attention</b> <b>layer</b> to g i j = SelfATTN(h i: j ) (11.20) eg named entity recognition (NER). Given a scheme for representing spans and set of named entity types, a span-based approach to NER is a straightforward classification problem; assign a label y, from the set of valid NER labels, to span in S(x).", "dateLastCrawled": "2022-01-21T19:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Finding Strong Gravitational Lenses Through <b>Self-Attention</b> | DeepAI", "url": "https://deepai.org/publication/finding-strong-gravitational-lenses-through-self-attention", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/finding-strong-gravitational-<b>lens</b>es-through-<b>self-attention</b>", "snippet": "We propose a new automated architecture based on the principle of <b>self-attention</b> to find strong gravitational lensing. The advantages of <b>self-attention</b> based encoder models over convolution neural networks are investigated and encoder models are analyzed to optimize performance. We constructed 21 <b>self-attention</b> based encoder models and four convolution neural networks trained to identify gravitational lenses from the Bologna <b>Lens</b> Challenge. Each model is trained separately using 18,000 ...", "dateLastCrawled": "2022-01-11T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "How <b>self attention</b> works when processing the word \u201cits\u201d. The attention <b>layer</b> has weights for each word, enabling the <b>layer</b> to created a \u201cweighted-mix\u201d of words as the output. Essentially the gray-box encodes information about the word \u201cits\u201d and \u201cdog\u201d. Note: You can <b>also</b> see that there is a masked <b>self-attention</b> <b>layer</b> in the ...", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Transformers are <b>RNNs: Fast Autoregressive Transformers with Linear</b> ...", "url": "http://proceedings.mlr.press/v119/katharopoulos20a/katharopoulos20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/katharopoulos20a/katharopoulos20a.pdf", "snippet": "convolutional <b>layer</b>. Here, we instead show that a <b>self-attention</b> <b>layer</b> trained with an autoregressive objective can be seen as a recurrent neural network and this observation can be used to signi\ufb01cantly speed up inference time of au- toregressive transformer models. 2.3. Linearized softmax For many years, softmax has been the bottleneck for train-ing classi\ufb01cation models with a large number of categories (Goodman,2001;Morin &amp; Bengio,2005;Mnih &amp; Hinton, 2009). Recent works (Blanc &amp; Rendle ...", "dateLastCrawled": "2022-01-30T21:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Hugging Face Pre-trained Models: Find the Best One for Your Task ...", "url": "https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best", "snippet": "The encoder consists of blocks, each of them comprising two parts: a <b>self-attention</b> <b>layer</b> followed by a small feed-forward network. The decoder <b>is similar</b> in structure to the encoder except that it includes a standard attention mechanism after each <b>self-attention</b> <b>layer</b> that attends to the output of the encoder. It <b>also</b> uses a form of autoregressive or causal <b>self-attention</b>, which allows the model to attend to past outputs.", "dateLastCrawled": "2022-02-02T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A review on the attention mechanism of <b>deep learning</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "snippet": "applied the <b>self-attention</b> mechanism to the computer vision task to solve this problem, <b>called</b> non-local attention, as shown in Fig. 13. They proposed the non-local module that got attention masks by calculating the correlation matrix between each spatial point in the feature map, then the attention guided dense contextual information to aggregate. However, this method <b>also</b> has the following problems: 1) Only the positional attention module is involved, not the commonly used channel ...", "dateLastCrawled": "2022-02-02T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Optical Computing and Transformer Networks</b> | by Optalysys - Medium", "url": "https://medium.com/optalysys/optical-computing-and-transformer-networks-8145b10f4857", "isFamilyFriendly": true, "displayUrl": "https://medium.com/optalysys/<b>optical-computing-and-transformer-networks</b>-8145b10f4857", "snippet": "<b>Self-attention</b> consists of one or more attention heads. Each attention head is a triplet of matrices (Q,K,V) with the same size, <b>called</b> the query, key, and value weights.", "dateLastCrawled": "2021-10-05T05:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Kernel <b>Self-Attention</b> for Weakly-supervised Image Classification using ...", "url": "https://www.readkong.com/page/kernel-self-attention-for-weakly-supervised-image-5842360", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/kernel-<b>self-attention</b>-for-weakly-supervised-image-5842360", "snippet": "<b>Self-Attention</b> is detecting the Retinal image screening dataset relationship between instances, so it can embed into the in- method accuracy F-score stance feature vectors the information about the presence LSA-AbMILP 76.3% 0.77 of <b>similar</b> instances or find that a combination of specific SA-AbMILP 75.2% 0.76 instances defines the bag as a positive. The experiments AbMILP 74.5% 0.74 on five datasets (MNIST, two histological datasets of breast GSA-AbMILP 74.5% 0.75 and colon cancer ...", "dateLastCrawled": "2022-01-29T18:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "FMAGAN:Fusing multiple attention and generative adversarial network to ...", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs211680", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs211680", "snippet": "At the same time, it should be noted that the introduction of <b>self-attention</b> greatly improved the calculated amount of the model, but it is undeniable that <b>self-attention</b> <b>also</b> improves the performance of the model. In order to achieve a better balance between model training speed and model performance, we added two CSAM modules into FMAGAN and achieved the best results in various indicators. The training time of the model increases significantly with the addition of CSAM modules. For this ...", "dateLastCrawled": "2022-02-03T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Deconstructing BERT, Part 2: Visualizing the Inner Workings of ...", "url": "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner...", "snippet": "(Note that <b>attention</b> is not the only component of BERT; there are <b>also</b> feed-forward layers, residual connections, and <b>layer</b> normalization modules that all work together with the <b>attention</b> component to produce the model output. But <b>attention</b> is the real workhorse, and so we\u2019ll focus on that. For more details on the other components, check out the tutorials in the references section.)", "dateLastCrawled": "2022-02-03T00:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Finding Strong Gravitational Lenses Through <b>Self-Attention</b> | DeepAI", "url": "https://deepai.org/publication/finding-strong-gravitational-lenses-through-self-attention", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/finding-strong-gravitational-<b>lens</b>es-through-<b>self-attention</b>", "snippet": "A physical interpretation of <b>self-attention</b> applied to feature vectors <b>can</b> <b>be thought</b> of as filtering the input features based on the correlation in the input. The structure of a multi-head attention <b>layer</b> is given in Fig. 2. It is possible to give the <b>self-attention</b> more power by creating several layers and dividing the input vector into smaller parts (H, number of heads). Then each attention <b>layer</b> is <b>called</b> a head and applies <b>self-attention</b> to one part of the divided input. 3.2.1 ...", "dateLastCrawled": "2022-01-11T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "How <b>self attention</b> works when processing the word \u201cits\u201d. The attention <b>layer</b> has weights for each word, enabling the <b>layer</b> to created a \u201cweighted-mix\u201d of words as the output. Essentially the gray-box encodes information about the word \u201cits\u201d and \u201cdog\u201d. Note: You <b>can</b> <b>also</b> see that there is a masked <b>self-attention</b> <b>layer</b> in the ...", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Frontiers | <b>Attention in Psychology, Neuroscience, and Machine Learning</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fncom.2020.00029", "snippet": "These weightings scale the word encodings themselves to create the next <b>layer</b> in the model, a process known as \u201c<b>self-attention</b>.\u201d This process repeats, and eventually interacts with the autoregressive decoder which <b>also</b> has attention mechanisms that allow it to flexibly focus on the encoded input (as in the standard form of attention) and on the previously generated output. The Transformer\u2014the name given to this new attention architecture\u2014outperformed many previous models and quickly ...", "dateLastCrawled": "2022-02-03T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machine learning applications for therapeutic tasks with genomics data", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8515011/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8515011", "snippet": "The human genome <b>can</b> <b>be thought</b> of as the instructions for building functional individuals. DNA sequences encode these instructions. Like a computer, for which we build a program based on 0/1 bit, the basic DNA sequence units are <b>called</b> nucleotides (A, C, G, and T). Given a list of nucleotides, a cell <b>can</b> build a diverse range of functional entities (programs). There are approximately 3 billion base pairs for the human genome, and more than 99.9% are identical between individuals. If a ...", "dateLastCrawled": "2022-01-03T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Deep Graph Mapper: Seeing Graphs Through the Neural <b>Lens</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8285761/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8285761", "snippet": "where the <b>lens</b> function f \u03b8 is a GNN <b>layer</b>, ... we <b>can</b> compute the adjacency matrix of the pooled graph as S T (A + I) S; the features are given by S T X. This method <b>can</b> <b>also</b> <b>be thought</b> as a version of DiffPool (Ying et al., 2018), where the low-entropy constraint on the cluster assignment distribution is topologically satisfied, since a point cannot be equally close to many other points on a line. Therefore, each node will belong only to a few clusters if the scale \u03b4 is appropriately set ...", "dateLastCrawled": "2021-08-13T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Galerkin Transformer: A One-Shot Experiment at NeurIPS 2021 - An ...", "url": "https://scaomath.github.io/blog/galerkin-transformer/", "isFamilyFriendly": true, "displayUrl": "https://scaomath.github.io/blog/galerkin-transformer", "snippet": "In many papers on the interpretation of the <b>self-attention</b> mechanism in Attention is all you need 1, including some recent attempts, making analogies with a \u201ckernel interpretation\u201d 2 3, or say linking the $\\text{Softmax}(QK^T)V$ as a learnable kernel map is a common practice. $\\text{Softmax}(QK^T)$ is described as a similarity measure for each position\u2019s feature vector (learned embeddings of the tokens) with that of every other position.", "dateLastCrawled": "2022-01-27T12:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deconstructing BERT, Part 2: Visualizing the Inner Workings of ...", "url": "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner...", "snippet": "(Note that <b>attention</b> is not the only component of BERT; there are <b>also</b> feed-forward layers, residual connections, and <b>layer</b> normalization modules that all work together with the <b>attention</b> component to produce the model output. But <b>attention</b> is the real workhorse, and so we\u2019ll focus on that. For more details on the other components, check out the tutorials in the references section.)", "dateLastCrawled": "2022-02-03T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>TWU Assessment Exam 3 (Ch 5</b>,14,15,16,22,23) - Quizlet", "url": "https://quizlet.com/386699892/twu-assessment-exam-3-ch-51415162223-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/386699892/<b>twu-assessment-exam-3-ch</b>-51415162223-flash-cards", "snippet": "-extent index finger onto the <b>lens</b> selector dial if refocus is needed-move without taking your face away from the instrument-dark room-have pt pick spot over shoulder-match sides with pt -place free hand on pt shoulder or head-6 inches from pt at 25 degree angle lateral to line of vision. Abnormalities. of the eyelid-Blepharitis-inflammation of lids-Periorbital edema- sinuses or angioedema -exophthalmos- protruding eyes from hyperthyroidism-enophthalmos- sunken eyes -ptosis- drooping upper ...", "dateLastCrawled": "2021-04-14T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Machine Learning Applications for Therapeutic Tasks with Genomics Data ...", "url": "https://www.readkong.com/page/machine-learning-applications-for-therapeutic-tasks-with-4264711", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/machine-learning-applications-for-therapeutic-tasks-with...", "snippet": "The human genome <b>can</b> <b>be thought</b> of as the instructions for building functional individuals. DNA sequences encode these instructions. Like a computer, where we build a program based on 0/1 bit, the basic DNA sequence units are <b>called</b> nucleotides (A, C, G, and T). Given a list of nucleotides, a cell <b>can</b> build a diverse range of functional entities (programs). There are approximately 3 billion base pairs for the human genome, and more than 99.9% are identical between individuals. If a subset of ...", "dateLastCrawled": "2022-01-18T18:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Nursing Asessment part 2 Flashcards | Quizlet", "url": "https://quizlet.com/17604809/nursing-asessment-part-2-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/17604809/nursing-asessment-part-2-flash-cards", "snippet": "Start studying Nursing Asessment part 2. Learn vocabulary, terms, and more with flashcards, games, and other study tools.", "dateLastCrawled": "2018-09-12T07:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Finding Strong Gravitational Lenses Through <b>Self-Attention</b> | DeepAI", "url": "https://deepai.org/publication/finding-strong-gravitational-lenses-through-self-attention", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/finding-strong-gravitational-<b>lens</b>es-through-<b>self-attention</b>", "snippet": "So, the <b>self-attention</b> layers <b>can</b> only filter the features and improve the accuracy by a small percentage (e.g. CNN 2 - 86.74% and <b>Lens</b> Detector 2 (CNN 2 as the backbone) - 88.13%). Nevertheless, for a model without transfer learning, there is a possibility for the CNN part in the encoder model to learn more features than a solo CNN about the image and improve the accuracy much better (e.g. <b>Lens</b> Detector 7 (CNN 3 backbone) - 91.45% and <b>Lens</b> Detector 15 (CNN 3 as the backbone without transfer ...", "dateLastCrawled": "2022-01-11T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "How <b>self attention</b> works when processing the word \u201cits\u201d. The attention <b>layer</b> has weights for each word, enabling the <b>layer</b> to created a \u201cweighted-mix\u201d of words as the output. Essentially the gray-box encodes information about the word \u201cits\u201d and \u201cdog\u201d. Note: You <b>can</b> <b>also</b> see that there is a masked <b>self-attention</b> <b>layer</b> in the ...", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "On the relationship between <b>self-attention</b> and convolution | Request PDF", "url": "https://www.researchgate.net/publication/343714257_On_the_relationship_between_self-attention_and_convolution", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/343714257_On_the_relationship_between_self...", "snippet": "For example, to replace a convolution <b>layer</b> with 7 \u00d7 7 filters (in the initial <b>layer</b>) with a <b>self-attention</b> <b>layer</b>, at least 49 attention heads need to be deployed [145]. Figure 7.2: ULSAM divides ...", "dateLastCrawled": "2022-01-29T07:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "notes/jurafsky-slp-ch11-transfer.md at master \u00b7 makrai/notes \u00b7 GitHub", "url": "https://github.com/makrai/notes/blob/master/jurafsky-slp-ch11-transfer.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/makrai/notes/blob/master/jurafsky-slp-ch11-transfer.md", "snippet": "without syntactic parse is to use a standard <b>self-attention</b> <b>layer</b> to g i j = SelfATTN(h i: j ) (11.20) eg named entity recognition (NER). Given a scheme for representing spans and set of named entity types, a span-based approach to NER is a straightforward classification problem; assign a label y, from the set of valid NER labels, to span in S(x).", "dateLastCrawled": "2022-01-21T19:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A review on the attention mechanism of <b>deep learning</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S092523122100477X", "snippet": "applied the <b>self-attention</b> mechanism to the computer vision task to solve this problem, <b>called</b> non-local attention, as shown in Fig. 13. They proposed the non-local module that got attention masks by calculating the correlation matrix between each spatial point in the feature map, then the attention guided dense contextual information to aggregate. However, this method <b>also</b> has the following problems: 1) Only the positional attention module is involved, not the commonly used channel ...", "dateLastCrawled": "2022-02-02T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Salient Object Detection Combining a <b>Self-Attention</b> Module and a ...", "url": "https://www.researchgate.net/publication/346253088_Salient_Object_Detection_Combining_a_Self-Attention_Module_and_a_Feature_Pyramid_Network", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/346253088_Salient_Object_Detection_Combining...", "snippet": "the score prediction for each pixel, the features will <b>also</b> be diluted, which could decrease the ability of. object localization. In this paper, we propose a novel pyramid <b>self-attention</b> module ...", "dateLastCrawled": "2022-01-10T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Hugging Face Pre-trained Models: Find the Best One for Your Task ...", "url": "https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best", "snippet": "The decoder is similar in structure to the encoder except that it includes a standard attention mechanism after each <b>self-attention</b> <b>layer</b> that attends to the output of the encoder. It <b>also</b> uses a form of autoregressive or causal <b>self-attention</b>, which allows the model to attend to past outputs. The T5 model was trained on unlabeled data which was generated using a cleaner version of common crawl, Colossal Clean Crawled Corpus(C4). With the help of a text-to-text transformer and a new pre ...", "dateLastCrawled": "2022-02-02T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "2020b a new sequence to sequence pre trained model which is pre trained ...", "url": "https://www.coursehero.com/file/p5ht2fk9o/2020b-a-new-sequence-to-sequence-pre-trained-model-which-is-pre-trained-by/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/p5ht2fk9o/2020b-a-new-sequence-to-sequence-pre-trained...", "snippet": "The main reason may be that our method uses <b>self-attention</b> <b>layer</b> to replace both <b>self-attention</b> <b>layer</b> and cross-attention <b>layer</b> in the NAT decoder part. cross-attention <b>layer</b> in the NAT decoder part. Ablation Study Influence of Mixup Source and Target Methods To study the influence of MIST training, we propose a static pseudo targets mix training method (static mixing), which generates pseudo targets from the trained models, and then training with both mixing data and original data. The ...", "dateLastCrawled": "2022-01-14T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "FMAGAN:Fusing multiple attention and generative adversarial network to ...", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs211680", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs211680", "snippet": "At the same time, it should be noted that the introduction of <b>self-attention</b> greatly improved the calculated amount of the model, but it is undeniable that <b>self-attention</b> <b>also</b> improves the performance of the model. In order to achieve a better balance between model training speed and model performance, we added two CSAM modules into FMAGAN and achieved the best results in various indicators. The training time of the model increases significantly with the addition of CSAM modules. For this ...", "dateLastCrawled": "2022-02-03T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Non-invasive <b>Self-attention</b> for Side Information Fusion in Sequential ...", "url": "https://www.readkong.com/page/non-invasive-self-attention-for-side-information-fusion-in-2349765", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/non-invasive-<b>self-attention</b>-for-side-information-fusion...", "snippet": "In Sec. 3.1 to 3.3, we specify visualization analysis for better interpretability. the research problem and explain side information, BERT, and <b>self-attention</b>. Then, we present our Non-invasive Self- 2 Related Works attention and different fusion operations in Sec. 3.4. Finally, the NOVA-BERT model is illustrated in Sec. 3.5.", "dateLastCrawled": "2022-02-01T14:42:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention in Psychology, Neuroscience, and <b>Machine</b> <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7177153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7177153", "snippet": "These weightings scale the word encodings themselves to create the next <b>layer</b> in the model, a process known as \u201c<b>self-attention</b>.\u201d This process repeats, and eventually interacts with the autoregressive decoder which <b>also</b> has attention mechanisms that allow it to flexibly focus on the encoded input (as in the standard form of attention) and on the previously generated output. The Transformer\u2014the name given to this new attention architecture\u2014outperformed many previous models and quickly ...", "dateLastCrawled": "2021-12-27T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>learning</b> glossary - DataTime", "url": "https://www.dtalg.com/article-1/", "isFamilyFriendly": true, "displayUrl": "https://www.dtalg.com/article-1", "snippet": "<b>self-attention</b> (<b>also</b> <b>called</b> <b>self-attention</b> <b>layer</b>) #language. A neural network <b>layer</b> that transforms a sequence of embeddings (for instance, token embeddings) into another sequence of embeddings. Each embedding in the output sequence is constructed by integrating information from the elements of the input sequence through an attention mechanism.", "dateLastCrawled": "2022-01-25T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "10.6. <b>Self-Attention</b> and <b>Positional Encoding</b> \u2014 Dive into Deep <b>Learning</b> ...", "url": "http://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_attention-mechanisms/<b>self-attention</b>-and-<b>positional-encoding</b>.html", "snippet": "In deep <b>learning</b>, we often use CNNs or RNNs to encode a sequence. Now with attention mechanisms, imagine that we feed a sequence of tokens into attention pooling so that the same set of tokens act as queries, keys, and values. Specifically, each query attends to all the key-value pairs and generates one attention output. Since the queries, keys, and values come from the same place, this performs <b>self-attention</b> [Lin et al., 2017b] [Vaswani et al., 2017], which is <b>also</b> <b>called</b> intra-attention ...", "dateLastCrawled": "2022-02-02T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Lecture 7: Transformers</b> - Deep <b>Learning</b>", "url": "https://chinmayhegde.github.io/dl-notes/notes/lecture07/", "isFamilyFriendly": true, "displayUrl": "https://chinmayhegde.github.io/dl-notes/notes/lecture07", "snippet": "<b>Self-Attention</b>. This is the point where papers-blogs-tweets-slides etc start talking about keys/values and attention mechanisms and everything goes a bit haywire. Let\u2019s just ignore all that for now, and instead talk about something <b>called</b> <b>self-attention</b>. The use of the \u201cself-\u201c prefix will become clear later on. Here is how it is defined.", "dateLastCrawled": "2022-02-03T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The encoder is composed of a stack of N = 6 identical layers. Each <b>layer</b> has two sub-layers. The first is a multi-head <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by <b>layer</b> normalization. That is, the output ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "<b>Self attention</b>, sometimes <b>called</b> intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. In simpler terms, <b>self attention</b> helps us create similar connections but within the same sentence. Look at the following example: \u201cI poured water from the bottle into the cup until it was full.\u201d it =&gt; cup \u201cI poured water from the bottle into the cup until it was empty.\u201d it=&gt; bottle. By changing one word ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning Papers: Molecules</b> - <b>Machine Learning</b> Applied", "url": "https://machinelearningapplied.com/machine-learning-papers-molecules/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>applied.com/<b>machine-learning-papers-molecules</b>", "snippet": "A <b>self-attention</b> based message passing neural network for predicting molecular lipophilicity and aqueous solubility - Tang et al 2020 . Efficient and accurate prediction of molecular properties, such as lipophilicity and solubility, is highly desirable for rational compound design in chemical and pharmaceutical industries. To this end, we build and apply a graph-neural-network framework <b>called</b> <b>self-attention</b>-based message-passing neural network (SAMPN) to study the relationship between ...", "dateLastCrawled": "2021-12-22T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Seven Myths in Machine Learning Research</b> | DeepAI", "url": "https://deepai.org/publication/seven-myths-in-machine-learning-research", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>seven-myths-in-machine-learning-research</b>", "snippet": "Importantly, Vaswani et al. noted that \u201dthe computational cost of a separable convolution is equal to the combination of a <b>self-attention</b> <b>layer</b> and a point-wise feed-forward <b>layer</b>.\u201d Even state-of-the-art GANS find <b>self-attention</b> superior to standard convolutions in its ability to model long-range, multi-scale dependencies [Zhang et al., 2018 ] .", "dateLastCrawled": "2022-01-12T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to perform Text Summarization with Python, HuggingFace Transformers ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2020/12/21/easy-text-summarization-with-hugging...", "snippet": "Summary &amp; Example: Text Summarization with Transformers. Transformers are taking the world of language processing by storm. These models, which learn to interweave the importance of tokens by means of a mechanism <b>called</b> <b>self-attention</b> and without recurrent segments, have allowed us to train larger models without all the problems of recurrent neural networks.", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Misnomers and Confusing Terms in Machine Learning</b>", "url": "https://product.hubspot.com/blog/misnomers-and-confusing-terms-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://product.hubspot.com/blog/<b>misnomers-and-confusing-terms-in-machine-learning</b>", "snippet": "The standard presentation of a multi-<b>layer</b> perceptron includes the statement that this architecture is composed of at least three layers of neurons: an input <b>layer</b>, a hidden <b>layer</b>, and an output <b>layer</b> (Haykin 2009, page 21). An artificial neuron (sorry again, Chollet) is supposed to 1) receive inputs, 2) combine them (often linearly), and 3) produce an output (often non-linearly). Instead, the neurons in the input <b>layer</b> start with a value, do nothing, and hand it off to the next <b>layer</b>. They ...", "dateLastCrawled": "2022-02-03T08:05:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(self-attention (also called self-attention layer))  is like +(lens)", "+(self-attention (also called self-attention layer)) is similar to +(lens)", "+(self-attention (also called self-attention layer)) can be thought of as +(lens)", "+(self-attention (also called self-attention layer)) can be compared to +(lens)", "machine learning +(self-attention (also called self-attention layer) AND analogy)", "machine learning +(\"self-attention (also called self-attention layer) is like\")", "machine learning +(\"self-attention (also called self-attention layer) is similar\")", "machine learning +(\"just as self-attention (also called self-attention layer)\")", "machine learning +(\"self-attention (also called self-attention layer) can be thought of as\")", "machine learning +(\"self-attention (also called self-attention layer) can be compared to\")"]}