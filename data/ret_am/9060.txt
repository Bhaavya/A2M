{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Value</b>-based Methods in Deep <b>Reinforcement Learning</b> | by Barak Or ...", "url": "https://towardsdatascience.com/value-based-methods-in-deep-reinforcement-learning-d40ca1086e1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>value</b>-based-methods-in-deep-<b>reinforcement-learning</b>-d40...", "snippet": "To promise optimal <b>value</b>: <b>state-action</b> pairs are represented discretely, and all actions are repeatedly sampled in all states. Q-<b>Learning</b> . Q <b>learning</b> in an off-policy method learns the <b>value</b> of taking action in a state and <b>learning</b> Q <b>value</b> and choosing how to act in the world. We define <b>state-action</b> <b>value</b> <b>function</b>: an expected return when starting in s, performing a, and following pi. Represented in a tabulated form. According to Q <b>learning</b>, the agent uses any policy to estimate Q that ...", "dateLastCrawled": "2022-01-29T11:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Value</b> <b>Function</b> <b>Approximation</b> \u2014 Prediction Algorithms | by Reuben ...", "url": "https://towardsdatascience.com/value-function-approximation-prediction-algorithms-98722818501b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>value</b>-<b>function</b>-<b>approximation</b>-prediction-<b>algorithms</b>...", "snippet": "<b>Value</b> <b>function</b> <b>approximation</b> tries to build some <b>function</b> to estimate the true <b>value</b> <b>function</b> by creating a compact representation of the <b>value</b> <b>function</b> that uses a smaller amount of parameters: A common practice is using deep <b>learning</b> \u2014 in that case, the weights of the neural network are the vector of weights w that will be used to estimate the <b>value</b> <b>function</b> across the entire state/<b>state-action</b> space.", "dateLastCrawled": "2022-01-30T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Q-<b>learning</b>: a <b>value</b>-based reinforcement <b>learning</b> <b>algorithm</b> | by Dhanoop ...", "url": "https://medium.com/intro-to-artificial-intelligence/q-learning-a-value-based-reinforcement-learning-algorithm-272706d835cf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/q-<b>learning</b>-a-<b>value</b>-based...", "snippet": "As we discussed in the action-<b>value</b> <b>function</b>, the above equation indicates how we compute the Q-<b>value</b> for an action a starting from state s in Q <b>learning</b>. It is the sum of immediate reward using a ...", "dateLastCrawled": "2022-01-29T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning Value Functions</b> \u2013 Ben Haanstra \u2013 Reinforcement <b>Learning</b> for ...", "url": "https://kofzor.github.io/Learning_Value_Functions/", "isFamilyFriendly": true, "displayUrl": "https://kofzor.github.io/<b>Learning_Value_Functions</b>", "snippet": "An action-<b>value</b> <b>function</b> or more commonly known as Q-<b>function</b> is a simple extension of the above that also accounts for actions. It is used to map combinations of states and actions to values. A single combination is often referred to as a <b>state-action</b> pair, and its <b>value</b> as a (policy) action-<b>value</b>. We use to denote the Q-<b>function</b> when following on , and let denote the action-<b>value</b> of a <b>state-action</b> pair . In the literature, it is common to leave out both and . The action-<b>value</b> is then ...", "dateLastCrawled": "2022-01-02T17:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Influence <b>Value</b> Q-<b>Learning</b>: A Reinforcement <b>Learning</b> <b>Algorithm</b> for ...", "url": "https://cdn.intechopen.com/pdfs/6185/InTech-Influence_value_q_learning_a_reinforcement_learning_algorithm_for_multi_agent_systems.pdf", "isFamilyFriendly": true, "displayUrl": "https://cdn.intechopen.com/pdfs/6185/InTech-Influence_<b>value</b>_q_<b>learning</b>_a_reinforcement...", "snippet": "Reinforcement <b>learning</b> algorithms calculate a <b>value</b> <b>function</b> for state predicates or for <b>state-action</b> pairs, having as goal the definition of a policy that best take advantage of these values. Q-<b>learning</b> (Watkins, 1989) is one of the most us ed reinforcement <b>learning</b> algorithms. It was widely applied in several problems <b>like</b> <b>learning</b> in robotics (Suh et al., 1997; Gu &amp; Hu, 2005), channel assignment in mobile communi cation systems (Junhong &amp; Haykin, 1999), in the block-pushing problem ...", "dateLastCrawled": "2022-01-05T23:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Reinforcement Learning Tutorial</b> - Javatpoint", "url": "https://www.javatpoint.com/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/reinforcement-<b>learning</b>", "snippet": "It learns the <b>value</b> <b>function</b> Q (S, a), which means how good to take action &quot;a&quot; at a particular state &quot;s.&quot; The below flowchart explains the working of Q- <b>learning</b>: <b>State Action</b> Reward <b>State action</b> (SARSA): SARSA stands for <b>State Action</b> Reward <b>State action</b>, which is an on-policy temporal difference <b>learning</b> method. The on-policy control method ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Epsilon-Greedy Q-learning</b> | Baeldung on Computer Science", "url": "https://www.baeldung.com/cs/epsilon-greedy-q-learning", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>epsilon-greedy-q-learning</b>", "snippet": "This is called the action-<b>value</b> <b>function</b> or Q-<b>function</b>. The <b>function</b> approximates the <b>value</b> of selecting a certain action in a certain state. In this case, is the action-<b>value</b> <b>function</b> learned by the <b>algorithm</b>. approximates the optimal action-<b>value</b> <b>function</b> . The output of the <b>algorithm</b> is calculated values. A Q-table for states and actions looks <b>like</b> this: An easy application of Q-<b>learning</b> is pathfinding in a maze, where the possible states and actions are trivial. With Q-<b>learning</b>, we can ...", "dateLastCrawled": "2022-01-30T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "QMIX: Monotonic <b>Value</b> <b>Function</b> Factorisation for Deep Multi-Agent ...", "url": "https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/rashidicml18.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/rashidicml18.pdf", "snippet": "(VDN), which allow for centralised <b>value</b>-<b>function</b> <b>learning</b> with decentralised execution. Their <b>algorithm</b> decomposes a central <b>state-action</b> <b>value</b> <b>function</b> into a sum of individual agent terms. This corresponds to the use of a degenerate fully disconnected coordination graph. VDN does not make use of additional state information during training ...", "dateLastCrawled": "2022-01-30T04:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What would a <b>state-action</b> <b>value</b> <b>function</b> learn if we placed multiple ...", "url": "https://www.quora.com/What-would-a-state-action-value-function-learn-if-we-placed-multiple-goals-on-the-state-space-and-moved-from-a-starting-point-to-a-goal-and-then-from-goal-to-goal-using-reinforcement-learning-with-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-would-a-<b>state-action</b>-<b>value</b>-<b>function</b>-learn-if-we-placed...", "snippet": "Answer (1 of 2): It depends on how you define the reward <b>function</b>, how far away the goal states are from each other, whether you are using discounting, whether goal states are absorbing, and what are the available actions (can you choose not to move?). If goals are not absorbing, for instance, th...", "dateLastCrawled": "2022-01-24T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Q_Learning_Simple</b> - Artificial Inteligence", "url": "https://leonardoaraujosantos.gitbook.io/artificial-inteligence/artificial_intelligence/reinforcement_learning/qlearning_simple", "isFamilyFriendly": true, "displayUrl": "https://leonardoaraujosantos.gitbook.io/.../reinforcement_<b>learning</b>/<b>qlearning_simple</b>", "snippet": "<b>Machine</b> <b>Learning</b>. Artificial Intelligence. OpenAI Gym. Tree Search. Markov Decision process. Reinforcement <b>Learning</b>. <b>Q_Learning_Simple</b>. Deep Q <b>Learning</b>. Deep Reinforcement <b>Learning</b> . Natural Language Processing. Appendix. Powered By GitBook. <b>Q_Learning_Simple</b>. Introduction. Q_<b>Learning</b> is a model free reinforcement <b>learning</b> technique. Here we are interested on finding through experiences with the environment the action-<b>value</b> <b>function</b> Q. When the Q <b>function</b> is found we can achieve optimal ...", "dateLastCrawled": "2022-01-30T06:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement <b>Learning</b>: Introduction to Policy Gradients | by Cheng Xi ...", "url": "https://medium.com/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/reinforcement-<b>learning</b>-introduction-to-policy...", "snippet": "In (5), we can replace the <b>state-action</b> <b>value</b> <b>function</b> with G\u209c, the cumulative discounted reward at timestep t. Then, we can simplify the equation using the fact d/dx[ln(x)] = 1/x.", "dateLastCrawled": "2022-01-28T08:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Function Approximation</b> in Reinforcement <b>Learning</b> | by Ziad SALLOUM ...", "url": "https://towardsdatascience.com/function-approximation-in-reinforcement-learning-85a4864d566", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>function-approximation</b>-in-reinforcement-<b>learning</b>-85a...", "snippet": "where \ud835\udf39 \ud835\udf6b(s) is roughly the derivative of J(\ud835\udf3d) relative to \ud835\udf3d, and \u237a is the <b>learning</b> rate ]0, 1]. IMPORTANT NOTE: Actually the derivation of J(\ud835\udf3d) relative to \ud835\udf3d is \ud835\udf39 [\ud835\udf6b(St)-\ud835\udf6b(St+1)], but in practice this <b>algorithm</b> has worse results.. We have established the <b>function approximation</b> for state-<b>value</b> <b>function</b>, now let\u2019s extend this notion to the action-<b>value</b> functions.", "dateLastCrawled": "2022-02-03T11:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Reinforcement Learning Tutorial</b> - Javatpoint", "url": "https://www.javatpoint.com/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/reinforcement-<b>learning</b>", "snippet": "It learns the <b>value</b> <b>function</b> Q (S, a), which means how good to take action &quot;a&quot; at a particular state &quot;s.&quot; The below flowchart explains the working of Q- <b>learning</b>: <b>State Action</b> Reward <b>State action</b> (SARSA): SARSA stands for <b>State Action</b> Reward <b>State action</b>, which is an on-policy temporal difference <b>learning</b> method. The on-policy control method ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Lecture 10: Q-Learning, Function</b> Approximation, Temporal Difference ...", "url": "http://katselis.web.engr.illinois.edu/ECE586/Lecture10.pdf", "isFamilyFriendly": true, "displayUrl": "katselis.web.engr.illinois.edu/ECE586/Lecture10.pdf", "snippet": "from other <b>machine</b> <b>learning</b> paradigms are summarized below: 1 ... The basic <b>learning</b> <b>algorithm</b> in this class is Q-<b>learning</b>. The aim of Q-<b>learning</b> is to approximate the optimal action-<b>value</b> <b>function</b> Qby generating a sequence fQ^ kg k 0 of such functions. The underlying idea is that if Q^ kis \u201cclose\u201d to Qfor some k, then the corresponding greedy policy with respect to Q^ kwill be close to the optimal policy which is greedy with respect to Q. 10.1 Q-<b>function</b> and Q-<b>learning</b> The Q-<b>learning</b> ...", "dateLastCrawled": "2022-02-02T05:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> Models - Javatpoint", "url": "https://www.javatpoint.com/machine-learning-models", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>machine</b>-<b>learning</b>-models", "snippet": "A <b>machine</b> <b>learning</b> model <b>is similar</b> to computer software designed to recognize patterns or behaviors based on previous experience or data. The <b>learning</b> <b>algorithm</b> discovers patterns within the training data, and it outputs an ML model which captures these patterns and makes predictions on new data. Let&#39;s understand an example of the ML model where we are creating an app to recognize the user&#39;s emotions based on facial expressions. So, creating such an app is possible by <b>Machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-02-02T20:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Types of Algorithms With Different <b>Machine Learning</b> <b>Algorithm</b> Examples", "url": "https://www.analytixlabs.co.in/blog/types-of-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.analytixlabs.co.in/blog/types-of-<b>machine-learning</b>", "snippet": "A supervised <b>machine learning</b> <b>algorithm</b> is actually told what to look for, and so it does until it finds the underlying patterns that yield the expected output to a satisfactory degree of accuracy. In other words, using these prior known outputs, the <b>machine learning</b> <b>algorithm</b> learns from the past data and then generates an equation for the label or the <b>value</b>. This stage is called the training stage. The <b>learning</b> <b>algorithm</b> tries to modify and improve the above <b>function</b> by comparing its ...", "dateLastCrawled": "2022-01-31T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Function</b> Approximation \u2014 <b>Machine</b> <b>Learning</b> for Scientists", "url": "https://ml-lectures.org/docs/reinforcement_learning/ml_reinforcement-learning-5.html", "isFamilyFriendly": true, "displayUrl": "https://ml-lectures.org/docs/reinforcement_<b>learning</b>/ml_reinforcement-<b>learning</b>-5.html", "snippet": "<b>Machine</b> <b>Learning</b> for Scientists. Lecture Introduction Structuring Data without Neural Networks Principle Component Analysis ... even if we could store all the values, the probability of visiting all <b>state-action</b> pairs with the above algorithms becomes increasingly unlikely, in other words most states will never be visited during training. Ideally, we should thus identify states that are \u2018<b>similar</b>\u2019, assign them \u2018<b>similar</b>\u2019 <b>value</b>, and choose \u2018<b>similar</b>\u2019 actions when in these states ...", "dateLastCrawled": "2022-01-26T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "QMIX: Monotonic <b>Value</b> <b>Function</b> Factorisation for Deep Multi-Agent ...", "url": "https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/rashidicml18.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/rashidicml18.pdf", "snippet": "(VDN), which allow for centralised <b>value</b>-<b>function</b> <b>learning</b> with decentralised execution. Their <b>algorithm</b> decomposes a central <b>state-action</b> <b>value</b> <b>function</b> into a sum of individual agent terms. This corresponds to the use of a degenerate fully disconnected coordination graph. VDN does not make use of additional state information during training ...", "dateLastCrawled": "2022-01-30T04:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "comparison - What are the differences between Q-<b>Learning</b> and A* ...", "url": "https://ai.stackexchange.com/questions/23072/what-are-the-differences-between-q-learning-and-a", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/.../23072/what-are-the-differences-between-q-<b>learning</b>-and-a", "snippet": "Q-<b>learning</b> and A* can both be viewed as search algorithms, but, apart from that, they are not very <b>similar</b>. Q-<b>learning</b> is a reinforcement <b>learning</b> <b>algorithm</b>, i.e. an <b>algorithm</b> that attempts to find a policy or, more precisely, <b>value</b> <b>function</b> (from which the policy can be derived) by taking stochastic moves (or actions) with some policy (which is different from the policy you want to learn), such as the $\\epsilon$-greedy policy, given the current estimate of the <b>value</b> <b>function</b>.Q-<b>learning</b> is a ...", "dateLastCrawled": "2022-01-24T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Reinforcement <b>Learning</b> - What&#39;s the formula for the <b>value</b> <b>function</b> ...", "url": "https://datascience.stackexchange.com/questions/25678/reinforcement-learning-whats-the-formula-for-the-value-function", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/25678", "snippet": "The usual formula that I encounter about the <b>value</b> <b>function</b> V (s) is: V ( s) = R ( s) + m a x a \u2208 A \u2211 s \u2032 \u2208 S T ( s, a, s \u2032) V ( s \u2032) where S is the set of states, A the set of actions, T the transition model. T ( s, a, s \u2032) = P ( s t + 1 = s \u2032 | s t = s, a t = a) and R the reward <b>function</b>. Since I&#39;m working on a model-based ...", "dateLastCrawled": "2022-01-20T12:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Reinforcement learning applied to airline</b> revenue management | SpringerLink", "url": "https://link.springer.com/article/10.1057/s41272-020-00228-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1057/s41272-020-00228-4", "snippet": "In RL, the optimal policy <b>can</b> be extracted from the <b>state-action</b> <b>value</b> <b>function</b> \\(Q^{*}(s,a)\\), which <b>can</b> <b>be thought</b> of as the revenue to go from state s, given the agent takes an exploratory action a, then acting following the optimal policy until termination. The DP for the <b>state-action</b> <b>function</b> and its relation to the <b>value</b> <b>function</b> is given below. Once the <b>state-action</b> <b>value</b> <b>function</b> has been determined, the optimal policy is easily determined by", "dateLastCrawled": "2022-01-22T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b>- Reinforcement <b>Learning</b>: The Q <b>Learning</b> <b>Algorithm</b> with ...", "url": "https://www.i2tutorials.com/machine-learning-tutorial/machine-learning-reinforcement-learning-the-q-learning-algorithm-with-an-illustrative-example/", "isFamilyFriendly": true, "displayUrl": "https://www.i2tutorials.com/<b>machine</b>-<b>learning</b>-tutorial/<b>machine</b>-<b>learning</b>-reinforcement...", "snippet": "The learner\u2019s hypothesis is represented in this method by a big table with a single entry for each <b>state-action</b> pair. The <b>value</b> for (s, a)-the learner\u2019s present hypothesis about the real but unknown <b>value</b> Q is stored in the database entry for the pair (s, a) (s, a). Initially, the table <b>can</b> be populated with random values (though it is easier to understand the <b>algorithm</b> if one assumes initial values of zero). The agent repeatedly observes its present state s, picks an action a, performs ...", "dateLastCrawled": "2022-01-24T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What would a <b>state-action</b> <b>value</b> <b>function</b> learn if we placed multiple ...", "url": "https://www.quora.com/What-would-a-state-action-value-function-learn-if-we-placed-multiple-goals-on-the-state-space-and-moved-from-a-starting-point-to-a-goal-and-then-from-goal-to-goal-using-reinforcement-learning-with-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-would-a-<b>state-action</b>-<b>value</b>-<b>function</b>-learn-if-we-placed...", "snippet": "Answer (1 of 2): It depends on how you define the reward <b>function</b>, how far away the goal states are from each other, whether you are using discounting, whether goal states are absorbing, and what are the available actions (<b>can</b> you choose not to move?). If goals are not absorbing, for instance, th...", "dateLastCrawled": "2022-01-24T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement <b>Learning</b>: Introduction to Policy Gradients | by Cheng Xi ...", "url": "https://medium.com/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/reinforcement-<b>learning</b>-introduction-to-policy...", "snippet": "In (5), we <b>can</b> replace the <b>state-action</b> <b>value</b> <b>function</b> with G\u209c, the cumulative discounted reward at timestep t. Then, we <b>can</b> simplify the equation using the fact d/dx[ln(x)] = 1/x.", "dateLastCrawled": "2022-01-28T08:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>3.7 Value Functions</b>", "url": "http://www.incompleteideas.net/book/ebook/node34.html", "isFamilyFriendly": true, "displayUrl": "www.incompleteideas.net/book/ebook/node34.html", "snippet": "3.8 Optimal <b>Value</b> Functions Up: 3. The Reinforcement <b>Learning</b> Previous: 3.6 Markov Decision Processes Contents <b>3.7 Value Functions</b>. Almost all reinforcement <b>learning</b> algorithms are based on estimating <b>value</b> functions--functions of states (or of <b>state-action</b> pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of &quot;how good&quot; here is defined in terms of future rewards that <b>can</b> be expected, or, to be ...", "dateLastCrawled": "2022-02-02T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Reinforcement Learning: Prediction, Control and</b> <b>Value</b> <b>Function</b> ...", "url": "https://deepai.org/publication/reinforcement-learning-prediction-control-and-value-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>reinforcement-learning-prediction-control-and</b>-<b>value</b>...", "snippet": "With the increasing power of computers and the rapid development of self-<b>learning</b> methodologies such as <b>machine</b> <b>learning</b> and artificial intelligence, the problem of constructing an automatic Financial Trading Systems (FTFs) becomes an increasingly attractive research topic.An intuitive way of developing such a trading <b>algorithm</b> is to use Reinforcement <b>Learning</b> (RL) algorithms, which does not require model-building. In this paper, we dive into the RL algorithms and illustrate the definitions ...", "dateLastCrawled": "2022-01-16T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Natural way to construct stochastic policy from <b>value</b> <b>function</b>?", "url": "https://stats.stackexchange.com/questions/390104/natural-way-to-construct-stochastic-policy-from-value-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/390104", "snippet": "For people who stumble upon this question later on, I did find an answer to this question. The soft policy (or stochastic/probabilistic policy) constructed from a <b>state-action</b> <b>value</b> <b>function</b> in the way I described in my question is called a Boltzmann policy.It is defined as follows: $$ \\pi_\\text{Boltzmann}(a|s)\\ =\\ \\frac{\\text{e}^{Q(s, a) / \\tau}}{\\sum_b\\text{e}^{Q(s, b)/\\tau}}\\ =\\ \\text{softmax}_a\\left( \\frac{Q(s,a)}{\\tau} \\right) $$ The generalized temperature $\\tau$ is a free parameter ...", "dateLastCrawled": "2022-01-13T02:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reinforcement <b>Learning</b>: Q-<b>Algorithm</b> in a Match to Sample Task \u2013 <b>Machine</b> ...", "url": "https://unrealai.wordpress.com/2017/12/19/q-learning/", "isFamilyFriendly": true, "displayUrl": "https://unrealai.wordpress.com/2017/12/19/q-<b>learning</b>", "snippet": "So for the Q table to represent all the possible <b>state action</b> pairs the agent <b>can</b> take is 8 by 8, indicating all possible <b>state action</b> pairs. But since if the lights are off, it cannot travel to a state in which the light is on (no time travel allowed!) we use -1 in the Reward table to signify actions that cannot be taken. We also need some code to account for if the light changes while the agent is in particular state. See Figure 7.", "dateLastCrawled": "2022-02-01T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Policy Gradients: REINFORCE with Baseline | by Cheng Xi Tsou | Nerd For ...", "url": "https://medium.com/nerd-for-tech/policy-gradients-reinforce-with-baseline-6c871a3a068", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/policy-gradients-reinforce-with-baseline-6c871a3a068", "snippet": "A more complex baseline we <b>can</b> use is a state-<b>value</b> <b>function</b>. Since the <b>learning</b> for this <b>algorithm</b> is episodic, we <b>can</b> use a state-<b>value</b> <b>function</b> that leans episodically as well. G\u209c - (S ...", "dateLastCrawled": "2022-02-03T09:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Modern <b>Deep Reinforcement Learning</b> Algorithms | DeepAI", "url": "https://deepai.org/publication/modern-deep-reinforcement-learning-algorithms", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/modern-<b>deep-reinforcement-learning</b>-<b>algorithms</b>", "snippet": "As an object of desire is a strategy, i. e. a <b>function</b> mapping agent\u2019s observations to possible actions, <b>reinforcement learning</b> is considered to be a subfiled of <b>machine</b> <b>learning</b>.But instead of <b>learning</b> from data, as it is established in classical supervised and unsupervised <b>learning</b> problems, the agent learns from experience of interacting with environment. Being more &quot;natural&quot; model of <b>learning</b>, this setting causes new challenges, peculiar only to <b>reinforcement learning</b>, such as ...", "dateLastCrawled": "2022-01-30T15:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Value</b>-based Methods in Deep <b>Reinforcement Learning</b> | by Barak Or ...", "url": "https://towardsdatascience.com/value-based-methods-in-deep-reinforcement-learning-d40ca1086e1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>value</b>-based-methods-in-deep-<b>reinforcement-learning</b>-d40...", "snippet": "To promise optimal <b>value</b>: <b>state-action</b> pairs are represented discretely, and all actions are repeatedly sampled in all states. Q-<b>Learning</b> . Q <b>learning</b> in an off-policy method learns the <b>value</b> of taking action in a state and <b>learning</b> Q <b>value</b> and choosing how to act in the world. We define <b>state-action</b> <b>value</b> <b>function</b>: an expected return when starting in s, performing a, and following pi. Represented in a tabulated form. According to Q <b>learning</b>, the agent uses any policy to estimate Q that ...", "dateLastCrawled": "2022-01-29T11:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Reinforcement</b> <b>Learning</b> \u2014 The <b>Value</b> <b>Function</b> | by Jingles (Hong Jing ...", "url": "https://towardsdatascience.com/reinforcement-learning-value-function-57b04e911152", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement</b>-<b>learning</b>-<b>value</b>-<b>function</b>-57b04e911152", "snippet": "The other choice would be to place it at the bottom row. State M should have a higher significance and <b>value</b> as <b>compared</b> to state N because it results in a higher possibility of victory. Therefore, at any given state, we <b>can</b> perform the action that brings us (or the agent) closer to receiving a reward, by picking the state that yields us the highest <b>value</b>. Tic Tac Toe \u2014 Initialise the <b>Value</b> <b>Function</b>. The <b>Value</b> <b>function</b> V(s) for a tic-tac-toe game is the probability of winning for achieving ...", "dateLastCrawled": "2022-02-03T19:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Q-<b>learning</b>: a <b>value</b>-based reinforcement <b>learning</b> <b>algorithm</b> | by Dhanoop ...", "url": "https://medium.com/intro-to-artificial-intelligence/q-learning-a-value-based-reinforcement-learning-algorithm-272706d835cf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/q-<b>learning</b>-a-<b>value</b>-based...", "snippet": "As we discussed in the action-<b>value</b> <b>function</b>, the above equation indicates how we compute the Q-<b>value</b> for an action a starting from state s in Q <b>learning</b>. It is the sum of immediate reward using a ...", "dateLastCrawled": "2022-01-29T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement <b>Learning</b>: Introduction to Policy Gradients | by Cheng Xi ...", "url": "https://medium.com/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/reinforcement-<b>learning</b>-introduction-to-policy...", "snippet": "In (5), we <b>can</b> replace the <b>state-action</b> <b>value</b> <b>function</b> with G\u209c, the cumulative discounted reward at timestep t. Then, we <b>can</b> simplify the equation using the fact d/dx[ln(x)] = 1/x.", "dateLastCrawled": "2022-01-28T08:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Batch <b>Value</b>-<b>function</b> Approximation with Only Realizability", "url": "http://proceedings.mlr.press/v139/xie21d.html", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v139/xie21d.html", "snippet": "Our <b>algorithm</b>, BVFT, breaks the hardness conjecture (albeit under a stronger notion of exploratory data) via a tournament procedure that reduces the <b>learning</b> problem to pairwise comparison, and solves the latter with the help of a <b>state-action</b>-space partition constructed from the <b>compared</b> functions. We also discuss how BVFT <b>can</b> be applied to model selection among other extensions and open problems.", "dateLastCrawled": "2021-12-26T11:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Addressing <b>Value</b> Estimation Errors in Reinforcement <b>Learning</b> with a ...", "url": "https://deepai.org/publication/addressing-value-estimation-errors-in-reinforcement-learning-with-a-state-action-return-distribution-function", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/addressing-<b>value</b>-estimation-errors-in-reinforcement...", "snippet": "The overestimations of RL were first found in Q-<b>learning</b> <b>algorithm</b> (watkins1989Q-<b>learning</b>), which is the prototype of most existing <b>value</b>-based RL algorithms (sutton2018reinforcement).For this <b>algorithm</b>, van2016double_DQN demonstrated that any kind of estimation errors <b>can</b> induce an upward bias, irrespective of whether these errors are caused by system noise, <b>function</b> approximation, or any other sources. This overestimation bias is firstly induced by the max operator over all noisy Q ...", "dateLastCrawled": "2022-01-08T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Scaling Reward Values for Improved Deep Reinforcement <b>Learning</b>", "url": "https://emuccino.github.io/machine/learning,/reinforcement/learning/2019/02/18/scaling-reward-values-for-improved-deep-reinforcement-learning.html", "isFamilyFriendly": true, "displayUrl": "https://emuccino.github.io/<b>machine</b>/<b>learning</b>,/reinforcement/<b>learning</b>/2019/02/18/scaling...", "snippet": "Deep Reinforcement <b>Learning</b> involves using a neural network as a universal <b>function</b> approximator to learn a <b>value</b> <b>function</b> that maps <b>state-action</b> pairs to their expected future reward given a particular reward <b>function</b>. This <b>can</b> be done many different ways. For example, a Monte Carlo based <b>algorithm</b> will observe total rewards following <b>state-action</b> pairs from a complete episode to make build training data for the neural network. Alternatively, a Temporal Difference approach would use ...", "dateLastCrawled": "2022-01-09T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b>: Algorithms, Real-World Applications and Research ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7983091/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7983091", "snippet": "Supervised: Supervised <b>learning</b> is typically the task of <b>machine</b> <b>learning</b> to learn a <b>function</b> that maps an input to an output based on sample input-output pairs [].It uses labeled training data and a collection of training examples to infer a <b>function</b>. Supervised <b>learning</b> is carried out when certain goals are identified to be accomplished from a certain set of inputs [], i.e., a task-driven approach.The most common supervised tasks are \u201cclassification\u201d that separates the data, and ...", "dateLastCrawled": "2022-01-27T01:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Reinforcement <b>Learning</b> - What&#39;s the formula for the <b>value</b> <b>function</b> ...", "url": "https://datascience.stackexchange.com/questions/25678/reinforcement-learning-whats-the-formula-for-the-value-function", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/25678", "snippet": "The usual formula that I encounter about the <b>value</b> <b>function</b> V (s) is: V ( s) = R ( s) + m a x a \u2208 A \u2211 s \u2032 \u2208 S T ( s, a, s \u2032) V ( s \u2032) where S is the set of states, A the set of actions, T the transition model. T ( s, a, s \u2032) = P ( s t + 1 = s \u2032 | s t = s, a t = a) and R the reward <b>function</b>. Since I&#39;m working on a model-based ...", "dateLastCrawled": "2022-01-20T12:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Machine</b> <b>Learning</b> in Marketing: Overview, <b>Learning</b> Strategies ...", "url": "https://www.researchgate.net/publication/344000369_Machine_Learning_in_Marketing_Overview_Learning_Strategies_Applications_and_Future_Developments", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344000369_<b>Machine</b>_<b>Learning</b>_in_Marketing...", "snippet": "algorithms include the following: Q-<b>Learning</b>, <b>State-Action</b>-Reward-<b>State-Action</b> (SARSA), and Deep Q Network (DQN). RL is likely to be the form of ML that most approximates with AI .", "dateLastCrawled": "2022-02-01T20:54:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Relationship between state (V) and action(Q) <b>value</b> <b>function</b> in ...", "url": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and-action-q-value-function-in-reinforcement-learning-bb9a988c0127", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and...", "snippet": "<b>Value</b> <b>function</b> can be defined as the expected <b>value</b> of an agent in a certain state. There are two types of <b>value</b> functions in RL: State-<b>value</b> and action-<b>value</b>. It is important to understand the\u2026", "dateLastCrawled": "2022-02-03T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine learning for biochemical engineering: A</b> review - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1369703X21001303", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1369703X21001303", "snippet": "<b>Value</b>-based algorithms, typically represented by Q-<b>learning</b>, explicitly learn and optimise the <b>state-action</b> <b>value</b> <b>function</b> and generate the optimal policy by acting greedily with respect to it i.e. choosing the control corresponding to the maximum Q \u03c0 x, u <b>value</b> (<b>state-action</b> <b>value</b>). There are also hybrid algorithms, such as actor-critic methods, which combine policy optimisation methods and <b>value</b>-based methods. Although RL has shown success in game-based control benchmarks, such as AlphaGo", "dateLastCrawled": "2022-01-26T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "AI and Reinforcement <b>Learning</b> \u2014 Machines that Learn through Experience ...", "url": "https://www.cantorsparadise.com/ai-and-reinforcement-learning-machines-that-learn-through-experience-e7eea7bb6765", "isFamilyFriendly": true, "displayUrl": "https://www.cantorsparadise.com/ai-and-reinforcement-<b>learning</b>-<b>machines</b>-that-learn...", "snippet": "To align the policy with the updated <b>value</b> <b>function</b>, the algorithm modifies the policy so it would greedily follow the <b>value</b> <b>function</b> (meaning, choosing to perform actions that has the highest <b>value</b>). The algorithm continues by generating a new episode, now under the improved policy, which, in turn, derives a more accurate <b>value</b> estimation and so on. In this process, both the policy and the <b>value</b> <b>function</b> converge to their optimal values, until sufficient accuracy is reached, or when no more ...", "dateLastCrawled": "2022-01-25T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reinforcement Learning: Prediction, Control and</b> <b>Value</b> <b>Function</b> ...", "url": "https://deepai.org/publication/reinforcement-learning-prediction-control-and-value-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>reinforcement-learning-prediction-control-and</b>-<b>value</b>...", "snippet": "<b>Reinforcement Learning: Prediction, Control and Value Function Approximation</b>. With the increasing power of computers and the rapid development of self-<b>learning</b> methodologies such as <b>machine</b> <b>learning</b> and artificial intelligence, the problem of constructing an automatic Financial Trading Systems (FTFs) becomes an increasingly attractive research ...", "dateLastCrawled": "2022-01-16T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning Analogy for Meditation (illustrated</b>) - LessWrong 2.0 ...", "url": "https://www.greaterwrong.com/posts/chHhuCvmZqYLM32gz/machine-learning-analogy-for-meditation-illustrated", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/chHhuCvmZqYLM32gz/<b>machine</b>-<b>learning</b>-<b>analogy</b>-for...", "snippet": "<b>Machine Learning Analogy for Meditation (illustrated</b>) ... and the algorithm we use includes a <b>value</b> table: [picture: table, actions on x-axis, states on y-axis, cells of table are estimated values of taking actions in states] A <b>value</b> isn\u2019t just the learned estimate of the immediate reward which you get by taking an action in a state, but rather, the estimate of the eventual rewards, in total, from that action. This makes the values difficult to estimate. An estimate is improved by <b>value</b> it", "dateLastCrawled": "2022-01-17T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>value</b> of a <b>function</b>?", "url": "https://psichologyanswers.com/library/lecture/read/57841-what-is-value-of-a-function", "isFamilyFriendly": true, "displayUrl": "https://psichologyanswers.com/library/lecture/read/57841-what-is-<b>value</b>-of-a-<b>function</b>", "snippet": "What is a <b>value</b> <b>function</b> reinforcement <b>learning</b>? <b>Value</b> <b>function</b> Many reinforcement <b>learning</b> introduce the notion of `<b>value</b>-<b>function</b>` which often denoted as V(s) . The <b>value</b> <b>function</b> represent how good is a state for an agent to be in. It is equal to expected total reward for an agent starting from state s . What is optimal <b>value</b> <b>function</b>? The optimal <b>Value</b> <b>function</b> is one which yields maximum <b>value</b> compared to all other <b>value</b> <b>function</b>. When we say we are solving an MDP it actually means we ...", "dateLastCrawled": "2022-01-15T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>SARSA</b> vs Q - <b>learning</b>", "url": "https://tcnguyen.github.io/reinforcement_learning/sarsa_vs_q_learning.html", "isFamilyFriendly": true, "displayUrl": "https://tcnguyen.github.io/reinforcement_<b>learning</b>/<b>sarsa</b>_vs_q_<b>learning</b>.html", "snippet": "<b>SARSA</b> will learn the optimal $\\epsilon$-greedy policy, i.e, the Q-<b>value</b> <b>function</b> will converge to a optimal Q-<b>value</b> <b>function</b> but in the space of $\\epsilon$-greedy policy only (as long as each <b>state action</b> pair will be visited infinitely). We expect that in the limit of $\\epsilon$ decaying to $0$, <b>SARSA</b> will converge to the overall optimal policy. I quote here a paragraph from", "dateLastCrawled": "2022-01-30T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>learning</b> and AI <b>in marketing \u2013 Connecting computing power to</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167811620300410", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167811620300410", "snippet": "<b>State-Action</b>-Reward-<b>State-Action</b>: 2.2.3: SVD: Singular <b>Value</b> Decomposition: 2.2.2: SVM: Support Vector <b>Machine</b> : 2.2.1: TD: Temporal-Difference: 2.2.3: UGC: User-Generated Content: 3.1: Table 3. Strengths and weaknesses of <b>machine</b> <b>learning</b> methods. Strength \u2022 Ability to handle unstructured data and data of hybrid formats \u2022 Ability to handle large data volume \u2022 Flexible model structure \u2022 Strong predictive performance. Weakness \u2022 Not easy to interpret \u2022 Relationship typically ...", "dateLastCrawled": "2022-01-12T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Value</b>-<b>function-based transfer for reinforcement</b> <b>learning</b> using ...", "url": "https://www.academia.edu/2661041/Value_function_based_transfer_for_reinforcement_learning_using_structure_mapping", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2661041/<b>Value</b>_<b>function_based_transfer_for_reinforcement</b>...", "snippet": "Abstract Transfer <b>learning</b> concerns applying knowledge learned in one task (the source) to improve <b>learning</b> another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about <b>analogy</b> making, to . \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset ...", "dateLastCrawled": "2022-01-19T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Reinforcement Learning with Factored States</b> and Actions.", "url": "https://www.researchgate.net/publication/220320206_Reinforcement_Learning_with_Factored_States_and_Actions", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220320206_Reinforcement_<b>Learning</b>_with...", "snippet": "Restricted In [25], the authors use Restricted Bolzman <b>Machine</b> to deal with MDPs of large state and action spaces, by modeling the <b>state-action</b> <b>value</b> <b>function</b> with the negative free energy of the ...", "dateLastCrawled": "2022-01-15T11:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(state-action value function)  is like +(machine learning algorithm)", "+(state-action value function) is similar to +(machine learning algorithm)", "+(state-action value function) can be thought of as +(machine learning algorithm)", "+(state-action value function) can be compared to +(machine learning algorithm)", "machine learning +(state-action value function AND analogy)", "machine learning +(\"state-action value function is like\")", "machine learning +(\"state-action value function is similar\")", "machine learning +(\"just as state-action value function\")", "machine learning +(\"state-action value function can be thought of as\")", "machine learning +(\"state-action value function can be compared to\")"]}