{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "An <b>introduction to sequence-to-sequence learning</b>. Published: February 19, 2019. Many interesting problems in artificial intelligence can be described in the following way: Map a sequence of inputs $\\mathbf{x}$ to the correct sequence of outputs $\\mathbf{y}$. Speech recognition is one example: the goal is to map an audio signal $\\mathbf{x}$ (a sequence of real-valued audio samples) to the correct text transcript $\\mathbf{y}$ (a sequence of letters). Other examples are machine translation ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Differences between Autoregressive, Autoencoding and Sequence</b>-to ...", "url": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive-autoencoding-and-sequence-to-sequence-models-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive...", "snippet": "A <b>sequence-to-sequence</b> model is capable of ingesting a sequence of a particular kind and outputting another sequence of another kind. In general, it\u2019s the model architecture visualized above. Such models are also called Seq2Seq models. There are many applications of performing <b>sequence-to-sequence</b> learning.", "dateLastCrawled": "2022-02-02T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Graph</b> Neural Network and Some of GNN Applications: Everything You Need ...", "url": "https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>graph</b>-neural-network-and-some-of-gnn-applications", "snippet": "The neural machine translation (NMT) is considered a <b>sequence-to-sequence</b> <b>task</b>. One of GNN\u2019s common applications is to incorporate semantic information into the NMT <b>task</b>. To do this, we utilize the Syntactic GCN on syntax-aware NMT tasks. We can also use the GGNN in NMT. It converts the syntactic dependency <b>graph</b> into a new structure by turning the edges into additional nodes and thus edges labels can be represented as embeddings . Relation extraction. Deep Learning: <b>Graph</b> LSTM/ <b>graph</b> ...", "dateLastCrawled": "2022-01-29T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sequence-to-sequence learning with Transducers</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2020/11/transducer/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2020/11/transducer", "snippet": "The go-to models for sequence transduction problems are attention-based <b>sequence-to-sequence</b> models, <b>like</b> RNN encoder-decoder models or Transformers. Here\u2019s a diagram of an attention model. (In the diagrams below, I\u2019ll use red to indicate that a module has access to $\\mathbf{x}$, blue to indicate access to $\\mathbf{y}$, and purple to indicate access to both $\\mathbf{x}$ and $\\mathbf{y}$.) The model encodes the input $\\mathbf{x}$ into a sequence of feature vectors, then computes the ...", "dateLastCrawled": "2022-01-31T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Sequence Models by Andrew Ng \u2014 11 Lessons Learned - Towards Data Science", "url": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d...", "snippet": "Many-to-many models are commonly referred to as <b>sequence-to-sequence</b> models in the literature. Lesson 3: How do language models and sequence generation work? Language models make predictions by estimating the probability of the next word given the words that precede it. After you\u2019ve trained a language model, the conditional distributions you\u2019ve estimated may be used to sample novel sequences. In the homework exercises, you train a language model on Shakespeare text and generate novel ...", "dateLastCrawled": "2022-01-29T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>How I Used Deep Learning To Train A Chatbot</b> To Talk <b>Like</b> Me (Sorta ...", "url": "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/<b>How-I-Used-Deep-Learning-to-Train-a-Chatbot</b>-to-Talk-<b>Like</b>-Me", "snippet": "One of the most important characteristics of <b>sequence to sequence</b> models is the versatility that it provides. When you think of traditional ML methods (linear regression, SVMs) and deep learning methods <b>like</b> CNNs, these models require a fixed size input, and produce fixed size outputs as well. The lengths of your inputs must be known beforehand. This is a significant limitation to tasks such as machine translation, speech recognition, and question answering. These are tasks where we don&#39;t ...", "dateLastCrawled": "2022-01-30T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to Develop an Encoder-Decoder Model for <b>Sequence-to-Sequence</b> ...", "url": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence...", "snippet": "Here you can see how the recursive use of the model can be used to build up output sequences. During prediction, the inference_encoder model is used to encode the input sequence once which returns states that are used to initialize the inference_decoder model. From that point, the inference_decoder model is used to generate predictions step by step.. The function below named predict_sequence() can be used after the model is trained to generate a target sequence given a source sequence.", "dateLastCrawled": "2022-01-29T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Machine Learning.. Machine in sample word is \u201cAny thing\u2026 | by Ayaz ...", "url": "https://ayazmehmood550.medium.com/machine-learning-57056b68188d", "isFamilyFriendly": true, "displayUrl": "https://ayazmehmood550.medium.com/machine-learning-57056b68188d", "snippet": "Machine in sample word is \u201cAny thing which replace human for some <b>task</b>\u201d. We can take an example of <b>Computer</b>, Before the invention of the <b>computer</b>, there were people in every Organization whose job was to compute things. After the <b>computer</b> revolution, it is now the <b>computer</b>\u2019s job to do the calculation. Now, we will head on to explain ...", "dateLastCrawled": "2022-01-18T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Teaching Machines to Read and Comprehend</b> | DeepAI", "url": "https://deepai.org/publication/teaching-machines-to-read-and-comprehend", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>teaching-machines-to-read-and-comprehend</b>", "snippet": "Smarnet: <b>Teaching Machines to Read and Comprehend</b> <b>Like</b> Human Machine Comprehension (MC) is a challenging <b>task</b> in Natural Language Pro... Zheqian Chen, et al. \u2219. share 7 research \u2219 09/09/2019. Picture What you <b>Read</b> Visualization refers to our ability to create an image in our head based... Ignazio Gallo, et al. \u2219. share 0 research \u2219 05/02/2020. <b>Teaching</b> Machine Comprehension with Compositional Explanations Advances in extractive machine reading comprehension (MRC) rely heavily ...", "dateLastCrawled": "2022-02-02T10:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Question <b>Answering with Python, HuggingFace Transformers</b> and Machine ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-question-answering-with-machine-learning-and-huggingface-transformers/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinecurve</b>.com/index.php/2020/12/21/easy-question-answering-with-machine...", "snippet": "Question answering is one such <b>task</b> for which Machine Learning can be used. In this article, ... just <b>like</b> in <b>Computer</b> Vision \u2013 with e.g. the MobileNet architecture, and others. One of these approaches and the one that lies at the basis of today\u2019s Transformer-based Question Answering pipeline is the DistilBERT architecture, which was proposed in a 2019 paper by Sanh et al. Here\u2019s the abstract for the work. If you would <b>like</b> to <b>read</b> about DistilBERT in more detail I\u2019d suggest clicking ...", "dateLastCrawled": "2022-02-01T15:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Graph</b> Neural Network and Some of GNN Applications: Everything You Need ...", "url": "https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>graph</b>-neural-network-and-some-of-gnn-applications", "snippet": "The neural machine translation (NMT) is considered a <b>sequence-to-sequence</b> <b>task</b>. One of GNN\u2019s common applications is to incorporate semantic information into the NMT <b>task</b>. To do this, we utilize the Syntactic GCN on syntax-aware NMT tasks. We can also use the GGNN in NMT. It converts the syntactic dependency <b>graph</b> into a new structure by turning the edges into additional nodes and thus edges labels can be represented as embeddings . Relation extraction. Deep Learning: <b>Graph</b> LSTM/ <b>graph</b> ...", "dateLastCrawled": "2022-01-29T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>How I Used Deep Learning To Train A Chatbot</b> To Talk Like Me (Sorta ...", "url": "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/<b>How-I-Used-Deep-Learning-to-Train-a-Chatbot</b>-to-Talk-Like-Me", "snippet": "A <b>sequence to sequence</b> model is composed of 2 main components, an encoder RNN and a decoder RNN (If you\u2019re a little shaky on RNNs, check out my previous blog post for a refresher). From a high level, the encoder\u2019s job is to encapsulate the information of the input text into a fixed representation. The decoder\u2019s is to take that representation, and generate a variable length text that best responds to it.", "dateLastCrawled": "2022-01-30T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Sequence Models by Andrew Ng \u2014 11 Lessons Learned - Towards Data Science", "url": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d...", "snippet": "Many-to-many models are commonly referred to as <b>sequence-to-sequence</b> models in the literature. Lesson 3: How do language models and sequence generation work? Language models make predictions by estimating the probability of the next word given the words that precede it. After you\u2019ve trained a language model, the conditional distributions you\u2019ve estimated may be used to sample novel sequences. In the homework exercises, you train a language model on Shakespeare text and generate novel ...", "dateLastCrawled": "2022-01-29T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. TBD ... - <b>Computer</b> Science", "url": "https://www.cs.wm.edu/~denys/pubs/seq2seq4repair_TSE_cameraready.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.wm.edu/~denys/pubs/seq2seq4repair_TSE_camera<b>read</b>y.pdf", "snippet": "called \u201c<b>sequence-to-sequence</b> learning\u201d, where \u201csequence\u201d refers to the sequence of words in a sentence. An early example of a <b>sequence-to-sequence</b> network [5] used a re-current neural network to <b>read</b> in tokens and to generate an output sequence, as shown in Figure 1. Let us consider that the input tokens are denoted x t, and after ...", "dateLastCrawled": "2021-10-13T01:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Develop an Encoder-Decoder Model for <b>Sequence-to-Sequence</b> ...", "url": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence...", "snippet": "Here you can see how the recursive use of the model can be used to build up output sequences. During prediction, the inference_encoder model is used to encode the input sequence once which returns states that are used to initialize the inference_decoder model. From that point, the inference_decoder model is used to generate predictions step by step.. The function below named predict_sequence() can be used after the model is trained to generate a target sequence given a source sequence.", "dateLastCrawled": "2022-01-29T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Introduction to Transformers in Machine Learning</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in...", "snippet": "This architecture, which lies at the basis of all Transformer related activities today, has solved one of the final problems in <b>sequence-to-sequence</b> models: that of sequential processing. No recurrent segments are necessary anymore, meaning that networks can benefit from parallelism, significantly boosting the training process. In fact, today\u2019s Transformers are trained with millions of sequences, if not more.", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Teaching Machines to Read and Comprehend</b> | DeepAI", "url": "https://deepai.org/publication/teaching-machines-to-read-and-comprehend", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>teaching-machines-to-read-and-comprehend</b>", "snippet": "<b>Teaching</b> machines to <b>read</b> natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention ...", "dateLastCrawled": "2022-02-02T10:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[R] MIT Presents New Approach for <b>Sequence-to-Sequence</b> Learning with ...", "url": "https://www.reddit.com/r/deeplearning/comments/pq19r7/r_mit_presents_new_approach_for/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/deeplearning/comments/pq19r7/r_mit_presents_new_approach_for", "snippet": "A new MIT CSAIL paper presents an alternative, hierarchical approach to <b>sequence-to-sequence</b> learning with quasi-synchronous grammars to improve <b>sequence-to-sequence</b> models\u2019 compositional generalization on diagnostic tasks. Here is a quick <b>read</b>: MIT Presents New Approach for <b>Sequence-to-Sequence</b> Learning with Latent Neural Grammars.", "dateLastCrawled": "2021-09-17T14:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to easily do Handwriting Recognition using Deep Learning", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "Scan, Attend and <b>Read</b>. In this seminal work Scan, Attend and <b>Read</b>(SAR) the authors propose the usage of an attention-based model for end-to-end handwriting recognition. The main contribution of the research is the automatic transcription of text without segmenting into lines as a pre-processing step and thus can scan an entire page and give ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is there a way of <b>teaching</b> a neural network using a string as input, a ...", "url": "https://www.quora.com/Is-there-a-way-of-teaching-a-neural-network-using-a-string-as-input-a-string-as-expected-output-and-to-teach-it-to-output-strings-when-given-a-string-as-input", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-a-way-of-<b>teaching</b>-a-neural-network-using-a-string-as...", "snippet": "Answer (1 of 3): Yes. Such networks are called <b>Sequence to Sequence</b> models. You already know that neural networks or any other model only understands numbers. So we have to find a way to represent these strings by numbers ( vectors to be more precise). For this, there are lot of approaches follow...", "dateLastCrawled": "2022-01-23T04:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "An <b>introduction to sequence-to-sequence learning</b>. Published: February 19, 2019. Many interesting problems in artificial intelligence <b>can</b> be described in the following way: Map a sequence of inputs $\\mathbf{x}$ to the correct sequence of outputs $\\mathbf{y}$. Speech recognition is one example: the goal is to map an audio signal $\\mathbf{x}$ (a sequence of real-valued audio samples) to the correct text transcript $\\mathbf{y}$ (a sequence of letters). Other examples are machine translation ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Differences between Autoregressive, Autoencoding and Sequence</b>-to ...", "url": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive-autoencoding-and-sequence-to-sequence-models-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive...", "snippet": "Even though the flow is more vertical than in the example above, you <b>can</b> see that it is in essence an encoder-decoder architecture performing <b>sequence-to-sequence</b> learning: We have N encoder segments that take inputs (in the form of a learned embedding) and encode it into a higher-dimensional intermediate representation (in the case of the original Transformer, it outputs a 512-dimensional state vector ).", "dateLastCrawled": "2022-02-02T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Sequence Models by Andrew Ng \u2014 11 Lessons Learned - Towards Data Science", "url": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d...", "snippet": "Word embeddings <b>can</b> <b>be thought</b> of as the vector representation of a given word. They <b>can</b> be trained using the Word2Vec, Negative Sampling or Glove algorithms. Word embedding models may be trained on a very large text corpus (say 100B words) and <b>can</b> then be used on a sequence prediction <b>task</b> with a smaller number of training example (say 10,000 words). For example, sentiment classification may use word embeddings to greatly reduce the number of training examples required to generate an ...", "dateLastCrawled": "2022-01-29T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The 7 <b>NLP</b> Techniques That Will Change How You Communicate in the Future ...", "url": "https://heartbeat.comet.ml/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-i-f0114b2f0497", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/the-7-<b>nlp</b>-techniques-that-will-change-how-you-communicate...", "snippet": "<b>Sequence to Sequence</b> Learning with Neural Networks proved the effectiveness of LSTM for Neural Machine Translation. It presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. The method uses a multilayered LSTM to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector.", "dateLastCrawled": "2022-01-23T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Develop an Encoder-Decoder Model for <b>Sequence-to-Sequence</b> ...", "url": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence...", "snippet": "Here you <b>can</b> see how the recursive use of the model <b>can</b> be used to build up output sequences. During prediction, the inference_encoder model is used to encode the input sequence once which returns states that are used to initialize the inference_decoder model. From that point, the inference_decoder model is used to generate predictions step by step.. The function below named predict_sequence() <b>can</b> be used after the model is trained to generate a target sequence given a source sequence.", "dateLastCrawled": "2022-01-29T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>How I Used Deep Learning To Train A Chatbot</b> To Talk Like Me (Sorta ...", "url": "https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/<b>How-I-Used-Deep-Learning-to-Train-a-Chatbot</b>-to-Talk-Like-Me", "snippet": "A <b>sequence to sequence</b> model is composed of 2 main components, an encoder RNN and a decoder RNN (If you\u2019re a little shaky on RNNs, check out my previous blog post for a refresher). From a high level, the encoder\u2019s job is to encapsulate the information of the input text into a fixed representation. The decoder\u2019s is to take that representation, and generate a variable length text that best responds to it.", "dateLastCrawled": "2022-01-30T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Self Supervised Representation Learning in NLP</b>", "url": "https://amitness.com/2020/05/self-supervised-learning-nlp/", "isFamilyFriendly": true, "displayUrl": "https://amitness.com/2020/05/self-supervised-learning-nlp", "snippet": "5 minute <b>read</b> While <b>Computer</b> Vision is making amazing progress on self-supervised learning only in the last few years, self-supervised learning has been a first-class citizen in NLP research for quite a while. Language Models have existed since the 90\u2019s even before the phrase \u201cself-supervised learning\u201d was termed. The Word2Vec paper from 2013 popularized this paradigm and the field has rapidly progressed applying these self-supervised methods across many problems. At the core of these ...", "dateLastCrawled": "2022-02-02T13:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Is there a way of <b>teaching</b> a neural network using a string as input, a ...", "url": "https://www.quora.com/Is-there-a-way-of-teaching-a-neural-network-using-a-string-as-input-a-string-as-expected-output-and-to-teach-it-to-output-strings-when-given-a-string-as-input", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-a-way-of-<b>teaching</b>-a-neural-network-using-a-string-as...", "snippet": "Answer (1 of 3): Yes. Such networks are called <b>Sequence to Sequence</b> models. You already know that neural networks or any other model only understands numbers. So we have to find a way to represent these strings by numbers ( vectors to be more precise). For this, there are lot of approaches follow...", "dateLastCrawled": "2022-01-23T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to <b>train GPT-2 to Create JSOn</b>? : LanguageTechnology", "url": "https://www.reddit.com/r/LanguageTechnology/comments/j1iaxq/how_to_train_gpt2_to_create_json/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/LanguageTechnology/comments/j1iaxq/how_to_train_gpt2_to...", "snippet": "So you&#39;re describing a <b>sequence to sequence</b> (seq2seq) <b>task</b>, which is a bit tricky with GPT-2 in huggingface. Most seq2seq tasks use a encoder-decoder setup where the encoder takes your inputs and encodes it into an intermediate representation and a decoder decodes into a target language. I don&#39;t think there&#39;s a straightforward way (e.g. using existing training scripts) in huggingface but I could be wrong. The combiners api", "dateLastCrawled": "2022-01-22T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Can</b> you build an AI <b>capable to understand a text</b> and answer questions ...", "url": "https://www.quora.com/Can-you-build-an-AI-capable-to-understand-a-text-and-answer-questions-about-it", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Can</b>-you-build-an-AI-<b>capable-to-understand-a-text</b>-and-answer...", "snippet": "Answer (1 of 4): Yes I <b>can</b>, and have, but it took about 800 days to program and is far from perfect. For instance, like myself, it has the tendency to take \u201c<b>can</b> you ...", "dateLastCrawled": "2022-01-14T06:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sequence-to-sequence learning with Transducers</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2020/11/transducer/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2020/11/transducer", "snippet": "<b>Sequence-to-sequence learning with Transducers</b>. Published: November 16, 2020 The Transducer (sometimes called the \u201cRNN Transducer\u201d or \u201cRNN-T\u201d, though it need not use RNNs) is a <b>sequence-to-sequence</b> model proposed by Alex Graves in \u201cSequence Transduction with Recurrent Neural Networks\u201d. The paper was published at the ICML 2012 Workshop on Representation Learning.Graves showed that the Transducer was a sensible model to use for speech recognition, achieving good results on a small ...", "dateLastCrawled": "2022-01-31T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Improving <b>Sequence-to-Sequence</b> Pre-training via Sequence Span Rewriting ...", "url": "https://www.readkong.com/page/improving-sequence-to-sequence-pre-training-via-sequence-3829690", "isFamilyFriendly": true, "displayUrl": "https://www.<b>read</b>kong.com/page/improving-<b>sequence-to-sequence</b>-pre-training-via-sequence...", "snippet": "Specifically, self-supervised training with the sequence span \u2022 First, the <b>task</b> of sequence span rewriting rewriting objective involves three steps: (1) text is closer to the downstream sequence trans- span masking (2) text infilling, and (3) sequence duction tasks since there exists references in span rewriting.", "dateLastCrawled": "2022-01-08T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Improving <b>Sequence-to-Sequence</b> Semantic Parser for <b>Task</b> Oriented Dialog ...", "url": "https://www.researchgate.net/publication/347236133_Improving_Sequence-to-Sequence_Semantic_Parser_for_Task_Oriented_Dialog", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/347236133_Improving_<b>Sequence-to-Sequence</b>...", "snippet": "Request PDF | On Jan 1, 2020, Chaoting Xuan published Improving <b>Sequence-to-Sequence</b> Semantic Parser for <b>Task</b> Oriented Dialog | Find, <b>read</b> and cite all the research you need on ResearchGate", "dateLastCrawled": "2021-12-14T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Effective <b>Sequence-to-Sequence</b> Dialogue State Tracking | DeepAI", "url": "https://deepai.org/publication/effective-sequence-to-sequence-dialogue-state-tracking", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/effective-<b>sequence-to-sequence</b>-dialogue-state-tracking", "snippet": "Full-history model: The most straightforward way of preparing the context is simply to concatenate turns from the entire history as inputs to the encoder, which ensures the model to have full access to the raw information required to predict the current state.This setup is also adopted by several generative dialogue models such as SimpleTOD Hosseini-Asl et al. (), Seq2Seq-DU Feng et al. and SOLOIST Peng et al. ().A potential drawback of the full-history model is that it may become ...", "dateLastCrawled": "2021-12-25T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Graph</b> Neural Network and Some of GNN Applications: Everything You Need ...", "url": "https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>graph</b>-neural-network-and-some-of-gnn-applications", "snippet": "The neural machine translation (NMT) is considered a <b>sequence-to-sequence</b> <b>task</b>. One of GNN\u2019s common applications is to incorporate semantic information into the NMT <b>task</b>. To do this, we utilize the Syntactic GCN on syntax-aware NMT tasks. We <b>can</b> also use the GGNN in NMT. It converts the syntactic dependency <b>graph</b> into a new structure by turning the edges into additional nodes and thus edges labels <b>can</b> be represented as embeddings . Relation extraction. Deep Learning: <b>Graph</b> LSTM/ <b>graph</b> ...", "dateLastCrawled": "2022-01-29T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Develop an Encoder-Decoder Model for <b>Sequence-to-Sequence</b> ...", "url": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence...", "snippet": "Here you <b>can</b> see how the recursive use of the model <b>can</b> be used to build up output sequences. During prediction, the inference_encoder model is used to encode the input sequence once which returns states that are used to initialize the inference_decoder model. From that point, the inference_decoder model is used to generate predictions step by step.. The function below named predict_sequence() <b>can</b> be used after the model is trained to generate a target sequence given a source sequence.", "dateLastCrawled": "2022-01-29T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Self Supervised Representation Learning in NLP</b>", "url": "https://amitness.com/2020/05/self-supervised-learning-nlp/", "isFamilyFriendly": true, "displayUrl": "https://amitness.com/2020/05/self-supervised-learning-nlp", "snippet": "5 minute <b>read</b> While <b>Computer</b> Vision is making amazing progress on self-supervised learning only in the last few years, self-supervised learning has been a first-class citizen in NLP research for quite a while. Language Models have existed since the 90\u2019s even before the phrase \u201cself-supervised learning\u201d was termed. The Word2Vec paper from 2013 popularized this paradigm and the field has rapidly progressed applying these self-supervised methods across many problems. At the core of these ...", "dateLastCrawled": "2022-02-02T13:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Computer</b> Vision for Driving Scene Understanding: from Autonomous ...", "url": "https://www.reddit.com/r/deeplearning/comments/ptip9m/computer_vision_for_driving_scene_understanding/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/deeplearning/comments/ptip9m/<b>computer</b>_vision_for_driving...", "snippet": "A new MIT CSAIL paper presents an alternative, hierarchical approach to <b>sequence-to-sequence</b> learning with quasi-synchronous grammars to improve <b>sequence-to-sequence</b> models\u2019 compositional generalization on diagnostic tasks. Here is a quick <b>read</b>: MIT Presents New Approach for <b>Sequence-to-Sequence</b> Learning with Latent Neural Grammars.", "dateLastCrawled": "2021-09-22T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to easily do Handwriting Recognition using Deep Learning", "url": "https://nanonets.com/blog/handwritten-character-recognition/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/handwritten-character-recognition", "snippet": "Handwriting Text Generation is the <b>task</b> of generating real looking handwritten text and thus <b>can</b> be used to augment the existing datasets. As we know deep learning requires a lot of data to train while obtaining huge corpus of labelled handwriting images for different languages is a cumbersome <b>task</b>. To solve this we <b>can</b> use Generative Adversarial Networks to generate training data. Let&#39;s discuss one such architecture here", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "PyTorch Reinforcement Learning: <b>Teaching</b> AI How to Play Flappy Bird ...", "url": "https://www.toptal.com/deep-learning/pytorch-reinforcement-learning-tutorial", "isFamilyFriendly": true, "displayUrl": "https://www.toptal.com/deep-learning/pytorch-reinforcement-learning-tutorial", "snippet": "In contrast, machine learning is a field of <b>computer</b> science which uses statistical methods to enable computers to learn and to extract knowledge from the data without being explicitly programmed. In this reinforcement learning tutorial, I\u2019ll show how we <b>can</b> use PyTorch to teach a reinforcement learning neural network how to play Flappy Bird.", "dateLastCrawled": "2022-02-03T17:46:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Read the model framework <b>Encoder-Decoder and Seq2Seq</b> in NLP - easyAI", "url": "https://easyai.tech/en/ai-definition/encoder-decoder-seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://easyai.tech/en/ai-definition/encoder-decoder-seq2seq", "snippet": "Encoder-Decoder This framework is a good illustration of the core ideas of <b>machine</b> <b>learning</b>: ... Seq2Seq (short for <b>Sequence-to-sequence</b>), as literally, enters a sequence and outputs another sequence. The most important aspect of this structure is that the length of the input sequence and the output sequence are variable. For example, the following picture: As shown above: 6 Chinese characters are input, and 3 English words are output. The length of the input and output are different. The ...", "dateLastCrawled": "2022-01-31T17:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Andrew-NG-Notes/andrewng-p-5-sequence-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence...", "snippet": "<b>Machine</b> translation (<b>sequence to sequence</b>): X: text sequence (in one language) Y: text sequence (in other language) Video activity recognition (sequence to one): X: video frames ; Y: label (activity) Name entity recognition (<b>sequence to sequence</b>): X: text sequence; Y: label sequence; Can be used by seach engines to index different type of words inside a text. All of these problems with different input and output (sequence or not) can be addressed as supervised <b>learning</b> with label data X, Y ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Jiajun Zhang - ACL Anthology", "url": "https://aclanthology.org/people/j/jiajun-zhang/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/people/j/jiajun-zhang", "snippet": "Neural <b>sequence-to-sequence</b> models have gained considerable success for this <b>task</b>, while most existing approaches only focus on improving the informativeness of the summary, which ignore the correctness, i.e., the summary should not contain unrelated information with respect to the source sentence. We argue that correctness is an essential requirement for summarization systems. Considering a correct summary is semantically entailed by the source sentence, we incorporate entailment knowledge ...", "dateLastCrawled": "2022-01-16T04:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(teaching a computer how to read)", "+(sequence-to-sequence task) is similar to +(teaching a computer how to read)", "+(sequence-to-sequence task) can be thought of as +(teaching a computer how to read)", "+(sequence-to-sequence task) can be compared to +(teaching a computer how to read)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}