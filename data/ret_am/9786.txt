{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Disparate</b> <b>Impact</b> in <b>Machine</b> <b>Learning</b> \u00bb Dome | Blog Archive | Boston ...", "url": "https://sites.bu.edu/dome/2020/06/08/disparate-impact-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://sites.bu.edu/dome/2020/06/08/<b>disparate</b>-<b>impact</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "Thus, instead of relying on either <b>disparate</b> <b>impact</b> or <b>disparate</b> treatment theory, perhaps legal analysis of discrimination in <b>machine</b> <b>learning</b> should be entirely outcomes-driven. If, in fact, an <b>algorithm</b> wrongly predicts the likelihood of an event occurring, and that <b>algorithm</b> is less accurate for protected class members than unprotected class members, the <b>algorithm</b> should be considered prima facie discriminatory. Such a solution is viable for examining recidivism, interest rates and loan ...", "dateLastCrawled": "2021-12-09T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Disparate</b> <b>Impact</b> Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-<b>impact</b>-analysis", "snippet": "<b>Disparate</b> <b>Impact</b> Analysis (DIA) Sensitivity Analysis(SA) As a matter of speaking, the above two features provide a solution to a common problem in ML: the multiplicity of good models. It is well understood that for the same set of input features and prediction targets, complex <b>machine</b> <b>learning</b> algorithms can produce multiple accurate models with very similar, but not the same, internal architectures: the multiplicity of good models [1]. This alone is an obstacle to interpretation, but when ...", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Algorithmic Fairness in <b>Machine</b> <b>Learning</b>", "url": "https://www2.cs.duke.edu/courses/spring19/compsci216/lectures/11-fairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www2.cs.duke.edu/courses/spring19/compsci216/lectures/11-fairness.pdf", "snippet": "<b>Disparate</b> <b>Impact</b> \u2022Arguably this is the only good measure if you think the dataare biased and you have a strong prior belief protected status is uncorrelated with outcomes. \u2022\u201cIn Griggs v. Duke Power Co. [20], the US Supreme Court ruled a business hiring decision illegal if it resulted in <b>disparate</b> <b>impact</b> by", "dateLastCrawled": "2022-01-21T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Fall 2020 Journal | Algorithms and Child Welfare: The <b>Disparate</b> <b>Impact</b> ...", "url": "https://bppj.berkeley.edu/2021/02/02/algorithms-and-child-welfare-the-disparate-impact-of-family-surveillance-in-risk-assessment-technologies/", "isFamilyFriendly": true, "displayUrl": "https://bppj.berkeley.edu/2021/02/02/<b>algorithms</b>-and-child-welfare-the-<b>disparate</b>-<b>impact</b>...", "snippet": "We believe that if <b>machine</b> <b>learning</b> is to continue to be used in social services, the history of the data must be considered [34]. Through our literature review, we did not find evidence of regulation over the child welfare data used in <b>machine</b> <b>learning</b> technologies. At the time of writing, Pennsylvania\u2019s statutes on Child Protective Services did not include any guidance on the use of <b>machine</b> <b>learning</b> or artificial intelligence. Searches for the words \u201cautomated\u201d and \u201c<b>algorithm</b> ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>How Do Machine Learning Algorithms Learn Bias</b>? | by SeattleDataGuy ...", "url": "https://betterprogramming.pub/how-do-machine-learning-algorithms-learn-bias-555809a1decb", "isFamilyFriendly": true, "displayUrl": "https://betterprogramming.pub/<b>how-do-machine-learning-algorithms-learn-bias</b>-555809a1decb", "snippet": "An <b>algorithm</b> that has <b>disparate</b> <b>impact</b> is causing people to lose jobs and social networks and ensuring the worst cold-start problem once someone has been released from prison. At the same time, people likely to commit crimes in the future are let to go free because the <b>algorithm</b> is blind to their criminality.", "dateLastCrawled": "2022-01-14T19:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Mitigating Bias in AI with</b> AIF360 | by Bryan Truong | Towards Data Science", "url": "https://towardsdatascience.com/mitigating-bias-in-ai-with-aif360-b4305d1f88a9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>mitigating-bias-in-ai-with</b>-aif360-b4305d1f88a9", "snippet": "As shown above, I arrived at a <b>disparate</b> <b>impact</b> ratio of .66. This <b>disparate</b> <b>impact</b> ratio is worse than the one from the actual test split, which was .83\u2013 less biased than the .66 of the model we just trained. This is not a surprise, as it has been shown time and time again that biases can easily get amplified in <b>machine</b> <b>learning</b> models.", "dateLastCrawled": "2022-02-02T22:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "When is a Chair not a Chair? Big Data Algorithms, <b>Disparate</b> <b>Impact</b>, and ...", "url": "http://users.umiacs.umd.edu/~oard/desi7/papers/JS.pdf", "isFamilyFriendly": true, "displayUrl": "users.umiacs.umd.edu/~oard/desi7/papers/JS.pdf", "snippet": "Big Data Algorithms, <b>Disparate</b> <b>Impact</b>, ... the equation,3 and bigger is better, where \u201cvery large data sets can improve even the worst <b>machine</b> <b>learning</b> algorithms.\u201d4 While there are detractors from this theory who argue that developers are sometimes too trusting of data to the detriment of <b>algorithm</b> development,5 the majority of current, conventional wisdom trusts in history and volume as proxies for quality. Quality Data comes from the Real World The historical portion of high quality ...", "dateLastCrawled": "2021-11-05T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Big Data&#39;s Disparate Impact</b> by Solon <b>Barocas</b>, Andrew D. Selbst :: SSRN", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899", "snippet": "In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining\u2019s victims would seem to lie in <b>disparate</b> <b>impact</b> doctrine. Case law and the Equal Employment Opportunity Commission\u2019s Uniform Guidelines, though, hold that a practice can be justified as a business necessity when its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. Unless there is a reasonably practical way ...", "dateLastCrawled": "2022-01-29T21:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Differential Fairness for <b>Machine Learning and Artificial Intelligence</b> ...", "url": "http://jfoulds.informationsystems.umbc.edu/slides/2018/Foulds_Nov_14_2018_MD_AI_Differential_Fairness.pdf", "isFamilyFriendly": true, "displayUrl": "jfoulds.informationsystems.umbc.edu/slides/2018/Foulds_Nov_14_2018_MD_AI_Differential...", "snippet": "<b>Machine</b> <b>Learning</b> \u2022 <b>Machine</b> <b>learning</b> algorithms, which make predictions based on data, are having an increasing <b>impact</b> on our daily lives. \u2022 Example: credit scoring - predicting whether you will repay or default on a loan \u2022 Given the feature vector, the <b>algorithm</b> learns to predict the class label", "dateLastCrawled": "2022-01-30T16:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "AI Fairness \u2014 Explanation of <b>Disparate Impact</b> Remover | by Stacey ...", "url": "https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/ai-fairness-explanation-of-<b>disparate-impact</b>-remover-ce0...", "snippet": "<b>Disparate Impact</b>. <b>Disparate Impact</b> is a metric to evaluate fairness. It compares the proportion of individuals that receive a positive output for two groups: an unprivileged group and a privileged group. The calculation is the proportion of the unprivileged group that received the positive outcome divided by the proportion of the privileged group that received the positive outcome. The industry standard is a four-fifths rule: if the unprivileged group receives a positive outcome less than 80 ...", "dateLastCrawled": "2022-01-29T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Disparate</b> <b>Impact</b> Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-<b>impact</b>-analysis", "snippet": "<b>Disparate</b> <b>Impact</b> Analysis (DIA) ... complex <b>machine</b> <b>learning</b> algorithms can produce multiple accurate models with very <b>similar</b>, but not the same, internal architectures: the multiplicity of good models [1]. This alone is an obstacle to interpretation, but when using these types of tools as interpretation tools or with interpretation tools, it is important to remember that details of explanations can change across multiple accurate models. This instability of explanations is a driving factor ...", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>AI Fairness \u2014 Explanation of Disparate Impact Remover</b> - Adolfo Eliaz\u00e0t ...", "url": "https://adolfoeliazat.com/2021/05/06/ai-fairness-explanation-of-disparate-impact-remover/", "isFamilyFriendly": true, "displayUrl": "https://adolfoeliazat.com/2021/05/06/<b>ai-fairness-explanation-of-disparate-impact-remover</b>", "snippet": "<b>Disparate</b> <b>Impact</b>. <b>Disparate</b> <b>Impact</b> is a metric to evaluate fairness. It compares the proportion of individuals that receive a positive output for two groups: an unprivileged group and a privileged group. The calculation is the proportion of the unprivileged group that received the positive outcome divided by the proportion of the privileged group that received the positive outcome. The industry standard is a four-fifths rule: if the unprivileged group receives a positive outcome less than 80 ...", "dateLastCrawled": "2022-01-22T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fairness in Machine Learning: Part</b> I - cs.uwaterloo.ca", "url": "https://cs.uwaterloo.ca/~xihe/cs848_f19/slides/05-module2-FairML1.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.uwaterloo.ca/~xihe/cs848_f19/slides/05-module2-FairML1.pdf", "snippet": "Certifying <b>Disparate</b> <b>Impact</b> \u2022<b>Disparate</b> <b>impact</b> is related to predictability. So what? \u2022Given D, we estimate: 1.The predictability (call it \u2018) of D. 2.B, the fraction of class X=0 predicted to have outcome 1. \u2022This yields an estimate on the possible <b>disparate</b> <b>impact</b> of any classifier built on D. \u2022How do we get these estimates?", "dateLastCrawled": "2021-09-14T22:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Fall 2020 Journal | Algorithms and Child Welfare: The <b>Disparate</b> <b>Impact</b> ...", "url": "https://bppj.berkeley.edu/2021/02/02/algorithms-and-child-welfare-the-disparate-impact-of-family-surveillance-in-risk-assessment-technologies/", "isFamilyFriendly": true, "displayUrl": "https://bppj.berkeley.edu/2021/02/02/<b>algorithms</b>-and-child-welfare-the-<b>disparate</b>-<b>impact</b>...", "snippet": "We believe that if <b>machine</b> <b>learning</b> is to continue to be used in social services, the history of the data must be considered [34]. Through our literature review, we did not find evidence of regulation over the child welfare data used in <b>machine</b> <b>learning</b> technologies. At the time of writing, Pennsylvania\u2019s statutes on Child Protective Services did not include any guidance on the use of <b>machine</b> <b>learning</b> or artificial intelligence. Searches for the words \u201cautomated\u201d and \u201c<b>algorithm</b> ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Disparate</b> <b>impact</b> in a <b>machine</b> <b>learning</b> model originates from bias in either the data or the algorithms. A popular example is the prejudicially biased data used for recidivism prediction. Due to <b>disparate</b> socioeconomic factors and systemic racism in the United States, blacks have historically been (and continue to be) incarcerated at higher rates than whites . Not coincidentally, blacks are also exonerated due to wrongful accusation at a considerably higher rate than whites . A recidivism ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Differential Fairness for <b>Machine Learning and Artificial Intelligence</b> ...", "url": "http://jfoulds.informationsystems.umbc.edu/slides/2018/Foulds_Nov_14_2018_MD_AI_Differential_Fairness.pdf", "isFamilyFriendly": true, "displayUrl": "jfoulds.informationsystems.umbc.edu/slides/2018/Foulds_Nov_14_2018_MD_AI_Differential...", "snippet": "\u2022 <b>Machine</b> <b>learning</b> algorithms, which make predictions based on data, are having an increasing <b>impact</b> on our daily lives. \u2022 Example: credit scoring - predicting whether you will repay or default on a loan \u2022 Given the feature vector, the <b>algorithm</b> learns to predict the class label \u2022 The models are \u201ctrained\u201d on many labeled feature vectors \u2022 This is called classification, an instance of supervised <b>machine</b> <b>learning</b> 2 # Late Payments % of available credit used Previous defaults ...", "dateLastCrawled": "2022-01-30T16:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Removing Unfair Bias in <b>Machine</b> <b>Learning</b>", "url": "https://community.ibm.com/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=618c7917-8d2b-ca78-5ecc-2286486b9c69&forceDialog=0", "isFamilyFriendly": true, "displayUrl": "https://community.ibm.com/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=...", "snippet": "Bias In the <b>Machine</b> <b>Learning</b> Pipeline dataset metric pre-processing <b>algorithm</b> in-processing <b>algorithm</b> post-processing <b>algorithm</b> classifier metric . Where Can You Intervene in the Pipeline? \u2022If you can modify the Training Data, then pre-processing can be used. \u2022If you can modify the <b>Learning</b> <b>Algorithm</b>, then in-processing can be used. \u2022If you can only treat the learned model as a black box and can\u2019t modify the training data or <b>learning</b> <b>algorithm</b>, then only post-processing can be used ...", "dateLastCrawled": "2022-01-29T04:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Algorithmic Bias and Regularisation in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/algorithmic-bias-and-regularisation-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>algorithmic-bias-and-regularisation-in-machine-learning</b>", "snippet": "Often, what is termed algorithmic bias in <b>machine</b> <b>learning</b> will be due to historic bias in the training data. But sometimes the bias may be introduced (or at least exacerbated) by the <b>algorithm</b> itself. The ways in which algorithms can actually accentuate bias has not received a lot of attention with researchers focusing directly on methods to eliminate bias - no matter the source.", "dateLastCrawled": "2022-01-26T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "There are two forms of <b>discrimination</b> that we will refer to as <b>disparate</b> <b>impact</b> and <b>disparate</b> treatment. ... In this case, the biases of humans are not mitigated by the <b>machine learning</b> <b>algorithm</b>. In fact, they are reproduced in the classifications that are made. Why does this happen? Recidivism scores such as those made by the Northpointe software are based on prior arrests, age of first police contact, parents\u2019 incarceration record. This information is shaped by biases in the world (such ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Algorithmic Fairness in <b>Machine</b> <b>Learning</b>", "url": "https://www2.cs.duke.edu/courses/spring19/compsci216/lectures/11-fairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www2.cs.duke.edu/courses/spring19/compsci216/lectures/11-fairness.pdf", "snippet": "<b>Disparate</b> <b>Impact</b> \u2022Arguably this is the only good measure if you think the dataare biased and you have a strong prior belief protected status is uncorrelated with outcomes. \u2022\u201cIn Griggs v. Duke Power Co. [20], the US Supreme Court ruled a business hiring decision illegal if it resulted in <b>disparate</b> <b>impact</b> by", "dateLastCrawled": "2022-01-21T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "HIRING BY <b>ALGORITHM</b> PREDICTING AND PREVENTING <b>DISPARATE</b> <b>IMPACT</b>", "url": "http://sorelle.friedler.net/papers/SSRN-id2746078.pdf", "isFamilyFriendly": true, "displayUrl": "sorelle.friedler.net/papers/SSRN-id2746078.pdf", "snippet": "Major advances in <b>machine</b> <b>learning</b> have encouraged corporations to rely on Big Data and algorithmic decision making with the presumption that such decisions are efficient and impartial. In this Essay, we show that protected information that is encoded in seemingly facially neutral data could be predicted with high accuracy by algorithms and employed in the decision-making process, thus resulting in a <b>disparate</b> <b>impact</b> on protected classes. We then demonstrate how it is possible to repair the ...", "dateLastCrawled": "2022-01-22T08:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Does mitigating ML&#39;s <b>disparate</b> <b>impact</b> require <b>disparate</b> treatment? - DeepAI", "url": "https://deepai.org/publication/does-mitigating-ml-s-disparate-impact-require-disparate-treatment", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/does-mitigating-ml-s-<b>disparate</b>-<b>impact</b>-require-<b>disparate</b>...", "snippet": "Algorithms exhibit <b>disparate</b> <b>impact</b> if they affect subgroups differently. <b>Disparate</b> <b>impact</b> <b>can</b> arise unintentionally and absent <b>disparate</b> treatment. The natural way to reduce <b>disparate</b> <b>impact</b> would be to apply <b>disparate</b> treatment in favor of the disadvantaged group, i.e. to apply affirmative action. However, owing to the practice&#39;s contested ...", "dateLastCrawled": "2021-12-15T05:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Mitigating Bias in AI with</b> AIF360 | by Bryan Truong | Towards Data Science", "url": "https://towardsdatascience.com/mitigating-bias-in-ai-with-aif360-b4305d1f88a9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>mitigating-bias-in-ai-with</b>-aif360-b4305d1f88a9", "snippet": "As shown above, I arrived at a <b>disparate</b> <b>impact</b> ratio of .66. This <b>disparate</b> <b>impact</b> ratio is worse than the one from the actual test split, which was .83\u2013 less biased than the .66 of the model we just trained. This is not a surprise, as it has been shown time and time again that biases <b>can</b> easily get amplified in <b>machine</b> <b>learning</b> models.", "dateLastCrawled": "2022-02-02T22:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fall 2020 Journal | Algorithms and Child Welfare: The <b>Disparate</b> <b>Impact</b> ...", "url": "https://bppj.berkeley.edu/2021/02/02/algorithms-and-child-welfare-the-disparate-impact-of-family-surveillance-in-risk-assessment-technologies/", "isFamilyFriendly": true, "displayUrl": "https://bppj.berkeley.edu/2021/02/02/<b>algorithms</b>-and-child-welfare-the-<b>disparate</b>-<b>impact</b>...", "snippet": "We believe that if <b>machine</b> <b>learning</b> is to continue to be used in social services, the history of the data must be considered [34]. Through our literature review, we did not find evidence of regulation over the child welfare data used in <b>machine</b> <b>learning</b> technologies. At the time of writing, Pennsylvania\u2019s statutes on Child Protective Services did not include any guidance on the use of <b>machine</b> <b>learning</b> or artificial intelligence. Searches for the words \u201cautomated\u201d and \u201c<b>algorithm</b> ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Review of Challenges and Opportunities in <b>Machine</b> <b>Learning</b> for Health", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7233077/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7233077", "snippet": "Importantly, clinical staff and <b>machine</b> <b>learning</b> researchers often have complementary skills, and many high-<b>impact</b> problems <b>can</b> only be tackled by collaborative efforts. We note several promising directions of research, specifically highlighting those that address issues of data non-stationarity, model interpretability, and discovering appropriate representations.", "dateLastCrawled": "2022-01-25T00:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Fair prediction with <b>disparate</b> <b>impact</b>: A study of bias in recidivism ...", "url": "https://www.arxiv-vanity.com/papers/1703.00056/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.00056", "snippet": "Keywords: <b>disparate</b> <b>impact</b>; bias; recidivism prediction; risk assessment; fair <b>machine</b> <b>learning</b> 1 Introduction Risk assessment instruments are gaining increasing popularity within the criminal justice system, with versions of such instruments being used or considered for use in pre-trial decision-making, parole decisions, and in some states even sentencing [ 1 , 2 , 3 ] .", "dateLastCrawled": "2022-01-11T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "When is a Chair not a Chair? Big Data Algorithms, <b>Disparate</b> <b>Impact</b>, and ...", "url": "http://users.umiacs.umd.edu/~oard/desi7/papers/JS.pdf", "isFamilyFriendly": true, "displayUrl": "users.umiacs.umd.edu/~oard/desi7/papers/JS.pdf", "snippet": "<b>Algorithm</b> quality is in turn intimately related to data quality and size, and as collection measures and relative data sizes increase in volume, algorithms commensurately increase in complexity. This makes insight into the inner workings of <b>algorithm</b> function more difficult, which is challenging at a time where both the effects of algorithms and the data associated with their development and utilization are undergoing additional scrutiny. This paper examines these trends, and considers both ...", "dateLastCrawled": "2021-11-05T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The ethics of <b>algorithms</b>: key problems and solutions", "url": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "snippet": "Actions (1) and (2) may be performed by (semi-)autonomous <b>algorithms</b>\u2014such as <b>machine</b> <b>learning</b> (ML) <b>algorithms</b>\u2014and this complicates, (3) the attribution of responsibility for the effects of actions that an <b>algorithm</b> may trigger. Here, ML is of particular interest, as a field which includes deep <b>learning</b> architectures. Computer systems deploying ML <b>algorithms</b> may be described as \u201cautonomous\u201d or \u201csemi-autonomous\u201d, to the extent that their outputs are induced from data and thus, non ...", "dateLastCrawled": "2022-01-30T20:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Disparate</b> <b>Impact</b> Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-<b>impact</b>-analysis", "snippet": "And such explanatory results <b>can</b> be accessed by the <b>Disparate</b> <b>Impact</b> Analysis and Sensitivity Analysis(SA) features/tools. With the above in mind, let us discover how we <b>can</b> better understand our models. References [1] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The Elements of Statistical <b>Learning</b>. Springer, New York, 2001. You will need the following to be able to do this self-paced course: Basic knowledge of <b>Machine</b> <b>Learning</b> and Statistics; Basic knowledge of Driverless AI or ...", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b>", "url": "https://www.ftc.gov/system/files/documents/public_events/1567421/fuaserisinghsrinivasan_updated2.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ftc.gov</b>/system/files/documents/public_events/1567421/fuaserisinghsriniva...", "snippet": "to <b>disparate</b> <b>impact</b> particularly when there are di\u21b5erences among groups based on demographic classes. In response, several \u201c<b>fair\u201d machine learning algorithms</b> that require <b>impact</b> parity (e.g., equal opportunity) have recently been proposed to adjust for the societal inequalities; advocates propose changing the law to allow the use of protected class-speci\ufb01c decision rules. We show that these \u201cfair\u201d algorithms that require <b>impact</b> parity, while conceptually appealing, <b>can</b> make ...", "dateLastCrawled": "2022-02-03T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "LOCKED OUT BY BIG DATA: HOW BIG DATA, ALGORITHMS AND <b>MACHINE</b> <b>LEARNING</b> ...", "url": "http://blogs.law.columbia.edu/hrlr/files/2020/11/251_Schneider.pdf", "isFamilyFriendly": true, "displayUrl": "blogs.law.columbia.edu/hrlr/files/2020/11/251_Schneider.pdf", "snippet": "<b>disparate</b> <b>impact</b> analysis\u2014and has proposed to specifically immunize housing providers that rely on algorithms in decision-making from liability.4 This article proceeds in five parts. Part I describes the advent of big data, algorithmic decision-making, and <b>machine</b> <b>learning</b>. Part II describes the relevant provisions of the Fair Housing Act and", "dateLastCrawled": "2022-01-27T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Review on Fairness in <b>Machine</b> <b>Learning</b> | ACM Computing Surveys", "url": "https://dl.acm.org/doi/10.1145/3494672", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3494672", "snippet": "<b>Disparate</b> <b>impact</b> : This measure was designed to mathematically represent the legal notion of <b>disparate</b> <b>impact</b>. It requires a high ratio between the positive prediction rates of both groups. This ensures that the proportion of the positive predictions is similar across groups. For example, if a positive prediction represents acceptance for a job, the condition requires the proportion of accepted applicants to be similar across groups. Formally, this measure is computed as follows:", "dateLastCrawled": "2022-02-07T09:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "\u201cUn\u201dFair <b>Machine</b> <b>Learning</b> Algorithms | Management Science", "url": "https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4065", "isFamilyFriendly": true, "displayUrl": "https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4065", "snippet": "However, in many cases, ensuring equal treatment leads to <b>disparate</b> <b>impact</b> particularly when there are differences among groups based on demographic classes. In response, several \u201cfair\u201d <b>machine</b> <b>learning</b> (ML) algorithms that require <b>impact</b> parity (e.g., equal opportunity) at the cost of equal treatment have recently been proposed to adjust for the societal inequalities. Advocates of fair ML propose changing the law to allow the use of protected class-specific decision rules. We show that ...", "dateLastCrawled": "2022-02-01T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Disparate</b> <b>impact</b> in a <b>machine</b> <b>learning</b> model originates from bias in either the data or the algorithms. A popular example is the prejudicially biased data used for recidivism prediction. Due to <b>disparate</b> socioeconomic factors and systemic racism in the United States, blacks have historically been (and continue to be) incarcerated at higher rates than whites . Not coincidentally, blacks are also exonerated due to wrongful accusation at a considerably higher rate than whites . A recidivism ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "DP-SGD vs PATE: Which Has Less <b>Disparate</b> <b>Impact</b> on Model Accuracy? | DeepAI", "url": "https://deepai.org/publication/dp-sgd-vs-pate-which-has-less-disparate-impact-on-model-accuracy", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/dp-sgd-vs-pate-which-has-less-<b>disparate</b>-<b>impact</b>-on-model...", "snippet": "Recent advances in differentially private deep <b>learning</b> have demonstrated that application of differential privacy, specifically the DP-SGD <b>algorithm</b>, has a <b>disparate</b> <b>impact</b> on different sub-groups in the population, which leads to a significantly high drop-in model utility for sub-populations that are under-represented (minorities), <b>compared</b> to well-represented ones. In this work, we aim to compare PATE, another mechanism for training deep <b>learning</b> models using differential privacy, with DP ...", "dateLastCrawled": "2022-01-05T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare", "url": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "Often, the approach to fairness draws upon the legal standard of <b>disparate</b> <b>impact</b>[2][3]. <b>Disparate</b> <b>impact</b> occurs when the predicted outcomes are different for different groups. Some examples of when this metric is used are recidivism[4], hiring[5][6], and loan applications[2]. This standard metric accounts for only one factor: the rate at which the <b>algorithm</b> predicts a person should benefit from a particular classification.", "dateLastCrawled": "2022-02-01T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding and Reducing Bias in <b>Machine Learning</b> | by Jaspreet ...", "url": "https://towardsdatascience.com/understanding-and-reducing-bias-in-machine-learning-6565e23900ac", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-and-reducing-bias-in-<b>machine-learning</b>...", "snippet": "A study done by ProPublica (Jeff Larson, 2016) showed that the <b>algorithm</b> was twice as likely to label black defenders as high risk who eventually did not reoffend as <b>compared</b> to white defenders. This is given by the False Positive rate (FP rate) of black defendants which is 44.85 (i.e. 44.85 percent of the black defendants classified as reoffending did not reoffend) as <b>compared</b> to 23.45 for white defendants. However, Northpointe came with the rebuttal that according to the measures they used ...", "dateLastCrawled": "2022-01-28T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "AI Fairness -A Brief Introduction to AI Fairness 360 | by ...", "url": "https://transformernlp.medium.com/ai-fairness-a-brief-introduction-to-ai-fairness-360-b2e39c96ca49", "isFamilyFriendly": true, "displayUrl": "https://transformernlp.medium.com/ai-fairness-a-brief-introduction-to-ai-fairness-360...", "snippet": "The choice among <b>algorithm</b> categories <b>can</b> partially be made based on the user persona\u2019s ability to intervene at different parts of a <b>machine</b> <b>learning</b> pipeline. If the user is allowed to modify the training data, then pre-processing <b>can</b> be used. If the user is allowed to change the <b>learning</b> <b>algorithm</b>, then in-processing <b>can</b> be used. If the user <b>can</b> only treat the learned model as a black box without any ability to modify the training data or <b>learning</b> <b>algorithm</b>, then only post-processing <b>can</b> ...", "dateLastCrawled": "2022-01-18T14:24:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A <b>machine learning</b> technique that iteratively combines a set of simple and not very accurate classifiers ... <b>disparate</b> <b>impact</b>. #fairness. Making decisions about people that <b>impact</b> different population subgroups disproportionately. This usually refers to situations where an algorithmic decision-making process harms or benefits some subgroups more than others. For example, suppose an algorithm that determines a Lilliputian&#39;s eligibility for a miniature-home loan is more likely to classify them ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare ...", "url": "https://towardsdatascience.com/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare-closedloop-ai-fc07b9c83487", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "<b>Disparate</b> <b>impact</b> occurs when the predicted outcomes are different for different groups. Some examples of when this metric is used are recidivism[4], hiring[5][6], and loan applications[2]. This standard metric accounts for only one factor: the rate at which the algorithm predicts a person should benefit from a particular classification. In the context of healthcare, the standard of <b>disparate</b> <b>impact</b> is entirely inappropriate. The above examples have a common characteristic; every individual ...", "dateLastCrawled": "2022-01-17T14:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare", "url": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "Often, the approach to fairness draws upon the legal standard of <b>disparate</b> <b>impact</b>[2][3]. <b>Disparate</b> <b>impact</b> occurs when the predicted outcomes are different for different groups. Some examples of when this metric is used are recidivism[4], hiring[5][6], and loan applications[2]. This standard metric accounts for only one factor: the rate at which the algorithm predicts a person should benefit from a particular classification.", "dateLastCrawled": "2022-02-01T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Assessing <b>Disparate</b> <b>Impact</b> of Personalized Interventions ...", "url": "https://proceedings.neurips.cc/paper/8603-assessing-disparate-impact-of-personalized-interventions-identifiability-and-bounds.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/8603-assessing-<b>disparate</b>-<b>impact</b>-of-personalized...", "snippet": "result in <b>disparate</b> <b>impact</b> (with regards to social welfare) for the same reasons that these disparities occur in <b>machine</b> <b>learning</b> classi\ufb01cation models [21]. (See Appendix C for an expanded discussion on our use of the term \u201c<b>disparate</b> <b>impact</b>.\u201d) However, in the problem of personalized interventions, the \u201cfundamental problem of causal inference,\u201d that outcomes are not observed for interventions not administered, poses a fundamental challenge for evaluating the fairness of any ...", "dateLastCrawled": "2021-09-17T16:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Feature Engineering for Machine Learning</b>: Why and How | by ...", "url": "https://medium.com/analytics-vidhya/feature-engineering-for-machine-learning-stem-to-shtem-submission-76903112e437", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>feature-engineering-for-machine-learning</b>-stem-to...", "snippet": "Here\u2019s a simple <b>analogy</b>: a student named Timmy, analogous to a supervised <b>machine</b> <b>learning</b> model, has spent the last few weeks studying for a math test so that he can answer questions correctly ...", "dateLastCrawled": "2021-09-13T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Responsible machine learning</b> protects intellectual property | World ...", "url": "https://www.weforum.org/agenda/2021/03/responsible-machine-learning-that-protects-intellectual-property/", "isFamilyFriendly": true, "displayUrl": "https://www.weforum.org/agenda/2021/03/<b>responsible-machine-learning</b>-that-protects...", "snippet": "Given <b>machine</b> <b>learning</b>\u2019s complexity and interdisciplinary nature, executives should employ a wide variety of approaches to manage the associated risks, which include building risk management into model development and applying holistic risk frameworks that leverage and adapt principles used in managing other types of enterprise risk. Executives must also simply step back regularly to consider the broad implications for employees and society when using ML.", "dateLastCrawled": "2022-01-29T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Algorithmic injustice: a relational ethics approach", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7892355/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7892355", "snippet": "<b>Machine</b> classification and prediction are practices that act directly upon the world and result in tangible <b>impact</b>.64 Various companies, institutes, and governments use <b>machine</b>-<b>learning</b> systems across a variety of areas. These systems process people&#39;s behaviors, actions, and the social world at large. The <b>machine</b>-detected patterns often provide \u201canswers\u201d to fuzzy, contingent, and open-ended questions. These \u201canswers\u201d neither reveal any causal relations nor provide explanation on why ...", "dateLastCrawled": "2022-01-26T01:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning is Popular</b> Right Now", "url": "https://machinelearningmastery.com/machine-learning-is-popular/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>machine-learning-is-popular</b>", "snippet": "Abundant and cheap computation has driven the abundance of data we are collecting and the increase in capability of <b>machine learning</b> methods. In this post you learned that <b>machine learning is popular</b> now for three reasons: The field has matured both in terms of identity and in terms of methods and tools. There is an abundance of data to learn from.", "dateLastCrawled": "2022-02-03T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Structural disconnects between algorithmic decision-making</b> and the law ...", "url": "https://blogs.icrc.org/law-and-policy/2019/04/25/structural-disconnects-algorithmic-decision-making-law/", "isFamilyFriendly": true, "displayUrl": "https://blogs.icrc.org/law-and-policy/2019/04/25/structural-disconnects-algorithmic...", "snippet": "And the definition of \u2018works\u2019 is based on (in the case of <b>machine</b> <b>learning</b>) compliance with some prespecified examples of scenarios that \u2018work\u2019 and scenarios that \u2018don\u2019t\u2019. To use a legal <b>analogy</b>, this would be analogous to defining a fair decision by coming up with a rule based on past decisions that someone decided were \u2018right\u2019 or \u2018wrong\u2019 based on past outcomes. In one sense this is entirely circular: we are deciding what is \u2018right\u2019 based on someone deciding what ...", "dateLastCrawled": "2022-01-25T19:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Using analogies to develop new products: What if a locomotive</b> was like ...", "url": "https://sloanreview.mit.edu/article/using-analogies-to-develop-new-products-what-if-a-locomotive-was-like-a-lego-construction/", "isFamilyFriendly": true, "displayUrl": "https://sloanreview.mit.edu/article/<b>using-analogies-to-develop-new-products</b>-what-if-a...", "snippet": "The results ranged from transferring technical solutions from one similar realm to another \u201d what the researchers call \u201cnear analogies\u201d \u201d to analogies between very <b>disparate</b> things, such as thinking about an egg as an <b>analogy</b> for the kind of protection fork-lift truck cabins should provide their drivers \u201d or shark skin as an <b>analogy</b> for what a bathing suit should be like. These the authors term \u201cfar analogies.\u201d They found that design teams tended to use \u201cfar analogies\u201d when ...", "dateLastCrawled": "2022-01-09T22:21:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(disparate impact)  is like +(machine learning algorithm)", "+(disparate impact) is similar to +(machine learning algorithm)", "+(disparate impact) can be thought of as +(machine learning algorithm)", "+(disparate impact) can be compared to +(machine learning algorithm)", "machine learning +(disparate impact AND analogy)", "machine learning +(\"disparate impact is like\")", "machine learning +(\"disparate impact is similar\")", "machine learning +(\"just as disparate impact\")", "machine learning +(\"disparate impact can be thought of as\")", "machine learning +(\"disparate impact can be compared to\")"]}