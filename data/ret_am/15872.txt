{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>K-Median Clustering</b>", "url": "http://www.archive2.cra.org/Activities/craw_archive/dmp/awards/2003/Mower/KMED.html", "isFamilyFriendly": true, "displayUrl": "www.archive2.cra.org/Activities/craw_archive/dmp/awards/2003/Mower/KMED.html", "snippet": "The goal of <b>K-Median clustering</b>, <b>like</b> KNN <b>clustering</b>, is to seperate the data into distinct groups based on the differences in the data. Thus, upon completion, the analyst will be left with k-distinct groups with distinctive characteristics. The goal of <b>K-Median clustering</b> in this instance is to form k-clusters of the adenocarcinoma data, each of which will have different surival times or rates. Basic Definitions: Back to Top: Cluster: - A subset of the data. Cluster Center: - A member of ...", "dateLastCrawled": "2022-02-01T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>K-Means Clustering Algorithm</b> - Javatpoint", "url": "https://www.javatpoint.com/k-means-clustering-algorithm-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>k-means-clustering-algorithm</b>-in-machine-learning", "snippet": "K-Means <b>Clustering</b> is an unsupervised learning <b>algorithm</b> that is used to solve the <b>clustering</b> problems in machine learning or data science. In this topic, we will learn what is <b>K-means clustering algorithm</b>, how the <b>algorithm</b> works, along with the Python implementation of k-means <b>clustering</b>.", "dateLastCrawled": "2022-02-03T14:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Robust <b>K-Median</b> and K-<b>Means Clustering Algorithms for Incomplete Data</b>", "url": "https://www.hindawi.com/journals/mpe/2016/4321928/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2016/4321928", "snippet": "Incomplete data with missing feature values are prevalent in <b>clustering</b> problems. Traditional <b>clustering</b> methods first estimate the missing values by imputation and then apply the classical <b>clustering</b> algorithms for complete data, such as <b>K-median</b> and K-means. However, in practice, it is often hard to obtain accurate estimation of the missing values, which deteriorates the performance of <b>clustering</b>. To enhance the robustness of <b>clustering</b> algorithms, this paper represents the missing values ...", "dateLastCrawled": "2022-01-24T12:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "K-means <b>Clustering</b> and Variants. The <b>clustering</b> problem is to group a ...", "url": "https://towardsdatascience.com/k-means-clustering-and-variants-703f0a09ac36", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/k-means-<b>clustering</b>-and-variants-703f0a09ac36", "snippet": "The K-means <b>clustering</b> <b>algorithm</b> requires fixing K, the number of clusters, in advance. This may seem <b>like</b> a serious limitation. In our example, how would we know the number of forests in advance? That said, we can imagine some user interface that lets the user try out different values of K and see what forests get found? This interactive process might yet be useful. The <b>Algorithm</b>. The K-means <b>clustering</b> <b>algorithm</b> works as follows. We are given the data set to be clustered, and a value of K ...", "dateLastCrawled": "2022-02-03T09:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "k <b>clustering</b> (means / medians) via Python | by pj | Medium", "url": "https://medium.com/@pasdan/k-clustering-means-medians-via-python-2a5f251582ee", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@pasdan/k-<b>clustering</b>-means-medians-via-python-2a5f251582ee", "snippet": "k <b>clustering</b> (means / medians) via Python. pj. Nov 10, 2018 \u00b7 2 min read. This is a quick walk through on setting up your own k <b>clustering</b> <b>algorithm</b> from scratch. This is meant to better ...", "dateLastCrawled": "2022-01-26T23:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>K-means</b> <b>Clustering</b>: <b>Algorithm</b>, Applications, Evaluation Methods, and ...", "url": "https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>k-means</b>-<b>clustering</b>-<b>algorithm</b>-applications-evaluation...", "snippet": "<b>Kmeans</b> <b>algorithm</b> is good in capturing structure of the data if clusters have a spherical-<b>like</b> shape. It always try to construct a nice spherical shape around the centroid. That means, the minute the clusters have a complicated geometric shapes, <b>kmeans</b> does a poor job in <b>clustering</b> the data. We\u2019ll illustrate three cases where <b>kmeans</b> will not perform well.", "dateLastCrawled": "2022-02-02T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Clustering</b> algorithms", "url": "https://maya-ackerman.com/wp-content/uploads/2018/09/ClusteringAlgorithms.pdf", "isFamilyFriendly": true, "displayUrl": "https://maya-ackerman.com/wp-content/uploads/2018/09/<b>ClusteringAlgorithms</b>.pdf", "snippet": "<b>K-median</b>! <b>Like</b> k-means, except that we do not square the distance to the center. !!!! Given a <b>clustering</b> {C1, C2, \u2026, Ck}, the <b>k-median</b> objective function is !!! Where \u00b5i is the mean of Ci. That is, !! 16 K-medoids! <b>Like</b> k-means, except that the centers must be part of the data set. !! Given a <b>clustering</b> {C1, C2, \u2026, Ck}, the k-medoids objective function is !!! where that minimizes the above sum. ! c i 2 C i Xk i=1 X x2Ci kx ci k2. 17 Min-sum! Given a <b>clustering</b> {C1, C2, \u2026, Ck}, the min ...", "dateLastCrawled": "2021-11-04T21:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>clustering</b> - <b>k-means vs k-median</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/109547/k-means-vs-k-median", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/109547/<b>k-means-vs-k-median</b>", "snippet": "I know there is k-means <b>clustering</b> <b>algorithm</b> and <b>k-median</b>. One that uses the mean as the center of the cluster and the other uses the median. My question is: when/where to use which? <b>clustering</b> k-means. Share. Cite. Improve this question. Follow asked Jul 27 &#39;14 at 10:37. Jack Twain Jack Twain. 7,741 14 14 gold badges 47 47 silver badges 73 73 bronze badges $\\endgroup$ 1 $\\begingroup$ You are going to have to define medians (and perhaps calculate them) if you have more than one dimension; if ...", "dateLastCrawled": "2022-01-31T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Data mining and data warehousing</b> multiple choice questions answers pdf", "url": "https://www.eguardian.co.in/data-mining-data-warehousing-multiple-choice-questions-answers-pdf/", "isFamilyFriendly": true, "displayUrl": "https://www.eguardian.co.in/data-mining-data-warehousing-multiple-choice-questions...", "snippet": "A priori <b>algorithm</b> operates in ___ method a. Bottom-up search method b. Breadth-first search method c. None of above d. Both a &amp; b . 2. A bi-directional search takes advantage of ___ process a. Bottom-up process b. Top-down process c. None d. Both a &amp; b. 3. The pincer-search has an advantage over a priori <b>algorithm</b> when the largest frequent item set is long. a. True b. false. 4. MCFS stand for a. Maximum Frequent Candidate Set b. Minimal Frequent Candidate Set c. None of above. 5. MFCS helps ...", "dateLastCrawled": "2022-01-30T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "K-ModesClustering. Whenever we talk about unsupervised\u2026 | by Shailja ...", "url": "https://medium.com/@shailja.nitp2013/k-modesclustering-ef6d9ef06449", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@shailja.nitp2013/k-modes<b>clustering</b>-ef6d9ef06449", "snippet": "<b>clustering</b> <b>algorithm</b> is to group the data objects X into K. clusters by minimize the cost function Eq. (1). The k-modes <b>clustering</b> <b>algorithm</b> is described as, Input: Data objects X, Number of ...", "dateLastCrawled": "2022-01-26T16:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Robust <b>K-Median</b> and K-<b>Means Clustering Algorithms for Incomplete Data</b>", "url": "https://www.hindawi.com/journals/mpe/2016/4321928/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2016/4321928", "snippet": "Incomplete data with missing feature values are prevalent in <b>clustering</b> problems. Traditional <b>clustering</b> methods first estimate the missing values by imputation and then apply the classical <b>clustering</b> algorithms for complete data, such as <b>K-median</b> and K-means. However, in practice, it is often hard to obtain accurate estimation of the missing values, which deteriorates the performance of <b>clustering</b>. To enhance the robustness of <b>clustering</b> algorithms, this paper represents the missing values ...", "dateLastCrawled": "2022-01-24T12:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Coresets for k-Means and <b>k-Median</b> <b>Clustering</b> and their Applications", "url": "https://www2.cs.duke.edu/courses/spring07/cps296.2/papers/har-peled_mazumdar.pdf", "isFamilyFriendly": true, "displayUrl": "https://www2.cs.duke.edu/courses/spring07/cps296.2/papers/har-peled_mazumdar.pdf", "snippet": "Coresets for k-Means and <b>k-Median</b> <b>Clustering</b> and ... dimension [AHV03, Har01a, APV02], although a <b>similar</b> but weaker concept was also used in high dimensions [BHI02, BC03, HV02]. In low dimensions coresets yield approximation <b>algorithm</b> with linear or near linear running time with an additional term that depends only on the size of the coreset. In the present case, the property we desire of the(k,\u03b5)-coreset is that the <b>clustering</b> cost of the coreset for any arbitrary set of k centers is ...", "dateLastCrawled": "2022-01-18T13:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Chapter 4: <b>Clustering</b>", "url": "https://www.dbs.ifi.lmu.de/Lehre/KDD/SS16/skript/4_Clustering(1).pdf", "isFamilyFriendly": true, "displayUrl": "https://www.dbs.ifi.lmu.de/Lehre/KDD/SS16/skript/4_<b>Clustering</b>(1).pdf", "snippet": "<b>K-Median</b> <b>Clustering</b> Problem: Sometimes, data is not numerical Idea: If there is an ordering on the data \ud835\udc4b= ... K-Modes <b>algorithm</b> proceeds <b>similar</b> to k-Means <b>algorithm</b> <b>Clustering</b> Partitioning Methods Variants: K-Medoid, K-Mode, <b>K-Median</b> 20 Huang, Z.: A Fast <b>Clustering</b> <b>Algorithm</b> to Cluster very Large Categorical Data Sets in Data Mining, In DMKD, 1997. 0 2 4 6 8 0 2 4 6 8. DATABASE SYSTEMS GROUP K-Mode <b>Clustering</b>: Example Employee-ID Profession Household Pets #133 Technician Cat #134 ...", "dateLastCrawled": "2022-02-02T20:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Research Article Robust <b>K-Median</b> and K-Means <b>Clustering</b> Algorithms for ...", "url": "https://downloads.hindawi.com/journals/mpe/2016/4321928.pdf", "isFamilyFriendly": true, "displayUrl": "https://downloads.hindawi.com/journals/mpe/2016/4321928.pdf", "snippet": "Our algorithms have <b>similar</b> computation complexity totheclassicalK-medianandK-meansclusteringalgorithms and are more e cient than the <b>clustering algorithms for incomplete data</b> proposed by Zhang et al. [ ] with the time complexity of (2) (when log 2 for the robust K-means <b>clustering</b> <b>algorithm</b>). e paper is organized as follows. Section reviews the classical <b>K-median</b> and K-mean algorithms and presents the robustK-medianandK-meansclusteringproblems.Section gives e ective algorithms for the ...", "dateLastCrawled": "2021-11-20T16:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Generating Normalized Cluster Centers with k-Medians", "url": "https://cs.brown.edu/~aritz/files/kmedian2005.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.brown.edu/~aritz/files/<b>kmedian</b>2005.pdf", "snippet": "The k-medians <b>clustering</b> <b>algorithm</b> is also an important <b>clustering</b> tool because of its well-known resistance to outliers. K-medians, however, is not trivially adapted to produce normalized cluster centers. We introduce a new <b>algorithm</b> (called MN), inspired by spherical k-means, that integrates with k-medians <b>clustering</b> to produce locally optimal normalized cluster centers. We then show theoretically and experimentally that MN produces clusters of significantly higher quality than one would ...", "dateLastCrawled": "2022-01-26T09:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Algorithms for <b>k-median clustering over distributed streams</b>", "url": "https://www.comp.nus.edu.sg/~sutanu/Papers/dsKmed.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.comp.nus.edu.sg/~sutanu/Papers/dsKmed.pdf", "snippet": "the <b>k-median</b> <b>clustering</b> cost with e cient communication. To the best of our knowledge, this important <b>clustering</b> problem has not been studied in the dis- tributed streaming scenario and this paper presents rst results in this direction. 1.1 Related work and contribution of this paper We brie y outline prior work related to <b>k-median</b> <b>clustering</b> in streaming models and the new results in this paper. <b>Clustering</b> is an important computational task and has applications in all area of data analytics ...", "dateLastCrawled": "2021-08-30T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "3.5 The K-<b>Medians and K-Modes Clustering Methods</b> - Week 2 | <b>Coursera</b>", "url": "https://www.coursera.org/lecture/cluster-analysis/3-5-the-k-medians-and-k-modes-clustering-methods-pShI2", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/.../3-5-the-k-<b>medians-and-k-modes-clustering-methods</b>-pShI2", "snippet": "The criterion function for the K-Medians <b>algorithm</b> is written as this. The center is sum, the total sum should be K from one to the number of cluster K, and for each cluster the object in the cluster you just look at the difference. The difference take the absolute value of their distance to the median. The K-Medians <b>clustering</b> <b>algorithm</b> essentially is written as follows. The first, at the very beginning we selected K points as the initial representative objects. That means as initial K ...", "dateLastCrawled": "2022-01-26T21:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Consistent <b>k -Median</b>: Simpler, Better and Robust", "url": "http://proceedings.mlr.press/v130/guo21a/guo21a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v130/guo21a/guo21a.pdf", "snippet": "A drawback of using <b>k -median</b> <b>clustering</b> on real-world data sets is that it is not robust to noisy data, i.e., a few outliers can completely change the cost as well as structure of solutions. Recognizing this shortcoming, Charikar et al. (2001) introduced a robust version of <b>k -median</b> problem called <b>k -median</b> with outliers . The problem <b>is similar</b> to <b>k -median</b> problem except one crucial di erence: An <b>algorithm</b> for <b>k -median</b> with outliers does not need to cluster all the points but can choose ...", "dateLastCrawled": "2021-11-26T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Sequential no-Substitution k-Median-Clustering</b> | DeepAI", "url": "https://deepai.org/publication/sequential-no-substitution-k-median-clustering", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>sequential-no-substitution-k-median-clustering</b>", "snippet": "The objective is to obtain a small R(P,T), compared to the optimal R(P,OPT). An offline <b>k -median</b> <b>algorithm</b> A takes as input a finite set of points S from X and outputs a k -<b>clustering</b> T \u2286S. We say that A is a \u03b2 -approximation offline <b>k -median</b> <b>algorithm</b>, for some \u03b2\u22651, if for all input sets S, R(S,A(S))\u2264\u03b2\u22c5R(S,OPTS).", "dateLastCrawled": "2021-12-22T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Data mining and data warehousing</b> multiple choice questions answers pdf", "url": "https://www.eguardian.co.in/data-mining-data-warehousing-multiple-choice-questions-answers-pdf/", "isFamilyFriendly": true, "displayUrl": "https://www.eguardian.co.in/data-mining-data-warehousing-multiple-choice-questions...", "snippet": "A priori <b>algorithm</b> operates in ___ method a. Bottom-up search method b. Breadth-first search method c. None of above d. Both a &amp; b . 2. A bi-directional search takes advantage of ___ process a. Bottom-up process b. Top-down process c. None d. Both a &amp; b. 3. The pincer-search has an advantage over a priori <b>algorithm</b> when the largest frequent item set is long. a. True b. false. 4. MCFS stand for a. Maximum Frequent Candidate Set b. Minimal Frequent Candidate Set c. None of above. 5. MFCS helps ...", "dateLastCrawled": "2022-01-30T18:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "-means <b>clustering</b>", "url": "https://www.cs.yale.edu/homes/el327/datamining2012aFiles/11_k_means_clustering.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.yale.edu/homes/el327/datamining2012aFiles/11_k_means_<b>clustering</b>.pdf", "snippet": "k-means or the k-means <b>algorithm</b>. This <b>algorithm</b> <b>can</b> <b>be thought</b> of as a potential function reducing <b>algorithm</b>. The potential function is f k means= X j2[k] X i2S j kx i jk2: The sets S j are the sets of points to which j is the closest center. In each step of the <b>algorithm</b> the potential function is reduced. Let\u2019s examine that. First, if the set of centers j are xed, the best assignment is clearly the one which assigns each data point to its closest center. Also, assume that is the center ...", "dateLastCrawled": "2021-11-19T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "1 A Soft <b>Clustering</b> <b>Algorithm</b>", "url": "http://www.cs.bilkent.edu.tr/~guvenir/courses/CS550/Workshop/Ertugrul_Kartal_Tabak.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.bilkent.edu.tr/~guvenir/courses/CS550/Workshop/Ertugrul_Kartal_Tabak.pdf", "snippet": "Clusters may <b>be thought</b> as the process of producing unlabeled categorized data. A. Terminology <b>Clustering</b> <b>can</b> be de\ufb01ned as grouping similar objects together. The similarity <b>can</b> be de\ufb01ned in various ways. The most common de\ufb01nition of similarity is the distance function. The distance function may be the Euclidian distance or p-norm on metric space, or it <b>can</b> be the cosine-distance as in text categorization. Metric System: A metric distance function is a function that obeys two rules ...", "dateLastCrawled": "2021-11-19T10:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>K-median</b> <b>clustering</b>, model-based compressiv e sensing, and sparse recov ...", "url": "https://dspace.mit.edu/bitstream/handle/1721.1/73011/Indyk_K-median%20clustering.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://dspace.mit.edu/bitstream/handle/1721.1/73011/Indyk_<b>K-median</b> <b>clustering</b>.pdf...", "snippet": "the best k-median4 <b>clustering</b> of x. Moreover, for each such center c, the value of x0 cis equal to the total weight of pixels in the cluster centered at c. Thus, a solution to the <b>k-median</b> problem provides a solution to our sparse recovery problem as well5. There has been prior work on the <b>k-median</b> problem in the streaming model under insertions and dele-tions of points [FS05, Ind04]. Such algorithms utilize linear sketches, and therefore implicitly provide schemes for approximating the k ...", "dateLastCrawled": "2022-02-01T03:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>K-means Clustering Algorithm: Applications, Types</b>, and Demos [Updated ...", "url": "https://www.simplilearn.com/tutorials/machine-learning-tutorial/k-means-clustering-algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.simplilearn.com/.../machine-learning-tutorial/k-means-<b>clustering</b>-<b>algorithm</b>", "snippet": "K-Means <b>Clustering</b> <b>Algorithm</b> ... It means the original point, which we <b>thought</b> was the centroid, will shift to the new position, which is the actual centroid for each of these groups. Step 4: Keep repeating step 2 and step 3 until convergence is achieved. Demo: K-Means <b>Clustering</b>. Problem Statement - Walmart wants to open a chain of stores across the state of Florida, and it wants to find the optimal store locations to maximize revenue. The issue here is if they open too many stores close to ...", "dateLastCrawled": "2022-02-02T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A &lt;Emphasis Type=&#39;Italic&#39;&gt;k&lt;/Emphasis&gt;-Median <b>Algorithm</b> with Running ...", "url": "https://link.springer.com/content/pdf/10.1023/b:mach.0000033115.78247.f0.pdf", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/content/pdf/10.1023/b:mach.0000033115.78247.f0.pdf", "snippet": "We also present a related <b>algorithm</b> for \ufb01nding a <b>clustering</b> that excludes a small number of outliers. Keywords: <b>clustering</b>, sampling, sublinear 1. Introduction <b>Clustering</b>, or grouping data into representative groups, is a tool commonly used in data analysis, and <b>can</b> often make the manipulation of large data sets simpler. For example, in a large customer database a cluster might represent customers with similar characteristics. In a vision application, a cluster might be the group of pixels ...", "dateLastCrawled": "2022-01-29T09:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "17 <b>Clustering</b> Algorithms Used In Data Science and Mining | by Mahmoud ...", "url": "https://towardsdatascience.com/17-clustering-algorithms-used-in-data-science-mining-49dbfa5bf69a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/17-<b>clustering</b>-<b>algorithms</b>-used-in-data-science-mining-49...", "snippet": "This <b>algorithm</b> <b>can</b> <b>be thought</b> of as a composition between k-means and k-modes algorithms. Using this <b>algorithm</b>, each data point has a weight being a part of numerical and categorical clusters. Moreover, each type of observation <b>can</b> be treated in a separate fashion where centroids play the role of an attractor in each type of cluster. The membership to a given data point <b>can</b> be controlled using a fuzzy membership function aij like in FCM. The cost function for K-prototypes. \ud835\udefe is used to ...", "dateLastCrawled": "2022-02-02T06:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "0368-3248-01-Algorithms in Data Mining Fall 2013 Lecture 10: -means ...", "url": "https://www.cs.yale.edu/homes/el327/datamining2013aFiles/10_k_means_clustering.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.yale.edu/homes/el327/datamining2013aFiles/10_k_means_<b>clustering</b>.pdf", "snippet": "This <b>algorithm</b> <b>can</b> <b>be thought</b> of as a potential function reducing <b>algorithm</b>. The potential function is f k means= X j2[k] X i2S j kx i jk2: The sets S jare the sets of points to which jis the closest center. In each step of the <b>algorithm</b> the potential function is reduced. Let\u2019s examine that. First, if the set of centers jare xed, the best assignment is clearly the one which assigns each data point to its closest center. Also, assume that is the center of a set of points S. Then, if we move ...", "dateLastCrawled": "2022-01-16T20:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "An improved (1+1) evolutionary <b>algorithm</b> for <b>k-median</b> <b>clustering</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0378437119316917", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0378437119316917", "snippet": "For convenience, we let C = D for the three datasets. We set \u03b5 = 0. 1 for the parameter \u03b5 in <b>Algorithm</b> 2, and \u03b4 = 0. 25 for the parameter \u03b4 in local search .Since all algorithms work with an individual except the PSO-BB-BC, we set a small population size (P = 20) for PSO-BB-BC for fairness, and other parameters are set to the same in .We independently run each <b>algorithm</b> 30 times for each instance.", "dateLastCrawled": "2022-01-10T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Clustering</b> via Similarity Functions: Theoretical Foundations and Algorithms", "url": "https://www.cs.cmu.edu/~ninamf/papers/clustering-bbv-long.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~ninamf/papers/<b>clustering</b>-bbv-long.pdf", "snippet": "such as <b>k-median</b>, min-sum or k-means, and then developing algorithms for approximately op-timizing this objective given a data set represented as a weighted graph [Charikar et al., 1999, Kannan et al., 2004, Jain and Vazirani, 2001]. That is, the graph is viewed as \u201cground truth\u201d and the goal is to design algorithms to optimize various objectives over this graph. However, for most <b>clustering</b> problems such as <b>clustering</b> documents by topic or <b>clustering</b> proteins by function, ground truth ...", "dateLastCrawled": "2022-02-03T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) A framework for statistical <b>clustering</b> with constant time ...", "url": "https://www.researchgate.net/publication/225168040_A_framework_for_statistical_clustering_with_constant_time_approximation_algorithms_for_K-median_and_K-means_clustering", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/225168040_A_framework_for_statistical...", "snippet": "<b>be thought</b> of as a singleton set). ... This guarantee <b>can</b> be compared to the guarantees of an offline <b>algorithm</b> that uses the same <b>k-median</b> <b>algorithm</b> A as a black box. As shown in [7], for S \u223c P ...", "dateLastCrawled": "2021-08-27T09:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Robust <b>K-Median</b> and K-<b>Means Clustering Algorithms for Incomplete Data</b>", "url": "https://www.hindawi.com/journals/mpe/2016/4321928/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2016/4321928", "snippet": "<b>Compared</b> with existing methods, the proposed algorithms are insensitive to estimation errors of the constructed intervals, especially when the missing rate is high. Comparisons and analysis of numerical experimental results on UCI data sets also validate the effectiveness of the proposed robust algorithms. <b>Compared</b> with existing algorithms, the advantages of the proposed robust <b>clustering</b> algorithms are twofold. First, our algorithms <b>can</b> cluster incomplete data without imputation for the ...", "dateLastCrawled": "2022-01-24T12:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sequential no-Substitution k-Median-Clustering</b> | DeepAI", "url": "https://deepai.org/publication/sequential-no-substitution-k-median-clustering", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>sequential-no-substitution-k-median-clustering</b>", "snippet": "The objective is to obtain a small R(P,T), <b>compared</b> to the optimal R(P,OPT). An offline <b>k -median</b> <b>algorithm</b> A takes as input a finite set of points S from X and outputs a k -<b>clustering</b> T \u2286S. We say that A is a \u03b2 -approximation offline <b>k -median</b> <b>algorithm</b>, for some \u03b2\u22651, if for all input sets S, R(S,A(S))\u2264\u03b2\u22c5R(S,OPTS).", "dateLastCrawled": "2021-12-22T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Research Article Robust <b>K-Median</b> and K-Means <b>Clustering</b> Algorithms for ...", "url": "https://downloads.hindawi.com/journals/mpe/2016/4321928.pdf", "isFamilyFriendly": true, "displayUrl": "https://downloads.hindawi.com/journals/mpe/2016/4321928.pdf", "snippet": "robust <b>K-median</b> <b>clustering</b> <b>algorithm</b> <b>can</b> be given in <b>Algo-rithm</b>. <b>Algorithm</b> (robust <b>K-median</b> <b>clustering</b> <b>algorithm</b>). Input .efeaturematrix ,intervalsize ( , )and 2. Output . e cluster prototype matrix and membership matrix . Step (initialization).Set iterationindex =0 and randomly select di erentrowsfrom astheinitialclusterprototypes {V", "dateLastCrawled": "2021-11-20T16:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Generating Normalized Cluster Centers with k-Medians", "url": "http://cs.brown.edu/~aritz/files/kmedian2005.pdf", "isFamilyFriendly": true, "displayUrl": "cs.brown.edu/~aritz/files/<b>kmedian</b>2005.pdf", "snippet": "The k-medians <b>clustering</b> <b>algorithm</b> is also an important <b>clustering</b> tool because of its well-known resistance to outliers. K-medians, however, is not trivially adapted to produce normalized cluster centers. We introduce a new <b>algorithm</b> (called MN), inspired by spherical k-means, that integrates with k-medians <b>clustering</b> to produce locally optimal normalized cluster centers. We then show theoretically and experimentally that MN produces clusters of significantly higher quality than one would ...", "dateLastCrawled": "2022-01-10T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Consistent <b>k -Median</b>: Simpler, Better and Robust", "url": "http://proceedings.mlr.press/v130/guo21a/guo21a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v130/guo21a/guo21a.pdf", "snippet": "mation ratio and recourse, <b>compared</b> to that of (Lattanzi and Vassilvitskii, 2017). 1 Introduction <b>Clustering</b> is one of the most fundamental primitives in unsupervised machine learning, and <b>k -median</b> <b>cluster-ing</b> is one of the most widely used primitives in prac-tice. Input to the problem consists of a set C of n points, a set F of potential median locations, a metric space d : (C [ F ) (C [ F ) ! R 0. The goal is to choose a subset S F of cardinality at most k so as to minimize P j 2 C d(j;S ...", "dateLastCrawled": "2021-11-26T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Clustering</b> Perturbation Resilient <b>k-Median</b> Instances", "url": "http://pages.cs.wisc.edu/~yliang/clusteringPRMedian.pdf", "isFamilyFriendly": true, "displayUrl": "pages.cs.wisc.edu/~yliang/<b>clustering</b>PRMedian.pdf", "snippet": "known results. Additionally, we give a sublinear-time <b>algorithm</b> which <b>can</b> return an implicit <b>clustering</b> from only access to a small random sample. 1 Introduction Problems of <b>clustering</b> data from pairwise distance information are a classical topic in machine learning. A common approach is to optimize various objective functions such as <b>k-median</b>, k-means or min-sum. However, for most natural <b>clustering</b> objectives, \ufb01nding the optimal solution is NP-hard. There has been substantial work on ...", "dateLastCrawled": "2021-08-27T00:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "On Coresets for <b>k -Median</b> and k -Means <b>Clustering</b> in Metric and ...", "url": "https://www.researchgate.net/publication/220617873_On_Coresets_for_k_-Median_and_k_-Means_Clustering_in_Metric_and_Euclidean_Spaces_and_Their_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220617873_On_Coresets_for_<b>k_-Median</b>_and_k...", "snippet": "Finally, we show that $\\epsilon$-coresets <b>can</b> be used to improve an existing approximation <b>algorithm</b> for $(1,\\ell)$-median <b>clustering</b> under the Fr\\&#39;echet distance, taking a further step in the ...", "dateLastCrawled": "2022-01-16T04:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Reconciliation k-median: Clustering with Non-Polarized Representatives</b> ...", "url": "https://deepai.org/publication/reconciliation-k-median-clustering-with-non-polarized-representatives", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>reconciliation-k-median-clustering-with</b>-non-polarized...", "snippet": "A standard approach to this problem is <b>clustering</b>: design a distance function that captures similarity between the news articles and apply a <b>clustering</b> <b>algorithm</b> on the resulting metric space. Common <b>clustering</b> formulations, such as <b>k-median</b> or k-means, <b>can</b> be used (Jain and Dubes, 1988). The original set of input news articles <b>can</b> then be ...", "dateLastCrawled": "2021-12-06T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Consistent <b>k-Median</b>: Simpler, Better and Robust", "url": "http://proceedings.mlr.press/v130/guo21a.html", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v130/guo21a.html", "snippet": "We show that a simple local-search based on-line <b>algorithm</b> <b>can</b> give a bicriteria constant approximation for the problem with O(k^2 log^2(nD)) swaps of medians (recourse) in total, where D is the diameter of the metric. When restricted to the problem without outliers, our <b>algorithm</b> is simpler, deterministic and gives better approximation ratio and recourse, <b>compared</b> to that of Lattanzi-Vassilvitskii [18].", "dateLastCrawled": "2021-12-26T20:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "K-ModesClustering. Whenever we talk about unsupervised\u2026 | by Shailja ...", "url": "https://medium.com/@shailja.nitp2013/k-modesclustering-ef6d9ef06449", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@shailja.nitp2013/k-modes<b>clustering</b>-ef6d9ef06449", "snippet": "<b>clustering</b> <b>algorithm</b> is to group the data objects X into K. clusters by minimize the cost function Eq. (1). The k-modes <b>clustering</b> <b>algorithm</b> is described as, Input: Data objects X, Number of ...", "dateLastCrawled": "2022-01-26T16:19:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Basics with the K-Nearest Neighbors Algorithm | by ...", "url": "https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine</b>-<b>learning</b>-basics-with-the-k-nearest-neighbors...", "snippet": "A supervised <b>machine</b> <b>learning</b> algorithm (as opposed to an unsupervised <b>machine</b> <b>learning</b> algorithm) ... The <b>analogy</b> above of teaching a child to identify a pig is another example of a classification problem. Image showing randomly generated data. This image shows a basic example of what classification data might look like. We have a predictor (or set of predictors) and a label. In the image, we might be trying to predict whether someone likes pineapple (1) on their pizza or not (0) based on ...", "dateLastCrawled": "2022-02-03T00:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Computational Theory of Clustering", "url": "https://www.cs.cmu.edu/~avrim/Talks/clustering08.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~avrim/Talks/clustering08.pdf", "snippet": "to <b>k-median</b> objective is \u03b5-close pointwiseto truth. \u2022 This is an assumption about how the similarity info relates to the target clustering. \u2022 Why not make it explicit? More generally: what natural propertiesof similarity info are sufficient to cluster well, and by what kinds of algorithms? <b>Analogy</b> to <b>learning</b>: what concept classes are", "dateLastCrawled": "2021-11-30T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Evaluation of Scalable Fair Clustering <b>Machine</b> <b>Learning</b> Methods for ...", "url": "https://link.springer.com/chapter/10.1007/978-3-030-74753-4_10", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-74753-4_10", "snippet": "Fair clustering <b>K-median</b> Fairness <b>Machine</b> <b>learning</b> ... <b>Analogy</b> of tactics, techniques, and procedures. J. Inf. Process. Syst. 15(4), 865\u2013889 (2019) Google Scholar. 13. H. Haddadpajouh, A. Azmoodeh, A. Dehghantanha, R.M. Parizi, MVFCC: A multi-view fuzzy consensus clustering model for malware threat attribution. IEEE Access 8, 139188\u2013139198 (2020) CrossRef Google Scholar. 14. H. Darabian et al., A multiview <b>learning</b> method for malware threat hunting: Windows, IoT and android as case ...", "dateLastCrawled": "2022-01-02T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Algorithmic Aspects of <b>Machine</b> <b>Learning</b>: Final Project", "url": "https://people.csail.mit.edu/moitra/docs/projects408.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.csail.mit.edu/moitra/docs/projects408.pdf", "snippet": "Algorithmic Aspects of <b>Machine</b> <b>Learning</b>: Final Project Instructor: Ankur Moitra Due: December 13th The last assignment for the semester is to write a 4 6 page nal paper. You can work in pairs or you can work alone, and there are two options: (1) You can write a literature review on some topic related to the material that we covered in class. In this class, we only focused on problems where there are provable guarantees so you should certainly choose a topic where there are provable ...", "dateLastCrawled": "2022-01-07T04:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>machine</b> <b>learning</b> - What makes the distance measure in k-medoid &quot;better ...", "url": "https://stackoverflow.com/questions/21619794/what-makes-the-distance-measure-in-k-medoid-better-than-k-means", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/21619794", "snippet": "<b>machine</b>-<b>learning</b> cluster-analysis data-mining k-means. Share. Improve this question. Follow edited Jun 4 &#39;15 at 9:37. Has QUIT--Anony-Mousse. 72.6k 12 12 gold badges 129 129 silver badges 187 187 bronze badges. asked Feb 7 &#39;14 at 5:08. tumultous_rooster tumultous_rooster. 11.1k 28 28 gold badges 83 83 silver badges 143 143 bronze badges. 2. 3. stats.stackexchange.com can be better place to get more deep and theoritical answers. \u2013 berkay. Feb 10 &#39;14 at 1:55. See my updated answer, for the ...", "dateLastCrawled": "2022-01-28T06:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Abstract - people.csail.mit.edu", "url": "https://people.csail.mit.edu/sclaici/pdf/claici2018coresets.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.csail.mit.edu/sclaici/pdf/claici2018coresets.pdf", "snippet": "training, k-means clustering, <b>k-median</b> clustering, and linear regression and show that we are competitive with previous coreset constructions. 1 Introduction Data sets with hundreds of millions of examples are becoming the norm in <b>machine</b> <b>learning</b>, whether for Bayesian inference, clustering, or regression. The complexity of algorithms for these tasks typically scales in the size of the data set, making it dif\ufb01cult to employ the entire input effectively. Several techniques attempt to ...", "dateLastCrawled": "2021-08-28T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Nuit Blanche: Coreset and PCA: Distributed k-Means and <b>k-Median</b> ...", "url": "https://nuit-blanche.blogspot.com/2014/10/coreset-and-pca-distributed-k-means-and.html", "isFamilyFriendly": true, "displayUrl": "https://nuit-blanche.blogspot.com/2014/10/coreset-and-pca-distributed-k-means-and.html", "snippet": "This paper provides new algorithms for distributed clustering for two popular center-based objectives, <b>k-median</b> and k-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by [13], we reduce the problem of finding a clustering with low cost to the problem of finding a coreset of small size. We provide a distributed method for constructing a global coreset which improves over the previous ...", "dateLastCrawled": "2022-01-31T22:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Ensemble Learning from Scratch</b>. Introducing Ensemble <b>Learning</b>, a\u2026 | by ...", "url": "https://towardsdatascience.com/ensemble-learning-from-scratch-20672123e6ca", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>ensemble-learning-from-scratch</b>-20672123e6ca", "snippet": "\u201cHands-On <b>Machine</b> <b>Learning</b> with Scikit-Learn, Keras and TensorFlow, 2nd Edition, by Aur\u00e9lien G\u00e9ron (O\u2019Reilly).\u201d, Chapter 7. But why is it like that? Wisdom of the crowd may sound \u2018smart\u2019, but it\u2019s still somewhat counterintuitive. To answer that, the following <b>analogy</b> may help shed some light on the mystery: Say that you tossing a slightly biased coin that has a 51% chance of coming up heads. If you do it 100 times, it is not very likely that there will be exactly 51 heads, and ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Beyond Worst-Case Analysis</b> | March 2019 | Communications of the ACM", "url": "https://cacm.acm.org/magazines/2019/3/234931-beyond-worst-case-analysis/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2019/3/234931", "snippet": "Much of the present and future of research going <b>beyond worst-case analysis</b> is motivated by advances in <b>machine</b> <b>learning</b>. p The unreasonable effectiveness of modern <b>machine</b> <b>learning</b> algorithms has thrown down the gauntlet to algorithms researchers, and there is perhaps no other problem domain with a more urgent need for the <b>beyond worst-case</b> approach.", "dateLastCrawled": "2021-12-30T23:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "* Artificial Intelligence - Table of contents", "url": "https://en.mimi.hu/artificial_intelligence/index_artificial_intelligence.html", "isFamilyFriendly": true, "displayUrl": "https://en.mimi.hu/artificial_intelligence", "snippet": "Adversarial <b>machine</b> <b>learning</b>. Adversarial networks. Advice taker. Affective computing. Affine layer B. Backpropagation. Back-propagation. Backpropagation algorithm. Backpropagation through time. Backtracking. Backward chaining. Bag of words. Bag of words model. Bag-of-words model ...", "dateLastCrawled": "2022-01-28T10:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Multi-coloring trees | Magnus M. Halldorsson - Academia.edu", "url": "https://www.academia.edu/1456399/Multi_coloring_trees", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/1456399/Multi_coloring_trees", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-06T00:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) On Variants of k-means Clustering", "url": "https://www.researchgate.net/publication/286513238_On_Variants_of_k-means_Clustering", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/286513238_On_Variants_of_k-means_Clustering", "snippet": "Abstract. \\textit {Clustering problems} often arise in the fields like data mining, <b>machine</b> <b>learning</b> etc. to group a collection of objects into similar groups with respect to a similarity (or ...", "dateLastCrawled": "2022-01-05T12:58:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(k-median)  is like +(clustering algorithm)", "+(k-median) is similar to +(clustering algorithm)", "+(k-median) can be thought of as +(clustering algorithm)", "+(k-median) can be compared to +(clustering algorithm)", "machine learning +(k-median AND analogy)", "machine learning +(\"k-median is like\")", "machine learning +(\"k-median is similar\")", "machine learning +(\"just as k-median\")", "machine learning +(\"k-median can be thought of as\")", "machine learning +(\"k-median can be compared to\")"]}