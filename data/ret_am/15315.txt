{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>BLEU</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/BLEU", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>BLEU</b>", "snippet": "<b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is an algorithm for evaluating the quality of text which has been machine-translated from one natural <b>language</b> to another. Quality is considered to be the correspondence between a machine&#39;s output and that of a human: &quot;the closer a machine translation is to a professional human translation, the better it is&quot; \u2013 this is the central idea behind <b>BLEU</b>. <b>BLEU</b> was one of the first metrics to claim a high correlation with human judgements of quality, and ...", "dateLastCrawled": "2022-01-25T14:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching algorithm that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score , which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years.", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise machine translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching algorithm that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to Calculating the <b>BLEU</b> Score for Text in Python", "url": "https://machinelearningmastery.com/calculate-bleu-score-for-text-python/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/calculate-<b>bleu</b>-score-for-text-pyth", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for comparing a candidate translation of text to one or more reference translations. Although developed for translation, it can be used to evaluate text generated for a suite of natural <b>language</b> processing tasks. In this tutorial, you will discover the <b>BLEU</b> score for evaluating and scoring candidate text using the NLTK library in", "dateLastCrawled": "2022-01-30T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "This measure, looking at n-grams overlap between the output and reference translations with a penalty for shorter outputs, is known as <b>BLEU</b> (short for \u201c<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>\u201d which people literally only ever say when explaining the acronym) and was developed by Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu at IBM in 2002. It\u2019s a very popular metric in NLP, particularly for tasks where the output of a system is a text string rather than a classification. This ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Metrics for NLG <b>evaluation</b>. Simple natural <b>language</b> processing\u2026 | by ...", "url": "https://medium.com/explorations-in-language-and-learning/metrics-for-nlg-evaluation-c89b6a781054", "isFamilyFriendly": true, "displayUrl": "https://medium.com/explorations-in-<b>language</b>-and-<b>learning</b>/<b>metric</b>s-for-nlg-<b>evaluation</b>-c...", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) This is by far the most popular <b>metric</b> for evaluating machine translation system. In <b>BLEU</b>, precision and recall are approximated by modified n-gram precision ...", "dateLastCrawled": "2022-02-03T08:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "Abstract and Figures. Our research extends the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical machine translation to make it more adjustable and robust. We intend to ...", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>The Most Common Evaluation Metrics In NLP</b> | by Kurtis Pykes | Towards ...", "url": "https://towardsdatascience.com/the-most-common-evaluation-metrics-in-nlp-ced6a763ac8b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>the-most-common-evaluation-metrics-in-nlp</b>-ced6a763ac8b", "snippet": "<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) The <b>BLEU</b> score evaluates the quality of text that has been translated by a machine from one natural <b>language</b> to another. Therefore, it\u2019s typically used for Machine-translation tasks, however, it\u2019s also being used in other tasks such as text generation, paraphrase generation, and text summarization.", "dateLastCrawled": "2022-02-03T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Metrics for NLG Evaluation</b> - GitHub Pages", "url": "https://desh2608.github.io/2017-09-16-metrics-for-nlg-evaluation/", "isFamilyFriendly": true, "displayUrl": "https://desh2608.github.io/2017-09-16-<b>metrics-for-nlg-evaluation</b>", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) This is by far the most popular metric for evaluating machine translation system. In <b>BLEU</b>, precision and recall are approximated by modified n-gram precision and best match length, respectively.. Modified n-gram precision: First, an n-gram precision is the fraction of n-grams in the candidate text which are present in any of the reference texts.From the example above, the unigram precision of A is 100%.", "dateLastCrawled": "2021-12-07T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) IMAGE CAPTION GENERATOR USING DEEP <b>LEARNING</b>-Convolutional Neural ...", "url": "https://www.researchgate.net/publication/357955678_IMAGE_CAPTION_GENERATOR_USING_DEEP_LEARNING-Convolutional_Neural_Network_Recurrent_Neural_Network_Bilingual_Evaluation_UnderstudyBLEU_scoreLong_Short_Time_Memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357955678_IMAGE_CAPTION_GENERATOR_USING_DEEP...", "snippet": "IMAGE CAPTION GENERATOR USING DEEP <b>LEARNING</b>-Convolutional Neural Network, Recurrent Neural Network, (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)<b>BLEU</b> score,Long Short Time Memory January 2022 Authors:", "dateLastCrawled": "2022-01-30T23:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>BLEU</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/BLEU", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>BLEU</b>", "snippet": "<b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is an algorithm for evaluating the quality of text which has been machine-translated from one natural <b>language</b> to another. Quality is considered to be the correspondence between a machine&#39;s output and that of a human: &quot;the closer a machine translation is to a professional human translation, the better it is&quot; \u2013 this is the central idea behind <b>BLEU</b>. <b>BLEU</b> was one of the first metrics to claim a high correlation with human judgements of quality, and ...", "dateLastCrawled": "2022-02-02T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>BLEU-BERT-y: Comparing sentence scores</b> | by Gergely D. N\u00e9meth | Towards ...", "url": "https://towardsdatascience.com/bleu-bert-y-comparing-sentence-scores-307e0975994d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bleu-bert-y-comparing-sentence-scores</b>-307e0975994d", "snippet": "<b>BLEU</b>. <b>BLEU</b>: <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> provides a score to compare sentences [1]. Originally, it was developed for translation, to evaluate a predicted translation using reference translations, however, it can be used for sentence similarity as well. Here is a good introduction of <b>BLEU</b> (or read the original paper).", "dateLastCrawled": "2022-02-02T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching algorithm that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to Calculating the <b>BLEU</b> Score for Text in Python", "url": "https://machinelearningmastery.com/calculate-bleu-score-for-text-python/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/calculate-<b>bleu</b>-score-for-text-pyth", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for comparing a candidate translation of text to one or more reference translations. Although developed for translation, it can be used to evaluate text generated for a suite of natural <b>language</b> processing tasks. In this tutorial, you will discover the <b>BLEU</b> score for evaluating and scoring candidate text using the NLTK library in", "dateLastCrawled": "2022-01-30T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "BLUE Score - Machine <b>Learning</b> Interviews", "url": "https://machinelearninginterview.com/topics/natural-language-processing/bleu-score/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>interview.com/topics/natural-<b>language</b>-processing/<b>bleu</b>-score", "snippet": "<b>BLEU</b> stands for <b>Bilingual</b> <b>evaluation</b> <b>Understudy</b>. It is a metric used to evaluate the quality of machine generated text by comparing it with a reference text that is supposed to be generated. Usually, the reference text is generated by a manual evaluator or a translator. Where is <b>BLEU</b> Score used? <b>BLEU</b> score was traditionally used to evaluate machine translation. Hence the <b>Bilingual</b>. (Note: Take a look at our article on a brief history of machine translation, that quickly summarizes various ...", "dateLastCrawled": "2022-01-28T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Machine Translation <b>Evaluation</b> with sacreBLEU and BERTScore | by Ng Wai ...", "url": "https://towardsdatascience.com/machine-translation-evaluation-with-sacrebleu-and-bertscore-d7fdb0c47eb3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/machine-translation-<b>evaluation</b>-with-sacre<b>bleu</b>-and-bert...", "snippet": "For your information, <b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is one of the most popular metric for evaluating machine-translated text. It can be used to evaluate translations of any <b>language</b> provided that there exists some form of word boundary in the text. <b>BLEU</b>\u2019s output is usually a score between 0 and 100, indicating the similarity value between the reference text and hypothesis text. The higher the value, the better the translations. Having said th a t, one of the major downside for ...", "dateLastCrawled": "2022-01-29T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise machine translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching algorithm that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "<b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical machine translation to make it more adjustable and robust . We in tend to adapt it to resemble human <b>evaluatio n</b> mor e.", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Tangled up in <b>BLEU</b>", "url": "https://skeptric.com/tangled-bleu/", "isFamilyFriendly": true, "displayUrl": "https://skeptric.com/tangled-<b>bleu</b>", "snippet": "One of the oldest examples is the <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>). This requires a source text to translate, as well as one or more reference translations. Then the automatic translation is evaluated based on average n-gram precision; that is taking all sequences of n tokens from the candidate translation, how many are in the reference translation? It turns out this depends a bit on some choices you make (how you tokenise, how you penalise very short translations), so there&#39;s a ...", "dateLastCrawled": "2021-12-08T11:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) IMAGE CAPTION GENERATOR USING DEEP <b>LEARNING</b>-Convolutional Neural ...", "url": "https://www.researchgate.net/publication/357955678_IMAGE_CAPTION_GENERATOR_USING_DEEP_LEARNING-Convolutional_Neural_Network_Recurrent_Neural_Network_Bilingual_Evaluation_UnderstudyBLEU_scoreLong_Short_Time_Memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357955678_IMAGE_CAPTION_GENERATOR_USING_DEEP...", "snippet": "IMAGE CAPTION GENERATOR USING DEEP <b>LEARNING</b>-Convolutional Neural Network, Recurrent Neural Network, (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)<b>BLEU</b> score,Long Short Time Memory . January 2022; Authors: Ch ...", "dateLastCrawled": "2022-01-30T23:48:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "<b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical machine translation to make it more adjustable and robust . We in tend to adapt it to resemble human <b>evaluatio n</b> mor e.", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "BLEURT -Failures. <b>A new</b> metric to measure the textual\u2026 | by Priyanshu ...", "url": "https://medium.com/vision-ml/bleurt-failures-494623b26352", "isFamilyFriendly": true, "displayUrl": "https://medium.com/vision-ml/<b>bleu</b>rt-failures-494623b26352", "snippet": "Hence <b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) was introduced. To know more about <b>BLEU</b> you <b>can</b> read it here. But in short, <b>BLEU</b> performs a certain type of n-gram intersection so as to calculate the ...", "dateLastCrawled": "2021-11-27T10:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Automatic Assessment of Open Ended</b> Questions with a <b>Bleu</b>-Inspired ...", "url": "https://www.researchgate.net/publication/221418794_Automatic_Assessment_of_Open_Ended_Questions_with_a_Bleu-Inspired_Algorithm_and_Shallow_NLP", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221418794_<b>Automatic_Assessment_of_Open_Ended</b>...", "snippet": "For the <b>evaluation</b> we use <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) and NIST (National Institute of Standards and Technology) scoring techniques. A detailed <b>evaluation</b> of these models is performed by ...", "dateLastCrawled": "2021-12-24T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Neural <b>Machine Translation</b>: Inner Workings, Seq2Seq, and Transformers ...", "url": "https://towardsdatascience.com/neural-machine-translation-inner-workings-seq2seq-and-transformers-229faff5895b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-<b>machine-translation</b>-inner-workings-seq2seq-and...", "snippet": "Alternatively, people mostly use the <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score. It produces a score between [0,1], in some cases, we multiply it with 100 for convenience. It uses n-grams (1, 2, 3, and 4 mostly) to evaluate the translation with some additional internal tricks. But this <b>evaluation</b> mechanism might slightly fail in this example ...", "dateLastCrawled": "2022-01-29T08:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Essential KPIs for SMT: F-Measure</b> \u2013 KantanMT \u2013 Machine <b>Learning</b> ...", "url": "https://kantanmtblog.com/2014/01/30/essential-kpis-for-smt-f-measure/", "isFamilyFriendly": true, "displayUrl": "https://kantanmtblog.com/2014/01/30/<b>essential-kpis-for-smt-f-measure</b>", "snippet": "In the next post I will look at <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) and examine how this metric helps us to further understand the quality of SMT engines. KantanMT\u2019s <b>new</b> BuildAnalytics technology illustrates the distribution of F-Measure, <b>BLEU</b>, and TER score across our members SMT engines. It also generates a Gap Analysis, highlighting missing words in members training data, and gives a provides KantanMT members with a training data rejects reports \u2013 great information that helps ...", "dateLastCrawled": "2022-01-31T23:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What are <b>the benefits/disadvantages associated with word</b> error rate ...", "url": "https://www.quora.com/What-are-the-benefits-disadvantages-associated-with-word-error-rate-WER-compared-to-the-BLEU-bilingual-evaluation-understudy-metric", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>the-benefits-disadvantages-associated-with-word</b>-error...", "snippet": "Answer: WER tends to predicate a raw metric of how many updates must be performed - in terms of deletion, editing, etc. Where as of <b>BLEU</b> tends to measure the translation strength - in comparison to what would be judged as being \u201cfavored by people\u201d. However - I think both of them run into distin...", "dateLastCrawled": "2022-01-21T01:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) ASSAMESE-ENGLISH <b>BILINGUAL</b> MACHINE TRANSLATION | International ...", "url": "https://www.academia.edu/8801304/ASSAMESE_ENGLISH_BILINGUAL_MACHINE_TRANSLATION", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/8801304/ASSAMESE_ENGLISH_<b>BILINGUAL</b>_MACHINE_TRANSLATION", "snippet": "<b>BLEU</b> The <b>BLEU</b> (<b>Bi-Lingual</b> <b>Evaluation</b> <b>Understudy</b>) toolkit helps us to determine the quality of our translation. We <b>can</b> test how good our translation is by translating the text and then running the <b>BLEU</b> script on it. A simple <b>BLEU</b> scoring script is multi-<b>bleu</b>.perl [1][4]. 4. METHODOLOGY Statistical Machine Translation system requires parallel corpus of source and target <b>language</b> pair for translation of sentences. We have used about 2500 parallel English and Assamese sentences related to ...", "dateLastCrawled": "2022-01-31T22:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "natural <b>language</b> - Shouldn&#39;t ROUGE-1 precision be equal to <b>BLEU</b> with w ...", "url": "https://stats.stackexchange.com/questions/502064/shouldnt-rouge-1-precision-be-equal-to-bleu-with-w-1-0-0-0-when-brevity-pe", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/502064/shouldnt-rouge-1-precision-be-equal...", "snippet": "I <b>thought</b> in your case the BP might be not 1. The penalty is not one if ... <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) <b>BLEU</b>: a Method for Automatic <b>Evaluation</b> of Machine Translation; What is ROUGE and how it works for <b>evaluation</b> of summarization tasks? Lecture 15: Natural <b>language</b> generation; Share. Cite. Improve this answer. Follow edited Jan 16 &#39;21 at 6:50. answered Jan 16 &#39;21 at 6:15. Lerner Zhang Lerner Zhang. 4,875 1 1 gold badge 30 30 silver badges 52 52 bronze badges $\\endgroup$ 1 ...", "dateLastCrawled": "2022-01-23T03:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Convolutional Neural Network and RNN</b> for OCR problem.", "url": "https://www.slideshare.net/vishalmishra982/convolutional-neural-network-and-rnn-for-ocr-problem-86087045", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/vishalmishra982/<b>convolutional-neural-network-and-rnn</b>-for...", "snippet": "RESULTS \u2022 The proposed method is compared with the previous two methods called INFTY and WYGIWYS on the bases of <b>BLEU</b> (<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>) metric and Exact Match. <b>BLEU</b> is a metric to evaluate the quality for the predicted Latex markup representation of the image. Exact Match is the metric which represents the percentage of the images classified correctly. \u2022 It <b>can</b> be seen that the proposed method scores better than the previous methods. The proposed model generated results ...", "dateLastCrawled": "2022-01-17T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Subjective</b> Answer <b>Evaluation</b> Using Machine <b>Learning</b>", "url": "https://www.acadpubl.eu/hub/2018-118-24/3/577.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.acadpubl.eu/hub/2018-118-24/3/577.pdf", "snippet": "<b>Evaluation</b> of subject answer checking isnt <b>a new</b> <b>thought</b>. It has been in the works since a decade and a half. A large number of tech- niques where experimented with to solve the problem e ciently. Natural <b>Language</b> processing, Latent Semantic Analysis, General-ized Latent Semantic Analysis, Bayes theorem, K- nearest neighbor, etc. In general they <b>can</b> be categorized as follows : Clustering tech-niques, classi cation techniques and natural <b>language</b> processing techniques.Intelligent Essay ...", "dateLastCrawled": "2022-01-30T10:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching algorithm that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Machine Translation <b>Evaluation</b> with sacreBLEU and BERTScore | by Ng Wai ...", "url": "https://towardsdatascience.com/machine-translation-evaluation-with-sacrebleu-and-bertscore-d7fdb0c47eb3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/machine-translation-<b>evaluation</b>-with-sacre<b>bleu</b>-and-bert...", "snippet": "For your information, <b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is one of the most popular metric for evaluating machine-translated text. It <b>can</b> be used to evaluate translations of any <b>language</b> provided that there exists some form of word boundary in the text. <b>BLEU</b>\u2019s output is usually a score between 0 and 100, indicating the similarity value between the reference text and hypothesis text. The higher the value, the better the translations. Having said th a t, one of the major downside for ...", "dateLastCrawled": "2022-01-29T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise machine translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching algorithm that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Essential KPIs For Your SMT Engines \u2013 KantanMT \u2013 Machine <b>Learning</b> ...", "url": "https://kantanmtblog.com/2014/01/28/essential-kpis-for-your-smt-engines/", "isFamilyFriendly": true, "displayUrl": "https://kantanmtblog.com/2014/01/28/essential-kpis-for-your-smt-engines", "snippet": "<b>BLEU</b>. <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is a KPI that measures the fluency of the translated output of an SMT engine, which means it measures how many words overlap in a given translation when <b>compared</b> to a reference translation. Higher scores are given to segments which contain a greater number of sequential words. <b>BLEU</b> is a major improvement on F-Measure as it takes word-order in account! A high <b>BLEU</b> KPI means that an SMT engine is producing highly fluent translations; a low score ...", "dateLastCrawled": "2022-01-20T17:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "<b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical machine translation to make it more adjustable and robust . We in tend to adapt it to resemble human <b>evaluatio n</b> mor e.", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Google Introduces BLEURT \u2014 a BERT-Based NLG <b>Evaluation</b> Metric | by ...", "url": "https://synced.medium.com/google-introduces-bleurt-a-bert-based-nlg-evaluation-metric-a441debdfae6", "isFamilyFriendly": true, "displayUrl": "https://synced.medium.com/google-introduces-<b>bleu</b>rt-a-bert-based-nlg-<b>evaluation</b>-metric...", "snippet": "Another available <b>evaluation</b> method is automatic metrics such as the popular <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score, but these are oftentimes unreliable when <b>compared</b> to human <b>evaluation</b>. The challenge is to develop a novel and automatic metric that <b>can</b> be robust and reliable enough to match the <b>evaluation</b> quality that human annotation <b>can</b> deliver. In the recently published paper BLEURT: <b>Learning</b> Robust Metrics for Text Generation, a Google Research team proposes the automatic metric ...", "dateLastCrawled": "2022-01-09T08:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "This measure, looking at n-grams overlap between the output and reference translations with a penalty for shorter outputs, is known as <b>BLEU</b> (short for \u201c<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>\u201d which people literally only ever say when explaining the acronym) and was developed by Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu at IBM in 2002. It\u2019s a very popular metric in NLP, particularly for tasks where the output of a system is a text string rather than a classification. This ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Evaluation</b> of machine translation with predictive metrics beyond <b>BLEU</b> ...", "url": "https://aclanthology.org/2005.mtsummit-papers.16.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2005.mtsummit-papers.16.pdf", "snippet": "2.5.1 <b>BLEU</b>/NIST . <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) is a semi-automated metric tuned by the US National Institute of Standards (NIST) and first developed by IBM (Papineni et al., 2001). In simple terms, <b>BLEU</b> counts the number of word. n-grams in a sentence to be evaluated which are common with one or more reference translations. A ...", "dateLastCrawled": "2021-11-03T20:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) IMAGE CAPTION GENERATOR USING DEEP <b>LEARNING</b>-Convolutional Neural ...", "url": "https://www.researchgate.net/publication/357955678_IMAGE_CAPTION_GENERATOR_USING_DEEP_LEARNING-Convolutional_Neural_Network_Recurrent_Neural_Network_Bilingual_Evaluation_UnderstudyBLEU_scoreLong_Short_Time_Memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357955678_IMAGE_CAPTION_GENERATOR_USING_DEEP...", "snippet": "IMAGE CAPTION GENERATOR USING DEEP <b>LEARNING</b>-Convolutional Neural Network, Recurrent Neural Network, (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)<b>BLEU</b> score,Long Short Time Memory . January 2022; Authors: Ch ...", "dateLastCrawled": "2022-01-30T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are <b>the benefits/disadvantages associated with word</b> error rate ...", "url": "https://www.quora.com/What-are-the-benefits-disadvantages-associated-with-word-error-rate-WER-compared-to-the-BLEU-bilingual-evaluation-understudy-metric", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>the-benefits-disadvantages-associated-with-word</b>-error...", "snippet": "Answer: WER tends to predicate a raw metric of how many updates must be performed - in terms of deletion, editing, etc. Where as of <b>BLEU</b> tends to measure the translation strength - in comparison to what would be judged as being \u201cfavored by people\u201d. However - I think both of them run into distin...", "dateLastCrawled": "2022-01-21T01:28:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language <b>Evaluation</b> | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) #language. A score between 0.0 and 1.0, inclusive, indicating the quality of a translation between two human languages (for example, between English and Russian). A <b>BLEU</b> score of 1.0 indicates a perfect translation; a <b>BLEU</b> score of 0.0 indicates a terrible translation. C. causal language model. #language. Synonym for unidirectional language model. See bidirectional language model to contrast different directional approaches in language modeling. crash ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Evaluation</b> of an <b>NLP</b> model \u2014 latest benchmarks | by Ria Kulshrestha ...", "url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>evaluation</b>-of-an-<b>nlp</b>-model-latest-benchmarks-90fd8ce6fae5", "snippet": "<b>BLEU</b> Score \u2014 <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>. As the name suggests, it was originally used to evaluate translations from one language to another. How to calculate <b>BLEU</b> score? Calculating unigram precision: Step 1: Look at each word in the output sentence and assign it a score of 1 if it shows up in any of the reference sentences and 0 if it doesn\u2019t. Step 2: Normalize that count, so that it\u2019s always between 0 and 1, by dividing the number of words that showed up in one of the reference ...", "dateLastCrawled": "2022-01-28T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Natrual language processing basic concepts - language model - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "<b>BLEU</b> stands for <b>bilingual</b> <b>evaluation</b> <b>understudy</b>. It&#39;s an automatic metric to evaluate how close a sequence of text generated by a language model is to a reference. At first, it&#39;s used to evaluate the quality of <b>machine</b> translation text. Now other natural language processing tasks such as task-oriented dialogue generation adopt it as well. For a reference &quot;The man returned to the store&quot;, a generated text &quot;the the man the&quot; would get a BLUE score as below. For each word in the generated text ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) pathak2019.pdf | Aditya Kumar Pathak and Priyankit Acharya ...", "url": "https://www.academia.edu/38228943/pathak2019_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38228943/pathak2019_pdf", "snippet": "The standard metric people are using for <b>evaluation</b> of MT systems is <b>BLEU</b> score.<b>Bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) is the algorithm to determine the quality of text translated by a <b>machine</b> translation. Quality is the comparison between <b>machine</b>-translated output to that of human-generated output; the closer <b>machine</b> translation is to human-generated translation, the better is the <b>BLEU</b> score. <b>BLEU</b> score is a n-gram overlap of <b>machine</b> translation to that of reference translation.<b>BLEU</b> \u00bc min ...", "dateLastCrawled": "2021-02-16T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "9.7. <b>Sequence</b> to <b>Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>), though originally proposed for evaluating <b>machine</b> translation results [Papineni et al., 2002], has been extensively used in measuring the quality of output sequences for different applications.", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>Computational</b> Limits of Deep <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-computational-limits-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>computational</b>-limits-of-deep-<b>learning</b>", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) [papineni2002bleu] score is a metric for translation and computes the similarity between human translation and <b>machine</b> translation based on n-gram. An n-gram is a continuous sequence of n items from a given text. The score is based on precision, brevity penalty, and clipping. The modified n-gram precision ...", "dateLastCrawled": "2022-01-28T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Sequence Models - Deep <b>Learning</b> Specialization 5 - Yuet&#39;s Blog", "url": "https://yestinyang.github.io/2018/02/19/DLS-5-Sequence-Models.html", "isFamilyFriendly": true, "displayUrl": "https://yestinyang.github.io/2018/02/19/DLS-5-Sequence-Models.html", "snippet": "<b>Bleu</b> Score: <b>bilingual</b> <b>evaluation</b> <b>understudy</b>. Evaluate \u2018accuracy\u2019 of a model predicting multiply equally good answers, being a substitute for human evaluating each output Attention Model. Counter the problem of long sentence, which requires the ability of memory but not badly need a NN to do this kind of job. Instead of \u2018remembering\u2019 the ...", "dateLastCrawled": "2022-01-22T18:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Arti cial Intelligence Master Thesis", "url": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "snippet": "2:87 <b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>) score over the baseline; attention model, for German-English translation, and 0:34 <b>BLEU</b> score improvement for Catalan-Spanish trans-lation. Keywords <b>Machine</b> <b>Learning</b>, Deep <b>Learning</b>, Natural Language Processing, Neural <b>Machine</b> Transla-tion", "dateLastCrawled": "2021-12-27T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> translation from text to sign language: a systematic review ...", "url": "https://link.springer.com/article/10.1007/s10209-021-00823-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10209-021-00823-1", "snippet": "The SMT component of the system uses MOSES for generating a language model and decodes the input sentence. The approach uses the <b>BLEU</b> metric for <b>evaluation</b> and reports the scores as <b>BLEU</b>-4 12.64% <b>BLEU</b>-3 19.28% <b>BLEU</b>-2 31.48% <b>BLEU</b>-1 53.17%. The results reported are satisfactory; however, the system needs a virtual avatar tool for completeness.", "dateLastCrawled": "2022-01-30T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Evaluation of machine translation systems and related procedures</b>", "url": "https://www.researchgate.net/publication/326320090_Evaluation_of_machine_translation_systems_and_related_procedures", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/326320090_<b>Evaluation</b>_of_<b>machine</b>_translation...", "snippet": "<b>Evaluation of machine translation systems and related procedures</b>. June 2018 ; Journal of Engineering and Applied Sciences 13(12):3961-3972; Project: <b>Machine</b> <b>learning</b>; Authors: Musatafa Albadr ...", "dateLastCrawled": "2022-01-15T04:04:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bleu (bilingual evaluation understudy))  is like +(learning a new language)", "+(bleu (bilingual evaluation understudy)) is similar to +(learning a new language)", "+(bleu (bilingual evaluation understudy)) can be thought of as +(learning a new language)", "+(bleu (bilingual evaluation understudy)) can be compared to +(learning a new language)", "machine learning +(bleu (bilingual evaluation understudy) AND analogy)", "machine learning +(\"bleu (bilingual evaluation understudy) is like\")", "machine learning +(\"bleu (bilingual evaluation understudy) is similar\")", "machine learning +(\"just as bleu (bilingual evaluation understudy)\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be thought of as\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be compared to\")"]}