{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Configure the <b>Learning</b> <b>Rate</b> When Training Deep <b>Learning</b> Neural ...", "url": "https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>learning</b>-<b>rate</b>-for-deep-<b>learning</b>-neural-networks", "snippet": "The amount of change to the model during each <b>step</b> of this search process, or the <b>step</b> <b>size</b>, is called the \u201c<b>learning</b> <b>rate</b>\u201d and provides perhaps the most important hyperparameter to tune for your neural network in order to achieve good performance on your problem. In this tutorial, you will discover the <b>learning</b> <b>rate</b> hyperparameter used when training deep <b>learning</b> neural networks. After completing this tutorial, you will know: <b>Learning</b> <b>rate</b> controls how quickly or slowly a neural network ...", "dateLastCrawled": "2022-02-02T08:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Gradient Descent \u2014 One Step at</b> a Time | by Asad Mumtaz | Towards Data ...", "url": "https://towardsdatascience.com/gradient-descent-one-step-at-a-time-3c39a3642333", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent-one-step-at</b>-a-time-3c39a3642333", "snippet": "The <b>step</b> <b>size</b> is regulated by a parameter called the <b>learning</b> <b>rate</b>. The <b>step</b> <b>size</b> then determines the new intercept to be used by the GD to calculate RSS: <b>step</b> <b>size</b> = slope * <b>learning</b> <b>rate</b> new intercept = old intercept - <b>step</b> <b>size</b>. The <b>learning</b> <b>rate</b> is set to a small number, usually 0.2, 0.1, or 0.01 in practice.", "dateLastCrawled": "2022-01-29T18:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Gentle Introduction to the Adam Optimization Algorithm for Deep <b>Learning</b>", "url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/adam-optimization-algorithm-for-deep-<b>learning</b>", "snippet": "Also referred to as the <b>learning</b> <b>rate</b> or <b>step</b> <b>size</b>. The proportion that weights are updated (e.g. 0.001). Larger values (e.g. 0.3) results in faster initial <b>learning</b> before the <b>rate</b> is updated. Smaller values (e.g. 1.0E-5) slow <b>learning</b> right down during training; beta1. The exponential decay <b>rate</b> for the first moment estimates (e.g. 0.9). beta2. The exponential decay <b>rate</b> for the second-moment estimates (e.g. 0.999). This value should be set close to 1.0 on problems with a sparse gradient ...", "dateLastCrawled": "2022-02-03T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Artificial Neural Network - Quick Guide</b> - <b>Tutorialspoint</b>", "url": "https://www.tutorialspoint.com/artificial_neural_network/artificial_neural_network_quick_guide.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/artificial_neural_network/artificial_neural_network...", "snippet": "<b>Step</b> 7 \u2212 Update the <b>learning</b> <b>rate</b> ... Here, \u03b8 &gt; 0 is the training <b>rate</b> (<b>step</b> <b>size</b>) that forces the algorithm to take small jumps. Estimating <b>Step</b> <b>Size</b>. Actually a wrong <b>step</b> <b>size</b> \u03b8 may not reach convergence, hence a careful selection of the same is very important. Following points must have to be remembered while choosing the <b>step</b> <b>size</b>. Do not choose too large <b>step</b> <b>size</b>, otherwise it will have a negative impact, i.e. it will diverge rather than converge. Do not choose too small <b>step</b> <b>size</b> ...", "dateLastCrawled": "2022-02-02T11:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "scikit-learn 1.0.2 documentation - scikit-learn: machine <b>learning</b> in Python", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/gene<b>rate</b>d/sklearn.neural_network.MLPClassifier...", "snippet": "The initial <b>learning</b> <b>rate</b> used. It controls the <b>step</b>-<b>size</b> in updating the weights. Only used when solver=\u2019sgd\u2019 or \u2018adam\u2019. power_t float, default=0.5. The exponent for inverse scaling <b>learning</b> <b>rate</b>. It is used in updating effective <b>learning</b> <b>rate</b> when the <b>learning</b>_<b>rate</b> is set to \u2018invscaling\u2019. Only used when solver=\u2019sgd\u2019. max_iter int, default=200. Maximum number of iterations. The solver iterates until convergence (determined by \u2018tol\u2019) or this number of iterations. For ...", "dateLastCrawled": "2022-02-03T06:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Hyper-parameter Tuning Techniques in <b>Deep Learning</b> | by Javaid Nabi ...", "url": "https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-<b>deep-learning</b>-4dad...", "snippet": "Here cyclical momentum is not better than a good constant value.If a constant <b>learning</b> <b>rate</b> is used then a large constant momentum (i.e., 0.9\u20130.99) will act <b>like</b> a pseudo increasing <b>learning</b> <b>rate</b> and will speed up the training. However, use of too large a value for momentum causes poor training results that are visible early in the training and this can be quickly tested.", "dateLastCrawled": "2022-01-30T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fundamentals of Neural Networks on Weights &amp; Biases", "url": "https://wandb.ai/site/articles/fundamentals-of-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://wandb.ai/site/articles/fundamentals-of-neural-networks", "snippet": "Picking the <b>learning</b> <b>rate</b> is very important, and you want to make sure you get this right! Ideally you want to re-tweak the <b>learning</b> <b>rate</b> when you tweak the other hyper-parameters of your network. To find the best <b>learning</b> <b>rate</b>, start with a very low values (10^-6) and slowly multiply it by a constant until it reaches a very high value (e.g. 10).", "dateLastCrawled": "2022-01-30T01:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Residual Networks (ResNet) - Deep <b>Learning</b> - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/residual-networks-resnet-deep-<b>learning</b>", "snippet": "In this <b>step</b>, we set the <b>learning</b> <b>rate</b> according to the number of epochs. As the number of epochs the <b>learning</b> <b>rate</b> must be decreased to ensure better <b>learning</b>. Code: Setting LR for different number of Epochs", "dateLastCrawled": "2022-02-02T16:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "torch.optim \u2014 <b>PyTorch</b> 1.10 documentation", "url": "https://pytorch.org/docs/stable/optim.html", "isFamilyFriendly": true, "displayUrl": "https://<b>pytorch</b>.org/docs/stable/optim.html", "snippet": "Decays the <b>learning</b> <b>rate</b> of each parameter group by gamma every <b>step</b>_<b>size</b> epochs. lr_scheduler.MultiStepLR. Decays the <b>learning</b> <b>rate</b> of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ConstantLR. Decays the <b>learning</b> <b>rate</b> of each parameter group by a small constant factor until the number of epoch reaches a pre-defined milestone: total_iters. lr_scheduler.LinearLR. Decays the <b>learning</b> <b>rate</b> of each parameter group by linearly changing small ...", "dateLastCrawled": "2022-02-03T01:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the trade-off between <b>batch size</b> and number of iterations to ...", "url": "https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/164876", "snippet": "The <b>size</b> of the <b>learning</b> <b>rate</b> is limited mostly by factors <b>like</b> how curved the cost function is. You can think of gradient descent as making a linear approximation to the cost function, then moving downhill along that approximate cost. If the cost function is highly non-linear (highly curved) then the approximation will not be very good for very far, so only small <b>step</b> sizes are safe. You can read more about this in Chapter 4 of the deep <b>learning</b> textbook, on numerical computation: http ...", "dateLastCrawled": "2022-01-27T19:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understand <b>the Impact of Learning Rate</b> on Neural Network Performance", "url": "https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/understand-the-dynamics-of-<b>learning</b>-<b>rate</b>-on-deep...", "snippet": "The amount that the weights are updated during training is referred to as the <b>step</b> <b>size</b> or the \u201c<b>learning</b> <b>rate</b>. \u201d Specifically, the <b>learning</b> <b>rate</b> is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The <b>learning</b> <b>rate</b> controls how quickly the model is adapted to the problem. Smaller <b>learning</b> rates require more training epochs given the smaller changes made to the weights each update, whereas larger ...", "dateLastCrawled": "2022-02-02T18:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning Rate Scheduling</b> - Deep <b>Learning</b> Wizard", "url": "https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/", "isFamilyFriendly": true, "displayUrl": "https://www.deep<b>learning</b>wizard.com/deep_<b>learning</b>/boosting_models_pytorch/lr_scheduling", "snippet": "SGD (model. parameters (), lr = <b>learning</b>_<b>rate</b>, momentum = 0.9, nesterov = True) &#39;&#39;&#39; <b>STEP</b> 7: INSTANTIATE <b>STEP</b> <b>LEARNING</b> SCHEDULER CLASS &#39;&#39;&#39; # <b>step</b>_<b>size</b>: at how many multiples of epoch you decay # <b>step</b>_<b>size</b> = 1, after every 2 epoch, new_lr = lr*gamma # <b>step</b>_<b>size</b> = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR (optimizer, <b>step</b>_<b>size</b> = 2, gamma = 0.1) &#39;&#39;&#39; <b>STEP</b> 7: TRAIN THE MODEL &#39;&#39;&#39; iter = 0 for epoch in range (num_epochs): # Decay <b>Learning</b> <b>Rate</b> scheduler ...", "dateLastCrawled": "2022-01-30T20:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning Rate</b> Schedule in Practice: an example with Keras and ...", "url": "https://towardsdatascience.com/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>learning-rate</b>-schedule-in-practice-an-example-with...", "snippet": "<b>Step</b>-based decay \u2014 <b>learning rate</b> 4. Exponential decay. Another popular <b>learning rate</b> schedule is to drop the <b>learning rate</b> at an exponential <b>rate</b>. Formally, it is defined as: <b>learning_rate</b> = initial_lr * e^(\u2212k * epoch) Where initial_lr is the initial <b>learning rate</b> such as 0.01, k is a hyperparameter, and epoch is the current", "dateLastCrawled": "2022-02-03T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Gentle Introduction to the Adam Optimization Algorithm for Deep <b>Learning</b>", "url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/adam-optimization-algorithm-for-deep-<b>learning</b>", "snippet": "Also referred to as the <b>learning</b> <b>rate</b> or <b>step</b> <b>size</b>. The proportion that weights are updated (e.g. 0.001). Larger values (e.g. 0.3) results in faster initial <b>learning</b> before the <b>rate</b> is updated. Smaller values (e.g. 1.0E-5) slow <b>learning</b> right down during training; beta1. The exponential decay <b>rate</b> for the first moment estimates (e.g. 0.9). beta2. The exponential decay <b>rate</b> for the second-moment estimates (e.g. 0.999). This value should be set close to 1.0 on problems with a sparse gradient ...", "dateLastCrawled": "2022-02-03T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Gradient Descent, the <b>Learning Rate</b>, and the importance of Feature ...", "url": "https://towardsdatascience.com/gradient-descent-the-learning-rate-and-the-importance-of-feature-scaling-6c0b416596e1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/gradient-descent-the-<b>learning-rate</b>-and-the-importance...", "snippet": "Loss surface. In the center of the plot, where parameters (b, w) have values close to (1, 2), the loss is at its minimum value.This is the point we\u2019re trying to reach using gradient descent. In the bottom, slightly to the left, there is the random start point, corresponding to our randomly initialized parameters (b = 0.49 and w = -0.13).. This is one of the nice things about tackling a simple problem like a linear regression with a single feature: we have only two parameters, and thus we ...", "dateLastCrawled": "2022-02-02T23:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Optimisation Techniques I \u00b7 <b>Deep Learning</b>", "url": "https://atcold.github.io/pytorch-Deep-Learning/en/week05/05-1/", "isFamilyFriendly": true, "displayUrl": "https://atcold.github.io/pytorch-<b>Deep-Learning</b>/en/week05/05-1", "snippet": "The $\\gamma$ parameter in the iterative update equation is called the <b>step</b> <b>size</b>. Generally we don\u2019t know the value of the optimal <b>step</b>-<b>size</b>; so we have to try different values. Standard practice is to try a bunch of values on a log-scale and then use the best one. There are a few different scenarios that can occur. The image above depicts these scenarios for a 1D quadratic. If the <b>learning</b> <b>rate</b> is too low, then we would make steady progress towards the minimum. However, this might take ...", "dateLastCrawled": "2022-01-29T13:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Optimizers</b> - Keras: the Python deep <b>learning</b> API", "url": "https://keras.io/api/optimizers/", "isFamilyFriendly": true, "displayUrl": "https://keras.io/api/<b>optimizers</b>", "snippet": "Adam (<b>learning</b>_<b>rate</b> = 0.01) model. compile (loss = &#39;categorical_crossentropy&#39;, optimizer = opt) You can either instantiate an optimizer before passing it to model.compile(), as in the above example, or you can pass it by its string identifier. In the latter case, the default parameters for the optimizer will be used. # pass optimizer by name: default parameters will be used model. compile (loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;) Usage in a custom training loop. When writing a ...", "dateLastCrawled": "2022-02-02T23:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "neural-networks-and-deep-<b>learning</b>/Deep Neural Network Application v3.py ...", "url": "https://github.com/fanghao6666/neural-networks-and-deep-learning/blob/master/py/Deep%20Neural%20Network%20Application%20v3.py", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/fanghao6666/neural-networks-and-deep-<b>learning</b>/blob/master/py/Deep...", "snippet": "layers_dims -- list containing the input <b>size</b> and each layer <b>size</b>, of length (number of layers + 1). <b>learning</b>_<b>rate</b> -- <b>learning</b> <b>rate</b> of the gradient descent update rule: num_iterations -- number of iterations of the optimization loop: print_cost -- if True, it prints the cost every 100 steps: Returns: parameters -- parameters learnt by the model ...", "dateLastCrawled": "2022-02-02T12:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Options for training deep <b>learning</b> neural network - MATLAB <b>trainingOptions</b>", "url": "https://www.mathworks.com/help/deeplearning/ref/trainingoptions.html", "isFamilyFriendly": true, "displayUrl": "https://www.mathworks.com/help/deep<b>learning</b>/ref/<b>trainingoptions</b>.html", "snippet": "Specify <b>Training Options</b>. Open Live Script. Create a set of options for training a network using stochastic gradient descent with momentum. Reduce the <b>learning</b> <b>rate</b> by a factor of 0.2 every 5 epochs. Set the maximum number of epochs for training to 20, and use a mini-batch with 64 observations at each iteration.", "dateLastCrawled": "2022-02-02T18:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Top <b>Deep Learning Interview Questions</b> &amp; Answers for 2022 | Simplilearn", "url": "https://www.simplilearn.com/tutorials/deep-learning-tutorial/deep-learning-interview-questions", "isFamilyFriendly": true, "displayUrl": "https://www.simplilearn.com/tutorials/deep-<b>learning</b>-tutorial/deep-<b>learning</b>-interview...", "snippet": "The Best Introduction to Deep <b>Learning</b> - A <b>Step</b> by <b>Step</b> Guide Lesson - 2. Top 10 Deep <b>Learning</b> Applications Used Across Industries Lesson - 3. What is Neural Network: Overview, Applications, and Advantages Lesson - 4. Neural Networks Tutorial Lesson - 5. Top 8 Deep <b>Learning</b> Frameworks Lesson - 6. Top 10 Deep <b>Learning</b> Algorithms You Should Know ...", "dateLastCrawled": "2022-02-02T12:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Visualizing Learning rate vs Batch</b> <b>size</b> - GitHub Pages", "url": "https://miguel-data-sc.github.io/2017-11-05-first/", "isFamilyFriendly": true, "displayUrl": "https://miguel-data-sc.github.io/2017-11-05-first", "snippet": "Aditionally to realizing this I also <b>thought</b> about how insightful it would be to use the tool to visualize the famous relationship between <b>learning</b> <b>rate</b> and batch <b>size</b>. For the ones unaware, general rule is \u201cbigger batch <b>size</b> bigger <b>learning</b> <b>rate</b>\u201d. This is just logical because bigger batch <b>size</b> means more confidence in the direction of your ...", "dateLastCrawled": "2022-02-01T00:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding <b>Backpropagation</b>", "url": "https://blog.quantinsti.com/backpropagation/", "isFamilyFriendly": true, "displayUrl": "https://blog.quantinsti.com/<b>backpropagation</b>", "snippet": "The <b>learning</b> <b>rate</b> controls the <b>step</b>-<b>size</b> of the movement towards the minima. Intuitively, if we have a large <b>learning</b> <b>rate</b> we are going to take big steps. In contrast, if we have a small <b>learning</b> <b>rate</b>, we are going to take small steps. Thus, the <b>learning</b> <b>rate</b> multiplied by the derivative <b>can</b> <b>be thought</b> of as steps being taken over the domain of our Loss function. Once we make this <b>step</b>, we update our weights. And this process is repeated for each feature.", "dateLastCrawled": "2022-01-30T11:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Linear Classification and Perceptron</b>", "url": "https://cmci.colorado.edu/classes/INFO-4604/files/slides-3_perceptron.pdf", "isFamilyFriendly": true, "displayUrl": "https://cmci.colorado.edu/classes/INFO-4604/files/slides-3_perceptron.pdf", "snippet": "<b>Learning</b> <b>Rate</b> Let\u2019s make a modification to the update rule: w j+= \u03b7 (y i\u2013f(x i)) x ij where \u03b7is called the <b>learning</b> rateor <b>step</b> <b>size</b>. \u2022When you update w jto be more positive or negative, this controls the <b>size</b> of the change you make (or, how large a \u201c<b>step</b>\u201d you take). \u2022If \u03b7=1 (a common value), then this is the same update rule from the earlier slide. <b>Learning</b> <b>Rate</b> How to choose the <b>step</b> <b>size</b>? \u2022If \u03b7is too small, the algorithm will be slow because the updates won\u2019t make much ...", "dateLastCrawled": "2022-02-02T03:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How <b>to Implement Gradient Descent Optimization from Scratch</b>", "url": "https://machinelearningmastery.com/gradient-descent-optimization-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>gradient-descent-optimization-from-scratch</b>", "snippet": "These two quick examples highlight the problems in selecting a <b>step</b> <b>size</b> that is too large or too small and the general importance of testing many different <b>step</b> <b>size</b> values for a given objective function. Finally, we <b>can</b> change the <b>learning</b> <b>rate</b> back to 0.1 and visualize the progress of the search on a plot of the target function.", "dateLastCrawled": "2022-02-02T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Stochastic gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Stochastic_gradient_descent</b>", "snippet": "where is a <b>step</b> <b>size</b> (sometimes called the <b>learning</b> <b>rate</b> in machine <b>learning</b>). In many cases, the summand functions have a simple form that enables inexpensive evaluations of the sum-function and the sum gradient. For example, in statistics, one-parameter exponential families allow economical function-evaluations and gradient-evaluations. However, in other cases, evaluating the sum-gradient may require expensive evaluations of the gradients from all summand functions. When the training set ...", "dateLastCrawled": "2022-02-02T23:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How <b>to Choose an Optimization Algorithm</b> - Machine <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/tour-of-optimization-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/tour-of-optimization-algorithms", "snippet": "The <b>step</b> <b>size</b> is a hyperparameter that controls how far to move in the search space, unlike \u201clocal descent algorithms\u201d that perform a full line search for each directional move. A <b>step</b> <b>size</b> that is too small results in a search that takes a long time and <b>can</b> get stuck, whereas a <b>step</b> <b>size</b> that is too large will result in zig-zagging or bouncing around the search space, missing the optima completely.", "dateLastCrawled": "2022-02-03T03:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Research in brain function and <b>learning</b>", "url": "https://www.apa.org/education-career/k12/brain-function", "isFamilyFriendly": true, "displayUrl": "https://<b>www.apa.org</b>/education-career/k12/brain-function", "snippet": "Don&#39;t adopt a one-<b>size</b>-fits-all approach. Experienced teachers vary skills and activities for different students within a grade. Some of this variability works because of the different life experiences of children and some works because of differences in brain maturity. But, for either reason, variety is a good thing. Don&#39;t place children in groups based solely on age. For some children, <b>learning</b> to read is a struggle. Many are not ready to learn to read until they are seven years old, while ...", "dateLastCrawled": "2022-02-03T01:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "Explanation: In <b>step</b> 2, the maximization <b>step</b>, the new counts of bases or amino acids for each position in the site found in <b>step</b> 1 are substituted for the previous set. <b>Step</b> 1 is then repeated using these new counts. The cycle is repeated until the algorithm converges on a solution and does not change with further cycles. At that time, the best location of the site in each sequence and the best estimate of the residue composition of each column in the site will be available.", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Everything you need to know about Adam <b>Optimizer</b> | by Nishant ... - Medium", "url": "https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@nishantnikhil/adam-<b>optimizer</b>-notes-ddac4fd7218", "snippet": "Everything you need to know about Adam <b>Optimizer</b>. Nishant Nikhil. Jan 26, 2017 \u00b7 3 min read. Paper : Adam: A Method for Stochastic Optimization. This is used to perform optimization and is one of ...", "dateLastCrawled": "2022-01-31T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "neural-networks-and-deep-<b>learning</b>/Logistic Regression with a Neural ...", "url": "https://github.com/fanghao6666/neural-networks-and-deep-learning/blob/master/py/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset%20v3.py", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/fanghao6666/neural-networks-and-deep-<b>learning</b>/blob/master/py...", "snippet": "# One common preprocessing <b>step</b> in machine <b>learning</b> is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel). # # &lt;!-- During the training of your model, you&#39;re going to multiply ...", "dateLastCrawled": "2022-02-02T04:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Learning Rate</b> Schedules and Adaptive <b>Learning Rate</b> Methods for Deep ...", "url": "https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>learning-rate</b>-schedules-and-adaptive-<b>learning-rate</b>...", "snippet": "<b>Step</b> decay schedule drops the <b>learning rate</b> by a factor every few epochs. The mathematical form of <b>step</b> decay is : lr = lr0 * drop^floor(epoch / epochs_drop) A typical way is to to drop the <b>learning rate</b> by half every 10 epochs. To implement this in Keras, we <b>can</b> define a <b>step</b> decay function and use LearningRateScheduler callback to take the <b>step</b> decay function as argument and return the updated <b>learning</b> rates for use in SGD optimizer. def <b>step</b>_decay(epoch): initial_lrate = 0.1 drop = 0.5 ...", "dateLastCrawled": "2022-01-28T23:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "python - Gradient descent explodes if <b>learning rate</b> is too large ...", "url": "https://stats.stackexchange.com/questions/315664/gradient-descent-explodes-if-learning-rate-is-too-large", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/315664/gradient-descent-explodes-if-<b>learning</b>...", "snippet": "The <b>learning rate</b> <b>can</b> seen as <b>step</b> <b>size</b>, \u03b7. As such, gradient descent is taking successive steps in the direction of the minimum. If the <b>step</b> <b>size</b> \u03b7 is too large, it <b>can</b> (plausibly) &quot;jump over&quot; the minima we are trying to reach, ie. we overshoot. This <b>can</b> lead to osculations around the minimum or in some cases to outright divergence.", "dateLastCrawled": "2022-01-24T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>Gradient Descent</b>? | <b>IBM</b>", "url": "https://www.ibm.com/cloud/learn/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ibm.com</b>/cloud/learn/<b>gradient-descent</b>", "snippet": "<b>Learning</b> <b>rate</b> (also referred to as <b>step</b> <b>size</b> or the alpha) is the <b>size</b> of the steps that are taken to reach the minimum. This is typically a small value, and it is evaluated and updated based on the behavior of the cost function. High <b>learning</b> rates result in larger steps but risks overshooting the minimum. Conversely, a low <b>learning</b> <b>rate</b> has small <b>step</b> sizes. While it has the advantage of more precision, the number of iterations compromises overall efficiency as this takes more time and ...", "dateLastCrawled": "2022-02-02T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Gentle Introduction to the Adam Optimization Algorithm for Deep <b>Learning</b>", "url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/adam-optimization-algorithm-for-deep-<b>learning</b>", "snippet": "Also referred to as the <b>learning</b> <b>rate</b> or <b>step</b> <b>size</b>. The proportion that weights are updated (e.g. 0.001). Larger values (e.g. 0.3) results in faster initial <b>learning</b> before the <b>rate</b> is updated. Smaller values (e.g. 1.0E-5) slow <b>learning</b> right down during training; beta1. The exponential decay <b>rate</b> for the first moment estimates (e.g. 0.9). beta2. The exponential decay <b>rate</b> for the second-moment estimates (e.g. 0.999). This value should be set close to 1.0 on problems with a sparse gradient ...", "dateLastCrawled": "2022-02-03T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Momentum and <b>Learning</b> <b>Rate</b> Adaptation - CNL", "url": "https://cnl.salk.edu/~schraudo/teach/NNcourse/momrate.html", "isFamilyFriendly": true, "displayUrl": "https://cnl.salk.edu/~schraudo/teach/NNcourse/mom<b>rate</b>.html", "snippet": "<b>Learning</b> <b>Rate</b> Adaptation In the section on preconditioning, we have employed simple heuristics to arrive at reasonable guesses for the global and local <b>learning</b> rates.It is possible to refine these values significantly once training has commenced, and the network&#39;s response to the data <b>can</b> be observed.", "dateLastCrawled": "2022-01-31T21:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Deep Learning Techniques: An Overview</b>", "url": "https://www.researchgate.net/publication/341652370_Deep_Learning_Techniques_An_Overview", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/341652370_<b>Deep_Learning_Techniques_An_Overview</b>", "snippet": "<b>step</b> <b>size</b>, it may arrive at the ... The most common adaptations of <b>learning</b> <b>rate</b> during training include techniques to reduce the <b>learning</b> <b>rate</b> over time [145], referred to as <b>learning</b> <b>rate</b> decay ...", "dateLastCrawled": "2022-01-31T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Professionals Point: <b>Difference</b> between GBM (<b>Gradient Boosting</b> ...", "url": "https://theprofessionalspoint.blogspot.com/2019/02/difference-between-gbm-gradient.html", "isFamilyFriendly": true, "displayUrl": "https://theprofessionalspoint.blogspot.com/2019/02/<b>difference</b>-between-gbm-gradient.html", "snippet": "By using gradient descent and updating our predictions based on a <b>learning</b> <b>rate</b> (the \u201c<b>step</b> <b>size</b>\u201d with which we descend the gradient), we <b>can</b> find the values where loss function is minimum. So, we are basically updating the predictions such that the sum of our residuals is close to 0 (or minimum) and predicted values are sufficiently close to actual values. XGBoost (Extreme <b>Gradient Boosting</b>) XGBoost stands for Extreme <b>Gradient Boosting</b>. XGBoost is a specific implementation of the ...", "dateLastCrawled": "2022-02-02T09:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is the trade-off between <b>batch size</b> and number of iterations to ...", "url": "https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/164876", "snippet": "The <b>size</b> of the <b>learning</b> <b>rate</b> is limited mostly by factors like how curved the cost function is. You <b>can</b> think of gradient descent as making a linear approximation to the cost function, then moving downhill along that approximate cost. If the cost function is highly non-linear (highly curved) then the approximation will not be very good for very far, so only small <b>step</b> sizes are safe. You <b>can</b> read more about this in Chapter 4 of the deep <b>learning</b> textbook, on numerical computation: http ...", "dateLastCrawled": "2022-01-27T19:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Reinforcement Learning Tutorial</b>: Semi-gradient n-<b>step</b> Sarsa and Sarsa(\u03bb ...", "url": "https://michaeloneill.github.io/RL-tutorial.html", "isFamilyFriendly": true, "displayUrl": "https://michaeloneill.github.io/RL-tutorial.html", "snippet": "<b>step</b>_<b>size</b> = 0.5 # Fraction of the way we want to move towards target lmbda = 0.92 # Level of bootstrapping (set to intermediate value) num_episodes = 500 estimator_lambda = QEstimator (<b>step</b>_<b>size</b> = <b>step</b>_<b>size</b>, trace = True) start_time = timeit. default_timer run_stats_lambda = run (&#39;sarsa_lambda&#39;, num_episodes, lmbda = lmbda, env = env, estimator = estimator_lambda) elapsed_time = timeit. default_timer ()-start_time plot_cost_to_go (env, estimator_lambda) print (&#39; {} episodes completed in ...", "dateLastCrawled": "2022-02-03T08:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Gentle Introduction to <b>Mini-Batch Gradient Descent</b> and How to ...", "url": "https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/gentle-introduction-<b>mini-batch-gradient-descent</b>...", "snippet": "Tip 3: Tune batch <b>size</b> and <b>learning</b> <b>rate</b> after tuning all other hyperparameters. \u2026 [batch <b>size</b>] and [<b>learning</b> <b>rate</b>] may slightly interact with other hyper-parameters so both should be re-optimized at the end. Once [batch <b>size</b>] is selected, it <b>can</b> generally be fixed while the other hyper-parameters <b>can</b> be further optimized (except for a ...", "dateLastCrawled": "2022-02-03T00:44:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Understanding Machine Learning by Analogy</b> with a Simple Contour Map ...", "url": "https://contemplations.blog/machine-learning-analogy-countour-map/", "isFamilyFriendly": true, "displayUrl": "https://<b>contemplations</b>.blog/<b>machine</b>-<b>learning</b>-<b>analogy</b>-countour-map", "snippet": "The Basis for <b>Machine</b> <b>Learning</b> by <b>Analogy</b>, Using a Contour Map. In this post, we will take a closer look at <b>Machine</b> <b>Learning</b> and its nephew, Deep <b>Learning</b>. There is no \u201c<b>Learning</b>\u201d (in the human sense) in either <b>Machine</b> <b>learning</b> or Deep <b>Learning</b>, there are only quite simple and readily available mathematical procedures which allow us to adapt parameters of many kinds of parameterized systems (or networks), such as a neural network, in such a way that the system (or network), together with ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Human learning as an analogy of machine learning</b> - Weina Jin, MD", "url": "https://weina.me/ml-vs-human-learning/", "isFamilyFriendly": true, "displayUrl": "https://weina.me/ml-vs-human-<b>learning</b>", "snippet": "As an <b>analogy</b>, for researchers, in the context that the <b>size</b> of the dataset can not be extended, the only prior that researchers can work on is the model architecture. Then the rest of the question becomes, how to design the <b>learning</b> environment (model architecture) that could best facilitate the desired <b>learning</b> outcome (better performance on CV tasks). I guess that could be the intuition to my question of why bother to design sophisticated CNN architecture.", "dateLastCrawled": "2020-07-13T08:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Preliminary performance study of a brief review on <b>machine</b> <b>learning</b> ...", "url": "https://link.springer.com/article/10.1007/s12652-021-03427-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12652-021-03427-y", "snippet": "By <b>analogy</b>, <b>machine</b> <b>learning</b> techniques such as obscure logic, gray matter theory, genetic algorithm, and subvector machines are used to improve predictions. Based on these similarities, we will explore software effort evaluation approaches and accurately compare some of the technologies commonly used in terms of <b>size</b>. Shashank et al. (Satapathy and Rath", "dateLastCrawled": "2022-01-02T12:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "What are the <b>basic concepts in machine learning</b>? I found that the best way to discover and get a handle on the <b>basic concepts in machine learning</b> is to review the introduction chapters to <b>machine learning</b> textbooks and to watch the videos from the first model in online courses. Pedro Domingos is a lecturer and professor on <b>machine learning</b> at the University of Washing and", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent With Momentum from Scratch</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>gradient-descent-with-momentum-from-scratch</b>", "snippet": "<b>Step</b> <b>Size</b> (alpha): Hyperparameter that controls how far to move in the search space against the gradient each iteration of the algorithm, also called the <b>learning</b> rate. If the <b>step</b> <b>size</b> is too small, the movement in the search space will be small and the search will take a long time. If the <b>step</b> <b>size</b> is too large, the search may bounce around ...", "dateLastCrawled": "2022-01-26T05:41:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(step size)  is like +(learning rate)", "+(step size) is similar to +(learning rate)", "+(step size) can be thought of as +(learning rate)", "+(step size) can be compared to +(learning rate)", "machine learning +(step size AND analogy)", "machine learning +(\"step size is like\")", "machine learning +(\"step size is similar\")", "machine learning +(\"just as step size\")", "machine learning +(\"step size can be thought of as\")", "machine learning +(\"step size can be compared to\")"]}