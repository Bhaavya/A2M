{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding <b>Representation</b> Learning With Autoencoder: Everything You ...", "url": "https://neptune.ai/blog/understanding-representation-learning-with-autoencoder-everything-you-need-to-know-about-representation-and-feature-learning", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/understanding-<b>representation</b>-learning-with-auto<b>encoder</b>...", "snippet": "By constructing our <b>encoder</b> model to output a range of possible values from which we\u2019ll randomly sample to feed <b>into</b> our decoder model, we\u2019re essentially enforcing a continuous, smooth latent space <b>representation</b>. For any sampling of the latent distributions, we\u2019re expecting our decoder model to be able to accurately reconstruct the <b>input</b>. Thus, values which are nearby to one another in latent space should correspond with very similar reconstructions.", "dateLastCrawled": "2022-02-03T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A PREPRINT", "url": "https://www.cfilt.iitb.ac.in/resources/surveys/manas_lit_survey_natural-answering.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cfilt.iitb.ac.in/resources/surveys/manas_lit_survey_natural-answering.pdf", "snippet": "\u2013Hidden State: This is a <b>black</b> <b>box</b> step, where <b>encoded</b> <b>input</b> is transformed to produce output of same length as the <b>input</b>. Simply length of vector produced by hidden state is same as the length of the column of embedding matrix. Number of hidden steps varies as per the suggested model. \u2013Decoder State: Decoder step reverses the process done by the <b>encoder</b> step and generat word on the basis of its embedding. 4.2 Neural Attention model of summarization It is also called as Attention Based ...", "dateLastCrawled": "2022-01-12T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Generating <b>Black</b>-<b>Box</b> Adversarial Examples for Text Classifiers Using a ...", "url": "https://deepai.org/publication/generating-black-box-adversarial-examples-for-text-classifiers-using-a-deep-reinforced-model", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/generating-<b>black</b>-<b>box</b>-adversarial-examples-for-text...", "snippet": "The <b>encoder</b> <b>transforms</b> <b>an input</b> text sequence <b>into</b> an abstract <b>representation</b> h. While the decoder is employed to generate the target sequence using the <b>encoded</b> <b>representation</b> h . However, there are several studies that have incorporated several modifications to the standard <b>encoder</b>-decoder framework [ bahdanau2014neural , luong2016achieving , luong2014addressing ] .", "dateLastCrawled": "2021-12-24T14:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>Illustrated Transformer</b> \u2013 Jay Alammar \u2013 Visualizing machine ...", "url": "https://jalammar.github.io/illustrated-transformer/", "isFamilyFriendly": true, "displayUrl": "https://jalammar.github.io/<b>illustrated-transformer</b>", "snippet": "Let\u2019s begin by looking at the model as a single <b>black</b> <b>box</b>. In a machine translation application, it would take a sentence in one language, and output its translation in another. Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them. The encoding component is a stack of encoders (the paper stacks six of them on top of each other \u2013 there\u2019s nothing magical about the number six, one can definitely experiment with other ...", "dateLastCrawled": "2022-02-03T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>LSTM-Based Neural Network Architecture to infer</b> Model Transformations", "url": "https://modeling-languages.com/lstm-neural-network-model-transformations/", "isFamilyFriendly": true, "displayUrl": "https://modeling-languages.com/lstm-neural-network-model-transformations", "snippet": "As a consequence, apart from the <b>encoder</b> and decoder, we need a layer to embed the <b>input</b> tree (representing the model after the preprocessing phase) <b>into</b> a format readable for the <b>encoder</b>. This <b>input</b> tree embedding layer is used to transform our <b>input</b> models in their tree-form <b>into</b> the numeric vectors which are the <b>input</b> to the LSTM <b>encoder</b>. We also need an output layer <b>that takes</b> the numeric vectors produced by the decoder and obtains the predicted (transformed) output model in its tree ...", "dateLastCrawled": "2022-02-02T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Tutorial 1: Un/Self-supervised learning methods \u2014 Neuromatch Academy ...", "url": "https://deeplearning.neuromatch.io/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html", "isFamilyFriendly": true, "displayUrl": "https://deeplearning.neuromatch.io/tutorials/W3D1_UnsupervisedAndSelfSupervised...", "snippet": "The classifier layer then <b>takes</b> the <b>encoder</b> features as <b>input</b>, predicting, for example, the shape latent dimension of <b>encoded</b> <b>input</b> images. Note on terminology: In this tutorial, both the terms representations and features are used to refer to the data embeddings learned in the final layer of the <b>encoder</b> network (of dimension 1x84, and indicated by a red dashed <b>box</b>) which are fed to the classifiers. <b>Encoder</b> network schematic\u00b6 # @markdown ### <b>Encoder</b> network schematic Image (filename = os ...", "dateLastCrawled": "2022-02-02T13:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How <b>to One Hot Encode Sequence Data</b> in Python", "url": "https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-one-hot-encode-", "snippet": "An LSTM <b>takes</b> <b>input</b> as [samples, timesteps, features], the one hot <b>encoded</b> <b>input</b> would be separate features. A dense <b>input</b>, e.g. integer encoding or an embedding, might be more effective. Compare the performance of different methods.", "dateLastCrawled": "2022-02-02T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Encoder</b> decoder network \u2014 sequence to sequence network, \ub610\ub294 seq2seq \ub124\ud2b8\uc6cc\ud06c ...", "url": "https://negra-familiaractress.com/m6yj679-k-g2/Encoder-decoder-network.html", "isFamilyFriendly": true, "displayUrl": "https://negra-familiaractress.com/m6yj679-k-g2/<b>Encoder</b>-decoder-network.html", "snippet": "Decoder <b>Encoder</b> \u2022 <b>Takes</b> <b>an input</b> image and generates a high-dimensional feature vector. \u2022 Aggregate features at multiple levels Decoder \u2022 <b>Takes</b> a high- dimensional feature vector and generates a semantic segmentation mask The <b>encoder</b>-decoder architecture can handle inputs and outputs that are both variable-length sequences, thus is suitable for sequence transduction problems such as machine translation. The <b>encoder</b> <b>takes</b> a variable-length sequence as the <b>input</b> <b>and transforms</b> <b>it into</b> a ...", "dateLastCrawled": "2021-11-25T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "One-hot encoding is the <b>representation</b> of categorical variables as binary vectors. Label Encoding is converting labels/words <b>into</b> numeric form. Using one-hot encoding increases the dimensionality of the data set. Label encoding doesn\u2019t affect the dimensionality of the data set. One-hot encoding creates a new variable for each level in the variable whereas, in Label encoding, the levels of a variable get <b>encoded</b> as 1 and 0. Deep Learning Interview Questions. Deep Learning is a part of ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Data Preparation</b> for <b>Gradient Boosting</b> with XGBoost in Python", "url": "https://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>data-preparation</b>-gradient-", "snippet": "XGBoost is a popular implementation of <b>Gradient Boosting</b> because of its speed and performance. Internally, XGBoost models represent all problems as a regression predictive modeling problem that only <b>takes</b> numerical values as <b>input</b>. If your data is in a different form, it must be prepared <b>into</b> the expected format. In this post, you will discover how to prepare your data for using with", "dateLastCrawled": "2022-02-02T07:03:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding <b>Representation</b> Learning With Autoencoder: Everything You ...", "url": "https://neptune.ai/blog/understanding-representation-learning-with-autoencoder-everything-you-need-to-know-about-representation-and-feature-learning", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/understanding-<b>representation</b>-learning-with-auto<b>encoder</b>...", "snippet": "By constructing our <b>encoder</b> model to output a range of possible values from which we\u2019ll randomly sample to feed <b>into</b> our decoder model, we\u2019re essentially enforcing a continuous, smooth latent space <b>representation</b>. For any sampling of the latent distributions, we\u2019re expecting our decoder model to be able to accurately reconstruct the <b>input</b>. Thus, values which are nearby to one another in latent space should correspond with very <b>similar</b> reconstructions.", "dateLastCrawled": "2022-02-03T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Generating <b>Black</b>-<b>Box</b> Adversarial Examples for Text Classifiers Using a ...", "url": "https://deepai.org/publication/generating-black-box-adversarial-examples-for-text-classifiers-using-a-deep-reinforced-model", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/generating-<b>black</b>-<b>box</b>-adversarial-examples-for-text...", "snippet": "The <b>encoder</b> <b>transforms</b> <b>an input</b> text sequence <b>into</b> an abstract <b>representation</b> h. While the decoder is employed to generate the target sequence using the <b>encoded</b> <b>representation</b> h . However, there are several studies that have incorporated several modifications to the standard <b>encoder</b>-decoder framework [ bahdanau2014neural , luong2016achieving , luong2014addressing ] .", "dateLastCrawled": "2021-12-24T14:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "State Of The Art Text <b>Summarisation Techniques</b>", "url": "https://humboldt-wi.github.io/blog/research/information_systems_1920/nlp_text_summarization_techniques/", "isFamilyFriendly": true, "displayUrl": "https://humboldt-wi.github.io/blog/research/information_systems_1920/nlp_text...", "snippet": "Now, the contents of the \u2018<b>black</b> <b>box</b>\u2019 in the diagram above can be examined. Sequence to sequence models rely on what is called an <b>encoder</b>-decoder architecture \u2013 a combination of layered RNNs that are arranged in way that allows them to perform the tasks of encoding a word sequence and then passing that <b>encoded</b> sequence to a decoder network to produce an output [15][16]. The <b>input</b> sequence is first tokenised (transformed from a collection of words <b>into</b> a collection of integers that ...", "dateLastCrawled": "2022-02-01T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>Illustrated Transformer</b> \u2013 Jay Alammar \u2013 Visualizing machine ...", "url": "https://jalammar.github.io/illustrated-transformer/", "isFamilyFriendly": true, "displayUrl": "https://jalammar.github.io/<b>illustrated-transformer</b>", "snippet": "Let\u2019s begin by looking at the model as a single <b>black</b> <b>box</b>. In a machine translation application, it would take a sentence in one language, and output its translation in another. Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them. The encoding component is a stack of encoders (the paper stacks six of them on top of each other \u2013 there\u2019s nothing magical about the number six, one can definitely experiment with other ...", "dateLastCrawled": "2022-02-03T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>LSTM-Based Neural Network Architecture to infer</b> Model Transformations", "url": "https://modeling-languages.com/lstm-neural-network-model-transformations/", "isFamilyFriendly": true, "displayUrl": "https://modeling-languages.com/lstm-neural-network-model-transformations", "snippet": "As a consequence, apart from the <b>encoder</b> and decoder, we need a layer to embed the <b>input</b> tree (representing the model after the preprocessing phase) <b>into</b> a format readable for the <b>encoder</b>. This <b>input</b> tree embedding layer is used to transform our <b>input</b> models in their tree-form <b>into</b> the numeric vectors which are the <b>input</b> to the LSTM <b>encoder</b>. We also need an output layer <b>that takes</b> the numeric vectors produced by the decoder and obtains the predicted (transformed) output model in its tree ...", "dateLastCrawled": "2022-02-02T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Tutorial 1: Un/Self-supervised learning methods \u2014 Neuromatch Academy ...", "url": "https://deeplearning.neuromatch.io/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html", "isFamilyFriendly": true, "displayUrl": "https://deeplearning.neuromatch.io/tutorials/W3D1_UnsupervisedAndSelfSupervised...", "snippet": "The classifier layer then <b>takes</b> the <b>encoder</b> features as <b>input</b>, predicting, for example, the shape latent dimension of <b>encoded</b> <b>input</b> images. Note on terminology: In this tutorial, both the terms representations and features are used to refer to the data embeddings learned in the final layer of the <b>encoder</b> network (of dimension 1x84, and indicated by a red dashed <b>box</b>) which are fed to the classifiers. <b>Encoder</b> network schematic\u00b6 # @markdown ### <b>Encoder</b> network schematic Image (filename = os ...", "dateLastCrawled": "2022-02-02T13:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) ADePT: Auto-<b>encoder</b> based Differentially Private Text Transformation", "url": "https://www.researchgate.net/publication/348980627_ADePT_Auto-encoder_based_Differentially_Private_Text_Transformation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348980627_ADePT_Auto-<b>encoder</b>_based...", "snippet": "auto-encoders (e.g. LSTM based sequence-to-. sequence models) for text transformation. An auto-. <b>encoder</b> \ufb01rst <b>transforms</b> a given text <b>input</b> <b>into</b> some. latent <b>representation</b>, followed by text ...", "dateLastCrawled": "2022-01-06T12:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Encoder</b> decoder network \u2014 sequence to sequence network, \ub610\ub294 seq2seq \ub124\ud2b8\uc6cc\ud06c ...", "url": "https://negra-familiaractress.com/m6yj679-k-g2/Encoder-decoder-network.html", "isFamilyFriendly": true, "displayUrl": "https://negra-familiaractress.com/m6yj679-k-g2/<b>Encoder</b>-decoder-network.html", "snippet": "The <b>encoder</b> <b>takes</b> a variable-length sequence as the <b>input</b> <b>and transforms</b> <b>it into</b> a state with a fixed shape net = encoderDecoderNetwork (inputSize,<b>encoder</b>,decoder) connects an <b>encoder</b> network and a decoder network to create an <b>encoder</b>-decoder network, net. This function requires Deep Learning Toolbox\u2122. net = encoderDecoderNetwork (inputSize,<b>encoder</b>,decoder,Name,Value) modifies aspects of the <b>encoder</b>-decoder network using name-value arguments The decoder is again a network (usually the same ...", "dateLastCrawled": "2021-11-25T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Data Preparation</b> for <b>Gradient Boosting</b> with XGBoost in Python", "url": "https://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>data-preparation</b>-gradient-", "snippet": "XGBoost is a popular implementation of <b>Gradient Boosting</b> because of its speed and performance. Internally, XGBoost models represent all problems as a regression predictive modeling problem that only <b>takes</b> numerical values as <b>input</b>. If your data is in a different form, it must be prepared <b>into</b> the expected format. In this post, you will discover how to prepare your data for using with", "dateLastCrawled": "2022-02-02T07:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Autoencoder Extraction Feature Pytorch [ZP6U0K]", "url": "https://sna.palermo.it/Autoencoder_Feature_Extraction_Pytorch.html", "isFamilyFriendly": true, "displayUrl": "https://sna.palermo.it/Auto<b>encoder</b>_Feature_Extraction_Pytorch.html", "snippet": "The <b>encoder</b> \\(\\phi\\) <b>takes</b> the <b>input</b> \\(\\mathbf{s}\\) <b>and transforms</b> <b>it into</b> a low-dimensional vector. The proposed method mainly consists of the data processing part, unsupervised feature learning part, and the support vector machine. We assume that in your current directory, there is a img. The results of autoencoder-based feature extraction approaches showed good performance, and the best result was the SMOTE oversampling-applied support vector machine algorithm consider both focal. MNIST ...", "dateLastCrawled": "2022-01-06T19:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Classifying Documents with Quantum-enhanced Transfer Learning</b> | by ...", "url": "https://towardsdatascience.com/classifying-documents-with-quantum-enhanced-transfer-learning-8ee6d04f3ccd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>classifying-documents-with-quantum-enhanced-transfer</b>...", "snippet": "The circuits <b>can</b> <b>be thought</b> of as a <b>black</b>-<b>box</b>, and the optimization is usually achieved through an iterative scheme for example by gradient-based optimization. Schematics of a variational quantum circuit. [Credits: Xanadu.ai] Document Classification. As I explained in Classifying Scientific Papers with Universal Sentence Embeddings, Google released a pre-trained version of the Universal Sentence <b>Encoder</b> (USE) model on TensorFlowHub. To use it: import tensorflow_hub as hub embed = hub.load ...", "dateLastCrawled": "2022-01-30T11:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding <b>Representation</b> Learning With Autoencoder: Everything You ...", "url": "https://neptune.ai/blog/understanding-representation-learning-with-autoencoder-everything-you-need-to-know-about-representation-and-feature-learning", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/understanding-<b>representation</b>-learning-with-auto<b>encoder</b>...", "snippet": "By constructing our <b>encoder</b> model to output a range of possible values from which we\u2019ll randomly sample to feed <b>into</b> our decoder model, we\u2019re essentially enforcing a continuous, smooth latent space <b>representation</b>. For any sampling of the latent distributions, we\u2019re expecting our decoder model to be able to accurately reconstruct the <b>input</b>. Thus, values which are nearby to one another in latent space should correspond with very similar reconstructions.", "dateLastCrawled": "2022-02-03T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Parrotron: An End-to-End Speech-to-Speech Conversion Model and its ...", "url": "https://deepai.org/publication/parrotron-an-end-to-end-speech-to-speech-conversion-model-and-its-applications-to-hearing-impaired-speech-and-speech-separation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/parrotron-an-end-to-end-speech-to-speech-conversion...", "snippet": "We use an end-to-end sequence-to-sequence model architecture <b>that takes</b> <b>an input</b> source speech and generates/synthesizes target speech as output. The only training requirement of such a model is a parallel corpus of paired <b>input</b>-output speech utterances. We refer to this speech-to-speech model as Parrotron. As shown in Figure 1, the network is composed of an <b>encoder</b> and a decoder with attention, followed by a vocoder to synthesize a time-domain waveform. The <b>encoder</b> converts a sequence of ...", "dateLastCrawled": "2022-01-18T09:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A <b>LSTM-Based Neural Network Architecture to infer</b> Model Transformations", "url": "https://modeling-languages.com/lstm-neural-network-model-transformations/", "isFamilyFriendly": true, "displayUrl": "https://modeling-languages.com/lstm-neural-network-model-transformations", "snippet": "As a consequence, apart from the <b>encoder</b> and decoder, we need a layer to embed the <b>input</b> tree (representing the model after the preprocessing phase) <b>into</b> a format readable for the <b>encoder</b>. This <b>input</b> tree embedding layer is used to transform our <b>input</b> models in their tree-form <b>into</b> the numeric vectors which are the <b>input</b> to the LSTM <b>encoder</b>. We also need an output layer <b>that takes</b> the numeric vectors produced by the decoder and obtains the predicted (transformed) output model in its tree ...", "dateLastCrawled": "2022-02-02T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Tutorial 1: Un/Self-supervised learning methods \u2014 Neuromatch Academy ...", "url": "https://deeplearning.neuromatch.io/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html", "isFamilyFriendly": true, "displayUrl": "https://deeplearning.neuromatch.io/tutorials/W3D1_UnsupervisedAndSelfSupervised...", "snippet": "The classifier layer then <b>takes</b> the <b>encoder</b> features as <b>input</b>, predicting, for example, the shape latent dimension of <b>encoded</b> <b>input</b> images. Note on terminology: In this tutorial, both the terms representations and features are used to refer to the data embeddings learned in the final layer of the <b>encoder</b> network (of dimension 1x84, and indicated by a red dashed <b>box</b>) which are fed to the classifiers. <b>Encoder</b> network schematic\u00b6 # @markdown ### <b>Encoder</b> network schematic Image (filename = os ...", "dateLastCrawled": "2022-02-02T13:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Data Preparation</b> for <b>Gradient Boosting</b> with XGBoost in Python", "url": "https://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>data-preparation</b>-gradient-", "snippet": "XGBoost is a popular implementation of <b>Gradient Boosting</b> because of its speed and performance. Internally, XGBoost models represent all problems as a regression predictive modeling problem that only <b>takes</b> numerical values as <b>input</b>. If your data is in a different form, it must be prepared <b>into</b> the expected format. In this post, you will discover how to prepare your data for using with", "dateLastCrawled": "2022-02-02T07:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How <b>to One Hot Encode Sequence Data</b> in Python", "url": "https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-one-hot-encode-", "snippet": "A mapping of all possible inputs is created from char values to integer values. This mapping is then used to encode the <b>input</b> string. We <b>can</b> see that the first letter in the <b>input</b> \u2018h\u2019 is <b>encoded</b> as 7, or the index 7 in the array of possible <b>input</b> values (alphabet). The integer encoding is then converted to a one hot encoding. This is done ...", "dateLastCrawled": "2022-02-02T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning With Python - Quick Guide</b>", "url": "https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_quick_guide.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/<b>machine_learning_with_python</b>/machine_learning_with...", "snippet": "In practice, SVM algorithm is implemented with kernel that <b>transforms</b> <b>an input</b> data space <b>into</b> the required form. SVM uses a technique called the kernel trick in which kernel <b>takes</b> a low dimensional <b>input</b> space <b>and transforms</b> <b>it into</b> a higher dimensional space. In simple words, kernel converts non-separable problems <b>into</b> separable problems by adding more dimensions to it. It makes SVM more powerful, flexible and accurate. The following are some of the types of kernels used by SVM \u2212", "dateLastCrawled": "2022-02-02T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Parrotron: An End-to-End Speech-to-Speech Conversion Model and its ...", "url": "https://www.readkong.com/page/parrotron-an-end-to-end-speech-to-speech-conversion-model-2956517", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/parrotron-an-end-to-end-speech-to-speech-conversion...", "snippet": "Page topic: &quot;Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation&quot;. Created by: Yolanda Martinez. Language: english.", "dateLastCrawled": "2022-01-18T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Machine learning and AI-based approaches for bioactive ligand ... - DeepAI", "url": "https://deepai.org/publication/machine-learning-and-ai-based-approaches-for-bioactive-ligand-discovery-and-gpcr-ligand-recognition", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/machine-learning-and-ai-based-approaches-for-bioactive...", "snippet": "Figure 3: A fully connected autoencoder, which consists of an <b>Encoder</b> submodule that compresses <b>an input</b> <b>representation</b> x <b>into</b> a lower-dimensional <b>representation</b> z. The Decoder, the second submodule of the autoencoder, converts the lower-dimensional <b>representation</b> as ^ x, such that reconstruction resembles the original <b>input</b>, ^ x \u2248 x.", "dateLastCrawled": "2022-01-31T11:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding <b>Representation</b> Learning With Autoencoder: Everything You ...", "url": "https://neptune.ai/blog/understanding-representation-learning-with-autoencoder-everything-you-need-to-know-about-representation-and-feature-learning", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/understanding-<b>representation</b>-learning-with-auto<b>encoder</b>...", "snippet": "By constructing our <b>encoder</b> model to output a range of possible values from which we\u2019ll randomly sample to feed <b>into</b> our decoder model, we\u2019re essentially enforcing a continuous, smooth latent space <b>representation</b>. For any sampling of the latent distributions, we\u2019re expecting our decoder model to be able to accurately reconstruct the <b>input</b>. Thus, values which are nearby to one another in latent space should correspond with very similar reconstructions.", "dateLastCrawled": "2022-02-03T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Generating <b>Black</b>-<b>Box</b> Adversarial Examples for Text Classifiers Using a ...", "url": "https://deepai.org/publication/generating-black-box-adversarial-examples-for-text-classifiers-using-a-deep-reinforced-model", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/generating-<b>black</b>-<b>box</b>-adversarial-examples-for-text...", "snippet": "The <b>encoder</b> <b>transforms</b> <b>an input</b> text sequence <b>into</b> an abstract <b>representation</b> h. While the decoder is employed to generate the target sequence using the <b>encoded</b> <b>representation</b> h . However, there are several studies that have incorporated several modifications to the standard <b>encoder</b>-decoder framework [ bahdanau2014neural , luong2016achieving , luong2014addressing ] .", "dateLastCrawled": "2021-12-24T14:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Generative time series models using Neural ODE in Variational ...", "url": "https://deepai.org/publication/generative-time-series-models-using-neural-ode-in-variational-autoencoders", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/generative-time-series-models-using-neural-ode-in...", "snippet": "Equation 2 the NODE uses a <b>black</b> <b>box</b> ordinary differential equation solver, which <b>takes</b> as <b>input</b> an initial hidden state h (0) to solve an initial value problem up to some given time T. This way the ODE solver yields a <b>representation</b> of a continuous hidden state trajectory, instead of a discrete amount of hidden states. This also means that any specific hidden state along the hidden trajectory <b>can</b> be evaluated, even with uneven step sizes, which is one of the advantages with this approach. i ...", "dateLastCrawled": "2022-01-31T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "State Of The Art Text <b>Summarisation Techniques</b>", "url": "https://humboldt-wi.github.io/blog/research/information_systems_1920/nlp_text_summarization_techniques/", "isFamilyFriendly": true, "displayUrl": "https://humboldt-wi.github.io/blog/research/information_systems_1920/nlp_text...", "snippet": "Now, the contents of the \u2018<b>black</b> <b>box</b>\u2019 in the diagram above <b>can</b> be examined. Sequence to sequence models rely on what is called an <b>encoder</b>-decoder architecture \u2013 a combination of layered RNNs that are arranged in way that allows them to perform the tasks of encoding a word sequence and then passing that <b>encoded</b> sequence to a decoder network to produce an output [15][16]. The <b>input</b> sequence is first tokenised (transformed from a collection of words <b>into</b> a collection of integers that ...", "dateLastCrawled": "2022-02-01T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>LSTM-Based Neural Network Architecture to infer</b> Model Transformations", "url": "https://modeling-languages.com/lstm-neural-network-model-transformations/", "isFamilyFriendly": true, "displayUrl": "https://modeling-languages.com/lstm-neural-network-model-transformations", "snippet": "As a consequence, apart from the <b>encoder</b> and decoder, we need a layer to embed the <b>input</b> tree (representing the model after the preprocessing phase) <b>into</b> a format readable for the <b>encoder</b>. This <b>input</b> tree embedding layer is used to transform our <b>input</b> models in their tree-form <b>into</b> the numeric vectors which are the <b>input</b> to the LSTM <b>encoder</b>. We also need an output layer <b>that takes</b> the numeric vectors produced by the decoder and obtains the predicted (transformed) output model in its tree ...", "dateLastCrawled": "2022-02-02T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) ADePT: Auto-<b>encoder</b> based Differentially Private Text Transformation", "url": "https://www.researchgate.net/publication/348980627_ADePT_Auto-encoder_based_Differentially_Private_Text_Transformation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348980627_ADePT_Auto-<b>encoder</b>_based...", "snippet": "auto-encoders (e.g. LSTM based sequence-to-. sequence models) for text transformation. An auto-. <b>encoder</b> \ufb01rst <b>transforms</b> a given text <b>input</b> <b>into</b> some. latent <b>representation</b>, followed by text ...", "dateLastCrawled": "2022-01-06T12:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Tutorial 1: Un/Self-supervised learning methods \u2014 Neuromatch Academy ...", "url": "https://deeplearning.neuromatch.io/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html", "isFamilyFriendly": true, "displayUrl": "https://deeplearning.neuromatch.io/tutorials/W3D1_UnsupervisedAndSelfSupervised...", "snippet": "The classifier layer then <b>takes</b> the <b>encoder</b> features as <b>input</b>, predicting, for example, the shape latent dimension of <b>encoded</b> <b>input</b> images. Note on terminology: In this tutorial, both the terms representations and features are used to refer to the data embeddings learned in the final layer of the <b>encoder</b> network (of dimension 1x84, and indicated by a red dashed <b>box</b>) which are fed to the classifiers. <b>Encoder</b> network schematic\u00b6 # @markdown ### <b>Encoder</b> network schematic Image (filename = os ...", "dateLastCrawled": "2022-02-02T13:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>The Algorithm Design Manual - Steven S</b>. Skiena | \u00d6zlem Ekici ...", "url": "https://www.academia.edu/48943214/The_Algorithm_Design_Manual_Steven_S_Skiena", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/48943214/<b>The_Algorithm_Design_Manual_Steven_S</b>_Skiena", "snippet": "Second Edition - Springer This book is intended as a manual on algorithm design, providing access to combinatorial algorithm technology for both students and computer professionals. It is divided <b>into</b> two parts: Techniques and Resources. The former", "dateLastCrawled": "2022-02-03T07:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A guide to machine learning for biologists | Nature Reviews Molecular ...", "url": "https://www.nature.com/articles/s41580-021-00407-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41580-021-00407-0", "snippet": "e | An autoencoder consists of an <b>encoder</b> neural network, which converts <b>an input</b> <b>into</b> a lower-dimensional latent <b>representation</b>, and a decoder neural network, which converts this latent ...", "dateLastCrawled": "2022-02-02T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "One-hot encoding is the <b>representation</b> of categorical variables as binary vectors. Label Encoding is converting labels/words <b>into</b> numeric form. Using one-hot encoding increases the dimensionality of the data set. Label encoding doesn\u2019t affect the dimensionality of the data set. One-hot encoding creates a new variable for each level in the variable whereas, in Label encoding, the levels of a variable get <b>encoded</b> as 1 and 0. Deep Learning Interview Questions. Deep Learning is a part of ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "9.6. <b>Encoder-Decoder</b> Architecture \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/<b>encoder-decoder</b>.html", "snippet": "<b>Encoder-Decoder</b> Architecture \u2014 Dive into Deep <b>Learning</b> 0.17.0 documentation. 9.6. <b>Encoder-Decoder</b> Architecture. As we have discussed in Section 9.5, <b>machine</b> translation is a major problem domain for sequence transduction models, whose input and output are both variable-length sequences. To handle this type of inputs and outputs, we can design ...", "dateLastCrawled": "2022-01-30T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Towards <b>Analogy</b>-Based Explanations in <b>Machine</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/341668539_Towards_Analogy-Based_Explanations_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/341668539_Towards_<b>Analogy</b>-Based_Explanations...", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that ...", "dateLastCrawled": "2022-01-06T06:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Dynamical machine learning volumetric reconstruction of</b> objects ...", "url": "https://www.nature.com/articles/s41377-021-00512-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41377-021-00512-x", "snippet": "The <b>encoder</b> and decoder also utilize separable convolution, in conjunction with residual <b>learning</b>, which is known to improve generalization in deep networks 90.", "dateLastCrawled": "2022-02-02T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Solving Word <b>Analogies: A Machine Learning Perspective</b> | Request PDF", "url": "https://www.researchgate.net/publication/335597029_Solving_Word_Analogies_A_Machine_Learning_Perspective", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335597029_Solving_Word_Analogies_A_<b>Machine</b>...", "snippet": "We introduce a supervised corpus-based <b>machine</b> <b>learning</b> algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT <b>analogy</b> questions, TOEFL synonym questions ...", "dateLastCrawled": "2021-10-16T21:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning to Generate Long-term Future via Hierarchical</b> Prediction", "url": "http://proceedings.mlr.press/v70/villegas17a.html", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v70/villegas17a.html", "snippet": "Our model is built with a combination of LSTM and <b>analogy</b> based <b>encoder</b>-decoder convolutional neural networks, which independently predict the video structure and generate the future frames, respectively. In experiments, our model is evaluated on the Human3.6M and Penn Action datasets on the task of long-term pixel-level video prediction of humans performing actions and demonstrate significantly better results than the state-of-the-art.} } Copy to Clipboard Download. Endnote %0 Conference ...", "dateLastCrawled": "2022-01-29T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The conceptual arithmetics of concepts | by Assaad MOAWAD | DataThings ...", "url": "https://medium.com/datathings/the-conceptual-arithmetics-of-concepts-369df29e4e0f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/datathings/the-conceptual-arithmetics-of-concepts-369df29e4e0f", "snippet": "<b>Machine</b> <b>learning</b> field is an amazing and very fast evolving domain. However, it is still hard to use it in its current state due to its cost and complexity. With time, we will have more and more ...", "dateLastCrawled": "2022-01-04T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Usage: In fraud detection, use Auto-<b>Encoder</b> to compress all data to dense vector, then kNN is used to detect outliers; Reinforcement <b>Learning</b> Definitions. Reinforcement <b>learning</b> (RL) is an area of <b>machine</b> <b>learning</b> that focuses on how agent to act in an environment in order to maximize some given reward. Markov Decision Processes (MDPs) Components", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b> <b>Logistic Regression</b> with Python and Scikit-learn | by ...", "url": "https://dmarcisovska.medium.com/machine-learning-logistic-regression-with-python-and-scikit-learn-f278843aca4e", "isFamilyFriendly": true, "displayUrl": "https://dmarcisovska.medium.com/<b>machine</b>-<b>learning</b>-<b>logistic-regression</b>-with-python-and...", "snippet": "Many <b>machine</b> <b>learning</b> algorithms perform better when numerical data is scaled to a standard range. Data may have different units (such as year, hours, months, USD Dollar, etc.) which may mean the variables have different scales. Differences in the scales across our data may increase the difficulty of the problem being modeled. Standardizing a dataset involves rescaling the distribution of data so that the mean of observed values is 0 and the standard deviation is 1.", "dateLastCrawled": "2022-01-14T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "<b>Encoder</b>-Decoder Attention: Attention between the input sequence and the output sequence. ... If you are looking for an <b>analogy</b> between self attention and attention, think of z serving the purpose of context vectors and not global alignment weights. The <b>Transformer</b>. \u26a0\ufe0f A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2) (left) The ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - What is an <b>autoencoder</b>? - Data Science Stack Exchange", "url": "https://datascience.stackexchange.com/questions/80389/what-is-an-autoencoder", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/80389", "snippet": "I am a student and I am studying <b>machine</b> <b>learning</b>. I am focusing on deep generative models, and in particular to autoencoders and variational autoencoders (VAE).. I am trying to understand the concept, but I am having some problems. So far, I have understood that an <b>autoencoder</b> takes an input, for example an image, and wants to reduce this image into a latent space, which should contain the underlying features of the dataset, with an operation of encoding, then, with an operation of decoding ...", "dateLastCrawled": "2022-01-26T06:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>LSTM Autoencoders</b> - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/lstm-autoencoders/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>lstm-autoencoders</b>", "snippet": "This is challenging because <b>machine</b> <b>learning</b> algorithms, and neural networks in particular, are designed to work with fixed length inputs. Another challenge with sequence data is that the temporal ordering of the observations can make it challenging to extract features suitable for use as input to supervised <b>learning</b> models, often requiring deep expertise in the domain or in the field of signal processing. Finally, many predictive modeling problems involving sequences require a prediction ...", "dateLastCrawled": "2022-02-03T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>machine</b> <b>learning</b> - <b>Parameters tuning for auto-encoders</b> - Cross Validated", "url": "https://stats.stackexchange.com/questions/235114/parameters-tuning-for-auto-encoders", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/235114/<b>parameters-tuning-for-auto-encoders</b>", "snippet": "Actually, the cost function of a sparse auto-<b>encoder is like</b>. I tested with my datasets, it seems that all these four parameters have impact on the final results. Are there any general rules of &#39;optimal&#39; settings of these four parameters? When I was using Support Vector <b>Machine</b> based classifier, there is a &#39;grid search&#39; method to optimize the two hyper-parameters of the SVM. Are there any similar method available for (sparse) auto-encoders? As far as I see, grid search is feasible to ...", "dateLastCrawled": "2022-01-28T00:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Log Data Anomaly Detection Using a <b>Machine</b> <b>Learning</b> Model", "url": "https://insights.ltts.com/story/log-data-anomaly-detection-using-a-machine-learning-model/page/1", "isFamilyFriendly": true, "displayUrl": "https://insights.ltts.com/story/log-data-anomaly-detection-using-a-<b>machine</b>-<b>learning</b>...", "snippet": "In this paper, we have explored various <b>machine</b> <b>learning</b> algorithms and an auto encoder to detect anomalies which can help the developers to quickly identify and derive relevant and appropriate information from the logs maintained. &lt;small&gt;An Industry Perspective. System Logs: An Industry Perspective . There are multiple examples of system generated logs in use: Events of logs generated from server application ; A database system maintaining transaction logs which could be used for ...", "dateLastCrawled": "2022-01-26T20:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>The security of machine learning</b> - researchgate.net", "url": "https://www.researchgate.net/publication/220343885_The_security_of_machine_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220343885_<b>The_security_of_machine_learning</b>", "snippet": "In particular, self-supervised <b>learning</b> aims to pre-train an encoder using a large amount of unlabeled data. The pre-trained <b>encoder is like</b> an &quot;operating system&quot; of the AI ecosystem. In ...", "dateLastCrawled": "2022-01-12T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Summary of \u2014 <b>SegNet</b>: <b>A Deep Convolutional Encoder-Decoder</b> Architecture ...", "url": "https://towardsdatascience.com/summary-of-segnet-a-deep-convolutional-encoder-decoder-architecture-for-image-segmentation-75b2805d86f5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/summary-of-<b>segnet</b>-<b>a-deep-convolutional-encoder-decoder</b>...", "snippet": "Fig 3: Encoder architecture. Each <b>encoder is like</b> Fig 3. The novelty is in the subsampling stage, Max-pooling is used to achieve translation invariance over small spatial shifts in the image, combine that with Subsampling and it leads to each pixel governing a larger input image context (spatial window). These methods achieve better classification accuracy but reduce the feature map size, this leads to lossy image representation with blurred boundaries which is not ideal for segmentation ...", "dateLastCrawled": "2022-01-30T01:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>machine</b> <b>learning</b> - What is the input for the prior model of VQ-VAE ...", "url": "https://ai.stackexchange.com/questions/17203/what-is-the-input-for-the-prior-model-of-vq-vae", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/17203", "snippet": "<b>machine</b>-<b>learning</b> generative-model variational-autoencoder. Share. Improve this question. Follow asked Dec 22 &#39;19 at 6:08. Diego Gomez Diego Gomez. 393 3 3 silver badges 9 9 bronze badges $\\endgroup$ Add a comment | 1 Answer Active Oldest Votes. 0 $\\begingroup$ Some notes about VQ-VAE: In the paper, they used PixelCNN to learn the prior. PixelCNN is trained on images. The discrete latent variables are just the indices of the embedding vectors. For example, you can put your embedding vectors ...", "dateLastCrawled": "2022-01-07T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[2110.15444] 10 Security and Privacy Problems in Self-Supervised <b>Learning</b>", "url": "https://arxiv.org/abs/2110.15444", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2110.15444", "snippet": "The pre-trained <b>encoder is like</b> an &quot;operating system&quot; of the AI ecosystem. Specifically, the encoder can be used as a feature extractor for many downstream tasks with little or no labeled training data. Existing studies on self-supervised <b>learning</b> mainly focused on pre-training a better encoder to improve its performance on downstream tasks in non-adversarial settings, leaving its security and privacy in adversarial settings largely unexplored. A security or privacy issue of a pre-trained ...", "dateLastCrawled": "2021-12-28T23:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "probability - why a denoising auto-<b>encoder is like</b> performing ...", "url": "https://math.stackexchange.com/questions/2318301/why-a-denoising-auto-encoder-is-like-performing-stochastic-gradient-this-on-this", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/2318301", "snippet": "why a denoising auto-<b>encoder is like</b> performing stochastic gradient this on this expression? Ask Question Asked 4 years, 7 months ago. Active 4 years, 7 months ago. Viewed 665 times 2 1 $\\begingroup$ I was reading ...", "dateLastCrawled": "2022-01-24T01:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2110.15444v2] 10 Security and Privacy Problems in Self-Supervised <b>Learning</b>", "url": "https://arxiv.org/abs/2110.15444v2", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2110.15444v2", "snippet": "Self-supervised <b>learning</b> has achieved revolutionary progress in the past several years and is commonly believed to be a promising approach for general-purpose AI. In particular, self-supervised <b>learning</b> aims to pre-train an encoder using a large amount of unlabeled data. The pre-trained <b>encoder is like</b> an &quot;operating system&quot; of the AI ecosystem. Specifically, the encoder can be used as a feature extractor for many downstream tasks with little or no labeled training data. Existing studies on ...", "dateLastCrawled": "2021-11-08T02:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Convolutional Coding</b> - GaussianWaves", "url": "https://www.gaussianwaves.com/2010/06/convolutional-coding-2/", "isFamilyFriendly": true, "displayUrl": "https://www.gaussianwaves.com/2010/06/<b>convolutional-coding</b>-2", "snippet": "Till now the <b>encoder is like</b> a black box to us in the sense that we don\u2019t know how the memory elements are utilized to generate the output bits from the input. To fully understand the encoder structure we need something called \u201cgenerator polynomials\u201d that tell us how the memory elements are linked to achieve encoding. The generator polynomials for a specific convolutional encoder set (n,k,L) are usually found through simulation. The set (n,k,L) along with n generator polynomials ...", "dateLastCrawled": "2022-01-09T00:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Categorical Encoding with CatBoost Encoder</b> - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/categorical-encoding-with-catboost-encoder/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>categorical-encoding-with-catboost-encoder</b>", "snippet": "Many <b>machine</b> <b>learning</b> algorithms require data to be numeric. So, before training a model, we need to convert categorical data into numeric form. There are various categorical encoding methods available. Catboost is one of them. Catboost is a target-based categorical encoder. It is a supervised encoder that encodes categorical columns according to the target value. It supports binomial and continuous targets. Target encoding is a popular technique used for categorical encoding. It replaces a ...", "dateLastCrawled": "2022-02-03T12:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Implementing an <b>Autoencoder</b> in TensorFlow 2.0 | by Abien Fred Agarap ...", "url": "https://towardsdatascience.com/implementing-an-autoencoder-in-tensorflow-2-0-5e86126e9f7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/implementing-an-<b>autoencoder</b>-in-tensorflow-2-0-5e86126e9f7", "snippet": "We deal with huge amount of data in <b>machine</b> <b>learning</b> which naturally leads to more computations. However, we can also just pick the parts of the data that contribute the most to a model\u2019s <b>learning</b>, thus leading to less computations. The process of choosing the important parts of the data is known as feature selection, which is among the number of use cases for an <b>autoencoder</b>. But what exactly is an <b>autoencoder</b>? Well, let\u2019s first recall that a neural network is a computational model that ...", "dateLastCrawled": "2022-02-03T07:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Intuitively Understanding Variational Autoencoders | by Irhum Shafkat ...", "url": "https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/intuitively-understanding-variational-<b>autoencoder</b>s-1bfe...", "snippet": "The <b>encoder is similar</b>, it is simply is a network that takes in an input and produces a much smaller representation (the encoding), that contains enough information for the next part of the network to process it into the desired output format. Typically, the encoder is trained together with the other parts of the network, optimized via back-propagation, to produce encodings specifically useful for the task at hand. In CNNs, the 1000-dimensional encodings produced are such that they\u2019re ...", "dateLastCrawled": "2022-02-02T13:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An Introduction to Generative <b>Deep Learning</b> | by Anil Chandra Naidu ...", "url": "https://medium.com/analytics-vidhya/an-introduction-to-generative-deep-learning-792e93d1c6d4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/an-introduction-to-generative-<b>deep-learning</b>-792e93...", "snippet": "An autoencoder is a type of ANN used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for ...", "dateLastCrawled": "2022-01-29T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Implementing an <b>Autoencoder</b> in TensorFlow 2.0 - Abien Fred Agarap", "url": "https://afagarap.github.io/2019/03/20/implementing-autoencoder-in-tensorflow-2.0.html", "isFamilyFriendly": true, "displayUrl": "https://afagarap.github.io/2019/03/20/implementing-<b>autoencoder</b>-in-tensorflow-2.0.html", "snippet": "Google announced a major upgrade on the world\u2019s most popular open-source <b>machine</b> <b>learning</b> library, TensorFlow, with a promise of focusing on simplicity and ease of use, eager execution, intuitive high-level APIs, and flexible model building on any platform. This post is a humble attempt to contribute to the body of working TensorFlow 2.0 examples. Specifically, we shall discuss the subclassing API implementation of an <b>autoencoder</b>. To install TensorFlow 2.0, use the following pip install ...", "dateLastCrawled": "2022-01-31T12:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Information | Free Full-Text | Deep <b>Learning</b> Models for Colorectal ...", "url": "https://www.mdpi.com/2078-2489/12/6/245/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2078-2489/12/6/245/htm", "snippet": "The structure of the <b>encoder is similar</b> to some image classification neural networks such as the convolutional layer, which includes the batch normalization, the rectified linear unit (the ReLu) activation function, and the pooling layer. The decoder part has the inversed layers used in the encoder, such as deconvolution layers and de-max_pool layers. The encoder part has 13 convolutional layers and 5 max_pooling layers, where the first 3 layers of the model have these characteristics: the ...", "dateLastCrawled": "2022-01-17T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Encoding</b> <b>categorical</b> variables - Stacked Turtles", "url": "https://kiwidamien.github.io/encoding-categorical-variables.html", "isFamilyFriendly": true, "displayUrl": "https://kiwidamien.github.io/<b>encoding</b>-<b>categorical</b>-variables.html", "snippet": "The way you encode <b>categorical</b> variables changes how effective your <b>machine</b> <b>learning</b> algorithm is. This article will go over some common <b>encoding</b> techniques, as well as their advantages and disadvantages. Some terminology. Levels: A levels of a non-numeric feature are the number of distinct values. The examples listed above are all examples of levels. The number of levels can vary wildly: the number of races for a patient is typically four (asian, black, hispanic, and white), the number of ...", "dateLastCrawled": "2022-01-30T23:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Hands-on with Feature Engineering Techniques</b>: Advanced Methods | by ...", "url": "https://heartbeat.comet.ml/hands-on-with-feature-engineering-advanced-methods-in-python-for-machine-learning-e05bf12da06a", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/<b>hands-on-with-feature-engineering</b>-advanced-methods-in...", "snippet": "This post is a part of a series about <b>feature engineering techniques</b> for <b>machine</b> <b>learning</b> with Python. You can check out the rest of the articles: <b>Hands-on with Feature Engineering Techniques</b>: Broad Introduction. <b>Hands-on with Feature Engineering Techniques</b>: Variable Types. <b>Hands-on with Feature Engineering Techniques</b>: Common Issues in Datasets. <b>Hands-on with Feature Engineering Techniques</b>: Imputing Missing Values. <b>Hands-on with Feature Engineering Techniques</b>: Encoding Categorical Variables ...", "dateLastCrawled": "2022-02-01T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fully Convolutional Refined Auto-Encoding Generative Adversarial ...", "url": "https://becominghuman.ai/3d-multi-object-gan-7b7cee4abf80", "isFamilyFriendly": true, "displayUrl": "https://becominghuman.ai/<b>3d-multi-object-gan</b>-7b7cee4abf80", "snippet": "The basic architecture of <b>encoder is similar</b> to discriminator network of 3DGAN[1]. The difference is the last layer which is 1x1x1 fully convolution.-Generator. The basic architecture of generator is also similar to 3DGAN[1] as above figure. The difference is the last layer which has 12 channels and is activated by softmax. Also, the first layer of latent space is flatten. -Discriminator. The basic architecture of discriminator is also similar to 3DGAN[1]. The difference is the activation ...", "dateLastCrawled": "2022-01-26T00:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Frontiers | Deep <b>Learning</b> for Understanding <b>Satellite Imagery</b>: An ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.534696/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.534696", "snippet": "The left half of the network (<b>encoder) is similar</b> to a CNN, tasked with coming up with a low dimensional dense representation of the input, and the right side (decoder) then up-samples the learned feature representations to the same shape as the input. The shortcut connections let information flow from the encoder to the decoder and help the network keeping spatial information. As the work of Li et al. (2017) has impressively shown, U-Nets benefit greatly from a deeper model architecture. It ...", "dateLastCrawled": "2022-01-31T16:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Encoder G25 G27 60 Slot - lgpfc.co.uk", "url": "https://lgpfc.co.uk/Encoder-G25-g27-60-Slot", "isFamilyFriendly": true, "displayUrl": "https://lgpfc.co.uk/Encoder-G25-g27-60-Slot", "snippet": "This gameplay is based on the traditional, casino-style slot <b>machine</b>. At the same time, each Online Encoder G25 G27 60 Slot Slots game will have its own unique set of individual rules and characteristics. Before playing any new Online Encoder G25 G27 60 Slot Slots game, you should become familiar with how the game works by trying the free demo version and having a close look at the game\u2019s paytable. Sports. Canada. The Canadian regulatory environment is <b>just as Encoder</b> G25 G27 60 Slot ...", "dateLastCrawled": "2022-01-16T21:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Google AI</b> Blog: July 2019", "url": "https://ai.googleblog.com/2019/07/", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com/2019/07", "snippet": "Such a multitask trained <b>encoder can be thought of as</b> <b>learning</b> a latent representation of the input that maintains information about the underlying linguistic content. Overview of the Parrotron model architecture. An input speech spectrogram is passed through encoder and decoder neural networks to generate an output spectrogram in a new voice. Case Studies To demonstrate a proof of concept, we worked with our fellow Google research scientist and mathematician Dimitri Kanevsky, who was born ...", "dateLastCrawled": "2022-01-29T22:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Google AI Blog: Parrotron: New Research into Improving Verbal ...", "url": "https://ai.googleblog.com/2019/07/parrotron-new-research-into-improving.html", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com/2019/07/parrotron-new-research-into-improving.html", "snippet": "Such a multitask trained <b>encoder can be thought of as</b> <b>learning</b> a latent representation of the input that maintains information about the underlying linguistic content. Overview of the Parrotron model architecture. An input speech spectrogram is passed through encoder and decoder neural networks to generate an output spectrogram in a new voice. Case Studies To demonstrate a proof of concept, we worked with our fellow Google research scientist and mathematician Dimitri Kanevsky, who was born ...", "dateLastCrawled": "2022-01-19T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "Intuitively, Transformer&#39;s <b>encoder can be thought of as</b> a sequence of reasoning steps (layers). At each step, tokens look at each other (this is where we need <b>attention</b> - self-<b>attention</b>), exchange information and try to understand each other better in the context of the whole sentence. This happens in several layers (e.g., 6).", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Using <b>Bidirectional</b> Generative Adversarial Networks to estimate Value ...", "url": "https://towardsdatascience.com/using-bidirectional-generative-adversarial-networks-to-estimate-value-at-risk-for-market-risk-c3dffbbde8dd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-<b>bidirectional</b>-generative-adversarial-networks-to...", "snippet": "Note that given an optimal discriminator, the objective function of the generator and <b>encoder can be thought of as</b> that of an autoencoder, where the generator plays the role of a decoder. The objective function of the generator and encoder is simply to minimize the objective function of the discriminator, i.e., we have not explicitly specified the structure of the reconstruction loss as one might do so with an autoencoder. This implicit minimization of the reconstruction loss is yet another ...", "dateLastCrawled": "2022-01-31T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Distributed Coding</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/engineering/distributed-coding", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/engineering/<b>distributed-coding</b>", "snippet": "A Wyner\u2013Ziv <b>encoder can be thought of as</b> a quantizer followed by a Slepian\u2013Wolf encoder. In cases of images and video, existing <b>distributed coding</b> schemes add the Wyner\u2013Ziv encoder into the standard transform coding structure. As with the centralized case, a linear transform is independently applied to each image or video frame. Each transform coefficient is still treated independently, but it is fed into a Wyner\u2013Ziv coder instead of a scalar quantizer and an entropy coder. We refer ...", "dateLastCrawled": "2022-01-04T19:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Parrotron: An End-to-End Speech-to-Speech Conversion Model and its ...", "url": "https://deepai.org/publication/parrotron-an-end-to-end-speech-to-speech-conversion-model-and-its-applications-to-hearing-impaired-speech-and-speech-separation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/parrotron-an-end-to-end-speech-to-speech-conversion...", "snippet": "We apply more modern <b>machine</b> <b>learning</b> techniques to this problem, and demonstrate that, given sufficient training data, ... Such a multitask trained <b>encoder can be thought of as</b> <b>learning</b> a latent representation of the input that maintains information about the underlying transcript, i.e. one that is closer to the latent representation learned within a TTS sequence-to-sequence network. The decoder input is created by concatenating a 64-dim embedding for the grapheme emitted at the previous ...", "dateLastCrawled": "2022-01-18T09:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Distributed Source Coding: Theory, Algorithms and Applications</b> - PDF ...", "url": "https://epdf.pub/distributed-source-coding-theory-algorithms-and-applications.html", "isFamilyFriendly": true, "displayUrl": "https://epdf.pub/<b>distributed-source-coding-theory-algorithms-and-applications</b>.html", "snippet": "A Wyner\u2013Ziv <b>encoder can be thought of as</b> a quantizer followed by a Slepian\u2013Wolf encoder. In cases of images and video, existing distributed coding schemes add the Wyner\u2013 Ziv encoder into the standard transform coding structure. As with the centralized case, a linear transform is independently applied to each image or video frame. Each transform coef\ufb01cient is still treated independently, but it is fed into a Wyner\u2013Ziv coder instead of a scalar quantizer and an entropy coder. We ...", "dateLastCrawled": "2021-12-28T03:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Hands-On <b>Convolutional Neural Networks with TensorFlow</b>: Solve computer ...", "url": "https://dokumen.pub/hands-on-convolutional-neural-networks-with-tensorflow-solve-computer-vision-problems-with-modeling-in-tensorflow-and-python-9781789132823-1789132827.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/hands-on-<b>convolutional-neural-networks-with-tensorflow</b>-solve...", "snippet": "In the <b>machine</b> <b>learning</b> stage, all the feature vectors will be given to a <b>machine</b> <b>learning</b> system that creates a model. We hope that this model can generalize and is able to predict the digit for any future images given to the system that it wasn\u2019t trained on. An integral part of an ML system is evaluation. When we evaluate our model, we see how well our model has done in a particular task. In our example, we would look at how accurately it can predict the digit from the image. Accuracy of ...", "dateLastCrawled": "2022-01-24T16:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Parrotron: An End-to-End Speech-to-Speech Conversion Model and ...", "url": "https://www.researchgate.net/publication/335829307_Parrotron_An_End-to-End_Speech-to-Speech_Conversion_Model_and_its_Applications_to_Hearing-Impaired_Speech_and_Speech_Separation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335829307_Parrotron_An_End-to-End_Speech-to...", "snippet": "W.-c. W oo, \u201cConvolutional LSTM network: A <b>machine</b> <b>learning</b> approach for precipitation nowcasting,\u201d in Advances in Neural Information Processing Systems , 2015, pp. 802\u2013810.", "dateLastCrawled": "2022-01-29T14:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Error Diagnosis of Deep Monocular Depth Estimation Models", "url": "http://vision.soic.indiana.edu/papers/errordiagnosis2021iros.pdf", "isFamilyFriendly": true, "displayUrl": "vision.soic.indiana.edu/papers/errordiagnosis2021iros.pdf", "snippet": "<b>Machine</b> <b>learning</b>-based approaches such as Make3D [6], and more recent techniques based on deep <b>learning</b> [7], [8], have shown signi\ufb01cant promise. These techniques take a variety of approaches. For example, instead of directly estimating depth, BTS [9] estimates the parameters of local planes at various scales. The model is trained using only ground truth depth, as the local plane parameters are learned implicitly by the net-work. PlaneRCNN [10], another state-of-the-art technique, estimates ...", "dateLastCrawled": "2021-09-30T12:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Automatic <b>Machine</b> Translation Evaluation in Many Languages via Zero ...", "url": "https://aclanthology.org/2020.emnlp-main.8.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.emnlp-main.8.pdf", "snippet": "We frame the task of <b>machine</b> translation evaluation as one of scoring <b>machine</b> transla-tion output with a sequence-to-sequence para-phraser, conditioned on a human reference. We propose training the paraphraser as a multi-lingual NMT system, treating paraphrasing as a zero-shot translation task (e.g., Czech to Czech). This results in the paraphraser\u2019s out-put mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a ...", "dateLastCrawled": "2022-01-21T14:24:00.0000000Z", "language": "en", "isNavigational": false}]], "all_bing_queries": ["+(encoder)  is like +(black box that takes an input and transforms it into an encoded representation)", "+(encoder) is similar to +(black box that takes an input and transforms it into an encoded representation)", "+(encoder) can be thought of as +(black box that takes an input and transforms it into an encoded representation)", "+(encoder) can be compared to +(black box that takes an input and transforms it into an encoded representation)", "machine learning +(encoder AND analogy)", "machine learning +(\"encoder is like\")", "machine learning +(\"encoder is similar\")", "machine learning +(\"just as encoder\")", "machine learning +(\"encoder can be thought of as\")", "machine learning +(\"encoder can be compared to\")"]}