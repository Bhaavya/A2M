{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Use Timesteps in LSTM Networks for <b>Time Series Forecasting</b>", "url": "https://machinelearningmastery.com/use-timesteps-lstm-networks-time-series-forecasting/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/use-<b>timestep</b>s-lstm-networks-<b>time-series-forecasting</b>", "snippet": "The Long Short-Term Memory (LSTM) network in Keras supports time steps. This raises the question as to whether lag observations for a univariate time series can be used as time steps for an LSTM and whether or not this improves forecast performance. In this tutorial, we will investigate the use of lag observations as time steps in LSTMs models in Python. After completing", "dateLastCrawled": "2022-02-02T08:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Top 5 Examples to Implement of <b>SQL Timestamp</b> - EDUCBA", "url": "https://www.educba.com/sql-timestamp/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>sql-timestamp</b>", "snippet": "A few functions <b>like</b> EXTRACT in SQL let us extract a specific piece of information from the timestamp. For example, we can extract <b>DAY</b>, MONTH, YEAR, HOUR, MINUTE, SECONDS, etc., from the timestamp. In the following examples, we have tried to extract <b>DAY</b> and MONTH from the timestamp. SELECT EXTRACT (<b>DAY</b> FROM &#39;2020-03-23 00:00&#39;:: timestamp);", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Sequence Modeling with Neural Networks - Part I - DataScienceCentral.com", "url": "https://www.datasciencecentral.com/sequence-modeling-with-neural-networks-part-i/", "isFamilyFriendly": true, "displayUrl": "https://www.datasciencecentral.com/sequence-modeling-with-neural-networks-part-i", "snippet": "The fact of not having <b>new</b> parameters for <b>every</b> point of the sequence also helps us deal with variable-length sequences. In case of a sequence that has a length of 4, we could unroll this RNN to four timesteps. In other cases, we can unroll it to ten timesteps since the length of the sequence is not prespecified in the algorithm. By unrolling we simply mean that we write out the network for the complete sequence. For example, if the sequence we care about is a sentence of 5 words, the ...", "dateLastCrawled": "2022-01-30T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "machine <b>learning</b> - What is the input to LSTM exactly? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/49883290/what-is-the-input-to-lstm-exactly", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/49883290", "snippet": "in each <b>timestep</b> each node gets a different input, the input <b>is like</b> a sliding window moving from left to right over the list. In this case <b>every</b> element from the list (<b>every</b> number) is presented unequally often to the LSTM. My last idea is: Input <b>timestep</b> 1: node 1 = 0, node 2 = 5, node 3 = 0 next <b>timestep</b>: node 1 = 4, node 2 = 3, node 3 = 3 ...", "dateLastCrawled": "2022-01-20T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "CS224N/Lin4 with Deep <b>Learning</b> tural Language Pr ocessing", "url": "https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture06-rnnlm.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture06-rnnlm.pdf", "snippet": "<b>every</b> <b>timestep</b>, so there is symmetry in how inputs are processed. RNN Disadvantages: \u2022 Recurrent computation is slow \u2022 In practice, difficult to access information from many steps back More on these later in the course 24. Training a RNN Language Model \u2022 Get a big corpus of text which is a sequence of words \u2022 Feed into RNN-LM; compute output distribution for <b>every</b> step t. \u2022i.e. predict probability dist of <b>every</b> <b>word</b>, given words so far \u2022 Loss function on step t is cross-entropy ...", "dateLastCrawled": "2022-02-02T01:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "All You Need to Know About Automatic Speech Recognition Transcription ...", "url": "https://www.rev.com/blog/guide-to-speech-recognition-transcription-models", "isFamilyFriendly": true, "displayUrl": "https://www.rev.com/blog/guide-to-speech-recognition-transcription-models", "snippet": "As mentioned, <b>one</b> of the key components of any ASR system is the acoustic model. This model takes as input the raw audio waveforms of human speech and provides predictions at each <b>timestep</b>. The waveform is typically broken into frames of around 25 ms and then the model gives a probabilistic prediction of which phoneme is being uttered in each frame. Phonemes are <b>like</b> the atomic units of pronunciation. They provide a way of identifying the different sounds associated with human speech. Many ...", "dateLastCrawled": "2022-02-02T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Word Embedding: CBOW &amp; Skip-gram</b>. Hi Guys! In this blog, I have written ...", "url": "https://medium.datadriveninvestor.com/word-embedding-cbow-skip-gram-8262e22fa7c", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/<b>word-embedding-cbow-skip-gram</b>-8262e22fa7c", "snippet": "Figure 1. <b>Word</b> notation. Focus <b>word</b> are those <b>word</b> (output) that this model need to predict from the given context <b>word</b>. So, from above example, w_3 was when <b>timestep</b> t=3; likewise with given <b>timestep</b> t, we can written as:. context <b>word</b>=w_(t-2) w_(t-1) w_(t+1) w_(t+2) \u2192two history and two future words from (the focus <b>word</b> w_(t)) current time t.focus <b>word</b>=w(t)It found out that by building a log-linear classifier with four future and four history words at the input got the best performance ...", "dateLastCrawled": "2022-01-18T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Guide to Text Classification and <b>Sentiment Analysis</b> | by Abhijit Roy ...", "url": "https://towardsdatascience.com/a-guide-to-text-classification-and-sentiment-analysis-2ab021796317", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-guide-to-text-classification-and-<b>sentiment-analysis</b>-2...", "snippet": "Motivation: Text Classification and <b>sentiment analysis</b> is a very common machine <b>learning</b> problem and is used in a lot of activities <b>like</b> product predictions, movie recommendations, and several others. Currently, for <b>every</b> machine learner <b>new</b> to this field, <b>like</b> myself, exploring this domain has become very important. After exploring the topic, I felt, if I share my experience through an article, it may help some people trying to explore this field.", "dateLastCrawled": "2022-02-03T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Twitter Hashtag Prediction Project using Machine <b>learning</b> - TechVidvan", "url": "https://techvidvan.com/tutorials/twitter-hashtag-prediction-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://techvidvan.com/tutorials/twitter-hashtag-prediction-machine-<b>learning</b>", "snippet": "Twitter Hashtag Prediction Project using Machine <b>learning</b>. In this project, we will create a Machine <b>Learning</b> model that will analyze tweets about the weather forecast and then predict all of the hashtags associated with those tweets. The model will take tweets as input and separate each <b>word</b> of those tweets into three groups, namely ...", "dateLastCrawled": "2022-01-29T22:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Using model.predict() with your TensorFlow / Keras model \u2013 <b>MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2020/02/21/how-to-predict-new-samples-with-your-keras-model/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinecurve</b>.com/index.php/2020/02/21/how-to-predict-<b>new</b>-samples-with-your...", "snippet": "Training machine <b>learning</b> models can be awesome if they are accurate. However, you then also want to use them in production. But how to do so? The first step is often to allow the models to generate <b>new</b> predictions, for data that you \u2013 instead of Keras \u2013 feeds it. This blog zooms in on that particular topic. By providing a Keras based ...", "dateLastCrawled": "2022-01-30T18:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Use Timesteps in LSTM Networks for <b>Time Series Forecasting</b>", "url": "https://machinelearningmastery.com/use-timesteps-lstm-networks-time-series-forecasting/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/use-<b>timestep</b>s-lstm-networks-<b>time-series-forecasting</b>", "snippet": "The Long Short-Term Memory (LSTM) network in Keras supports time steps. This raises the question as to whether lag observations for a univariate time series can be used as time steps for an LSTM and whether or not this improves forecast performance. In this tutorial, we will investigate the use of lag observations as time steps in LSTMs models in Python. After completing", "dateLastCrawled": "2022-02-02T08:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Top 5 Examples to Implement of <b>SQL Timestamp</b> - EDUCBA", "url": "https://www.educba.com/sql-timestamp/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>sql-timestamp</b>", "snippet": "Timestamptz data type also has a storage size <b>similar</b> to a timestamp of 8 bytes that can accept date and time values ranging from 4713 BC and 294276 AD and provides a resolution of 1 microsecond or 14 digits. In this article, we will be <b>learning</b> the functions and differences of \u201ctimestamp\u201d and \u201ctimestamptz\u201d data types with the help of a few examples. Start Your Free Data Science Course. Hadoop, Data Science, Statistics &amp; others. Syntax and Parameters: The basic syntax of \u201ctimestamp ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Sequence Modeling with Neural Networks - Part I - DataScienceCentral.com", "url": "https://www.datasciencecentral.com/sequence-modeling-with-neural-networks-part-i/", "isFamilyFriendly": true, "displayUrl": "https://www.datasciencecentral.com/sequence-modeling-with-neural-networks-part-i", "snippet": "The fact of not having <b>new</b> parameters for <b>every</b> point of the sequence also helps us deal with variable-length sequences. In case of a sequence that has a length of 4, we could unroll this RNN to four timesteps. In other cases, we can unroll it to ten timesteps since the length of the sequence is not prespecified in the algorithm. By unrolling we simply mean that we write out the network for the complete sequence. For example, if the sequence we care about is a sentence of 5 words, the ...", "dateLastCrawled": "2022-01-30T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "RStudio AI Blog: <b>Time Series Forecasting with Recurrent Neural Networks</b>", "url": "https://blogs.rstudio.com/ai/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2017-12-20-<b>time-series-forecasting-with-recurrent</b>...", "snippet": "steps = 6 \u2014 Observations will be sampled at <b>one</b> data point per hour. delay = 144 \u2014 Targets will be 24 hours in the future. To get started, you need to do two things: Preprocess the data to a format a neural network can ingest. This is easy: the data is already numerical, so you don\u2019t need to do any vectorization. But each time series in the data is on a different scale (for example, temperature is typically between -20 and +30, but atmospheric pressure, measured in mbar, is around ...", "dateLastCrawled": "2022-01-29T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Word Embedding: CBOW &amp; Skip-gram</b>. Hi Guys! In this blog, I have written ...", "url": "https://medium.datadriveninvestor.com/word-embedding-cbow-skip-gram-8262e22fa7c", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/<b>word-embedding-cbow-skip-gram</b>-8262e22fa7c", "snippet": "Introduce techniques that can be used for <b>learning</b> high-quality <b>word</b> vectors from high data set of billions of words and with millions of words in the vocabulary. For measuring the quality of the resulting vector representations, with the expectation that not only <b>similar</b> words tends to have closer to each other but also have multiple degree of similarity[1]. For example, the male/female relationship is automatically learned, and with the induced vector representations, \u201cKing - Man + Woman ...", "dateLastCrawled": "2022-01-18T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Build your LSTM language model with Tensorflow | by MilkKnight | Medium", "url": "https://medium.com/@MilkKnight/build-your-lstm-language-model-with-tensorflow-3416142c9919", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@MilkKnight/build-your-lstm-language-model-with-tensorflow-3416142c9919", "snippet": "<b>One</b> thing important is that you need to tell the begin and the end of a sentence to utilize the information of <b>every</b> <b>word</b> in <b>one</b> sentence entirely. As you can see in Fig.1, for sequence \u201c1 2605 ...", "dateLastCrawled": "2022-02-03T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Text generation with an RNN</b> | TensorFlow", "url": "https://www.tensorflow.org/text/tutorials/text_generation", "isFamilyFriendly": true, "displayUrl": "https://www.tensorflow.org/text/tutorials/text_generation", "snippet": "BISHOP OF ELY: Marry, and will, my lord, to weep in such a <b>one</b> were prettiest; Yet now I was adopted heir Of the world&#39;s lamentable <b>day</b>, To watch the next way with his father with his face? ESCALUS: The cause why then we are all resolved more sons. VOLUMNIA: O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead, And love and pale as any will to that <b>word</b>. QUEEN ELIZABETH: But how long have I heard the soul for this world, And show ...", "dateLastCrawled": "2022-01-30T11:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "MathBot A Deep <b>Learning</b> based Elementary School Math <b>Word</b> Problem Solver", "url": "https://cs230.stanford.edu/projects_fall_2019/reports/26262150.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs230.stanford.edu/projects_fall_2019/reports/26262150.pdf", "snippet": "<b>Similar</b> to the way we teach our elementary school kids to learn to solve natural language based math <b>word</b> problems, we should be able to train a machine <b>learning</b> system to solve same problems. These elementary school <b>word</b> prob-lems involve <b>one</b> or more algebraic equations comprising of any combination of four arithmetic operations, namely", "dateLastCrawled": "2022-01-31T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Techniques to Handle Very <b>Long Sequences</b> with LSTMs", "url": "https://machinelearningmastery.com/handle-long-sequences-long-short-term-memory-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/handle-<b>long-sequences</b>-long-short-term-memory...", "snippet": "Long Short-Term Memory or LSTM recurrent neural networks are capable of <b>learning</b> and remembering over <b>long sequences</b> of inputs. LSTMs work very well if your problem has <b>one</b> output for <b>every</b> input, like time series forecasting or text translation. But LSTMs can be challenging to use when you have very long input sequences and only <b>one</b> or a handful of outputs. This", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[R] You May <b>Not Need Attention (summary + PyTorch code in</b> comments ...", "url": "https://www.reddit.com/r/MachineLearning/comments/9t88jj/r_you_may_not_need_attention_summary_pytorch_code/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/9t88jj/r_you_may_not_need_attention...", "snippet": "A recurrent language model receives at <b>every</b> <b>timestep</b> the current input <b>word</b> and has to predict the next <b>word</b> in the dataset. To translate with such a model, simply give it the current <b>word</b> from the source sentence and have it try to predict the next <b>word</b> from the target sentence. Obviously, in many cases such a simple model wouldn&#39;t work. For example, if your sentence was &quot;The white dog&quot; and you wanted to translate to Spanish (&quot;El perro blanco&quot;), at the 2nd <b>timestep</b>, the input would be ...", "dateLastCrawled": "2021-08-22T01:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Use Timesteps in LSTM Networks for <b>Time Series Forecasting</b>", "url": "https://machinelearningmastery.com/use-timesteps-lstm-networks-time-series-forecasting/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/use-<b>timestep</b>s-lstm-networks-<b>time-series-forecasting</b>", "snippet": "E.g. I have a dataset with time interval of <b>every</b> 15 mins. If I set <b>timestep</b> to 96(1 <b>Day</b>) and built a LSTM model then I cannot forecast on test(1 Month) set since I get only (2880/96 = ) 30 values and not 2880 values.", "dateLastCrawled": "2022-02-02T08:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning to generate lyrics and music with Recurrent Neural Networks</b>", "url": "http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "warmspringwinds.github.io/pytorch/rnns/2018/01/27/<b>learning-to-generate-lyrics-and</b>...", "snippet": "Now, let\u2019s try to see the similarities between the character-level model and our <b>new</b> task. In the current case, we will have to predict the pitches that will be played on the next <b>timestep</b>, given all the previously played pitches. So, if you look at the picture of the piano roll, each column represents some kind of a musical character and given all the previous musical characters, we want to predict the next <b>one</b>. Let\u2019s pay attention to the difference between a text character and a ...", "dateLastCrawled": "2022-01-30T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Vanilla <b>Recurrent Neural Network</b> - Machine <b>Learning</b> Notebook", "url": "https://calvinfeng.gitbook.io/machine-learning-notebook/supervised-learning/recurrent-neural-network/recurrent_neural_networks/", "isFamilyFriendly": true, "displayUrl": "https://calvinfeng.gitbook.io/machine-<b>learning</b>-notebook/supervised-<b>learning</b>/recurrent...", "snippet": "This <b>can</b> <b>be thought</b> of as image captioning. We have <b>one</b> image as a fixed size input and the. output <b>can</b> be words or sentences which are variable in length. Many-to-<b>one</b>. This is used for sentiment classification. The input is expected to be a sequence of words or. even paragraphs of words. The output <b>can</b> be a regression output with continuous values which. represent the likelihood of having a positive sentiment. Many-to-many. This model is ideal for machine translation like the <b>one</b> we see on ...", "dateLastCrawled": "2022-01-23T23:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ocr deep <b>learning</b> | TheAILearner", "url": "https://theailearner.com/tag/ocr-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://theailearner.com/tag/ocr-deep-<b>learning</b>", "snippet": "<b>One</b> thing we <b>can</b> do is devise a rule like \u201c<b>one</b> character corresponds to some fixed time steps\u201d. For instance, for the above image, if we have 10 timesteps, then we <b>can</b> repeat \u201cState\u201d as \u201cS, S, T, T, A, A, T, T, E, E\u201d (repeat each character twice) and then train the network. But this approach <b>can</b> be easily violated for different fonts, writing styles, etc. Approach 2. Another approach <b>can</b> be to manually annotate the data for each <b>time step</b> as shown below. Then train the network ...", "dateLastCrawled": "2022-01-31T05:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Casual Intro to Reinforcement <b>Learning</b> | by Alif Ilham Madani | Towards ...", "url": "https://towardsdatascience.com/casual-intro-to-reinforcement-learning-4a78b57d4686", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/casual-intro-to-reinforcement-<b>learning</b>-4a78b57d4686", "snippet": "Core Concepts. There are several aspects that we must know when understanding reinforcement <b>learning</b>. They are environment, reward signal, and agent.An agent also includes agent state, policy, and value function.. The interaction between the agent and the environment <b>can</b> be described as follows.At each <b>timestep</b>, the agent receives observation and reward, while executing an action.On the other hand, the environment receives an action and then emits observation and reward.. A reward is a ...", "dateLastCrawled": "2021-11-26T23:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "MathBot A Deep <b>Learning</b> based Elementary School Math <b>Word</b> Problem Solver", "url": "https://cs230.stanford.edu/projects_fall_2019/reports/26262150.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs230.stanford.edu/projects_fall_2019/reports/26262150.pdf", "snippet": "time <b>every</b> <b>day</b> to check their kids\u2019 homework and tests. We <b>thought</b> that MathBot will be of immense help to the par-ents, if it <b>can</b> scan the kids\u2019 <b>word</b> problem and output an equation and \ufb01nal solution. Further, as mentioned by (Zhang et al.,2019), designing an automatic solver for mathematical <b>word</b> problems has", "dateLastCrawled": "2022-01-31T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Object Recognition Using Deep Learning</b> \u2013 IJERT", "url": "https://www.ijert.org/object-recognition-using-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/<b>object-recognition-using-deep-learning</b>", "snippet": "Without an external supervisory signal, it <b>can</b> be more ambiguous as to how to specify the Objective function, and unsupervised <b>learning</b> is currently <b>one</b> of the big open problems in ML [14]. <b>One</b> of the common training objectives of UL is found in an Auto-Encoder, in which the target is the same as the input, and the <b>learning</b> algorithm tries to learn how to compress and decompress each training example with minimal loss. If successful, it finds regularities in the training data, and learns ...", "dateLastCrawled": "2022-02-03T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "step <b>controls in transient structural analysis</b> ... - Ansys <b>Learning</b> Forum", "url": "https://forum.ansys.com/discussion/839/step-controls-in-transient-structural-analysis-sittings", "isFamilyFriendly": true, "displayUrl": "https://forum.ansys.com/discussion/839/step-<b>controls-in-transient-structural-analysis</b>...", "snippet": "Issue 1) There is a huge 10 second gap in the data. Delete the first two lines and subtract 10 from the Time values unless you must specifically simulate the first 10 seconds, the acceleration history should begin at zero with no gaps. Issue 2) The mean acceleration is 0.05 m/s^2 for 200 ms.", "dateLastCrawled": "2022-01-31T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Can</b> <b>every</b> supervised <b>learning</b> problem be transformed to an equivalent ...", "url": "https://www.quora.com/Can-every-supervised-learning-problem-be-transformed-to-an-equivalent-unsupervised-reinforcement-learning-RL-problem", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Can</b>-<b>every</b>-supervised-<b>learning</b>-problem-be-transformed-to-an...", "snippet": "Answer (1 of 2): Your question is very important as a very fundamental issue in studying machine <b>learning</b> is to know if there is <b>one</b> basic modality of adaptation that is primary, or if there are several. Just as physicists believe that ultimately all of their field <b>can</b> be reduced down to the stu...", "dateLastCrawled": "2022-01-21T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Boy Meets World", "url": "https://thetimestep.blogspot.com/", "isFamilyFriendly": true, "displayUrl": "https://the<b>timestep</b>.blogspot.com", "snippet": "It&#39;s <b>one</b> of those tiny ways that <b>New</b> York not only makes you fall in love with it all over again, but it also reminds you of how lonely of a place it is. It just makes you feel better about yourself, because you know there are so many people just like you who try to paint the town, only to go home alone. It&#39;s a paint-by-numbers of the lonely 20-somethings of <b>New</b> York. I came back to the city a week ago today. I was disheartened to find that my usual stomach-flipping excitement at the first ...", "dateLastCrawled": "2021-12-04T10:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Use Timesteps in LSTM Networks for <b>Time Series Forecasting</b>", "url": "https://machinelearningmastery.com/use-timesteps-lstm-networks-time-series-forecasting/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/use-<b>timestep</b>s-lstm-networks-<b>time-series-forecasting</b>", "snippet": "The Long Short-Term Memory (LSTM) network in Keras supports time steps. This raises the question as to whether lag observations for a univariate time series <b>can</b> be used as time steps for an LSTM and whether or not this improves forecast performance. In this tutorial, we will investigate the use of lag observations as time steps in LSTMs models in Python. After completing", "dateLastCrawled": "2022-02-02T08:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Guide to Text Classification and <b>Sentiment Analysis</b> | by Abhijit Roy ...", "url": "https://towardsdatascience.com/a-guide-to-text-classification-and-sentiment-analysis-2ab021796317", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-guide-to-text-classification-and-<b>sentiment-analysis</b>-2...", "snippet": "Now, these weights get updated at <b>every</b> <b>timestep</b> with <b>every</b> <b>word</b>, and after the 10th <b>word</b> or timestamp, the final timestamp in our case the model has gone through all the words in the samples, so we get a matrix of size 16 x 64, which is basically the weight values of the 64 internal nodes corresponding to each sample. But, what we don\u2019t see are the weight matrices of the gates which are also optimized. These 64 values in a row basically represent the weights of an individual sample in the ...", "dateLastCrawled": "2022-02-03T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Word Embedding: CBOW &amp; Skip-gram</b>. Hi Guys! In this blog, I have written ...", "url": "https://medium.datadriveninvestor.com/word-embedding-cbow-skip-gram-8262e22fa7c", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/<b>word-embedding-cbow-skip-gram</b>-8262e22fa7c", "snippet": "Figure 1. <b>Word</b> notation. Focus <b>word</b> are those <b>word</b> (output) that this model need to predict from the given context <b>word</b>. So, from above example, w_3 was when <b>timestep</b> t=3; likewise with given <b>timestep</b> t, we <b>can</b> written as:. context <b>word</b>=w_(t-2) w_(t-1) w_(t+1) w_(t+2) \u2192two history and two future words from (the focus <b>word</b> w_(t)) current time t.focus <b>word</b>=w(t)It found out that by building a log-linear classifier with four future and four history words at the input got the best performance ...", "dateLastCrawled": "2022-01-18T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "RStudio AI Blog: <b>Time Series Forecasting with Recurrent Neural Networks</b>", "url": "https://blogs.rstudio.com/ai/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2017-12-20-<b>time-series-forecasting-with-recurrent</b>...", "snippet": "steps = 6 \u2014 Observations will be sampled at <b>one</b> data point per hour. delay = 144 \u2014 Targets will be 24 hours in the future. To get started, you need to do two things: Preprocess the data to a format a neural network <b>can</b> ingest. This is easy: the data is already numerical, so you don\u2019t need to do any vectorization. But each time series in the data is on a different scale (for example, temperature is typically between -20 and +30, but atmospheric pressure, measured in mbar, is around ...", "dateLastCrawled": "2022-01-29T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "All You Need to Know About Automatic Speech Recognition Transcription ...", "url": "https://www.rev.com/blog/guide-to-speech-recognition-transcription-models", "isFamilyFriendly": true, "displayUrl": "https://www.rev.com/blog/guide-to-speech-recognition-transcription-models", "snippet": "Acoustic models also differ between languages. An acoustic model used for English, for example, <b>can</b>\u2019t be used for German. However, if the two languages have some vocal similarities, as these two do, then engineers <b>can</b> use a technique called transfer <b>learning</b> to take the original model and transfer it to the <b>new</b> language.. This process involves taking the pretrained weights from the original model and fine tuning them on a <b>new</b> dataset in the target language.Transfer <b>learning</b> is a relatively ...", "dateLastCrawled": "2022-02-02T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine translation</b> with the <b>seq2seq</b> model: Different approaches | by ...", "url": "https://towardsdatascience.com/machine-translation-with-the-seq2seq-model-different-approaches-f078081aaa37", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-translation</b>-with-the-<b>seq2seq</b>-model-different...", "snippet": "<b>Machine translation</b> is a computational linguistics sub-field that examines how software is used to t r anslate text or speech from <b>one</b> language to another. MT performs mechanical substitution of words in <b>one</b> language for words in another on a simple level, but this alone rarely yields effective translation since it involves comprehension of entire sentences and their nearest counterparts in the target language.", "dateLastCrawled": "2022-02-03T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Learning</b> a Natural Language Interface with Neural Programmer | DeepAI", "url": "https://deepai.org/publication/learning-a-natural-language-interface-with-neural-programmer", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning</b>-a-natural-language-interface-with-neural...", "snippet": "We combine the models by averaging the predicted softmax distributions of the models at <b>every</b> <b>timestep</b>. While it is generally believed that neural network models require a large number of training examples <b>compared</b> to simpler linear models to get good performance, our model achieves competitive performance on this small dataset containing only 10,000 examples with weak supervision.", "dateLastCrawled": "2022-01-30T14:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning to generate lyrics and music with Recurrent Neural Networks</b>", "url": "http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "warmspringwinds.github.io/pytorch/rnns/2018/01/27/<b>learning-to-generate-lyrics-and</b>...", "snippet": "For example, pay attention on how you read any <b>word</b>: you start with the first letter, which is usually hard to predict, but as you reach the end of a <b>word</b> you <b>can</b> sometimes guess the next letter. When you read any <b>word</b> you are implicitly using some rules which you learned by reading other texts: for example, with each additional letter that you read from a <b>word</b>, the probability of a space character increases (really long words are rare) or the probability of any consonant after the letter ...", "dateLastCrawled": "2022-01-30T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Update LSTM Networks During Training <b>for Time Series Forecasting</b>", "url": "https://machinelearningmastery.com/update-lstm-networks-training-time-series-forecasting/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/update-lstm-networks-training-<b>time-series-forecasting</b>", "snippet": "For example, 2 update epochs for each test pattern could <b>be compared</b> to a fixed model trained for 500 + (12-1) * 2) or 522 epochs, an update model 5 <b>compared</b> to a fixed model fit for 500 + (12-1) * 5) or 555 epochs, and so on. Completely <b>New</b> Model. Add an experiment where a <b>new</b> model is fit after each test pattern is added to the training ...", "dateLastCrawled": "2022-02-03T09:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[<b>D] TensorFlow vs. PyTorch: why is dynamic better</b>?", "url": "https://www.reddit.com/r/MachineLearning/comments/5otdwl/d_tensorflow_vs_pytorch_why_is_dynamic_better/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/5otdwl/<b>d_tensorflow_vs_pytorch_why</b>...", "snippet": "Perhaps the biggest advantage of all is instead of essentially <b>learning</b> a <b>new</b> language (the &quot;graph construction&quot; language), you <b>can</b> use the standard control and computation structures from your favorite language. This gives dynamic toolkits a big headstart on the <b>learning</b> curve, in addition to all the previously mentioned advantages. 6. Share. Report Save. level 1 \u00b7 5y \u00b7 edited 5y. In my mind, the best example of a model that needs dynamic graph are neural module networks [1, 2]. These ...", "dateLastCrawled": "2021-08-09T13:03:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Multistep Time Series Forecasting with</b> LSTMs in Python", "url": "https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>multi-step-time-series-forecasting</b>-long-short-term...", "snippet": "How to Setup a Python Environment for <b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> with Anaconda; Next, let\u2019s take a look at a standard <b>time series forecasting</b> problem that we can use as context for this experiment. Need help with Deep <b>Learning</b> for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course . Shampoo Sales Dataset. This dataset describes the monthly number of sales of ...", "dateLastCrawled": "2022-02-02T18:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "noc20 cs50 assigment 12 - NPTEL", "url": "https://nptel.ac.in/content/storage2/courses/downloads_new/106106184/noc20_cs50_assigment_12.pdf", "isFamilyFriendly": true, "displayUrl": "https://nptel.ac.in/content/storage2/courses/downloads_new/106106184/noc20_cs50...", "snippet": "NPTEL \u00bb Deep <b>Learning</b> - Part 1 Unit 13 - Week 11 Course outline How does an NPTEL online course work? Week O Week 1 week 2 ... current state of the network at <b>timestep</b> i sigmoid at <b>timestep</b> previous state of the network at <b>timestep</b> i next state of the network at <b>timestep</b> i No, the answer is incorrect. Score: 0 Accepted Answers: current state of the network at <b>timestep</b> i 8) Which neural network architecture would be suitable to find the nth character given the character sequence of length n ...", "dateLastCrawled": "2022-01-25T10:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A distributed <b>machine learning</b> approach that trains <b>machine learning</b> models using decentralized examples residing on devices such as smartphones. In federated <b>learning</b>, a subset of devices downloads the current model from a central coordinating server. The devices use the examples stored on the devices to make improvements to the model. The devices then upload the model improvements (but not the training examples) to the coordinating server, where they are aggregated with other updates to ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Interpretability in <b>Machine</b> <b>Learning</b>: An Overview", "url": "https://thegradient.pub/interpretability-in-ml-a-broad-overview/", "isFamilyFriendly": true, "displayUrl": "https://thegradient.pub/interpretability-in-ml-a-broad-overview", "snippet": "First, interpretability in <b>machine</b> <b>learning</b> is useful because it can aid in trust. As humans, we may be reluctant to rely on <b>machine</b> <b>learning</b> models for certain critical tasks, e.g., medical diagnosis, unless we know &quot;how they work.&quot; There&#39;s often a fear of the unknown when trusting in something opaque, which we see when people confront new technology, and this can slow down adoption. Approaches to interpretability that focus on transparency could help mitigate some of these fears.", "dateLastCrawled": "2022-02-01T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Using Analog For AI | Alchip</b> Technologies, Limited", "url": "https://www.alchip.com/alchip-in-the-news/using-analog-for-ai/", "isFamilyFriendly": true, "displayUrl": "https://www.alchip.com/alchip-in-the-news/<b>using-analog-for-ai</b>", "snippet": "As <b>machine</b> <b>learning</b> applications spread out, more energy efficient adaptive mixed-signal analog-front devices will be needed.\u201d Could analog help? It has been proven that AI functions can be performed using orders of magnitude less power and that it is capable of solving problems far more complex than AI systems currently being developed. That example is the mammalian brain. Even the most power hungry, the human brain, only consumes about 25W. The power consumption of a TPU is likely ...", "dateLastCrawled": "2022-01-31T14:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Everything you need to know about <b>Graph Theory</b> for Deep <b>Learning</b> | by ...", "url": "https://towardsdatascience.com/graph-theory-and-deep-learning-know-hows-6556b0e9891b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>graph-theory</b>-and-deep-<b>learning</b>-know-hows-6556b0e9891b", "snippet": "An easy way to think about it is using an <b>analogy</b> to names, characters, and people: a node is a person, a node\u2019s label is a person\u2019s name, and the node\u2019s features are the person\u2019s characteristics. Graphs can be directed or undirected: Note that directed graphs can have undirected edges too. A node in the graph can even have an edge that points/connects to itself. This is known as a self-loop. Graphs can be either: Heterogeneous \u2014 composed of different types of nodes; Homogeneous ...", "dateLastCrawled": "2022-02-03T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Computing Time Part (I): Recurrent Neural Networks</b> \u2013 The Beauty of ...", "url": "https://thebeautyofml.wordpress.com/2017/01/01/computing-time-part-i-recurrent-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://thebeautyofml.wordpress.com/2017/01/01/<b>computing-time-part-i-recurrent-neural</b>...", "snippet": "Nothing will surprise you more than recurrent nets if you practice <b>machine</b> <b>learning</b>. Recurrent net is the most powerful, successful and the luckiest neural network ever. Today\u2019s research in deep <b>learning</b> relies heavily on recurrent nets, although they are not recognized as deep <b>learning</b> techniques. The history of recurrent nets returns back to 1980s but only saw this renaissance with the rise of deep <b>learning</b> and deep neural networks. Introduction. Before introducing how recurrent neural ...", "dateLastCrawled": "2022-01-23T05:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Learning</b> an Internal Dynamics Model from Control Demonstration", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3929129/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3929129", "snippet": "Due to sensory feedback delay, the feedback available at <b>timestep</b> t represents the plant state at <b>timestep</b> t \u2212 \u03c4, where \u03c4 is the feedback delay. To predict the current plant state, the subject can use f 1 as a forward model, propagating y t\u2212\u03c4 (or a noise-corrupted function of it) forward in time using knowledge of the plant dynamics and previously issued controls u t\u2212\u03c4, \u2026, u t\u22121.In general, the subject\u2019s internal beliefs {x t} may be inconsistent with the actual plant states ...", "dateLastCrawled": "2017-01-01T08:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>learning</b> molecular dynamics for the simulation of infrared ...", "url": "https://pubs.rsc.org/en/content/articlelanding/2017/sc/c7sc02267k#!", "isFamilyFriendly": true, "displayUrl": "https://pubs.rsc.org/en/content/articlelanding/2017/sc/c7sc02267k", "snippet": "1 Introduction . <b>Machine</b> <b>learning</b> (ML) \u2013 the science of autonomously <b>learning</b> complex relationships from data \u2013 has experienced an immensely successful resurgence during the last decade. 1,2 Increasingly powerful ML algorithms form the basis of a wealth of fascinating applications, with image and speech recognition, search engines or even self-driving cars being only a few examples. In a similar manner, ML based techniques have lead to several exciting developments in the field of ...", "dateLastCrawled": "2022-02-03T01:13:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Novel Re-<b>weighting Method for Connectionist Temporal Classification</b> ...", "url": "https://deepai.org/publication/a-novel-re-weighting-method-for-connectionist-temporal-classification", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-novel-re-<b>weighting-method-for-connectionist-temporal</b>...", "snippet": "The connectionist temporal classification (CTC) enables end-to-end sequence <b>learning</b> by maximizing the probability of correctly recognizing sequences during training. With an extra blank class, the CTC implicitly converts recognizing a sequence into classifying each timestep within the sequence. But the CTC loss is not intuitive for such classification task, so the class imbalance within each sequence, caused by the overwhelming blank timesteps, is a knotty problem.", "dateLastCrawled": "2021-12-21T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Novel Re-weighting Method for <b>Connectionist Temporal Classification</b> ...", "url": "https://www.arxiv-vanity.com/papers/1904.10619/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1904.10619", "snippet": "The <b>connectionist temporal classification</b> (CTC) enables end-to-end sequence <b>learning</b> by maximizing the probability of correctly recognizing sequences during training. With an extra blank class, the CTC implicitly converts recognizing a sequence into classifying each timestep within the sequence. But the CTC loss is not intuitive for such classification task, so the class imbalance within each sequence, caused by the overwhelming blank timesteps, is a knotty problem. In this paper, we define ...", "dateLastCrawled": "2021-11-30T10:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Neural network augmented wave-equation simulation", "url": "https://slim.gatech.edu/Publications/Public/TechReport/2019/siahkoohi2019TRnna/siahkoohi2019TRnna.html", "isFamilyFriendly": true, "displayUrl": "https://slim.gatech.edu/Publications/Public/TechReport/2019/siahkoohi2019TRnna/...", "snippet": "We describe how we augment low-fidelity physics with <b>learning</b> techniques to handle incomplete and/or inaccurate physics, where the low-fidelity physics is modeled via finite-difference method with a poor discretization of the Laplacian. To ensure accuracy, the temporal and spatial discretization in high-fidelity wave-equation simulations have to be chosen very fine, typically one to two orders of magnitude smaller than Nyquist sampling rate. As mentioned earlier, we will utilize a poor ...", "dateLastCrawled": "2021-12-16T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "COWES: Web user clustering based on evolutionary web sessions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0169023X09000792", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0169023X09000792", "snippet": "Let us look at novel clusters which can be discovered based on evolutionary characteristics of web usage data in our motivating example in Fig. 2.Pages accessed in a web session can be organized into a hierarchical structure, called web session tree, based on the URLs of the pages .For example, Fig. 1b is the web session tree constructed for the pages in the web session shown in Fig. 1a. A web session tree represents the information needs of a user.", "dateLastCrawled": "2021-12-12T14:51:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(timestep)  is like +(learning one new word every day)", "+(timestep) is similar to +(learning one new word every day)", "+(timestep) can be thought of as +(learning one new word every day)", "+(timestep) can be compared to +(learning one new word every day)", "machine learning +(timestep AND analogy)", "machine learning +(\"timestep is like\")", "machine learning +(\"timestep is similar\")", "machine learning +(\"just as timestep\")", "machine learning +(\"timestep can be thought of as\")", "machine learning +(\"timestep can be compared to\")"]}