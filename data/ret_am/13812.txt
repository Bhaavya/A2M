{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Intuition behind <b>Log-loss</b> score. In <b>Machine</b> <b>Learning</b>, classification ...", "url": "https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/intuition-behind-<b>log-loss</b>-score-4e0c9979680a", "snippet": "A <b>model</b> with lower <b>log-loss</b> score is better than the one with higher <b>log-loss</b> score, provided both the models are applied to the same distribution of dataset. We cannot compare <b>log-loss</b> scores of two models applied on two different datasets. How to interpret <b>log-loss</b> score? Consider a sample of 10 emails with 9 hams.", "dateLastCrawled": "2022-01-29T07:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Log</b> <b>Loss</b> - <b>Logistic Regression</b>&#39;s Cost Function for Beginners", "url": "https://www.analyticsvidhya.com/blog/2020/11/binary-cross-entropy-aka-log-loss-the-cost-function-used-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.analyticsvidhya.com/b<b>log</b>/2020/11/binary-cross-entropy-aka-<b>log</b>-<b>loss</b>-the...", "snippet": "Take a <b>log</b> of corrected probabilities. Take the negative average of the values we get in the 2nd step. If we summarize all the above steps, we can use the formula:-. Here Yi represents the actual class and <b>log</b> (p (yi)is the probability of that class. p (yi) is the probability of 1. 1-p (yi) is the probability of 0.", "dateLastCrawled": "2022-02-03T15:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What\u2019s considered a good <b>Log</b> <b>Loss</b> in <b>Machine</b> <b>Learning</b> ? | by Federico ...", "url": "https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@fzammito/whats-considered-a-good-<b>log</b>-<b>loss</b>-in-<b>machine</b>-<b>learning</b>-a529...", "snippet": "In the case of the <b>Log</b> <b>Loss</b> metric, one usual \u201cwell-known\u201d metric is to say that 0.693 is the non-informative value. This figure is obtained by predicting p = 0.5 for any class of a binary ...", "dateLastCrawled": "2022-01-30T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "Types of <b>Loss Functions in Machine Learning</b>. Below are the different types of the <b>loss</b> function in <b>machine</b> <b>learning</b> which are as follows: 1. Regression <b>loss</b> functions. Linear regression is a fundamental concept of this function. Regression <b>loss</b> functions establish a linear relationship between a dependent variable (Y) and an independent ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to interpret <b>loss</b> and accuracy for a <b>machine</b> <b>learning</b> <b>model</b>", "url": "https://stackoverflow.com/questions/34518656/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/34518656", "snippet": "The lower the <b>loss</b>, the better a <b>model</b> (unless the <b>model</b> has over-fitted to the training data). The <b>loss</b> is calculated on training and validation and its interperation is how well the <b>model</b> is doing for these two sets. Unlike accuracy, <b>loss</b> is not a percentage. It is a summation of the errors made for each example in training or validation sets.", "dateLastCrawled": "2022-02-02T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> Functions in Neural Networks - theaidream.com", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5 Regression <b>Loss</b> Functions All <b>Machine</b> Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-<b>machine</b>-learners-should...", "snippet": "Choosing the right <b>loss</b> function for fitting a <b>machine</b> <b>learning</b> <b>model</b>. Get started. Open in app. Sign in. Get started . Follow. 11.3K Followers \u00b7 Data Science ML/DL MLOps Contribute Explore Comet. About. Get started. Open in app. 5 Regression <b>Loss</b> Functions All <b>Machine</b> Learners Should Know. Choosing the right <b>loss</b> function for fitting a <b>model</b>. Prince Grover. Jun 5, 2018 \u00b7 11 min read. All the algorithms in <b>machine</b> <b>learning</b> rely on minimizing or maximizing a function, which we call ...", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "sklearn.metrics.<b>log_loss</b> \u2014 scikit-learn 1.0.2 documentation", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.<b>log_loss</b>.html", "snippet": "sklearn.metrics.<b>log_loss</b>\u00b6 sklearn.metrics. <b>log_loss</b> (y_true, y_pred, *, eps = 1e-15, normalize = True, sample_weight = None, labels = None) [source] \u00b6 <b>Log loss</b>, aka logistic <b>loss</b> or cross-entropy <b>loss</b>. This is the <b>loss</b> function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative <b>log</b>-likelihood of a logistic <b>model</b> that returns y_pred probabilities for its training data y_true.The <b>log loss</b> is only defined for two or more labels.", "dateLastCrawled": "2022-02-02T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding binary <b>cross-entropy</b> / <b>log</b> <b>loss</b>: a visual explanation ...", "url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-binary-<b>cross-entropy</b>-<b>log</b>-<b>loss</b>-a-visual...", "snippet": "Binary <b>Cross-Entropy</b> / <b>Log</b> <b>Loss</b>. where y is the label (1 for green points and 0 for red points) and p(y) is the predicted probability of the point being green for all N points.. Reading this formula, it tells you that, for each green point (y=1), it adds <b>log</b>(p(y)) to the <b>loss</b>, that is, the <b>log</b> probability of it being green.Conversely, it adds <b>log</b>(1-p(y)), that is, the <b>log</b> probability of it being red, for each red point (y=0).Not necessarily difficult, sure, but no so intuitive too\u2026", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>What is an intuitive explanation for the log</b> <b>loss</b> function? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-for-the-log-loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-an-intuitive-explanation-for-the-log</b>-<b>loss</b>-function", "snippet": "Answer (1 of 8): To me an intuitive explanation is that minimizing the <b>log</b> <b>loss</b> equals minimizing the Kullback-Leibler divergence (Kullback\u2013Leibler divergence - Wikipedia) between the function you want to optimize (for example a neural network) and the true function that generates the data (from ...", "dateLastCrawled": "2022-01-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What\u2019s considered a good <b>Log</b> <b>Loss</b> in <b>Machine</b> <b>Learning</b> ? | by Federico ...", "url": "https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@fzammito/whats-considered-a-good-<b>log</b>-<b>loss</b>-in-<b>machine</b>-<b>learning</b>-a529...", "snippet": "<b>Log</b> <b>Loss</b> <b>is similar</b> to the Accuracy, but it will favor models that distinguish more strongly the classes. <b>Log</b> <b>Loss</b> it useful to compare models not only on their output but on their probabilistic ...", "dateLastCrawled": "2022-01-30T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding the <b>log</b> <b>loss function</b> | by Susmith Reddy | Analytics ...", "url": "https://medium.com/analytics-vidhya/understanding-the-loss-function-of-logistic-regression-ac1eec2838ce", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/understanding-the-<b>loss-function</b>-of-<b>log</b>istic...", "snippet": "Let\u2019s demystify \u201c<b>Log</b> <b>Loss Function</b>.\u201d. It is important to first understand the <b>log</b> function before jumping into <b>log</b> <b>loss</b>. If we plot y = <b>log</b> (x), the graph in quadrant II looks like this. y ...", "dateLastCrawled": "2022-01-30T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Cross-entropy <b>loss</b> is often simply referred to as \u201ccross-entropy,\u201d \u201clogarithmic <b>loss</b>,\u201d \u201clogistic <b>loss</b>,\u201d or \u201c<b>log</b> <b>loss</b>\u201d for short. Each predicted probability is compared to the actual class output value (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected value.", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction to <b>Loss</b> Functions - Algorithmia Blog", "url": "https://www.algorithmia.com/blog/introduction-to-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://www.algorithmia.com/b<b>log</b>/introduction-to-<b>loss</b>-functions", "snippet": "<b>Log</b> <b>loss</b> (cross entropy <b>loss</b>) ... \u201c<b>Loss</b> functions are a key part of any <b>machine</b> <b>learning</b> <b>model</b>: they define an objective against which the performance of your <b>model</b> is measured, and the setting of weight parameters learned by the <b>model</b> is determined by minimizing a chosen <b>loss function</b>. There are several different common <b>loss</b> functions to choose from: the cross-entropy <b>loss</b>, the mean-squared <b>error</b>, the huber <b>loss</b>, and the hinge <b>loss</b> \u2013 just to name a few. \u201d Some Thoughts About The ...", "dateLastCrawled": "2022-02-03T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss Function</b> (Part II): <b>Logistic Regression</b> | by Shuyu Luo | Towards ...", "url": "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/optimization-<b>loss-function</b>-under-the-hood-part-ii-d20a...", "snippet": "The <b>loss function</b> of <b>logistic regression</b> is doing this exactly which is called Logistic <b>Loss</b>. See as below. If y = 1, looking at the plot below on left, when prediction = 1, the cost = 0, when prediction = 0, the <b>learning</b> algorithm is punished by a very large cost. Similarly, if y = 0, the plot on right shows, predicting 0 has no punishment but predicting 1 has a large value of cost.", "dateLastCrawled": "2022-02-02T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "8 popular Evaluation Metrics for <b>Machine</b> <b>Learning</b> Models - Just into Data", "url": "https://www.justintodata.com/machine-learning-model-evaluation-metrics/", "isFamilyFriendly": true, "displayUrl": "https://www.justintodata.com/<b>machine</b>-<b>learning</b>-<b>model</b>-evaluation-metrics", "snippet": "<b>log</b> <b>loss</b> = \u2013 <b>log</b>(P(y|p)) = \u2013 (y*<b>log</b>(p) + (1-y)*<b>log</b>(1-p)) It\u2019s not as intuitive to understand compared to other metrics, but the smaller this function, the better the <b>model</b>. For a more clear explanation, check out the logistic regression article below. Further Reading: Logistic Regression for <b>Machine</b> <b>Learning</b>: complete Tutorial", "dateLastCrawled": "2022-02-03T07:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "<b>Loss</b> functions are a key part of any <b>machine</b> <b>learning</b> <b>model</b>: they define an objective against which the performance of your <b>model</b> is measured, and the setting of weight parameters learned by the <b>model</b> is determined by minimizing a chosen <b>loss</b> function. There are several different common <b>loss</b> functions to choose from: the cross-entropy <b>loss</b>, the mean-squared <b>error</b>, the huber <b>loss</b>, and the hinge <b>loss</b> - just to name a few. Given a particular <b>model</b>, each <b>loss</b> function has particular properties ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "5 Regression <b>Loss</b> Functions All <b>Machine</b> Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-<b>machine</b>-learners-should...", "snippet": "Choosing the right <b>loss</b> function for fitting a <b>machine</b> <b>learning</b> <b>model</b>. Get started. Open in app. Sign in. Get started. Follow. 11.3K Followers \u00b7 Data Science ML/DL MLOps Contribute Explore Comet. About. Get started. Open in app. 5 Regression <b>Loss</b> Functions All <b>Machine</b> Learners Should Know. Choosing the right <b>loss</b> function for fitting a <b>model</b>. Prince Grover. Jun 5, 2018 \u00b7 11 min read. All the algorithms in <b>machine</b> <b>learning</b> rely on minimizing or maximizing a function, which we call ...", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Training Loss</b> and Validation <b>Loss</b> in Deep <b>Learning</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/48226086/training-loss-and-validation-loss-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/48226086", "snippet": "In <b>machine</b> <b>learning</b> and deep <b>learning</b> there are basically three cases. 1) Underfitting. This is the only case where <b>loss</b> &gt; validation_<b>loss</b>, but only slightly, if <b>loss</b> is far higher than validation_<b>loss</b>, please post your code and data so that we can have a look at. 2) Overfitting. <b>loss</b> &lt;&lt; validation_<b>loss</b>.", "dateLastCrawled": "2022-02-03T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the relationship between the <b>accuracy</b> and the <b>loss</b> in deep ...", "url": "https://datascience.stackexchange.com/questions/42599/what-is-the-relationship-between-the-accuracy-and-the-loss-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/42599", "snippet": "I have created three different models using deep <b>learning</b> for multi-class classification and each <b>model</b> gave me a different <b>accuracy</b> and <b>loss</b> value. The results of the testing <b>model</b> as the following: First <b>Model</b>: <b>Accuracy</b>: 98.1% <b>Loss</b>: 0.1882. Second <b>Model</b>: <b>Accuracy</b>: 98.5% <b>Loss</b>: 0.0997. Third <b>Model</b>: <b>Accuracy</b>: 99.1% <b>Loss</b>: 0.2544. My questions are:", "dateLastCrawled": "2022-01-26T03:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - <b>Log Loss</b> function in scikit-learn returns different ...", "url": "https://stats.stackexchange.com/questions/321333/log-loss-function-in-scikit-learn-returns-different-values", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/321333/<b>log-loss</b>-function-in-scikit-learn...", "snippet": "$\\begingroup$ My confusion came from how scikit learn handles encodings. It must use hashes to encode the labels. The ordering of hashed labels would the same regardless of the first samples label. To be specific, if the class of the first sample in the test set differs from that in the training set, the encoding of the classes remains the same, so the <b>log loss</b> of the test set and the train set will be comparable.", "dateLastCrawled": "2022-01-26T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How to interpret <b>loss</b> and accuracy for a <b>machine</b> <b>learning</b> <b>model</b>", "url": "https://stackoverflow.com/questions/34518656/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/34518656", "snippet": "The lower the <b>loss</b>, the better a <b>model</b> (unless the <b>model</b> has over-fitted to the training data). The <b>loss</b> is calculated on training and validation and its interperation is how well the <b>model</b> is doing for these two sets. Unlike accuracy, <b>loss</b> is not a percentage. It is a summation of the errors made for each example in training or validation sets.", "dateLastCrawled": "2022-02-02T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding binary <b>cross-entropy</b> / <b>log</b> <b>loss</b>: a visual explanation ...", "url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-binary-<b>cross-entropy</b>-<b>log</b>-<b>loss</b>-a-visual...", "snippet": "Binary <b>Cross-Entropy</b> / <b>Log</b> <b>Loss</b>. where y is the label (1 for green points and 0 for red points) and p(y) is the predicted probability of the point being green for all N points.. Reading this formula, it tells you that, for each green point (y=1), it adds <b>log</b>(p(y)) to the <b>loss</b>, that is, the <b>log</b> probability of it being green.Conversely, it adds <b>log</b>(1-p(y)), that is, the <b>log</b> probability of it being red, for each red point (y=0).Not necessarily difficult, sure, but no so intuitive too\u2026", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Concepts</b> | <b>Machine</b> <b>Learning</b>", "url": "https://m-clark.github.io/introduction-to-machine-learning/concepts.html", "isFamilyFriendly": true, "displayUrl": "https://m-clark.github.io/introduction-to-<b>machine</b>-<b>learning</b>/<b>concepts</b>.html", "snippet": "If you\u2019re not familiar, deviance <b>can</b> conceptually <b>be thought</b> of as the GLM version of residual variance. This <b>loss</b> is equivalent to binomial <b>log</b> likelihood when \\(y\\) is on the 0-1 scale. Exponential. Exponential <b>loss</b> is yet another <b>loss</b> function at our disposal. \\[L(Y, f(X)) = \\sum e^{-yf}\\] Hinge <b>Loss</b>. A final <b>loss</b> function to consider, typically used with support vector machines, is the hinge <b>loss</b> function. \\[L(Y, f(X)) = \\max(1-yf, 0)\\] Here negative values of \\(yf\\) are ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How <b>to Diagnose Overfitting and Underfitting of</b> LSTM Models", "url": "https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/diagnose-overfitting-underfitting-lstm-<b>models</b>", "snippet": "For example, if your <b>model</b> was compiled to optimize the <b>log</b> <b>loss</b> (binary_crossentropy) and measure accuracy each epoch, then the <b>log</b> <b>loss</b> and accuracy will be calculated and recorded in the history trace for each training epoch.Each score is accessed by a key in the history object returned from calling fit().By default, the <b>loss</b> optimized when fitting the <b>model</b> is called \u201c<b>loss</b>\u201d and accuracy is called \u201cacc\u201c.", "dateLastCrawled": "2022-02-02T23:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>loss</b>-functions-and-optimization-algorithms...", "snippet": "The choice of Optimisation Algorithms and <b>Loss</b> Functions for a deep <b>learning</b> <b>model</b> <b>can</b> play a big role in producing optimum and faster results. Before we begin, let us see how different components ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - Does KNN have a <b>loss function</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/420416/does-knn-have-a-loss-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/420416/does-knn-have-a-<b>loss-function</b>", "snippet": "2 Answers2. Show activity on this post. k -NN does not have a <b>loss function</b> that <b>can</b> be minimized during training. In fact, this algorithm is not trained at all. The only &quot;training&quot; that happens for k -NN, is memorising the data (creating a local copy), so that during prediction you <b>can</b> do a search and majority vote.", "dateLastCrawled": "2022-02-01T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>machine</b> <b>learning</b> <b>model</b> to identify early stage symptoms of SARS-Cov-2 ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305929/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7305929", "snippet": "This <b>can</b> be challenging given the delay in symptom presentation; however, <b>machine</b> <b>learning</b> algorithms provide a promising approach to address this problem that <b>can</b> be rapidly and cheaply applied in a pandemic situation. In our study, we developed and tested a range of <b>machine</b> <b>learning</b> approaches and found the most significant clinical COVID-19 predictive features were (in descending order): lung infection, cough, pneumonia, runny nose, travel history, fever, isolation, age, muscle soreness ...", "dateLastCrawled": "2022-02-02T13:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "<b>Loss</b> functions are a key part of any <b>machine</b> <b>learning</b> <b>model</b>: they define an objective against which the performance of your <b>model</b> is measured, and the setting of weight parameters learned by the <b>model</b> is determined by minimizing a chosen <b>loss</b> function. There are several different common <b>loss</b> functions to choose from: the cross-entropy <b>loss</b>, the mean-squared <b>error</b>, the huber <b>loss</b>, and the hinge <b>loss</b> - just to name a few. Given a particular <b>model</b>, each <b>loss</b> function has particular properties ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[<b>MCQ&#39;s] Machine Learning - Last Moment Tuitions</b>", "url": "https://lastmomenttuitions.com/mcqs-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://lastmomenttuitions.com/<b>mcqs-machine-learning</b>", "snippet": "Newton\u2019s method is seldom used in <b>machine</b> <b>learning</b> because a. common <b>loss</b> functions are not self-concordant b. Newton\u2019s method does not work well on noisy data c. <b>machine</b> <b>learning</b> researchers don\u2019t really understand linear algebra d. it is generally not practical to form or store the Hessian in such problems, due to large problem size answer : d. <b>Learning</b> with Regression and trees. Module 04. 1. In practice, Line of best fit or regression line is found when _____ a) Sum of residuals ...", "dateLastCrawled": "2022-02-03T02:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "Types of <b>Loss Functions in Machine Learning</b>. Below are the different types of the <b>loss</b> function in <b>machine</b> <b>learning</b> which are as follows: 1. Regression <b>loss</b> functions. Linear regression is a fundamental concept of this function. Regression <b>loss</b> functions establish a linear relationship between a dependent variable (Y) and an independent ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding the <b>log</b> <b>loss function</b> | by Susmith Reddy | Analytics ...", "url": "https://medium.com/analytics-vidhya/understanding-the-loss-function-of-logistic-regression-ac1eec2838ce", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/understanding-the-<b>loss-function</b>-of-<b>log</b>istic...", "snippet": "Let\u2019s demystify \u201c<b>Log</b> <b>Loss Function</b>.\u201d. It is important to first understand the <b>log</b> function before jumping into <b>log</b> <b>loss</b>. If we plot y = <b>log</b> (x), the graph in quadrant II looks like this. y ...", "dateLastCrawled": "2022-01-30T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions in Neural Networks - theaidream.com", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "Cross-Entropy <b>loss</b> is also called logarithmic <b>loss</b>, <b>log</b> <b>loss</b>, or logistic <b>loss</b>. Each predicted class probability is <b>compared</b> to the actual class desired output 0 or 1 and a score/<b>loss</b> is calculated that penalizes the probability based on how far it is from the actual expected value. The penalty is logarithmic in nature yielding a large score for large differences close to 1 and small score for small differences tending to 0.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Most Common <b>Loss</b> Functions in <b>Machine</b> <b>Learning</b> | by Sparsh Gupta ...", "url": "https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/most-common-<b>loss</b>-functions-in-<b>machine</b>-<b>learning</b>-c7212a99dae0", "snippet": "As a core element, the <b>Loss function</b> is a method of evaluating your <b>Machine</b> <b>Learning</b> algorithm that how well it models your featured dataset. It is defined as a measurement of how good your <b>model</b> is in terms of predicting the expected outcome.. The Cost function and <b>Loss function</b> refer to the same context. The cost function is a function that is calculated as the average of all <b>loss function</b> values.", "dateLastCrawled": "2022-02-03T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to interpret \u201c<b>loss</b>\u201d and \u201c<b>accuracy</b>\u201d for a <b>machine</b> <b>learning</b> <b>model</b> ...", "url": "https://intellipaat.com/community/368/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/community/368/how-to-interpret-<b>loss</b>-and-<b>accuracy</b>-for-a-<b>machine</b>...", "snippet": "A <b>loss</b> function is used to optimize a <b>machine</b> <b>learning</b> algorithm. The <b>loss</b> is calculated on training and validation and its interpretation is based on how well the <b>model</b> is doing in these two sets. It is the sum of errors made for each example in training or validation sets. <b>Loss</b> value implies how poorly or well a <b>model</b> behaves after each iteration of optimization. An <b>accuracy</b> metric is used to measure the algorithm\u2019s performance in an interpretable way. The <b>accuracy</b> of a <b>model</b> is usually ...", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>machine</b> <b>learning</b> - What&#39;s considered a good <b>log loss</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/276067/whats-considered-a-good-log-loss", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/276067", "snippet": "then calculate <b>log_loss</b> on your test data_set using these test click through rates as predictions. This is then the optimal <b>logloss</b> on your test set for a <b>model</b> only using website ids. The problem is we <b>can</b> make this <b>loss</b> as small as we like by just adding more features until each record is uniquely identified.", "dateLastCrawled": "2022-01-30T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5 Regression <b>Loss</b> Functions All <b>Machine</b> Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-<b>machine</b>-learners-should...", "snippet": "Choosing the right <b>loss</b> function for fitting a <b>machine</b> <b>learning</b> <b>model</b>. Get started. Open in app. Sign in. Get started. Follow. 11.3K Followers \u00b7 Data Science ML/DL MLOps Contribute Explore Comet. About. Get started. Open in app. 5 Regression <b>Loss</b> Functions All <b>Machine</b> Learners Should Know. Choosing the right <b>loss</b> function for fitting a <b>model</b>. Prince Grover. Jun 5, 2018 \u00b7 11 min read. All the algorithms in <b>machine</b> <b>learning</b> rely on minimizing or maximizing a function, which we call ...", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>MCQ-ML</b> - <b>Machine</b> <b>Learning</b> Questions &amp;amp; Solutions Question Context A ...", "url": "https://www.studocu.com/in/document/savitribai-phule-pune-university/bsc-computer-science/mcq-ml/11200181", "isFamilyFriendly": true, "displayUrl": "https://www.<b>studocu</b>.com/in/document/savitribai-phule-pune-university/bsc-computer...", "snippet": "If a classifier is confident about an incorrect classification, then <b>log</b>-<b>loss</b> will; penalise it heavily a particular observation, the classifier assigns a very small probability for the; correct class then the corresponding contribution tLower the <b>log</b>-<b>loss</b>, the better is the <b>model</b>. o the <b>log</b>-<b>loss</b> will be very large. A) 1 and 3 B) 2 and 3 C) 1 ...", "dateLastCrawled": "2022-01-30T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "<b>Loss</b> functions are a key part of any <b>machine</b> <b>learning</b> <b>model</b>: they define an objective against which the performance of your <b>model</b> is measured, and the setting of weight parameters learned by the <b>model</b> is determined by minimizing a chosen <b>loss</b> function. There are several different common <b>loss</b> functions to choose from: the cross-entropy <b>loss</b>, the mean-squared <b>error</b>, the huber <b>loss</b>, and the hinge <b>loss</b> - just to name a few. Given a particular <b>model</b>, each <b>loss</b> function has particular properties ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to interpret <b>loss</b> and accuracy for a <b>machine</b> <b>learning</b> <b>model</b>", "url": "https://stackoverflow.com/questions/34518656/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/34518656", "snippet": "The lower the <b>loss</b>, the better a <b>model</b> (unless the <b>model</b> has over-fitted to the training data). The <b>loss</b> is calculated on training and validation and its interperation is how well the <b>model</b> is doing for these two sets. Unlike accuracy, <b>loss</b> is not a percentage. It is a summation of the errors made for each example in training or validation sets.", "dateLastCrawled": "2022-02-02T23:45:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "1 Motivation: <b>log</b> <b>loss</b> in online <b>learning</b>.", "url": "https://www.cs.princeton.edu/courses/archive/spring08/cos511/scribe_notes/0421.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.princeton.edu/courses/archive/spring08/cos511/scribe_notes/0421.pdf", "snippet": "COS 511: Theoretical <b>Machine</b> <b>Learning</b> Lecturer: Rob Schapire Lecture #20 Scribe: Joe Wenjie Jiang April 21, 2008 1 Motivation: <b>log</b> <b>loss</b> in online <b>learning</b>. In the last lecture, we started to discuss the following model of online <b>learning</b>, in which our goal is to minimize the total <b>log</b> <b>loss</b>: Let X be the space of all possible outcomes in any time step. There are N experts from whom we can consult. At each time step t = 1,...,T: \u2022 Each expert i predicts pt,i, which is a distribution over all ...", "dateLastCrawled": "2021-11-29T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machined Learnings: ML and OR: An <b>analogy</b> with cost-sensitive ...", "url": "http://www.machinedlearnings.com/2010/07/ml-and-or.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2010/07/ml-and-or.html", "snippet": "Nonetheless I&#39;ve been amusing myself by thinking about it, in particular trying to think about it from a <b>machine</b> <b>learning</b> reduction standpoint. The simplest well-understood reduction that I can think of which is analogous to supplying estimates to a linear program is the reduction of cost-sensitive multiclass classification (CSMC) to regression.", "dateLastCrawled": "2021-12-25T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, ... Cross-Entropy (aka <b>log</b> <b>loss</b>): calculates the differences between the predicted class probabilities and those from ground truth across a logarithmic scale. Useful for object detection. Weighted Cross-Entropy: improves on Cross-Entropy accuracy by adding weights to certain aspects (e.g., certain object classes) which are under-represented in the data (e.g., objects occurring in fewer data samples\u00b3 ...", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "ML Concepts. A.I \u2014 Intelligence showed by machines which is common for humans <b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>.We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Why and How to use <b>Cross Entropy</b>. The fundamental reasons for ...", "url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-and-how-to-use-<b>cross-entropy</b>-4e983cbdd873", "snippet": "A convenient way to think of <b>log</b> <b>loss</b> is as follows: If the model predicts that an observation should be labeled 1 and assigns a high probability to that prediction, a high penalty will be incurred when the true label is 0. If the model had assigned a lower probability to that prediction, a lower penalty would have been incurred. The reason for taking the <b>log</b> of predicted probabilities goes back to the original formulation of entropy. Information Theory looks at entropy as a measure of ...", "dateLastCrawled": "2022-01-31T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - What is the relation between a <b>loss</b> function and an ...", "url": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-loss-function-and-an-energy-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-<b>loss</b>...", "snippet": "A <b>loss</b> function is a function that measures the distance between the expected value and the actual value of a model (an example of a <b>loss</b> function is the cross entropy).. An energy function can be defined as a function that we want to minimise or maximise and it is a function of the variables of the system. It is referred to as &quot;energy function&quot; because it is often related or compared to the concept of &quot;energy&quot; in physics. These two expression seem to refer to the same concept.", "dateLastCrawled": "2022-01-17T15:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How To <b>Implement Logistic Regression</b> From Scratch in Python", "url": "https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>implement-logistic-regression</b>-stochastic-gradient...", "snippet": "Kick-start your project with my new book <b>Machine</b> <b>Learning</b> Algorithms From Scratch, including step-by-step tutorials and the Python source code files for all examples. Let\u2019s get started. Update Jan/2017: Changed the calculation of fold_size in cross_validation_split() to always be an integer. Fixes issues with Python 3. Update Mar/2018: Added alternate link to download the dataset as the original appears to have been taken down. Update Aug/2018: Tested and updated to work with Python 3.6 ...", "dateLastCrawled": "2022-02-02T07:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>All Unit MCQ questions of ML</b> \u2013 TheCodingShef", "url": "https://thecodingshef.com/all-unit-mcq-questions-of-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thecodingshef.com/<b>all-unit-mcq-questions-of</b>-<b>machine</b>-<b>learning</b>", "snippet": "Correct option is C. Choose the correct option regarding <b>machine</b> <b>learning</b> (ML) and artificial intelligence (AI) ML is a set of techniques that turns a dataset into a software. AI is a software that can emulate the human mind. ML is an alternate way of programming intelligent machines. All of the above. Correct option is D.", "dateLastCrawled": "2022-01-30T22:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Predicting the 2019 All-<b>NBA teams with machine learning</b> - <b>Dribble Analytics</b>", "url": "https://dribbleanalytics.blog/2019/03/ml-all-nba-predict/", "isFamilyFriendly": true, "displayUrl": "https://<b>dribbleanalytics</b>.blog/2019/03/ml-all-nba-predict", "snippet": "<b>Log loss is like</b> accuracy, but instead of analyzing the labeled predictions, it analyzes the prediction probabilities. This is particularly important given that we\u2019re more interested in the probabilities than we are in the actual labels. A \u201cperfect\u201d model will have a log loss of 0. The table below shows each model\u2019s log loss. Model Log loss; SVC: 0.416: RF: 0.416: KNN: 0.403: DNN: 0.43: The SVC and RF have the same log loss, while the KNN has the lowest. Next, let\u2019s look at the ...", "dateLastCrawled": "2022-01-04T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "[OC] Predicting the 2019 All-<b>NBA teams with machine learning</b> : nba", "url": "https://www.reddit.com/r/nba/comments/aw51j6/oc_predicting_the_2019_allnba_teams_with_machine/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../aw51j6/oc_predicting_the_2019_allnba_teams_with_<b>machine</b>", "snippet": "[OC] Predicting the 2019 All-<b>NBA teams with machine learning</b>. Original Content. This post has a lot of graphs. If you don&#39;t want to click on each one individually, they&#39;re all in an imgur album here. There is a tl;dr and summary infographic at the very end. Introduction . Last year, media members unanimously selected LeBron James to the All-NBA first team, giving him a record 12 All-NBA first team selections. However, given the Lakers recent struggles and LeBron&#39;s absence earlier in the ...", "dateLastCrawled": "2021-10-14T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting the 2019 All-NBA teams with <b>machine</b> <b>learning</b>", "url": "https://dribbleanalytics.blogspot.com/2019/03/ml-all-nba-predict.html", "isFamilyFriendly": true, "displayUrl": "https://dribbleanalytics.blogspot.com/2019/03/ml-all-nba-predict.html", "snippet": "Predicting the 2019 All-NBA teams with <b>machine</b> <b>learning</b> Get link; Facebook; Twitter; Pinterest; Email; Other Apps; March 01, 2019 There is a summary at the bottom if you want to skip to the results. Introduction Last year, media members unanimously selected LeBron James to the All-NBA first team, giving him a record 12 All-NBA first team selections. However, given the Lakers recent struggles and LeBron&#39;s absence earlier in the season, LeBron might miss not only the first team but also the ...", "dateLastCrawled": "2021-12-11T07:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What\u2019s considered a good Log <b>Loss</b> in <b>Machine</b> <b>Learning</b> ? | by Federico ...", "url": "https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@fzammito/whats-considered-a-good-log-<b>loss</b>-in-<b>machine</b>-<b>learning</b>-a529...", "snippet": "<b>Log Loss is similar</b> to the Accuracy, but it will favor models that distinguish more strongly the classes. Log <b>Loss</b> it useful to compare models not only on their output but on their probabilistic ...", "dateLastCrawled": "2022-01-30T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>What is an intuitive explanation for the log</b> loss function? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-for-the-log-loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-an-intuitive-explanation-for-the-log</b>-loss-function", "snippet": "Answer (1 of 8): To me an intuitive explanation is that minimizing the log loss equals minimizing the Kullback-Leibler divergence (Kullback\u2013Leibler divergence - Wikipedia) between the function you want to optimize (for example a neural network) and the true function that generates the data (from ...", "dateLastCrawled": "2022-01-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Key techniques for Evaluating <b>Machine</b> <b>Learning</b> models - Data Analytics", "url": "https://vitalflux.com/key-techniques-evaluating-machine-learning-models-performance/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/key-techniques-evaluating-<b>machine</b>-<b>learning</b>-models-performance", "snippet": "Log loss is used to evaluate the performance of classification <b>machine</b> <b>learning</b> models that are built using classification algorithms such as logistic regression, support vector <b>machine</b> (SVM), random forest, and gradient boosting. The idea behind the use of <b>Log loss is similar</b> to taking a base-e exponential or natural logarithm in order to compare model scores from high-value functions which may indicate poor <b>machine</b> <b>learning</b> model performance. The logarithmic loss value is defined as ...", "dateLastCrawled": "2022-01-31T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss In Machine Learning</b> - 02/2021 - Course f", "url": "https://www.coursef.com/loss-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.coursef.com/<b>loss-in-machine-learning</b>", "snippet": "<b>Log Loss is similar</b> to the Accuracy, but it will favor models that ... Two of the most popular loss functions in <b>machine</b> <b>learning</b> are the 0-1 loss function and the quadratic loss function. The 0-1 loss function is an indicator function that returns 1 when the target and output are not equal and zero otherwise: 0-1 Loss: The quadratic loss is a commonly used symmetric loss \u2026 161 People Used View all course \u203a\u203a Visit Site \u2039 1; 2 \u203a FAQs. Do online classes have tests? Not all online ...", "dateLastCrawled": "2021-02-08T01:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Diagnosing malaria from some symptoms: a <b>machine</b> <b>learning</b> approach and ...", "url": "https://link.springer.com/article/10.1007/s12553-020-00488-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12553-020-00488-5", "snippet": "<b>Machine</b> <b>learning</b> tools have become available in the diagnosis and prediction of diseases, thereby saving costs and improving the likelihood of survivorship, especially in some terminal diseases. In the case of infectious diseases, early diagnosis is highly needed in isolating the subjects to reduce the spread of the disease. Researchers continue to propose new data mining tools that help in the early diagnosis of diseases, reducing the mortality rate, and improving the quality of life of ...", "dateLastCrawled": "2021-12-03T05:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(log loss)  is like +(error in a machine learning model)", "+(log loss) is similar to +(error in a machine learning model)", "+(log loss) can be thought of as +(error in a machine learning model)", "+(log loss) can be compared to +(error in a machine learning model)", "machine learning +(log loss AND analogy)", "machine learning +(\"log loss is like\")", "machine learning +(\"log loss is similar\")", "machine learning +(\"just as log loss\")", "machine learning +(\"log loss can be thought of as\")", "machine learning +(\"log loss can be compared to\")"]}