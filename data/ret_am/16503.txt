{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Machine <b>Learning: Algorithms, Real-World Applications and</b> Research ...", "url": "https://link.springer.com/article/10.1007/s42979-021-00592-x", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s42979-021-00592-x", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>): <b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) is an iterative method for optimizing an objective function with appropriate smoothness properties, where the word \u2018<b>stochastic</b>\u2019 refers to random probability. This reduces the computational burden, particularly in high-dimensional optimization problems, allowing for faster iterations in exchange for a lower convergence rate. A <b>gradient</b> is the slope of a function that calculates a variable\u2019s degree of change in response ...", "dateLastCrawled": "2022-01-30T14:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "What is the difference between <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) and <b>gradient</b> <b>descent</b> (GD)? <b>Gradient</b> <b>Descent</b> and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> are the algorithms that find the set of parameters that will minimize a loss function. The difference is that in <b>Gradient</b> Descend, all training samples are evaluated for each set of parameters. While in <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> only one training sample is evaluated for the set of parameters identified. 22. What is the exploding <b>gradient</b> problem while ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A complete guide to linear regression using gene expression data for ...", "url": "https://medium.com/leukemiaairesearch/a-complete-guide-to-linear-regression-using-gene-expression-data-an-introduction-f861596ff0d0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/leukemiaairesearch/a-complete-guide-to-linear-regression-using-gene...", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) <b>Behind</b> the hood, scikit-learn is using the ordinary least squares method. However, when <b>you</b> are using a penalty term is using <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>).", "dateLastCrawled": "2022-01-07T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Gentle Introduction to the Adam Optimization Algorithm for Deep Learning", "url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning", "snippet": "The choice of optimization algorithm for your deep learning model can mean the difference between good results in minutes, hours, and days. The Adam optimization algorithm is an extension to <b>stochastic</b> <b>gradient</b> <b>descent</b> that has recently seen broader adoption for deep learning applications in computer vision and natural language processing. In this post, <b>you</b> will get a gentle introduction to the Adam", "dateLastCrawled": "2022-02-03T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Hands-on Machine Learning: Keras-TensorFlow - A Hugo website", "url": "https://www.danli.org/2021/06/21/hands-on-machine-learning-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.danli.org/2021/06/21/hands-on-machine-learning-keras", "snippet": "Regarding the optimizer, &quot;<b>sgd</b>&quot; means that we will train the model using simple <b>Stochastic</b> <b>Gradient</b> <b>Descent</b>. In other words, Keras will perform the backpropagation algorithm described earlier (i.e., reverse-mode autodiff plus <b>Gradient</b> <b>Descent</b>). We will discuss more efficient optimizers in Chapter 11 (they improve the <b>Gradient</b> <b>Descent</b> part, not the autodiff).", "dateLastCrawled": "2022-01-30T18:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "4. Model Training Patterns - <b>Machine Learning Design Patterns</b> [Book]", "url": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "snippet": "This means that each worker, typically a GPU, has a copy of the model on device and, for a single <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) step, a mini-batch of data is split among each of the separate workers. Each device performs a forward pass with their portion of the mini-batch and computes gradients for each parameter of the model. These locally computed gradients are then collected from each device and aggregated (for example, averaged) to produce a single <b>gradient</b> update for each parameter ...", "dateLastCrawled": "2022-01-30T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Learning to Rank with Neural Networks", "url": "https://www.slideshare.net/BhaskarMitra3/learning-to-rank-with-neural-networks-226255754", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/BhaskarMitra3/learning-to-rank-with-neural-networks-226255754", "snippet": "Goal: iteratively update the learnable parameters such that the loss \ud835\udc59 is minimized Compute the <b>gradient</b> of the loss \ud835\udc59 w.r.t. each parameter (e.g., \ud835\udc641) \ud835\udf15\ud835\udc59 \ud835\udf15\ud835\udc641 = \ud835\udf15\ud835\udc59 \ud835\udf15\ud835\udc662 \u00d7 \ud835\udf15\ud835\udc662 \ud835\udf15\ud835\udc661 \u00d7 \ud835\udf15\ud835\udc661 \ud835\udf15\ud835\udc641 Update the parameter value based on the <b>gradient</b> with \ud835\udf02 as the learning rate \ud835\udc641 \ud835\udc5b\ud835\udc52\ud835\udc64 = \ud835\udc641 \ud835\udc5c\ud835\udc59\ud835\udc51 \u2212 \ud835\udf02 \u00d7 \ud835\udf15\ud835\udc59 \ud835\udf15\ud835\udc641 <b>STOCHASTIC</b> <b>GRADIENT</b> <b>DESCENT</b> (<b>SGD</b>) Task: regression Training data: \ud835\udc65, \ud835\udc66 pairs Model: NN (1 ...", "dateLastCrawled": "2022-01-09T11:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Naive Bayes Classifiers - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/naive-bayes-classifiers/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/naive-bayes-classifiers", "snippet": "Naive Bayes classifiers are a collection of classification algorithms based on Bayes\u2019 Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other. To start with, let us consider a dataset.", "dateLastCrawled": "2022-02-02T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Understanding Logistic Regression - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/understanding-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/understanding-logistic-regression", "snippet": "Logistic Regression model accuracy(in %): 95.6884561892. At last, here are some points about Logistic regression to ponder upon: Does NOT assume a linear relationship between the dependent variable and the independent variables, but it does assume a linear relationship between the logit of the explanatory variables and the response.; Independent variables can be even the power terms or some other nonlinear transformations of the original independent variables.", "dateLastCrawled": "2022-02-02T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Predicting <b>House</b> Prices with Linear Regression | Machine Learning from ...", "url": "https://towardsdatascience.com/predicting-house-prices-with-linear-regression-machine-learning-from-scratch-part-ii-47a0238aeac1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/predicting-<b>house</b>-prices-with-linear-regression-machine...", "snippet": "Music artist Recommender System using <b>Stochastic</b> <b>Gradient</b> <b>Descent</b>; Fashion product image classification using Neural Networks; Build a taxi driving agent in a post-apocalyptic world using Reinforcement Learning ; I know that <b>you</b>\u2019ve always dreamed of dominating the housing market. Until now, that was impossible. But with this limited offer <b>you</b> can\u2026 got a bit sidetracked there. Let\u2019s start building our model with Python, but this time we will use it on a more realistic dataset. Complete ...", "dateLastCrawled": "2022-02-03T02:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Machine <b>Learning: Algorithms, Real-World Applications and</b> Research ...", "url": "https://link.springer.com/article/10.1007/s42979-021-00592-x", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s42979-021-00592-x", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>): <b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) is an iterative method for optimizing an objective function with appropriate smoothness properties, where the word \u2018<b>stochastic</b>\u2019 refers to random probability. This reduces the computational burden, particularly in high-dimensional optimization problems, allowing for faster iterations in exchange for a lower convergence rate. A <b>gradient</b> is the slope of a function that calculates a variable\u2019s degree of change in response ...", "dateLastCrawled": "2022-01-30T14:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "What is the difference between <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) and <b>gradient</b> <b>descent</b> (GD)? <b>Gradient</b> <b>Descent</b> and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> are the algorithms that find the set of parameters that will minimize a loss function. The difference is that in <b>Gradient</b> Descend, all training samples are evaluated for each set of parameters. While in <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> only one training sample is evaluated for the set of parameters identified. 22. What is the exploding <b>gradient</b> problem while ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A complete guide to linear regression using gene expression data for ...", "url": "https://medium.com/leukemiaairesearch/a-complete-guide-to-linear-regression-using-gene-expression-data-an-introduction-f861596ff0d0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/leukemiaairesearch/a-complete-guide-to-linear-regression-using-gene...", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) <b>Behind</b> the hood, scikit-learn is using the ordinary least squares method. However, when <b>you</b> are using a penalty term is using <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>).", "dateLastCrawled": "2022-01-07T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An Introduction to <b>Gradient</b> <b>Descent</b> and Linear Regression", "url": "https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://spin.atomicobject.com/2014/06/24/<b>gradient</b>-<b>descent</b>-linear-regression", "snippet": "<b>Gradient</b> <b>descent</b> is one of those \u201cgreatest hits\u201d algorithms that can offer a new perspective for solving problems. Unfortunately, it\u2019s rarely taught in undergraduate computer science programs. In this post I\u2019ll give an introduction to the <b>gradient</b> <b>descent</b> algorithm, and walk through an example that demonstrates how <b>gradient</b> <b>descent</b> can be used to solve machine learning problems such as linear regression.", "dateLastCrawled": "2022-02-02T13:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is the difference between a batch <b>and a stochastic gradient descent</b>?", "url": "https://www.quora.com/What-is-the-difference-between-a-batch-and-a-stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-difference-between-a-batch-and-a-<b>stochastic</b>-<b>gradient</b>...", "snippet": "Answer (1 of 4): Just setting some context here. When training a model, we need solve an optimization problem of the <b>following</b> form. min f = \\sum_{i = 1}^m loss(\\hat y_i, y_i) That is <b>you</b> want to minimize the loss between the true values y_i and predicted values \\hat y_i. Here m denotes the si...", "dateLastCrawled": "2022-01-07T19:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Gentle Introduction to the Adam Optimization Algorithm for Deep Learning", "url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning", "snippet": "The choice of optimization algorithm for your deep learning model can mean the difference between good results in minutes, hours, and days. The Adam optimization algorithm is an extension to <b>stochastic</b> <b>gradient</b> <b>descent</b> that has recently seen broader adoption for deep learning applications in computer vision and natural language processing. In this post, <b>you</b> will get a gentle introduction to the Adam", "dateLastCrawled": "2022-02-03T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Gradient descent</b> on non-convex functions - Cross Validated", "url": "https://stats.stackexchange.com/questions/327251/gradient-descent-on-non-convex-functions", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/327251/<b>gradient-descent</b>-on-non-convex-functions", "snippet": "First, they are no longer working in a deterministic framework, instead opting for the more practically relevant <b>stochastic</b> approximation framework on a finite sum (think <b>Stochastic</b> <b>Gradient Descent</b>). The primary differences there are that the step-size requires some additional care, and the <b>gradient</b> becomes a random variable. Additionally, they relax the assumption that all saddles are strict, and look for a second-order stationary point. That is, a point such that, $ \\|\\nabla(f) \\| \\leq ...", "dateLastCrawled": "2022-01-23T20:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "4. Model Training Patterns - <b>Machine Learning Design Patterns</b> [Book]", "url": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "snippet": "This means that each worker, typically a GPU, has a copy of the model on device and, for a single <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) step, a mini-batch of data is split among each of the separate workers. Each device performs a forward pass with their portion of the mini-batch and computes gradients for each parameter of the model. These locally computed gradients are then collected from each device and aggregated (for example, averaged) to produce a single <b>gradient</b> update for each parameter ...", "dateLastCrawled": "2022-01-30T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Learning to Rank with Neural Networks", "url": "https://www.slideshare.net/BhaskarMitra3/learning-to-rank-with-neural-networks-226255754", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/BhaskarMitra3/learning-to-rank-with-neural-networks-226255754", "snippet": "Goal: iteratively update the learnable parameters such that the loss \ud835\udc59 is minimized Compute the <b>gradient</b> of the loss \ud835\udc59 w.r.t. each parameter (e.g., \ud835\udc641) \ud835\udf15\ud835\udc59 \ud835\udf15\ud835\udc641 = \ud835\udf15\ud835\udc59 \ud835\udf15\ud835\udc662 \u00d7 \ud835\udf15\ud835\udc662 \ud835\udf15\ud835\udc661 \u00d7 \ud835\udf15\ud835\udc661 \ud835\udf15\ud835\udc641 Update the parameter value based on the <b>gradient</b> with \ud835\udf02 as the learning rate \ud835\udc641 \ud835\udc5b\ud835\udc52\ud835\udc64 = \ud835\udc641 \ud835\udc5c\ud835\udc59\ud835\udc51 \u2212 \ud835\udf02 \u00d7 \ud835\udf15\ud835\udc59 \ud835\udf15\ud835\udc641 <b>STOCHASTIC</b> <b>GRADIENT</b> <b>DESCENT</b> (<b>SGD</b>) Task: regression Training data: \ud835\udc65, \ud835\udc66 pairs Model: NN (1 ...", "dateLastCrawled": "2022-01-09T11:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How To Implement The Perceptron Algorithm From Scratch In Python", "url": "https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/implement-perceptron", "snippet": "Batch <b>Stochastic</b> <b>Gradient</b> <b>Descent</b>. Change the <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm to accumulate updates across each epoch and only update the weights in a batch at the end of the epoch. Additional Regression Problems. Apply the technique to other classification problems on the UCI machine learning repository. Did <b>you</b> explore any of these extensions? Let me know about it in the comments below. Review. In this tutorial, <b>you</b> discovered how to implement the Perceptron algorithm using ...", "dateLastCrawled": "2022-01-30T02:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[D] <b>Can</b> <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Converge on Non-Convex Functions ...", "url": "https://www.reddit.com/r/MachineLearning/comments/slnvzw/d_can_stochastic_gradient_descent_converge_on/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/slnvzw/d_<b>can</b>_<b>stochastic</b>_<b>gradient</b>_<b>descent</b>_converge_on", "snippet": "- The authors make further restrictions and prove that regardless of whether the iterates diverge or remain finite \u2014 the norm of the <b>gradient</b> function evaluated at <b>Stochastic</b> <b>Gradient</b> <b>Descent</b>&#39;s iterates converges to zero with probability one and in expectation; thus broadening the scope of functions to which <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> <b>can</b> be applied to while maintaining rigorous guarantees of its global behavior.", "dateLastCrawled": "2022-02-07T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Gentle Introduction to the Adam Optimization Algorithm for Deep Learning", "url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning", "snippet": "The choice of optimization algorithm for your deep learning model <b>can</b> mean the difference between good results in minutes, hours, and days. The Adam optimization algorithm is an extension to <b>stochastic</b> <b>gradient</b> <b>descent</b> that has recently seen broader adoption for deep learning applications in computer vision and natural language processing. In this post, <b>you</b> will get a gentle introduction to the Adam", "dateLastCrawled": "2022-02-03T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "An Introduction to <b>Gradient</b> <b>Descent</b> and Linear Regression", "url": "https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://spin.atomicobject.com/2014/06/24/<b>gradient</b>-<b>descent</b>-linear-regression", "snippet": "<b>Gradient</b> <b>descent</b> is one of those \u201cgreatest hits\u201d algorithms that <b>can</b> offer a new perspective for solving problems. Unfortunately, it\u2019s rarely taught in undergraduate computer science programs. In this post I\u2019ll give an introduction to the <b>gradient</b> <b>descent</b> algorithm, and walk through an example that demonstrates how <b>gradient</b> <b>descent</b> <b>can</b> be used to solve machine learning problems such as linear regression.", "dateLastCrawled": "2022-02-02T13:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>What is TensorFlow</b>? Top various uses of <b>TensorFlow</b>", "url": "https://www.mygreatlearning.com/blog/what-is-tensorflow-machine-learning-library-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>what-is-tensorflow</b>-machine-learning-library-explained", "snippet": "<b>SGD</b> (<b>Stochastic</b> <b>Gradient</b> <b>Descent</b>) If we have 50000 datapoints with 10 features, then it is necessary to compute 50000*10 times on each iteration. So, let us consider 500 iterations for building a model which will take 50000*10*500 computations in total to complete the process. So, for this huge processing, <b>SGD</b> or <b>stochastic</b> <b>gradient</b> <b>descent</b> ...", "dateLastCrawled": "2022-01-31T12:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "4. Model Training Patterns - <b>Machine Learning Design Patterns</b> [Book]", "url": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "snippet": "This means that each worker, typically a GPU, has a copy of the model on device and, for a single <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) step, a mini-batch of data is split among each of the separate workers. Each device performs a forward pass with their portion of the mini-batch and computes gradients for each parameter of the model. These locally computed gradients are then collected from each device and aggregated (for example, averaged) to produce a single <b>gradient</b> update for each parameter ...", "dateLastCrawled": "2022-01-30T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Kaggle Digit Recognizer :: The Convolutional Neural Network path to ...", "url": "https://insufficientinformation.wordpress.com/2017/10/15/kaggle-digit-recognizer-the-convolutional-neural-network-path-to-high-accuracy/", "isFamilyFriendly": true, "displayUrl": "https://insufficientinformation.wordpress.com/2017/10/15/kaggle-digit-recognizer-the...", "snippet": "In our case, we use the <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) updates with Nesterov momentum. The learning rate controls the size of the update steps while a higher value of the update momentum (which defaults to 0.9) results in smoothing over a larger number of update steps. An epoch refers to a full training cycle.", "dateLastCrawled": "2022-01-31T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How To Implement The Perceptron Algorithm From Scratch In Python", "url": "https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/implement-perceptron", "snippet": "I have a question though: I <b>thought</b> to have read somewhere that in \u2018<b>stochastic</b>\u2019 <b>gradient</b> <b>descent</b>, the weights have to be initialised to a small random value (hence the \u201c<b>stochastic</b>\u201d) instead of zero, to prevent some nodes in the net from becoming or remaining inactive due to zero multiplication. I see in your <b>gradient</b> <b>descent</b> algorithm, <b>you</b> initialise the weights to zero. Could <b>you</b> elaborate some on the choice of the zero init value? My understanding may be incomplete, but this ...", "dateLastCrawled": "2022-01-30T02:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Victoria&#39;s ML Implementation Notes - Persagen", "url": "http://persagen.com/files/ml-implementation_notes.html", "isFamilyFriendly": true, "displayUrl": "persagen.com/files/ml-implementation_notes.html", "snippet": "When <b>you</b> train a convolutional neural network and <b>you</b> set the <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm as its optimizer, <b>you</b> usually have three hyperparameters to tune, which are : the learning rate [ \u03b1], the momentum and the weight decay. I know few heuristics to decrease the learning rate. The first one is to tune a fourth hyperparameter (a learning rate decay) which decreases the learning rate at each time step such as a minibatch or an epoch. The second one is to set different values for ...", "dateLastCrawled": "2022-02-02T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Short Question | Short Question Online Test - Avatto", "url": "https://avatto.com/interview-questions/short-question/", "isFamilyFriendly": true, "displayUrl": "https://avatto.com/interview-questions/short-question", "snippet": "One problem we face with <b>SGD</b> and mini-batch <b>gradient</b> <b>descent</b> is that there will be too many oscillations in the <b>gradient</b> steps. This oscillation happens because we update the parameter of the network after iterating through every point or every n data points and thus the direction of the update will possess some variances causing oscillation in the <b>gradient</b> steps.", "dateLastCrawled": "2022-01-26T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Mesin Belajar</b>: 2018", "url": "https://mesin-belajar.blogspot.com/2018/", "isFamilyFriendly": true, "displayUrl": "https://<b>mesin-belajar</b>.blogspot.com/2018", "snippet": "With each batch of <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>), your network should be getting closer and closer to a global minimum value for the loss. As it gets closer to this minimum, it hence makes sense that the learning rate should get smaller so that your algorithm does not overshoot, and instead settles as close to this point as possible. Cosine annealing solves this problem by decreasing the learning rate <b>following</b> the cosine function as seen in the figure below.", "dateLastCrawled": "2022-01-22T20:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[D] <b>Can</b> <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Converge on Non-Convex Functions ...", "url": "https://www.reddit.com/r/MachineLearning/comments/slnvzw/d_can_stochastic_gradient_descent_converge_on/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/slnvzw/d_<b>can</b>_<b>stochastic</b>_<b>gradient</b>_<b>descent</b>_converge_on", "snippet": "However, I am interested in learning about to what extent convergence of <b>Gradient</b> <b>Descent</b> Based Algorithms (e.g. <b>Stochastic</b> <b>Gradient</b> <b>Descent</b>) has been studied for (non-deterministic) Non-Convex Functions. For instance, in Machine Learning applications with Neural Networks in the real world - Loss Functions almost always tend to be Non-Convex. Seeing as Non-Convex Functions usually have Saddle Points (i.e. point where the first derivatives of the Loss Function is 0), these usually &quot;trap&quot; and ...", "dateLastCrawled": "2022-02-07T14:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "170 Machine Learning Interview Question and Answers in 2022", "url": "https://www.mygreatlearning.com/blog/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreatlearning.com/blog/<b>machine-learning-interview-questions</b>", "snippet": "What is the difference between <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) and <b>gradient</b> <b>descent</b> (GD)? <b>Gradient</b> <b>Descent</b> and <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> are the algorithms that find the set of parameters that will minimize a loss function. The difference is that in <b>Gradient</b> Descend, all training samples are evaluated for each set of parameters. While in <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> only one training sample is evaluated for the set of parameters identified. 22. What is the exploding <b>gradient</b> problem while ...", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is the difference between a batch <b>and a stochastic gradient descent</b>?", "url": "https://www.quora.com/What-is-the-difference-between-a-batch-and-a-stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-difference-between-a-batch-and-a-<b>stochastic</b>-<b>gradient</b>...", "snippet": "Answer (1 of 4): Just setting some context here. When training a model, we need solve an optimization problem of the <b>following</b> form. min f = \\sum_{i = 1}^m loss(\\hat y_i, y_i) That is <b>you</b> want to minimize the loss between the true values y_i and predicted values \\hat y_i. Here m denotes the si...", "dateLastCrawled": "2022-01-07T19:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Sentiment Analysis Techniques and Approaches \u2013 IJERT", "url": "https://www.ijert.org/sentiment-analysis-techniques-and-approaches", "isFamilyFriendly": true, "displayUrl": "https://www.ijert.org/sentiment-analysis-techniques-and-approaches", "snippet": "<b>Gradient</b> <b>descent</b> is an algorithm that starts from a random point of the problem statement dataset values and at each iteration optimizes the results by decreasing the difference between the minimum cost and initial cost using the concept of slopes and derivatives. It was used to improve the performance of logistic regression. The resulting values of the accuracy of the model using the Hold out method came out to be 0.63 while in the case of a 10-fold cross-validation method, it came out as 0 ...", "dateLastCrawled": "2022-02-02T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "4. Model Training Patterns - <b>Machine Learning Design Patterns</b> [Book]", "url": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html", "snippet": "This means that each worker, typically a GPU, has a copy of the model on device and, for a single <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) step, a mini-batch of data is split among each of the separate workers. Each device performs a forward pass with their portion of the mini-batch and computes gradients for each parameter of the model. These locally computed gradients are then collected from each device and aggregated (for example, averaged) to produce a single <b>gradient</b> update for each parameter ...", "dateLastCrawled": "2022-01-30T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Getting Started With <b>TensorFlow</b> in Angular - Where good ideas find <b>you</b>.", "url": "https://medium.com/ngconf/getting-started-with-tensorflow-in-angular-36c0e9d26964", "isFamilyFriendly": true, "displayUrl": "https://medium.com/ngconf/getting-started-with-<b>tensorflow</b>-in-angular-36c0e9d26964", "snippet": "The TF optimizer selected for this process is called <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>). We briefly discussed classical <b>gradient</b> <b>descent</b> (GD) above. <b>SGD</b> is an approximation to GD that estimates ...", "dateLastCrawled": "2022-01-29T11:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Neural Coding and Computation Lab - Pillow Lab Blog | Neural Coding and ...", "url": "https://pillowlab.wordpress.com/", "isFamilyFriendly": true, "displayUrl": "https://pillowlab.wordpress.com", "snippet": "The optimization\u2014<b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) with all of its hyperparameters (learning rate, momentum, \u2026) and variants\u2014was an afterthought, and I chose hyperparameters that seemed reasonable. Still, a nagging question persisted in the back of my mind: What if different hyperparameters led to even better performance? It was an obvious case of fear-of-missing-out (FOMO). Grid search (aka brute force) and black-box optimization techniques should be last resorts. Due to FOMO, I began ...", "dateLastCrawled": "2021-12-31T21:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Predicting <b>House</b> Prices with Linear Regression | Machine Learning from ...", "url": "https://towardsdatascience.com/predicting-house-prices-with-linear-regression-machine-learning-from-scratch-part-ii-47a0238aeac1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/predicting-<b>house</b>-prices-with-linear-regression-machine...", "snippet": "Music artist Recommender System using <b>Stochastic</b> <b>Gradient</b> <b>Descent</b>; Fashion product image classification using Neural Networks; Build a taxi driving agent in a post-apocalyptic world using Reinforcement Learning ; I know that <b>you</b>\u2019ve always dreamed of dominating the housing market. Until now, that was impossible. But with this limited offer <b>you</b> <b>can</b>\u2026 got a bit sidetracked there. Let\u2019s start building our model with Python, but this time we will use it on a more realistic dataset. Complete ...", "dateLastCrawled": "2022-02-03T02:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Victoria&#39;s ML Implementation Notes - Persagen", "url": "http://persagen.com/files/ml-implementation_notes.html", "isFamilyFriendly": true, "displayUrl": "persagen.com/files/ml-implementation_notes.html", "snippet": "When <b>you</b> train a convolutional neural network and <b>you</b> set the <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm as its optimizer, <b>you</b> usually have three hyperparameters to tune, which are : the learning rate [ \u03b1], the momentum and the weight decay. I know few heuristics to decrease the learning rate. The first one is to tune a fourth hyperparameter (a learning rate decay) which decreases the learning rate at each time step such as a minibatch or an epoch. The second one is to set different values for ...", "dateLastCrawled": "2022-02-02T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Convolutional neural network</b> - SlideShare", "url": "https://www.slideshare.net/xuyangela/convolutional-neural-network-76142939", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/xuyangela/<b>convolutional-neural-network</b>-76142939", "snippet": "Maybe <b>You</b> Should Talk <b>to Someone</b>: A Therapist, HER Therapist, and Our Lives Revealed Lori Gottlieb (4.5/5) Free. Junky: The Definitive Text of &quot;Junk&quot; William S. Burroughs (4/5) Free . The Coming Plague: Newly Emerging Diseases in a World Out of Balance Laurie Garrett (4.5/5) Free. Homo Deus: A Brief History of Tomorrow Yuval Noah Harari (4.5/5) Free. Chaos: Making a New Science James Gleick (4.5/5) Free. The Obesity Code: Unlocking the Secrets of Weight Loss (Why Intermittent Fasting Is the ...", "dateLastCrawled": "2022-02-01T08:37:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! | by Aishwarya V ...", "url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>stochastic-gradient-descent</b>-clearly-explained-53d239905d31", "snippet": "<b>Stochastic gradient descent</b> is a very popular and common algorithm used in various <b>Machine</b> <b>Learning</b> algorithms, most importantly forms the basis of Neural Networks. In this article, I have tried my\u2026 Get started. Open in app. Sign in. Get started. Follow. 618K Followers \u00b7 Editors&#39; Picks Features Deep Dives Grow Contribute. About. Get started. Open in app. <b>Stochastic Gradient Descent</b> \u2014 Clearly Explained !! Aishwarya V Srinivasan. Sep 7, 2019 \u00b7 4 min read. <b>Stochastic gradient descent</b> is a ...", "dateLastCrawled": "2022-02-02T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Stochastic gradient descent</b> - The <b>Learning</b> <b>Machine</b>", "url": "https://the-learning-machine.com/article/optimization/stochastic-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://the-<b>learning</b>-<b>machine</b>.com/article/optimization/<b>stochastic-gradient-descent</b>", "snippet": "<b>Stochastic gradient descent</b> (<b>SGD</b>) is an approach for unconstrained optimization.<b>SGD</b> is the workhorse of optimization for <b>machine</b> <b>learning</b> approaches. It is used as a faster alternative for training support vector machines and is the preferred optimization routine for deep <b>learning</b> approaches.. In this article, we will motivate the formulation for <b>stochastic gradient descent</b> and provide interactive demos over multiple univariate and multivariate functions to show it in action.", "dateLastCrawled": "2022-01-26T05:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> in Theory and Practice", "url": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/~optas/data/stanford_qual_exams.pdf", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) is the most widely used optimization method in the <b>machine</b> <b>learning</b> community. Researchers in both academia and industry have put considerable e ort to optimize <b>SGD</b>\u2019s runtime performance and to develop a theoretical framework for its empirical success. For example, recent advancements in deep neural networks have been largely achieved because, surprisingly, <b>SGD</b> has been found adequate to train them. Here we present three works highlighting desirable ...", "dateLastCrawled": "2022-01-30T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> <b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b>", "url": "http://www.bel.utcluj.ro/dce/didactic/eai/04_GradientDescent_ML.pdf", "isFamilyFriendly": true, "displayUrl": "www.bel.utcluj.ro/dce/didactic/eai/04_<b>GradientDescent</b>_ML.pdf", "snippet": "<b>Gradient</b> <b>Descent</b> for <b>Machine</b> <b>Learning</b> Elements of Artificial Intelligence G. Oltean BGD vs. <b>SGD</b> The summation part is important, especially with the concept of batch <b>gradient</b> <b>descent</b> (BGD) vs. <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>). In Batch <b>Gradient</b> <b>Descent</b>, all the training data is taken into consideration to take a single step (one training epoch ...", "dateLastCrawled": "2022-02-03T10:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Adam, <b>Momentum and Stochastic Gradient Descent</b> - <b>Machine</b> <b>Learning</b> From ...", "url": "https://mlfromscratch.com/optimizers-explained/", "isFamilyFriendly": true, "displayUrl": "https://mlfromscratch.com/optimizers-explained", "snippet": "The basic difference between batch <b>gradient</b> <b>descent</b> (BGD) and <b>stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>), is that we only calculate the cost of one example for each step in <b>SGD</b>, but in BGD, we have to calculate the cost for all training examples in the dataset. Trivially, this speeds up neural networks greatly. Exactly this is the motivation behind <b>SGD</b>. The equation for <b>SGD</b> is used to update parameters in a neural network \u2013 we use the equation to update parameters in a backwards pass, using ...", "dateLastCrawled": "2022-02-02T18:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Scikit Learn - Stochastic Gradient Descent</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_stochastic_gradient_descent</b>.htm", "snippet": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (<b>SGD</b>) is a simple yet efficient optimization algorithm used to find the values of parameters/coefficients of functions that minimize a cost function. In other words, it is used for discriminative <b>learning</b> of linear classifiers under convex loss functions such as SVM and Logistic regression. It has been successfully applied to large-scale datasets because the update to the coefficients is performed for each training instance, rather than at the end of instances.", "dateLastCrawled": "2022-02-03T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Stochastic</b> <b>Gradient</b> <b>Descent</b> using PyTorch | by Ashish Pandey | Geek ...", "url": "https://medium.com/geekculture/stochastic-gradient-descent-using-pytotch-bdd3ba5a3ae3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/<b>stochastic</b>-<b>gradient</b>-<b>descent</b>-using-pytotch-bdd3ba5a3ae3", "snippet": "Nearly all approaches start with the basic idea of multiplying the <b>gradient</b> by some small number, called the <b>learning</b> rate (LR). The <b>learning</b> rate is often a number between 0.001 and 0.1, although ...", "dateLastCrawled": "2022-01-29T14:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Batch, Mini Batch &amp; <b>Stochastic</b> <b>Gradient Descent</b> | by Sushant Patrikar ...", "url": "https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/batch-mini-batch-<b>stochastic</b>-<b>gradient-descent</b>-7a62ecba642a", "snippet": "In Batch <b>Gradient Descent</b> we were considering all the examples for every step of <b>Gradient Descent</b>. But what if our dataset is very huge. Deep <b>learning</b> models crave for data. The more the data the more chances of a model to be good. Suppose our dataset has 5 million examples, then just to take one step the model will have to calculate the gradients of all the 5 million examples. This does not seem an efficient way. To tackle this problem we have <b>Stochastic</b> <b>Gradient Descent</b>. In <b>Stochastic</b> ...", "dateLastCrawled": "2022-02-02T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Lecture 2: Neural Nets</b> - Deep <b>Learning</b>", "url": "https://chinmayhegde.github.io/dl-notes/notes/lecture02/", "isFamilyFriendly": true, "displayUrl": "https://chinmayhegde.github.io/dl-notes/notes/lecture02", "snippet": "<b>Stochastic</b> <b>gradient</b> <b>descent</b> (<b>SGD</b>) One issue with <b>gradient</b> <b>descent</b> is the uncomfortable fact that one needs to repeatedly compute the <b>gradient</b>. Let us see why this can be challenging. The <b>gradient</b> <b>descent</b> iteration for the least squares loss is given by: \\[w_{k+1} = w_k + \\alpha_k \\sum_{i=1}^n (y_i - \\langle w_k, x_i \\rangle) x_i\\] So, per ...", "dateLastCrawled": "2022-02-03T00:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Gradient Descent in Logistic Regression [Explained for Beginners</b> ...", "url": "https://www.upgrad.com/blog/gradient-descent-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/<b>gradient-descent-in-logistic-regression</b>", "snippet": "It\u2019s massive, and hence there was a need for a slightly modified <b>Gradient</b> <b>Descent</b> Algorithm, namely \u2013 <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> Algorithm (<b>SGD</b>). The only difference <b>SGD</b> has with Normal <b>Gradient</b> <b>Descent</b> is that, in <b>SGD</b>, we don\u2019t deal with the entire training instance at a single time. In <b>SGD</b>, we compute the <b>gradient</b> of the cost function for just a single random example at each iteration. Now, doing so brings down the time taken for computations by a huge margin especially for large ...", "dateLastCrawled": "2022-01-28T16:49:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Gossip <b>Learning</b> as a Decentralized Alternative to Federated <b>Learning</b>", "url": "http://publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "isFamilyFriendly": true, "displayUrl": "publicatio.bibl.u-szeged.hu/15824/1/dais19a.pdf", "snippet": "Federated <b>learning</b> is adistributed <b>machine</b> <b>learning</b> approach for computing models over data collected by edge devices. Most impor-tantly, the data itself is not collected centrally, but a master-worker ar-chitecture is applied where a master node performs aggregation and the edge devices are the workers, not unlike the parameter server approach. Gossip <b>learning</b> also assumes that the data remains at the edge devices, but it requires no aggregation server or any central component. In this ...", "dateLastCrawled": "2022-01-27T14:06:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(stochastic gradient descent (sgd))  is like +(someone following behind you with a map)", "+(stochastic gradient descent (sgd)) is similar to +(someone following behind you with a map)", "+(stochastic gradient descent (sgd)) can be thought of as +(someone following behind you with a map)", "+(stochastic gradient descent (sgd)) can be compared to +(someone following behind you with a map)", "machine learning +(stochastic gradient descent (sgd) AND analogy)", "machine learning +(\"stochastic gradient descent (sgd) is like\")", "machine learning +(\"stochastic gradient descent (sgd) is similar\")", "machine learning +(\"just as stochastic gradient descent (sgd)\")", "machine learning +(\"stochastic gradient descent (sgd) can be thought of as\")", "machine learning +(\"stochastic gradient descent (sgd) can be compared to\")"]}