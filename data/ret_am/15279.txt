{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How does the <b>Bellman</b> <b>Equation</b> help to solve <b>Reinforcement Learning</b>?", "url": "https://www.aegissofttech.com/articles/derived-bellman-equation-for-solving-reinforcement-learning.html", "isFamilyFriendly": true, "displayUrl": "https://www.aegissofttech.com/articles/derived-<b>bellman</b>-<b>equation</b>-for-solving...", "snippet": "Therefore, it&#39;s easy to see that the Belmont <b>equation</b> boils down to two equations. The first <b>equation</b> is v of s 1 equals B1 plus c11 at times V of s1 one plus C12V of times s2. The second <b>equation</b> is v of s2 equals B2 plus C 21 at times V of s2 plus C 22 times V as 2. Please find below the two equations:", "dateLastCrawled": "2021-12-02T09:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Reinforcement <b>Learning</b>: <b>Bellman</b> <b>Equation</b> and Optimality (Part 2) | by ...", "url": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/reinforcement-<b>learning</b>-markov-decision-process-part-2...", "snippet": "<b>Bellman</b> <b>Equation</b> for Value Function (State-Value Function) From the above <b>equation</b>, we can see that the value of a s tate can be decomposed into immediate reward(R[t+1]) plus the value of successor state(v[S (t+1)]) with a discount factor(\ud835\udefe).This still stands for <b>Bellman</b> Expectation <b>Equation</b>. But now what we are doing is we are finding the value of a particular state subjected to some policy(\u03c0).This is the difference between the <b>Bellman</b> <b>Equation</b> and the <b>Bellman</b> Expectation <b>Equation</b>.", "dateLastCrawled": "2022-02-02T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>Bellman Equation</b>. V-function and Q-function Explained | by Jordi ...", "url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-<b>bellman-equation</b>-59258a0d3fa7", "snippet": "In summary, we can say that the <b>Bellman equation</b> decomposes the value function into two parts, the immediate reward plus the discounted future values. This <b>equation</b> simplifies the computation of the value function, such that rather than summing <b>over</b> multiple <b>time</b> steps, we can find the optimal solution of a complex problem by breaking it down into simpler, recursive subproblems and finding their optimal solutions.", "dateLastCrawled": "2022-02-03T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Role of <b>Bellman</b>&#39;s <b>Equation</b> in Reinforcement <b>Learning</b>", "url": "https://www.slideshare.net/VARUNKUMAR391/role-of-bellmans-equation-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/.../role-of-<b>bellman</b>s-<b>equation</b>-in-reinforcement-<b>learning</b>", "snippet": "1. Role of <b>Bellman</b>\u2019s <b>Equation</b> in Reinforcement <b>Learning</b> Dr. Varun Kumar Dr. Varun Kumar <b>Machine</b> <b>Learning</b> 1 / 17 2. Outlines 1 Policy in Reinforcement <b>Learning</b> 2 Basic Problem 3 <b>Bellman</b>\u2019s Optimality Criterion 4 Dynamic Programming Algorithm 5 <b>Bellman</b>\u2019s Optimality <b>Equation</b> 6 Example 7 References Dr. Varun Kumar <b>Machine</b> <b>Learning</b> 2 / 17 3.", "dateLastCrawled": "2022-02-02T13:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>machine</b> <b>learning</b> - About the <b>time</b> differences in the <b>Bellman</b> <b>equation</b> ...", "url": "https://datascience.stackexchange.com/questions/53097/about-the-time-differences-in-the-bellman-equation", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/53097/about-the-<b>time</b>-differences-in...", "snippet": "<b>Data Science Stack Exchange</b> is a question and answer site for Data science professionals, <b>Machine</b> <b>Learning</b> specialists, and those interested in <b>learning</b> more about the field. It only takes a minute to sign up. Sign up to join this community", "dateLastCrawled": "2022-01-14T12:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "On Generalized <b>Bellman</b> Equations and Temporal-Di erence <b>Learning</b>", "url": "https://jmlr.csail.mit.edu/papers/volume19/17-283/17-283.pdf", "isFamilyFriendly": true, "displayUrl": "https://jmlr.csail.mit.edu/papers/volume19/17-283/17-283.pdf", "snippet": "Journal of <b>Machine</b> <b>Learning</b> Research 19 (2018) 1-49 Submitted 5/17; Published 9/18 On Generalized <b>Bellman</b> Equations and Temporal-Di erence <b>Learning</b> Huizhen Yu janey.hzyu@gmail.com Reinforcement <b>Learning</b> and Arti cial Intelligence Group Department of Computing Science, University of Alberta Edmonton, AB, T6G 2E8, Canada A. Rupam Mahmood rupam@kindred.ai Kindred Inc. 243 College St Toronto, ON M5T 1R5, Canada Richard S. Sutton rsutton@ualberta.ca Reinforcement <b>Learning</b> and Arti cial ...", "dateLastCrawled": "2021-09-19T05:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "reinforcement <b>learning</b> - How to setup the <b>Bellman</b> <b>Equation</b> as a linear ...", "url": "https://cs.stackexchange.com/questions/142128/how-to-setup-the-bellman-equation-as-a-linear-system-of-equation", "isFamilyFriendly": true, "displayUrl": "https://cs.stackexchange.com/questions/142128/how-to-setup-the-<b>bellman</b>-<b>equation</b>-as-a...", "snippet": "I was watching a video on Reinforcement <b>Learning</b> by Andrew Ng, and at about minute 23 of the video he mentions that we can represent the <b>Bellman</b> <b>equation</b> as a linear <b>system</b> of equations. I am talking", "dateLastCrawled": "2022-01-10T20:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fundamentals of Reinforcement Learning: Policies, Value Functions</b> ...", "url": "https://www.mlq.ai/reinforcement-learning-policies-value-functions-bellman-equation/", "isFamilyFriendly": true, "displayUrl": "https://www.mlq.ai/<b>reinforcement-learning-policies-value-functions</b>-<b>bellman</b>-<b>equation</b>", "snippet": "Another important point about policies is that they depend only on the current state, as opposed to other information <b>like</b> <b>time</b> or previous states. In other words, in a Markov decision process the current state defines all the information used to select the current action. In summary, an agents behavior in an environment is specified by a policy that maps the current state to a set of probabilities for taking each action. The policy also only depends on the current state. Value Functions ...", "dateLastCrawled": "2022-01-31T05:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>the difference between bellman equation and</b> TD Q-<b>learning</b>? - Quora", "url": "https://www.quora.com/What-is-the-difference-between-bellman-equation-and-TD-Q-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-difference-between-bellman-equation-and</b>-TD-Q-<b>learning</b>", "snippet": "Answer: The main difference is that the <b>Bellman</b> <b>Equation</b> requires that you know the Reward Function. Here, knowing the reward function means that you can predict the reward you would receive when executing an action in a given state without necessarily actually executing it. On the other hand, ...", "dateLastCrawled": "2022-01-24T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Hamilton-Jacobi-<b>Bellman</b> Equations for Q-<b>Learning</b> in Continuous <b>Time</b> ...", "url": "https://deepai.org/publication/hamilton-jacobi-bellman-equations-for-q-learning-in-continuous-time", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../hamilton-jacobi-<b>bellman</b>-<b>equations</b>-for-q-<b>learning</b>-in-continuous-<b>time</b>", "snippet": "Continuous-<b>time</b> <b>system</b> identification with neural networks: model structures and fitting criteria ... , while the literature on continuous-<b>time</b> Q-<b>learning</b> is sparse. In discrete <b>time</b>, the <b>Bellman</b> <b>equation</b> for Q-functions can be defined by using dynamic programming in a straightforward manner. However, the corresponding <b>Bellman</b> <b>equation</b> for continuous-<b>time</b> Q-functions has not yet been fully characterized despite some prior attempts. A variant of Q-function is used in [3, 4], which has a ...", "dateLastCrawled": "2022-01-04T09:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement <b>Learning</b>: <b>Bellman</b> <b>Equation</b> and Optimality (Part 2) | by ...", "url": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/reinforcement-<b>learning</b>-markov-decision-process-part-2...", "snippet": "<b>Bellman</b> <b>Equation</b> for Value Function (State-Value Function) From the above <b>equation</b>, we can see that the value of a s tate can be decomposed into immediate reward(R[t+1]) plus the value of successor state(v[S (t+1)]) with a discount factor(\ud835\udefe).This still stands for <b>Bellman</b> Expectation <b>Equation</b>. But now what we are doing is we are finding the value of a particular state subjected to some policy(\u03c0).This is the difference between the <b>Bellman</b> <b>Equation</b> and the <b>Bellman</b> Expectation <b>Equation</b>.", "dateLastCrawled": "2022-02-02T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Bellman</b> <b>Equation</b>", "url": "https://www.codingninjas.com/codestudio/library/the-bellman-equation-1496", "isFamilyFriendly": true, "displayUrl": "https://www.codingninjas.com/codestudio/library/the-<b>bellman</b>-<b>equation</b>-1496", "snippet": "The <b>Bellman</b> <b>Equation</b> has a broader application in solving problems of reinforcement <b>learning</b>. It helps machines learn using rewards as favorable reinforcement.", "dateLastCrawled": "2022-01-21T04:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What are some alternatives to <b>the Bellman Equation in reinforcement</b> ...", "url": "https://www.quora.com/What-are-some-alternatives-to-the-Bellman-Equation-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-some-alternatives-to-the-<b>Bellman</b>-<b>Equation</b>-in...", "snippet": "Answer (1 of 3): To understand when you might diverge from the <b>Bellman</b> <b>equation</b> it\u2019s important to understand what it\u2019s for. Specifically, the <b>Bellman</b> <b>equation</b> defines the expected future (discounted) return from each state in a Markov decision process and can be used to iteratively estimate this ...", "dateLastCrawled": "2022-01-13T18:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bellman</b> <b>equation</b> - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Bellman_equation", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Bellman</b>_<b>equation</b>", "snippet": "<b>Bellman</b> flow chart. A <b>Bellman</b> <b>equation</b>, named after Richard E. <b>Bellman</b>, is a necessary condition for optimality associated with the mathematical optimization method known as dynamic programming. [1] It writes the &quot;value&quot; of a decision problem at a certain point in <b>time</b> in terms of the payoff from some initial choices and the &quot;value&quot; of the remaining decision problem that results from those initial choices.", "dateLastCrawled": "2022-01-20T22:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Solving the Linear <b>Bellman</b> <b>Equation</b> via Dual Kernel Embeddings", "url": "https://homes.cs.washington.edu/~bboots/files/LSMDP-NIPSWS.pdf", "isFamilyFriendly": true, "displayUrl": "https://homes.cs.washington.edu/~bboots/files/LSMDP-NIPSWS.pdf", "snippet": "In the following we brie\ufb02y introduce the linearization of the <b>Bellman</b> optimality <b>equation</b>. 2.1 Continuous <b>Time</b> Stochastic Optimal Control We consider a stochastic optimal control problem in the \ufb01rst-exit setting with state x 2X\u02c6Rn and control u 2U\u02c6Rm. The goal is to construct a control policy u = \u02c7(x) that minimize the expected cumulative cost. The problem can be de\ufb01ned as follows minimize \u02c7 v\u02c7(x(0)) = E q(x(T)) + Z T 0 L(x(t);\u02c7(x(t)))dt ; subject to dx = (x)dt+ B(x)(udt+ \u02d9d ...", "dateLastCrawled": "2022-02-02T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Hamilton-Jacobi-<b>Bellman</b> Equations for Q-<b>Learning</b> in Continuous <b>Time</b> ...", "url": "https://deepai.org/publication/hamilton-jacobi-bellman-equations-for-q-learning-in-continuous-time", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/.../hamilton-jacobi-<b>bellman</b>-<b>equations</b>-for-q-<b>learning</b>-in-continuous-<b>time</b>", "snippet": "The standard Q-function used in reinforcement <b>learning</b> is shown to be the unique viscosity solution of the HJB <b>equation</b>. A necessary and sufficient condition for optimality is provided using the viscosity solution framework. By using the HJB <b>equation</b>, we develop a Q-<b>learning</b> method for continuous-<b>time</b> dynamical systems. A DQN-like algorithm is ...", "dateLastCrawled": "2022-01-04T09:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fundamentals of Reinforcement Learning: Policies, Value Functions</b> ...", "url": "https://www.mlq.ai/reinforcement-learning-policies-value-functions-bellman-equation/", "isFamilyFriendly": true, "displayUrl": "https://www.mlq.ai/<b>reinforcement-learning-policies-value-functions</b>-<b>bellman</b>-<b>equation</b>", "snippet": "In the next section, we&#39;ll look at how value functions can be computed with the <b>Bellman</b> <b>equation</b>. Deriving the <b>Bellman</b> <b>Equation</b>. In reinforcement <b>learning</b>, we want the agent to be able to relate the value of the current state to the value of future states, without waiting to observe all future rewards.", "dateLastCrawled": "2022-01-31T05:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 7, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Bellman equation</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Bellman_equation", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Bellman_equation</b>", "snippet": "A <b>Bellman equation</b>, named after Richard E. <b>Bellman</b>, is a necessary condition for optimality associated with the mathematical optimization method known as dynamic programming. It writes the &quot;value&quot; of a decision problem at a certain point in <b>time</b> in terms of the payoff from some initial choices and the &quot;value&quot; of the remaining decision problem that results from those initial choices.", "dateLastCrawled": "2022-02-03T02:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>the difference between bellman equation and</b> TD Q-<b>learning</b>? - Quora", "url": "https://www.quora.com/What-is-the-difference-between-bellman-equation-and-TD-Q-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-difference-between-bellman-equation-and</b>-TD-Q-<b>learning</b>", "snippet": "Answer: The main difference is that the <b>Bellman</b> <b>Equation</b> requires that you know the Reward Function. Here, knowing the reward function means that you can predict the reward you would receive when executing an action in a given state without necessarily actually executing it. On the other hand, ...", "dateLastCrawled": "2022-01-24T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Disruptive Machine Learning</b>. <b>Machine</b> <b>learning</b> is about to transform ...", "url": "https://towardsdatascience.com/disruptive-machine-learning-7eafd8088166", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>disruptive-machine-learning</b>-7eafd8088166", "snippet": "That sounds obvious, after all, we\u2019ve all been <b>learning</b> all the <b>time</b>. It\u2019s what human beings do, right? But it\u2019s a lot more subtle than that. We know we learn, but really we had absolutely know idea how we learn. We didn\u2019t come to understand that until we tried to teach computers anything at all. Dr. Marvin Minsky demonstrates an early AI <b>system</b>. Fifty ye a rs ago the idea of \u2018artificial intelligence\u2019 seemed less like a pipe dream \u2014 among those in the know \u2014 than something we ...", "dateLastCrawled": "2022-01-15T16:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How does the <b>Bellman</b> <b>Equation</b> help to solve <b>Reinforcement Learning</b>?", "url": "https://www.aegissofttech.com/articles/derived-bellman-equation-for-solving-reinforcement-learning.html", "isFamilyFriendly": true, "displayUrl": "https://www.aegissofttech.com/articles/derived-<b>bellman</b>-<b>equation</b>-for-solving...", "snippet": "Note that the supplies to a tree that <b>can</b> have any number of children. The rule remains the same. The expected value is the weighted sum of each of the child node values weighted by the respective probabilities. Of course, this is just the definition of the expected value for a discrete random variable. If X is our random variable, then the expected value of x is just the sum <b>over</b> all possible values of x of X times P of X.", "dateLastCrawled": "2021-12-02T09:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An Introduction to Reinforcement <b>Learning</b> \u2013 I :: Markov Decision ...", "url": "https://insufficientinformation.wordpress.com/2019/04/20/an-introduction-to-reinforcement-learning-i-markov-decision-processes/", "isFamilyFriendly": true, "displayUrl": "https://insufficientinformation.wordpress.com/2019/04/20/an-introduction-to...", "snippet": "This <b>Bellman</b> <b>equation</b> <b>can</b> be expressed using matrices: The above is a <b>system</b> of linear equations and <b>can</b> be solved directly, though with considerable computational complexity (O(n^3)). For larger MRPs , solutions have to be iterative using techniques such as Dynamic Programming and Temporal-Difference <b>Learning</b> .", "dateLastCrawled": "2022-02-01T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Crystal Clear <b>Reinforcement</b> <b>Learning</b> | by Baijayanta Roy | Towards Data ...", "url": "https://towardsdatascience.com/crystal-clear-reinforcement-learning-7e6c1541365e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/crystal-clear-<b>reinforcement</b>-<b>learning</b>-7e6c1541365e", "snippet": "It uses off-policy data and the <b>Bellman</b> <b>equation</b> to learn the Q-function and uses the Q-function to learn the policy. DDPG <b>can</b> <b>be thought</b> of as being deep Q-<b>learning</b> for continuous action spaces. DDPG is an off-policy algorithm (use replay buffer) DDPG <b>can</b> only be used for environments with continuous action spaces", "dateLastCrawled": "2022-01-29T12:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Deep Reinforcement Learning</b>: Guide to Deep Q-<b>Learning</b>", "url": "https://www.mlq.ai/deep-reinforcement-learning-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.mlq.ai/<b>deep-reinforcement-learning</b>-q-<b>learning</b>", "snippet": "What is <b>Reinforcement Learning</b>? The <b>Bellman</b> <b>Equation</b>; Markov Decision Processes (MDPs) Q-<b>Learning</b> Intuition; Temporal Difference; Deep Q-<b>Learning</b> Intuition ; Experience Replay; Action Selection Policies; Summary: Deep Q-<b>Learning</b>; This post may contain affiliate links. See our policy page for more information. 1. What is <b>Reinforcement Learning</b>? A key differentiator of <b>reinforcement learning</b> from supervised or unsupervised <b>learning</b> is the presence of two things: An environment: This could be ...", "dateLastCrawled": "2022-02-02T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to be an adaptive thinker | Artificial Intelligence By Example", "url": "https://subscription.packtpub.com/book/big-data-and-business-intelligence/9781788990547/1/ch01lvl1sec03/how-to-be-an-adaptive-thinker", "isFamilyFriendly": true, "displayUrl": "https://subscription.packtpub.com/book/big-data-and-business-intelligence/...", "snippet": "In this chapter, we are going to tackle Markov&#39;s Decision Process (Q function) and apply it to reinforcement <b>learning</b> with the <b>Bellman</b> <b>equation</b>. You <b>can</b> find tons of source code and examples on the web. However, most of them are toy experiments that have nothing to do with real life. For example, reinforcement <b>learning</b> <b>can</b> be applied to an e-commerce business delivery person, self-driving vehicle, or a drone. You will find a program that calculates a drone delivery. However, it has many ...", "dateLastCrawled": "2021-12-25T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Part XIII Reinforcement Learning and Control</b>", "url": "http://cs229.stanford.edu/notes2020spring/cs229-notes12.pdf", "isFamilyFriendly": true, "displayUrl": "cs229.stanford.edu/notes2020spring/cs229-notes12.pdf", "snippet": "Our goal in reinforcement <b>learning</b> is to choose actions <b>over</b> <b>time</b> so as to maximize the expected value of the total payo : E R(s 0) + R(s 1) + 2R(s 2) + Note that the reward at timestep tis discounted by a factor of t. Thus, to make this expectation large, we would like to accrue positive rewards as soon as possible (and postpone negative rewards as long as possible). In economic applications where R() is the amount of money made, also has a natural interpretation in terms of the interest ...", "dateLastCrawled": "2022-02-01T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Failure prediction using machine learning in</b> a virtualised HPC <b>system</b> ...", "url": "https://link.springer.com/article/10.1007/s10586-019-02917-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10586-019-02917-1", "snippet": "With the advent of <b>machine</b> <b>learning</b> techniques, the ability to learn from past information to predict future pattern of behaviours makes it possible to predict potential <b>system</b> failure more accurately. Thus, in this paper, we explore the predictive abilities of <b>machine</b> <b>learning</b> by applying a number of algorithms to improve the accuracy of failure prediction. We have developed a failure prediction model using <b>time</b> series and <b>machine</b> <b>learning</b>, and performed comparison based tests on the ...", "dateLastCrawled": "2022-02-02T16:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reinforcement <b>Learning</b> with Q tables | by Mohit Mayank | ITNEXT", "url": "https://itnext.io/reinforcement-learning-with-q-tables-5f11168862c8", "isFamilyFriendly": true, "displayUrl": "https://itnext.io/reinforcement-<b>learning</b>-with-q-tables-5f11168862c8", "snippet": "Anyone with a little knowledge of <b>machine</b> <b>learning</b> will advice you to use convolution neural network and train with the provided images, and yeah it will work. But how? Well, without going into details (maybe an article on this later?!) you train the neural network on sample images first. While training the neural network learns the little features and pattern unique to dog\u2019s image. During training you know the expected output, it is a dog images, so whenever the network predicts wrong we ...", "dateLastCrawled": "2022-01-29T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "linkedin-skill-assessments-quizzes/<b>machine</b>-<b>learning</b>-quiz.md at master ...", "url": "https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes/blob/master/machine-learning/machine-learning-quiz.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/.../blob/master/<b>machine</b>-<b>learning</b>/<b>machine</b>-<b>learning</b>-quiz.md", "snippet": "The supervisor asks to create a <b>machine</b> <b>learning</b> <b>system</b> that will help your hr dep. classify job applicants into well-defined groups.What type of <b>system</b> are more likely to recommend? Q49. Someone of your data science team recommends that you use decision trees, naive Bayes and K-nearest neighbor, all at the same <b>time</b>, on the same training data, and then average the results.", "dateLastCrawled": "2022-02-02T14:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Using Reinforcement Learning to solve Gridworld</b> \u2013 Giga thoughts", "url": "https://gigadom.in/2019/09/02/using-reinforcement-learning-to-solve-gridworld/", "isFamilyFriendly": true, "displayUrl": "https://gigadom.in/2019/09/02/<b>using-reinforcement-learning-to-solve-gridworld</b>", "snippet": "Reinforcement <b>Learning</b> (RL) involves decision making under uncertainty which tries to maximize return <b>over</b> successive states.There are four main elements of a Reinforcement <b>Learning</b> <b>system</b>: a policy, a reward signal, a value function. The policy is a mapping from the states to actions or a probability distribution of actions. Every action the agent takes results in a numerical reward. The agent\u2019s sole purpose is to maximize the reward in the long run.", "dateLastCrawled": "2022-02-02T12:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement <b>Learning</b>: <b>Bellman</b> <b>Equation</b> and Optimality (Part 2) | by ...", "url": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/reinforcement-<b>learning</b>-markov-decision-process-part-2...", "snippet": "<b>Bellman</b> <b>Equation</b> for Value Function (State-Value Function) From the above <b>equation</b>, we <b>can</b> see that the value of a s tate <b>can</b> be decomposed into immediate reward(R[t+1]) plus the value of successor state(v[S (t+1)]) with a discount factor(\ud835\udefe).This still stands for <b>Bellman</b> Expectation <b>Equation</b>. But now what we are doing is we are finding the value of a particular state subjected to some policy(\u03c0).This is the difference between the <b>Bellman</b> <b>Equation</b> and the <b>Bellman</b> Expectation <b>Equation</b>.", "dateLastCrawled": "2022-02-02T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Bellman Equation</b>. V-function and Q-function Explained | by Jordi ...", "url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-<b>bellman-equation</b>-59258a0d3fa7", "snippet": "In summary, we <b>can</b> say that the <b>Bellman equation</b> decomposes the value function into two parts, the immediate reward plus the discounted future values. This <b>equation</b> simplifies the computation of the value function, such that rather than summing <b>over</b> multiple <b>time</b> steps, we <b>can</b> find the optimal solution of a complex problem by breaking it down into simpler, recursive subproblems and finding their optimal solutions.", "dateLastCrawled": "2022-02-03T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "On Generalized <b>Bellman</b> Equations and Temporal-Di erence <b>Learning</b>", "url": "https://jmlr.csail.mit.edu/papers/volume19/17-283/17-283.pdf", "isFamilyFriendly": true, "displayUrl": "https://jmlr.csail.mit.edu/papers/volume19/17-283/17-283.pdf", "snippet": "Journal of <b>Machine</b> <b>Learning</b> Research 19 (2018) 1-49 Submitted 5/17; Published 9/18 On Generalized <b>Bellman</b> Equations and Temporal-Di erence <b>Learning</b> Huizhen Yu janey.hzyu@gmail.com Reinforcement <b>Learning</b> and Arti cial Intelligence Group Department of Computing Science, University of Alberta Edmonton, AB, T6G 2E8, Canada A. Rupam Mahmood rupam@kindred.ai Kindred Inc. 243 College St Toronto, ON M5T 1R5, Canada Richard S. Sutton rsutton@ualberta.ca Reinforcement <b>Learning</b> and Arti cial ...", "dateLastCrawled": "2021-09-19T05:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "On Generalized <b>Bellman</b> Equations and Temporal-Di erence <b>Learning</b>", "url": "http://www.incompleteideas.net/papers/YMS-2018.pdf", "isFamilyFriendly": true, "displayUrl": "www.incompleteideas.net/papers/YMS-2018.pdf", "snippet": "Journal of <b>Machine</b> <b>Learning</b> Research 19 (2018) 1-49 Submitted 5/17; Published 9/18 On Generalized <b>Bellman</b> Equations and Temporal-Di\u21b5erence <b>Learning</b> Huizhen Yu janey.hzyu@gmail.com Reinforcement <b>Learning</b> and Arti\ufb01cial Intelligence Group Department of Computing Science, University of Alberta Edmonton, AB, T6G 2E8, Canada A. Rupam Mahmood rupam@kindred.ai Kindred Inc. 243 College St Toronto, ON M5T 1R5, Canada Richard S. Sutton rsutton@ualberta.ca Reinforcement <b>Learning</b> and Arti\ufb01cial ...", "dateLastCrawled": "2021-08-26T15:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The <b>Bellman</b> <b>Equation</b>", "url": "https://www.codingninjas.com/codestudio/library/the-bellman-equation-1496", "isFamilyFriendly": true, "displayUrl": "https://www.codingninjas.com/codestudio/library/the-<b>bellman</b>-<b>equation</b>-1496", "snippet": "The <b>Bellman</b> <b>Equation</b> has a broader application in solving problems of reinforcement <b>learning</b>. It helps machines learn using rewards as favorable reinforcement.", "dateLastCrawled": "2022-01-21T04:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "reinforcement <b>learning</b> - How to setup the <b>Bellman</b> <b>Equation</b> as a linear ...", "url": "https://cs.stackexchange.com/questions/142128/how-to-setup-the-bellman-equation-as-a-linear-system-of-equation", "isFamilyFriendly": true, "displayUrl": "https://cs.stackexchange.com/questions/142128/how-to-setup-the-<b>bellman</b>-<b>equation</b>-as-a...", "snippet": "I was watching a video on Reinforcement <b>Learning</b> by Andrew Ng, and at about minute 23 of the video he mentions that we <b>can</b> represent the <b>Bellman</b> <b>equation</b> as a linear <b>system</b> of equations. I am talking . Stack Exchange Network. Stack Exchange network consists of 178 Q&amp;A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange. Loading\u2026 0 +0; Tour Start here for a quick ...", "dateLastCrawled": "2022-01-10T20:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What are some alternatives to <b>the Bellman Equation in reinforcement</b> ...", "url": "https://www.quora.com/What-are-some-alternatives-to-the-Bellman-Equation-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-some-alternatives-to-the-<b>Bellman</b>-<b>Equation</b>-in...", "snippet": "Answer (1 of 3): To understand when you might diverge from the <b>Bellman</b> <b>equation</b> it\u2019s important to understand what it\u2019s for. Specifically, the <b>Bellman</b> <b>equation</b> defines the expected future (discounted) return from each state in a Markov decision process and <b>can</b> be used to iteratively estimate this ...", "dateLastCrawled": "2022-01-13T18:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning: Reinforcement Learning \u2014 Markov Decision Processes</b> ...", "url": "https://medium.com/machine-learning-bites/machine-learning-reinforcement-learning-markov-decision-processes-431762c7515b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/<b>machine</b>-<b>learning</b>-bites/<b>machine</b>-<b>learning</b>-reinforcement-<b>learning</b>...", "snippet": "Things are stationary, therefore rules do no change <b>over</b> <b>time</b>. Going back to the reinforcement <b>learning</b> definition above, when <b>compared</b> to the rest of the <b>machine</b> <b>learning</b> tools, we <b>can</b> see better ...", "dateLastCrawled": "2022-01-10T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "linkedin-skill-assessments-quizzes/<b>machine</b>-<b>learning</b>-quiz.md at master ...", "url": "https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes/blob/master/machine-learning/machine-learning-quiz.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/.../blob/master/<b>machine</b>-<b>learning</b>/<b>machine</b>-<b>learning</b>-quiz.md", "snippet": "The supervisor asks to create a <b>machine</b> <b>learning</b> <b>system</b> that will help your hr dep. classify job applicants into well-defined groups.What type of <b>system</b> are more likely to recommend? Q49. Someone of your data science team recommends that you use decision trees, naive Bayes and K-nearest neighbor, all at the same <b>time</b>, on the same training data, and then average the results.", "dateLastCrawled": "2022-02-02T14:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Failure prediction using machine learning in</b> a virtualised HPC <b>system</b> ...", "url": "https://link.springer.com/article/10.1007/s10586-019-02917-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10586-019-02917-1", "snippet": "With the advent of <b>machine</b> <b>learning</b> techniques, the ability to learn from past information to predict future pattern of behaviours makes it possible to predict potential <b>system</b> failure more accurately. Thus, in this paper, we explore the predictive abilities of <b>machine</b> <b>learning</b> by applying a number of algorithms to improve the accuracy of failure prediction. We have developed a failure prediction model using <b>time</b> series and <b>machine</b> <b>learning</b>, and performed comparison based tests on the ...", "dateLastCrawled": "2022-02-02T16:45:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bellman Optimality Equation in Reinforcement Learning</b>", "url": "https://www.analyticsvidhya.com/blog/2021/02/understanding-the-bellman-optimality-equation-in-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.analyticsvidhya.com/blog/2021/02/understanding-the-<b>bellman</b>-optimality...", "snippet": "The Q-<b>learning</b> algorithm (which is nothing but a technique to solve the optimal policy problem) iteratively updates the Q-values for each state-action pair using the <b>Bellman</b> Optimality <b>Equation</b> until the Q-function (Action-Value function) converges to the optimal Q-function, q\u2217. This process is called Value-Iteration.", "dateLastCrawled": "2022-01-30T16:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Automating Analogy: Identifying Meaning Across Domains</b> via AI | by Sean ...", "url": "https://towardsdatascience.com/automating-analogy-using-ai-to-help-researchers-make-discoveries-1ca04e9b620", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/automating-<b>analogy</b>-using-ai-to-help-researchers-make...", "snippet": "That optimization is driven by Hamilton\u2013Jacobi\u2013<b>Bellman</b> <b>equation</b> (HJB), ... This is the power of using automated <b>analogy</b> to make connections between areas we might never think to link together. It\u2019s a nice example of augmenting the way people already work, by using \u201cintelligent\u201d machines that operate in a similar fashion. But, is it really worth exploring the use of the HJB <b>equation</b> matched with Clarke gradients, as used by the authors of an economics journal, to learn the ...", "dateLastCrawled": "2022-01-24T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Reinforcement Learning</b> as Heuristic Search <b>Analogy</b> - DataDrivenInvestor", "url": "https://medium.datadriveninvestor.com/reinforcement-learning-as-heuristic-search-analogy-31d92b06dadd", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/<b>reinforcement-learning</b>-as-heuristic-search...", "snippet": "Essentially <b>Bellman</b> Optimality <b>Equation</b> says to choose the action that maximizes R(s) + (Some Heuristic). The Heuristic here is the value of your future state upon choosing your action (a), It is also called Value Function, denoted by V. In essence the heuristic changes for every state and action you are in. In this way, the RL algorithm can essentially model most arbitrary heuristic functions present in A* algorithms. So how exactly does it learn this heuristic. Well I will tell you one way ...", "dateLastCrawled": "2022-01-21T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Statistics <b>and Samples in Distributional Reinforcement Learning</b>", "url": "http://proceedings.mlr.press/v97/rowland19a/rowland19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/rowland19a/rowland19a.pdf", "snippet": "Statistics <b>and Samples in Distributional Reinforcement Learning</b> ... The classical <b>Bellman</b> <b>equation</b> (<b>Bellman</b>,1957) relates ex-pected returns at each state-action pair (x;a) 2XA to the expected returns at possible next states in the MDP by: Q\u02c7(x;a)=E\u02c7[R0+ Q\u02c7(X1;A1)jX0=x;A0=a]: (1) This gives rise to the following \ufb01xed-point iteration scheme Q(x;a) E\u02c7[R0 + Q(X1;A1)jX0 = x;A0 = a]; (2) for updating a collection of approximations (Q(x;a)j(x;a) 2 XA ) towards their true values. This ...", "dateLastCrawled": "2021-11-30T16:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>MDP and the Bellman equation</b> | ROS Robotics Projects - Second Edition", "url": "https://subscription.packtpub.com/book/iot-and-hardware/9781838649326/8/ch08lvl1sec76/mdp-and-the-bellman-equation", "isFamilyFriendly": true, "displayUrl": "https://subscription.packtpub.com/.../8/ch08lvl1sec76/<b>mdp-and-the-bellman-equation</b>", "snippet": "<b>MDP and the Bellman equation</b>. In order to solve any reinforcement <b>learning</b> problem, the problem should be defined or modeled as a MDP. A Markov property is termed by the following condition: the future is independent of the past, given the present. This means that the system doesn&#39;t depend on any past history of data and the future depends only ...", "dateLastCrawled": "2021-12-24T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An <b>introduction to Q-Learning: Reinforcement Learning</b>", "url": "https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/an-<b>introduction-to-q-learning-reinforcement-learning</b>", "snippet": "In that description of how we pursue our goals in daily life, we framed for ourselves a representative <b>analogy</b> of reinforcement <b>learning</b>. Let me summarize the above example reformatting the main points of interest. Our reality contains environments in which we perform numerous actions. Sometimes we get good or positive rewards for some of these actions in order to achieve goals. During the entire course of life, our mental and physical states evolve. We strengthen our actions in order to get ...", "dateLastCrawled": "2022-01-31T09:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Physics-informed <b>machine</b> <b>learning</b>", "url": "https://www.researchgate.net/publication/351814752_Physics-informed_machine_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/351814752_Physics-informed_<b>machine</b>_<b>learning</b>", "snippet": "<b>Machine</b> <b>learning</b> has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained ...", "dateLastCrawled": "2022-01-26T05:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reinforcement <b>learning</b>: <b>Temporal-Difference</b>, SARSA, Q-<b>Learning</b> ...", "url": "https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/reinforcement-<b>learning</b>-<b>temporal-difference</b>-sarsa-q...", "snippet": "<b>Bellman</b> <b>equation</b>; Value, policy functions and iterations; Some Psychology. You may skip this section, it\u2019s optional and not a pre-requisite for the rest of the post. I love studying artificial intelligence concepts while correlating the m to psychology \u2014 Human behaviour and the brain. Reinforcement <b>learning</b> is no exception. Our topic of interest \u2014 <b>Temporal difference</b> was a term coined by Richard S. Sutton. This post is derived from his and Andrew Barto \u2019s book \u2014 An introduction to ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Exam summary: Dynamic Programming and Optimal Control</b> | SeanBone.ch", "url": "http://seanbone.ch/exam-summary-dynamic-programming-and-optimal-control/", "isFamilyFriendly": true, "displayUrl": "seanbone.ch/<b>exam-summary-dynamic-programming-and-optimal-control</b>", "snippet": "Solving the <b>Bellman</b> <b>Equation</b> Value Iteration (VI) Policy Iteration (PI) <b>Analogy</b> and comparison between VI and PI; Variants of VI and PI; Connections to Linear Algebra; Linear Programming (LP) Discounted Problems; Shortest Path problems and Deterministic Finite State systems The Shortest Path (SP) problem; The Deterministic Finite State (DFS) problem; Equivalence of SP and DFS; Hidden Markov Models (HMM) and the Viterbi algorithm; Shortest Path algorithms Label-correcting methods (LCA ...", "dateLastCrawled": "2022-01-22T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Algorithms for Solving High Dimensional PDEs: From Nonlinear ... - DeepAI", "url": "https://deepai.org/publication/algorithms-for-solving-high-dimensional-pdes-from-nonlinear-monte-carlo-to-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/algorithms-for-solving-high-dimensional-pdes-from...", "snippet": "In recent years, tremendous progress has been made on numerical algorithms for solving partial differential equations (PDEs) in a very high dimension, using ideas from either nonlinear (multilevel) Monte Carlo or deep <b>learning</b>.They are potentially free of the curse of dimensionality for many different applications and have been proven to be so in the case of some nonlinear Monte Carlo methods for nonlinear parabolic PDEs. In this paper, we review these numerical and theoretical advances.", "dateLastCrawled": "2022-01-09T23:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "5 most common evaluation metrics used by a <b>machine</b> <b>learning</b> &amp; deep ...", "url": "https://maciejzalwert.medium.com/5-most-common-evaluation-metrics-used-by-a-machine-learning-deep-learning-scientists-that-you-3eaa295f9fdc", "isFamilyFriendly": true, "displayUrl": "https://maciejzalwert.medium.com/5-most-common-evaluation-metrics-used-by-a-<b>machine</b>...", "snippet": "5 the most common evaluation metrics used by a <b>machine</b> <b>learning</b> &amp; deep <b>learning</b> scientists that you should know in depth. Evaluation metrics are the foundations of every ML/AI project. The main goal is to evaluate performance of a particular model. Unfortunately, very often happens that certain metrics are not completely understood \u2014 especially with a client side. In this article I will introduce 5 most common metrics and try to show some potential idiosyncratic* risks they have. Accuracy ...", "dateLastCrawled": "2022-01-26T12:22:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(bellman equation)  is like +(machine learning system \"learning\" over time)", "+(bellman equation) is similar to +(machine learning system \"learning\" over time)", "+(bellman equation) can be thought of as +(machine learning system \"learning\" over time)", "+(bellman equation) can be compared to +(machine learning system \"learning\" over time)", "machine learning +(bellman equation AND analogy)", "machine learning +(\"bellman equation is like\")", "machine learning +(\"bellman equation is similar\")", "machine learning +(\"just as bellman equation\")", "machine learning +(\"bellman equation can be thought of as\")", "machine learning +(\"bellman equation can be compared to\")"]}