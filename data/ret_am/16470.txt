{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "It turns out we can derive the mean-<b>squared</b> <b>loss</b> by considering a typical linear regression problem. ... (s_j\\) is a probability assigned to an incorrect class) to be large, which will in <b>turn</b> encourage \\(s_j\\) to be low. This alternative version seems to tie in more closely to the binary cross entropy that we obtained from the maximum likelihood estimate, but the first version appears to be more commonly used both in practice and in teaching. It turns out that it doesn\u2019t really matter ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Lesson 5: Regression <b>Shrinkage</b> Methods - STAT ONLINE", "url": "https://online.stat.psu.edu/stat508/book/export/html/732", "isFamilyFriendly": true, "displayUrl": "https://online.stat.psu.edu/stat508/book/export/html/732", "snippet": "Here you can see that when \\(\\beta^2/\\sigma^2\\) is set at 0.5, 1.0 and 2.0 the <b>squared</b> <b>loss</b> ratio is always &lt; 1 no matter what \\(d^{2}_{j}\\) is and you will always benefit by doing ridge regression (with \\(\\lambda = 1\\)). If \\(\\beta^2/\\sigma^2\\) becomes too big, at first the <b>squared</b> <b>loss</b> ratio is &lt; 1 and you still benefit for small \\(d^{2}_{j ...", "dateLastCrawled": "2022-01-31T06:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What is <b>the reason behind taking squared error for</b> the cost ... - Quora", "url": "https://www.quora.com/What-is-the-reason-behind-taking-squared-error-for-the-cost-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-reason-behind-taking-squared-error-for</b>-the-cost-function", "snippet": "Answer (1 of 2): Firstly, it is important to understand why we can\u2019t simply take the difference between the predicted and actual values of the parameter for the cost function. What we essentially want to achieve using the cost function is a measure of how far off our estimation is from the actua...", "dateLastCrawled": "2022-01-16T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "python - Deep-Learning Nan <b>loss</b> reasons - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/40050397", "snippet": "You can often tell if this is the case if the <b>loss</b> begins to increase and then diverges to infinity. I am not to familiar with the DNNClassifier but I am guessing it uses the categorical cross entropy cost function. This involves <b>taking</b> the log of the prediction which diverges as the prediction approaches zero. That is why people usually add a small epsilon value to the prediction to prevent this divergence. I am guessing the DNNClassifier probably does this or uses the tensorflow opp for it ...", "dateLastCrawled": "2022-01-28T09:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Multinomial Logistic <b>Loss</b> vs (Cross Entropy vs Square Error) - Cross ...", "url": "https://stats.stackexchange.com/questions/166958/multinomial-logistic-loss-vs-cross-entropy-vs-square-error/172790", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/166958/multinomial-logistic-<b>loss</b>-vs-cross...", "snippet": "SHORT ANSWER According to other answers Multinomial Logistic <b>Loss</b> and Cross Entropy <b>Loss</b> are the same. Cross Entropy <b>Loss</b> is an alternative cost function for NN with sigmoids activation function introduced artificially to eliminate the dependency on $\\sigma&#39;$ on the update equations. Some times this term slows down the learning process ...", "dateLastCrawled": "2022-01-28T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to <b>Interpret Regression Models that have Significant Variables</b> but ...", "url": "https://statisticsbyjim.com/regression/low-r-squared-regression/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/regression/low-r-<b>squared</b>-regression", "snippet": "That same reason applies to R-<b>squared</b>, which is literally just <b>taking</b> r (the correlation) and squaring it. So, a generic rule that applies to all cases across all fields of study just isn\u2019t possible. Consequently, I don\u2019t subscribe to any of them. Instead, you need to be familiar with the subject area and what other studies have obtained. Then you can see how your study compares to them. Reply. Reem says. December 11, 2021 at 1:00 am. Hi Jim, Thank you for the useful information. I just ...", "dateLastCrawled": "2022-02-02T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Neural Network</b> Training <b>Is Like</b> Lock Picking - Cross Validated", "url": "https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/352036", "snippet": "A lot of times you&#39;ll see an initial <b>loss</b> of something ridiculous, <b>like</b> 6.5. Conceptually this means that your output is heavily saturated, for example toward 0. For example $-0.3\\ln(0.99)-0.7\\ln(0.01) = 3.2$, so if you&#39;re seeing a <b>loss</b> that&#39;s bigger than 1, it&#39;s likely your model is very skewed. This usually happens when your <b>neural network</b> weights aren&#39;t properly balanced, especially closer to the softmax/sigmoid. So this would tell you if your initialization is bad. You can study this ...", "dateLastCrawled": "2022-01-30T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Customize <b>loss function</b> to make LSTM model more applicable in stock ...", "url": "https://towardsdatascience.com/customize-loss-function-to-make-lstm-model-more-applicable-in-stock-price-prediction-b1c50e50b16c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/customize-<b>loss-function</b>-to-make-lstm-model-more...", "snippet": "I t\u2019s not because something goes <b>wrong</b> in the tutorials or the model is not well-trained enough. But fundamentally, there are several major limitations that are hard to solve. 1. All free libraries only provide daily data of stock price \u2014 without real-time data, it\u2019s impossible for us to execute any orders within the day. 2. The commonly used <b>loss function</b> (MSE) is a purely statistical <b>loss function</b> \u2013pure price difference doesn\u2019t represent the full picture. 3. LSTM model or any ...", "dateLastCrawled": "2022-01-28T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "multivariable calculus - <b>Gradient</b> of <b>squared</b> $2$-norm - Mathematics ...", "url": "https://math.stackexchange.com/questions/883016/gradient-of-squared-2-norm", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/883016", "snippet": "This answer is not useful. Show activity on this post. Another approach that extends to more general settings is to use the connection between the norm and the inner product, \u2016 x \u2016 2 = ( x, x). We have the finite difference, \u2016 x + s h \u2016 2 \u2212 \u2016 x \u2016 2 = ( x + s h, x + s h) \u2212 ( x, x) = ( x, x) + 2 s ( x, h) + s 2 ( h, h) \u2212 ( x, x ...", "dateLastCrawled": "2022-01-25T05:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - <b>NaN</b> <b>loss</b> when training regression network - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/37232782", "snippet": "Regression with neural networks is hard to get working because the output is unbounded, so you are especially prone to the exploding gradients problem (the likely cause of the nans).. Historically, one key solution to exploding gradients was to reduce the learning rate, but with the advent of per-parameter adaptive learning rate algorithms <b>like</b> Adam, you no longer need to set a learning rate to get good performance.", "dateLastCrawled": "2022-01-27T22:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "For example, the cross-entropy <b>loss</b> would invoke a much higher <b>loss</b> than the hinge <b>loss</b> if our (un-normalized) scores were \\([10, 8, 8]\\) versus \\([10, -10, -10]\\), where the first class is correct. In fact, the (multi-class) hinge <b>loss</b> would recognize that the correct class score already exceeds the other scores by more than the margin, so it will invoke zero <b>loss</b> on both scores. Once the margins are satisfied, the SVM will no longer optimize the weights in an attempt to \u201cdo better ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Picking <b>Loss</b> Functions - A comparison between MSE, Cross Entropy, and ...", "url": "https://rohanvarma.me/Loss-Functions/?source=post_page---------------------------", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions/?source=post_page---------------------------", "snippet": "It turns out we can derive the mean-<b>squared</b> <b>loss</b> by considering a typical linear regression problem. ... we can write down the likelihood by simply <b>taking</b> the product across the data: <b>Similar</b> to above, we can take the log of the above expression and use properties of logs to simplify, and finally invert our entire expression to obtain the cross entropy <b>loss</b>: The Cross-Entropy <b>Loss</b> in the case of multi-class classification. Let\u2019s supposed that we\u2019re now interested in applying the cross ...", "dateLastCrawled": "2020-05-27T14:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Lesson 5: Regression <b>Shrinkage</b> Methods - STAT ONLINE", "url": "https://online.stat.psu.edu/stat508/book/export/html/732", "isFamilyFriendly": true, "displayUrl": "https://online.stat.psu.edu/stat508/book/export/html/732", "snippet": "Here you can see that when \\(\\beta^2/\\sigma^2\\) is set at 0.5, 1.0 and 2.0 the <b>squared</b> <b>loss</b> ratio is always &lt; 1 no matter what \\(d^{2}_{j}\\) is and you will always benefit by doing ridge regression (with \\(\\lambda = 1\\)). If \\(\\beta^2/\\sigma^2\\) becomes too big, at first the <b>squared</b> <b>loss</b> ratio is &lt; 1 and you still benefit for small \\(d^{2}_{j ...", "dateLastCrawled": "2022-01-31T06:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Multinomial Logistic <b>Loss</b> vs (Cross Entropy vs Square Error) - Cross ...", "url": "https://stats.stackexchange.com/questions/166958/multinomial-logistic-loss-vs-cross-entropy-vs-square-error/172790", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/166958/multinomial-logistic-<b>loss</b>-vs-cross...", "snippet": "SHORT ANSWER According to other answers Multinomial Logistic <b>Loss</b> and Cross Entropy <b>Loss</b> are the same. Cross Entropy <b>Loss</b> is an alternative cost function for NN with sigmoids activation function introduced artificially to eliminate the dependency on $\\sigma&#39;$ on the update equations. Some times this term slows down the learning process ...", "dateLastCrawled": "2022-01-28T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Loss functions for classification</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Loss_functions_for_classification", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Loss_functions_for_classification</b>", "snippet": "In machine learning and mathematical optimization, <b>loss functions for classification</b> are computationally feasible <b>loss</b> functions representing the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to). Given as the space of all possible inputs (usually ), and = {,} as the set of labels (possible outputs), a typical goal of classification algorithms is to find a function : \u21a6 which best predicts a label ...", "dateLastCrawled": "2022-02-03T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Customize <b>loss function</b> to make LSTM model more applicable in stock ...", "url": "https://towardsdatascience.com/customize-loss-function-to-make-lstm-model-more-applicable-in-stock-price-prediction-b1c50e50b16c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/customize-<b>loss-function</b>-to-make-lstm-model-more...", "snippet": "I t\u2019s not because something goes <b>wrong</b> in the tutorials or the model is not well-trained enough. But fundamentally, there are several major limitations that are hard to solve. 1. All free libraries only provide daily data of stock price \u2014 without real-time data, it\u2019s impossible for us to execute any orders within the day. 2. The commonly used <b>loss function</b> (MSE) is a purely statistical <b>loss function</b> \u2013pure price difference doesn\u2019t represent the full picture. 3. LSTM model or any ...", "dateLastCrawled": "2022-01-28T14:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "definition - Why square the difference instead of <b>taking</b> the absolute ...", "url": "https://stats.stackexchange.com/questions/118/why-square-the-difference-instead-of-taking-the-absolute-value-in-standard-devia", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/118", "snippet": "Squaring however does have a problem as a measure of spread and that is that the units are all <b>squared</b>, whereas we might prefer the spread to be in the same units as the original data (think of <b>squared</b> pounds, <b>squared</b> dollars, or <b>squared</b> apples). Hence the square root allows us to return to the original units. I suppose you could say that absolute difference assigns equal weight to the spread of data whereas squaring emphasises the extremes. Technically though, as others have pointed out ...", "dateLastCrawled": "2022-01-29T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "Neural network models learn a mapping from inputs to outputs from examples and the choice of <b>loss</b> function must match the framing of the specific predictive modeling problem, such as classification or regression. Further, the configuration of the output layer must also be appropriate for the chosen <b>loss</b> function.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Gradient Descent for Linear Regression</b> Explained, Step by Step", "url": "https://machinelearningcompass.com/machine_learning_math/gradient_descent_for_linear_regression/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningcompass.com/machine_learning_math/gradient_descent_for_linear...", "snippet": "As you may notice, this looks very <b>similar</b> to our simplified example with q q q and q \u2032 q&#39; q \u2032! Here we have three variables, so we want to get to the \u201cpoint\u201d where z = 0 z=0 z = 0. But because we are in three dimensions, there isn\u2019t just one point where this is true, there is an entire plane where z = 0 z=0 z = 0. So to make the plot a bit easier to interpret, I also added in a greenish plane at z = 0 z=0 z = 0. We can now apply the same logic we used in our 2d-example. Currently ...", "dateLastCrawled": "2022-01-30T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - <b>NaN</b> <b>loss</b> when training regression network - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/37232782", "snippet": "Regression with neural networks is hard to get working because the output is unbounded, so you are especially prone to the exploding gradients problem (the likely cause of the nans).. Historically, one key solution to exploding gradients was to reduce the learning rate, but with the advent of per-parameter adaptive learning rate algorithms like Adam, you no longer need to set a learning rate to get good performance.", "dateLastCrawled": "2022-01-27T22:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "In this post, I\u2019ll discuss three common <b>loss</b> functions: the mean-<b>squared</b> (MSE) <b>loss</b>, cross-entropy <b>loss</b>, and the hinge <b>loss</b>. These are the most commonly used functions I\u2019ve seen used in traditional machine learning and deep learning models, so I <b>thought</b> it would be a good idea to figure out the underlying theory behind each one, and when to prefer one over the others.", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Picking <b>Loss</b> Functions - A comparison between MSE, Cross Entropy, and ...", "url": "https://rohanvarma.me/Loss-Functions/?source=post_page---------------------------", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions/?source=post_page---------------------------", "snippet": "In this post, I\u2019ll discuss three common <b>loss</b> functions: the mean-<b>squared</b> (MSE) <b>loss</b>, cross-entropy <b>loss</b>, and the hinge <b>loss</b>. These are the most commonly used functions I\u2019ve seen used in traditional machine learning and deep learning models, so I <b>thought</b> it would be a good idea to figure out the underlying theory behind each one, and when to prefer one over the others.", "dateLastCrawled": "2020-05-27T14:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Multinomial Logistic <b>Loss</b> vs (Cross Entropy vs Square Error) - Cross ...", "url": "https://stats.stackexchange.com/questions/166958/multinomial-logistic-loss-vs-cross-entropy-vs-square-error/172790", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/166958/multinomial-logistic-<b>loss</b>-vs-cross...", "snippet": "SHORT ANSWER According to other answers Multinomial Logistic <b>Loss</b> and Cross Entropy <b>Loss</b> are the same. Cross Entropy <b>Loss</b> is an alternative cost function for NN with sigmoids activation function introduced artificially to eliminate the dependency on $\\sigma&#39;$ on the update equations. Some times this term slows down the learning process ...", "dateLastCrawled": "2022-01-28T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "MSE is <b>negative</b> when returned by cross_val_score \u00b7 Issue #2439 \u00b7 scikit ...", "url": "https://github.com/scikit-learn/scikit-learn/issues/2439", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/scikit-learn/scikit-learn/issues/2439", "snippet": "This also happened to Brier_score_<b>loss</b>, it works perfectly fine using Brier_score_<b>loss</b>, but it gets confusing when it comes from the GridSearchCV, the <b>negative</b> Brier_score_<b>loss</b> returns. At least, it would be better output something like, because Brier_score_<b>loss</b> is a <b>loss</b> (the lower the better), the scoring function here flip the sign to make it <b>negative</b>.", "dateLastCrawled": "2022-01-29T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "definition - Why square the difference instead of <b>taking</b> the absolute ...", "url": "https://stats.stackexchange.com/questions/118/why-square-the-difference-instead-of-taking-the-absolute-value-in-standard-devia", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/118", "snippet": "To summarise, least absolute deviations is more robust to outliers than ordinary least squares, but it <b>can</b> be unstable (small change in even a single datum <b>can</b> give big change in fitted line) and doesn&#39;t always have a unique solution - there <b>can</b> be a whole range of fitted lines. Also least absolute deviations requires iterative methods, while ordinary least squares has a simple closed-form solution, though that&#39;s not such a big deal now as it was in the days of Gauss and Legendre, of course.", "dateLastCrawled": "2022-01-29T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Ordinary Least Squares Linear Regression: Flaws, Problems and Pitfalls</b> ...", "url": "http://www.clockbackward.com/2009/06/18/ordinary-least-squares-linear-regression-flaws-problems-and-pitfalls/", "isFamilyFriendly": true, "displayUrl": "www.clockbackward.com/2009/06/18/<b>ordinary-least-squares-linear-regression</b>-flaws...", "snippet": "This solution for c0, c1, and c2 (which <b>can</b> <b>be thought</b> of as the plane 52.8233 \u2013 0.0295932 x1 + 0.101546 x2) <b>can</b> be visualized as: That means that for a given weight and age we <b>can</b> attempt to estimate a person\u2019s height by simply looking at the \u201cheight\u201d of the plane for their weight and age. Why Is Least Squares So Popular? We\u2019ve now seen that least <b>squared</b> regression provides us with a method for measuring \u201caccuracy\u201d (i.e. the sum of <b>squared</b> errors) and that is what makes it ...", "dateLastCrawled": "2022-01-28T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "My Weight <b>Loss</b> Journey \u2013 <b>SquaRed</b> Dishes", "url": "https://squareddishes.wordpress.com/2015/10/07/my-weight-loss-journey/", "isFamilyFriendly": true, "displayUrl": "https://<b>squared</b>dishes.wordpress.com/2015/10/07/my-weight-<b>loss</b>-journey", "snippet": "After the initial 25 pound <b>loss</b>, I plateaued. But it was entirely my fault. I didn\u2019t eat what I was supposed to. I was eating junk, processed foods, and fast foods like they were going out of style. Mentally, I had convinced myself that I was <b>taking</b> the medication how I was supposed to and it would take care of the rest. Boy, have I been ...", "dateLastCrawled": "2022-01-26T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Today I heard that voltage <b>squared</b> equals power, I <b>thought</b> voltage X ...", "url": "https://www.quora.com/Today-I-heard-that-voltage-squared-equals-power-I-thought-voltage-X-current-equals-power-What-s-the-difference", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Today-I-heard-that-voltage-<b>squared</b>-equals-power-I-<b>thought</b>...", "snippet": "Answer (1 of 8): What you heard has a ring of truth to it in the sense that there is proportionality relationship. Power is proportional to the voltage square. To sort all this out only two relationship are needed: P=IV and V=IR Those two equations and a bit of algebra are all that is needed to...", "dateLastCrawled": "2022-01-16T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Wrong</b> Calculations: Why Does Excel Show <b>Wrong</b> Results &amp; How to Fix It", "url": "https://professor-excel.com/wrong-calculations-why-does-excel-show-a-wrong-result/", "isFamilyFriendly": true, "displayUrl": "https://professor-excel.com/<b>wrong</b>-calculations-why-does-excel-show-<b>a-wrong</b>-result", "snippet": "I <b>can</b> take them out and put them back in and am still getting <b>a wrong</b> answer. I have finally added them on an adding machine and found that one of the answers is correct. I recreated both columns using copy and paste (columns now side by side). The numbers are identical but yield different totals. I am at a <b>loss</b> to figure out why this is happening.", "dateLastCrawled": "2022-02-02T07:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is there <b>any chance that a calculator can show a wrong</b> answer? - Quora", "url": "https://www.quora.com/Is-there-any-chance-that-a-calculator-can-show-a-wrong-answer", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-<b>any-chance-that-a-calculator-can-show-a-wrong</b>-answer", "snippet": "Answer (1 of 3): Of course. Most calculators perform their calculations with only a fixed precision. The first electronic calculators that appeared usually worked in base 10, using 4 bits to store each base-10 fraction and exponent digit, and did their arithmetic using BCD addition, subtraction, ...", "dateLastCrawled": "2022-02-02T21:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "It turns out we <b>can</b> derive the mean-<b>squared</b> <b>loss</b> by considering a typical linear regression problem. ... we <b>can</b> write down the likelihood by simply <b>taking</b> the product across the data: \\[L(x, y) = \\prod_{i = 1}^{N}[h_\\theta(x_i)]^{(y_i)} [1 - h_\\theta(x_i)]^{(1 - y_i)}\\] Similar to above, we <b>can</b> take the log of the above expression and use properties of logs to simplify, and finally invert our entire expression to obtain the cross entropy <b>loss</b>: \\[J = -\\sum_{i=1}^{N} y_i\\log (h_\\theta(x_i ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Picking <b>Loss</b> Functions - A comparison between MSE, Cross Entropy, and ...", "url": "https://rohanvarma.me/Loss-Functions/?source=post_page---------------------------", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions/?source=post_page---------------------------", "snippet": "It turns out we <b>can</b> derive the mean-<b>squared</b> <b>loss</b> by considering a typical linear regression problem. ... we <b>can</b> write down the likelihood by simply <b>taking</b> the product across the data: Similar to above, we <b>can</b> take the log of the above expression and use properties of logs to simplify, and finally invert our entire expression to obtain the cross entropy <b>loss</b>: The Cross-Entropy <b>Loss</b> in the case of multi-class classification. Let\u2019s supposed that we\u2019re now interested in applying the cross ...", "dateLastCrawled": "2020-05-27T14:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Does <b>iron loss depend on current or voltage? - Quora</b>", "url": "https://www.quora.com/Does-iron-loss-depend-on-current-or-voltage", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Does-<b>iron-loss-depend-on-current-or-voltage</b>", "snippet": "If you are asking about a transformer, the two electrical (heating) losses are \u201ccopper\u201d and \u201ciron\u201d. The copper <b>loss</b> is due to the series resistance of the windings and is proportional to the current <b>squared</b>. The magnetic field in the core is howev...", "dateLastCrawled": "2021-09-17T03:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Ordinary Least Squares Linear Regression: Flaws, Problems and Pitfalls</b> ...", "url": "http://www.clockbackward.com/2009/06/18/ordinary-least-squares-linear-regression-flaws-problems-and-pitfalls/", "isFamilyFriendly": true, "displayUrl": "www.clockbackward.com/2009/06/18/<b>ordinary-least-squares-linear-regression</b>-flaws...", "snippet": "Least squares regression <b>can</b> perform very badly when some points in the training data have excessively large or small values for the dependent variable <b>compared</b> to the rest of the training data. The reason for this is that since the least squares method is concerned with minimizing the sum of the <b>squared</b> error, any training point that has a dependent value that differs a lot from the rest of the data will have a disproportionately large effect on the resulting constants that are being solved ...", "dateLastCrawled": "2022-01-28T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lesson 5: Regression <b>Shrinkage</b> Methods - STAT ONLINE", "url": "https://online.stat.psu.edu/stat508/book/export/html/732", "isFamilyFriendly": true, "displayUrl": "https://online.stat.psu.edu/stat508/book/export/html/732", "snippet": "Here you <b>can</b> see that when \\(\\beta^2/\\sigma^2\\) is set at 0.5, 1.0 and 2.0 the <b>squared</b> <b>loss</b> ratio is always &lt; 1 no matter what \\(d^{2}_{j}\\) is and you will always benefit by doing ridge regression (with \\(\\lambda = 1\\)). If \\(\\beta^2/\\sigma^2\\) becomes too big, at first the <b>squared</b> <b>loss</b> ratio is &lt; 1 and you still benefit for small \\(d^{2}_{j}\\) . However, as \\(d^{2}_{j}\\) gets big this ratio very quickly overshoots 1 and ridge regression is not doing as well as basic linear regression. The ...", "dateLastCrawled": "2022-01-31T06:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 5, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Loss functions for classification</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Loss_functions_for_classification", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Loss_functions_for_classification</b>", "snippet": "Bayes consistency. Utilizing Bayes&#39; theorem, it <b>can</b> be shown that the optimal /, i.e., the one that minimizes the expected risk associated with the zero-one <b>loss</b>, implements the Bayes optimal decision rule for a binary classification problem and is in the form of / (\u2192) = {(\u2192) &gt; (\u2192) (\u2192) = (\u2192) (\u2192) &lt; (\u2192). A <b>loss</b> function is said to be classification-calibrated or Bayes consistent if its optimal is such that / (\u2192) = \u2061 ((\u2192)) and is thus optimal under the Bayes decision rule. A ...", "dateLastCrawled": "2022-02-03T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Introduction <b>Loss</b> Functions - Optimization | Coursera", "url": "https://www.coursera.org/lecture/launching-machine-learning/introduction-loss-functions-dWGPb", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/launching-machine-learning/introduction-<b>loss</b>...", "snippet": "One of the most commonly used <b>loss</b> functions for classification is called Cross Entropy, or log <b>loss</b>. Here, we have a similar graph to what we saw in the last slide, only instead of showing the <b>loss</b> for RMSE, I&#39;ve shown the value of a new <b>loss</b> function called Cross Entropy. Note that unlike RMSE, cross entropy penalizes bad predictions very strongly, even in this limited domain. Let&#39;s walk through an example so we <b>can</b> better understand how the formula works. The formula for cross entropy ...", "dateLastCrawled": "2022-01-14T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Low R-<b>squared values in multiple regression analysis</b>?", "url": "https://www.researchgate.net/post/Low-R-squared-values-in-multiple-regression-analysis", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/Low-R-<b>squared-values-in-multiple-regression-analysis</b>", "snippet": "R-<b>squared</b> does not indicate whether a regression model is adequate. we <b>can</b> have a low R-<b>squared</b> value for a good model, or a high R-<b>squared</b> value for a model that does not fit the data. high R^2 ...", "dateLastCrawled": "2022-02-02T18:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What should I do when my <b>neural network</b> doesn&#39;t learn? - Cross Validated", "url": "https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/352036", "snippet": "<b>Loss</b> functions are not measured on the correct scale (for example, cross-entropy <b>loss</b> <b>can</b> be expressed in terms of ... by <b>taking</b> the square roots of the expected output. This will avoid gradient issues for saturated sigmoids, at the output. 3) Generalize your model outputs to debug. As an example, imagine you&#39;re using an LSTM to make predictions from time-series data. Maybe in your example, you only care about the latest prediction, so your LSTM outputs a single value and not a sequence ...", "dateLastCrawled": "2022-01-30T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Loss</b> function - zubabin.wixsite.com", "url": "https://zubabin.wixsite.com/amritpal/post/optimizing-loss-function", "isFamilyFriendly": true, "displayUrl": "https://zubabin.wixsite.com/amritpal/post/optimizing-<b>loss</b>-function", "snippet": "The <b>loss</b> tells you how <b>wrong</b> your model\u2019s predictions are. For instance, in multi-label problems, where an example <b>can</b> belong to multiple classes at the same time, the model tries to decide for each class whether the example belongs to that class or not.", "dateLastCrawled": "2021-12-13T11:31:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> fundamentals I: An <b>analogy</b> | Finn Rietz.dev", "url": "http://www.finnrietz.dev/machine%20learning/part-1-analogy/", "isFamilyFriendly": true, "displayUrl": "www.finnrietz.dev/<b>machine</b> <b>learning</b>/part-1-<b>analogy</b>", "snippet": "And this is what the <b>loss</b> function does, so the <b>loss</b> function for a <b>Machine</b> <b>learning</b> algorithm is like the teacher for the real-world dermatologist in-training. In mathematical terms, the <b>loss</b> function could look something like this: \\(L = (y_i - \\hat{y_i})^2\\), where \\(y_i\\) is the actual output value (the one that the teacher has written down) and \\(\\hat{y_i}\\) is the one our <b>learning</b> algorithm produced.", "dateLastCrawled": "2022-01-16T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "However, in <b>machine</b> <b>learning</b> methodology, <b>squared</b> <b>loss</b> will be minimized with respect to ... If one of the supports doesn\u2019t exist, the beam will eventually fall down by moving out of balance. A similar <b>analogy</b> is applied for comparing statistical modeling and <b>machine</b> <b>learning</b> methodologies here. The two-point validation is performed on the statistical modeling methodology on training data using overall model accuracy and individual parameters significance test. Due to the fact that either ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machined Learnings: ML and OR: An <b>analogy</b> with cost-sensitive ...", "url": "http://www.machinedlearnings.com/2010/07/ml-and-or.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2010/07/ml-and-or.html", "snippet": "Nonetheless I&#39;ve been amusing myself by thinking about it, in particular trying to think about it from a <b>machine</b> <b>learning</b> reduction standpoint. The simplest well-understood reduction that I can think of which is analogous to supplying estimates to a linear program is the reduction of cost-sensitive multiclass classification (CSMC) to regression.", "dateLastCrawled": "2021-12-25T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "Optimization methods are applied to minimize the <b>loss</b> function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one <b>loss</b> is L0-1 = 1 (m &lt;= 0); in zero-one <b>loss</b>, value of <b>loss</b> is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this <b>loss</b> is it is not differentiable, non-convex, and also NP-hard. Hence, in order to make optimization feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Stratified <b>Sampling Meets Machine Learning</b>", "url": "http://proceedings.mlr.press/v48/liberty16.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v48/liberty16.pdf", "snippet": "model in this <b>analogy</b> is the vector of probabilities p. A standard objective in regression problems is to mini-mize the <b>squared</b> <b>loss</b>, L(~y;y) = (~y 2y) . In our set- ting, however, the prediction ~y is itself a random vari-able. By taking the expectation over the random bits of the sampling algorithm and by overloading the <b>loss</b> func-tion L(p;q) := P i q 2(1=p i 1), our goal is modi\ufb01ed to minimize E q[E y~(~y y)2] = E q X i q2 i (1=p i 1) = E qL(p;q) : Strati\ufb01ed Sampling Meets <b>Machine</b> ...", "dateLastCrawled": "2021-10-13T10:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bias-Variance Decomposition</b> - mlxtend", "url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/", "isFamilyFriendly": true, "displayUrl": "rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp", "snippet": "We can decompose a <b>loss</b> function such as the <b>squared</b> <b>loss</b> into three terms, a variance, bias, and a noise term (and the same is true for the decomposition of the 0-1 <b>loss</b> later). However, for simplicity, we will ignore the noise term. Before we introduce the <b>bias-variance decomposition</b> of the 0-1 <b>loss</b> for classification, let us start with the decomposition of the <b>squared</b> <b>loss</b> as an easy warm-up exercise to get familiar with the overall concept. The previous section already listed the common ...", "dateLastCrawled": "2022-01-31T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Mean squared error</b> in <b>machine</b> <b>learning</b> | by Aaron Li | MLearning.ai ...", "url": "https://medium.com/mlearning-ai/gradient-descent-4fda4e3fbdc0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/gradient-descent-4fda4e3fbdc0", "snippet": "For the ones <b>learning</b> <b>machine</b> <b>learning</b> in a bottom up approach, I suggest you try to train some models in Google Co-lab and get an idea of how <b>machine</b> <b>learning</b> works.", "dateLastCrawled": "2022-01-30T03:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "<b>Machine</b> <b>Learning</b> A Quantitative Approach Henry H. Liu P PerfMath. ... Bayesian, (4) <b>Analogy</b>, and (5) Unsupervised <b>learning</b>. Pedro Domingos proposed these five ML paradigms, and \u00a71.3 explains briefly what each of these five ML paradigms is about. <b>MACHINE</b> <b>LEARNING</b>: A QUANTITATIVE APPROACH 5 2 <b>Machine</b> <b>Learning</b> Fundamentals Illustrated with Regression 2.1 Try to find a publicly available <b>machine</b> <b>learning</b> dataset and apply an end-to-end procedure similar to the one we used with the fuel economy ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A novel <b>semi-supervised support vector machine with asymmetric</b> squared ...", "url": "https://link.springer.com/article/10.1007/s11634-020-00390-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11634-020-00390-y", "snippet": "In the field of <b>machine</b> <b>learning</b>, loss function is usually one of the key issues in designing <b>learning</b> algorithms since most problems require it to describe the cost of the discrepancy between the prediction and the observation. In fact, the use of the loss function can be traced back to a long time ago. For example, the least-square loss function for regression was already employed by Legendre, Gauss, and Adrain in the early 19th century (Steinwart and Christmann 2008). At present, various ...", "dateLastCrawled": "2021-11-13T10:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(squared loss)  is like +(taking a wrong turn)", "+(squared loss) is similar to +(taking a wrong turn)", "+(squared loss) can be thought of as +(taking a wrong turn)", "+(squared loss) can be compared to +(taking a wrong turn)", "machine learning +(squared loss AND analogy)", "machine learning +(\"squared loss is like\")", "machine learning +(\"squared loss is similar\")", "machine learning +(\"just as squared loss\")", "machine learning +(\"squared loss can be thought of as\")", "machine learning +(\"squared loss can be compared to\")"]}