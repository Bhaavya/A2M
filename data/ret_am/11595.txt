{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> Learning | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-learning-3ff8ba1040cb", "snippet": "4.2 <b>Demographic</b> <b>Parity</b>. <b>Demographic</b> <b>Parity</b>, also called Independence, Statistical <b>Parity</b>, is one of the most well-known criteria for <b>fairness</b>. Formulation: C is independent of A: P\u2080 [C = c] = P\u2081 [C = c] \u2200 c \u2208 {0,1} In our example, this means the acceptance rates of the applicants from the two groups must be equal. In practice, there are ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Evaluating Fairness in <b>Machine</b> Learning - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "A model has <b>Demographic</b> <b>Parity</b> if the predicted positive rates (selection rates) are approximately the same for all groups of the protected attribute. Two common measures are the Statistical <b>Parity</b> Difference and the Disparate Impact Ratio. The Statistical <b>Parity</b> Difference is the difference in the probability of prediction between the two groups. A difference of 0 indicates that the model is <b>perfectly</b> fair relative to the protected attribute (it favors neither the privileged nor the ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "On Fairness and Calibration - NeurIPS", "url": "https://proceedings.neurips.cc/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf", "snippet": "Denition 3. A classier h t is <b>perfectly</b> <b>calibrated</b> if8 p 2 [0;1],P (x ;y ) G t y =1 j h t(x )= p = p. It is commonly accepted amongst practitioners that both classiers h 1 and h 2 should be <b>calibrated</b> with respect to groups G 1 and G 2 to prevent discrimination [ 4,10 ,12 ,16 ]. Intuitively, this prevents", "dateLastCrawled": "2022-01-30T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "FNNC: Achieving Fairness through Neural Networks", "url": "https://www.ijcai.org/Proceedings/2020/0315.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/2020/0315.pdf", "snippet": "focus on fairness constraints <b>like</b> Disparate Im-pact, <b>Demographic</b> <b>Parity</b>, and Equalized Odds, which are non-decomposable and non-convex. Re-searchers de\ufb01ne convex surrogates of the con- straints and then apply convex optimization frame-works to obtain fair classi\ufb01ers. Surrogates serve as an upper bound to the actual constraints, and con-vexifying fairness constraints is challenging. We propose a neural network-based framework, FNNC, to achieve fairness while maintaining high accuracy in ...", "dateLastCrawled": "2021-12-24T06:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The <b>Impossibility Theorem</b> of <b>Machine</b> Fairness \u2013 A Causal Perspective ...", "url": "https://deepai.org/publication/the-impossibility-theorem-of-machine-fairness-a-causal-perspective", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>impossibility-theorem</b>-of-<b>machine</b>-fairness-a-causal...", "snippet": "The <b>Impossibility Theorem</b> of <b>Machine</b> Fairness \u2013 A Causal Perspective. With the increasing pervasive use of <b>machine</b> learning in social and economic settings, there has been an interest in the notion of <b>machine</b> bias in the AI community. Models trained on historic data reflect the biases that exist in society and are propagated to the future ...", "dateLastCrawled": "2022-01-27T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Conditional statistical <b>parity</b> (Corbett-Davies et al., 2017), called also conditional discrimination-aware classification in Kamiran, Zliobaite, and Calders (2013) is a variant of statistical <b>parity</b> obtained by controlling on a set of legitimate attributes. 18 The legitimate attributes (we refer to them as E) among X are correlated with the sensitive attribute A and give some factual information about the label at the same time leading to a legitimate discrimination.", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Sample Complexity of Uniform Convergence for Multicalibration", "url": "https://proceedings.neurips.cc/paper/2020/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Paper.pdf", "snippet": "compares them. There is a variety of fairness notions for group fairness, such as <b>demographic</b> <b>parity</b>, equalized odds, equalized opportunity, and more (see Barocas et al. (2019)). Our main focus would be on multicalibration criteria for group fairness Hebert-Johnson et al. (2018). Multicalibration of a predictor is de\ufb01ned as follows. There is a prespeci\ufb01ed set of subpopulations of interest. The predictor returns a value for each individual (which can be interpreted as a probability). The ...", "dateLastCrawled": "2021-10-17T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Critical Audit of Accuracy and <b>Demographic</b> Biases within Toxicity ...", "url": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi?article=1206&context=senior_theses", "isFamilyFriendly": true, "displayUrl": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi?article=1206&amp;context=senior...", "snippet": "Hutchinson et al. [21] explore 50 years of research centred on <b>machine</b> learn- ing fairness to determine historical lessons and areas of potential further work. The paper goes even further than similar work by calling for more actionable", "dateLastCrawled": "2022-01-05T11:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>On Fairness and Calibration</b> - ResearchGate", "url": "https://www.researchgate.net/publication/319534462_On_Fairness_and_Calibration", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319534462_<b>On_Fairness_and_Calibration</b>", "snippet": "A classi\ufb01er h t is <b>perfectly</b> <b>calibrated</b> if \u2200 p \u2208 [0, 1], P (x,y) \u223c G t y = 1 | h t (x) = p = p. It is commonly accepted amongst practitioners that both classi\ufb01ers h 1", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Attacking <b>discrimination with smarter machine learning</b> | Hacker News", "url": "https://news.ycombinator.com/item?id=13004790", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=13004790", "snippet": "&quot;We proposed a fairness measure that accomplishes two important desiderata. First, it remedies the main conceptual shortcomings of <b>demographic</b> <b>parity</b> as a fairness notion. Second, it is fully aligned with the central goal of supervised <b>machine</b> learning, that is, to build higher accuracy classifiers.&quot; Your dispute may be with the authors.", "dateLastCrawled": "2021-04-23T12:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> Learning | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-learning-3ff8ba1040cb", "snippet": "4.2 <b>Demographic</b> <b>Parity</b>. <b>Demographic</b> <b>Parity</b>, also called Independence, Statistical <b>Parity</b>, is one of the most well-known criteria for <b>fairness</b>. Formulation: C is independent of A: P\u2080 [C = c] = P\u2081 [C = c] \u2200 c \u2208 {0,1} In our example, this means the acceptance rates of the applicants from the two groups must be equal. In practice, there are ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Impossibility Theorem</b> of <b>Machine</b> Fairness \u2013 A Causal Perspective ...", "url": "https://deepai.org/publication/the-impossibility-theorem-of-machine-fairness-a-causal-perspective", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>impossibility-theorem</b>-of-<b>machine</b>-fairness-a-causal...", "snippet": "The causal diagram of a data generation process corresponding to a classifier that satisfies <b>demographic</b> <b>parity</b> is shown below. It can be observed that Y and ^ Y are d-connected through the unblocked represented by the curved line. Similarly, Y and A are d-connected. Thus the assumption that the model is well <b>calibrated</b> and that the sensitive ...", "dateLastCrawled": "2022-01-27T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Evaluating Fairness in <b>Machine</b> Learning - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "A model has <b>Demographic</b> <b>Parity</b> if the predicted positive rates (selection rates) are approximately the same for all groups of the protected attribute. Two common measures are the Statistical <b>Parity</b> Difference and the Disparate Impact Ratio. The Statistical <b>Parity</b> Difference is the difference in the probability of prediction between the two groups. A difference of 0 indicates that the model is <b>perfectly</b> fair relative to the protected attribute (it favors neither the privileged nor the ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "On Fairness and Calibration - NeurIPS", "url": "https://proceedings.neurips.cc/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf", "snippet": "Denition 3. A classier h t is <b>perfectly</b> <b>calibrated</b> if8 p 2 [0;1],P (x ;y ) G t y =1 j h t(x )= p = p. It is commonly accepted amongst practitioners that both classiers h 1 and h 2 should be <b>calibrated</b> with respect to groups G 1 and G 2 to prevent discrimination [ 4,10 ,12 ,16 ]. Intuitively, this prevents", "dateLastCrawled": "2022-01-30T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Conditional statistical <b>parity</b> (Corbett-Davies et al., 2017), called also conditional discrimination-aware classification in Kamiran, Zliobaite, and Calders (2013) is a variant of statistical <b>parity</b> obtained by controlling on a set of legitimate attributes. 18 The legitimate attributes (we refer to them as E) among X are correlated with the sensitive attribute A and give some factual information about the label at the same time leading to a legitimate discrimination.", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Sample Complexity of Uniform Convergence for Multicalibration", "url": "https://proceedings.neurips.cc/paper/2020/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Paper.pdf", "snippet": "compares them. There is a variety of fairness notions for group fairness, such as <b>demographic</b> <b>parity</b>, equalized odds, equalized opportunity, and more (see Barocas et al. (2019)). Our main focus would be on multicalibration criteria for group fairness Hebert-Johnson et al. (2018). Multicalibration of a predictor is de\ufb01ned as follows. There is a prespeci\ufb01ed set of subpopulations of interest. The predictor returns a value for each individual (which can be interpreted as a probability). The ...", "dateLastCrawled": "2021-10-17T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "AIES 2020 - <b>Measuring Fairness in an Unfair World</b> (formatted)", "url": "https://www.jherington.com/docs/Herington_AIES-2020.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.jherington.com/docs/Herington_AIES-2020.pdf", "snippet": "Examples of such measures include \u201c<b>demographic</b> <b>parity</b>\u201d [18], \u201cstatistical <b>parity</b>\u201d [13] and \u201canti-classification\u201d [10]. A failure of these measures is intuitively troubling because it suggests that there is an open causal pathway between A and C \u2013 e.g. race is partially causing the algorithm\u2019s predictions. Since, in ideal circumstances, it would be unfair for race (or other sensitive attributes) to influence our treatment of individuals, algorithms which fail to satisfy ...", "dateLastCrawled": "2021-11-01T11:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Multi-group Agnostic PAC Learnability", "url": "http://proceedings.mlr.press/v139/rothblum21a/rothblum21a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v139/rothblum21a/rothblum21a.pdf", "snippet": "to ensure <b>parity</b> or balance between <b>demographic</b> groups, e.g. <b>similar</b> rates of positive predictions or <b>similar</b> false positive or false negative rates (Hardt et al.,2016;Kleinberg et al.,2016). Other works consider accuracy guarantees, such as calibration (Dawid,1982), for protected groups. Protections at the level of a single group might be too weak (Dwork et al.,2012), and recent works have studied ex-tending these notions to the setting of multiple overlapping groups (Hebert-Johnson et al ...", "dateLastCrawled": "2021-11-26T06:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Critical Audit of Accuracy and <b>Demographic</b> Biases within Toxicity ...", "url": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi?article=1206&context=senior_theses", "isFamilyFriendly": true, "displayUrl": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi?article=1206&amp;context=senior...", "snippet": "Hutchinson et al. [21] explore 50 years of research centred on <b>machine</b> learn-ing fairness to determine historical lessons and areas of potential further work. The paper goes even further than <b>similar</b> work by calling for more actionable work on the causes of unfairness as opposed to directionless conjecture on the nature of fairness. They also ...", "dateLastCrawled": "2022-01-05T11:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Potential impact of climate change on the geographical distribution of ...", "url": "https://parasitesandvectors.biomedcentral.com/articles/10.1186/s13071-019-3744-9", "isFamilyFriendly": true, "displayUrl": "https://parasitesandvectors.biomedcentral.com/articles/10.1186/s13071-019-3744-9", "snippet": "Maxent is a <b>machine</b>-learning method that assesses the probability of a species distribution, by estimating a probability of ... (MESS), which represents how <b>similar</b> a point is to a reference set of points with respect to a set of predictor variables. MESS provides an index of environmental similarity between each pixel and the median of the most dissimilar variable in M [62, 63]. We also performed mobility-oriented <b>parity</b> (MOP) analysis, which identifies areas of strict extrapolation and ...", "dateLastCrawled": "2022-01-31T02:03:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The <b>Impossibility Theorem</b> of <b>Machine</b> Fairness \u2013 A Causal Perspective ...", "url": "https://deepai.org/publication/the-impossibility-theorem-of-machine-fairness-a-causal-perspective", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>impossibility-theorem</b>-of-<b>machine</b>-fairness-a-causal...", "snippet": "The causal diagram of a data generation process corresponding to a classifier that satisfies <b>demographic</b> <b>parity</b> is shown below. It <b>can</b> be observed that Y and ^ Y are d-connected through the unblocked represented by the curved line. Similarly, Y and A are d-connected. Thus the assumption that the model is well <b>calibrated</b> and that the sensitive ...", "dateLastCrawled": "2022-01-27T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Measure and Mismeasure of Fairness</b>: A Critical Review of Fair ...", "url": "https://deepai.org/publication/the-measure-and-mismeasure-of-fairness-a-critical-review-of-fair-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>measure-and-mismeasure-of-fairness</b>-a-critical...", "snippet": "In our running pretrial example, <b>demographic</b> <b>parity</b> means that detention rates are equal across race groups; and <b>parity</b> of false positive rates means that among defendants who would not have gone on to commit a violent crime if released, detention rates are equal across race groups. <b>Demographic</b> <b>parity</b> is not strictly speaking a measure of \u201cerror\u201d, but we nonetheless include it under classification <b>parity</b> since it <b>can</b> be computed from a confusion matrix. We note that <b>demographic</b> <b>parity</b> is ...", "dateLastCrawled": "2021-12-16T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Fairness as Equal Concession: Critical Remarks on Fair AI | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11948-021-00348-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11948-021-00348-z", "snippet": "Fazelpour and Lipton note industry AI products hastily certifying themselves as fair on account of controlling for <b>demographic</b> <b>parity</b>, placing some blame on fair AI literature making it possible: \u201cIn many papers, these fairness-inspired <b>parity</b> metrics are described as definitions of fairness and the resulting algorithms that satisfy the parities are claimed axiomatically to be fair\u201d (Fazelpour and Lipton, 2020 9).", "dateLastCrawled": "2022-01-31T20:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AIES 2020 - <b>Measuring Fairness in an Unfair World</b> (formatted)", "url": "https://www.jherington.com/docs/Herington_AIES-2020.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.jherington.com/docs/Herington_AIES-2020.pdf", "snippet": "Examples of such measures include \u201c<b>demographic</b> <b>parity</b>\u201d [18], \u201cstatistical <b>parity</b>\u201d [13] and \u201canti-classification\u201d [10]. A failure of these measures is intuitively troubling because it suggests that there is an open causal pathway between A and C \u2013 e.g. race is partially causing the algorithm\u2019s predictions. Since, in ideal circumstances, it would be unfair for race (or other sensitive attributes) to influence our treatment of individuals, algorithms which fail to satisfy ...", "dateLastCrawled": "2021-11-01T11:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Avoiding Disparity Amplification under Different ... - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/1808.08619/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1808.08619", "snippet": "We mathematically compare four competing definitions of group-level nondiscrimination: <b>demographic</b> <b>parity</b>, equalized odds, predictive <b>parity</b>, and calibration. Using the theoretical framework of Friedler et al., we study the properties of each definition under various worldviews, which are assumptions about how, if at all, the observed data is biased. We argue that different worldviews call for different definitions of fairness, and we specify the worldviews that, when combined with the ...", "dateLastCrawled": "2021-10-16T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "ECEM 2017 - ncbi.nlm.<b>nih.gov</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7141056/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7141056", "snippet": "One of our most frequent movements, saccades are <b>thought</b> to be subject to an internal timer that may also be susceptible to entrainment. To investigate the influence of musical tempi on eye movements we developed a continuous visual search task that minimized extraneous influences on saccadic timing by having participants look clockwise around an ellipse of small circles, in search of a change in the circle\u2019s letter or colour. Target presentation was either gaze contingent, tap-contingent ...", "dateLastCrawled": "2022-01-27T11:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>On Fairness and Calibration</b> - ResearchGate", "url": "https://www.researchgate.net/publication/319534462_On_Fairness_and_Calibration", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319534462_<b>On_Fairness_and_Calibration</b>", "snippet": "A classi\ufb01er h t is <b>perfectly</b> <b>calibrated</b> if \u2200 p \u2208 [0, 1], P (x,y) \u223c G t y = 1 | h t (x) = p = p. It is commonly accepted amongst practitioners that both classi\ufb01ers h 1", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Max Min</b> - Bo Waggoner", "url": "https://www.bowaggoner.com/blahg/2019/02-23-fairness-in-ml/index.html", "isFamilyFriendly": true, "displayUrl": "https://www.bowaggoner.com/blahg/2019/02-23-fairness-in-ml/index.html", "snippet": "Fairness in <b>Machine</b> Learning: A Biased Primer. Posted: 2019-02-23. This post will summarize some of the motivation for and definitions of fairness in ML. This is a huge and evolving topic -- here we&#39;ll focus a bit on discrimination in algorithmic decisionmaking. We&#39;ll spend relatively less time on motivations and real-world examples, more at the highest and lowest-level meanings of fair <b>machine</b> learning. Note - throughout the post, I&#39;ll try to use a variety of fairness examples such as ...", "dateLastCrawled": "2021-12-29T18:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Estimating a Dynamic Model of Sex Selection</b> in China | Demography ...", "url": "https://read.dukeupress.edu/demography/article/48/2/783/169797/Estimating-a-Dynamic-Model-of-Sex-Selection-in", "isFamilyFriendly": true, "displayUrl": "https://read.dukeupress.edu/demography/article/48/2/783/169797/Estimating-a-Dynamic...", "snippet": "The model <b>can</b> also <b>be thought</b> to represent a forecast for fertility patterns if son preference were to decline over time or because of secular changes in China, such as the effective implementation of a wider old-age support program currently being discussed (Diamond et al. 2005), or a diminution of son preference as witnessed in Korea (Chung and Das Gupta 2007). The motivation for a direct subsidy of sons is clear because rural areas of China are unlikely to rapidly modify, in an acceptably ...", "dateLastCrawled": "2022-01-30T20:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Equality of Opportunity in Supervised Learning - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/1610.02413/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1610.02413", "snippet": "We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups ...", "dateLastCrawled": "2021-09-12T12:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Evaluating Fairness in <b>Machine</b> Learning - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Machine</b> learning models <b>can</b> also be a source of disparate impact in their implementation, through unconscious human biases that affect the fair interpretation or use of the model&#39;s results. This reference does not cover measurement of fairness at implementation. However, if you are interested in fair implementation, we recommend looking at Google&#39;s Fairness Indicators. Harms. In evaluating the potential impact of an ML model, it <b>can</b> be helpful to first clarify what specific harm(s) <b>can</b> be ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Measure and Mismeasure of Fairness</b>: A Critical Review of Fair ...", "url": "https://deepai.org/publication/the-measure-and-mismeasure-of-fairness-a-critical-review-of-fair-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>measure-and-mismeasure-of-fairness</b>-a-critical...", "snippet": "In our running pretrial example, <b>demographic</b> <b>parity</b> means that detention rates are equal across race groups; and <b>parity</b> of false positive rates means that among defendants who would not have gone on to commit a violent crime if released, detention rates are equal across race groups. <b>Demographic</b> <b>parity</b> is not strictly speaking a measure of \u201cerror\u201d, but we nonetheless include it under classification <b>parity</b> since it <b>can</b> be computed from a confusion matrix. We note that <b>demographic</b> <b>parity</b> is ...", "dateLastCrawled": "2021-12-16T12:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Critical Audit of Accuracy and <b>Demographic</b> Biases within Toxicity ...", "url": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi?article=1206&context=senior_theses", "isFamilyFriendly": true, "displayUrl": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi?article=1206&amp;context=senior...", "snippet": "From this expression, we see that an instrument <b>can</b> satisfy predictive <b>parity</b>, but if the prevalence di ers between groups, the instrument cannot achieve equal FPRs and FNRs across those groups. As the recidivism rate among black defendants in the data is 51%, <b>compared</b> with 39% for white", "dateLastCrawled": "2022-01-05T11:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ECEM 2017 - ncbi.nlm.<b>nih.gov</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7141056/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7141056", "snippet": "Our model asserts that each decision to move the eyes is an evaluation of the relative benefit expected from moving the eyes to a new location <b>compared</b> with that expected by continuing to fixate the current target. The eyes move when the evidence that favors moving to a new location sufficiently outweighs that favoring staying at the present location. This single decision process <b>can</b> explain both when and where people look in scenes.", "dateLastCrawled": "2022-01-27T11:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Fairness in <b>machine</b> learning <b>can</b> be categorized according to two dimensions, namely, the task and the type of learning. For the first dimension, there are two tasks in fairness-aware <b>machine</b> learning: discrimination discovery (or assessment) and discrimination removal (or prevention). Discrimination discovery task focuses on assessing and measuring bias in datasets or in predictions made by the MLDM. Discrimination removal focuses on preventing discrimination by manipulating datasets (pre ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>On Fairness and Calibration</b> - ResearchGate", "url": "https://www.researchgate.net/publication/319534462_On_Fairness_and_Calibration", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319534462_<b>On_Fairness_and_Calibration</b>", "snippet": "A classi\ufb01er h t is <b>perfectly</b> <b>calibrated</b> if \u2200 p \u2208 [0, 1], P (x,y) \u223c G t y = 1 | h t (x) = p = p. It is commonly accepted amongst practitioners that both classi\ufb01ers h 1", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Snapshot of the Frontiers <b>of Fairness in Machine Learning</b> | May 2020 ...", "url": "https://cacm.acm.org/magazines/2020/5/244336-a-snapshot-of-the-frontiers-of-fairness-in-machine-learning/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2020/5/244336-a-snapshot-of-the-frontiers-of-fairness...", "snippet": "(See Dwork 19 for a litany of ways in which statistical <b>parity</b> and similar notions <b>can</b> fail to provide meaningful guarantees, and Kearns 40 for examples of how some of these weaknesses carry over to definitions that equalize false positive and negative rates.) Different statistical measures of fairness <b>can</b> be at odds with one another. For example, Chouldechova 15 and Kleinberg 46 prove a fundamental impossibility result: except in trivial settings, it is impossible to simultaneously equalize ...", "dateLastCrawled": "2022-01-29T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Mammographic Density and Breast Cancer Risk: Evaluation of a Novel ...", "url": "https://cebp.aacrjournals.org/content/18/6/1754", "isFamilyFriendly": true, "displayUrl": "https://cebp.aacrjournals.org/content/18/6/1754", "snippet": "Information on <b>demographic</b> and ethnic characteristics, and on risk factors for breast cancer, was obtained by telephone interview including age, body weight before diagnosis, height, <b>parity</b>, alcohol use, hormone use, both past and present, menopausal status, and previous history of mammograms and biopsies. We mailed letters to 919 cases, and 534 (58%) gave consent, and to 3,297 controls, of whom 1,375 (42%) gave consent.", "dateLastCrawled": "2021-12-29T00:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Avoiding Disparity Amplification under Different ... - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/1808.08619/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1808.08619", "snippet": "We mathematically compare four competing definitions of group-level nondiscrimination: <b>demographic</b> <b>parity</b>, equalized odds, predictive <b>parity</b>, and calibration. Using the theoretical framework of Friedler et al., we study the properties of each definition under various worldviews, which are assumptions about how, if at all, the observed data is biased. We argue that different worldviews call for different definitions of fairness, and we specify the worldviews that, when combined with the ...", "dateLastCrawled": "2021-10-16T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Can</b> nonexperts really emulate statistical learning methods? A comment ...", "url": "https://www.cambridge.org/core/journals/political-analysis/article/can-nonexperts-really-emulate-statistical-learning-methods-a-comment-on-the-accuracy-fairness-and-limits-of-predicting-recidivism/8E6CF2C5F1BB5704C01027A0AE35E01A", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/political-analysis/article/<b>can</b>-nonexperts...", "snippet": "For a model or method to be <b>perfectly</b> <b>calibrated</b>, its predicted probabilities should equal the true probabilities (Hern\u00e1ndez-Orallo, Flach, and Ferri Reference Hern\u00e1ndez-Orallo, Flach and Ferri 2012). With empirical data, this <b>can</b> be evaluated by aggregating the data into bins. The top two panels in Figures", "dateLastCrawled": "2021-10-14T03:14:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Pandas <b>Machine</b> <b>Learning</b> Example", "url": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "snippet": "Regardless of your dataset, <b>demographic</b> <b>parity</b> is a <b>machine</b> <b>learning</b> algorithms. Data Munging It helps us to missing data of wedge form with another. Python with datetime module, i should equal to bring new example <b>machine</b>. Quite possibly the state important part clean the <b>machine</b> <b>learning</b> process is understanding the data you are working with and advantage it relates to reflect task you front to solve. Viewing the corresponding number of dropping down arrow illustrates that are not only all ...", "dateLastCrawled": "2022-01-24T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An example of prediction which complies with <b>Demographic</b> <b>Parity</b> and ...", "url": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-demographic-parity-and-equalizes-group-wise-risks-in-the-context", "isFamilyFriendly": true, "displayUrl": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-<b>demographic</b>...", "snippet": "However, <b>Demographic</b> <b>Parity</b> and EGWR only define fairness on the group level and inspecting the individual level reveals a critical flow of this prediction rule. We have constrained our predictors to those that do not produce Disparate Treatment by prohibiting them from having the sensitive variable as direct input. Nevertheless, enforcing group level fairness constraints (such as DP and EGWR) forces the prediction rule to guess the sensitive attribute corresponding to a given feature vector", "dateLastCrawled": "2022-02-05T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Heidari et al. have written a paper comparing the three criteria \u2013 <b>demographic</b> <b>parity</b>, equality of opportunity, and predictive <b>parity</b> \u2013 to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the <b>analogy</b> is fascinating, it too assumes that we may take what is in the data at face value. In their likening predictive <b>parity</b> to luck egalitarianism, they have to go to especially great lengths, in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Mitigating Unwanted Biases with Adversarial <b>Learning</b> - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/1801.07593/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1801.07593", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concerning <b>demographic</b> groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or income ...", "dateLastCrawled": "2021-10-04T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mitigating Unwanted Biases with Adversarial <b>Learning</b>", "url": "http://www.m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf", "isFamilyFriendly": true, "displayUrl": "www.m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concern-ing <b>demographic</b> groups are in the training data, well-trained models will re\ufb02ect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an ad-versary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or in ...", "dateLastCrawled": "2022-01-23T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "This includes measures such as <b>Demographic</b> <b>Parity</b> / Statistical <b>Parity</b> (Dwork et al., 2012), Equalized Odds Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are trying to test for equivalence. In another survey of fairness definitions, Verma &amp; Rubin (2018) listed 20 definitions of fairness, 13 belonging to ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The <b>measure and mismeasure of fairness: a critical review</b> of fair ...", "url": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness", "snippet": "In case you\u2019re wondering where on earth I\u2019m going with this\u2026 it\u2019s a very stretched <b>analogy</b> I\u2019ve been playing with in my mind. One premise of many models of fairness in <b>machine</b> <b>learning</b> is that you can measure (\u2018prove\u2019) fairness of a <b>machine</b> <b>learning</b> model from within the system \u2013 i.e. from properties of the model itself and perhaps the data it is trained on. Beyond the questions of whether any one model of fairness is better or worse than another, I\u2019m coming to the ...", "dateLastCrawled": "2022-01-30T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Fighting Money Laundering with Statistics and <b>Machine</b> <b>Learning</b>: An ...", "url": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-machine-learning-an-introduction-and-review", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-<b>machine</b>...", "snippet": "Statistics and <b>machine</b> <b>learning</b> have long promised efficient and robust techniques for AML. So far, though, these remain to manifest [Grint2001]. One reason is that the academic literature is small and fragmented [Leite2019, Ngai2011]. With this paper, we aim to do three things. First, we propose a unified terminology to homogenize and associate research methodologies. Second, we review selected, exemplary methods. Third, we present recent <b>machine</b> <b>learning</b> concepts that have the potential to ...", "dateLastCrawled": "2022-01-28T21:12:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(demographic parity)  is like +(a machine that is perfectly calibrated)", "+(demographic parity) is similar to +(a machine that is perfectly calibrated)", "+(demographic parity) can be thought of as +(a machine that is perfectly calibrated)", "+(demographic parity) can be compared to +(a machine that is perfectly calibrated)", "machine learning +(demographic parity AND analogy)", "machine learning +(\"demographic parity is like\")", "machine learning +(\"demographic parity is similar\")", "machine learning +(\"just as demographic parity\")", "machine learning +(\"demographic parity can be thought of as\")", "machine learning +(\"demographic parity can be compared to\")"]}