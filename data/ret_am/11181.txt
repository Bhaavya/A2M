{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Hebb Nets, Perceptrons and Adaline Nets Based on Fausette\u2019s ...", "url": "http://www.cs.uccs.edu/~jkalita/work/cs587/2014/03SimpleNets.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.uccs.edu/~jkalita/work/cs587/2014/03SimpleNets.pdf", "snippet": "\u2013 If n = 3 (three input units), then <b>the decision</b> <b>boundary</b> is a <b>two dimensional</b> <b>plane</b> in a three dimensional space \u2013 In general, a <b>decision</b> <b>boundary</b> is a n-1 dimensional hyper-<b>plane</b> in an n dimensional space, which partition the space into two <b>decision</b> regions \u2013 This simple network thus can classify a given pattern into one of the two classes, provided one of these two classes is entirely in one <b>decision</b> region (one side of <b>the decision</b> <b>boundary</b>) and the other class is in another ...", "dateLastCrawled": "2022-02-02T20:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Logistic Regression</b> and <b>Decision</b> <b>Boundary</b> | by Anuradha Wickramarachchi ...", "url": "https://towardsdatascience.com/logistic-regression-and-decision-boundary-eab6e00c1e8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>logistic-regression</b>-and-<b>decision</b>-<b>boundary</b>-eab6e00c1e8", "snippet": "The fundamental application of <b>logistic regression</b> is to determine a <b>decision</b> <b>boundary</b> for a binary <b>classification</b> problem. Although the baseline is to identify a binary <b>decision</b> <b>boundary</b>, the approach can be very well applied for scenarios with multiple <b>classification</b> classes or multi-class <b>classification</b>. What is <b>the decision</b> <b>boundary</b>? <b>Decision</b> Boundaries. In the above diagram, the dashed line can be identified a s <b>the decision</b> <b>boundary</b> since we will observe instances of a different class ...", "dateLastCrawled": "2022-02-02T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How To Plot A <b>Decision</b> <b>Boundary</b> For Machine Learning Algorithms in ...", "url": "https://hackernoon.com/how-to-plot-a-decision-boundary-for-machine-learning-algorithms-in-python-3o1n3w07", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/how-to-plot-a-<b>decision</b>-<b>boundary</b>-for-machine-learning-algorithms...", "snippet": "This is called a <b>decision</b> surface or <b>decision</b> <b>boundary</b>, and it provides a diagnostic tool for understanding a model on a predictive <b>classification</b> modeling task. Although the notion of a \u201csurface\u201d suggests a <b>two-dimensional</b> feature space, the method can be used with feature spaces with more than two dimensions, where a surface is created for each pair of input features.", "dateLastCrawled": "2022-01-28T06:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Plotting the decision boundary of a logistic regression</b> model", "url": "https://scipython.com/blog/plotting-the-decision-boundary-of-a-logistic-regression-model/", "isFamilyFriendly": true, "displayUrl": "https://scipython.com/blog/<b>plotting-the-decision-boundary-of-a-logistic-regression</b>-model", "snippet": "<b>Plotting the decision boundary of a logistic regression</b> model. In the notation of this previous post, a logistic regression binary <b>classification</b> model takes an input feature vector, x, and returns a probability, y ^, that x belongs to a particular class: y ^ = P ( y = 1 | x). The model is trained on a set of provided example feature vectors, x ...", "dateLastCrawled": "2022-02-02T21:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Assignment 3 (Sol.)", "url": "https://nptel.ac.in/content/storage2/courses/106106139/Assignments/Solution3.pdf", "isFamilyFriendly": true, "displayUrl": "https://nptel.ac.in/content/storage2/courses/106106139/Assignments/Solution3.pdf", "snippet": "<b>the decision</b> <b>boundary</b> will be perpendicular bisector of the line joining the means. Sol. (b) &amp; (c) The rst statement is not true because unequal priors can cause <b>the decision</b> <b>boundary</b> to shift away form the center of the line joining the two means. 8. For a two class classi cation problem, which among the following are true?", "dateLastCrawled": "2022-02-02T02:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Decision Boundary</b> Visualization(A-Z) | by Navoneel Chakrabarty ...", "url": "https://towardsdatascience.com/decision-boundary-visualization-a-z-6a63ae9cca7d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>decision-boundary</b>-visualization-a-z-6a63ae9cca7d", "snippet": "theta_1, theta_2, theta_3, \u2026., theta_n are the parameters of Logistic Regression and x_1, x_2, \u2026, x_n are the features. So, h(z) is a Sigmoid Function whose range is from 0 to 1 (0 and 1 inclusive). For plotting <b>Decision Boundary</b>, h(z) is taken equal to the <b>threshold</b> value used in the Logistic Regression, which is conventionally 0.5.", "dateLastCrawled": "2022-01-31T03:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "CSE 455/555 Spring <b>2011 Homework 1: Bayesian Decision Theory Jason</b> J. Corso", "url": "https://cse.buffalo.edu/~jcorso/t/2011S_555/files/homework1-solutions.pdf", "isFamilyFriendly": true, "displayUrl": "https://cse.buffalo.edu/~jcorso/t/2011S_555/files/homework1-solutions.pdf", "snippet": "When all the covariance matrices are the same, <b>the decision</b> <b>boundary</b> will be a straight line in <b>two dimensional</b> case, and <b>plane</b> or hyper-<b>plane</b> in three or higher dimensional space. In particular if the covariance matrix are in a special diagonal form \u03a3 = \u03c32 \u00b7 I, the direction of <b>the decision</b> surface will be perpendicular to", "dateLastCrawled": "2022-01-29T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "python - Plotting a <b>decision boundary</b> separating 2 classes using ...", "url": "https://stackoverflow.com/questions/22294241/plotting-a-decision-boundary-separating-2-classes-using-matplotlibs-pyplot", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/22294241", "snippet": "<b>The decision boundary</b> is given by g above. Then to plot <b>the decision</b> hyper-<b>plane</b> (line in 2D), you need to evaluate g for a 2D mesh, then get the contour which will give a separating line. You can also assume to have equal co-variance matrices for both distributions, which will give a linear <b>decision boundary</b>.", "dateLastCrawled": "2022-01-29T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "COMS 4721: Machine Learning for Data Science 4ptLecture 8, 2/14/2017", "url": "http://www.columbia.edu/~jwp2128/Teaching/W4721/Spring2017/slides/lecture_2-14-17.pdf", "isFamilyFriendly": true, "displayUrl": "<b>www.columbia.edu</b>/~jwp2128/Teaching/W4721/Spring2017/slides/lecture_2-14-17.pdf", "snippet": "In the 3-dimensional case, the grid <b>plane</b> separates R 1 from R 2. w i = 1 &quot; 2 \u00b5 i (52) and w i0 =! 1 2&quot; 2 \u00b5 t i \u00b5 i + ln P (! i). (53) We call w i0 the <b>threshold</b> or bias in the ith direction. <b>threshold</b> bias A classi\u00deer that uses linear discriminant functions is called a linear machine . This linear machine", "dateLastCrawled": "2021-12-23T01:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "An Idiot\u2019s guide to Support vector machines (SVMs)", "url": "http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf", "isFamilyFriendly": true, "displayUrl": "<b>web.mit.edu</b>/6.034/wwwbob/<b>svm</b>-notes-long-08.pdf", "snippet": "The <b>plane</b> H 0 is the median in between, where w\u2022x i +b =0 H 1 H 2 H 0 Moving a support vector moves <b>the decision</b> <b>boundary</b> Moving the other vectors has no effect The optimization algorithm to generate the weights proceeds in such a way that only the support vectors determine the weights and thus the <b>boundary</b>", "dateLastCrawled": "2022-02-02T22:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Logistic Regression</b> and <b>Decision</b> <b>Boundary</b> | by Anuradha Wickramarachchi ...", "url": "https://towardsdatascience.com/logistic-regression-and-decision-boundary-eab6e00c1e8", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>logistic-regression</b>-and-<b>decision</b>-<b>boundary</b>-eab6e00c1e8", "snippet": "The fundamental application of <b>logistic regression</b> is to determine a <b>decision</b> <b>boundary</b> for a binary <b>classification</b> problem. Although the baseline is to identify a binary <b>decision</b> <b>boundary</b>, the approach can be very well applied for scenarios with multiple <b>classification</b> classes or multi-class <b>classification</b>. What is the <b>decision</b> <b>boundary</b>? <b>Decision</b> Boundaries. In the above diagram, the dashed line can be identified a s the <b>decision</b> <b>boundary</b> since we will observe instances of a different class ...", "dateLastCrawled": "2022-02-02T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Introduction to Neural Networks Computing", "url": "https://www.csee.umbc.edu/~ypeng/NNCourse/NN-Ch2.PDF", "isFamilyFriendly": true, "displayUrl": "https://www.csee.umbc.edu/~ypeng/NNCourse/NN-Ch2.PDF", "snippet": "\u2013 If n = 3 (three input units), then the <b>decision</b> <b>boundary</b> is a <b>two dimensional</b> <b>plane</b> in a three dimensional space \u2013 In general, a <b>decision</b> <b>boundary</b> is a n-1 dimensional hyper -<b>plane</b> in an n dimensional space, which partition the space into two <b>decision</b> regions \u2013 This simple network thus can classify a given pattern", "dateLastCrawled": "2021-09-11T12:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Two-dimensional Determination of the Decision Boundary</b> for a ...", "url": "https://www.researchgate.net/publication/333004508_Two-dimensional_Determination_of_the_Decision_Boundary_for_a_Radar_Detection_Method_in_the_Moment_Space", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333004508_<b>Two-dimensional</b>_Determination_of...", "snippet": "<b>decision</b> <b>threshold</b> directly, ... the general <b>boundary</b> shape for the <b>two-dimensional</b> case. From the calculus theory, it is known that the general second order equation fo r two independent ...", "dateLastCrawled": "2021-09-30T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Frontiers | MVPA-Light: <b>A Classification and Regression Toolbox for</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fnins.2020.00289/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fnins.2020.00289", "snippet": "\u2022 <b>Decision</b> <b>boundary</b>. Classifiers partition feature space into separate regions. Each region is assigned to a specific class. Classifiers make predictions for a test sample by looking up into which region it falls. The <b>boundary</b> between regions is known as <b>decision</b> <b>boundary</b>. For linear classifiers, the <b>decision</b> <b>boundary</b> is also known as a hyperplane. \u2022 <b>Decision</b> value. Classifiers such as LDA and SVM produce <b>decision</b> values which can be thresholded to produce class labels. For linear ...", "dateLastCrawled": "2022-01-26T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Bayesian <b>Decision</b> Theory - byclb.com", "url": "http://byclb.com/TR/Tutorials/neural_networks/ch4_1.htm", "isFamilyFriendly": true, "displayUrl": "byclb.com/TR/Tutorials/neural_networks/ch4_1.htm", "snippet": "The <b>decision</b> <b>boundary</b> is a line orthogonal to the line joining the two means. If P(w i) \u00b9 P (w j) the point x 0 shifts away from the more likely mean. Note, however, that if the variance is small relative to the squared distance , then the position of the <b>decision</b> <b>boundary</b> is relatively insensitive to the exact values of the prior probabilities. In other words, there are 80% apples entering the store. If you observe some feature vector of color and weight that is just a little closer to the ...", "dateLastCrawled": "2022-02-02T14:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Hebb Nets, Perceptrons and Adaline Nets Based on Fausette\u2019s ...", "url": "http://www.cs.uccs.edu/~jkalita/work/cs587/2014/03SimpleNets.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.uccs.edu/~jkalita/work/cs587/2014/03SimpleNets.pdf", "snippet": "\u2022 <b>Decision</b> region/<b>boundary</b> n = 2, b != 0, is a line, called <b>decision</b> <b>boundary</b>, which partitions the <b>plane</b> into two <b>decision</b> regions If a point/pattern is in the positive region, then , and the output is one (belongs to class one) Otherwise, w w , output \u20131 (belongs to class two) n = 2, b = 0 would result a <b>similar</b> partition 2", "dateLastCrawled": "2022-02-02T20:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "python - Plotting a <b>decision boundary</b> separating 2 classes using ...", "url": "https://stackoverflow.com/questions/22294241/plotting-a-decision-boundary-separating-2-classes-using-matplotlibs-pyplot", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/22294241", "snippet": "The <b>decision boundary</b> is given by g above. Then to plot the <b>decision</b> hyper-<b>plane</b> (line in 2D), you need to evaluate g for a 2D mesh, then get the contour which will give a separating line. You can also assume to have equal co-variance matrices for both distributions, which will give a linear <b>decision boundary</b>.", "dateLastCrawled": "2022-01-29T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Application of Machine <b>Learning Techniques in Mineral Classification</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0920410520312328", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0920410520312328", "snippet": "In the <b>two dimensional</b> space, the <b>Decision</b> <b>boundary</b> of an SVM is a curve dividing a <b>plane</b> into two parts. The process is to choose the most confident <b>decision</b> <b>boundary</b> among multiple hyperplanes with the maximum margin (defined as the maximum width the <b>decision</b> <b>boundary</b> area can be increased to before hitting a data point) is proposed. Kernels are needed regarding nonlinear separating problems, which transforms the input data points to higher dimension space, then converting non-separable ...", "dateLastCrawled": "2021-12-30T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Logistic Regression Introduction - Coursera", "url": "https://ja.coursera.org/lecture/introduction-to-machine-learning-supervised-learning/logistic-regression-introduction-vpnSn", "isFamilyFriendly": true, "displayUrl": "https://ja.coursera.org/lecture/introduction-to-machine-learning-supervised-learning/...", "snippet": "The <b>decision</b> <b>boundary</b> is a point where it meets the probability calls 0.5. So the occasion looks like this, and you can get the value out of it. If we have two features, the data relying on the <b>two-dimensional</b> space and then the <b>decision</b> <b>boundary</b> becomes a line. So we can find the line occasion here which will draw this line if it&#39;s a multi variant, have a multidimensional more than three. The <b>decision</b> <b>boundary</b> will be a hyper <b>plane</b>. Okay, let&#39;s talk about what if we have a multiple ...", "dateLastCrawled": "2022-02-02T22:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Perceptron</b>: Explanation, Implementation and a Visual Example | by ...", "url": "https://towardsdatascience.com/perceptron-explanation-implementation-and-a-visual-example-3c8e76b4e2d1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>perceptron</b>-explanation-implementation-and-a-visual...", "snippet": "As you see above, the <b>decision</b> <b>boundary</b> of a <b>perceptron</b> with 2 inputs is a line. If there were 3 inputs, the <b>decision</b> <b>boundary</b> would be a 2D <b>plane</b>. In general, if we have n inputs the <b>decision</b> <b>boundary</b> will be a n-1 dimensional object called a hyperplane that separates our n-dimensional feature space into 2 parts: one in which the points are classified as positive, and one in which the points are classified as negative(by convention, we will consider points that are exactly on the <b>decision</b> ...", "dateLastCrawled": "2022-01-29T17:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Hebb Nets, Perceptrons and Adaline Nets Based on Fausette\u2019s ...", "url": "http://www.cs.uccs.edu/~jkalita/work/cs587/2014/03SimpleNets.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.uccs.edu/~jkalita/work/cs587/2014/03SimpleNets.pdf", "snippet": "\u2013 If n = 3 (three input units), then the <b>decision</b> <b>boundary</b> is a <b>two dimensional</b> <b>plane</b> in a three dimensional space \u2013 In general, a <b>decision</b> <b>boundary</b> is a n-1 dimensional hyper-<b>plane</b> in an n dimensional space, which partition the space into two <b>decision</b> regions \u2013 This simple network thus <b>can</b> classify a given pattern into one of the two classes, provided one of these two classes is entirely in one <b>decision</b> region (one side of the <b>decision</b> <b>boundary</b>) and the other class is in another ...", "dateLastCrawled": "2022-02-02T20:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "10-701 Midterm Exam Solutions, Spring 2007", "url": "https://www.cs.cmu.edu/~aarti/Class/10701/exams/midterm2007s-solution.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~aarti/Class/10701/exams/midterm2007s-solution.pdf", "snippet": "COMMON MISTAKE 2: Some students confused the e\ufb00ect of C and <b>thought</b> that a large C meant that the algorithm would be more tolerant of misclassi\ufb01cations. 2. [4 points] For C \u22480, indicate in the \ufb01gure below, where you would expect the <b>decision</b> <b>boundary</b> to be? Justify your answer. \u22c6 SOLUTION: The classi\ufb01er <b>can</b> maximize the margin between most of the points, while misclassifying a few points, because the penalty is so low. See below for the <b>boundary</b> learned by libSVM with C = 0.00005 ...", "dateLastCrawled": "2022-02-01T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Support vector machines: The linearly separable case</b>", "url": "https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html", "isFamilyFriendly": true, "displayUrl": "https://nlp.stanford.edu/IR-book/html/htmledition/<b>support-vector-machines-the-linearly</b>...", "snippet": "A value of indicates one class, and a value of the other class.. We are confident in the <b>classification</b> of a point if it is far away from the <b>decision</b> <b>boundary</b>. For a given data set and <b>decision</b> hyperplane, we define the functional margin of the example with respect to a hyperplane as the quantity .The functional margin of a data set with respect to a <b>decision</b> surface is then twice the functional margin of any of the points in the data set with minimal functional margin (the factor of 2 ...", "dateLastCrawled": "2022-02-02T10:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Chapter 2 (part 3) Bayesian <b>Decision</b> Theory", "url": "https://ahmadzadeh.iut.ac.ir/sites/ahmadzadeh.iut.ac.ir/files//files_course/pr-ch2-part3.pdf", "isFamilyFriendly": true, "displayUrl": "https://ahmadzadeh.iut.ac.ir/sites/ahmadzadeh.iut.ac.ir/files//files_course/pr-ch2...", "snippet": "\u2022 We saw that the minimum error-rate <b>classification</b> <b>can</b> be achieved by the discriminant function. ... whose Bayes <b>decision boundary</b> is that hyperquadric. These Variances are indicated by the contours of constant probability density. 21. 22 FIGURE 2.15. Arbitrary three-dimensional Gaussian distributions yield Bayes <b>decision</b> boundaries that are <b>two-dimensional</b> hyperquadrics. There are even degenerate cases in which the <b>decision boundary</b> is a line. 23 FIGURE 2.16. The <b>decision</b> regions for ...", "dateLastCrawled": "2022-02-02T17:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Coursera\u2019s machine learning course week three (logistic ... - Linbug", "url": "https://linbug.github.io/machine%20learning/2015/07/27/Logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://linbug.github.io/machine learning/2015/07/27/Logistic-regression", "snippet": "You <b>can</b> see that on our <b>two-dimensional</b> scatter plot, the <b>decision</b> <b>boundary</b> is represented as a line. Any points that fall to the bottom left of the <b>decision</b> <b>boundary</b> are students that will not be admitted to university, whereas those falling to the top right will be admitted. If there was only one input feature, the <b>decision</b> <b>boundary</b> would have been a point, and if there had been three input features it would have been a <b>plane</b> in the three-dimensional space, and so on. We had already been ...", "dateLastCrawled": "2022-01-31T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "7 Supervised Learning : <b>Classification</b> \u2013 Machine Learning \u2013 w3sdev", "url": "https://w3sdev.com/7-supervised-learning-classification-machine-learning.html", "isFamilyFriendly": true, "displayUrl": "https://w3sdev.com/7-supervised-learning-<b>classification</b>-machine-learning.html", "snippet": "The reason for considering <b>two-dimensional</b> data space is that we are considering just the two features of the Student data set, i.e. \u2018Aptitude\u2019 and \u2018Communication\u2019, for doing the <b>classification</b>. The feature \u2018Name\u2019 is ignored because, as we <b>can</b> understand, it has no role to play in deciding the class value. The test data point for student Josh is represented as an asterisk in the same space. To find out the closest or nearest neighbours of the test data point, Euclidean distance ...", "dateLastCrawled": "2021-12-26T17:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Bayesian <b>Decision</b> Theory - byclb.com", "url": "http://byclb.com/TR/Tutorials/neural_networks/ch4_1.htm", "isFamilyFriendly": true, "displayUrl": "byclb.com/TR/Tutorials/neural_networks/ch4_1.htm", "snippet": "If we employ a zero-one or <b>classification</b> loss, our <b>decision</b> boundaries are determined by the <b>threshold</b>, if ... Samples drawn from a <b>two-dimensional</b> Gaussian lie in a cloud centered on the mean. The ellipses show lines of equal probability density of the Gaussian. Figure 4. 6: The contour lines show the regions for which the function has constant density. From the equation for the normal density, it is apparent that points, which have the same density, must have the same constant term (x-\u00b5 ...", "dateLastCrawled": "2022-02-02T14:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "vertopal.com_DSBch3b.pdf - Chapter 3 Introduction to Predictive ...", "url": "https://www.coursehero.com/file/127645389/vertopalcom-DSBch3bpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/127645389/vertopalcom-DSBch3bpdf", "snippet": "In higher dimensions, since each node of a <b>classification</b> tree tests one variable it may <b>be thought</b> of as \u201cfixing\u201d that one dimension of a <b>decision</b> <b>boundary</b>; therefore, for a problem of n variables, each node of a <b>classification</b> tree imposes an (n\u20131)-dimensional \u201chyperplane\u201d <b>decision</b> <b>boundary</b> on the instance space.", "dateLastCrawled": "2022-01-29T03:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Linear Discriminant Functions - byclb.com", "url": "https://www.byclb.com/TR/Tutorials/neural_networks/ch9_1.htm", "isFamilyFriendly": true, "displayUrl": "https://www.byclb.com/TR/Tutorials/neural_networks/ch9_1.htm", "snippet": "9.2 Linear Discriminant Functions and <b>Decision</b> Surfaces. A discriminant function that is a linear combination of the components of x <b>can</b> be written as. (9.1) where w is the weight vector and w0 the bias or <b>threshold</b> weight. Linear discriminant functions are going to be studied for the two-category case, multi-category case, and general case ...", "dateLastCrawled": "2022-01-29T06:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>IS 300 (Analytics) Midterm</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/273111798/is-300-analytics-midterm-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/273111798/<b>is-300-analytics-midterm</b>-flash-cards", "snippet": "Discriminates between classes and the function of the <b>decision</b> <b>boundary</b> is a linear combination\u2014a weighted sum\u2014of the attributes. A linear discriminant function is a numeric <b>classification</b> model. (i.e. Age = (-1.5) x Balance + 60) Linear discriminant function. The weights of the linear function (wi) are the parameters. The data mining is going to &quot;fit&quot; this parameterized model to a dataset\u2014meaning specifically, to find a good set of weights on the features. General Linear Model. A ...", "dateLastCrawled": "2021-11-21T06:30:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Linear <b>Classification</b> Rules", "url": "https://www.asc.ohio-state.edu/goel.1/STATLEARN/LECTURE_SLIDES/LinearClassificationRules05.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.asc.ohio-state.edu/goel.1/STATLEARN/LECTURE_SLIDES/Linear<b>Classification</b>...", "snippet": "<b>Decision</b> Boundaries: Prior Probability Effect FIGURE 5. As the priors are changed, the <b>decision</b> <b>boundary</b> shifts. For suf ficiently disparate priors the <b>boundary</b> will not lie between the means of these one-, and <b>two-dimensional</b> spherical Gaussian distributions. From: Richard O.Duda, Peter E. Hart, and David G. Stork, Pattern <b>Classification</b>.", "dateLastCrawled": "2021-11-08T10:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Hyperplane as a <b>decision</b> <b>boundary</b> for a two-class, <b>two-dimensional</b> ...", "url": "https://researchgate.net/figure/Hyperplane-as-a-decision-boundary-for-a-two-class-two-dimensional-pattern-classification_fig7_222699870", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Hyper<b>plane</b>-as-a-<b>decision</b>-<b>boundary</b>-for-a-two-class-two...", "snippet": "Download scientific diagram | Hyperplane as a <b>decision</b> <b>boundary</b> for a two-class, <b>two-dimensional</b> pattern <b>classification</b> problem. from publication: Chaotic maps and pattern recognition - The XOR ...", "dateLastCrawled": "2021-05-09T09:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "CSE 455/555 Spring <b>2011 Homework 1: Bayesian Decision Theory Jason</b> J. Corso", "url": "https://cse.buffalo.edu/~jcorso/t/2011S_555/files/homework1-solutions.pdf", "isFamilyFriendly": true, "displayUrl": "https://cse.buffalo.edu/~jcorso/t/2011S_555/files/homework1-solutions.pdf", "snippet": "straight line in <b>two dimensional</b> case, and <b>plane</b> or hyper-<b>plane</b> in three or higher dimensional space. In particular if the covariance matrix are in a special diagonal form \u03a3 = \u03c32 \u00b7 I, the direction of the <b>decision</b> surface will be perpendicular to the direction between two means. If the covariance matrices are different for each class, then the quadratic term in the function of <b>decision</b> surface will not be canceled, and thus it will not be a straight <b>plane</b>. Let\u2019s restrict us to a simpler ...", "dateLastCrawled": "2022-01-29T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Coursera\u2019s machine learning course week three (logistic ... - Linbug", "url": "https://linbug.github.io/machine%20learning/2015/07/27/Logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://linbug.github.io/machine learning/2015/07/27/Logistic-regression", "snippet": "You <b>can</b> see that on our <b>two-dimensional</b> scatter plot, the <b>decision</b> <b>boundary</b> is represented as a line. Any points that fall to the bottom left of the <b>decision</b> <b>boundary</b> are students that will not be admitted to university, whereas those falling to the top right will be admitted. If there was only one input feature, the <b>decision</b> <b>boundary</b> would have been a point, and if there had been three input features it would have been a <b>plane</b> in the three-dimensional space, and so on. We had already been ...", "dateLastCrawled": "2022-01-31T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>two dimensional accuracy-based measure for classification performance</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0020025516319181", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0020025516319181", "snippet": "Thus, the performance of multiclassifiers is represented <b>in a two dimensional</b> space where the models <b>can</b> <b>be compared</b> in a more fair manner, providing greater awareness of the strategies that are more accurate when trying to improve the performance of a classifier. Furthermore we experimentally analyze the behavior of seven different performance metrics based on the computation of the confusion matrix values in several scenarios, identifying clusters and relationships between measures. As ...", "dateLastCrawled": "2021-12-21T09:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Support vector machines: The linearly separable case</b>", "url": "https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html", "isFamilyFriendly": true, "displayUrl": "https://nlp.stanford.edu/IR-book/html/htmledition/<b>support-vector-machines-the-linearly</b>...", "snippet": "A value of indicates one class, and a value of the other class.. We are confident in the <b>classification</b> of a point if it is far away from the <b>decision</b> <b>boundary</b>. For a given data set and <b>decision</b> hyperplane, we define the functional margin of the example with respect to a hyperplane as the quantity .The functional margin of a data set with respect to a <b>decision</b> surface is then twice the functional margin of any of the points in the data set with minimal functional margin (the factor of 2 ...", "dateLastCrawled": "2022-02-02T10:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "COMS 4721: Machine Learning for Data Science 4ptLecture 8, 2/14/2017", "url": "http://www.columbia.edu/~jwp2128/Teaching/W4721/Spring2017/slides/lecture_2-14-17.pdf", "isFamilyFriendly": true, "displayUrl": "<b>www.columbia.edu</b>/~jwp2128/Teaching/W4721/Spring2017/slides/lecture_2-14-17.pdf", "snippet": "So we <b>can</b> write the <b>decision</b> rule for this Bayes classi\ufb01er as a linear one: f(x) = sign(xTw + w 0): I This is what we saw last lecture (but now class 0 is called 1) I The Bayes classi\ufb01er produced a linear <b>decision</b> <b>boundary</b> in the data space when 1 = 0. I w and w 0 are obtained through a speci\ufb01c equation. 2.6. DISCRIMINANT FUNCTIONS FOR ...", "dateLastCrawled": "2021-12-23T01:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Perceptron</b>: Explanation, Implementation and a Visual Example | by ...", "url": "https://towardsdatascience.com/perceptron-explanation-implementation-and-a-visual-example-3c8e76b4e2d1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>perceptron</b>-explanation-implementation-and-a-visual...", "snippet": "As you see above, the <b>decision</b> <b>boundary</b> of a <b>perceptron</b> with 2 inputs is a line. If there were 3 inputs, the <b>decision</b> <b>boundary</b> would be a 2D <b>plane</b>. In general, if we have n inputs the <b>decision</b> <b>boundary</b> will be a n-1 dimensional object called a hyperplane that separates our n-dimensional feature space into 2 parts: one in which the points are classified as positive, and one in which the points are classified as negative(by convention, we will consider points that are exactly on the <b>decision</b> ...", "dateLastCrawled": "2022-01-29T17:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>SVM Algorithm Tutorial</b>: Steps for Building Models Using Python and Sklearn", "url": "https://intellipaat.com/blog/tutorial/machine-learning-tutorial/svm-algorithm-in-python/", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/blog/tutorial/machine-learning-tutorial/svm-algorithm-", "snippet": "SVM Figure 5: Margin and Maximum Margin Classifier. The region that the closest points define around the <b>decision</b> <b>boundary</b> is known as the margin. That is why the <b>decision</b> <b>boundary</b> of a support vector machine model is known as the maximum margin classifier or the maximum margin hyperplane.. In other words, here\u2019s how a support vector machine algorithm model works:", "dateLastCrawled": "2022-02-02T03:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Support Vector Machine</b> (SVM) Algorithm - <b>Javatpoint</b>", "url": "https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.<b>javatpoint</b>.com/machine-learning-<b>support-vector-machine</b>-algorithm", "snippet": "The goal of the SVM algorithm is to create the best line or <b>decision</b> <b>boundary</b> that <b>can</b> segregate n-dimensional space into classes so that we <b>can</b> easily put the new data point in the correct category in the future. This best <b>decision</b> <b>boundary</b> is called a hyperplane. SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as <b>Support Vector Machine</b>. Consider the below diagram in which there are ...", "dateLastCrawled": "2022-02-02T22:46:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Classification in Machine Learning</b> | by Apoorva Dave | DataDrivenInvestor", "url": "https://medium.datadriveninvestor.com/classification-in-machine-learning-db33514c77ad", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/<b>classification-in-machine-learning</b>-db33514c77ad", "snippet": "There are different algorithms in <b>Machine</b> <b>Learning</b> to solve <b>classification</b> problem. SVM. In SVM or Support Vector Machines, we differentiate between the categories by separating the classes with an optimal hyperplane. Optimal hyperplane is the plane which will have the maximum margin. In the figure, there could have been multiple hyperplanes separating the classes but the optimal plane is the one with maximum margin as shown above. The points that are closest to hyperplane which define the ...", "dateLastCrawled": "2022-01-20T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Evaluating a <b>Machine</b> <b>Learning</b> Model: Regression and <b>Classification</b> ...", "url": "https://arnavbansal-8232.medium.com/evaluating-a-machine-learning-model-regression-and-classification-metrics-4f2316e180b4", "isFamilyFriendly": true, "displayUrl": "https://arnavbansal-8232.medium.com/evaluating-a-<b>machine</b>-<b>learning</b>-model-regression-and...", "snippet": "<b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> plays a vital role in all this. The most important part of anything which is done in the field of <b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> is its application. Applications are driven by performance and performance is achieved by better and improved results. Now, there are many automated tools, libraries and scripts which allows you to directly run a ML model without knowing anything about it. This results in fast development, and especially beginners are ...", "dateLastCrawled": "2022-01-05T00:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding <b>Classification</b> Thresholds Using Isocurves | by Druce ...", "url": "https://towardsdatascience.com/understanding-classification-thresholds-using-isocurves-9e5e7e00e5a2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>classification</b>-<b>thresholds</b>-using-isocurves...", "snippet": "The ROC curve visualizes the set of feasible solutions, as you vary the <b>classification</b> <b>threshold</b>, implicitly varying the cost of false positives relative to false negatives. If the positive class represents the detection of a stop sign or a medical condition, the cost of a false negative is high. You need a low <b>threshold</b> to minimize false negatives, which cause you to blow through an intersection and collide with traffic or fail to obtain further life-saving diagnosis or treatment. If your ...", "dateLastCrawled": "2022-02-02T19:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) A <b>Machine Learning Application for Classification of Chemical</b> Spectra", "url": "https://www.researchgate.net/publication/226296679_A_Machine_Learning_Application_for_Classification_of_Chemical_Spectra", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/226296679_A_<b>Machine</b>_<b>Learning</b>_Application_for...", "snippet": "One-class <b>classification</b> is a specialized form of <b>classification</b> from the field of <b>machine</b> <b>learning</b>. Traditional <b>classification</b> attempts to assign unknowns to known classes, but cannot handle ...", "dateLastCrawled": "2021-12-08T10:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Machine Learning Application for Classification of Chemical</b> Spectra", "url": "https://www.analyzeiq.com/publications/ai2008-classification-of-spectra.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.analyzeiq.com/publications/ai2008-<b>classification</b>-of-spectra.pdf", "snippet": "A <b>Machine Learning Application for Classification of Chemical</b> Spectra Michael G. Madden1 2and Tom Howley Abstract. ... SIMCA (Soft Independent Modeling of Class <b>Analogy</b>) is the most widely used chemometric <b>classification</b> technique [8]. In binary <b>classification</b>, SIMCA generates a separate PCA model for the set of samples of both classes. In prediction, the distance of a test sample to either model is calculated. Statistical tests are then used to determine if the test sample belongs to either ...", "dateLastCrawled": "2022-02-01T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>MACHINE LEARNING OF HYBRID CLASSIFICATION MODELS FOR DECISION SUPPORT</b>", "url": "http://portal.sinteza.singidunum.ac.rs/Media/files/2014/318-323.pdf", "isFamilyFriendly": true, "displayUrl": "portal.sinteza.singidunum.ac.rs/Media/files/2014/318-323.pdf", "snippet": "<b>machine</b> <b>learning</b>, <b>classification</b>, hybrid models, decision support, predictive accuracy, comprehensibility. Impact of Internet on Business activities in Serbia and Worldwide Uticaj Interneta na poslovanje u Srbiji i svetu doI: 10.15308/SInteZa-2014-318-323 INTRODUCTION <b>Machine</b> <b>Learning</b> algorithms are used in data mining applications to retrieve hidden information that may be used in decision-making [1]. \u02d9 ere are various basic <b>learning</b> methods like rule-based <b>learning</b>, case-based reasoning ...", "dateLastCrawled": "2022-02-03T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Classification</b> - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/<b>classification</b>.html", "snippet": "Simply put, the goal of <b>classification</b> is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. <b>Classification</b> also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning</b> <b>Evaluation Metrics</b> - GitHub Pages", "url": "https://kevalnagda.github.io/evaluation-metrics", "isFamilyFriendly": true, "displayUrl": "https://kevalnagda.github.io/<b>evaluation-metrics</b>", "snippet": "Confusion Matrix Confusion Matrix is a performance measurement for a <b>machine learning</b> <b>classification</b> problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values as shown below. It is very useful for measuring other <b>evaluation metrics</b> such as Recall, Precision, Specificity, Accuracy, and most importantly AUC-ROC Curve. Following is an example in terms of pregnancy <b>analogy</b> to help you better understand TP, TN, FP, and FN. True ...", "dateLastCrawled": "2021-10-13T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding <b>AUC</b> - ROC Curve | by Sarang Narkhede | Towards Data Science", "url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>auc</b>-roc-curve-68b2303cc9c5", "snippet": "<b>AUC</b> - ROC curve is a performance measurement for the <b>classification</b> problems at various <b>threshold</b> settings. ROC is a probability curve and <b>AUC</b> represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the <b>AUC</b>, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By <b>analogy</b>, the Higher the <b>AUC</b>, the better the model is at distinguishing between patients with the disease and no disease.", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>All Unit MCQ questions of ML</b> \u2013 TheCodingShef", "url": "https://thecodingshef.com/all-unit-mcq-questions-of-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thecodingshef.com/<b>all-unit-mcq-questions-of</b>-<b>machine</b>-<b>learning</b>", "snippet": "Correct option is C. Choose the correct option regarding <b>machine</b> <b>learning</b> (ML) and artificial intelligence (AI) ML is a set of techniques that turns a dataset into a software. AI is a software that can emulate the human mind. ML is an alternate way of programming intelligent machines. All of the above. Correct option is D.", "dateLastCrawled": "2022-01-30T22:09:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(classification threshold)  is like +(the decision boundary in a two-dimensional plane)", "+(classification threshold) is similar to +(the decision boundary in a two-dimensional plane)", "+(classification threshold) can be thought of as +(the decision boundary in a two-dimensional plane)", "+(classification threshold) can be compared to +(the decision boundary in a two-dimensional plane)", "machine learning +(classification threshold AND analogy)", "machine learning +(\"classification threshold is like\")", "machine learning +(\"classification threshold is similar\")", "machine learning +(\"just as classification threshold\")", "machine learning +(\"classification threshold can be thought of as\")", "machine learning +(\"classification threshold can be compared to\")"]}