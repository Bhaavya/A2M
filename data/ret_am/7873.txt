{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>the advantage/disadvantage of Hinge-loss compared</b> to cross ...", "url": "https://www.quora.com/What-is-the-advantage-disadvantage-of-Hinge-loss-compared-to-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-advantage-disadvantage-of-Hinge-loss-compared</b>-to...", "snippet": "Answer (1 of 2): Cross Entropy (or Log <b>Loss</b>), Hing <b>Loss</b> (SVM <b>Loss</b>), Squared <b>Loss</b> etc. are different forms of <b>Loss</b> functions. Log <b>Loss</b> in the classification context gives Logistic Regression, while the <b>Hinge</b> <b>Loss</b> is Support Vector Machines. Logistic Regression and SVMs both are linear classifiers,...", "dateLastCrawled": "2022-01-29T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "The <b>Hinge</b> <b>Loss</b> Equation def <b>Hinge</b>(yhat, y): return np.max(0,1 - yhat * y) Where y is the actual label (-1 or 1) and \u0177 is the prediction; The <b>loss</b> is 0 when the signs of the labels and prediction ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How is <b>hinge</b> <b>loss</b> calculated?", "url": "https://psichologyanswers.com/library/lecture/read/130931-how-is-hinge-loss-calculated", "isFamilyFriendly": true, "displayUrl": "https://psichologyanswers.com/library/lecture/read/130931-how-is-<b>hinge</b>-<b>loss</b>-calculated", "snippet": "From our SVM model, we <b>know</b> that <b>hinge</b> <b>loss</b> = [0, 1- yf(x)].. What is <b>hinge</b> <b>loss</b> in SVM? In machine learning, the <b>hinge</b> <b>loss</b> is a <b>loss</b> function used for training classifiers. The <b>hinge</b> <b>loss</b> is used for &quot;maximum-margin&quot; classification, most notably for support vector machines (SVMs).For an intended output t = \u00b11 and a classifier score y, the <b>hinge</b> <b>loss</b> of the prediction y is defined as.. Is <b>hinge</b> <b>loss</b> differentiable?", "dateLastCrawled": "2022-01-14T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding <b>Ranking Loss, Contrastive Loss</b>, Margin <b>Loss</b>, Triplet <b>Loss</b> ...", "url": "https://gombru.github.io/2019/04/03/ranking_loss/", "isFamilyFriendly": true, "displayUrl": "https://gombru.github.io/2019/04/03/ranking_<b>loss</b>", "snippet": "Understanding <b>Ranking Loss, Contrastive Loss</b>, Margin <b>Loss</b>, Triplet <b>Loss</b>, <b>Hinge</b> <b>Loss</b> and all those confusing names. Apr 3, 2019. After the success of my post Understanding Categorical Cross-Entropy <b>Loss</b>, Binary Cross-Entropy <b>Loss</b>, Softmax <b>Loss</b>, Logistic <b>Loss</b>, Focal <b>Loss</b> and all those confusing names, and after checking that Triplet <b>Loss</b> outperforms Cross-Entropy <b>Loss</b> in my main research topic (Multi-Modal Retrieval) I decided to write a similar post explaining Ranking Losses functions ...", "dateLastCrawled": "2022-01-30T07:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Most Used <b>Loss</b> <b>Functions To Optimize Machine Learning Algorithms</b>", "url": "https://analyticsindiamag.com/most-used-loss-functions-to-optimize-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/most-used-<b>loss</b>-<b>functions-to-optimize-machine-learning</b>...", "snippet": "The simple intuition behind <b>hinge</b> <b>loss</b> is, it works on <b>the difference</b> of sign. For e.g. the target variable has values <b>like</b> -1 and 1 and the model predicts 1 whereas the actual class is -1, the function will impose a higher penalty at this point because it <b>can</b> sense <b>the difference</b> in the sign. The below given formula would work if the class labels are -1 and +1. Squared <b>Hinge</b> <b>Loss</b>: Squared <b>hinge</b> <b>loss</b> is nothing but an extension to <b>hinge</b> <b>loss</b>. At times we don\u2019t need to <b>know</b> the ...", "dateLastCrawled": "2022-02-01T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> Functions in Neural Networks - theaidream.com", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to <b>predict</b> the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies <b>between</b> (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>What Are Different Loss Functions Used as Optimizers</b> in Neural Networks ...", "url": "https://www.analyticssteps.com/blogs/what-are-different-loss-functions-used-optimizers-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.analyticssteps.com/blogs/<b>what-are-different-loss-functions-used</b>-optimizers...", "snippet": "Classification <b>loss</b> is the case where the aim is to <b>predict</b> the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies <b>between</b> (0-9), in these kinds of scenarios classification <b>loss</b> is used. Whereas if the problem is regression <b>like</b> predicting the continuous values for example, if need to <b>predict</b> the weather conditions or predicting the prices of houses on the basis of some features. In this type of case ...", "dateLastCrawled": "2022-02-02T14:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Support Vector Machine vs Logistic Regression</b>", "url": "https://gdcoder.com/support-vector-machine-vs-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://gdcoder.com/<b>support-vector-machine-vs-logistic-regression</b>", "snippet": "How <b>you</b> define this notion of \u201cbest\u201d gives <b>you</b> different models <b>like</b> SVM and logistic regression (LR). What is Support Vector Machine? The objective of the support vector machine algorithm is to find the hyperplane that has the maximum margin in an N-dimensional space(N \u2014 the number of features) that distinctly classifies the data points. Data points falling on either side of the hyperplane <b>can</b> be attributed to different classes. Also, the dimension of the hyperplane depends upon the ...", "dateLastCrawled": "2022-02-02T14:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning Interview Questions</b> (2022) - InterviewBit", "url": "https://www.interviewbit.com/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.interviewbit.com/<b>machine-learning-interview-questions</b>", "snippet": "P-value is the minimum significant level at which <b>you</b> <b>can</b> reject the null hypothesis. The lower the p-value, the more likely <b>you</b> reject the null hypothesis. 30. What are Parametric and Non-Parametric Models? Parametric models will have limited parameters and to <b>predict</b> new data, <b>you</b> only need to <b>know</b> the parameter of the model.", "dateLastCrawled": "2022-02-03T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - 0-1 <b>Loss</b> Function explanation - Cross Validated", "url": "https://stats.stackexchange.com/questions/284028/0-1-loss-function-explanation", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/284028", "snippet": "Show activity on this post. Yes, this is basically it: <b>you</b> count the number of misclassified items. There is nothing more behind it, it is a very basic <b>loss</b> function. What follows, 0-1 <b>loss</b> leads to estimating mode of the target distribution (as compared to L 1 <b>loss</b> for estimating median and L 2 <b>loss</b> for estimating mean). Share.", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>the advantage/disadvantage of Hinge-loss compared</b> to cross ...", "url": "https://www.quora.com/What-is-the-advantage-disadvantage-of-Hinge-loss-compared-to-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-advantage-disadvantage-of-Hinge-loss-compared</b>-to...", "snippet": "Answer (1 of 2): Cross Entropy (or Log <b>Loss</b>), Hing <b>Loss</b> (SVM <b>Loss</b>), Squared <b>Loss</b> etc. are different forms of <b>Loss</b> functions. Log <b>Loss</b> in the classification context gives Logistic Regression, while the <b>Hinge</b> <b>Loss</b> is Support Vector Machines. Logistic Regression and SVMs both are linear classifiers,...", "dateLastCrawled": "2022-01-29T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How is <b>hinge</b> <b>loss</b> calculated?", "url": "https://psichologyanswers.com/library/lecture/read/130931-how-is-hinge-loss-calculated", "isFamilyFriendly": true, "displayUrl": "https://psichologyanswers.com/library/lecture/read/130931-how-is-<b>hinge</b>-<b>loss</b>-calculated", "snippet": "From our SVM model, we <b>know</b> that <b>hinge</b> <b>loss</b> = [0, 1- yf(x)].. What is <b>hinge</b> <b>loss</b> in SVM? In machine learning, the <b>hinge</b> <b>loss</b> is a <b>loss</b> function used for training classifiers. The <b>hinge</b> <b>loss</b> is used for &quot;maximum-margin&quot; classification, most notably for support vector machines (SVMs).For an intended output t = \u00b11 and a classifier score y, the <b>hinge</b> <b>loss</b> of the prediction y is defined as.. Is <b>hinge</b> <b>loss</b> differentiable?", "dateLastCrawled": "2022-01-14T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Support Vector Machine vs Logistic Regression</b>", "url": "https://gdcoder.com/support-vector-machine-vs-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://gdcoder.com/<b>support-vector-machine-vs-logistic-regression</b>", "snippet": "If <b>you</b> look at the optimization problems of linear SVM and (regularized) LR, they are very <b>similar</b>: That is, they only differ in the <b>loss</b> function \u2014 SVM minimizes <b>hinge</b> <b>loss</b> while logistic regression minimizes logistic <b>loss</b>. <b>Loss</b> functions. There are 2 differences to note: Logistic <b>loss</b> diverges faster than <b>hinge</b> <b>loss</b>. So, in general, it will ...", "dateLastCrawled": "2022-02-02T14:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding <b>Ranking Loss, Contrastive Loss</b>, Margin <b>Loss</b>, Triplet <b>Loss</b> ...", "url": "https://gombru.github.io/2019/04/03/ranking_loss/", "isFamilyFriendly": true, "displayUrl": "https://gombru.github.io/2019/04/03/ranking_<b>loss</b>", "snippet": "That score <b>can</b> be binary (<b>similar</b> / dissimilar). As an example, imagine a face verification dataset, where we <b>know</b> which face images belong to the same person (<b>similar</b>), and which not (dissimilar). Using a Ranking <b>Loss</b> function, we <b>can</b> train a CNN to infer if two face images belong to the same person or not. To use a Ranking <b>Loss</b> function we first extract features from two (or three) input data points and get an embedded representation for each of them. Then, we define a metric function to ...", "dateLastCrawled": "2022-01-30T07:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to use <b>categorical / multiclass hinge with TensorFlow</b> 2 and Keras ...", "url": "https://www.machinecurve.com/index.php/2019/10/17/how-to-use-categorical-multiclass-hinge-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/.../10/17/how-to-use-categorical-multiclass-<b>hinge</b>-with-keras", "snippet": "Computing the <b>loss</b> \u2013 the <b>difference</b> <b>between</b> actual target and predicted targets \u2013 is then equal to computing the <b>hinge</b> <b>loss</b> for taking the prediction for all the computed classes, except for the target class, since <b>loss</b> is always 0 there. The <b>hinge</b> <b>loss</b> computation itself <b>is similar</b> to the traditional <b>hinge</b> <b>loss</b>. Categorical <b>hinge</b> <b>loss</b> <b>can</b> be optimized as well and hence used for generating decision boundaries in multiclass machine learning problems. Let\u2019s now see how we <b>can</b> implement ...", "dateLastCrawled": "2022-01-29T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "python - Why SGDClassifier with <b>hinge</b> <b>loss</b> is faster than SVC ...", "url": "https://stackoverflow.com/questions/60061412/why-sgdclassifier-with-hinge-loss-is-faster-than-svc-implementation-in-scikit-le", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/60061412", "snippet": "As we <b>can</b> see there is a huge <b>difference</b> <b>between</b> all of them, but linear and SGDC have more or less the same time. The time keeps being a little bit different, but this will always happen since the execution of each algorithm does not come from the same code.", "dateLastCrawled": "2022-01-29T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>What is an intuitive explanation of the multiclass hinge</b> <b>loss</b>? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-of-the-multiclass-hinge-loss", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-an-intuitive-explanation-of-the-multiclass-hinge</b>-<b>loss</b>", "snippet": "Answer (1 of 2): Consider the class j selected by the max above. If it is not y (for which the <b>loss</b> is 0 in the equation), then j is an incorrect class and it satisfies (because of the max) w_j^T x - w_y^T x +1 &gt;=0 \\Rightarrow w_j^T x &gt;= w_y^T x +1 This means that it&#39;s doing better than the tr...", "dateLastCrawled": "2022-01-20T23:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>What Are Different Loss Functions Used as Optimizers</b> in Neural Networks ...", "url": "https://www.analyticssteps.com/blogs/what-are-different-loss-functions-used-optimizers-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.analyticssteps.com/blogs/<b>what-are-different-loss-functions-used</b>-optimizers...", "snippet": "Classification <b>loss</b> is the case where the aim is to <b>predict</b> the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies <b>between</b> (0-9), in these kinds of scenarios classification <b>loss</b> is used. Whereas if the problem is regression like predicting the continuous values for example, if need to <b>predict</b> the weather conditions or predicting the prices of houses on the basis of some features. In this type of case ...", "dateLastCrawled": "2022-02-02T14:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding the 3 most common <b>loss</b> functions for Machine Learning ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "To calculate the MSE, <b>you</b> take the <b>difference</b> <b>between</b> your model\u2019s predictions and the ground truth, square it, and average it out across the whole dataset. The MSE will never be negative, since we are always squaring the errors. The MSE is formally defined by the following equation: Where N is the number of samples we are testing against. The code is simple enough, we <b>can</b> write it in plain numpy and plot it using matplotlib: MSE <b>Loss Function</b>. Advantage: The MSE is great for ensuring that ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning Interview Questions</b> (2022) - InterviewBit", "url": "https://www.interviewbit.com/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.interviewbit.com/<b>machine-learning-interview-questions</b>", "snippet": "Explain the <b>Difference</b> <b>Between</b> Classification and Regression? Advanced Machine Learning Questions. 13. What is F1 score? How would <b>you</b> use it? 14. Define Precision and Recall? 15. How to Tackle Overfitting and Underfitting? 16. What is a Neural Network? 17. What are <b>Loss</b> Function and Cost Functions? Explain the key <b>Difference</b> <b>Between</b> them? 18. What is Ensemble learning? 19. How do <b>you</b> make sure which Machine Learning Algorithm to use? 20. How to Handle Outlier Values? 21. What is a Random ...", "dateLastCrawled": "2022-02-03T00:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions in <b>Machine Learning</b> - 360DigiTMG", "url": "https://360digitmg.com/loss-functions-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://360digitmg.com/<b>loss</b>-functions-in-<b>machine-learning</b>", "snippet": "Binary cross entropy is the measure of the <b>difference</b> <b>between</b> the probability distributions for a set of given random variables and/or events.In the case of a two class classification, target variables are have two classes and the cross-entropy <b>can</b> be defined as: <b>Hinge</b> <b>Loss</b>: This <b>loss</b> typically serves as an alternative to the cross-entropy and was initially developed to use with the support vector machine algorithm. It typically works best when the values of the output variable are in the ...", "dateLastCrawled": "2022-01-30T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "Graph for -log(x) This is pretty simple, the more your input increases, the more output goes lower. If <b>you</b> have a small input(x=0.5) so the output is going to be high(y=0.305).", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Cost Functions In Machine Learning</b> - The Click Reader", "url": "https://www.theclickreader.com/cost-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.theclickreader.com/<b>cost-functions-in-machine-learning</b>", "snippet": "It maps the output to values <b>between</b> 1, 0, -1. The <b>Hinge</b> <b>loss</b> function is calculated as, where is actual value of the output, is the classification score predicted by the model. From the function, it <b>can</b> be determined that when , then the cost function is zero. However, when , the cost function increases.", "dateLastCrawled": "2022-02-02T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Support Vector Machine</b> \u2014 Introduction to Machine Learning Algorithms ...", "url": "https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>support-vector-machine</b>-introduction-to-machine-learning...", "snippet": "The <b>loss</b> function that helps maximize the margin is <b>hinge</b> <b>loss</b>. <b>Hinge</b> <b>loss</b> function (function on left <b>can</b> be represented as a function on the right) The cost is 0 if the predicted value and the actual value are of the same sign. If they are not, we then calculate the <b>loss</b> value. We also add a regularization parameter the cost function. The objective of the regularization parameter is to balance the margin maximization and <b>loss</b>. After adding the regularization parameter, the cost functions ...", "dateLastCrawled": "2022-02-03T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions in Machine Learning: An Easy Overview(2021)", "url": "https://www.jigsawacademy.com/blogs/ai-ml/loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.<b>jigsawacademy</b>.com/blogs/ai-ml/<b>loss</b>-function", "snippet": "Although <b>you</b> might not have <b>thought</b> this before, a color scheme is one of the more crucial decisions <b>you</b> <b>can</b> make in the web design process. According to one study, it takes under 2 minutes for people to form an opinion on whether or not they want to buy a product, and <b>between</b> 60 to 90 percent of that decision is based just on colors. If that statistic doesn\u2019t make <b>you</b> think twice about your color choices, it should. However, with thousands of palettes to choose from, picking the right one ...", "dateLastCrawled": "2022-01-26T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding binary <b>cross-entropy</b> / log <b>loss</b>: a visual explanation ...", "url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-binary-<b>cross-entropy</b>-log-<b>loss</b>-a-visual...", "snippet": "Binary <b>Cross-Entropy</b> / Log <b>Loss</b>. where y is the label (1 for green points and 0 for red points) and p(y) is the predicted probability of the point being green for all N points.. Reading this formula, it tells <b>you</b> that, for each green point (y=1), it adds log(p(y)) to the <b>loss</b>, that is, the log probability of it being green.Conversely, it adds log(1-p(y)), that is, the log probability of it being red, for each red point (y=0).Not necessarily difficult, sure, but no so intuitive too\u2026", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>An Intro to Linear Classification with Python</b> - PyImageSearch", "url": "https://www.pyimagesearch.com/2016/08/22/an-intro-to-linear-classification-with-python/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/2016/08/22/<b>an-intro-to-linear-classification-with-python</b>", "snippet": "While <b>hinge</b> <b>loss</b> is used in many machine learning applications (e.g., SVMs), I <b>can</b> almost guarantee with absolute certainty that <b>you</b>\u2019ll see cross-entropy <b>loss</b> with more frequency primarily due to the fact that Softmax classifiers output probabilities rather than margins. Probabilities are much easier for us as humans to interpret, so this fact is a particularly nice quality of cross-entropy <b>loss</b> and Softmax classifiers. For more information on <b>hinge</b> <b>loss</b> and cross-entropy <b>loss</b>, please refer to", "dateLastCrawled": "2022-02-03T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is <b>the difference between Inverse reinforcement Learning</b> and ...", "url": "https://www.quora.com/What-is-the-difference-between-Inverse-reinforcement-Learning-and-supervised-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-difference-between-Inverse-reinforcement-Learning</b>...", "snippet": "Answer: In supervised learning, <b>you</b> are given training examples, where these examples have inputs and labels for these inputs. For instance, your training set might consist of images as an input, that are labeled as either dogs or cats. The goal, then, is given a new image, <b>you</b> should be able to ...", "dateLastCrawled": "2022-01-18T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Scikit Learn - Quick Guide</b> - <b>Tutorialspoint</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_quick_guide.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_quick_guide</b>.htm", "snippet": "If <b>loss</b> = \u2018epsilon-insensitive\u2019, any <b>difference</b>, <b>between</b> current prediction and the correct label, less than the threshold would be ignored. 10: max_iter \u2212 int, optional, default = 1000. As name suggest, it represents the maximum number of passes over the epochs i.e. training data. 11: warm_start \u2212 bool, optional, default = false. With this parameter set to True, we <b>can</b> reuse the solution of the previous call to fit as initialization. If we choose default i.e. false, it will erase ...", "dateLastCrawled": "2022-01-30T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>difference</b> <b>between</b> expect and deserve? - Quora", "url": "https://www.quora.com/What-is-difference-between-expect-and-deserve", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>difference</b>-<b>between</b>-expect-and-deserve", "snippet": "Answer (1 of 3): Do not expect, deserve. These are difficult times. One should try to deserve as many positions as one <b>can</b>; but one should not expect anything at all. Lord Krishna already said in Mahabharat: Do your duty, but do not expect anything in return. There are contradictions, however. Ma...", "dateLastCrawled": "2022-01-17T03:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>the advantage/disadvantage of Hinge-loss compared</b> to cross ...", "url": "https://www.quora.com/What-is-the-advantage-disadvantage-of-Hinge-loss-compared-to-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-advantage-disadvantage-of-Hinge-loss-compared</b>-to...", "snippet": "Answer (1 of 2): Cross Entropy (or Log <b>Loss</b>), Hing <b>Loss</b> (SVM <b>Loss</b>), Squared <b>Loss</b> etc. are different forms of <b>Loss</b> functions. Log <b>Loss</b> in the classification context gives Logistic Regression, while the <b>Hinge</b> <b>Loss</b> is Support Vector Machines. Logistic Regression and SVMs both are linear classifiers,...", "dateLastCrawled": "2022-01-29T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding <b>Ranking Loss, Contrastive Loss</b>, Margin <b>Loss</b>, Triplet <b>Loss</b> ...", "url": "https://gombru.github.io/2019/04/03/ranking_loss/", "isFamilyFriendly": true, "displayUrl": "https://gombru.github.io/2019/04/03/ranking_<b>loss</b>", "snippet": "We call it siamese nets. But a pairwise ranking <b>loss</b> <b>can</b> be used in other setups, or with other nets. Is this setup positive and negative pairs of training data points are used. Positive pairs are composed by an anchor sample \\(x_a\\) and a positive sample \\(x_p\\), which is similar to \\(x_a\\) in the metric we aim to learn, and negative pairs composed by an anchor sample \\(x_a\\) and a negative sample \\(x_n\\), which is dissimilar to \\(x_a\\) in that metric. The objective is to learn ...", "dateLastCrawled": "2022-01-30T07:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to <b>predict</b> the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies <b>between</b> (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "python - Why SGDClassifier with <b>hinge</b> <b>loss</b> is faster than SVC ...", "url": "https://stackoverflow.com/questions/60061412/why-sgdclassifier-with-hinge-loss-is-faster-than-svc-implementation-in-scikit-le", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/60061412", "snippet": "As we <b>can</b> see there is a huge <b>difference</b> <b>between</b> all of them, but linear and SGDC have more or less the same time. The time keeps being a little bit different, but this will always happen since the execution of each algorithm does not come from the same code.", "dateLastCrawled": "2022-01-29T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Multi-class SVM Loss - PyImageSearch</b>", "url": "https://www.pyimagesearch.com/2016/09/05/multi-class-svm-loss/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/2016/09/05/<b>multi-class-svm-loss</b>", "snippet": "Figure 2: An example of applying <b>hinge</b> <b>loss</b> to a 3-class image classification problem. Let\u2019s again compute the <b>loss</b> for the dog class: &gt;&gt;&gt; max(0, 1.49 - (-0.39) + 1) + max(0, 4.21 - (-0.39) + 1) 8.48 &gt;&gt;&gt; Notice how that our summation has expanded to include two terms \u2014 the <b>difference</b> <b>between</b> the predicted dog score and both the cat and horse score.. Similarly, we <b>can</b> compute the <b>loss</b> for the cat class:", "dateLastCrawled": "2022-02-02T22:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What <b>are the advantages of hinge loss over</b> log <b>loss</b>? - Quora", "url": "https://www.quora.com/What-are-the-advantages-of-hinge-loss-over-log-loss", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>are-the-advantages-of-hinge-loss-over</b>-log-<b>loss</b>", "snippet": "Answer: <b>Hinge</b> <b>loss</b> is easier to compute than log <b>loss</b>. Ditto for its derivative or subgradient. <b>Hinge</b> <b>loss</b> also induces sparsity in the solution, if the ML weights are a linear combination of the training observations. <b>Hinge</b> <b>loss</b> <b>can</b> also be faster to train via SGD, since much of the time the g...", "dateLastCrawled": "2022-01-20T07:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Loss</b> Functions in <b>Machine Learning</b> - 360DigiTMG", "url": "https://360digitmg.com/loss-functions-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://360digitmg.com/<b>loss</b>-functions-in-<b>machine-learning</b>", "snippet": "Binary cross entropy is the measure of the <b>difference</b> <b>between</b> the probability distributions for a set of given random variables and/or events.In the case of a two class classification, target variables are have two classes and the cross-entropy <b>can</b> be defined as: <b>Hinge</b> <b>Loss</b>: This <b>loss</b> typically serves as an alternative to the cross-entropy and was initially developed to use with the support vector machine algorithm. It typically works best when the values of the output variable are in the ...", "dateLastCrawled": "2022-01-30T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "After completing this tutorial, <b>you</b> will <b>know</b>: ... Cross-entropy will calculate a score that summarizes the average <b>difference</b> <b>between</b> the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0. Cross-entropy <b>can</b> be specified as the <b>loss</b> function in Keras by specifying \u2018binary_crossentropy\u2018 when compiling the model. 1. model. compile (<b>loss</b> = &#39;binary_crossentropy&#39;, optimizer = opt, metrics = [&#39;accuracy&#39;]) The ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "<b>Loss</b>. <b>You</b> do however want to <b>know</b> how well the model performs with respect to the targets originally set. A well-performing model would be interesting for production usage, whereas an ill-performing model must be optimized before it <b>can</b> be actually used. This is where the concept of <b>loss</b> enters the equation. Most generally speaking, the <b>loss</b> allows us to compare <b>between</b> some actual targets and predicted targets. It does so by imposing a \u201ccost\u201d (or, using a different term, a \u201c<b>loss</b> ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to interpret \u201c<b>loss</b>\u201d and \u201c<b>accuracy</b>\u201d for a machine learning model ...", "url": "https://intellipaat.com/community/368/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/community/368/how-to-interpret-<b>loss</b>-and-<b>accuracy</b>-for-a-machine...", "snippet": "Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples.. <b>Loss</b> is the result of a bad prediction. A <b>loss</b> is a number indicating how bad the model&#39;s prediction was on a single example.. If the model&#39;s prediction is perfect, the <b>loss</b> is zero; otherwise, the <b>loss</b> is greater. The goal of training a model is to find a set of weights and biases that have low <b>loss</b>, on average, across all examples.", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Main concepts behind <b>Machine</b> <b>Learning</b> | by Leven.co.in | Medium", "url": "https://in-leven.medium.com/main-concepts-behind-machine-learning-848ec516ef94", "isFamilyFriendly": true, "displayUrl": "https://in-leven.medium.com/main-concepts-behind-<b>machine</b>-<b>learning</b>-848ec516ef94", "snippet": "The two most common <b>loss</b> function are <b>hinge</b>-<b>loss</b> and cross-entropy. The first one is used in SVM (Supported Vector Machines) classifiers and it concerns in getting the correct class score greater than the other scores by a margin \u0394. Formula for <b>hinge</b>-<b>loss</b>. s\u1d62 is the correct score category. The second one is used in Softmax classifiers which interprets the scores as probabilities, always trying to get the correct class close to 1. Formula for cross-entropy. s\u1d62 the correct category score ...", "dateLastCrawled": "2022-01-14T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Main concepts behind Machine Learning</b> | by Bruno Eidi Nishimoto ...", "url": "https://medium.com/neuronio/main-concepts-behind-machine-learning-22cd81d68a11", "isFamilyFriendly": true, "displayUrl": "https://medium.com/neuronio/<b>main-concepts-behind-machine-learning</b>-22cd81d68a11", "snippet": "The two most common <b>loss</b> function are <b>hinge</b>-<b>loss</b> and cross-entropy. The first one is used in SVM (Supported Vector Machines) classifiers and it concerns in getting the correct class score greater ...", "dateLastCrawled": "2022-01-19T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How is <b>hinge</b> <b>loss</b> calculated? - psichologyanswers.com", "url": "https://psichologyanswers.com/library/lecture/read/130931-how-is-hinge-loss-calculated", "isFamilyFriendly": true, "displayUrl": "https://psichologyanswers.com/library/lecture/read/130931-how-is-<b>hinge</b>-<b>loss</b>-calculated", "snippet": "In <b>machine</b> <b>learning</b>, the <b>hinge</b> <b>loss</b> is a <b>loss</b> function used for training classifiers. The <b>hinge</b> <b>loss</b> is used for &quot;maximum-margin&quot; classification, most notably for support vector machines ( SVMs ). For an intended output t = \u00b11 and a classifier score y, the <b>hinge</b> <b>loss</b> of the prediction y is defined as.", "dateLastCrawled": "2022-01-14T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "In contrast, in <b>machine</b> <b>learning</b> methodology, log <b>loss</b> will be minimized with respect to ... <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the following stages occur: Stage 1 ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, squared <b>hinge</b> <b>loss</b> function (as against <b>hinge</b> <b>loss</b> function) and l2 penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "Optimization methods are applied to minimize the <b>loss</b> function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one <b>loss</b> is L0-1 = 1 (m &lt;= 0); in zero-one <b>loss</b>, value of <b>loss</b> is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this <b>loss</b> is it is not differentiable, non-convex, and also NP-hard. Hence, in order to make optimization feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Models 1.1 Support vector <b>machine</b> 1.1.1 Principle 1.1.2 Kernel 1.1.3 Soft margin SVM 1.1.4 <b>Hinge</b> <b>loss</b> view 1.1.5 Multi-class SVM 1.1.6 Extensions 1.2 Tree-based models 1.2.1 Decision tree 1.2.2 Random forest 1.2.3 Gradient boosted decision trees 1.2.4 Tools 1.3 EM Principle 1.4 MaxEnt 1.4.1 Entropy 1.5 Model selection 1.5.1 Under-fitting / Over-fitting 1.5.2 Model ensemble, sklearn 2.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Statistical <b>Learning</b> Theory and the C-<b>Loss</b> cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/c<b>loss</b>.pdf", "snippet": "Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. Empirical Risk Minimization (ERM) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the Risk functional as L(.) is called the <b>Loss</b> function, and minimize it w.r.t. w achieving the best possible <b>loss</b>. But we can not do this integration because the joint is normally not ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Lecture 10. Support Vector Machines (cont.)", "url": "https://trevorcohn.github.io/comp90051-2017/slides/10_soft_margin_svm.pdf", "isFamilyFriendly": true, "displayUrl": "https://trevorcohn.github.io/comp90051-2017/slides/10_soft_<b>margin</b>_svm.pdf", "snippet": "Statistical <b>Machine</b> <b>Learning</b> (S2 2017) Deck 10 This lecture \u2022 Soft <b>margin</b> SVM \u2217Intuition and problem formulation \u2022 Solving the optimisation \u2217Transforming the original objective \u2217Re-parameterisation \u2022 Finishing touches \u2217Complementary slackness \u2217Solving the dual problem 2. Statistical <b>Machine</b> <b>Learning</b> (S2 2017) Deck 10 Brief recap: <b>hard margin</b> SVM \u2022 SVM is a linear binary classifier \u2022 Informally: aims for the safest boundary \u2022 Formally: derive distance from point to ...", "dateLastCrawled": "2022-01-30T22:43:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "We\u2019re then using <b>machine</b> <b>learning</b> for ... The squared <b>hinge loss is like</b> the hinge formula displayed above, but then the \\(max()\\) function output is squared. This helps achieving two things: Firstly, it makes the loss value more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add to the loss more significantly than smaller errors. Note that simiarly, this may also mean that you\u2019ll need to inspect your dataset for the presence of such outliers first. Secondly ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - <b>hinge loss</b> vs logistic loss advantages and ...", "url": "https://stats.stackexchange.com/questions/146277/hinge-loss-vs-logistic-loss-advantages-and-disadvantages-limitations", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/146277/<b>hinge-loss</b>-vs-logistic-loss...", "snippet": "<b>machine</b>-<b>learning</b> svm loss-functions computer-vision. Share. Cite. Improve this question. Follow edited Jul 23 &#39;18 at 15:41. DHW. 644 3 3 silver badges 13 13 bronze badges. asked Apr 14 &#39;15 at 11:18. user570593 user570593. 1,059 2 2 gold badges 12 12 silver badges 19 19 bronze badges $\\endgroup$ Add a comment | 3 Answers Active Oldest Votes. 31 $\\begingroup$ Logarithmic loss minimization leads to well-behaved probabilistic outputs. <b>Hinge loss</b> leads to some (not guaranteed) sparsity on the ...", "dateLastCrawled": "2022-01-26T09:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>A Course in Machine Learning</b> | AZERTY UIOP - Academia.edu", "url": "https://www.academia.edu/11902068/A_Course_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/11902068/<b>A_Course_in_Machine_Learning</b>", "snippet": "<b>A Course in Machine Learning</b>. \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset password. Enter the email address you signed up with and we&#39;ll email you a reset link. Need an account? Click here to sign up. Log In Sign ...", "dateLastCrawled": "2022-01-23T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Course in <b>Machine</b> <b>Learning</b>", "url": "http://ciml.info/dl/v0_8/ciml-v0_8-ch12.pdf", "isFamilyFriendly": true, "displayUrl": "ciml.info/dl/v0_8/ciml-v0_8-ch12.pdf", "snippet": "160 a course in <b>machine</b> <b>learning</b> fortunately, not only is the zero-norm non-convex, it\u2019s also discrete. Optimizing it is NP-hard. A reasonable middle-ground is the one-norm: jjwjj 1 = \u00e5 djw j. It is indeed convex: in fact, it is the tighest \u2018p norm that is convex. Moreover, its gradients do not go to zero as in the two-norm. <b>Just as hinge-loss</b> is the tightest convex upper bound on zero-one error, the one-norm is the tighest convex upper bound on the zero-norm. At this point, you should ...", "dateLastCrawled": "2021-09-07T20:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Course in <b>Machine</b> <b>Learning</b> | PDF | <b>Machine</b> <b>Learning</b> | Prediction", "url": "https://www.scribd.com/document/346469890/a-course-in-machine-learning-pdf", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/346469890/a-course-in-<b>machine</b>-<b>learning</b>-pdf", "snippet": "The <b>machine</b> <b>learning</b> algorithm has succeeded if its performance on the test data is high. 1.2 Some Canonical <b>Learning</b> Problems. There are a large number of typical inductive <b>learning</b> problems. The primary difference between them is in what type of thing theyre trying to predict. Here are some examples: Regression: trying to predict a real value. For instance, predict the value of a stock tomorrow given its past performance. Or predict Alices score on the <b>machine</b> <b>learning</b> final exam based on ...", "dateLastCrawled": "2021-12-06T00:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Course in <b>Machine</b> <b>Learning</b>", "url": "http://ciml.info/dl/v0_9/ciml-v0_9-ch12.pdf", "isFamilyFriendly": true, "displayUrl": "ciml.info/dl/v0_9/ciml-v0_9-ch12.pdf", "snippet": "162 a course in <b>machine</b> <b>learning</b> pect the algorithm to converge. Unfortunately, in comparisong to gradient descent, stochastic gradient is quite sensitive to the selection of a good <b>learning</b> rate. There is one more practical issues related to the use of SGD as a <b>learning</b> algorithm: do you really select a random point (or subset of random points) at each step, or do you stream through the data in order. The answer is akin to the answer of the same question for the perceptron algorithm ...", "dateLastCrawled": "2021-09-20T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "- <b>A Course in Machine Learning</b> - Studylib", "url": "https://studylib.net/doc/8792694/--a-course-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://studylib.net/doc/8792694/--<b>a-course-in-machine-learning</b>", "snippet": "Free essays, homework help, flashcards, research papers, book reports, term papers, history, science, politics", "dateLastCrawled": "2021-12-27T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Ciml <b>v0 - 8 All Machine Learning</b> | <b>Machine Learning</b> | Prediction", "url": "https://www.scribd.com/document/172987143/Ciml-v0-8-All-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/172987143/Ciml-<b>v0-8-All-Machine-Learning</b>", "snippet": "The <b>machine learning</b> algorithm has succeeded if its performance on the test data is high. 1.2 Some Canonical <b>Learning</b> Problems. There are a large number of typical inductive <b>learning</b> problems. The primary difference between them is in what type of thing theyre trying to predict. Here are some examples: Regression: trying to predict a real value. For instance, predict the value of a stock tomorrow given its past performance. Or predict Alices score on the <b>machine learning</b> nal exam based on ...", "dateLastCrawled": "2022-01-19T05:02:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(hinge loss)  is like +(the difference between what you know and what you can predict)", "+(hinge loss) is similar to +(the difference between what you know and what you can predict)", "+(hinge loss) can be thought of as +(the difference between what you know and what you can predict)", "+(hinge loss) can be compared to +(the difference between what you know and what you can predict)", "machine learning +(hinge loss AND analogy)", "machine learning +(\"hinge loss is like\")", "machine learning +(\"hinge loss is similar\")", "machine learning +(\"just as hinge loss\")", "machine learning +(\"hinge loss can be thought of as\")", "machine learning +(\"hinge loss can be compared to\")"]}