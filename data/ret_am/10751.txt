{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Reinforcement Learning Tutorial</b> - Javatpoint", "url": "https://www.javatpoint.com/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>reinforcement</b>-<b>learning</b>", "snippet": "It learns the <b>value</b> <b>function</b> Q (S, a), which means how good to take action &quot;a&quot; at a particular state &quot;s.&quot; The below flowchart explains the working of Q- <b>learning</b>: <b>State Action</b> <b>Reward</b> <b>State action</b> (SARSA): SARSA stands for <b>State Action</b> <b>Reward</b> <b>State action</b>, which is an on-policy temporal difference <b>learning</b> method. The on-policy control method ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Reinforcement</b> <b>Learning</b>: <b>Bellman</b> Equation and Optimality (Part 2) | by ...", "url": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement</b>-<b>learning</b>-markov-decision-process-part-2...", "snippet": "<b>Bellman</b> Expectation Equation for <b>State-Action</b> <b>Value</b> <b>Function</b> (Q-<b>Function</b>) Let\u2019s call this Equation 2.From the above equation, we can see that the <b>State-Action</b> <b>Value</b> of a state can be decomposed into the immediate <b>reward</b> we get on performing a certain action in state(s) and moving to another state(s\u2019) plus the discounted <b>value</b> of the <b>state-action</b> <b>value</b> of the state(s\u2019) with respect to the some action(a) our agent will take from that state on-wards.. Going Deeper into <b>Bellman</b> Expectation ...", "dateLastCrawled": "2022-02-02T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "3.7 <b>Value</b> Functions", "url": "http://incompleteideas.net/book/first/ebook/node34.html", "isFamilyFriendly": true, "displayUrl": "incompleteideas.net/book/first/ebook/node34.html", "snippet": "3.8 Optimal <b>Value</b> Functions Up: 3. The <b>Reinforcement</b> <b>Learning</b> Previous: 3.6 Markov Decision Processes Contents 3.7 <b>Value</b> Functions. Almost all <b>reinforcement</b> <b>learning</b> algorithms are based on estimating <b>value</b> functions--functions of states (or of <b>state-action</b> pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of &quot;how good&quot; here is defined in terms of future rewards that can be expected, or, to be ...", "dateLastCrawled": "2022-01-30T21:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Is <b>the reward</b> <b>function</b> the same as <b>the value function in reinforcement</b> ...", "url": "https://www.quora.com/Is-the-reward-function-the-same-as-the-value-function-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-<b>the-reward</b>-<b>function</b>-the-same-as-the-<b>value</b>-<b>function</b>-in...", "snippet": "Answer: No. Whereas <b>the reward</b> signal ([code ]R[/code], a distribution over all states [code ]S[/code]) indicates what is good in an immediate sense, a <b>value</b> <b>function</b> ([code ]V[/code], a distribution over all states [code ]S[/code]) specifies what is good in the long run.. Roughly speaking, the ...", "dateLastCrawled": "2022-01-16T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to make a <b>reward</b> <b>function</b> <b>in reinforcement learning</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/189067/how-to-make-a-reward-function-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/189067", "snippet": "Regarding &quot;<b>value</b> <b>function</b>&quot;, I usually reserve this term for state-<b>value</b> or action-<b>value</b> mappings, i.e. a <b>function</b> the agent uses to estimate estimate future <b>reward</b>. So &quot;<b>value</b>&quot; is related to &quot;<b>reward</b>&quot;, but <b>reward</b> is part of the problem, not the algorithm solving the problem. Perhaps the emphasis in AI has been on showing off your <b>learning</b> algorithm, by stipulating binary, distal, sparse rewards -- but if you have control over <b>the reward</b> <b>function</b>, life is easier if it&#39;s &quot;nice&quot;.", "dateLastCrawled": "2022-01-26T20:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Q-<b>learning</b>: a <b>value</b>-based <b>reinforcement</b> <b>learning</b> algorithm | by Dhanoop ...", "url": "https://medium.com/intro-to-artificial-intelligence/q-learning-a-value-based-reinforcement-learning-algorithm-272706d835cf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/q-<b>learning</b>-a-<b>value</b>-based...", "snippet": "As we discussed in the action-<b>value</b> <b>function</b>, the above equation indicates how we compute the Q-<b>value</b> for an action a starting from state s in Q <b>learning</b>. It is the sum of immediate <b>reward</b> using a ...", "dateLastCrawled": "2022-01-29T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Reinforcement Learning</b> and Q <b>learning</b> \u2014An example of the \u2018taxi problem ...", "url": "https://towardsdatascience.com/reinforcement-learning-and-q-learning-an-example-of-the-taxi-problem-in-python-d8fd258d6d45", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning</b>-and-q-<b>learning</b>-an-example-of-the...", "snippet": "Q <b>Learning</b>. Q <b>Learning</b> is a type of <b>Value</b>-based <b>learning</b> algorithms.The agent\u2019s objective is to optimize a \u201c<b>Value</b> <b>function</b>\u201d suited to the problem it faces. We have previously defined a <b>reward</b> <b>function</b> R(s,a), in Q <b>learning</b> we have a <b>value</b> <b>function</b> which is similar to <b>the reward</b> <b>function</b>, but it assess a particular action in a particular state for a given policy.It takes into account of all future rewards in resulting from taking that particular action, not just a current <b>reward</b>.", "dateLastCrawled": "2022-02-02T20:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Is a cost <b>function</b> the same as a <b>reward function in reinforcement learning</b>?", "url": "https://www.quora.com/Is-a-cost-function-the-same-as-a-reward-function-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-a-cost-<b>function</b>-the-same-as-a-<b>reward</b>-<b>function</b>-in...", "snippet": "Answer (1 of 2): The terms cost and <b>reward</b> are basically antonyms in this context. Phrasing the objective in terms of cost (to be minimized) is more common in the closely related field of optimal control, whereas <b>reinforcement</b> <b>learning</b> folks usually talk about <b>reward</b> (to be maximized). A <b>reward</b> ...", "dateLastCrawled": "2022-01-22T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The <b>Bellman Equation</b>. V-<b>function</b> and Q-<b>function</b> Explained | by Jordi ...", "url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-<b>bellman-equation</b>-59258a0d3fa7", "snippet": "Almost all <b>Reinforcement</b> <b>Learning</b> algorithms executed by the Agents involve estimating <b>value</b> functions \u2014 functions of states or of <b>state-action</b> pairs. These are the so-called <b>Value</b>-based Agents. A <b>value</b> <b>function</b> estimates how good it is for the Agent to be in a given state (or how good it is to perform a given action in a given state) in terms of return G .", "dateLastCrawled": "2022-02-03T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>reinforcement</b> <b>learning</b> - What are features for <b>state-action</b> pairs in RL ...", "url": "https://datascience.stackexchange.com/questions/47456/what-are-features-for-state-action-pairs-in-rl", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/47456", "snippet": "I started reading from p. 198 in Sutton&#39;s book for <b>Value</b> <b>Function</b> Approximation but also did not see examples for &quot;features of <b>state-action</b> pairs&quot; . My best guess is for example in Cartpole-V1 (discrete action space) would be to add one more number to the tuple describing the <b>state-action</b> pair, ie.", "dateLastCrawled": "2022-01-28T12:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Reinforcement</b> <b>Learning</b>: <b>Bellman</b> Equation and Optimality (Part 2) | by ...", "url": "https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement</b>-<b>learning</b>-markov-decision-process-part-2...", "snippet": "<b>Bellman</b> Expectation Equation for <b>State-Action</b> <b>Value</b> <b>Function</b> (Q-<b>Function</b>) Let\u2019s call this Equation 2.From the above equation, we can see that the <b>State-Action</b> <b>Value</b> of a state can be decomposed into the immediate <b>reward</b> we get on performing a certain action in state(s) and moving to another state(s\u2019) plus the discounted <b>value</b> of the <b>state-action</b> <b>value</b> of the state(s\u2019) with respect to the some action(a) our agent will take from that state on-wards.. Going Deeper into <b>Bellman</b> Expectation ...", "dateLastCrawled": "2022-02-02T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Reinforcement Learning Tutorial</b> - Javatpoint", "url": "https://www.javatpoint.com/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>reinforcement</b>-<b>learning</b>", "snippet": "It learns the <b>value</b> <b>function</b> Q (S, a), which means how good to take action &quot;a&quot; at a particular state &quot;s.&quot; The below flowchart explains the working of Q- <b>learning</b>: <b>State Action</b> <b>Reward</b> <b>State action</b> (SARSA): SARSA stands for <b>State Action</b> <b>Reward</b> <b>State action</b>, which is an on-policy temporal difference <b>learning</b> method. The on-policy control method ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "States, Actions, Rewards \u2014 The Intuition behind <b>Reinforcement Learning</b> ...", "url": "https://towardsdatascience.com/states-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/states-actions-<b>rewards</b>-the-intuition-behind...", "snippet": "<b>Reinforcement learning</b> is particularly opportune for such comparisons. At its core, any <b>reinforcement learning</b> task is defined by three things \u2014 states, actions and rewards. States are a representation of the current world or environment of the task. Actions are something an RL agent can do to change these states. And rewards are the utility the agent receives for performing the \u201cright\u201d actions. So the states tell the agent what situation it is in currently, and the rewards signal the ...", "dateLastCrawled": "2022-02-02T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fundamentals of Reinforcement Learning: Policies, Value Functions</b> ...", "url": "https://www.mlq.ai/reinforcement-learning-policies-value-functions-bellman-equation/", "isFamilyFriendly": true, "displayUrl": "https://www.mlq.ai/<b>reinforcement-learning-policies-value-functions</b>-bellman-equation", "snippet": "The <b>value</b> <b>function</b> is so useful <b>in reinforcement</b> <b>learning</b> as it&#39;s essentially a stand-in for the average of an infinite number of possible values. Action-<b>Value</b> Bellman Equation. The Bellman equation for the action-<b>value</b> <b>function</b> <b>is similar</b> in that it is a recursive equation for the <b>value</b> of a <b>state-action</b> pair of future possible pairs.", "dateLastCrawled": "2022-01-31T05:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Reinforcement Learning</b> and Q <b>learning</b> \u2014An example of the \u2018taxi problem ...", "url": "https://towardsdatascience.com/reinforcement-learning-and-q-learning-an-example-of-the-taxi-problem-in-python-d8fd258d6d45", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning</b>-and-q-<b>learning</b>-an-example-of-the...", "snippet": "Q <b>Learning</b>. Q <b>Learning</b> is a type of <b>Value</b>-based <b>learning</b> algorithms.The agent\u2019s objective is to optimize a \u201c<b>Value</b> <b>function</b>\u201d suited to the problem it faces. We have previously defined a <b>reward</b> <b>function</b> R(s,a), in Q <b>learning</b> we have a <b>value</b> <b>function</b> which <b>is similar</b> <b>to the reward</b> <b>function</b>, but it assess a particular action in a particular state for a given policy.It takes into account of all future rewards in resulting from taking that particular action, not just a current <b>reward</b>.", "dateLastCrawled": "2022-02-02T20:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>reinforcement</b> <b>learning</b> - Apart from the state and <b>state-action</b> <b>value</b> ...", "url": "https://ai.stackexchange.com/questions/10575/apart-from-the-state-and-state-action-value-functions-what-are-other-examples-o", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/10575/apart-from-the-state-and-<b>state-action</b>...", "snippet": "<b>Artificial Intelligence Stack Exchange</b> is a question and answer site for people interested in conceptual questions about life and challenges in a world where &quot;cognitive&quot; functions can be mimicked in purely digital environment.", "dateLastCrawled": "2022-01-07T08:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Model-free <b>reinforcement</b> <b>learning</b> \u2014 Introduction to <b>Reinforcement</b> <b>Learning</b>", "url": "https://gibberblot.github.io/rl-notes/single-agent/model-free.html", "isFamilyFriendly": true, "displayUrl": "https://gibberblot.github.io/rl-notes/single-agent/model-free.html", "snippet": "SARSA (<b>State-action</b>-<b>reward</b>-<b>state-action</b>) is an on-policy <b>reinforcement</b> <b>learning</b> algorithm. It is very <b>similar</b> to Q-<b>learning</b>, except that in its update rule, instead of estimate the future discount <b>reward</b> using \\(\\max{a \\in A(s)} Q(s&#39;,a)\\) , it actually selects the next action that it will execute, and updates using that instead.", "dateLastCrawled": "2022-01-30T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Is a cost <b>function</b> the same as a <b>reward function in reinforcement learning</b>?", "url": "https://www.quora.com/Is-a-cost-function-the-same-as-a-reward-function-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-a-cost-<b>function</b>-the-same-as-a-<b>reward</b>-<b>function</b>-in...", "snippet": "Answer (1 of 2): The terms cost and <b>reward</b> are basically antonyms in this context. Phrasing the objective in terms of cost (to be minimized) is more common in the closely related field of optimal control, whereas <b>reinforcement</b> <b>learning</b> folks usually talk about <b>reward</b> (to be maximized). A <b>reward</b> ...", "dateLastCrawled": "2022-01-22T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>reinforcement</b> <b>learning</b> - When to use the state <b>value</b> <b>function</b> $V(s ...", "url": "https://ai.stackexchange.com/questions/32464/when-to-use-the-state-value-function-vs-and-when-to-use-the-state-action-val", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/32464/when-to-use-the-state-<b>value</b>-<b>function</b>-vs...", "snippet": "This makes the action <b>value</b> <b>function</b> Q necessary for model-free <b>value</b>-based control methods. Hence Monte-Carlo Control, SARSA, Q-<b>Learning</b> will all use action values. Hence Monte-Carlo Control, SARSA, Q-<b>Learning</b> will all use action values.", "dateLastCrawled": "2022-01-14T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>reinforcement</b> <b>learning</b> - What are features for <b>state-action</b> pairs in RL ...", "url": "https://datascience.stackexchange.com/questions/47456/what-are-features-for-state-action-pairs-in-rl", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/47456", "snippet": "I started reading from p. 198 in Sutton&#39;s book for <b>Value</b> <b>Function</b> Approximation but also did not see examples for &quot;features of <b>state-action</b> pairs&quot; . My best guess is for example in Cartpole-V1 (discrete action space) would be to add one more number to the tuple describing the <b>state-action</b> pair, ie.", "dateLastCrawled": "2022-01-28T12:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Introduction to <b>Reinforcement</b> <b>Learning</b>", "url": "https://djrusso.github.io/RLCourse/slides/week7.pdf", "isFamilyFriendly": true, "displayUrl": "https://djrusso.github.io/RLCourse/slides/week7.pdf", "snippet": "<b>Learning</b> a <b>state-action</b> <b>value</b> <b>function</b> (Q-<b>function</b>) 3. Adapting the policy as data is collected (changes how future data is collected) ... functions&quot; Q(s;a), which <b>can</b> <b>be thought</b> of as the \\<b>value</b>&quot; of a <b>state-action</b> pair. This shift in focus is due to the fact that <b>reinforcement</b>-<b>learning</b> algorithms need to do more than simply evaluate a xed policy { they also need to control the data-collection process through the actions they take (this will be emphasized in the next section). However, as we ...", "dateLastCrawled": "2021-08-31T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>reinforcement</b> <b>learning</b> - What are features for <b>state-action</b> pairs in RL ...", "url": "https://datascience.stackexchange.com/questions/47456/what-are-features-for-state-action-pairs-in-rl", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/47456", "snippet": "I started reading from p. 198 in Sutton&#39;s book for <b>Value</b> <b>Function</b> Approximation but also did not see examples for &quot;features of <b>state-action</b> pairs&quot; . My best guess is for example in Cartpole-V1 (discrete action space) would be to add one more number to the tuple describing the <b>state-action</b> pair, ie.", "dateLastCrawled": "2022-01-28T12:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>3.7 Value Functions</b>", "url": "http://www.incompleteideas.net/book/ebook/node34.html", "isFamilyFriendly": true, "displayUrl": "www.incompleteideas.net/book/ebook/node34.html", "snippet": "3.8 Optimal <b>Value</b> Functions Up: 3. The <b>Reinforcement</b> <b>Learning</b> Previous: 3.6 Markov Decision Processes Contents <b>3.7 Value Functions</b>. Almost all <b>reinforcement</b> <b>learning</b> algorithms are based on estimating <b>value</b> functions--functions of states (or of <b>state-action</b> pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of &quot;how good&quot; here is defined in terms of future rewards that <b>can</b> be expected, or, to be ...", "dateLastCrawled": "2022-02-02T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What&#39;s the difference between <b>model</b>-free and <b>model-based</b> <b>reinforcement</b> ...", "url": "https://ai.stackexchange.com/questions/4456/whats-the-difference-between-model-free-and-model-based-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/4456", "snippet": "A <b>value</b> <b>function</b> <b>can</b> <b>be thought</b> of as a <b>function</b> which evaluates a state (or an action taken in a state), for all states. From this <b>value</b> <b>function</b>, a policy <b>can</b> then be derived. In practice, one way to distinguish between <b>model-based</b> or <b>model</b>-free algorithms is to look at the algorithms and see if they use the transition or <b>reward</b> <b>function</b>.", "dateLastCrawled": "2022-01-27T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Reinforcement</b> <b>Learning</b>: Introduction to Policy Gradients | by Cheng Xi ...", "url": "https://medium.com/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nerd-for-tech/<b>reinforcement</b>-<b>learning</b>-introduction-to-policy...", "snippet": "In (5), we <b>can</b> replace the <b>state-action</b> <b>value</b> <b>function</b> with G\u209c, the cumulative discounted <b>reward</b> at timestep t. Then, we <b>can</b> simplify the equation using the fact d/dx[ln(x)] = 1/x.", "dateLastCrawled": "2022-01-28T08:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What would a <b>state-action</b> <b>value</b> <b>function</b> learn if we placed multiple ...", "url": "https://www.quora.com/What-would-a-state-action-value-function-learn-if-we-placed-multiple-goals-on-the-state-space-and-moved-from-a-starting-point-to-a-goal-and-then-from-goal-to-goal-using-reinforcement-learning-with-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-would-a-<b>state-action</b>-<b>value</b>-<b>function</b>-learn-if-we-placed...", "snippet": "Answer (1 of 2): It depends on how you define the <b>reward</b> <b>function</b>, how far away the goal states are from each other, whether you are using discounting, whether goal states are absorbing, and what are the available actions (<b>can</b> you choose not to move?). If goals are not absorbing, for instance, th...", "dateLastCrawled": "2022-01-24T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "python - How to <b>implement the state value function</b>? - Stack Overflow", "url": "https://stackoverflow.com/questions/37803003/how-to-implement-the-state-value-function", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/37803003", "snippet": "I added some explanation, it is about a equation named <b>value</b> <b>function</b> <b>in reinforcement</b> <b>learning</b>, I <b>thought</b> it was a quite simple and basic conception, however the wrong output is telling me that I&#39;m wrong -,-\u2013", "dateLastCrawled": "2022-01-12T20:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is the <b>best Reward function in Reinforcement Learning</b>?", "url": "https://www.researchgate.net/post/What_is_the_best_Reward_function_in_Reinforcement_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/What_is_the_<b>best_Reward_function_in_Reinforcement</b>...", "snippet": "The goal is to drive at a desired speed without crashing into other cars. The state contains the velocities and positions of the agent&#39;s car and the surrounding cars. Rewards: -100 for crashing ...", "dateLastCrawled": "2022-01-29T10:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is a cost <b>function</b> the same as a <b>reward function in reinforcement learning</b>?", "url": "https://www.quora.com/Is-a-cost-function-the-same-as-a-reward-function-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-a-cost-<b>function</b>-the-same-as-a-<b>reward</b>-<b>function</b>-in...", "snippet": "Answer (1 of 2): The terms cost and <b>reward</b> are basically antonyms in this context. Phrasing the objective in terms of cost (to be minimized) is more common in the closely related field of optimal control, whereas <b>reinforcement</b> <b>learning</b> folks usually talk about <b>reward</b> (to be maximized). A <b>reward</b> ...", "dateLastCrawled": "2022-01-22T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Understanding the role of the <b>discount factor</b> <b>in reinforcement</b> <b>learning</b> ...", "url": "https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/221402", "snippet": "<b>Reinforcement</b> <b>learning</b> techniques <b>can</b> be used to solve MDPs. An MDP provides a mathematical framework for modeling decision-making situations where outcomes are partly random and partly under the control of the decision maker. An MDP is defined via a state space $\\mathcal{S}$, an action space $\\mathcal{A}$, a <b>function</b> of transition probabilities between states (conditioned to the action taken by the decision maker), and a <b>reward</b> <b>function</b>. In its basic setting, the decision maker takes and ...", "dateLastCrawled": "2022-01-24T21:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Is the <b>reward</b> <b>function</b> the same as <b>the value function in reinforcement</b> ...", "url": "https://www.quora.com/Is-the-reward-function-the-same-as-the-value-function-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-the-<b>reward</b>-<b>function</b>-the-same-as-the-<b>value</b>-<b>function</b>-in...", "snippet": "Answer: No. Whereas the <b>reward</b> signal ([code ]R[/code], a distribution over all states [code ]S[/code]) indicates what is good in an immediate sense, a <b>value</b> <b>function</b> ([code ]V[/code], a distribution over all states [code ]S[/code]) specifies what is good in the long run.. Roughly speaking, the ...", "dateLastCrawled": "2022-01-16T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Scaling <b>Reward</b> Values for Improved Deep <b>Reinforcement Learning</b> | by ...", "url": "https://medium.com/mindboard/scaling-reward-values-for-improved-deep-reinforcement-learning-e9a89f89411d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mindboard/scaling-<b>reward</b>-<b>values</b>-for-improved-deep-<b>reinforcement</b>...", "snippet": "Deep <b>Reinforcement Learning</b> involves using a neural network as a universal <b>function</b> approximator to learn a <b>value</b> <b>function</b> that maps <b>state-action</b> pairs to their expected future <b>reward</b> given a ...", "dateLastCrawled": "2022-02-02T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Reward</b> <b>function</b> and initial values : Better choices for accelerated ...", "url": "https://gjlaurent.github.io/assets/papers/Matignon2006reward.pdf", "isFamilyFriendly": true, "displayUrl": "https://gjlaurent.github.io/assets/papers/Matignon2006<b>reward</b>.pdf", "snippet": "expected immediate <b>reward</b> or <b>reinforcement</b> received under each transition. The goal is to learn a mapping from states to actions, called a policy, \u03c0. In this work, we have validated our analysis with Q-<b>learning</b> [2] algorithm. In Q-<b>learning</b>, an action-<b>value</b> <b>function</b> Q\u03c0(s,a) is estimated over the <b>learning</b> process and stored in a tabular representation. An action-<b>value</b> represents the expected sum of rewards 1 the agent expects to receive by executing the action a from state s and following ...", "dateLastCrawled": "2021-12-29T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How to make a <b>reward</b> <b>function</b> <b>in reinforcement learning</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/189067/how-to-make-a-reward-function-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/189067", "snippet": "$\\begingroup$ I think this answer mixes up <b>reward</b> and <b>value</b> functions. For instance it talks about &quot;finding&quot; a <b>reward</b> <b>function</b>, which might be something you do in inverse <b>reinforcement learning</b>, but not in RL used for control. Also, it talks about the need for <b>reward</b> <b>function</b> to be continuous and differentiable, and that is not only not required, it usually is not the case. You are far more likely to find simple +1 for success, or fixed -1 per time step taken in the literature, than to find ...", "dateLastCrawled": "2022-01-26T20:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Is a cost <b>function</b> the same as a <b>reward function in reinforcement learning</b>?", "url": "https://www.quora.com/Is-a-cost-function-the-same-as-a-reward-function-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-a-cost-<b>function</b>-the-same-as-a-<b>reward</b>-<b>function</b>-in...", "snippet": "Answer (1 of 2): The terms cost and <b>reward</b> are basically antonyms in this context. Phrasing the objective in terms of cost (to be minimized) is more common in the closely related field of optimal control, whereas <b>reinforcement</b> <b>learning</b> folks usually talk about <b>reward</b> (to be maximized). A <b>reward</b> ...", "dateLastCrawled": "2022-01-22T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Reinforcement</b> <b>Learning</b> \u2014 The <b>Value</b> <b>Function</b> | by Jingles (Hong Jing ...", "url": "https://towardsdatascience.com/reinforcement-learning-value-function-57b04e911152", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement</b>-<b>learning</b>-<b>value</b>-<b>function</b>-57b04e911152", "snippet": "Fig 5: Update the <b>value</b> of state s. State s\u2019 is the next state of the current state s.We <b>can</b> update the <b>value</b> of the current state s by adding the differences in <b>value</b> between state s and s\u2019. \u03b1 is the <b>learning</b> rate.. As multiple actions <b>can</b> be taken at any given state, so constantly picking only one action at a state that used to bring success might end up missing other better states to be in.", "dateLastCrawled": "2022-02-03T19:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Q-<b>learning</b>: a <b>value</b>-based <b>reinforcement</b> <b>learning</b> algorithm | by Dhanoop ...", "url": "https://medium.com/intro-to-artificial-intelligence/q-learning-a-value-based-reinforcement-learning-algorithm-272706d835cf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/q-<b>learning</b>-a-<b>value</b>-based...", "snippet": "As we discussed in the action-<b>value</b> <b>function</b>, the above equation indicates how we compute the Q-<b>value</b> for an action a starting from state s in Q <b>learning</b>. It is the sum of immediate <b>reward</b> using a ...", "dateLastCrawled": "2022-01-29T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The <b>Bellman Equation</b>. V-<b>function</b> and Q-<b>function</b> Explained | by Jordi ...", "url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-<b>bellman-equation</b>-59258a0d3fa7", "snippet": "Almost all <b>Reinforcement</b> <b>Learning</b> algorithms executed by the Agents involve estimating <b>value</b> functions \u2014 functions of states or of <b>state-action</b> pairs. These are the so-called <b>Value</b>-based Agents. A <b>value</b> <b>function</b> estimates how good it is for the Agent to be in a given state (or how good it is to perform a given action in a given state) in terms of return G .", "dateLastCrawled": "2022-02-03T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>reinforcement</b> <b>learning</b> - Why is the <b>state-action</b> <b>value</b> <b>function</b> ...", "url": "https://stats.stackexchange.com/questions/90452/why-is-the-state-action-value-function-required-when-the-model-is-unknown", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/90452/why-is-the-<b>state-action</b>-<b>value</b>-<b>function</b>...", "snippet": "Stack Exchange network consists of 178 Q&amp;A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.. Visit Stack Exchange", "dateLastCrawled": "2022-01-11T05:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep <b>learning</b> - What&#39;s the principle to design the <b>reward</b> <b>function</b>, of ...", "url": "https://stackoverflow.com/questions/63233562/whats-the-principle-to-design-the-reward-function-of-dqn", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/63233562", "snippet": "I&#39;m designing a <b>reward</b> <b>function</b> of a DQN model, the most tricky part of Deep <b>reinforcement</b> <b>learning</b> part. I referred several cases, and noticed usually the <b>reward</b> will set in [-1, 1]. Considering if the negative <b>reward</b> is triggered less times, more &quot;sparse&quot; <b>compared</b> with positive <b>reward</b>, the positive <b>reward</b> could be lower than 1.", "dateLastCrawled": "2022-01-25T07:56:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Relationship between state (V) and action(Q) <b>value</b> <b>function</b> in ...", "url": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and-action-q-value-function-in-reinforcement-learning-bb9a988c0127", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and...", "snippet": "<b>Value</b> <b>function</b> can be defined as the expected <b>value</b> of an agent in a certain state. There are two types of <b>value</b> functions in RL: State-<b>value</b> and action-<b>value</b>. It is important to understand the\u2026", "dateLastCrawled": "2022-02-03T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Value</b>. State-<b>value</b> <b>function</b> v_\u03c0: gives us the <b>value</b> of a state under \u03c0; Action-<b>value</b> <b>function</b> q_\u03c0: gives us the <b>value</b> of an action under \u03c0. q_\u03c0 is referred to as the Q-<b>function</b>, and the output from the <b>function</b> for any given <b>state-action</b> pair is called a Q-<b>value</b>.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Teaching machines to behave: Reinforcement <b>Learning</b> | by Diego Gomez ...", "url": "https://towardsdatascience.com/rl-4-all-1-3edac941fe37", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/rl-4-all-1-3edac941fe37", "snippet": "Notice that, as in all <b>machine</b> <b>learning</b> sub-fields, RL programs are not coded explicitly to perform optimally in some given task. ... Similarly, a* is the optimal <b>state-action</b>-<b>value</b> <b>function</b>, obtained if followed an optimal policy \u03c0*. Optimal <b>value</b> functions: Obtained when following a policy \u03c0 that maximizes v(s) and a(s,a) Assuming that the optimal <b>value</b> functions v* and a* are known, but the optimal policy is not, it is possible to build an optimal policy in the following ways: When v*(s ...", "dateLastCrawled": "2022-01-08T13:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Value</b>-<b>Function</b>-<b>Based Transfer for Reinforcement Learning</b> Using ...", "url": "https://www.researchgate.net/publication/221604435_Value-Function-Based_Transfer_for_Reinforcement_Learning_Using_Structure_Mapping", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221604435_<b>Value</b>-<b>Function</b>-Based_Transfer_for...", "snippet": "chological and computational theory about <b>analogy</b> making, ... the form of a <b>state-action</b> <b>value</b> <b>function</b>, or a q-<b>functio n</b>. A. q-<b>function</b> q: S \u00d7 A 7\u2192 R maps from <b>state-action</b> pairs to. real ...", "dateLastCrawled": "2021-10-16T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning for biochemical engineering: A</b> review - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1369703X21001303", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1369703X21001303", "snippet": "<b>Value</b>-based algorithms, typically represented by Q-<b>learning</b>, explicitly learn and optimise the <b>state-action</b> <b>value</b> <b>function</b> and generate the optimal policy by acting greedily with respect to it i.e. choosing the control corresponding to the maximum Q \u03c0 x, u <b>value</b> (<b>state-action</b> <b>value</b>). There are also hybrid algorithms, such as actor-critic methods, which combine policy optimisation methods and <b>value</b>-based methods. Although RL has shown success in game-based control benchmarks, such as AlphaGo", "dateLastCrawled": "2022-01-26T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "AI and Reinforcement <b>Learning</b> \u2014 Machines that Learn through Experience ...", "url": "https://www.cantorsparadise.com/ai-and-reinforcement-learning-machines-that-learn-through-experience-e7eea7bb6765", "isFamilyFriendly": true, "displayUrl": "https://www.cantorsparadise.com/ai-and-reinforcement-<b>learning</b>-<b>machines</b>-that-learn...", "snippet": "To align the policy with the updated <b>value</b> <b>function</b>, the algorithm modifies the policy so it would greedily follow the <b>value</b> <b>function</b> (meaning, choosing to perform actions that has the highest <b>value</b>). The algorithm continues by generating a new episode, now under the improved policy, which, in turn, derives a more accurate <b>value</b> estimation and so on. In this process, both the policy and the <b>value</b> <b>function</b> converge to their optimal values, until sufficient accuracy is reached, or when no more ...", "dateLastCrawled": "2022-01-25T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning Analogy for Meditation (illustrated</b>) - LessWrong 2.0 ...", "url": "https://www.greaterwrong.com/posts/chHhuCvmZqYLM32gz/machine-learning-analogy-for-meditation-illustrated", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/chHhuCvmZqYLM32gz/<b>machine</b>-<b>learning</b>-<b>analogy</b>-for...", "snippet": "<b>Machine Learning Analogy for Meditation (illustrated</b>) ... and the algorithm we use includes a <b>value</b> table: [picture: table, actions on x-axis, states on y-axis, cells of table are estimated values of taking actions in states] A <b>value</b> isn\u2019t just the learned estimate of the immediate reward which you get by taking an action in a state, but rather, the estimate of the eventual rewards, in total, from that action. This makes the values difficult to estimate. An estimate is improved by <b>value</b> it", "dateLastCrawled": "2022-01-17T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Reinforcement Learning: Prediction, Control and</b> <b>Value</b> <b>Function</b> ...", "url": "https://deepai.org/publication/reinforcement-learning-prediction-control-and-value-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>reinforcement-learning-prediction-control-and</b>-<b>value</b>...", "snippet": "<b>Reinforcement Learning: Prediction, Control and Value Function Approximation</b>. With the increasing power of computers and the rapid development of self-<b>learning</b> methodologies such as <b>machine</b> <b>learning</b> and artificial intelligence, the problem of constructing an automatic Financial Trading Systems (FTFs) becomes an increasingly attractive research ...", "dateLastCrawled": "2022-01-16T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>value</b> of a <b>function</b>?", "url": "https://psichologyanswers.com/library/lecture/read/57841-what-is-value-of-a-function", "isFamilyFriendly": true, "displayUrl": "https://psichologyanswers.com/library/lecture/read/57841-what-is-<b>value</b>-of-a-<b>function</b>", "snippet": "What is a <b>value</b> <b>function</b> reinforcement <b>learning</b>? <b>Value</b> <b>function</b> Many reinforcement <b>learning</b> introduce the notion of `<b>value</b>-<b>function</b>` which often denoted as V(s) . The <b>value</b> <b>function</b> represent how good is a state for an agent to be in. It is equal to expected total reward for an agent starting from state s . What is optimal <b>value</b> <b>function</b>? The optimal <b>Value</b> <b>function</b> is one which yields maximum <b>value</b> compared to all other <b>value</b> <b>function</b>. When we say we are solving an MDP it actually means we ...", "dateLastCrawled": "2022-01-15T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>SARSA</b> vs Q - <b>learning</b>", "url": "https://tcnguyen.github.io/reinforcement_learning/sarsa_vs_q_learning.html", "isFamilyFriendly": true, "displayUrl": "https://tcnguyen.github.io/reinforcement_<b>learning</b>/<b>sarsa</b>_vs_q_<b>learning</b>.html", "snippet": "<b>SARSA</b> will learn the optimal $\\epsilon$-greedy policy, i.e, the Q-<b>value</b> <b>function</b> will converge to a optimal Q-<b>value</b> <b>function</b> but in the space of $\\epsilon$-greedy policy only (as long as each <b>state action</b> pair will be visited infinitely). We expect that in the limit of $\\epsilon$ decaying to $0$, <b>SARSA</b> will converge to the overall optimal policy. I quote here a paragraph from", "dateLastCrawled": "2022-01-30T04:17:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(state-action value function)  is like +(the reward function in reinforcement learning)", "+(state-action value function) is similar to +(the reward function in reinforcement learning)", "+(state-action value function) can be thought of as +(the reward function in reinforcement learning)", "+(state-action value function) can be compared to +(the reward function in reinforcement learning)", "machine learning +(state-action value function AND analogy)", "machine learning +(\"state-action value function is like\")", "machine learning +(\"state-action value function is similar\")", "machine learning +(\"just as state-action value function\")", "machine learning +(\"state-action value function can be thought of as\")", "machine learning +(\"state-action value function can be compared to\")"]}