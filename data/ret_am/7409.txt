{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian</b> vs <b>Neural</b> <b>Networks</b> \u2013 Ehud Reiter&#39;s Blog", "url": "https://ehudreiter.com/2021/07/05/bayesian-vs-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://ehudreiter.com/2021/07/05/<b>bayesian</b>-vs-<b>neural</b>-<b>networks</b>", "snippet": "<b>other</b> <b>Bayesian</b> vs <b>Neural</b> <b>Networks</b>. Jul 5, 2021 Jul 5, 2021 ehudreiter. I\u2019m working and collaborating with several people who are interesting in explaining <b>Bayesian</b> reasoning, especially <b>Bayesian</b> <b>networks</b> (see below). We have a small group that meets monthly to discuss this, and last week we ended up also talking about why people would use <b>Bayesian</b> models instead of <b>neural</b> <b>networks</b>, focusing on medical decision support (not NLP). Which I found very interesting and thought-provoking. One ...", "dateLastCrawled": "2022-02-01T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Explaining <b>Bayesian</b> <b>Neural</b> <b>Networks</b> | DeepAI", "url": "https://deepai.org/publication/explaining-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/explaining-<b>bayesian</b>-<b>neural</b>-<b>networks</b>", "snippet": "<b>Bayesian</b> approaches such as <b>Bayesian</b> <b>Neural</b> <b>Networks</b> (BNNs) so far have a limited form of transparency (model transparency) already built-in through their prior weight distribution, but notably, they lack explanations of their predictions for given instances. In this work, we bring together these two perspectives of transparency into a holistic explanation framework for explaining BNNs. Within the <b>Bayesian</b> framework, the <b>network</b> weights follow a probability distribution. Hence, the standard ...", "dateLastCrawled": "2022-01-26T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "[2104.14421] What Are <b>Bayesian</b> <b>Neural</b> <b>Network</b> Posteriors Really <b>Like</b>?", "url": "https://arxiv.org/abs/2104.14421", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2104.14421", "snippet": "The posterior over <b>Bayesian</b> <b>neural</b> <b>network</b> (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in <b>Bayesian</b> deep learning, we instead use full-batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve ...", "dateLastCrawled": "2021-12-15T11:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bayesian</b> <b>Neural</b> <b>Networks</b>: 3 <b>Bayesian</b> CNN | by Adam Woolf | Towards Data ...", "url": "https://towardsdatascience.com/bayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bayesian</b>-<b>neural</b>-<b>networks</b>-3-<b>bayesian</b>-cnn-6ecd842eeff3", "snippet": "Being able to perform very well with considerably less data, while throwing in an ability to generalise better makes <b>Bayesian</b> <b>neural</b> <b>networks</b> desirable. And that\u2019s without considering the <b>other</b> advantages. There is however one disadvantage to a <b>Bayesian</b> implementations that become important in this chapter. <b>Bayesian</b> implementations need more parameters. Considering that a whole distribution replaces every weight value parameter it\u2019s surprising that only twice as many parameters are ...", "dateLastCrawled": "2022-01-30T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bayesian</b> <b>Neural</b> <b>Networks</b>: 2 Fully Connected in TensorFlow and Pytorch ...", "url": "https://towardsdatascience.com/bayesian-neural-networks-2-fully-connected-in-tensorflow-and-pytorch-7bf65fb4697", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bayesian</b>-<b>neural</b>-<b>networks</b>-2-fully-connected-in-tensor...", "snippet": "Learn how to convert a normal fully connected (dense) <b>neural network</b> to a <b>Bayesian</b> <b>neural network</b>; Appreciate the advantages and shortcomings of the current implementation; The data is from a n experiment in egg boiling. The boil durations are provided along with the egg\u2019s weight in grams and the finding on cutting it open. Findings are categorised into one of three classes: under cooked, soft-boiled and hard-boiled. We want the egg\u2019s outcome from its weight and boiling time. The problem ...", "dateLastCrawled": "2022-01-26T15:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is the relationship between <b>bayesian</b> and <b>neural</b> <b>networks</b>? - Stack ...", "url": "https://stackoverflow.com/questions/305279/what-is-the-relationship-between-bayesian-and-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/305279", "snippet": "At a glance, <b>bayesian</b> <b>networks</b> look at bit <b>like</b> a specific type of <b>neural</b> <b>networks</b>. Can anyone sum up their relationship, and if there is any connection beyond the apparent similarity? <b>neural-network</b> <b>bayesian</b>-<b>networks</b>. Share. Follow edited Jul 19 &#39;09 at 5:22. bias. 1,527 3 3 gold badges 19 19 silver badges 35 35 bronze badges. asked Nov 20 &#39;08 at 13:24. Morten Christiansen Morten Christiansen. 18k 19 19 gold badges 63 63 silver badges 92 92 bronze badges. 2. 1. should migrate to stats ...", "dateLastCrawled": "2022-01-15T19:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bayesian Neural Network</b> Series Post 1: Need for <b>Bayesian</b> <b>Networks</b> - Medium", "url": "https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/<b>neural</b>space/<b>bayesian-neural-network</b>-series-post-1-need-for-<b>bayesian</b>...", "snippet": "<b>Bayesian neural</b> <b>networks</b>, on the <b>other</b> hand, are more robust to over-fitting, and can easily learn from small datasets. The <b>Bayesian</b> approach further offers uncertainty estimates via its ...", "dateLastCrawled": "2022-01-31T18:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[D] Dropout for <b>Bayesian</b> <b>Neural</b> <b>Networks</b> and Alternatives ...", "url": "https://www.reddit.com/r/MachineLearning/comments/j81jmt/d_dropout_for_bayesian_neural_networks_and/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/j81jmt/d_dropout_for_<b>bayesian</b>_<b>neural</b>_<b>networks</b>_and", "snippet": "Unlike traditional <b>neural</b> <b>networks</b> where each layer of nodes is connected to every node in the next layer, a graph <b>neural</b> <b>network</b> has a graph-<b>like</b> structure. With this model, they managed to simulate a wide range of materials including sand, water, goop, and rigid solids. Instead of predicting the positions of particles, the model predicts the accelerations, and the velocities and positions are computed using an Euler integration. The simulation data were generated using a range of physics ...", "dateLastCrawled": "2021-09-09T16:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An Example of a <b>Bayesian</b> <b>Neural</b> <b>Network</b> Using PyTorch | James D. McCaffrey", "url": "https://jamesmccaffrey.wordpress.com/2021/08/30/an-example-of-a-bayesian-neural-network-using-pytorch/", "isFamilyFriendly": true, "displayUrl": "https://jamesmccaffrey.wordpress.com/2021/08/30/an-example-of-a-<b>bayesian</b>-<b>neural</b>...", "snippet": "The most common approach for creating a <b>Bayesian</b> <b>neural</b> <b>network</b> is to use a standard <b>neural</b> library, such as PyTorch or Keras, plus a <b>Bayesian</b> library such as Pyro. These <b>Bayesian</b> libraries are complex and have a steep learning curve. I recently stumbled across a lightweight <b>Bayesian</b> <b>network</b> library for PyTorch that allowed me to explore <b>Bayesian</b> <b>neural</b> <b>networks</b>. The library was created by a single guy, \u201cHarry24k\u201d, and is very, very impressive. The library is called torchbnn and was at:", "dateLastCrawled": "2022-02-02T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - Difference between Bayes <b>network</b>, <b>neural</b> <b>network</b> ...", "url": "https://stats.stackexchange.com/questions/94511/difference-between-bayes-network-neural-network-decision-tree-and-petri-nets", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/94511", "snippet": "The <b>Bayesian</b> <b>network</b> is different from the <b>Neural</b> <b>Network</b> in that it is explicit reasoning, even though probabilistic and hence could have multiple stable states based on each step being revisited and modified within legal values, just <b>like</b> an algorithm. It is a robust way to reason probabilistically, but it involves encoding of probabilities, conjecturing the points where randomized actions can happen and hence need more heuristic effort to build.", "dateLastCrawled": "2022-01-28T22:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian</b> vs <b>Neural</b> <b>Networks</b> \u2013 Ehud Reiter&#39;s Blog", "url": "https://ehudreiter.com/2021/07/05/bayesian-vs-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://ehudreiter.com/2021/07/05/<b>bayesian</b>-vs-<b>neural</b>-<b>networks</b>", "snippet": "<b>other</b> <b>Bayesian</b> vs <b>Neural</b> <b>Networks</b>. Jul 5, 2021 Jul 5, 2021 ehudreiter. I\u2019m working and collaborating with several people who are interesting in explaining <b>Bayesian</b> reasoning, especially <b>Bayesian</b> <b>networks</b> (see below). We have a small group that meets monthly to discuss this, and last week we ended up also talking about why people would use <b>Bayesian</b> models instead of <b>neural</b> <b>networks</b>, focusing on medical decision support (not NLP). Which I found very interesting and thought-provoking. One ...", "dateLastCrawled": "2022-02-01T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Explaining <b>Bayesian</b> <b>Neural</b> <b>Networks</b> | DeepAI", "url": "https://deepai.org/publication/explaining-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/explaining-<b>bayesian</b>-<b>neural</b>-<b>networks</b>", "snippet": "<b>Bayesian</b> approaches such as <b>Bayesian</b> <b>Neural</b> <b>Networks</b> (BNNs) so far have a limited form of transparency (model transparency) already built-in through their prior weight distribution, but notably, they lack explanations of their predictions for given instances. In this work, we bring together these two perspectives of transparency into a holistic explanation framework for explaining BNNs. Within the <b>Bayesian</b> framework, the <b>network</b> weights follow a probability distribution. Hence, the standard ...", "dateLastCrawled": "2022-01-26T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bayesian Neural</b> <b>Networks</b>: 1 Why Bother? | by Adam Woolf | Towards Data ...", "url": "https://towardsdatascience.com/bayesian-neural-networks-1-why-bother-b585375b38ec", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bayesian-neural</b>-<b>networks</b>-1-why-b<b>other</b>-b585375b38ec", "snippet": "But if we\u2019ve trained the <b>Bayesian Neural</b> <b>network</b> well our predictions will be very <b>similar</b> (for continuous, floating point, regression outputs) and the same for (categorical outputs.). And if we train our <b>neural</b> <b>network</b> badly we\u2019ll get very different results each time. If the <b>Bayesian</b> <b>network</b> could speak it would say the difference between it\u2019s predictions being <b>similar</b> or its predictions being very different is its uncertainty. Think of uncertainty as confidence (only <b>Bayesian neural</b> ...", "dateLastCrawled": "2022-01-26T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bayesian</b> <b>Neural</b> <b>Networks</b>: 2 Fully Connected in TensorFlow and Pytorch ...", "url": "https://towardsdatascience.com/bayesian-neural-networks-2-fully-connected-in-tensorflow-and-pytorch-7bf65fb4697", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bayesian</b>-<b>neural</b>-<b>networks</b>-2-fully-connected-in-tensor...", "snippet": "Learn how to convert a normal fully connected (dense) <b>neural network</b> to a <b>Bayesian</b> <b>neural network</b>; Appreciate the advantages and shortcomings of the current implementation; The data is from a n experiment in egg boiling. The boil durations are provided along with the egg\u2019s weight in grams and the finding on cutting it open. Findings are categorised into one of three classes: under cooked, soft-boiled and hard-boiled. We want the egg\u2019s outcome from its weight and boiling time. The problem ...", "dateLastCrawled": "2022-01-26T15:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Deep Bayesian Neural Networks</b>.. ways and whys. | by Stefano Cosentino ...", "url": "https://stefano-cosentino.medium.com/deep-bayesian-neural-networks-952763a9537", "isFamilyFriendly": true, "displayUrl": "https://stefano-cosentino.medium.com/<b>deep-bayesian-neural-networks</b>-952763a9537", "snippet": "A conventional <b>network</b> might over-confidently misjudge the position of the flying-car, and do the wrong thing, while a <b>Bayesian</b> <b>network</b> has access to its uncertainty and would suggest to slow down. An real-case example of conventional Vs <b>Bayesian</b> approach in <b>neural</b> <b>networks</b> with <b>similar</b> architectures (from some of my previous working building phoneme classifiers).", "dateLastCrawled": "2022-02-01T07:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is the <b>difference between a Bayesian network and Bayesian neural</b> ...", "url": "https://www.researchgate.net/post/What_is_the_difference_between_a_Bayesian_network_and_Bayesian_neural_network", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/What_is_the_<b>difference_between_a_Bayesian_network</b>...", "snippet": "<b>Bayesian</b> <b>neural</b> <b>networks</b> marginalize over the distribution of parameters in order to make predictions. So the <b>Bayesian</b> approach allows different models to be compared (e.g. no of hidden units).", "dateLastCrawled": "2022-02-02T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[2104.14421] What Are <b>Bayesian</b> <b>Neural</b> <b>Network</b> Posteriors Really Like?", "url": "https://arxiv.org/abs/2104.14421", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2104.14421", "snippet": "The posterior over <b>Bayesian</b> <b>neural</b> <b>network</b> (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in <b>Bayesian</b> deep learning, we instead use full-batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve ...", "dateLastCrawled": "2021-12-15T11:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What Are <b>Bayesian</b> <b>Neural</b> <b>Network</b> Posteriors Really Like?", "url": "http://proceedings.mlr.press/v139/izmailov21a/izmailov21a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v139/izmailov21a/izmailov21a.pdf", "snippet": "and tend to focus on relatively small datasets and <b>networks</b>. We on the <b>other</b> hand experiment with practical architectures and datasets and use up to 105 leapfrog steps per iteration to ensure good mixing. Our work is aimed at understanding the properties of true <b>Bayesian</b> <b>neural</b> <b>networks</b>. In a <b>similar</b> direction,Hron et al.", "dateLastCrawled": "2022-01-29T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] Dropout for <b>Bayesian</b> <b>Neural</b> <b>Networks</b> and Alternatives ...", "url": "https://www.reddit.com/r/MachineLearning/comments/j81jmt/d_dropout_for_bayesian_neural_networks_and/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/j81jmt/d_dropout_for_<b>bayesian</b>_<b>neural</b>_<b>networks</b>_and", "snippet": "Unlike traditional <b>neural</b> <b>networks</b> where each layer of nodes is connected to every node in the next layer, a graph <b>neural</b> <b>network</b> has a graph-like structure. With this model, they managed to simulate a wide range of materials including sand, water, goop, and rigid solids. Instead of predicting the positions of particles, the model predicts the accelerations, and the velocities and positions are computed using an Euler integration. The simulation data were generated using a range of physics ...", "dateLastCrawled": "2021-09-09T16:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "TensorBNN: <b>Bayesian</b> inference for <b>neural</b> <b>networks</b> using TensorFlow ...", "url": "https://www.sciencedirect.com/science/article/pii/S0010465521002800", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0010465521002800", "snippet": "An interesting approximation to the posterior density of <b>neural</b> <b>network</b> parameters, and therefore to <b>Bayesian</b> <b>neural</b> <b>networks</b>, is available through the DenseFlipout and DenseVariational layers of TFP. This method approximates a <b>Bayesian</b> <b>neural</b> <b>network</b> using a combination of variational inference and sampling. A forward pass through the <b>network</b> samples the <b>network</b> parameters from a variational density, a diagonal multivariate Gaussian whose means and standard deviations are optimized during ...", "dateLastCrawled": "2022-01-22T22:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian</b> <b>Neural Network</b> Series Post 2: Background Knowledge | by Kumar ...", "url": "https://medium.com/neuralspace/bayesian-neural-network-series-post-2-background-knowledge-fdec6ac62d43", "isFamilyFriendly": true, "displayUrl": "https://medium.com/<b>neural</b>space/<b>bayesian</b>-<b>neural-network</b>-series-post-2-background...", "snippet": "Let\u2019s start this post by breaking <b>Bayesian</b> <b>Neural</b> <b>Networks</b> into <b>Bayesian</b> ... <b>N eural Network</b>, on the <b>other</b> hand, <b>can</b> <b>be thought</b> of an end to end systems or set of algorithms that mimic the human ...", "dateLastCrawled": "2022-01-30T12:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Neural</b> <b>Networks</b> from a <b>Bayesian</b> Perspective * Machine Learning", "url": "https://machinelearningmastery.in/2021/11/03/neural-networks-from-a-bayesian-perspective/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.in/2021/11/03/<b>neural</b>-<b>networks</b>-from-a-<b>bayesian</b>-perspective", "snippet": "<b>Neural</b> <b>networks</b> from a <b>Bayesian</b> perspective A <b>neural</b> <b>network</b>\u2019s goal is to estimate the likelihood p(y|x,w). This is true even when you\u2019re not explicitly doing that, e.g. when you minimize MSE. To find the best model weights we <b>can</b> use Maximum Likelihood Estimation (MLE): Alternatively, we <b>can</b> use our prior knowledge, represented as a prior distribution over the weights, and maximize the posterior distribution. This approach is called Maximum Aposteriori Estimation (MAP): The term logP(w ...", "dateLastCrawled": "2022-01-22T23:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bayesian</b> vs <b>Neural</b> <b>Networks</b> \u2013 Ehud Reiter&#39;s Blog", "url": "https://ehudreiter.com/2021/07/05/bayesian-vs-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://ehudreiter.com/2021/07/05/<b>bayesian</b>-vs-<b>neural</b>-<b>networks</b>", "snippet": "To put this another way, using a <b>Bayesian</b> <b>network</b> to identify images of cats is stupid; <b>neural</b> models are much better in contexts where there is lots of training data, no gold-standard \u201cclinical trial\u201d knowledge, no need to justify results, and where today\u2019s model <b>can</b> be expected to work just as well in a month\u2019s time. But in fast-changing medical situations such as Covid, there are real drawbacks to <b>neural</b> models, and alternatives need to be considered.", "dateLastCrawled": "2022-02-01T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Are <b>Bayesian</b> <b>neural</b> <b>networks</b> intrinsically good at out-of-distribution ...", "url": "https://deepai.org/publication/are-bayesian-neural-networks-intrinsically-good-at-out-of-distribution-detection", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/are-<b>bayesian</b>-<b>neural</b>-<b>networks</b>-intrinsically-good-at-out...", "snippet": "The need to avoid confident predictions on unfamiliar data has sparked interest in out-of-distribution (OOD) detection. It is widely assumed that <b>Bayesian</b> <b>neural</b> <b>networks</b> (BNN) are well suited for this task, as the endowed epistemic uncertainty should lead to disagreement in predictions on outliers.In this paper, we question this assumption and provide empirical evidence that proper <b>Bayesian</b> inference with common <b>neural</b> <b>network</b> architectures does not necessarily lead to good OOD detection ...", "dateLastCrawled": "2022-01-24T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bayesian</b> <b>Neural</b> <b>Networks</b>: 2 Fully Connected in TensorFlow and Pytorch ...", "url": "https://towardsdatascience.com/bayesian-neural-networks-2-fully-connected-in-tensorflow-and-pytorch-7bf65fb4697", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bayesian</b>-<b>neural</b>-<b>networks</b>-2-fully-connected-in-tensor...", "snippet": "Learn how to convert a normal fully connected (dense) <b>neural network</b> to a <b>Bayesian</b> <b>neural network</b>; Appreciate the advantages and shortcomings of the current implementation; The data is from a n experiment in egg boiling. The boil durations are provided along with the egg\u2019s weight in grams and the finding on cutting it open. Findings are categorised into one of three classes: under cooked, soft-boiled and hard-boiled. We want the egg\u2019s outcome from its weight and boiling time. The problem ...", "dateLastCrawled": "2022-01-26T15:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bayesian</b> <b>Neural</b> <b>Networks</b>: 3 <b>Bayesian</b> CNN | by Adam Woolf | Towards Data ...", "url": "https://towardsdatascience.com/bayesian-neural-networks-3-bayesian-cnn-6ecd842eeff3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bayesian</b>-<b>neural</b>-<b>networks</b>-3-<b>bayesian</b>-cnn-6ecd842eeff3", "snippet": "Understand how parameter problems of <b>Bayesian</b> <b>neural</b> <b>networks</b> influence training; As we\u2019ve discovered in earlier articles, <b>Bayesian</b> analysis deals in distributions and not single values. We\u2019ve seen it with normal distributions where we\u2019re getting a continuous floating point back with the most likely return value of the mean. The distribution for a categorical becomes the discreet (piano key rather than violin string). For probabilities we\u2019ll get a specific result such as a class, an ...", "dateLastCrawled": "2022-01-30T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Dynamic <b>Bayesian</b> <b>Neural</b> <b>Networks</b> | DeepAI", "url": "https://deepai.org/publication/dynamic-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/dynamic-<b>bayesian</b>-<b>neural</b>-<b>networks</b>", "snippet": "Dynamic <b>Bayesian</b> <b>Neural</b> <b>Networks</b>. We define an evolving in time <b>Bayesian</b> <b>neural</b> <b>network</b> called a Hidden Markov <b>neural</b> <b>network</b>. The weights of the feed-forward <b>neural</b> <b>network</b> are modelled with the hidden states of a Hidden Markov model, the whose observed process is given by the available data. A filtering algorithm is used to learn a ...", "dateLastCrawled": "2022-01-09T11:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "machine learning - What are the <b>downsides of bayesian neural networks</b> ...", "url": "https://stats.stackexchange.com/questions/311008/what-are-the-downsides-of-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/311008", "snippet": "<b>Bayesian</b> <b>neural</b> nets (BNN) are very popular topic. With development of variational approximation it became possible to train such models much faster then with Monte Carlo sampling. BNNs allow such interesting features as natural regularisation and even uncertainty estimation. So, the question is: why haven&#39;t we still completely migrated on BNNs? I <b>can</b> assume that variational inference does not provide enough accuracy. Is it the only reason? machine-learning deep-learning <b>bayesian</b>-<b>network</b> ...", "dateLastCrawled": "2022-01-07T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An Example of a <b>Bayesian</b> <b>Neural</b> <b>Network</b> Using PyTorch | James D. McCaffrey", "url": "https://jamesmccaffrey.wordpress.com/2021/08/30/an-example-of-a-bayesian-neural-network-using-pytorch/", "isFamilyFriendly": true, "displayUrl": "https://jamesmccaffrey.wordpress.com/2021/08/30/an-example-of-a-<b>bayesian</b>-<b>neural</b>...", "snippet": "A <b>Bayesian</b> <b>neural</b> <b>network</b> for the Iris dataset. The demo predicts the class probabilities three times for input = [5.0, 2.0, 3.0, 2.0] and gets three slightly different results because the weights are distributions instead of fixed values. At first <b>thought</b> this doesn\u2019t seem useful at all. There are two advantages to a <b>Bayesian</b> <b>neural</b> <b>network</b>. First, the weights variability greatly deters model overfitting. Second, if you look at multiple output values from one input, and you see very ...", "dateLastCrawled": "2022-02-02T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "From <b>Bayesian</b> <b>Networks</b> to <b>Neural</b> <b>Networks</b>: how multivariate regression ...", "url": "https://stats.stackexchange.com/questions/260444/from-bayesian-networks-to-neural-networks-how-multivariate-regression-can-be-tr", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/260444/from-", "snippet": "While one <b>can</b> use the graph structure of the <b>Bayesian</b> <b>network</b> to give the graph structure for the <b>neural</b> <b>network</b>, from a <b>neural</b> <b>network</b> point of view this doesn&#39;t seem reasonable. I don&#39;t currently have the time to work out the details, but as a <b>thought</b> experiment imagine a data set where the independent variables were all statisitically $\\endgroup$", "dateLastCrawled": "2022-01-24T15:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>a Bayesian Neural Networks? Background, Basic Idea &amp; Function</b> ...", "url": "https://www.upgrad.com/blog/bayesian-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/<b>bayesian</b>-<b>neural</b>-<b>networks</b>", "snippet": "The <b>Bayesian</b> <b>Neural</b> <b>Networks</b> are those criteria or parameters that are under most circumstances expressed as distribution and are usually learned through the concept of <b>Bayesian</b> Inference, as <b>compared</b> to a deterministic value. They have an inner ability to digest the complex, non-linear function from the data and then to express the uncertainties- both at the same time. It has hence also led them to a higher role in the pursuit to garner and build a more reliable and competent AI.", "dateLastCrawled": "2022-02-02T22:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is the <b>difference between a Bayesian network and Bayesian neural</b> ...", "url": "https://www.researchgate.net/post/What_is_the_difference_between_a_Bayesian_network_and_Bayesian_neural_network", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/What_is_the_<b>difference_between_a_Bayesian_network</b>...", "snippet": "<b>Bayesian</b> <b>neural</b> <b>networks</b> marginalize over the distribution of parameters in order to make predictions. So the <b>Bayesian</b> approach allows different models to <b>be compared</b> (e.g. no of hidden units).", "dateLastCrawled": "2022-02-02T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "machine learning - Difference between Bayes <b>network</b>, <b>neural</b> <b>network</b> ...", "url": "https://stats.stackexchange.com/questions/94511/difference-between-bayes-network-neural-network-decision-tree-and-petri-nets", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/94511", "snippet": "<b>Bayesian</b> <b>Network</b>: The <b>Bayesian</b> <b>Network</b> is a directed acyclic graph, which more like the flowchart, only that the flow chart <b>can</b> have cyclic loops. The <b>Bayesian</b> <b>network</b> unlike the flow chart <b>can</b> have multiple start points. It basically traces the propagation of events across multiple ambiguous points, where the event diverges probabilistically between pathways. Obviously, at any given point in the <b>network</b>, the probability of that node being visited is dependent on the joint probability of the ...", "dateLastCrawled": "2022-01-28T22:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bayesian regularization of neural networks</b>", "url": "https://pubmed.ncbi.nlm.nih.gov/19065804/", "isFamilyFriendly": true, "displayUrl": "https://pubmed.ncbi.nlm.nih.gov/19065804", "snippet": "<b>Bayesian</b> regularized artificial <b>neural</b> <b>networks</b> (BRANNs) are more robust than standard back-propagation nets and <b>can</b> reduce or eliminate the need for lengthy cross-validation. <b>Bayesian</b> regularization is a mathematical process that converts a nonlinear regression into a &quot;well-posed&quot; statistical problem in the manner of a ridge regression. The advantage of BRANNs is that the models are robust and the validation process, which scales as O(N2) in normal regression methods, such as back ...", "dateLastCrawled": "2022-02-02T07:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>GitHub</b> - <b>LukasRinder/bayesian-neural-networks</b>: Different ...", "url": "https://github.com/LukasRinder/bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/LukasRinder/<b>bayesian</b>-<b>neural</b>-<b>networks</b>", "snippet": "Furthermore, the uncertainty estimates from the variational <b>Bayesian</b> <b>neural</b> <b>networks</b> are used to perform approximate Thompson sampling within a deep Q-<b>network</b> (DQN) for efficient exploration. The approaches are <b>compared</b> against each <b>other</b> and against the well known epsilon-greedy strategy.", "dateLastCrawled": "2021-09-19T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Combining <b>Bayesian</b> <b>Neural Networks</b> and Ensemble techniques | by Dylan ...", "url": "https://towardsdatascience.com/combining-bayesian-neural-networks-and-ensemble-techniques-a4a3a9072e79", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/combining-<b>bayesian</b>-<b>neural-networks</b>-and-ensemble...", "snippet": "\u2018An Ensemble of <b>Bayesian</b> <b>Neural Networks</b> for Exoplanetary Atmospheric Retrieval ... We <b>can</b> imagine our <b>neural</b> <b>network</b> as usual which we denote f\u1d42(X). However to find our models weights, instead of just training the NN using the normal MAP loss function (usually L2 regularised loss function), we also add noise into our loss term. We train multiple models with each different instance of the noisy loss function and this gives us different optimised weights for each model. When making ...", "dateLastCrawled": "2022-02-01T05:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Explaining <b>Bayesian</b> <b>Neural</b> <b>Networks</b> | DeepAI", "url": "https://deepai.org/publication/explaining-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/explaining-<b>bayesian</b>-<b>neural</b>-<b>networks</b>", "snippet": "Layer-wise Relevance Propagation [bach2015pixel] is a model-aware explanation technique that <b>can</b> be applied for feed-forward <b>neural</b> <b>networks</b> and <b>can</b> be used for different types of inputs, such as images, videos, or text [anders2019understanding, arras2017relevant, montavon2018methods, binder2021morphological]. The underlying idea of the LRP algorithm is to use the <b>network</b> weights and the <b>neural</b> activations computed in the forward-pass to propagate the relevant output back through the <b>network</b> ...", "dateLastCrawled": "2022-01-26T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bayesian</b> convolutional <b>neural</b> <b>networks</b> for predicting the terrestrial ...", "url": "https://www.sciencedirect.com/science/article/pii/S0022169421012944", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0022169421012944", "snippet": "A <b>Bayesian</b> convolutional <b>neural</b> <b>network</b> is proposed to reconstruct the TWSA signals during GRACE and GRACE-FO gap. \u2022 The <b>Bayesian</b> training strategy enables the quantification of predictive uncertainties. \u2022 A clearly improved gap-filling performance is obtained in comparison with previous studies. \u2022 The improved infilling product <b>can</b> reliably maintain the data continuity and enhance data consistency. Abstract. The monthly terrestrial water storage anomaly (TWSA) observations during the ...", "dateLastCrawled": "2022-01-28T19:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "machine learning - What are the <b>downsides of bayesian neural networks</b> ...", "url": "https://stats.stackexchange.com/questions/311008/what-are-the-downsides-of-bayesian-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/311008", "snippet": "<b>Bayesian</b> <b>neural</b> nets (BNN) are very popular topic. With development of variational approximation it became possible to train such models much faster then with Monte Carlo sampling. BNNs allow such interesting features as natural regularisation and even uncertainty estimation. So, the question is: why haven&#39;t we still completely migrated on BNNs? I <b>can</b> assume that variational inference does not provide enough accuracy. Is it the only reason? machine-learning deep-learning <b>bayesian</b>-<b>network</b> ...", "dateLastCrawled": "2022-01-07T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] <b>advantages of bayesian neural network over regular</b> <b>neural</b> <b>networks</b> ...", "url": "https://www.reddit.com/r/statistics/comments/mrfc9x/d_advantages_of_bayesian_neural_network_over/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/mrfc9x/d_<b>advantages_of_bayesian_neural_network</b>_over", "snippet": "This is a really good video that explains the differences between regular <b>neural</b> <b>networks</b> and <b>bayesian</b> <b>neural</b> <b>networks</b>. Apparently in <b>bayesian</b> <b>neural</b> <b>networks</b>, the weights aren&#39;t assigned a fixed value but instead each weight is assigned a probability distribution (e.g. each weight is given a normal probability distribution and we calculate the mean and variance).", "dateLastCrawled": "2021-04-15T14:04:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>machine</b> <b>learning</b> approach to <b>Bayesian</b> parameter estimation | npj ...", "url": "https://www.nature.com/articles/s41534-021-00497-w", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41534-021-00497-w", "snippet": "The parameter estimation discussed in this manuscript is divided in two parts: i) a <b>neural</b> <b>network</b> is trained and ii) <b>Bayesian</b> estimation performed on a test set, which we detail below.", "dateLastCrawled": "2022-02-03T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "9.7. <b>Bayesian</b> <b>neural</b> networks \u2014 <b>Learning</b> from data", "url": "https://furnstahl.github.io/Physics-8820/notebooks/Machine_learning/Bayesian_neural_networks_tif285.html", "isFamilyFriendly": true, "displayUrl": "https://furnstahl.github.io/.../<b>Machine</b>_<b>learning</b>/<b>Bayesian</b>_<b>neural</b>_<b>networks</b>_tif285.html", "snippet": "<b>Bayesian</b> <b>neural</b> networks differ from plain <b>neural</b> networks in that their weights are assigned a probability distribution instead of a single value or point estimate. These probability distributions describe the uncertainty in weights and can be used to estimate uncertainty in predictions. Training a <b>Bayesian</b> <b>neural</b> <b>network</b> via variational inference learns the parameters of these distributions instead of the weights directly.", "dateLastCrawled": "2021-12-21T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Researchers Explore <b>Bayesian</b> <b>Neural</b> Networks -- Pure AI", "url": "https://pureai.com/articles/2021/09/07/bayesian-neural-networks.aspx", "isFamilyFriendly": true, "displayUrl": "https://pureai.com/articles/2021/09/07/<b>bayesian</b>-<b>neural</b>-<b>networks</b>.aspx", "snippet": "<b>Bayesian</b> <b>neural</b> networks are best explained using an <b>analogy</b> example. Suppose that instead of a <b>neural</b> <b>network</b>, you have a prediction equation y = (8.5 * x1) + (9.5 * x2) + 2.5 where y is the predicted income of an employee, x1 is normalized age, and x2 is years of job tenure. The predicted income of a 30-year old who has been on the job for 4 years would be y = (8.5 * 3.0) + (9.5 * 4.0) + 2.5 = 64.5 = $64,500. If you feed the same (age, tenure) input of (3.0, 4.0) to the prediction equation ...", "dateLastCrawled": "2022-01-30T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bayesian</b> <b>Neural Network</b> Series Post 2: Background Knowledge | by Kumar ...", "url": "https://medium.com/neuralspace/bayesian-neural-network-series-post-2-background-knowledge-fdec6ac62d43", "isFamilyFriendly": true, "displayUrl": "https://medium.com/<b>neural</b>space/<b>bayesian</b>-<b>neural-network</b>-series-post-2-background...", "snippet": "I will try to brief the <b>neural</b> networks <b>analogy</b> with the brain and will spend more time explaining the Probabilistic <b>Machine</b> <b>Learning</b> segments that we will work on in future. Brain Analogies. A ...", "dateLastCrawled": "2022-01-30T12:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep <b>neural</b> <b>network</b> models, and it has been used for conducting ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bayesian Belief Network in Artificial Intelligence</b> - Javatpoint", "url": "https://www.javatpoint.com/bayesian-belief-network-in-artificial-intelligence", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>bayesian-belief-network-in-artificial-intelligence</b>", "snippet": "<b>Bayesian Belief Network in artificial intelligence</b>. <b>Bayesian</b> belief <b>network</b> is key computer technology for dealing with probabilistic events and to solve a problem which has uncertainty. We can define a <b>Bayesian</b> <b>network</b> as: &quot;A <b>Bayesian</b> <b>network</b> is a probabilistic graphical model which represents a set of variables and their conditional ...", "dateLastCrawled": "2022-02-02T21:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Neural Networks and Learning Machines</b> - uniba.sk", "url": "http://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf", "isFamilyFriendly": true, "displayUrl": "dai.fmph.uniba.sk/courses/NN/haykin.<b>neural</b>-<b>networks</b>.3ed.2009.pdf", "snippet": "<b>Neural Networks and Learning Machines</b> Third Edition Simon Haykin McMaster University Hamilton, Ontario, Canada New York Boston San Francisco London Toronto Sydney Tokyo Singapore Madrid Mexico City Munich Paris Cape Town Hong Kong Montreal. Library of Congress Cataloging-in-Publication Data Haykin, Simon <b>Neural networks and learning machines</b> / Simon Haykin.\u20143rd ed. p. cm. Rev. ed of: <b>Neural</b> networks. 2nd ed., 1999. Includes bibliographical references and index. ISBN-13: 978-0-13-147139-9 ...", "dateLastCrawled": "2022-02-02T18:43:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bayesian neural network)  is like +(other neural networks)", "+(bayesian neural network) is similar to +(other neural networks)", "+(bayesian neural network) can be thought of as +(other neural networks)", "+(bayesian neural network) can be compared to +(other neural networks)", "machine learning +(bayesian neural network AND analogy)", "machine learning +(\"bayesian neural network is like\")", "machine learning +(\"bayesian neural network is similar\")", "machine learning +(\"just as bayesian neural network\")", "machine learning +(\"bayesian neural network can be thought of as\")", "machine learning +(\"bayesian neural network can be compared to\")"]}