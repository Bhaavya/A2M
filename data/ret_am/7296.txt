{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Artificial intelligence with deep learning in nuclear medicine and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8665861/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8665861", "snippet": "The versatility of deep learning lies behind the <b>universal</b> <b>approximation</b> <b>theorem</b>, stating that feedforward networks with at least one hidden layer, using a nonlinear activation and a linear output layer, can approximate any continuous function [16, 17]. That is, these deep learning models should be able to fit any sufficiently well-behaved training data to arbitrary precision by expanding the hidden layer size, thereby allowing the network to model increasingly <b>complex</b> functions. One of the ...", "dateLastCrawled": "2022-01-29T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Artificial intelligence: A powerful paradigm for scientific research ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666675821001041", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666675821001041", "snippet": "The well-known <b>Universal</b> <b>Approximation</b> <b>Theorem</b> suggests that, under very mild conditions, any continuous function can be uniformly approximated on compact domains by neural networks, 39 which serves a vital function in the interpretability of neural networks. However, in real applications, ML models seem to admit accurate approximations of many extremely complicated functions, sometimes even black boxes, which are far beyond the scope of continuous functions. To understand the effectiveness ...", "dateLastCrawled": "2022-01-30T00:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep learning theory</b> lecture notes - Matus Telgarsky.", "url": "https://mjt.cs.illinois.edu/dlt/", "isFamilyFriendly": true, "displayUrl": "https://mjt.cs.illinois.edu/dlt", "snippet": "The answer will be yes, and we will use this to resolve the classical <b>universal</b> <b>approximation</b> question with a single hidden layer. Definition 2.1. A class of functions \\mathcal{F} is a <b>universal</b> approximator over a compact set S if for every continuous function g and target accuracy \\epsilon&gt;0, there exists f\\in\\mathcal{F} with \\sup_{x\\in S} |f(x) - g(x)| \\leq \\epsilon. Remark 2.4. Typically we will take S = [0,1]^d; we can then reduce arbitrary compact sets to this case by defining a new ...", "dateLastCrawled": "2022-02-03T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is the difference between a <b>neural network</b> and a deep <b>neural</b> ...", "url": "https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/182734", "snippet": "However, 10 years ago in class I learned that having several layers or one layer (not counting the input and output layers) was equivalent in terms of the functions a <b>neural network</b> is able to represent (see Cybenko&#39;s <b>Universal</b> <b>approximation</b> <b>theorem</b>), and that having more layers made it more <b>complex</b> to analyse without gain in performance. Obviously, that is not the case anymore.", "dateLastCrawled": "2022-01-29T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Physical Basis of DIMENSIONAL ANALYSIS", "url": "http://web.mit.edu/2.25/www/pdf/DA_unified.pdf", "isFamilyFriendly": true, "displayUrl": "<b>web.mit.edu</b>/2.25/www/pdf/DA_unified.pdf", "snippet": "<b>complex</b> business. Physics starts by <b>breaking</b> the descriptive process <b>down</b> <b>into</b> <b>simpler</b> terms. An object or event is described in terms of basic properties <b>like</b> length, mass, color, shape, speed, and time. None of these properties can be defined in absolute terms, but only by reference to something else: an object has the length of a meter stick, we say, the color of an orange, the weight of a certain familiar lump of material, or the shape of a sphere. The references may be made more precise ...", "dateLastCrawled": "2022-01-31T10:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Computational Complexity Theory (Stanford Encyclopedia of Philosophy)", "url": "https://plato.stanford.edu/entries/computational-complexity/", "isFamilyFriendly": true, "displayUrl": "https://<b>plato.stanford.edu</b>/entries/computational-<b>complex</b>ity", "snippet": "<b>Like</b> Theorems 4.2 and 4.3, <b>Theorem</b> 4.4 is significant because it provides another machine-independent characterization of an important complexity class. Recall, however, that Cobham was working at a time when the mathematical status of the notion of feasibility was still under debate. So it is also reasonable to ask if the definition of \\(\\mathcal{F}\\) can be understood as providing an independently motivated analysis of feasible computability akin to the analyses which Church and Turing are ...", "dateLastCrawled": "2022-01-30T14:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Processing Machine Data With Machine Learning | Avast", "url": "https://blog.avast.com/processing-machine-data-with-machine-learning-avast", "isFamilyFriendly": true, "displayUrl": "https://blog.avast.com/processing-machine-data-with-machine-learning-avast", "snippet": "At this point, there is a trade-off: More complete information (e.g. <b>complex</b> <b>approximation</b> of the distribution, sequential information, or entropy) leads to slower learning, yet <b>simpler</b> pooling may lead to underutilization of the structural information contained in the array. Interestingly, in most of the security machine data, our models perform well with a trivial pair of pooling functions that extract only the average and maximum of values in the array. We can even formally prove that, in ...", "dateLastCrawled": "2022-01-31T23:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "notes/Artificial Intelligence.md at master - <b>GitHub</b>", "url": "https://github.com/brylevkirill/notes/blob/master/Artificial%20Intelligence.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/brylevkirill/notes/blob/master/Artificial Intelligence.md", "snippet": "&quot;Reinforcement learning agents <b>like</b> Hutter\u2019s <b>universal</b>, Pareto optimal, incomputable AIXI heavily rely on the definition of the rewards, which are necessarily given by some \u201cteacher\u201d to define the <b>tasks</b> to solve. Therefore, as is, AIXI cannot be said to be a fully autonomous agent. From the point of view of artificial general intelligence, this can be argued to be an incomplete definition of a generally intelligent agent. Furthermore, it has recently been shown that AIXI can converge ...", "dateLastCrawled": "2021-09-11T15:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A survey of the recent architectures of deep convolutional neural ...", "url": "https://link.springer.com/article/10.1007/s10462-020-09825-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10462-020-09825-6", "snippet": "The attention module is branched off <b>into</b> trunk and mask branches that adopt bottom-up, top-<b>down</b> learning strategy. The assembly of two different learning strategies <b>into</b> the attention module enables fast feed-forward processing and top-<b>down</b> attention feedback in a single feed-forward process. The bottom-up feed-forward structure produces low-resolution feature-maps with strong semantic information. Whereas, top-<b>down</b> architecture produces dense features to make an inference of each pixel.", "dateLastCrawled": "2022-02-02T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>ICML2020</b>/<b>icml2020</b>_1.md at master \u00b7 <b>haozhangcn/ICML2020</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/haozhangcn/ICML2020/blob/master/icml2020_1.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/haozhangcn/<b>ICML2020</b>/blob/master/<b>icml2020</b>_1.md", "snippet": "Black-box variational inference tries to approximate a <b>complex</b> target distribution through a gradient-based optimization of the parameters of a <b>simpler</b> distribution. Provable convergence guarantees require structural properties of the objective. This paper shows that for location-scale family approximations, if the target is M-Lipschitz smooth, then so is the \u201cenergy\u201d part of the variational objective. The key proof idea is to describe gradients in a certain inner-product space, thus ...", "dateLastCrawled": "2022-01-29T11:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Artificial intelligence: A powerful paradigm for scientific research ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666675821001041", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666675821001041", "snippet": "The well-known <b>Universal</b> <b>Approximation</b> <b>Theorem</b> suggests that, under very mild conditions, any continuous function can be uniformly approximated on compact domains by neural networks, 39 which serves a vital function in the interpretability of neural networks. However, in real applications, ML models seem to admit accurate approximations of many extremely complicated functions, sometimes even black boxes, which are far beyond the scope of continuous functions. To understand the effectiveness ...", "dateLastCrawled": "2022-01-30T00:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Deep learning theory lecture notes", "url": "https://mjt.cs.illinois.edu/dlt/index.html", "isFamilyFriendly": true, "displayUrl": "https://mjt.cs.illinois.edu/dlt/index.html", "snippet": "Remark 2.7 (other <b>universal</b> <b>approximation</b> proofs). (Cybenko 1989) Assume contradictorily you miss some functions. By duality, 0 =\\int \\sigma(a^{\\scriptscriptstyle\\mathsf{T}}x -b) {\\text{d}}\\mu(x) for some signed measure \\mu, all (a,b). Using Fourier, can show this implies \\mu = 0 (Leshno et al. 1993) If \\sigma a polynomial, ; else can (roughly) get derivatives and polynomials of all orders (we\u2019ll have homework problems on this). (Barron 1993) Use inverse Fourier representation to construct ...", "dateLastCrawled": "2022-01-30T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Artificial intelligence with deep learning in nuclear medicine and ...", "url": "https://ejnmmiphys.springeropen.com/articles/10.1186/s40658-021-00426-y", "isFamilyFriendly": true, "displayUrl": "https://ejnmmiphys.springeropen.com/articles/10.1186/s40658-021-00426-y", "snippet": "The versatility of deep learning lies behind the <b>universal</b> <b>approximation</b> <b>theorem</b>, stating that feedforward networks with at least one hidden layer, using a nonlinear activation and a linear output layer, can approximate any continuous function [16, 17]. That is, these deep learning models should be able to fit any sufficiently well-behaved training data to arbitrary precision by expanding the hidden layer size, thereby allowing the network to model increasingly <b>complex</b> functions. One of the ...", "dateLastCrawled": "2022-01-29T17:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is the difference between a <b>neural network</b> and a deep <b>neural</b> ...", "url": "https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/182734", "snippet": "However, 10 years ago in class I learned that having several layers or one layer (not counting the input and output layers) was equivalent in terms of the functions a <b>neural network</b> is able to represent (see Cybenko&#39;s <b>Universal</b> <b>approximation</b> <b>theorem</b>), and that having more layers made it more <b>complex</b> to analyse without gain in performance. Obviously, that is not the case anymore.", "dateLastCrawled": "2022-01-29T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Universal Differential Equations for Scientific Machine Learning</b>", "url": "https://www.researchgate.net/publication/338569581_Universal_Differential_Equations_for_Scientific_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338569581_<b>Universal</b>_Differential_Equations...", "snippet": "are known from the choice of model, the remaining unknown portions are the. 188. functional \u03c3 T ( t, X t) \u2207 u ( t, X t) and initial condition U (0) = u (0, \u03b6), the latter b eing the point ...", "dateLastCrawled": "2022-01-21T21:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Physical Basis of DIMENSIONAL ANALYSIS", "url": "http://web.mit.edu/2.25/www/pdf/DA_unified.pdf", "isFamilyFriendly": true, "displayUrl": "<b>web.mit.edu</b>/2.25/www/pdf/DA_unified.pdf", "snippet": "Dimensional analysis offers a method for reducing <b>complex</b> physical problems to the simplest (that is, most economical) form prior to obtaining a quantitative answer. Bridgman (1969) explains it thus: &quot;The principal use of dimensional analysis is to deduce from a study of the dimensions of the variables in any physical system certain limitations on the form of any possible relationship between those variables. The method is of great generality and mathematical simplicity&quot;. At the heart of ...", "dateLastCrawled": "2022-01-31T10:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "notes/Artificial Intelligence.md at master - <b>GitHub</b>", "url": "https://github.com/brylevkirill/notes/blob/master/Artificial%20Intelligence.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/brylevkirill/notes/blob/master/Artificial Intelligence.md", "snippet": "&quot;We defined a new kind of <b>universal</b> intelligent agents, named knowledge-seeking agents, which differ significantly from the traditional reinforcement learning framework and its associated <b>universal</b> optimal learner AIXI: Their purpose is not to solve particular, narrow <b>tasks</b>, given or defined by experts like humans, but to be fully autonomous and to depend on no external intelligent entity. Full autonomy is an important property if we are to create generally intelligent agents, that should ...", "dateLastCrawled": "2021-09-11T15:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A survey of the recent architectures of deep convolutional neural ...", "url": "https://link.springer.com/article/10.1007/s10462-020-09825-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10462-020-09825-6", "snippet": "The attention module is branched off <b>into</b> trunk and mask branches that adopt bottom-up, top-<b>down</b> learning strategy. The assembly of two different learning strategies <b>into</b> the attention module enables fast feed-forward processing and top-<b>down</b> attention feedback in a single feed-forward process. The bottom-up feed-forward structure produces low-resolution feature-maps with strong semantic information. Whereas, top-<b>down</b> architecture produces dense features to make an inference of each pixel.", "dateLastCrawled": "2022-02-02T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Data Structures And Algorithm Analysis</b> - Best Writers", "url": "https://blog.bestwriters.org/2020/09/23/data-structures-and-algorithm-analysis/", "isFamilyFriendly": true, "displayUrl": "https://blog.bestwriters.org/2020/09/23/<b>data-structures-and-algorithm-analysis</b>", "snippet": "The cost incurred at each of the two sub- nodes at the second level of recursion is cn=2. We continue expanding each node in the tree by <b>breaking</b> it <b>into</b> its constituent parts as determined by the recurrence, until the problem sizes get <b>down</b> to 1, each with a cost of c. Part (d) shows the resulting recursion tree.", "dateLastCrawled": "2022-02-03T01:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Artificial intelligence: A powerful paradigm for scientific research", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8633405/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8633405", "snippet": "The well-known <b>Universal</b> <b>Approximation</b> <b>Theorem</b> suggests that, under very mild conditions, any continuous function <b>can</b> be uniformly approximated on compact domains by neural networks,39 which serves a vital function in the interpretability of neural networks. However, in real applications, ML models seem to admit accurate approximations of many extremely complicated functions, sometimes even black boxes, which are far beyond the scope of continuous functions. To understand the effectiveness ...", "dateLastCrawled": "2022-01-28T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Artificial intelligence with deep learning in nuclear medicine and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8665861/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8665861", "snippet": "The versatility of deep learning lies behind the <b>universal</b> <b>approximation</b> <b>theorem</b>, stating that feedforward networks with at least one hidden layer, using a nonlinear activation and a linear output layer, <b>can</b> approximate any continuous function [16, 17]. That is, these deep learning models should be able to fit any sufficiently well-behaved training data to arbitrary precision by expanding the hidden layer size, thereby allowing the network to model increasingly <b>complex</b> functions. One of the ...", "dateLastCrawled": "2022-01-29T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Artificial intelligence: A powerful paradigm for scientific research ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666675821001041", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666675821001041", "snippet": "The well-known <b>Universal</b> <b>Approximation</b> <b>Theorem</b> suggests that, under very mild conditions, any continuous function <b>can</b> be uniformly approximated on compact domains by neural networks, 39 which serves a vital function in the interpretability of neural networks. However, in real applications, ML models seem to admit accurate approximations of many extremely complicated functions, sometimes even black boxes, which are far beyond the scope of continuous functions. To understand the effectiveness ...", "dateLastCrawled": "2022-01-30T00:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "3 Dynamical Systems: When the Simple Is <b>Complex</b>: New Mathematical ...", "url": "https://www.nap.edu/read/1859/chapter/4", "isFamilyFriendly": true, "displayUrl": "https://www.nap.edu/read/1859/chapter/4", "snippet": "A <b>complex</b> number <b>can</b> <b>be thought</b> of as a point in the plane, specified by two numbers, one to say how far left or right the <b>complex</b> number is, the other to say how far up or <b>down</b> it is. Thus we <b>can</b> write a <b>complex</b> number z as z = x + iy , where x and y are real numbers, and i is an imaginary number, the square root of-1.", "dateLastCrawled": "2022-02-02T00:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What is the difference between a <b>neural network</b> and a deep <b>neural</b> ...", "url": "https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/182734", "snippet": "However, 10 years ago in class I learned that having several layers or one layer (not counting the input and output layers) was equivalent in terms of the functions a <b>neural network</b> is able to represent (see Cybenko&#39;s <b>Universal</b> <b>approximation</b> <b>theorem</b>), and that having more layers made it more <b>complex</b> to analyse without gain in performance. Obviously, that is not the case anymore.", "dateLastCrawled": "2022-01-29T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Simulation Intelligence: Towards a New Generation of Scientific Methods ...", "url": "https://deepai.org/publication/simulation-intelligence-towards-a-new-generation-of-scientific-methods", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/simulation-intelligence-towards-a-new-generation-of...", "snippet": "Neural networks (NNs) <b>can</b> also be well-suited to the surrogate modeling task as function <b>approximation</b> machines: A feedforward network de\ufb01nes a mapping y = f (x; \u03b8) and learns the value of the parameters \u03b8 that result in the best function <b>approximation</b>. The <b>Universal</b> <b>Approximation</b> <b>Theorem</b> demonstrates that sufficiently large NNs <b>can</b> ...", "dateLastCrawled": "2022-02-02T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Computational Complexity Theory (Stanford Encyclopedia of Philosophy)", "url": "https://plato.stanford.edu/entries/computational-complexity/", "isFamilyFriendly": true, "displayUrl": "https://<b>plato.stanford.edu</b>/entries/computational-<b>complex</b>ity", "snippet": "An algorithm based on dynamic programming solves an instance of such a problem by recursively <b>breaking</b> it <b>into</b> subproblems, whose optimal values are then computed and stored in a manner which <b>can</b> then be efficiently reassembled to achieve an optimal overall solution. Bellman (1962) showed that the naive time complexity of \\(O(n!)\\) for \\(\\sc{TSP}\\) could be improved to \\(O(2^n n^2)\\) via the use of dynamic programming. The question thus arose whether it was possible to improve upon such ...", "dateLastCrawled": "2022-01-30T14:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Matrix estimation by <b>Universal</b> Singular Value Thresholding", "url": "https://www.researchgate.net/publication/233846821_Matrix_estimation_by_Universal_Singular_Value_Thresholding", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/233846821_Matrix_estimation_by_<b>Universal</b>...", "snippet": "We mention that an analogous version of <b>Theorem</b> 3 <b>can</b> be proved forr \u03b5 under the conditions in Lemma 4, by repeating the proof of <b>Theorem</b> 3 and making use of Theorems 2 and 5.", "dateLastCrawled": "2022-01-09T15:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 9, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Problem solving</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Problem_solving", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Problem_solving</b>", "snippet": "The two approaches share an emphasis on relatively <b>complex</b>, semantically rich, computerized laboratory <b>tasks</b>, constructed to resemble real-life problems. The approaches differ somewhat in their theoretical goals and methodology, however. The tradition initiated by Broadbent emphasizes the distinction between cognitive <b>problem-solving</b> processes that operate under awareness versus outside of awareness, and typically employs mathematically well-defined computerized systems. The tradition ...", "dateLastCrawled": "2022-01-30T02:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A survey of the recent architectures of deep convolutional neural ...", "url": "https://link.springer.com/article/10.1007/s10462-020-09825-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10462-020-09825-6", "snippet": "Cs\u00e1ji represented a <b>universal</b> <b>approximation</b> <b>theorem</b> in 2001, which states that a single hidden layer is sufficient to approximate any function. However, this comes at the cost of exponentially many neurons; thus, it often makes it computationally non-realistic (Cs\u00e1ji 2001). In this regard, Bengio and Delalleau (Delalleau and Bengio 2011) suggested that deeper networks <b>can</b> maintain the expressive power of the network at a reduced cost (Wang and Raj 2017). In 2013, Bengio et al. empirically ...", "dateLastCrawled": "2022-02-02T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is the difference between a <b>neural network</b> and a deep <b>neural</b> ...", "url": "https://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network-and-w", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/182734", "snippet": "However, 10 years ago in class I learned that having several layers or one layer (not counting the input and output layers) was equivalent in terms of the functions a <b>neural network</b> is able to represent (see Cybenko&#39;s <b>Universal</b> <b>approximation</b> <b>theorem</b>), and that having more layers made it more <b>complex</b> to analyse without gain in performance. Obviously, that is not the case anymore.", "dateLastCrawled": "2022-01-29T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Artificial intelligence: A powerful paradigm for scientific research ...", "url": "https://www.sciencedirect.com/science/article/pii/S2666675821001041", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666675821001041", "snippet": "The well-known <b>Universal</b> <b>Approximation</b> <b>Theorem</b> suggests that, under very mild conditions, any continuous function <b>can</b> be uniformly approximated on compact domains by neural networks, 39 which serves a vital function in the interpretability of neural networks. However, in real applications, ML models seem to admit accurate approximations of many extremely complicated functions, sometimes even black boxes, which are far beyond the scope of continuous functions. To understand the effectiveness ...", "dateLastCrawled": "2022-01-30T00:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Artificial intelligence with deep learning in nuclear medicine and ...", "url": "https://ejnmmiphys.springeropen.com/articles/10.1186/s40658-021-00426-y", "isFamilyFriendly": true, "displayUrl": "https://ejnmmiphys.springeropen.com/articles/10.1186/s40658-021-00426-y", "snippet": "The versatility of deep learning lies behind the <b>universal</b> <b>approximation</b> <b>theorem</b>, stating that feedforward networks with at least one hidden layer, using a nonlinear activation and a linear output layer, <b>can</b> approximate any continuous function [16, 17]. That is, these deep learning models should be able to fit any sufficiently well-behaved training data to arbitrary precision by expanding the hidden layer size, thereby allowing the network to model increasingly <b>complex</b> functions. One of the ...", "dateLastCrawled": "2022-01-29T17:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Deep learning to infer eddy heat fluxes from sea surface height ...", "url": "https://www.nature.com/articles/s41467-020-20779-9", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-020-20779-9", "snippet": "In theory, deep ANNs <b>can</b> approximate nonlinear mappings of any complexity (see the <b>universal</b> <b>approximation</b> <b>theorem</b> 52,53), provided the network contains a sufficient number of free parameters, and ...", "dateLastCrawled": "2022-02-01T16:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep learning theory lecture notes", "url": "https://mjt.cs.illinois.edu/dlt/index.html", "isFamilyFriendly": true, "displayUrl": "https://mjt.cs.illinois.edu/dlt/index.html", "snippet": "<b>Theorem</b> 5.1 ((Telgarsky 2015, 2016)) was the earliest proof showing that a deep network <b>can</b> not be approximated by a reasonably-sized shallow network, however prior work showed a separation for exact representation of deep sum-product networks as <b>compared</b> with shallow ones (Bengio and Delalleau 2011). A sum-product network has nodes which compute affine transformations or multiplications, and thus a multi-layer sum-product network is a polynomial, and this result, while interesting, does not ...", "dateLastCrawled": "2022-01-30T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The Physical Basis of DIMENSIONAL ANALYSIS", "url": "http://web.mit.edu/2.25/www/pdf/DA_unified.pdf", "isFamilyFriendly": true, "displayUrl": "<b>web.mit.edu</b>/2.25/www/pdf/DA_unified.pdf", "snippet": "<b>complex</b> business. Physics starts by <b>breaking</b> the descriptive process <b>down</b> <b>into</b> <b>simpler</b> terms. An object or event is described in terms of basic properties like length, mass, color, shape, speed, and time. None of these properties <b>can</b> be defined in absolute terms, but only by reference to something else: an object has the length of a meter stick, we say, the color of an orange, the weight of a certain familiar lump of material, or the shape of a sphere. The references may be made more precise ...", "dateLastCrawled": "2022-01-31T10:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Complexity And <b>Approximation</b>: Combinatorial Optimization Problems And ...", "url": "https://vdoc.pub/documents/complexity-and-approximation-combinatorial-optimization-problems-and-their-approximability-properties-6r7gn3h426v0", "isFamilyFriendly": true, "displayUrl": "https://vdoc.pub/documents/<b>complex</b>ity-and-<b>approximation</b>-combinatorial-optimization...", "snippet": "Essentially, they are characterized by the use of randomization in rounding real solution to integer ones but, in all cases, we will see that the resulting algorithms <b>can</b> be transformed <b>into</b> deterministic ones without any loss in the <b>approximation</b> performance. Such techniques are, indeed, quite interesting from the application point of view, because their discovery led to very good <b>approximation</b> algorithms for some relevant optimization problems. The sixth, seventh, and eighth chapters are ...", "dateLastCrawled": "2021-11-11T12:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Deep <b>Learning for NLP and Speech Recognition</b> | William Jacome ...", "url": "https://www.academia.edu/43190210/Deep_Learning_for_NLP_and_Speech_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/43190210/Deep_<b>Learning_for_NLP_and_Speech_Recognition</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-02T18:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Alan Edelman</b> - Massachusetts Institute of Technology", "url": "https://math.mit.edu/~edelman/publications.php", "isFamilyFriendly": true, "displayUrl": "https://math.mit.edu/~edelman/publications.php", "snippet": "Abstract: Algorithms for the N-body problem are <b>compared</b> and contrasted, particularly those where N is in the range for which direct methods outperform <b>approximation</b> methods. With fewer bodies than processors, the so-called \u201creplicated orrery\u201d on a three-dimensional grid has been used successfully on the Connection Machine CM-2 architecture. With more bodies, the &quot;rotated and translated Gray codes&quot; is an ideal direct algorithm for machines such as the CM-2 in that it takes optimal ...", "dateLastCrawled": "2022-01-30T14:03:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Approximation Theorem</b>. The power of Neural Networks | by ...", "url": "https://medium.com/swlh/universal-approximation-theorem-d1a1a67c1b5b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>universal-approximation-theorem</b>-d1a1a67c1b5b", "snippet": "<b>Universal Approximation Theorem</b>, in its lose form, states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate any continuous function. Whoa ...", "dateLastCrawled": "2022-01-28T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal Approximation Theorem</b>, Neural Nets &amp; Lego Blocks | by ...", "url": "https://medium.com/analytics-vidhya/universal-approximation-theorem-neural-nets-lego-blocks-1f5a7d93542a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>universal-approximation-theorem</b>-neural-nets-lego...", "snippet": "In this post, we will look at the <b>Universal Approximation Theorem</b> \u2014 one of the fundamental theorems on which the entire concept of Deep <b>Learning</b> is based upon. We will make use of lego blocks ...", "dateLastCrawled": "2022-01-28T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> Complex Functions using <b>Universal</b> Approximate <b>Theorem</b> - Ai Nxt", "url": "https://ainxt.co.in/learning-complex-functions-using-universal-approximate-theorem/", "isFamilyFriendly": true, "displayUrl": "https://ainxt.co.in/<b>learning</b>-complex-functions-using-<b>universal</b>-approximate-<b>theorem</b>", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b>. No matter how complex our output logic is, we can use collection of neurons and form Dense Neural Network to approximate our function. This is known as \u201c<b>UNIVERSAL</b> <b>APPROXIMATION</b> <b>THEOREM</b>\u201c. Lets take an example of Two Dimensional data where y = f(x) i.e. y is some function of x. Now, we need to find that ...", "dateLastCrawled": "2022-01-21T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Universal Approximation Theorems</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336361517_Universal_Approximation_Theorems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336361517_<b>Universal_Approximation_Theorems</b>", "snippet": "In the <b>machine</b> <b>learning</b> literature, <b>universal</b> <b>approximation</b> refers to a model class\u2019 ability. to generically approximate any member of a large topological space whose elements are. functions, or ...", "dateLastCrawled": "2022-01-25T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Illustrative Proof of <b>Universal Approximation Theorem</b> | HackerNoon", "url": "https://hackernoon.com/illustrative-proof-of-universal-approximation-theorem-5845c02822f6", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/illustrative-proof-of-<b>universal-approximation-theorem</b>-5845c02822f6", "snippet": "We will talk about the <b>Universal approximation theorem</b> and we will also prove the <b>theorem</b> graphically. The most commonly used sigmoid function is the logistic function, which has a characteristic of an \u201cS\u201d shaped curve. In real life, we deal with complex functions where the relationship between input and output might be complex. To solve this problem, let&#39;s take an <b>analogy</b> of building a house. The way we are going to create complex functions is that we will combine the sigmoids neurons ...", "dateLastCrawled": "2022-02-01T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "ne.neural evol - <b>Universal Approximation Theorem</b> \u2014 Neural Networks ...", "url": "https://cstheory.stackexchange.com/questions/17545/universal-approximation-theorem-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/17545", "snippet": "<b>Universal approximation theorem</b> states that &quot;the standard multilayer feed-forward network with a single hidden layer, ... There is an advanced result, key to <b>machine</b> <b>learning</b>, known as Kolmogorov&#39;s <b>theorem</b> [1]; I have never seen an intuitive sketch of why it works. This may have to do with the different cultures that approach it. The applied <b>learning</b> crowd regards Kolmogorov&#39;s <b>theorem</b> as an existence <b>theorem</b> that merely indicates that NNs may exist, so at least the structure is not overly ...", "dateLastCrawled": "2022-02-03T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "neural networks - <b>Universal Approximation Theorem and high dimension</b> ...", "url": "https://stats.stackexchange.com/questions/298622/universal-approximation-theorem-and-high-dimension-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/298622/<b>universal</b>-<b>approximation</b>-<b>theorem</b>-and...", "snippet": "Cross Validated is a question and answer site for people interested in statistics, <b>machine</b> <b>learning</b>, data analysis, data mining, and data visualization. It only takes a minute to sign up. Sign up to join this community", "dateLastCrawled": "2022-01-17T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Neural Networks and Learning Machines</b> - etsmtl.ca", "url": "https://cours.etsmtl.ca/sys843/REFS/Books/ebook_Haykin09.pdf", "isFamilyFriendly": true, "displayUrl": "https://cours.etsmtl.ca/sys843/REFS/Books/ebook_Haykin09.pdf", "snippet": "15.3 <b>Universal</b> <b>Approximation</b> <b>Theorem</b> 797 15.4 Controllability and Observability 799 15.5 Computational Power of Recurrent Networks 804 15.6 <b>Learning</b> Algorithms 806 15.7 Back Propagation Through Time 808 15.8 Real-Time Recurrent <b>Learning</b> 812 15.9 Vanishing Gradients in Recurrent Networks 818", "dateLastCrawled": "2022-01-31T06:23:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(universal approximation theorem)  is like +(breaking down complex tasks into simpler tasks)", "+(universal approximation theorem) is similar to +(breaking down complex tasks into simpler tasks)", "+(universal approximation theorem) can be thought of as +(breaking down complex tasks into simpler tasks)", "+(universal approximation theorem) can be compared to +(breaking down complex tasks into simpler tasks)", "machine learning +(universal approximation theorem AND analogy)", "machine learning +(\"universal approximation theorem is like\")", "machine learning +(\"universal approximation theorem is similar\")", "machine learning +(\"just as universal approximation theorem\")", "machine learning +(\"universal approximation theorem can be thought of as\")", "machine learning +(\"universal approximation theorem can be compared to\")"]}