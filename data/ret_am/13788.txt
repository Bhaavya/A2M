{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least Squares Regression</b> - How to Create <b>Line</b> of Best Fit?", "url": "https://www.wallstreetmojo.com/least-squares-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.wallstreetmojo.com/<b>least-squares-regression</b>", "snippet": "<b>Line</b> of Best Fit in the <b>Least</b> Square <b>Regression</b>. The <b>line</b> of best fit is <b>a straight</b> <b>line</b> drawn <b>through</b> a scatter of data <b>points</b> that best represents the relationship between them. Let us consider the following graph wherein <b>a set</b> of data is plotted along the x and y-axis. These data <b>points</b> are represented using the blue dots. Three lines are ...", "dateLastCrawled": "2022-02-03T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>The Least Squares Regression Method</b> \u2013 How to Find the <b>Line</b> of Best Fit", "url": "https://www.freecodecamp.org/news/the-least-squares-regression-method-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.freecodecamp.org/news/<b>the-least-squares-regression-method</b>-explained", "snippet": "What is <b>the Least Squares Regression method</b> and why use it? <b>Least</b> <b>squares</b> is a method to apply linear <b>regression</b>. It helps us predict results based on an existing <b>set</b> of data as well as clear anomalies in our data. Anomalies are values that are too good, or bad, to be true or that represent rare cases. For example, say we have a list of how many topics future engineers here at freeCodeCamp can solve if they invest 1, 2, or 3 hours continuously. Then we can predict how many topics will be ...", "dateLastCrawled": "2022-02-03T15:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Least</b>-<b>Squares</b> <b>Regression</b> - NPTEL", "url": "https://nptel.ac.in/content/storage2/courses/122104019/numerical-analysis/Rathish-kumar/least-square/r1.htm", "isFamilyFriendly": true, "displayUrl": "https://nptel.ac.in/.../122104019/numerical-analysis/Rathish-kumar/<b>least-square</b>/r1.htm", "snippet": "2.4.2 <b>Least Square</b> Fit of <b>a Straight</b> <b>Line</b> Suppose that we are given a data <b>set</b> of observations from an experiment. Say that we are interested in <b>fitting</b> <b>a straight</b> <b>line</b> to the given data. Find the &#39; &#39; residuals by: Now consider the sum of the <b>squares</b> of i.e Note that is a function of parameters a and b. We need to find a,b such that is minimum.", "dateLastCrawled": "2022-02-03T02:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>What Is the Least Squares</b> <b>Regression</b> <b>Line</b>?", "url": "https://www.thoughtco.com/what-is-a-least-squares-line-3126250", "isFamilyFriendly": true, "displayUrl": "https://www.thoughtco.com/what-is-a-<b>least-squares-line</b>-3126250", "snippet": "Since the <b>least squares line</b> minimizes the squared distances between the <b>line</b> and our <b>points</b>, we can think of this <b>line</b> as the one that best fits our data. This is why the <b>least squares line</b> is also known as the <b>line</b> of best fit. Of all of the possible lines that could be drawn, the <b>least squares line</b> is closest to the <b>set</b> of data as a whole. This may mean that our <b>line</b> will miss hitting any of the <b>points</b> in our <b>set</b> of data.", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Least Squares Fitting of Data to</b> a Curve", "url": "https://web.cecs.pdx.edu/~gerry/nmm/course/slides/ch09Slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.cecs.pdx.edu/~gerry/nmm/course/slides/ch09Slides.pdf", "snippet": "Orthogonal Distance <b>Regression</b> problem. See, e.g., \u02daAke Bj\u00a8 ork, Numerical Methods for <b>Least</b> <b>Squares</b> Problems, 1996, SIAM, Philadelphia. y d 2 d 1 x 1 d 3 d 4 x 2 x 3 x 4 NMM: <b>Least</b> <b>Squares</b> Curve-<b>Fitting</b> page 7 . <b>Least</b> <b>Squares</b> Fit (1) The <b>least</b> <b>squares</b> \ufb01t is obtained by choosing the \u03b1 and \u03b2 so that Xm i=1 r2 i is a minimum. Let \u03c1 = r 2 2 to simplify the notation. Find \u03b1 and \u03b2 by minimizing \u03c1 = \u03c1(\u03b1,\u03b2). The minimum requires \u2202\u03c1 \u2202\u03b1 \u02db \u02db \u02db \u02db \u03b2=constant =0 and \u2202\u03c1 \u2202\u03b2 ...", "dateLastCrawled": "2022-02-02T05:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Applied Numerical Methods Curve Fitting: Least Squares Regression</b>, In\u2026", "url": "https://www.slideshare.net/brianerandio/numerical-method-curve-fitting-least-squares-regression-interpolation", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/brianerandio/numerical-method-curve-<b>fitting</b>-<b>least</b>-<b>squares</b>...", "snippet": "Linear <b>Regression</b> The simplest example of a <b>least</b>-<b>squares</b> approximation is <b>fitting</b> <b>a straight</b> <b>line</b> to <b>a set</b> of paired observations: (x1, y1), (x2, y2), . . . , (xn, yn). The mathematical expression for the <b>straight</b> <b>line</b> is y = a0 + a1x + e (17.1) \u2022\u2022Where a0 and a1 are coefficients representing the intercept and the slope, respectively, and e is the error, or residual, between the model and the observations, which can be represented by rearranging Eq. (17.1) as e = y \u2212 a0 \u2212 a1x Thus ...", "dateLastCrawled": "2022-01-27T22:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Least Square Regression Line - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/least-square-regression-line/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>least</b>-square-<b>regression</b>-<b>line</b>", "snippet": "Given <b>a set</b> of coordinates in the form of (X, Y), the task is to find the <b>least</b> <b>regression</b> <b>line</b> that can be formed.. In statistics, Linear <b>Regression</b> is a linear approach to model the relationship between a scalar response (or dependent variable), say Y, and one or more explanatory variables (or independent variables), say X. <b>Regression</b> <b>Line</b>: If our data shows a linear relationship between X and Y, then the <b>straight</b> <b>line</b> which best describes the relationship is the <b>regression</b> <b>line</b>.It is the ...", "dateLastCrawled": "2022-01-30T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Linear Regression</b>-Equation, Formula and Properties", "url": "https://byjus.com/maths/linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>linear-regression</b>", "snippet": "<b>Linear regression</b> determines the <b>straight</b> <b>line</b>, called the <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> or LSRL, that best expresses observations in a bivariate analysis of data <b>set</b>. Suppose Y is a dependent variable, and X is an independent variable, then the population <b>regression</b> <b>line</b> is given by; Y = B 0 +B 1 X. Where. B 0 is a constant. B 1 is the <b>regression</b> coefficient. If a random sample of observations is given, then the <b>regression</b> <b>line</b> is expressed by; \u0177 = b 0 + b 1 x. where b 0 is a constant, b 1 ...", "dateLastCrawled": "2022-02-02T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Lecture 2 Linear <b>Regression</b>: A Model for the Mean", "url": "http://www.columbia.edu/~so33/SusDev/Lecture2.pdf", "isFamilyFriendly": true, "displayUrl": "<b>www.columbia.edu</b>/~so33/SusDev/Lecture2.pdf", "snippet": "<b>Least</b> <b>Squares</b> Procedure(cont.) Note that the <b>regression</b> <b>line</b> always goes <b>through</b> the mean X, Y. Relation Between Yield and Fertilizer 0 20 40 60 80 100 0 100 200 300 400 500 600 700 800 Fertilizer (lb/Acre) Yield (Bushel/Acre) That is, for any value of the Trend <b>line</b> independent variable there is a single most likely value for the dependent ...", "dateLastCrawled": "2022-02-03T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Least</b> <b>squares</b> is a method of <b>fitting</b> a <b>regression</b> <b>line</b> which is robust ...", "url": "https://www.quora.com/Least-squares-is-a-method-of-fitting-a-regression-line-which-is-robust-i-e-safe-from-outliers-True-or-False", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Least</b>-<b>squares</b>-is-a-method-of-<b>fitting</b>-a-<b>regression</b>-<b>line</b>-which-is...", "snippet": "Answer (1 of 2): This is false. So it is \u201c<b>least</b> <b>squares</b>\u201d - the square of the residual is what you are looking to minimise. Consider your point with the highest residuals and move it some small amount. Consider how your <b>line</b> of best fit will move. Consider for a given small change how much your g...", "dateLastCrawled": "2022-01-08T16:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least Square Method</b> - Definition, Graph and Formula", "url": "https://byjus.com/maths/least-square-method/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>least-square-method</b>", "snippet": "The <b>least square method</b> is the process of finding the best-<b>fitting</b> curve or <b>line</b> of best fit for <b>a set</b> of data <b>points</b> by reducing the sum of the <b>squares</b> of the offsets (residual part) of the <b>points</b> from the curve. During the process of finding the relation between two variables, the trend of outcomes are estimated quantitatively. This process is termed as <b>regression</b> analysis.The method of curve <b>fitting</b> is an approach to <b>regression</b> analysis.", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>The Least Squares Regression Method</b> \u2013 How to Find the <b>Line</b> of Best Fit", "url": "https://www.freecodecamp.org/news/the-least-squares-regression-method-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.freecodecamp.org/news/<b>the-least-squares-regression-method</b>-explained", "snippet": "What is <b>the Least Squares Regression method</b> and why use it? <b>Least</b> <b>squares</b> is a method to apply linear <b>regression</b>. It helps us predict results based on an existing <b>set</b> of data as well as clear anomalies in our data. Anomalies are values that are too good, or bad, to be true or that represent rare cases. For example, say we have a list of how many topics future engineers here at freeCodeCamp can solve if they invest 1, 2, or 3 hours continuously. Then we can predict how many topics will be ...", "dateLastCrawled": "2022-02-03T15:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Chapter 7: <b>Correlation and Simple Linear Regression</b> \u2013 Natural Resources ...", "url": "https://milnepublishing.geneseo.edu/natural-resources-biometrics/chapter/chapter-7-correlation-and-simple-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://milnepublishing.geneseo.edu/.../chapter-7-<b>correlation-and-simple-linear-regression</b>", "snippet": "An ordinary <b>least</b> <b>squares</b> <b>regression</b> <b>line</b> minimizes the sum of the squared errors between the observed and predicted values to create a best <b>fitting</b> <b>line</b>. The differences between the observed and predicted values are squared to deal with the positive and negative differences.", "dateLastCrawled": "2022-02-02T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "As stated above, the method of <b>least</b> <b>squares</b> minimizes the sum of <b>squares</b> of the deviations of the <b>points</b> about the <b>regression</b> <b>line</b>. Consider the small data <b>set</b> illustrated in Fig. Fig.9. 9. This figure shows that, for a particular value of x, the distance of y from the mean of y (the total deviation) is the sum of the distance of the fitted y ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Linear Regression</b>-Equation, Formula and Properties", "url": "https://byjus.com/maths/linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>linear-regression</b>", "snippet": "<b>Linear regression</b> determines the <b>straight</b> <b>line</b>, called the <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> or LSRL, that best expresses observations in a bivariate analysis of data <b>set</b>. Suppose Y is a dependent variable, and X is an independent variable, then the population <b>regression</b> <b>line</b> is given by; Y = B 0 +B 1 X. Where. B 0 is a constant. B 1 is the <b>regression</b> coefficient. If a random sample of observations is given, then the <b>regression</b> <b>line</b> is expressed by; \u0177 = b 0 + b 1 x. where b 0 is a constant, b 1 ...", "dateLastCrawled": "2022-02-02T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>regression</b> - <b>Fitting</b> <b>a straight</b> <b>line</b>: Total <b>Least</b> <b>Squares</b> or Ordinary ...", "url": "https://stats.stackexchange.com/questions/240142/fitting-a-straight-line-total-least-squares-or-ordinary-least-squares", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/240142/<b>fitting</b>-<b>a-straight</b>-<b>line</b>-total-<b>least</b>...", "snippet": "I want to fit <b>a straight</b> <b>line</b> <b>through</b> a scatter plot of two timeseries to understand the influence sea surface temperatures (x-axis) have on land temperature over a particular region (y-axis). I have calculated the correlation coefficient which isn&#39;t particularly strong (0.16), but I also want to fit <b>a straight</b> <b>line</b> <b>through</b> this data, which is the part I&#39;m not sure about. For TLS (Total <b>Least</b> <b>Squares</b>) I have used", "dateLastCrawled": "2022-02-01T10:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>LEAST-SQUARES FITTING OF A STRAIGHT LINE</b>", "url": "http://stockage.univ-brest.fr/~herbette/Data-Analysis/york_cjp_1966_least-square_straight_line.pdf", "isFamilyFriendly": true, "displayUrl": "stockage.univ-brest.fr/.../Data-Analysis/york_cjp_1966_<b>least</b>-square_<b>straight</b>_<b>line</b>.pdf", "snippet": "This demonstrates that the best <b>line</b> goes <b>through</b> the center of gravity of the data (X, 7) when this point is defined as above. Eliminating a between equations (15) and (16) yields + C wiuiv, = 0, P where The slope of the best <b>straight</b> <b>line</b> is now given by solving equation (20) for b. We call equation (20) the &quot;<b>Least</b>-<b>Squares</b> Cubic&quot;. Before ...", "dateLastCrawled": "2022-02-03T12:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Applied Numerical Methods Curve Fitting: Least Squares Regression</b>, In\u2026", "url": "https://www.slideshare.net/brianerandio/numerical-method-curve-fitting-least-squares-regression-interpolation", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/brianerandio/numerical-method-curve-<b>fitting</b>-<b>least</b>-<b>squares</b>...", "snippet": "Linear <b>Regression</b> The simplest example of a <b>least</b>-<b>squares</b> approximation is <b>fitting</b> <b>a straight</b> <b>line</b> to <b>a set</b> of paired observations: (x1, y1), (x2, y2), . . . , (xn, yn). The mathematical expression for the <b>straight</b> <b>line</b> is y = a0 + a1x + e (17.1) \u2022\u2022Where a0 and a1 are coefficients representing the intercept and the slope, respectively, and e is the error, or residual, between the model and the observations, which can be represented by rearranging Eq. (17.1) as e = y \u2212 a0 \u2212 a1x Thus ...", "dateLastCrawled": "2022-01-27T22:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Up: Previous: Subsections - CHERIC", "url": "https://www.cheric.org/files/education/cyberlecture/e200113/e200113-801.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cheric.org/files/education/cyberlecture/e200113/e200113-801.pdf", "snippet": "Linear <b>Regression</b> A <b>least</b>-<b>squares</b> approximation is <b>fitting</b> <b>a straight</b> <b>line</b> to <b>a set</b> of paired observation. The mathematical expression for the <b>straight</b> <b>line</b> is The error, or residual, is the discrepancy between the true value of and the approximate value, and that is The criterion for <b>least</b>-<b>squares</b> <b>regression</b> is To determine values of and ...", "dateLastCrawled": "2022-01-31T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why is robust <b>regression</b> an alternative to <b>least</b> <b>squares</b> ... - Quora", "url": "https://www.quora.com/Why-is-robust-regression-an-alternative-to-least-squares-regression", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-robust-<b>regression</b>-an-alternative-to-<b>least</b>-<b>squares</b>-<b>regression</b>", "snippet": "Answer (1 of 2): I\u2019m going to assume that you are given a dataset and when you ran the <b>regression</b> (OLS), and checked for heteroscedasticity, the null of no het was rejected immediately (P-value &lt;0.05). Here is where it gets interesting. If you were to continue using the normal OLS where your obj...", "dateLastCrawled": "2022-01-08T23:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>What Is the Least Squares</b> <b>Regression</b> <b>Line</b>?", "url": "https://www.thoughtco.com/what-is-a-least-squares-line-3126250", "isFamilyFriendly": true, "displayUrl": "https://www.<b>thought</b>co.com/what-is-a-<b>least-squares-line</b>-3126250", "snippet": "Since the <b>least squares line</b> minimizes the squared distances between the <b>line</b> and our <b>points</b>, we <b>can</b> think of this <b>line</b> as the one that best fits our data. This is why the <b>least squares line</b> is also known as the <b>line</b> of best fit. Of all of the possible lines that could be drawn, the <b>least squares line</b> is closest to the <b>set</b> of data as a whole. This may mean that our <b>line</b> will miss hitting any of the <b>points</b> in our <b>set</b> of data.", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>Least Squares Regression Line</b> - GitHub Pages", "url": "https://saylordotorg.github.io/text_introductory-statistics/s14-04-the-least-squares-regression-l.html", "isFamilyFriendly": true, "displayUrl": "https://saylordotorg.github.io/.../s14-04-the-<b>least-squares-regression</b>-l.html", "snippet": "How well <b>a straight</b> <b>line</b> fits a data <b>set</b> is measured by the sum of the squared errors. The <b>least squares regression line</b> is the <b>line</b> that best fits the data. Its slope and y-intercept are computed from the data using formulas. The slope \u03b2 ^ 1 of the <b>least squares regression line</b> estimates the size and direction of the mean change in the dependent variable y when the independent variable x is increased by one unit. The sum of the squared errors S S E of the <b>least squares regression line</b> <b>can</b> ...", "dateLastCrawled": "2022-02-02T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "The equation of the <b>regression</b> <b>line</b> for the A&amp;E data (Fig. (Fig.7) 7) is as follows: ln urea = 0.72 + (0.017 \u00d7 age) (calculated using the method of <b>least</b> <b>squares</b>, which is described below). The gradient of this <b>line</b> is 0.017, which indicates that for an increase of 1 year in age the expected increase in ln urea is 0.017 units (and hence the expected increase in urea is 1.02 mmol/l). The predicted ln urea of a patient aged 60 years, for example, is 0.72 + (0.017 \u00d7 60) = 1.74 units. This ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Least Squares</b> Linear <b>Regression</b> In Python | by Cory Maklin | Towards ...", "url": "https://towardsdatascience.com/least-squares-linear-regression-in-python-54b87fc49e77", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>least-squares</b>-<b>line</b>ar-<b>regression</b>-in-python-54b87fc49e77", "snippet": "<b>Least Squares</b> Linear <b>Regression</b> In Python. Cory Maklin. Aug 16, 2019 \u00b7 6 min read. As the name implies, the method of <b>Least Squares</b> minimizes the sum of the <b>squares</b> of the residuals between the observed targets in the dataset, and the targets predicted by the linear approximation. In this proceeding article, we\u2019ll see how we <b>can</b> go about finding the best <b>fitting</b> <b>line</b> using linear algebra as opposed to something like gradient descent. Algorithm. Contrary to what I had initially <b>thought</b> ...", "dateLastCrawled": "2022-02-02T08:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "12 <b>Regression</b>\u2019", "url": "https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.colorado.edu</b>/amath/sites/default/files/attached-files/ch12_0.pdf", "snippet": "the)<b>line</b>)and)a)negative)number)if)it)liesbelow)the)<b>line</b> . The)residual)<b>can</b>)<b>be)thought</b>)of)asa)measure)of)deviation and we)<b>can</b>)summarize)the)notation)in)the)following)way: (x i, y\u02c6 i) Y i = 0 + 1x i + i \u21e1 \u02c6 0 + \u02c6 1x i +\u02c6 i = Y\u02c6 i +\u02c6 i) Y i Y\u02c6 i =\u02c6 i", "dateLastCrawled": "2022-01-30T05:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Biostatistics Series Module 6: Correlation and Linear <b>Regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5122272/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5122272", "snippet": "But how do we fit <b>a straight</b> <b>line</b> to a scattered <b>set</b> <b>of points</b> which seem to be in linear relationship? If the <b>points</b> are not all on a single <b>straight</b> <b>line</b>, we <b>can</b>, by eye estimation, draw multiple lines that seem to fit the series of data <b>points</b> on the scatter diagram. But which is the <b>line</b> of best fit? This problem had mathematicians stumped literally for centuries. The solution was in the form of the method of <b>least</b> <b>squares</b>, which was first published by the French mathematician Adrien ...", "dateLastCrawled": "2022-02-02T04:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Least-Squares Fitting of a Straight Line</b> - ResearchGate", "url": "https://www.researchgate.net/publication/237207593_Least-Squares_Fitting_of_a_Straight_Line", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../237207593_<b>Least-Squares_Fitting_of_a_Straight_Line</b>", "snippet": "Plotting these velocity components along a 350-km-long profile (extending from the eastern Qaidam basin to the Hexi Corridor) and <b>least</b>-square <b>fitting of a straight line</b> <b>through</b> the data (cf. York ...", "dateLastCrawled": "2022-01-18T23:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Chapter 5 Linear <b>regression</b> | Modern Statistical Methods for Psychology", "url": "https://bookdown.org/gregcox7/ims_psych/model-slr.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/gregcox7/ims_psych/model-slr.html", "snippet": "<b>Points</b> that fall horizontally far from the <b>line</b> are <b>points</b> of high leverage; these <b>points</b> <b>can</b> strongly influence the slope of the <b>least</b> <b>squares</b> <b>line</b>. If one of these high leverage <b>points</b> does appear to actually invoke its influence on the slope of the <b>line</b> \u2013 as in Plots C, D, and E of Figures 5.17 and 5.18 \u2013 then we call it an influential point .", "dateLastCrawled": "2022-01-31T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is the intuitive meaning of the solution to the problem of ... - Quora", "url": "https://www.quora.com/What-is-the-intuitive-meaning-of-the-solution-to-the-problem-of-finding-least-squares-linear-regression-coefficients", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-intuitive-meaning-of-the-solution-to-the-problem-of...", "snippet": "Answer (1 of 5): I don&#39;t know if this is a helpful way to look at it for you, but it certainly is different. In Y ~= X * B, we think of B as the operator, sending X to Y. But you could think of X as a transformation too, transforming B into Y. Viewed that way, solving this is like figuring out t...", "dateLastCrawled": "2022-01-25T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "plot - R draw (abline + lm) <b>line-of-best-fit</b> <b>through</b> arbitrary point ...", "url": "https://stackoverflow.com/questions/16140582/r-draw-abline-lm-line-of-best-fit-through-arbitrary-point", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/16140582", "snippet": "I am trying to draw a <b>least</b> <b>squares</b> <b>regression</b> <b>line</b> using abline(lm ... (x=10,y=50) while still minimising the distance to the other <b>points</b>? # force <b>through</b> [10,50] - red <b>line</b> ?? r plot <b>line</b> point <b>least</b>-<b>squares</b>. Share. Improve this question. Follow edited May 23 &#39;17 at 10:29. Community Bot. 1 1 1 silver badge. asked Apr 22 &#39;13 at 6:18. thelatemail thelatemail. 84.2k 12 12 gold badges 119 119 silver badges 174 174 bronze badges. Add a comment | 2 Answers Active Oldest Votes. 14 A rough ...", "dateLastCrawled": "2022-01-27T06:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Least Square Method</b> - Definition, Graph and Formula", "url": "https://byjus.com/maths/least-square-method/", "isFamilyFriendly": true, "displayUrl": "https://byjus.com/maths/<b>least-square-method</b>", "snippet": "The <b>least square method</b> is the process of finding the best-<b>fitting</b> curve or <b>line</b> of best fit for <b>a set</b> of data <b>points</b> by reducing the sum of the <b>squares</b> of the offsets (residual part) of the <b>points</b> from the curve. During the process of finding the relation between two variables, the trend of outcomes are estimated quantitatively. This process is termed as <b>regression</b> analysis.The method of curve <b>fitting</b> is an approach to <b>regression</b> analysis.", "dateLastCrawled": "2022-02-02T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Least Squares Regression</b> - How to Create <b>Line</b> of Best Fit?", "url": "https://www.wallstreetmojo.com/least-squares-regression/", "isFamilyFriendly": true, "displayUrl": "https://www.wallstreetmojo.com/<b>least-squares-regression</b>", "snippet": "<b>Line</b> of Best Fit in the <b>Least</b> Square <b>Regression</b>. The <b>line</b> of best fit is <b>a straight</b> <b>line</b> drawn <b>through</b> a scatter of data <b>points</b> that best represents the relationship between them. Let us consider the following graph wherein <b>a set</b> of data is plotted along the x and y-axis. These data <b>points</b> are represented using the blue dots. Three lines are ...", "dateLastCrawled": "2022-02-03T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Statistics review 7: Correlation and <b>regression</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC374386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC374386", "snippet": "The equation of the <b>regression</b> <b>line</b> for the A&amp;E data (Fig. (Fig.7) 7) is as follows: ln urea = 0.72 + (0.017 \u00d7 age) (calculated using the method of <b>least</b> <b>squares</b>, which is described below). The gradient of this <b>line</b> is 0.017, which indicates that for an increase of 1 year in age the expected increase in ln urea is 0.017 units (and hence the expected increase in urea is 1.02 mmol/l). The predicted ln urea of a patient aged 60 years, for example, is 0.72 + (0.017 \u00d7 60) = 1.74 units. This ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Chapter 7: <b>Correlation and Simple Linear Regression</b> \u2013 Natural Resources ...", "url": "https://milnepublishing.geneseo.edu/natural-resources-biometrics/chapter/chapter-7-correlation-and-simple-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://milnepublishing.geneseo.edu/.../chapter-7-<b>correlation-and-simple-linear-regression</b>", "snippet": "An ordinary <b>least</b> <b>squares</b> <b>regression</b> <b>line</b> minimizes the sum of the squared errors between the observed and predicted values to create a best <b>fitting</b> <b>line</b>. The differences between the observed and predicted values are squared to deal with the positive and negative differences.", "dateLastCrawled": "2022-02-02T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Mathematics for Machine Learning : Linear <b>Regression</b> &amp; <b>Least</b> Square ...", "url": "https://towardsdatascience.com/mathematics-for-machine-learning-linear-regression-least-square-regression-de09cf53757c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/mathematics-for-machine-learning-<b>line</b>ar-<b>regression</b>...", "snippet": "Equation of <b>Straight</b> <b>Line</b> from 2 <b>Points</b>. The e q uation of <b>a straight</b> <b>line</b> is written using the y = mx + b, where m is the slope (Gradient) and b is y-intercept (where the <b>line</b> crosses the Y axis). Once we get the equation of <b>a straight</b> <b>line</b> from 2 <b>points</b> in space in y = mx + b format, we <b>can</b> use the same equation to predict the <b>points</b> at ...", "dateLastCrawled": "2022-02-02T15:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A tutorial on <b>the total least squares method for fitting</b> <b>a straight</b> ...", "url": "https://www.researchgate.net/publication/272179120_A_tutorial_on_the_total_least_squares_method_for_fitting_a_straight_line_and_a_plane", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/272179120_A_tutorial_on_the_total_<b>least</b>...", "snippet": "A tutorial on <b>the total least squares method for fitting</b> <b>a straight</b> <b>line</b> and a plane 167 Abstract\u2014The classic <b>least</b> <b>squares</b> <b>regression</b> fits a <b>line</b> to data where errors may occur only in the ...", "dateLastCrawled": "2021-12-28T01:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Chapter 3 Multiple Linear Regression Model</b> The linear model", "url": "http://home.iitk.ac.in/~shalab/regression/Chapter3-Regression-MultipleLinearRegressionModel.pdf", "isFamilyFriendly": true, "displayUrl": "home.iitk.ac.in/~shalab/<b>regression</b>/Chapter3-<b>Regression</b>-<b>MultipleLinearRegressionModel</b>.pdf", "snippet": "So a simple linear <b>regression</b> model <b>can</b> be expressed as income education ... Principle of ordinary <b>least</b> <b>squares</b> (OLS) Let B be the <b>set</b> of all possible vectors . If there is no further information, the B is k-dimensional real Euclidean space. The object is to find a vector bbb b&#39; ( , ,..., ) 12 k from B that minimizes the sum of squared deviations of &#39; , i s i.e., 2 1 &#39; ( )&#39;( ) n i i S y X y X for given y and X. A minimum will always exist as S() is a real-valued, convex and differentiable ...", "dateLastCrawled": "2022-02-02T07:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Least-Squares Fitting of a Straight Line</b> - ResearchGate", "url": "https://www.researchgate.net/publication/237209736_Least-Squares_Fitting_of_a_Straight_Line", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../237209736_<b>Least-Squares_Fitting_of_a_Straight_Line</b>", "snippet": "In <b>least</b> square <b>fitting</b>, vertical <b>least</b> <b>squares</b> <b>fitting</b> proceeds by finding the sum of <b>squares</b> of the vertical derivations R 2 (refer Equation B.1) of <b>a set</b> of n data <b>points</b> [75]. a 1 , a 2", "dateLastCrawled": "2022-01-05T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Least</b> <b>squares</b> is a method of <b>fitting</b> a <b>regression</b> <b>line</b> which is robust ...", "url": "https://www.quora.com/Least-squares-is-a-method-of-fitting-a-regression-line-which-is-robust-i-e-safe-from-outliers-True-or-False", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Least</b>-<b>squares</b>-is-a-method-of-<b>fitting</b>-a-<b>regression</b>-<b>line</b>-which-is...", "snippet": "Answer (1 of 2): This is false. So it is \u201c<b>least</b> <b>squares</b>\u201d - the square of the residual is what you are looking to minimise. Consider your point with the highest residuals and move it some small amount. Consider how your <b>line</b> of best fit will move. Consider for a given small change how much your g...", "dateLastCrawled": "2022-01-08T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>DS 303 Midterm 1</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/273703181/ds-303-midterm-1-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/273703181/<b>ds-303-midterm-1</b>-flash-cards", "snippet": "The <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b> is fit to <b>a set</b> of data. If one of the data <b>points</b> has a positive residual, then Question options: A) the association between the values of the response and explanatory variables must be positive. B) the point must lie above the <b>least</b>-<b>squares</b> <b>regression</b> <b>line</b>. C) the point must lie near the right edge of the scatterplot. D) all of the above. B. A random sample of 79 companies from the Forbes 500 list (which actually consists of nearly 800 companies) was ...", "dateLastCrawled": "2021-11-08T06:35:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS 189/289A: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189s21/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189s21", "snippet": "LDA vs. logistic <b>regression</b>: advantages and disadvantages. ROC curves. Weighted <b>least</b>-<b>squares</b> <b>regression</b>. <b>Least</b>-<b>squares</b> polynomial <b>regression</b>. Read ISL, Sections 4.4.3, 7.1, 9.3.3; ESL, Section 4.4.1. Optional: here is a fine short discussion of ROC curves\u2014but skip the incoherent question at the top and jump straight to the answer.", "dateLastCrawled": "2022-01-31T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>CS 189/289A</b>: Introduction to <b>Machine</b> <b>Learning</b>", "url": "https://people.eecs.berkeley.edu/~jrs/189/", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~jrs/189", "snippet": "<b>regression</b>: <b>least</b>-<b>squares</b> linear <b>regression</b>, logistic <b>regression</b>, polynomial <b>regression</b>, ridge <b>regression</b>, Lasso; density estimation: maximum likelihood estimation (MLE); dimensionality reduction: principal components analysis (PCA), random projection; and clustering: k-means clustering, hierarchical clustering, spectral graph clustering. Useful Links. Access the <b>CS 189/289A</b> Piazza discussion group. If you want an instructional account, you can get one online. Go to the same link if you ...", "dateLastCrawled": "2022-02-02T17:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "A difficult <b>regression</b> parameter estimation problem is posed when the data sample is hypothesized to have been generated by more than a single <b>regression</b> model. To find the best-fitting number and ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "LSEbA: <b>least squares regression and estimation by analogy</b> in a semi ...", "url": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10664-010-9128-6", "snippet": "In this study, we indicatively applied the ordinary <b>least</b> <b>squares</b> <b>regression</b> and the estimation by <b>analogy</b> technique for the computation of the parametric and non-parametric part, respectively. However, there are lots of other well-known methods that can substitute the abovementioned methods and can be used for evaluation of these components. For example, practitioners may use a robust <b>regression</b> in the computation of the parametric portion of the proposed model in order to have a model less ...", "dateLastCrawled": "2021-12-03T23:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Big Problem with Linear <b>Regression</b> and How to Solve It | Towards Data ...", "url": "https://towardsdatascience.com/robust-regression-23b633e5d6a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/robust-<b>regression</b>-23b633e5d6a5", "snippet": "Introduction to Robust <b>Regression</b> in <b>Machine</b> <b>Learning</b>. Hussein Abdulrahman . Just now \u00b7 7 min read. The idea behind classic linear <b>regression</b> is simple: draw a \u201cbest-fit\u201d line across the data points that minimizes the mean squared errors: Classic linear <b>regression</b> with ordinary <b>least</b> <b>squares</b>. (Image by author) Looks good. But we don\u2019t always get such clean, well behaved data in real life. Instead, we may get something like this: Same algorithm as above, but now performing poorly due ...", "dateLastCrawled": "2022-02-01T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Machine</b> <b>learning</b> terminology for model building and validation There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear <b>regression</b> with two independent variables is trying to fit the best plane with\u2026", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Trends <b>in artificial intelligence, machine learning, and chemometrics</b> ...", "url": "https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ansa.202000162", "isFamilyFriendly": true, "displayUrl": "https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ansa.202000162", "snippet": "The derived spectra were analyzed for classification and quantification purposes using soft independent modeling of class <b>analogy</b> (SIMCA), artificial neural network (ANN), and partial <b>least</b> <b>squares</b> <b>regression</b> (PLSR). A good classification of tomatoes based on their carotenoid profile of 93% and 100% is shown using SIMCA and ANN, respectively. Besides this result, PLSR and ANN were able to achieve a good quantification of all-", "dateLastCrawled": "2022-02-01T19:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "econometrics - Principle of <b>Analogy</b> and Method of Moments - Cross Validated", "url": "https://stats.stackexchange.com/questions/272803/principle-of-analogy-and-method-of-moments", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/272803/principle-of-<b>analogy</b>-and-method-of...", "snippet": "<b>Least</b> <b>squares</b> estimator in the classical linear <b>regression</b> model is a Method of Moments estimator. The model is. y = X \u03b2 + u. Instead of minimizing the sum of squared residuals, we can obtain the OLS estimator by noting that under the assumptions of the specific model, it holds that (&quot;orhtogonality condition&quot;) E ( X \u2032 u) = 0.", "dateLastCrawled": "2022-01-25T20:40:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bayesian <b>Learning</b> - Rebellion Research", "url": "https://www.rebellionresearch.com/bayesian-learning", "isFamilyFriendly": true, "displayUrl": "https://www.rebellionresearch.com/bayesian-<b>learning</b>", "snippet": "Linear Regression example of <b>machine learning Least Squares Regression can be thought of as</b> a very limited <b>learning</b> algorithm, where the training set consists of a number of x and y data pairs. The task would be trying to predict the y value, and the performance measure would be the sum of the squared differences between the predicted and actual y\u2019s.", "dateLastCrawled": "2022-01-19T02:15:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(least squares regression)  is like +(fitting a straight line through a set of points)", "+(least squares regression) is similar to +(fitting a straight line through a set of points)", "+(least squares regression) can be thought of as +(fitting a straight line through a set of points)", "+(least squares regression) can be compared to +(fitting a straight line through a set of points)", "machine learning +(least squares regression AND analogy)", "machine learning +(\"least squares regression is like\")", "machine learning +(\"least squares regression is similar\")", "machine learning +(\"just as least squares regression\")", "machine learning +(\"least squares regression can be thought of as\")", "machine learning +(\"least squares regression can be compared to\")"]}