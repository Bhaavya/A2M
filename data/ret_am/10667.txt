{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[2022] What Is <b>Sequence-to-Sequence</b> Keras <b>Learning</b> and How To Perform ...", "url": "https://proxet.com/blog/how-to-perform-sequence-to-sequence-learning-in-keras/", "isFamilyFriendly": true, "displayUrl": "https://proxet.com/blog/how-to-perform-<b>sequence-to-sequence</b>-<b>learning</b>-in-keras", "snippet": "This is exactly how the algorithm of seq2seq <b>learning</b> looks <b>like</b>. Let\u2019s see what applications this model can be used for and the examples of <b>sequence-to-sequence</b> <b>learning</b> in Keras code. What is <b>Sequence-to-Sequence</b> <b>Learning</b>. <b>Sequence to Sequence</b> (also called seq2seq) models is a special class of Recurrent Neural Network architectures that is usually used for machine translation, question answering, development of chatbots, text summarization, and so on. How GPU Acceleration and Sequence ...", "dateLastCrawled": "2022-01-30T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "tensorflow - <b>sequence to sequence</b> <b>learning</b> for <b>language</b> translation ...", "url": "https://stackoverflow.com/questions/46962582/sequence-to-sequence-learning-for-language-translation-what-about-unseen-words", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/46962582", "snippet": "<b>sequence to sequence</b> <b>learning</b> is a powerful mechanism for <b>language</b> translation, especially using it locally in a context specific case. I am following this pytorch tutorial for the <b>task</b>. However, the tutorial did not split the data into training and testing. You might think its not a big deal, just split it up, use one chunk for training and ...", "dateLastCrawled": "2022-01-16T12:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "The most popular <b>sequence-to-sequence</b> <b>task</b> is translation: usually, from one natural <b>language</b> to another. In the last couple of years, commercial systems became surprisingly good at machine translation - check out, for example, Google Translate, Yandex Translate, DeepL Translator, Bing Microsoft Translator.Today we will learn about the core part of these systems.", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sequence-to-Sequence</b> <b>Language</b> Grounding of Non-Markovian <b>Task</b> Speci\ufb01cations", "url": "https://nakulgopalan.github.io/docs/sequence-sequence-language.pdf", "isFamilyFriendly": true, "displayUrl": "https://nakulgopalan.github.io/docs/sequence-sequence-<b>language</b>.pdf", "snippet": "neural <b>sequence-to-sequence</b> <b>learning</b> models can successfully ground <b>language</b> to this semantic representation, we also provide analysis that highlights generalization to novel, unseen logical forms as an open problem for this class of model. We evaluate our system within two simulated robot domains as well as on a physical robot, demonstrating accurate <b>language</b> grounding alongside a signi\ufb01cant expansion in the space of interpretable robot behaviors. I. INTRODUCTION The broad spectrum of ...", "dateLastCrawled": "2022-01-27T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "An <b>introduction to sequence-to-sequence learning</b>. Published: February 19, 2019. Many interesting problems in artificial intelligence can be described in the following way: Map a sequence of inputs $\\mathbf{x}$ to the correct sequence of outputs $\\mathbf{y}$. Speech recognition is one example: the goal is to map an audio signal $\\mathbf{x}$ (a sequence of real-valued audio samples) to the correct text transcript $\\mathbf{y}$ (a sequence of letters). Other examples are machine translation ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A ten-minute introduction to <b>sequence-to-sequence</b> <b>learning</b> in Keras", "url": "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html", "isFamilyFriendly": true, "displayUrl": "https://blog.keras.io/a-ten-minute-introduction-to-<b>sequence-to-sequence</b>-<b>learning</b>-in...", "snippet": "<b>Sequence-to-sequence</b> <b>learning</b> (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French). &quot;the cat sat on the mat&quot;-&gt; [Seq2Seq model]-&gt; &quot;le chat etait assis sur le tapis&quot; This can be used for machine translation or for free-from question answering (generating a natural <b>language</b> answer given a natural <b>language</b> question) -- in general, it is applicable any time you need to ...", "dateLastCrawled": "2022-01-29T00:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Natural <b>Language</b> Processing with Deep <b>Learning</b> CS224N/Ling284", "url": "https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture07-nmt.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture07-nmt.pdf", "snippet": "Natural <b>Language</b> Processing with Deep <b>Learning</b> CS224N/Ling284 Christopher Manning Lecture 7: Machine Translation, <b>Sequence-to-Sequence</b> and Attention. Lecture Plan Today we will: 1.Introduce <b>a new</b> <b>task</b>: Machine Translation [15 mins], which is a major use-case of 2.<b>A new</b> neural architecture: <b>sequence-to-sequence</b>[45 mins],which is improved by 3.<b>A new</b> neural technique: attention [20 mins] \u2022Announcements \u2022Please accept your Azure Lab Assignment (to get GPU)! Today!!! See Ed. \u2022Assignment 3 ...", "dateLastCrawled": "2022-02-01T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning</b> to Progressively Recognize <b>New</b> Named Entities with Sequence to ...", "url": "https://aclanthology.org/C18-1185.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C18-1185.pdf", "snippet": "<b>Learning</b> to Progressively Recognize <b>New</b> Named Entities with <b>Sequence to Sequence</b> Models Lingzhen Chen University of Trento Povo, Italy lingzhen.chen@unitn.it Alessandro Moschitti Amazon Manhattan Beach, CA, USA amosch@amazon.com Abstract In this paper, we propose to use a <b>sequence to sequence</b> model for Named Entity Recognition (NER) and we explore the effectiveness of such model in a progressive NER setting \u2013 a Transfer <b>Learning</b> (TL) setting. We train an initial model on source data and ...", "dateLastCrawled": "2022-02-02T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "paper review: \u201cBART: Denoising <b>Sequence-to-Sequence</b> Pre-training for ...", "url": "https://medium.com/mlearning-ai/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/paper-summary-bart-denoising-<b>sequence-to-sequence</b>-pre...", "snippet": "This configuration is to show that a pretrained BART model itself as a whole can be utilized by adding the small front encoder for machine translation <b>task</b> on <b>a new</b> <b>language</b>.", "dateLastCrawled": "2022-02-01T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Classical Structured Prediction Losses for Sequence to Sequence</b> <b>Learning</b>", "url": "https://aclanthology.org/N18-1033/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/N18-1033", "snippet": "\ufeff%0 Conference Proceedings %T <b>Classical Structured Prediction Losses for Sequence to Sequence</b> <b>Learning</b> %A Edunov, Sergey %A Ott, Myle %A Auli, Michael %A Grangier, David %A Ranzato, Marc\u2019Aurelio %S Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human <b>Language</b> Technologies, Volume 1 (Long Papers) %D 2018 %8 jun %I Association for Computational Linguistics %C <b>New</b> Orleans, Louisiana %F edunov-etal-2018-classical %X There ...", "dateLastCrawled": "2022-01-19T08:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[2022] What Is <b>Sequence-to-Sequence</b> Keras <b>Learning</b> and How To Perform ...", "url": "https://proxet.com/blog/how-to-perform-sequence-to-sequence-learning-in-keras/", "isFamilyFriendly": true, "displayUrl": "https://proxet.com/blog/how-to-perform-<b>sequence-to-sequence</b>-<b>learning</b>-in-keras", "snippet": "An equally important <b>task</b> for any successful data scientist or analyst is to identify and create <b>new</b> tasks that can be solved analytically. The latter is a very different exercise and does not need a lot of coding experience or mathematical background. All you need to know is what is possible and what is not, using a given tool. \u201d \u2014 Tavish Srivastava, Co-Founder and Chief Strategy Officer of Analytics. Here\u2019s an interesting example of the LSTM model in Keras \u2014 the development of a ...", "dateLastCrawled": "2022-01-30T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Multi-<b>task</b> <b>Sequence to Sequence Learning</b> | DeepAI", "url": "https://deepai.org/publication/multi-task-sequence-to-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/multi-<b>task</b>-<b>sequence-to-sequence-learning</b>", "snippet": "Despite the popularity of multi-<b>task</b> <b>learning</b> and <b>sequence to sequence learning</b>, there has been little work in combining MTL with seq2seq <b>learning</b>. To the best of our knowledge, there is only one recent publication by Dong et al. which applies a seq2seq models for machine translation, where the goal is to translate from one <b>language</b> to multiple languages. In this work, we propose three MTL approaches that complement one another: (a) the one-to-many approach \u2013 for tasks that can have an ...", "dateLastCrawled": "2022-02-02T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "The most popular <b>sequence-to-sequence</b> <b>task</b> is translation: usually, from one natural <b>language</b> to another. In the last couple of years, commercial systems became surprisingly good at machine translation - check out, for example, Google Translate, Yandex Translate, DeepL Translator, Bing Microsoft Translator. Today we will learn about the core part of these systems. Except for the popular machine translation between natural languages, you can translate between programming languages (see e.g ...", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "paper review: \u201cBART: Denoising <b>Sequence-to-Sequence</b> Pre-training for ...", "url": "https://medium.com/mlearning-ai/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/paper-summary-bart-denoising-<b>sequence-to-sequence</b>-pre...", "snippet": "This configuration is to show that a pretrained BART model itself as a whole can be utilized by adding the small front encoder for machine translation <b>task</b> on <b>a new</b> <b>language</b>.", "dateLastCrawled": "2022-02-01T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sequence to Sequence Learning with Neural Networks</b>", "url": "https://wandb.ai/authors/seq2seq/reports/Sequence-to-Sequence-Learning-with-Neural-Networks--Vmlldzo0Mzg0MTI", "isFamilyFriendly": true, "displayUrl": "https://wandb.ai/authors/seq2seq/reports/<b>Sequence-to-Sequence-Learning-with</b>-Neural...", "snippet": "<b>Task</b>. The <b>task</b> is to emulate a translator. We will be given a sentence in a given <b>language</b> which we need to translate into another <b>language</b>. Prior to this paper, this <b>task</b> was viewed as a mapping <b>task</b>, where systems had to map <b>similar</b> contextual phrases in different languages. This way of approaching the problem was too tough for a <b>learning</b> system.", "dateLastCrawled": "2022-01-13T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sequence to Sequence</b> Coreference Resolution", "url": "https://aclanthology.org/2020.crac-1.5.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2020.crac-1.5.pdf", "snippet": "paper we present <b>a new</b> approach to face coreference resolution as a <b>sequence to sequence</b> <b>task</b> based on the Transformer architecture. This approach is simple and universal, compatible with any <b>language</b> or dataset (regardless of singletons) and easier to integrate with current <b>language</b> models architectures. We test it on the ARRAU corpus, where we get 65.6 F1 CoNLL. We see this approach not as a \ufb01nal goal, but a means to pretrain <b>sequence to sequence</b> <b>language</b> models (T5) on coreference ...", "dateLastCrawled": "2021-11-20T14:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Learning</b> to Progressively Recognize <b>New</b> Named Entities with Sequence to ...", "url": "https://aclanthology.org/C18-1185.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C18-1185.pdf", "snippet": "<b>Learning</b> to Progressively Recognize <b>New</b> Named Entities with <b>Sequence to Sequence</b> Models Lingzhen Chen University of Trento Povo, Italy lingzhen.chen@unitn.it Alessandro Moschitti Amazon Manhattan Beach, CA, USA amosch@amazon.com Abstract In this paper, we propose to use a <b>sequence to sequence</b> model for Named Entity Recognition (NER) and we explore the effectiveness of such model in a progressive NER setting \u2013 a Transfer <b>Learning</b> (TL) setting. We train an initial model on source data and ...", "dateLastCrawled": "2022-02-02T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "AI Quantification of <b>Language</b> Puzzle to <b>Language</b> <b>Learning</b> ...", "url": "http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "snippet": "quantifying the effectiveness of these games in <b>learning</b> <b>a new</b> <b>language</b>. Secondarily my goal for this project is to measure the effectiveness of exercises for transfer <b>learning</b> in machine translation. As part of CS297, I worked on projects involving different artificial intelligence topics. Mainly the focus was on different types of neural networks and their implementations. <b>Language</b> modeling and <b>sequence-to-sequence</b> <b>learning</b> were topics providing possible insight into the end goal of the ...", "dateLastCrawled": "2021-11-13T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Convolutional Sequence to Sequence Learning</b>", "url": "http://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v70/gehring17a/gehring17a.pdf", "snippet": "English-Romanian translation we achieve <b>a new</b> state of the art, outperforming the previous best result by 1.9 BLEU. On WMT\u201914 English-German we outperform the strong LSTM setup of Wu et al. (2016)by0.5BLEUandonWMT\u201914 English-French we outperform the likelihood trained system of Wu et al. (2016)by1.6BLEU.Furthermore,ourmodelcan translate unseen sentences at an order of magnitude faster speed than Wu et al. (2016)onGPUandCPUhardware(\u00a74,\u00a75). 2. Recurrent <b>Sequence to Sequence</b> <b>Learning</b> ...", "dateLastCrawled": "2022-02-02T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>complete guide to transfer learning from English to</b> other Languages ...", "url": "https://towardsdatascience.com/a-complete-guide-to-transfer-learning-from-english-to-other-languages-using-sentence-embeddings-8c427f8804a9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-<b>complete-guide-to-transfer-learning-from-english-to</b>...", "snippet": "Same CBOW embedding, even though their meanings are opposite (Image by author). Seq2seq models[4] like RNNs, LSTMs, GRUs, etc.. on the other hand, take a sequence of items (words, letters, time series, etc) and outputs another sequence of items by processing word embeddings one by one while maintaining a hidden state that stores context information.The embedding produced by encoder captures the context of the input sequence in the form of a hidden state vector and sends it to the decoder ...", "dateLastCrawled": "2022-01-25T06:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "MULTI <b>TASK</b> <b>SEQUENCE TO SEQUENCE</b> <b>LEARNING</b>", "url": "https://nlp.stanford.edu/pubs/luong2016iclr_multi.pdf", "isFamilyFriendly": true, "displayUrl": "https://nlp.stanford.edu/pubs/luong2016iclr_multi.pdf", "snippet": "less in terms of perplexities but more on BLEU scores comparedto skip-<b>thought</b>. 1 INTRODUCTION Multi-<b>task</b> <b>learning</b> (MTL) is an important machine <b>learning</b> paradigm that aims at improving the generalization performance of a <b>task</b> using other related tasks. Such framework has been widelystudiedbyThrun(1996);Caruana(1997);Evgeniou&amp; Pontil(2004);Ando &amp; Zhang(2005); Argyriouet al. (2007); Kumar &amp; III (2012), among many others. In the context of deep neural net-works, MTL has been applied ...", "dateLastCrawled": "2021-11-23T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Multi-task Sequence to Sequence Learning</b> | DeepMind", "url": "https://deepmind.com/research/publications/2019/multi-task-sequence-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepmind.com/research/publications/2019/multi-<b>task</b>-sequence-sequence-<b>learning</b>", "snippet": "<b>Sequence to sequence learning</b> has recently emerged as <b>a new</b> paradigm in supervised <b>learning</b>. To date, most of its applications focused on only one <b>task</b> and not much work explored this framework for multiple tasks. This paper examines three settings to <b>multi-task sequence to sequence learning</b>: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder <b>can</b> be ...", "dateLastCrawled": "2021-12-07T23:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "MIT Presents <b>New</b> Approach for <b>Sequence-to-Sequence</b> <b>Learning</b> with Latent ...", "url": "https://medium.com/syncedreview/mit-presents-new-approach-for-sequence-to-sequence-learning-with-latent-neural-grammars-60cd5dfe8f32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/syncedreview/mit-presents-<b>new</b>-approach-for-<b>sequence-to-sequence</b>...", "snippet": "<b>Sequence to sequence</b> modelling (seq2seq) with neural networks has become the de facto standard for sequence prediction tasks such as those found in <b>language</b> modelling and machine translation.", "dateLastCrawled": "2021-11-21T23:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Seq2seq and <b>Attention</b> - GitHub Pages", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html", "isFamilyFriendly": true, "displayUrl": "https://lena-voita.github.io/nlp_course/seq2seq_and_<b>attention</b>.html", "snippet": "The most popular <b>sequence-to-sequence</b> <b>task</b> is translation: usually, from one natural <b>language</b> to another. In the last couple of years, commercial systems became surprisingly good at machine translation - check out, for example, Google Translate, Yandex Translate, DeepL Translator, Bing Microsoft Translator.Today we will learn about the core part of these systems.", "dateLastCrawled": "2022-02-02T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sequence to Sequence Learning with Neural Networks</b>", "url": "https://wandb.ai/authors/seq2seq/reports/Sequence-to-Sequence-Learning-with-Neural-Networks--Vmlldzo0Mzg0MTI", "isFamilyFriendly": true, "displayUrl": "https://wandb.ai/authors/seq2seq/reports/<b>Sequence-to-Sequence-Learning-with</b>-Neural...", "snippet": "Introduction. In the age of attention and transformers, I <b>thought</b> writing a simple report on <b>sequence to sequence</b> modelling thinking it would be a good starting point for a lot of people. In this article I will try declassifying the paper <b>Sequence to Sequence Learning with Neural Networks</b> by Ilya Sutskever et. al. Here in this paper, the authors have presented an end to end <b>learning</b> system that helps in translating one <b>language</b> into another.", "dateLastCrawled": "2022-01-13T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[1511.06114] Multi-<b>task</b> <b>Sequence to Sequence Learning</b>", "url": "https://arxiv.org/abs/1511.06114", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1511.06114", "snippet": "<b>Sequence to sequence learning</b> has recently emerged as <b>a new</b> paradigm in supervised <b>learning</b>. To date, most of its applications focused on only one <b>task</b> and not much work explored this framework for multiple tasks. This paper examines three multi-<b>task</b> <b>learning</b> (MTL) settings for <b>sequence to sequence</b> models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder ...", "dateLastCrawled": "2021-07-27T17:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An <b>introduction to sequence-to-sequence learning</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2019/02/seq2seq/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2019/02/seq2seq", "snippet": "An <b>introduction to sequence-to-sequence learning</b>. Published: February 19, 2019. Many interesting problems in artificial intelligence <b>can</b> be described in the following way: Map a sequence of inputs $\\mathbf{x}$ to the correct sequence of outputs $\\mathbf{y}$. Speech recognition is one example: the goal is to map an audio signal $\\mathbf{x}$ (a sequence of real-valued audio samples) to the correct text transcript $\\mathbf{y}$ (a sequence of letters). Other examples are machine translation ...", "dateLastCrawled": "2022-02-02T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Making Predictions with Sequences - Machine <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/sequence-prediction/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/sequence-prediction", "snippet": "<b>Sequence to Sequence</b> Prediction; Sequence. Often we deal with sets in applied machine <b>learning</b> such as a train or test sets of samples. Each sample in the set <b>can</b> <b>be thought</b> of as an observation from the domain. In a set, the order of the observations is not important. A sequence is different. The sequence imposes an explicit order on the ...", "dateLastCrawled": "2022-02-02T04:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[1511.06114v1] <b>Multi-task Sequence to Sequence Learning</b>", "url": "https://arxiv.org/abs/1511.06114v1", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1511.06114v1", "snippet": "Abstract: <b>Sequence to sequence learning</b> has recently emerged as <b>a new</b> paradigm in supervised <b>learning</b>. To date, most of its applications focused on only one <b>task</b> and not much work explored this framework for multiple tasks. This paper examines three settings to <b>multi-task sequence to sequence learning</b>: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder ...", "dateLastCrawled": "2022-01-24T08:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is there an implementation of the LSTM in the <b>Sequence to Sequence</b> ...", "url": "https://www.quora.com/Is-there-an-implementation-of-the-LSTM-in-the-Sequence-to-Sequence-Learning-paper", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-an-implementation-of-the-LSTM-in-the-Sequence-to...", "snippet": "Answer (1 of 3): Yes and No. Yes, because there are plenty of RNN and LSTM packages out there. Using Torch, Theano or Caffe, it&#39;s relatively easy to implement the model described in the paper. Specifically, if you want to do Neural Machine Translation, which is the experiment mentioned in the pa...", "dateLastCrawled": "2022-01-14T21:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "paper review: \u201cBART: Denoising <b>Sequence-to-Sequence</b> Pre-training for ...", "url": "https://medium.com/mlearning-ai/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/paper-summary-bart-denoising-<b>sequence-to-sequence</b>-pre...", "snippet": "This configuration is to show that a pretrained BART model itself as a whole <b>can</b> be utilized by adding the small front encoder for machine translation <b>task</b> on <b>a new</b> <b>language</b>.", "dateLastCrawled": "2022-02-01T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Multi-task Sequence to Sequence Learning</b> | DeepMind", "url": "https://deepmind.com/research/publications/2019/multi-task-sequence-sequence-learning", "isFamilyFriendly": true, "displayUrl": "https://deepmind.com/research/publications/2019/multi-<b>task</b>-sequence-sequence-<b>learning</b>", "snippet": "<b>Sequence to sequence learning</b> has recently emerged as <b>a new</b> paradigm in supervised <b>learning</b>. To date, most of its applications focused on only one <b>task</b> and not much work explored this framework for multiple tasks. This paper examines three settings to <b>multi-task sequence to sequence learning</b>: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder <b>can</b> be ...", "dateLastCrawled": "2021-12-07T23:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "paper review: \u201cBART: Denoising <b>Sequence-to-Sequence</b> Pre-training for ...", "url": "https://chadrick-kwag.medium.com/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe", "isFamilyFriendly": true, "displayUrl": "https://chadrick-kwag.medium.com/paper-summary-bart-denoising-<b>sequence-to-sequence</b>-pre...", "snippet": "This configuration is to show that a pretrained BART model itself as a whole <b>can</b> be utilized by adding the small front encoder for machine translation <b>task</b> on <b>a new</b> <b>language</b>. The existing BART\u2019s first encoder\u2019s embedding layer is replaced to a randomly initialized encoder, and then the entire model is trained end-to-end. This <b>new</b> encoder ...", "dateLastCrawled": "2022-01-11T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "[1511.06114v3] <b>Multi-task Sequence to Sequence Learning</b>", "url": "https://arxiv.org/abs/1511.06114v3", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1511.06114v3", "snippet": "<b>Sequence to sequence learning</b> has recently emerged as <b>a new</b> paradigm in supervised <b>learning</b>. To date, most of its applications focused on only one <b>task</b> and not much work explored this framework for multiple tasks. This paper examines three settings to <b>multi-task sequence to sequence learning</b>: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder <b>can</b> be ...", "dateLastCrawled": "2021-11-14T09:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Progress in Neural NLP: Modeling, <b>Learning</b>, and Reasoning", "url": "https://www.sciencedirect.com/science/article/pii/S2095809919304928", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2095809919304928", "snippet": "Many NLP tasks <b>can</b> be formulated as a <b>sequence-to-sequence</b> <b>task</b>, such as MT (i.e., given the source <b>language</b> word sequence, generate the target <b>language</b> word sequence), QA (i.e., given the word sequence of a question, generate the word sequence of an answer), and dialog (i.e., given the word sequence of user input, generate the word sequence of response). 2.2.2. Encoder\u2013decoder framework. Ref. proposed an encoder\u2013decoder framework for <b>sequence-to-sequence</b> modeling. As shown in Fig. 5 ...", "dateLastCrawled": "2022-01-30T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Candidate fusion: Integrating language modelling</b> into a sequence-to ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320320305938", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320320305938", "snippet": "Moreover, it provides suggestions from an external <b>language</b> knowledge, as <b>a new</b> input to the <b>sequence-to-sequence</b> recognizer. Hence, Candidate Fusion provides two improvements. On the one hand, the <b>sequence-to-sequence</b> recognizer has the flexibility to not only combine the information from itself and the <b>language</b> model, but also choose the importance of the information provided by the <b>language</b> model. On the other hand, the external <b>language</b> model has the ability to adapt itself to the ...", "dateLastCrawled": "2021-12-17T10:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "AI Quantification of <b>Language</b> Puzzle to <b>Language</b> <b>Learning</b> ...", "url": "http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Fall19/harita/Project_report.pdf", "snippet": "quantifying the effectiveness of these games in <b>learning</b> <b>a new</b> <b>language</b>. Secondarily my goal for this project is to measure the effectiveness of exercises for transfer <b>learning</b> in machine translation. As part of CS297, I worked on projects involving different artificial intelligence topics. Mainly the focus was on different types of neural networks and their implementations. <b>Language</b> modeling and <b>sequence-to-sequence</b> <b>learning</b> were topics providing possible insight into the end goal of the ...", "dateLastCrawled": "2021-11-13T22:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Sequence-to-sequence learning with Transducers</b> - Loren Lugosch", "url": "https://lorenlugosch.github.io/posts/2020/11/transducer/", "isFamilyFriendly": true, "displayUrl": "https://lorenlugosch.github.io/posts/2020/11/transducer", "snippet": "<b>Sequence-to-sequence learning with Transducers</b>. Published: November 16, 2020 The Transducer (sometimes called the \u201cRNN Transducer\u201d or \u201cRNN-T\u201d, though it need not use RNNs) is a <b>sequence-to-sequence</b> model proposed by Alex Graves in \u201cSequence Transduction with Recurrent Neural Networks\u201d. The paper was published at the ICML 2012 Workshop on Representation <b>Learning</b>.Graves showed that the Transducer was a sensible model to use for speech recognition, achieving good results on a small ...", "dateLastCrawled": "2022-01-31T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Presenting Multitask Learning as Question Answering</b>: The Natural ...", "url": "https://www.techleer.com/articles/527-presenting-multitask-learning-as-question-answering-the-natural-language-decathlon/", "isFamilyFriendly": true, "displayUrl": "https://www.techleer.com/articles/527-<b>presenting-multitask-learning-as-question</b>...", "snippet": "Individually Deep <b>learning</b> on many Natural <b>Language</b> Processing (NLP) tasks has improved performance. However, general NLP models that focuses on the particularities of a single metric, dataset, and <b>task</b> cannot emerge within the same paradigm. Here you are presented a challenge that is spanned around ten tasks: Question answering, Machine translation, Summarization, Natural <b>Language</b> Inference, Sentiment analysis, Semantic role labeling, Relation extraction, Goal-oriented dialogue, Semantic ...", "dateLastCrawled": "2022-01-18T10:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "CLIP vs Vision <b>Language</b> Pre-training Vs VisionEncoderDecoder", "url": "https://analyticsindiamag.com/clip-vs-vision-language-pre-training-vs-visionencoderdecoder/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/clip-vs-vision-<b>language</b>-pre-training-vs-visionencoderdecoder", "snippet": "Multimodal <b>learning</b>. Released in January last year, Contrastive <b>Language</b>\u2013Image Pre-training, or CLIP, is built on a large body of work on zero-shot transfer, natural <b>language</b> supervision, and multimodal <b>learning</b>. OpenAI showed that scaling a simple pre-training <b>task</b> is sufficient to achieve competitive zero-shot performance on a wide range of image classification datasets. This method uses available sources of supervision \u2013 the text paired with images found on the internet. The data is ...", "dateLastCrawled": "2022-02-01T16:59:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The language of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Popular deep-<b>learning</b> architectures are long short-term memory (LSTM) , <b>sequence-to-sequence</b> (seq2seq) and attention . In seq2seq models, a text is transformed using an encoder component, then a separate decoder uses the encoded representation to solve some <b>task</b> (e.g. translating between English and French). Attention models use attention layers (also called attention heads) that allow the network to concentrate on specific tokens in the text", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is the race over for <b>Seq2Seq</b> models? | by Thushan Ganegedara | Towards ...", "url": "https://towardsdatascience.com/is-the-race-over-for-seq2seq-models-adef2b24841c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/is-the-race-over-for-<b>seq2seq</b>-models-adef2b24841c", "snippet": "This goes for any <b>machine</b> <b>learning</b> <b>task</b>, be it <b>machine</b> translation, dependency parsing or language modelling. Self-attention layer enables to transformer to exactly do that. While processing the word \u201cits\u201d, the model can look at all the other words and decide for itself which words are important to \u201c mix \u201d into the output, so that the transformer can solve the <b>task</b> effectively.", "dateLastCrawled": "2022-02-02T02:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "When you talk about <b>Machine</b> <b>Learning</b> in Natural Language Processing these days, all you hear is one thing \u2013 Transformers. Models based on this Deep <b>Learning</b> architecture have taken the NLP world by storm since 2017. In fact, they are the go-to approach today, and many of the approaches build on top of the original Transformer, one way or another. Transformers are however not simple. The original Transformer architecture is quite complex and the same is true for many of the spin-off ...", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "9.7. <b>Sequence to Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "As we have seen in Section 9.5, in <b>machine</b> translation both the input and output are a variable-length <b>sequence</b>.To address this type of problem, we have designed a general encoder-decoder architecture in Section 9.6.In this section, we will use two RNNs to design the encoder and the decoder of this architecture and apply it to <b>sequence to sequence</b> <b>learning</b> for <b>machine</b> translation [Sutskever et al., 2014] [Cho et al., 2014b].. Following the design principle of the encoder-decoder architecture ...", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Geometric deep <b>learning</b> on molecular representations | Nature <b>Machine</b> ...", "url": "https://www.nature.com/articles/s42256-021-00418-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42256-021-00418-8", "snippet": "In <b>analogy</b> to some popular pre-deep <b>learning</b> ... which can be cast as a <b>sequence-to-sequence</b> translation <b>task</b> in which the string representations of the reactants are mapped to those of the ...", "dateLastCrawled": "2022-01-29T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning Gist</b> \u00b7 GitHub", "url": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "isFamilyFriendly": true, "displayUrl": "https://gist.github.com/sgoyal1012/b30d70d12b6efad88bb285e8e709b161", "snippet": "Week 3: <b>Sequence to sequence</b> architectures. <b>Sequence to sequence</b> models Language translation for example; Image captioning, caption an image; Picking the most likely model <b>Machine</b> Transation Model Split into a model encoding the sentence; and then a language model. Calculate the probability of an English sentence conditioned on a French sentence.", "dateLastCrawled": "2022-01-29T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Benefits of AI and Deep <b>Learning</b> - <b>Machine</b> <b>Learning</b> Company ...", "url": "https://www.folio3.ai/blog/advantages-of-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.folio3.ai/blog/<b>advantages-of-neural-networks</b>", "snippet": "<b>Sequence-To-Sequence</b> models are mainly applied in question answering, <b>machine</b> translations systems, and chatbots. What Are The <b>Advantages of Neural Networks</b> . There are various <b>advantages of neural networks</b>, some of which are discussed below: 1) Store information on the entire network. Just like it happens in traditional programming where information is stored on the network and not on a database. If a few pieces of information disappear from one place, it does not stop the whole network ...", "dateLastCrawled": "2022-02-02T09:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (LSTM) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sequence Classification with LSTM Recurrent Neural Networks in Python ...", "url": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/sequence-classification-", "snippet": "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the <b>task</b> is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep-<b>learning</b>-coursera/Week 1 Quiz - Introduction to deep <b>learning</b>.md ...", "url": "https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Kulbear/deep-<b>learning</b>-coursera/blob/master/Neural Networks and Deep...", "snippet": "Week 1 Quiz - Introduction to deep <b>learning</b>. What does the <b>analogy</b> \u201cAI is the new electricity\u201d refer to? AI is powering personal devices in our homes and offices, similar to electricity. Through the \u201csmart grid\u201d, AI is delivering a new wave of electricity. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Similar to electricity starting about 100 years ago, AI is transforming multiple industries. Note: Andrew ...", "dateLastCrawled": "2022-02-03T05:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sequence-to-sequence task)  is like +(learning a new language)", "+(sequence-to-sequence task) is similar to +(learning a new language)", "+(sequence-to-sequence task) can be thought of as +(learning a new language)", "+(sequence-to-sequence task) can be compared to +(learning a new language)", "machine learning +(sequence-to-sequence task AND analogy)", "machine learning +(\"sequence-to-sequence task is like\")", "machine learning +(\"sequence-to-sequence task is similar\")", "machine learning +(\"just as sequence-to-sequence task\")", "machine learning +(\"sequence-to-sequence task can be thought of as\")", "machine learning +(\"sequence-to-sequence task can be compared to\")"]}