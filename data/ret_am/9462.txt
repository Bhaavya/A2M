{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Teaching</b> a 15 year old <b>backpropagation</b> - Carina Kaltenbach", "url": "https://carina-kaltenbach.com/mentoring-101", "isFamilyFriendly": true, "displayUrl": "https://carina-kaltenbach.com/mentoring-101", "snippet": "<b>Like</b> <b>how to do</b> <b>something</b> right. Or why there\u2019s this weird convention everyone follows. <b>Teaching</b> <b>someone</b> means you can\u2019t hack your way around the problem, but you have to actually explain how it is done, and why it is done. Be <b>like</b> the top answers on Stack Overflow. And if all of that doesn\u2019t convince you, it\u2019s a great way of procrastinating. You won\u2019t be doing the actual important tasks in your life, but you won\u2019t spend your time on Netflix or YouTube, so who can judge you for it ...", "dateLastCrawled": "2021-12-11T21:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Step by Step <b>Backpropagation</b> Example \u2013 Matt Mazur", "url": "https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/comment-page-5/", "isFamilyFriendly": true, "displayUrl": "https://mattmazur.com/2015/03/17/a-step-by-step-<b>backpropagation</b>-example/comment-page-5", "snippet": "Background. <b>Backpropagation</b> is a common method for training a neural network. There is no shortage of papers online that attempt to explain how <b>backpropagation</b> works, but few that include an example with actual numbers. This post is my attempt to explain how it works with a concrete example that folks can compare their own calculations to in order to ensure they understand <b>backpropagation</b> correctly.", "dateLastCrawled": "2022-01-27T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to answer questions about backpropagating specific architectures or ...", "url": "https://stats.meta.stackexchange.com/questions/5648/how-to-answer-questions-about-backpropagating-specific-architectures-or-function", "isFamilyFriendly": true, "displayUrl": "https://stats.meta.stackexchange.com/questions/5648/how-to-answer-questions-about-back...", "snippet": "There are quite a few questions asking how to run <b>backpropagation</b> when dealing with skip connections, residual networks, &quot;mixed&quot; RNN-CNNs, attention mechanisms, etc. I suspect the reason for the", "dateLastCrawled": "2021-11-29T11:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Talk:Backpropagation</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Talk:Backpropagation", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Talk:Backpropagation</b>", "snippet": "<b>Backpropagation</b> is within the scope of WikiProject Robotics, which aims to build a comprehensive and detailed guide to Robotics on <b>Wikipedia</b>. If you would <b>like</b> to participate, you can choose to , or visit the project page (), where you can join the project and see a list of open tasks. Start This article has been rated as Start-Class on the project&#39;s quality scale. Mid This article has been rated as Mid-importance on the project&#39;s importance scale", "dateLastCrawled": "2021-08-13T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Is there neurological evidence the brain does <b>something</b> <b>like</b> backprop ...", "url": "https://www.quora.com/Is-there-neurological-evidence-the-brain-does-something-like-backprop", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-neurological-evidence-the-brain-does-<b>something</b>-<b>like</b>...", "snippet": "Answer (1 of 3): Not exactly. First, let&#39;s make a quick distinction. <b>Backpropagation</b> a mechanism which changes all of a networks weights in a certain direction to change the output of the network towards a specified goal. If you remove the neural metaphor, what this actually looks <b>like</b> is changi...", "dateLastCrawled": "2022-01-09T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Backpropagation</b> is a leaky abstraction | Hacker News", "url": "https://news.ycombinator.com/item?id=13215590", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=13215590", "snippet": "The &quot;code reuse as much as possible&quot; mantra applies to when you&#39;re &quot;using&quot; a technique to <b>do</b> <b>something</b> else, not when you&#39;re &quot;learning&quot; the technique itself. They might as well register for Calculus and ask &quot;why learn integrals and derivatives when mathematica can <b>do</b> them for you&quot;, or take an Aerodynamics class and ask &quot;why learn fluid mechanics and dynamics, heck newton&#39;s laws, when an airplane can run on autopilot.&quot; I doubt &quot;because calculus is a leaky abstraction&quot; or &quot;because fluid ...", "dateLastCrawled": "2021-07-06T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Explaining Feedforward, <b>Backpropagation</b> and Optimization: The Math ...", "url": "https://www.reddit.com/r/datascience/comments/cmgz5e/explaining_feedforward_backpropagation_and/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/cmgz5e/explaining_feedforward_<b>backpropagation</b>_and", "snippet": "<b>Do</b> you really think by reading this <b>someone</b> learned everything you tried to teach?). I would honestly recommend just to focus on your school stuff and go join a club and go make a blog about <b>something</b> neat but not super technical <b>like</b> how to set up shell scripts to run your code on a remote server that you don&#39;t have sudo rights to nor it has screen/tux etc installed and is pretty vanilla RHEL. You&#39;re way out of your league and it shows. Anyone with even a little bit of experience can tell ...", "dateLastCrawled": "2021-11-04T06:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[D] Explaining Feedforward, <b>Backpropagation</b> and Optimization: The Math ...", "url": "https://www.reddit.com/r/MachineLearning/comments/cmfncc/d_explaining_feedforward_backpropagation_and/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/cmfncc/d_explaining_feedforward_<b>backpropagation</b>_and", "snippet": "It&#39;s no different than a function <b>like</b> f(x)=mx+b, where you fit a line by modifying the `m` and `b` parameters of this linear function. The only difference is using matrices instead of scalars and composing these simple functions together such that you get f(g(h(x))) (nested functions). I don&#39;t know why everyone keeps <b>teaching</b> neural nets <b>like</b> they&#39;re made of neurons and synapses.", "dateLastCrawled": "2021-08-28T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Observational Learning</b>: Examples, Stages, History", "url": "https://www.verywellmind.com/what-is-observational-learning-2795402", "isFamilyFriendly": true, "displayUrl": "https://www.verywellmind.com/what-is-<b>observational-learning</b>-2795402", "snippet": "Examples. Stages of <b>Observational Learning</b>. Influential Factors. Positive and Negative Outcomes. <b>Observational learning</b> describes the process of learning by watching others, retaining the information, and then later replicating the behaviors that were observed. There are a number of learning theories, such as classical conditioning and operant ...", "dateLastCrawled": "2022-02-02T10:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - Is it reasonable to study neural networks without ...", "url": "https://stats.stackexchange.com/questions/210056/is-it-reasonable-to-study-neural-networks-without-mathematical-education", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/210056", "snippet": "For example, take a look at the <b>Backpropagation</b> Algorithm.. For more about maths in machine learning you can read this blog post. Is it possible to build interesting projects in this area, <b>like</b> those playing atari, using only high-level tools? Yes, it is. There are many great tools out there that allow you to <b>do</b> this.", "dateLastCrawled": "2022-01-28T19:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Step by Step <b>Backpropagation</b> Example \u2013 Matt Mazur", "url": "https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/comment-page-5/", "isFamilyFriendly": true, "displayUrl": "https://mattmazur.com/2015/03/17/a-step-by-step-<b>backpropagation</b>-example/comment-page-5", "snippet": "Background. <b>Backpropagation</b> is a common method for training a neural network. There is no shortage of papers online that attempt to explain how <b>backpropagation</b> works, but few that include an example with actual numbers. This post is my attempt to explain how it works with a concrete example that folks can compare their own calculations to in order to ensure they understand <b>backpropagation</b> correctly.", "dateLastCrawled": "2022-01-27T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning Crash Course: Part</b> 3 - ML@B Blog", "url": "https://ml.berkeley.edu/blog/posts/crash-course/part-3/", "isFamilyFriendly": true, "displayUrl": "https://ml.berkeley.edu/blog/posts/crash-course/part-3", "snippet": "<b>Backpropagation</b> works in a <b>similar</b> way. When the neural network outputs the wrong answer (doesn\u2019t push the marble off the table), you find the slopes of the output layer (the domino closest to the marble) first because it was the direct cause of the incorrect answer. And since the output layer depends on the hidden layer, you\u2019ll have to fix that too by finding the slopes and using gradient descent. Eventually you\u2019ll work your way back to the hidden layer closest to the input layer.", "dateLastCrawled": "2022-01-30T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Is there neurological evidence the brain does <b>something</b> like backprop ...", "url": "https://www.quora.com/Is-there-neurological-evidence-the-brain-does-something-like-backprop", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-neurological-evidence-the-brain-does-<b>something</b>-like...", "snippet": "Answer (1 of 3): Not exactly. First, let&#39;s make a quick distinction. <b>Backpropagation</b> a mechanism which changes all of a networks weights in a certain direction to change the output of the network towards a specified goal. If you remove the neural metaphor, what this actually looks like is changi...", "dateLastCrawled": "2022-01-09T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Spiking Neural Network (SNN) with PyTorch</b>: towards bridging the gap ...", "url": "https://guillaume-chevalier.com/spiking-neural-network-snn-with-pytorch-where-backpropagation-engenders-stdp-hebbian-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>guillaume-chevalier</b>.com/<b>spiking-neural-network-snn-with-pytorch</b>-where...", "snippet": "Watching this video of Blake Richard, at about 11 minutes, he says that he have been thinking for a long time on how the brain could <b>do</b> <b>backpropagation</b> or <b>something</b> <b>similar</b> to it. And he says that in Deep Learning, we have stripped the complexity out of the neurons and that real neurons are way more complicated than neurons used in Deep Learning. He even says that perhaps by stripping away the complexity, we have made things harder to understand. Later when he describes his work, he talks of ...", "dateLastCrawled": "2022-01-26T07:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Talk:Backpropagation</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Talk:Backpropagation", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Talk:Backpropagation</b>", "snippet": "<b>Someone</b> added a section on <b>backpropagation</b> of action potential in biological neurons. I just backed this change out. The material is irrelevant in the context of this article. (In this article, backpropgation refers to reverse propagation of errors through the computation graph. As far as anyone knows, this has nothing to <b>do</b> with the potential jump of action potentials invading the proximal regions of the dendritic arbor)", "dateLastCrawled": "2021-08-13T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Problem with backpropagation</b>. - ResearchGate", "url": "https://www.researchgate.net/post/Problem_with_backpropagation2", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Problem_with_backpropagation</b>2", "snippet": "<b>Problem with backpropagation</b>. I am implementing <b>Backpropagation</b> for on a stock price of a company. But I am facing a unusual problem, whenever I go for training the algorithms produces same output ...", "dateLastCrawled": "2021-11-22T15:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to answer questions about backpropagating specific architectures or ...", "url": "https://stats.meta.stackexchange.com/questions/5648/how-to-answer-questions-about-backpropagating-specific-architectures-or-function", "isFamilyFriendly": true, "displayUrl": "https://stats.meta.stackexchange.com/questions/5648/how-to-answer-questions-about-back...", "snippet": "There are quite a few questions asking how to run <b>backpropagation</b> when dealing with skip connections, residual networks, &quot;mixed&quot; RNN-CNNs, attention mechanisms, etc. I suspect the reason for the", "dateLastCrawled": "2021-11-29T11:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Backpropagation</b> is a leaky abstraction | Hacker News", "url": "https://news.ycombinator.com/item?id=13215590", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=13215590", "snippet": "The &quot;code reuse as much as possible&quot; mantra applies to when you&#39;re &quot;using&quot; a technique to <b>do</b> <b>something</b> else, not when you&#39;re &quot;learning&quot; the technique itself. They might as well register for Calculus and ask &quot;why learn integrals and derivatives when mathematica can <b>do</b> them for you&quot;, or take an Aerodynamics class and ask &quot;why learn fluid mechanics and dynamics, heck newton&#39;s laws, when an airplane can run on autopilot.&quot; I doubt &quot;because calculus is a leaky abstraction&quot; or &quot;because fluid ...", "dateLastCrawled": "2021-07-06T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GitHub</b> - guillaume-chevalier/Spiking-Neural-Network-SNN-with-PyTorch ...", "url": "https://github.com/guillaume-chevalier/Spiking-Neural-Network-SNN-with-PyTorch-where-Backpropagation-engenders-STDP", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/guillaume-chevalier/Spiking-Neural-Network-SNN-with-PyTorch-where...", "snippet": "Watching this video of Blake Richard, at about 11 minutes, he says that he have been thinking for a long time on how the brain could <b>do</b> <b>backpropagation</b> or <b>something</b> <b>similar</b> to it. And he says that in Deep Learning, we have stripped the complexity out of the neurons and that real neurons are way more complicated than neurons used in Deep Learning. He even says that perhaps by stripping away the complexity, we have made things harder to understand. Later when he describes his work, he talks of ...", "dateLastCrawled": "2022-02-02T15:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Machine Learning is for Humans \u2014 <b>how to do</b> it, and teach it too | by ...", "url": "https://gbenedis.medium.com/machine-learning-is-for-humans-how-to-do-it-and-teach-it-too-1370c119375f", "isFamilyFriendly": true, "displayUrl": "https://gbenedis.medium.com/machine-learning-is-for-humans-<b>how-to-do</b>-it-and-teach-it...", "snippet": "In math and science, a \u201cblack box\u201d typically refers to <b>something</b> we can\u2019t \u201csee\u201d inside, but whose behavior we can learn about by putting things into it and looking at what comes out. It\u2019s <b>similar</b> to the concept of a function. Before we walk through the math of machine learning (that is, before we open this black box), let\u2019s get a feel for what goes in and what comes out.", "dateLastCrawled": "2021-12-11T20:40:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Step by Step <b>Backpropagation</b> Example \u2013 Matt Mazur", "url": "https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/comment-page-5/", "isFamilyFriendly": true, "displayUrl": "https://mattmazur.com/2015/03/17/a-step-by-step-<b>backpropagation</b>-example/comment-page-5", "snippet": "Background. <b>Backpropagation</b> is a common method for training a neural network. There is no shortage of papers online that attempt to explain how <b>backpropagation</b> works, but few that include an example with actual numbers. This post is my attempt to explain how it works with a concrete example that folks <b>can</b> compare their own calculations to in order to ensure they understand <b>backpropagation</b> correctly.", "dateLastCrawled": "2022-01-27T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Spiking Neural Network (SNN) with PyTorch</b>: towards bridging the gap ...", "url": "https://guillaume-chevalier.com/spiking-neural-network-snn-with-pytorch-where-backpropagation-engenders-stdp-hebbian-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>guillaume-chevalier</b>.com/<b>spiking-neural-network-snn-with-pytorch</b>-where...", "snippet": "Watching this video of Blake Richard, at about 11 minutes, he says that he have been thinking for a long time on how the brain could <b>do</b> <b>backpropagation</b> or <b>something</b> similar to it. And he says that in Deep Learning, we have stripped the complexity out of the neurons and that real neurons are way more complicated than neurons used in Deep Learning. He even says that perhaps by stripping away the complexity, we have made things harder to understand. Later when he describes his work, he talks of ...", "dateLastCrawled": "2022-01-26T07:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Is there neurological evidence the brain does <b>something</b> like backprop ...", "url": "https://www.quora.com/Is-there-neurological-evidence-the-brain-does-something-like-backprop", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-neurological-evidence-the-brain-does-<b>something</b>-like...", "snippet": "Answer (1 of 3): Not exactly. First, let&#39;s make a quick distinction. <b>Backpropagation</b> a mechanism which changes all of a networks weights in a certain direction to change the output of the network towards a specified goal. If you remove the neural metaphor, what this actually looks like is changi...", "dateLastCrawled": "2022-01-09T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>GitHub</b> - guillaume-chevalier/Spiking-Neural-Network-SNN-with-PyTorch ...", "url": "https://github.com/guillaume-chevalier/Spiking-Neural-Network-SNN-with-PyTorch-where-Backpropagation-engenders-STDP", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/guillaume-chevalier/Spiking-Neural-Network-SNN-with-PyTorch-where...", "snippet": "Watching this video of Blake Richard, at about 11 minutes, he says that he have been thinking for a long time on how the brain could <b>do</b> <b>backpropagation</b> or <b>something</b> similar to it. And he says that in Deep Learning, we have stripped the complexity out of the neurons and that real neurons are way more complicated than neurons used in Deep Learning. He even says that perhaps by stripping away the complexity, we have made things harder to understand. Later when he describes his work, he talks of ...", "dateLastCrawled": "2022-02-02T15:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What are the three best papers to read to understand <b>backpropagation</b> in ...", "url": "https://www.quora.com/What-are-the-three-best-papers-to-read-to-understand-backpropagation-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-the-three-best-papers-to-read-to-understand...", "snippet": "Answer (1 of 4): Forget about papers. Dig up your old calculus textbooks instead. Look up gradients, and look up the chain rule for derivation. Now, look at a feed-forward neural network. Consider it to be a function that takes in some inputs and some parameters and spits out a loss function. T...", "dateLastCrawled": "2022-01-15T19:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Explaining Feedforward, <b>Backpropagation</b> and Optimization: The Math ...", "url": "https://www.reddit.com/r/datascience/comments/cmgz5e/explaining_feedforward_backpropagation_and/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/cmgz5e/explaining_feedforward_<b>backpropagation</b>_and", "snippet": "The activation value is <b>something</b> between 0 and 1 if the activation function is a sigmoid. But it may be only 0 or only 1 (step function), it may be between -1 and 1 (tanh), it might be between 0 and positive x (ReLu), or one of the other ranges / strategies out there. I think the definition of activation is unclear. Activation is the value of how triggered or fired an neuron is -- how much the neuron responded to <b>something</b> in the input. I&#39;m missing the discussion of the activation function ...", "dateLastCrawled": "2021-11-04T06:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Backpropagation</b> is a leaky abstraction | Hacker News", "url": "https://news.ycombinator.com/item?id=13215590", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=13215590", "snippet": "The &quot;code reuse as much as possible&quot; mantra applies to when you&#39;re &quot;using&quot; a technique to <b>do</b> <b>something</b> else, not when you&#39;re &quot;learning&quot; the technique itself. They might as well register for Calculus and ask &quot;why learn integrals and derivatives when mathematica <b>can</b> <b>do</b> them for you&quot;, or take an Aerodynamics class and ask &quot;why learn fluid mechanics and dynamics, heck newton&#39;s laws, when an airplane <b>can</b> run on autopilot.&quot; I doubt &quot;because calculus is a leaky abstraction&quot; or &quot;because fluid ...", "dateLastCrawled": "2021-07-06T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Observational Learning</b>: Examples, Stages, History", "url": "https://www.verywellmind.com/what-is-observational-learning-2795402", "isFamilyFriendly": true, "displayUrl": "https://www.verywellmind.com/what-is-<b>observational-learning</b>-2795402", "snippet": "Keep in mind, this is different than simply copying <b>someone</b> else&#39;s behavior. Instead, <b>observational learning</b> may incorporate a social and/or motivational component that influences whether the observer will choose to engage in or avoid a certain behavior. Attention . For an observer to learn, they must be in the right mindset to <b>do</b> so. This means having the energy to learn, remaining focused on what the model is engaging in, and being able to observe the model for enough time to grasp what ...", "dateLastCrawled": "2022-02-02T10:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] Explaining Feedforward, <b>Backpropagation</b> and Optimization: The Math ...", "url": "https://www.reddit.com/r/MachineLearning/comments/cmfncc/d_explaining_feedforward_backpropagation_and/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/cmfncc/d_explaining_feedforward_<b>backpropagation</b>_and", "snippet": "The matrix is the set of parameters and these parameters <b>can</b> be optimized by an algorithm like gradient descent to produce desired outputs. That&#39;s it. It&#39;s no different than a function like f(x)=mx+b, where you fit a line by modifying the `m` and `b` parameters of this linear function. The only difference is using matrices instead of scalars and composing these simple functions together such that you get f(g(h(x))) (nested functions). I don&#39;t know why everyone keeps <b>teaching</b> neural nets like ...", "dateLastCrawled": "2021-08-28T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is AI <b>Riding a One-Trick Pony</b>? | <b>MIT Technology Review</b>", "url": "https://www.technologyreview.com/2017/09/29/67852/is-ai-riding-a-one-trick-pony/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.technologyreview.com</b>/2017/09/29/67852/is-ai-<b>riding-a-one-trick-pony</b>", "snippet": "Neural nets <b>can</b> <b>be thought</b> of as trying to take things\u2014images, words, recordings of <b>someone</b> talking, medical data\u2014and put them into what mathematicians call a high-dimensional vector space ...", "dateLastCrawled": "2022-01-29T20:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>backpropagation</b> and what is it doing? [video]", "url": "https://hacker-news.news/post/15625330", "isFamilyFriendly": true, "displayUrl": "https://hacker-news.news/post/15625330", "snippet": "Keeping track of properties is not really <b>something</b> &#39;visualization&#39; necessarily helps with though, more symbolic reasoning through proofs. ... I mean that if you <b>do</b> it, you <b>can</b> still apply your spatial reasoning where it&#39;s appropriate. By adamnemecek 2017-11-04 17:43 2 reply . I&#39;m not sure I know what you are talking about but let&#39;s not throw away the baby with the bath water. By kobeya 2017-11-04 18:08 5 reply . What he means is that we only really have intuition for 1, 2, and 2.5D visuals ...", "dateLastCrawled": "2022-01-22T06:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning Crash Course: Part</b> 3 \u2014 Neural Networks | by Machine ...", "url": "https://medium.com/@ml.at.berkeley/machine-learning-crash-course-part-3-4a5bfd72294a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ml.at.berkeley/<b>machine-learning-crash-course-part</b>-3-4a5bfd72294a", "snippet": "To introduce <b>backpropagation</b>, let\u2019s start with another analogy. Don\u2019t worry if you <b>can</b>\u2019t see how this has anything to <b>do</b> with <b>backpropagation</b> (trust us, it does). Let\u2019s say you want to ...", "dateLastCrawled": "2021-12-11T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Explaining Feedforward, <b>Backpropagation</b> and Optimization: The Math ...", "url": "https://www.reddit.com/r/datascience/comments/cmgz5e/explaining_feedforward_backpropagation_and/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/cmgz5e/explaining_feedforward_<b>backpropagation</b>_and", "snippet": "The activation value is <b>something</b> between 0 and 1 if the activation function is a sigmoid. But it may be only 0 or only 1 (step function), it may be between -1 and 1 (tanh), it might be between 0 and positive x (ReLu), or one of the other ranges / strategies out there. I think the definition of activation is unclear. Activation is the value of how triggered or fired an neuron is -- how much the neuron responded to <b>something</b> in the input. I&#39;m missing the discussion of the activation function ...", "dateLastCrawled": "2021-11-04T06:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Spiking-Neural-Network-SNN-with-PyTorch-where-<b>Backpropagation</b>-engenders ...", "url": "https://github.com/guillaume-chevalier/Spiking-Neural-Network-SNN-with-PyTorch-where-Backpropagation-engenders-STDP/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/guillaume-chevalier/Spiking-Neural-Network-SNN-with-PyTorch-where...", "snippet": "Watching this video of Blake Richard, at about 11 minutes, he says that he have been thinking for a long time on how the brain could <b>do</b> <b>backpropagation</b> or <b>something</b> similar to it. And he says that in Deep Learning, we have stripped the complexity out of the neurons and that real neurons are way more complicated than neurons used in Deep Learning. He even says that perhaps by stripping away the complexity, we have made things harder to understand. Later when he describes his work, he talks of ...", "dateLastCrawled": "2022-01-25T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Is there neurological evidence the brain does <b>something</b> like backprop ...", "url": "https://www.quora.com/Is-there-neurological-evidence-the-brain-does-something-like-backprop", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-neurological-evidence-the-brain-does-<b>something</b>-like...", "snippet": "Answer (1 of 3): Not exactly. First, let&#39;s make a quick distinction. <b>Backpropagation</b> a mechanism which changes all of a networks weights in a certain direction to change the output of the network towards a specified goal. If you remove the neural metaphor, what this actually looks like is changi...", "dateLastCrawled": "2022-01-09T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[D] Explaining Feedforward, <b>Backpropagation</b> and Optimization: The Math ...", "url": "https://www.reddit.com/r/MachineLearning/comments/cmfncc/d_explaining_feedforward_backpropagation_and/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/cmfncc/d_explaining_feedforward_<b>backpropagation</b>_and", "snippet": "The matrix is the set of parameters and these parameters <b>can</b> be optimized by an algorithm like gradient descent to produce desired outputs. That&#39;s it. It&#39;s no different than a function like f(x)=mx+b, where you fit a line by modifying the `m` and `b` parameters of this linear function. The only difference is using matrices instead of scalars and composing these simple functions together such that you get f(g(h(x))) (nested functions). I don&#39;t know why everyone keeps <b>teaching</b> neural nets like ...", "dateLastCrawled": "2021-08-28T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Teaching</b> AI to <b>Learn How Humans Plan Efficiently</b> | by Agni Kumar ...", "url": "https://towardsdatascience.com/teaching-ai-to-learn-how-humans-plan-efficiently-1d031c8727b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>teaching</b>-ai-to-<b>learn-how-humans-plan-efficiently</b>-1d031c...", "snippet": "Whether planning <b>something</b> simple like cooking dinner or <b>something</b> complex like a trip abroad, we usually begin with a rough mental sketch of the goals we want to achieve (\u201cgo to India, then return back home\u201d). This sketch is then progressively refined into a detailed sequence of sub-goals (\u201cbook flight ticket\u201d, \u201cpack luggage\u201d), sub-sub-goals, and so on, down to the actual sequence of bodily movements that is much more complicated than the original plan.", "dateLastCrawled": "2022-01-26T06:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Backpropagation</b> is a leaky abstraction | Hacker News", "url": "https://news.ycombinator.com/item?id=13215590", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=13215590", "snippet": "The &quot;code reuse as much as possible&quot; mantra applies to when you&#39;re &quot;using&quot; a technique to <b>do</b> <b>something</b> else, not when you&#39;re &quot;learning&quot; the technique itself. They might as well register for Calculus and ask &quot;why learn integrals and derivatives when mathematica <b>can</b> <b>do</b> them for you&quot;, or take an Aerodynamics class and ask &quot;why learn fluid mechanics and dynamics, heck newton&#39;s laws, when an airplane <b>can</b> run on autopilot.&quot; I doubt &quot;because calculus is a leaky abstraction&quot; or &quot;because fluid ...", "dateLastCrawled": "2021-07-06T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How the <b>backpropagation</b> algorithm works | Hacker News", "url": "https://news.ycombinator.com/item?id=7588158", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=7588158", "snippet": "So the way it described <b>backpropagation</b> just &quot;clicked&quot; in a way that the previous articles that used tons of summation/matrix math/probability and calculus had not. I\u2019ve read some positively atrocious papers where it was like the authors were showing their notation prowess rather than conveying information. Your chapters have a good balance, although for <b>someone</b> who\u2019s been out of college for 15 years like me, it might be nice to have a tiny refresher about derivatives before you dive in ...", "dateLastCrawled": "2020-11-26T13:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are Max <b>Pooling, Average Pooling, Global Max</b> ... - MachineCurve", "url": "https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling-global-max-pooling-and-global-average-pooling/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling...", "snippet": "It <b>can</b> <b>be compared</b> to shrinking an image to reduce its pixel density. Let&#39;s pause for a second! \ud83d\udc69\u200d\ud83d\udcbb . Blogs at MachineCurve teach Machine Learning for Developers. Sign up to MachineCurve&#39;s free Machine Learning update today! You will learn new things and better understand concepts you already know. We send emails at least every Friday. Welcome! Email Address . By signing up, you consent that any information you receive <b>can</b> include services and special offers by email. Hervatte (n.d ...", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Back Propagation Neural Network</b>: What is <b>Backpropagation</b> Algorithm in ...", "url": "https://www.guru99.com/backpropogation-neural-network.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/backpropogation-neural-network.html", "snippet": "A neural network is a group of connected it I/O units where each connection has a weight associated with its computer programs. <b>Backpropagation</b> is a short form for \u201cbackward propagation of errors.\u201d. It is a standard method of training artificial neural networks. <b>Back propagation</b> algorithm in <b>machine</b> <b>learning</b> is fast, simple and easy to program.", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> fundamentals I: An <b>analogy</b> | Finn Rietz.dev", "url": "http://www.finnrietz.dev/machine%20learning/part-1-analogy/", "isFamilyFriendly": true, "displayUrl": "www.finnrietz.dev/<b>machine</b> <b>learning</b>/part-1-<b>analogy</b>", "snippet": "In <b>machine</b> <b>learning</b>, <b>learning</b> manifests on the parameters of the <b>learning</b> algorithm. What exactly these parameters are depends on the specific <b>learning</b> algorithm, but for an artificial neural network, the parameters would be the interneural connections and their associated weights. More general, the parameters of our <b>learning</b> algorithm govern how we map from input to output, independently of the specific <b>learning</b> algorithm.", "dateLastCrawled": "2022-01-16T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Lecture 17: Neural Network and <b>Backpropagation</b> I - [SCS4049-02] <b>Machine</b> ...", "url": "https://spark-lab.com/wp-content/uploads/2020/11/Lecture17-Neural-Network-and-Backpropagation-I-final.pdf", "isFamilyFriendly": true, "displayUrl": "https://spark-lab.com/.../2020/11/Lecture17-Neural-Network-and-<b>Backpropagation</b>-I-final.pdf", "snippet": "<b>Learning</b> whenever neural networks are involved (even shallow ones). For many years researchers struggled to find a way to train MLPs, without success. But in 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a groundbreaking paper9 introducing the <b>backpropagation</b> training algorithm, which is still used today. In short, it is ...", "dateLastCrawled": "2022-01-22T09:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Neural networks and <b>backpropagation</b> explained in a simple way ...", "url": "https://datathings.com/blog/post/neuralnet/", "isFamilyFriendly": true, "displayUrl": "https://datathings.com/blog/post/neuralnet", "snippet": "Neural networks and <b>backpropagation</b> explained in a simple way Assaad Moawad 23-02-2017 <b>machine</b> <b>learning</b> \u00b7 neural networks. Any complex system can be abstracted in a simple way, or at least dissected to its basic abstract components. Complexity arises by the accumulation of several simple layers. The goal of this post, is to explain how neural ...", "dateLastCrawled": "2021-12-04T09:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Deep <b>Learning</b> Part 3: <b>Backpropagation</b>; Nothing But a Game of Telephone ...", "url": "https://medium.com/geekculture/deep-learning-part-3-backpropagation-nothing-but-a-game-of-telephone-e0d716f6d362", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/deep-<b>learning</b>-part-3-<b>backpropagation</b>-nothing-but-a-game...", "snippet": "Deep <b>Learning</b> Part 3: <b>Backpropagation</b>; Nothing But a Game of Telephone . An intuitive way of understanding <b>backpropagation</b> by relating it to the game of \u201ctelephone\u201d Ali H Khanafer. Follow. Aug ...", "dateLastCrawled": "2021-10-17T17:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "MLNotes.pdf - <b>Machine</b> <b>learning</b> notes 1 <b>Back Propagation</b> <b>Backpropagation</b> ...", "url": "https://www.coursehero.com/file/121946064/MLNotespdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/121946064/MLNotespdf", "snippet": "It is a standard method of training artificial neural networks <b>Back propagation</b> algorithm in <b>machine</b> <b>learning</b> is fast, simple and easy to program A feedforward BPN network is an artificial neural network. Two Types of <b>Backpropagation</b> Networks are 1)Static <b>Back-propagation</b> 2) Recurrent <b>Backpropagation</b> In 1961, the basics concept of continuous <b>backpropagation</b> were derived in the context of control theory by J. Kelly, Henry Arthur, and E. Bryson. <b>Back propagation</b> in data mining simplifies the ...", "dateLastCrawled": "2022-01-19T17:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Is training <b>a neural network</b> like forming a habit? | Blog", "url": "https://jmsbrdy.com/blog/habit-formation-as-analogy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://jmsbrdy.com/blog/habit-formation-as-<b>analogy</b>-for-<b>machine</b>-<b>learning</b>", "snippet": "Reward: during <b>backpropagation</b>, ... In fact, the <b>analogy</b> also works at the level of the network as a whole: Cue: transform our input example and input it into the first layer of the network; Routine: the network processes the input through its layers to produce a result; Reward: calculate how accurate the result is \u2013 compared to the labeling of the input example \u2013 and backpropagate; So, from a process perspective there do seem to broad similarities between how we \u2013 as humans \u2013 form ...", "dateLastCrawled": "2021-12-29T12:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - Intuition in <b>Backpropagation</b> (gradient descent ...", "url": "https://datascience.stackexchange.com/questions/15689/intuition-in-backpropagation-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/15689", "snippet": "The <b>analogy</b> is useful in that it explains the progression and goal of the repeated calculations across layers. It is also correct in that values of the gradient in one layer depend critically on values in &quot;higher&quot; layers. This can lead to intuitions about managing problems with gradient values in deep networks.", "dateLastCrawled": "2022-01-21T04:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What are the similarities and differences between the <b>backpropagation</b> ...", "url": "https://www.quora.com/What-are-the-similarities-and-differences-between-the-backpropagation-learning-algorithm-frequently-used-in-the-multilayer-artificial-neural-networks-and-the-way-brains-learn-and-memorize", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-the-similarities-and-differences-between-the...", "snippet": "Answer (1 of 2): I would say that the <b>analogy</b> is not true, mainly, the <b>learning</b> and memorisation occurring in the brain is invariably associated with its architectural constraints, not the algorithms, this particular point is mostly true for ANNs; especially considering that other algorithms can ...", "dateLastCrawled": "2022-01-16T14:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the difference between <b>backpropagation</b> and reinforcement ...", "url": "https://www.quora.com/What-is-the-difference-between-backpropagation-and-reinforcement-learning-in-training-artificial-neural-networks-Are-the-two-techniques-completely-different-or-related", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-difference-between-<b>backpropagation</b>-and-reinforcement...", "snippet": "Answer (1 of 5): <b>Back-propagation</b> is a way to train a network\u2019s weights. It uses a loss function and back-prop to decide what weights to change. Reinforcement <b>learning</b> is a way to select actions. Lets say for example you want to want to recognize a pedestrian. You can use a convolution neural net...", "dateLastCrawled": "2022-01-13T19:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - Why are neural networks described as <b>black-box</b> ...", "url": "https://stats.stackexchange.com/questions/93705/why-are-neural-networks-described-as-black-box-models", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/93705", "snippet": "$\\begingroup$ I like to add point to Jack, when we look at MLP in <b>machine</b> <b>learning</b> point of view, neural networks are not <b>black box</b> anymore. With simple sigmoid function we shall be able to interpret input and out relation with an equation. $\\endgroup$ \u2013 user131276. Sep 16 &#39;16 at 6:23 $\\begingroup$ It depends on the complexity of the model. You can have simple neural networks that can be considered interpretable models. Usually, in practical applications, they are black-boxes because, as ...", "dateLastCrawled": "2022-01-26T21:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What Makes Backpropagation So Elegant? | by Tyron Jung | The Feynman ...", "url": "https://medium.com/the-feynman-journal/what-makes-backpropagation-so-elegant-657f3afbbd", "isFamilyFriendly": true, "displayUrl": "https://medium.com/the-feynman-journal/what-makes-backpropagation-so-elegant-657f3afbbd", "snippet": "What Backpropagation Looks Like. In part 3, we visualized what the <b>learning</b> process looks like for a deep neural network (specifically, a Multilayer Perceptron or MLP): Left: The output landscape ...", "dateLastCrawled": "2022-01-01T08:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Data Mining: An Introduction to Neural Networks in SQL Server - CodeProject", "url": "https://www.codeproject.com/Articles/5321780/Data-Mining-An-Introduction-to-Neural-Networks-in", "isFamilyFriendly": true, "displayUrl": "https://<b>www.codeproject.com</b>/Articles/5321780/Data-Mining-An-Introduction-to-Neural...", "snippet": "Artificial Intelligence, <b>machine</b> <b>learning</b> and deep <b>learning</b> are now commonly used terms. Things have certainly progressed a lot over the last few years with use of AI in various aspects of everyday life including conversational AI like chatbots for customer service, etc. Truly intelligent machines would be the ones that can learn, reason, and make decisions like humans. Neural Networks (NNs) are perhaps the closest to provide the answer. Human brains are made up of connected networks of ...", "dateLastCrawled": "2022-02-03T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Deep Learning Has Reinvented Quality Control in</b> Manufacturing\u2014but It ...", "url": "https://spectrum.ieee.org/deep-learning-has-reinvented-quality-control-in-manufacturingbut-it-hasnt-gone-far-enough", "isFamilyFriendly": true, "displayUrl": "https://spectrum.ieee.org/<b>deep-learning-has-reinvented-quality-control-in</b>...", "snippet": "<b>Machine</b> <b>learning</b> and artificial intelligence (AI) have already penetrated so deeply into our life and work that you might have forgotten what interactions with machines used to be like. We used to ...", "dateLastCrawled": "2022-01-31T12:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "understanding backpropagation - cslxiao - \u535a\u5ba2\u56ed", "url": "https://www.cnblogs.com/cslxiao/p/6080063.html", "isFamilyFriendly": true, "displayUrl": "https://www.cnblogs.com/cslxiao/p/6080063.html", "snippet": "Introduction. I n the section on the backpropagation algorithm, you were briefly introduced to backpropagation as a means of deriving gradients for <b>learning</b> in the sparse autoencoder.It turns out that together with matrix calculus, this provides a powerful method and intuition for deriving gradients for more complex matrix functions (functions from matrices to the reals, or symbolically, from ).", "dateLastCrawled": "2022-02-01T02:35:00.0000000Z", "language": "zh_chs", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How Deep <b>Learning</b> Works - IEEE Spectrum", "url": "https://spectrum.ieee.org/what-is-deep-learning/neural-network", "isFamilyFriendly": true, "displayUrl": "https://spectrum.ieee.org/what-is-deep-<b>learning</b>/neural-network", "snippet": "<b>Machine</b> <b>learning</b> and artificial intelligence (AI) have already penetrated so deeply into our life and work that you might have forgotten what interactions with machines used to be like. We used to ...", "dateLastCrawled": "2022-01-28T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Monitoring of</b> the <b>daily living</b> activities in smart home care | Human ...", "url": "https://hcis-journal.springeropen.com/articles/10.1186/s13673-017-0113-6", "isFamilyFriendly": true, "displayUrl": "https://hcis-journal.springeropen.com/articles/10.1186/s13673-017-0113-6", "snippet": "<b>Backpropagation is like</b> a teacher\u2019s instruction when the calculated output of the network is compared to the desired output. Subsequently, with the backward propagation of the signal, the weights are adjusted so that the net responds with the desired output to the pattern. It has been shown that such networks are able to approximate any continuous function with the required accuracy and therefore have a wide use, for example, for regression analysis . In general, backpropagation is the ...", "dateLastCrawled": "2022-01-29T07:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What Deep <b>Learning Means for Artificial Intelligence</b>", "url": "https://www.slideshare.net/jmugan/what-deep-learning-means-for-artificial-intelligence", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/jmugan/what-deep-<b>learning-means-for-artificial-intelligence</b>", "snippet": "Training with supervised <b>learning</b> Supervised <b>Learning</b>: You show the network a bunch of things with a labels saying what they are, and you want the network to learn to classify future things without labels. \ud835\udc64 \ud835\udc4a \ud835\udc66 \ud835\udc65 [16.2, 17.3, \u221252.3, 11.1] Why Google\u2019s Deep <b>Learning</b> toolbox is called TensorFlow. y: output x: input h: number of hidden neurons n: length of vector x <b>Learning</b> is <b>learning</b> parameter values", "dateLastCrawled": "2022-01-21T13:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "part of Course 321 - Library for End-to-End <b>Machine</b> <b>Learning</b>", "url": "https://e2eml.school/convolution_one_d.html", "isFamilyFriendly": true, "displayUrl": "https://e2eml.school/convolution_one_d.html", "snippet": "The process for setting up a layer for <b>backpropagation is similar</b> no matter what that layer does. The output gradient (the partial derivative of the loss function with respect to each of the layer&#39;s outputs) is the starting point. To make backpropagation work, we have to calculate two groups of derivatives, the input gradient (the partial derivative of each of the layer&#39;s inputs with respect to the loss function), and the parameter gradient (the partial derivative of each or of the layer&#39;s ...", "dateLastCrawled": "2022-02-03T19:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Simple RNNs and their Backpropagation | CS-677", "url": "https://pantelis.github.io/cs677/docs/common/lectures/rnn/simple-rnn/", "isFamilyFriendly": true, "displayUrl": "https://pantelis.github.io/cs677/docs/common/lectures/rnn/<b>simple-rnn</b>", "snippet": "A more practical <b>simple RNN</b> architecture is shown below. <b>Simple RNN</b> with recurrences between hidden units. This architecture can compute any computable function and therefore is a Universal Turing <b>Machine</b>. Notice how the path from input. x t \u2212 1. \\bm x_ {t-1} xt\u22121. .", "dateLastCrawled": "2022-02-03T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "May 2017 ARTIFICIAL INTELLIGENCE: WHAT&#39;S NOW, WHAT&#39;S NEW AND WHAT&#39;S NEXT", "url": "https://www.mediastruction.com/wp-content/uploads/2017/05/eMarketer_Report_Artificial_Intelligence_Now_New_Next.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mediastruction.com/wp-content/uploads/2017/05/eMarketer_Report_Artificial...", "snippet": "algorithmic approach, called <b>backpropagation, is similar</b> to statistical regression. Genetic algorithms. <b>Machine</b> <b>learning</b> optimization algorithms that work by mimicking the evolutionary process using natural selection, recombination and muta t ion. They are particularly effective at optimizing problems with a large number of possible solutions.", "dateLastCrawled": "2022-01-09T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Simple RNNs and their Backpropagation</b> | Data Mining", "url": "https://pantelis.github.io/cs634/docs/common/lectures/rnn/simple-rnn/", "isFamilyFriendly": true, "displayUrl": "https://pantelis.github.io/cs634/docs/common/lectures/rnn/simple-rnn", "snippet": "A more practical simple RNN architecture is shown below. Simple RNN with recurrences between hidden units. This architecture can compute any computable function and therefore is a Universal Turing <b>Machine</b>. Notice how the path from input. x t \u2212 1. \\bm x_ {t-1} xt\u22121.", "dateLastCrawled": "2022-02-03T05:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning on Dope: Part 1</b> \u2013 analyticsjourneyblog", "url": "https://analyticsjourneyblog.wordpress.com/2017/08/06/learning-on-dope-part-1/", "isFamilyFriendly": true, "displayUrl": "https://analyticsjourneyblog.wordpress.com/2017/08/06/<b>learning-on-dope-part-1</b>", "snippet": "Rprop (Resilient <b>backpropagation) is similar</b> to gain in that the signs of the current gradient and of the previous gradient are compared to decide on how to update the parameters. Rprop, however, deviates from what we have seen of far about gradient descent in that it does not rely on the value of the gradients to change the weights. It relies only on the sign of the gradient. The parameters are updated only \u201cby a weight-specific, so-called \u2018update-value\u2019", "dateLastCrawled": "2022-01-19T21:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>ARTIFICIAL INTELLIGENCE TRENDS 2019 ROUNDUP</b>", "url": "https://on.emarketer.com/rs/867-SLG-901/images/eMarketer_Roundup_AI_Trends_2019.pdf", "isFamilyFriendly": true, "displayUrl": "https://on.emarketer.com/rs/867-SLG-901/images/eMarketer_Roundup_AI_Trends_2019.pdf", "snippet": "approach, called <b>backpropagation, is similar</b> to statistical regression. Computer vision: Also called <b>machine</b> vision. The branch of AI that deals with interpreting and extracting meaning from visual elements in the real world, including printed characters or images such as faces, objects and scenes. It also incorporates image processing,", "dateLastCrawled": "2021-12-30T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A <b>TensorFlow</b> Glossary/Cheat Sheet | by Bill Prin | Google Cloud ...", "url": "https://medium.com/google-cloud/a-tensorflow-glossary-cheat-sheet-382583b22932", "isFamilyFriendly": true, "displayUrl": "https://medium.com/google-cloud/a-<b>tensorflow</b>-glossary-cheat-sheet-382583b22932", "snippet": "ML = <b>Machine</b> <b>Learning</b>; <b>TensorFlow</b> API Levels. Low-level API: TF code that makes TF Graphs of TF Ops. You can use this API for purposes other than ML as evidenced by the Mandelbrot tutorial. If you ...", "dateLastCrawled": "2022-01-28T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "I know how to build neural network using Keras, can I apply for <b>machine</b> ...", "url": "https://www.quora.com/I-know-how-to-build-neural-network-using-Keras-can-I-apply-for-machine-learning-engineer-job", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/I-know-how-to-build-neural-network-using-Keras-can-I-apply-for...", "snippet": "Answer (1 of 2): I lead a larger data science team, and I review a lot of resumes from both current and aspiring data scientists. One of the problems that I see is when someone runs an algorithm once or twice, and then says that they are a data scientist. In your specific case, is this the first...", "dateLastCrawled": "2022-01-16T05:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Mastering <b>Machine</b> <b>Learning</b> With scikit-learn - Data Science - 38", "url": "https://www.passeidireto.com/arquivo/88585574/mastering-machine-learning-with-scikit-learn/38", "isFamilyFriendly": true, "displayUrl": "https://www.passeidireto.com/arquivo/88585574/mastering-<b>machine</b>-<b>learning</b>-with-scikit...", "snippet": "Veja gr\u00e1tis o arquivo Mastering <b>Machine</b> <b>Learning</b> With scikit-learn enviado para a disciplina de Data Science Categoria: Prova - 38 - 88585574", "dateLastCrawled": "2021-12-08T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How do convolutional neural networks work? - Quora", "url": "https://www.quora.com/How-do-convolutional-neural-networks-work", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-do-convolutional-neural-networks-work", "snippet": "Answer (1 of 6): Convolutional neural networks (CNN) are a type of deep <b>learning</b> neural networks that are commonly used to classify images. CNNs are known for their ability to reduce computational time and adapt to different variations of images (for example, a well-trained CNN can detect an obje...", "dateLastCrawled": "2022-01-17T23:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Boosted Backpropagation Learning for Training Deep</b> Modular Networks", "url": "https://www.ri.cmu.edu/pub_files/2010/5/451.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ri.cmu.edu/pub_files/2010/5/451.pdf", "snippet": "agation. <b>Just as backpropagation</b> allows a separation of concerns between modules, the proposed approach cleanly separates the problem of credit assignment for modules in the network from the problem of <b>learn-ing</b>. This separation allows both a broader class of <b>learning</b> machines to be applied within the network", "dateLastCrawled": "2021-07-18T02:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> Computer Architecture Pdf - 05/2021", "url": "https://www.coursef.com/machine-learning-computer-architecture-pdf", "isFamilyFriendly": true, "displayUrl": "https://www.coursef.com/<b>machine</b>-<b>learning</b>-computer-architecture-pdf", "snippet": "<b>just as backpropagation</b> trailed more conventional <b>machine</b> <b>learning</b> approaches for decades due to the lack of GPUs. One of the prime examples of an algorithm which is not well matched to SIMD architecture is Monte Carlo Tree Search used in the Google Deepmind\u2019s AlphaGo system [8].", "dateLastCrawled": "2021-05-23T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Overview of the IBM Neural Computer Architecture | DeepAI", "url": "https://deepai.org/publication/overview-of-the-ibm-neural-computer-architecture", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/overview-of-the-ibm-neural-computer-architecture", "snippet": "Therefore, as <b>machine</b> intelligence algorithms continue to evolve, it is unfortunate that promising approaches may be sidelined simply because they do not map well to a GPU, <b>just as backpropagation</b> trailed more conventional <b>machine</b> <b>learning</b> approaches for decades due to the lack of GPUs. One of the prime examples of an algorithm which is not well matched to SIMD architecture is Monte Carlo Tree Search used in the Google Deepmind\u2019s AlphaGo system", "dateLastCrawled": "2022-01-21T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "IEEE TRANSACTIONS, DRAFT MANUSCRIPT, FEB 2020 1 Overview of the IBM ...", "url": "https://arxiv.org/pdf/2003.11178.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2003.11178.pdf", "snippet": "<b>just as backpropagation</b> trailed more conventional <b>machine</b> <b>learning</b> approaches for decades due to the lack of GPUs. One of the prime examples of an algorithm which is not well matched to SIMD architecture is Monte Carlo Tree Search used in the Google Deepmind\u2019s AlphaGo system [8]. This indicates the need for new computer architectures The authors, except H. Huels, are with the IBM Research Division, Almaden Research Center, San Jose, CA 95120. H. Huels is with IBM Boeblingen, Germany. E ...", "dateLastCrawled": "2021-08-31T04:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Active Semi-Supervised <b>Learning</b> Method with Hybrid Deep Belief Networks", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0107122", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0107122", "snippet": "Socher et al. introduce a novel <b>machine</b> <b>learning</b> framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Socher et al. introduce the recursive neural tensor network for semantic compositionality over a sentiment treebank. The key issue of traditional DBN is the efficiency of RBM training. Convolutional neural networks (CNN), which are specifically designed to deal with the variability of two dimensional shapes, have had great success in ...", "dateLastCrawled": "2020-11-13T18:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Active deep <b>learning</b> method for semi-supervised sentiment ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231213004888", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231213004888", "snippet": "The parameter space w N is initialized randomly, <b>just as backpropagation</b> algorithm. Then ADN architecture is constructed. The top hidden layer is formulated as (17) h t N (x) = c t N + \u2211 s = 1 D N \u2212 1 w st N h s N \u2212 1 (x) t = 1, \u2026, D N. For supervised <b>learning</b>, the ADN architecture is trained by L labeled data. The optimization problem is formulized as (18) argmin h N f (h N (X L), Y L) where (19) f (h N (X L), Y L) = \u2211 i = 1 L \u2211 j = 1 C T (h j N (x i) y j i) and the loss ...", "dateLastCrawled": "2021-11-25T17:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Discriminative deep belief networks for <b>visual data classification</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0031320310005789", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320310005789", "snippet": "The parameter space w N is initialized randomly, <b>just as backpropagation</b> algorithm: (14) h t N (x) = c t N + \u2211 s = 1 D N \u2212 1 w st N h s N \u2212 1 (x), t = 1, \u2026, D N. 3.4. Supervised <b>learning</b>. After greedy layer-wise unsupervised <b>learning</b>, h N (x) is the representation of x. In this section, we will use L labeled data to refine the parameter space W for better discriminative ability. This task can be formulated to an optimization problem: (15) arg min W f (h N (X), Y) where (16) f (h N (X ...", "dateLastCrawled": "2022-01-15T17:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Active Semi-Supervised Learning Method with Hybrid</b> Deep Belief ...", "url": "https://www.researchgate.net/publication/265516926_Active_Semi-Supervised_Learning_Method_with_Hybrid_Deep_Belief_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/265516926", "snippet": "shapes, have had great success in <b>machine</b> <b>learning</b> tasks and. represent one of the early successes of deep <b>learning</b> [16]. Desjardins and Bengio [17] adapt RBM to operate in a. convolutional manner ...", "dateLastCrawled": "2022-01-15T01:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Research paper on computer architecture on riscv implementation ...", "url": "https://www.studocu.com/en-us/document/carnegie-mellon-university/introduction-to-computer-architecture/research-paper-on-computer-architecture-on-riscv-implementation/14310703", "isFamilyFriendly": true, "displayUrl": "https://www.studocu.com/en-us/document/carnegie-mellon-university/introduction-to...", "snippet": "Research paper on computer architecture , for computer architecture involving riscv implementation in terms of pipelining and performance of the riscv processor", "dateLastCrawled": "2022-01-26T18:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What is <b>Backpropagation</b>? - Definition from Techopedia", "url": "https://www.techopedia.com/definition/17833/backpropagation", "isFamilyFriendly": true, "displayUrl": "https://<b>www.techopedia.com</b>/definition/17833", "snippet": "<b>Machine</b> <b>Learning</b>; <b>Backpropagation</b> ; <b>Backpropagation</b>. Last updated: October 13, 2021 Table of Contents. What Does <b>Backpropagation</b> Mean? Techopedia Explains <b>Backpropagation</b>; What Does <b>Backpropagation</b> Mean? <b>Backpropagation</b> is an algorithm used in artificial intelligence to fine-tune mathematical weight functions and improve the accuracy of an artificial neural network&#39;s outputs. A neural network can be thought of as a group of connected input/output nodes. The level of accuracy each node ...", "dateLastCrawled": "2022-02-03T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) A <b>backpropagation learning framework for feedforward</b> neural networks.", "url": "https://www.researchgate.net/publication/221380153_A_backpropagation_learning_framework_for_feedforward_neural_networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221380153_A_backpropagation_<b>learning</b>...", "snippet": "The Levenberg-Marquardt <b>Backpropagation can be thought of as</b> an enhanced backpropagation because it uses a technique from backpropagation, a gradient descent method, plus the Gauss-Newton method ...", "dateLastCrawled": "2022-01-03T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fighting Cancer with Artificial Intelligence</b>: Part 0 \u2014 Deep <b>Learning</b> ...", "url": "https://towardsdatascience.com/fighting-cancer-with-artificial-intelligencepart-0-deep-learning-a6f0b375c8c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>fighting-cancer-with-artificial-intelligence</b>part-0-deep...", "snippet": "Essentially a <b>machine</b> <b>learning</b> algorithm works to transform data in a useful way such that it produces meaningful output. For example, ... \u201c<b>Backpropagation can [\u2026] be thought of as</b> gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher\u201d . Backpropagation provides detailed information about how changing the weights and biases will affect the entire network ...", "dateLastCrawled": "2022-01-31T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "AI glossary | Deep <b>learning</b> definitions | <b>Peltarion</b> Platform", "url": "https://peltarion.com/knowledge-center/documentation/glossary", "isFamilyFriendly": true, "displayUrl": "https://<b>peltarion</b>.com/knowledge-center/documentation/glossary", "snippet": "<b>Backpropagation can be thought of as</b> an implementation of the chain rule of derivatives for computation graphs. Batch. A batch is a fixed number of examples used in one training iteration during the model training phase. Batch gradient descent . Batch gradient descent is an implementation of gradient descent which computes the real gradient of the loss function by taking into account all the training examples. In practice, batch gradient descent is rarely used for deep <b>learning</b> applications ...", "dateLastCrawled": "2022-01-31T09:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Learning</b> Important Features Through Propagating Activation Differences", "url": "https://dl.acm.org/doi/pdf/10.5555/3305890.3306006", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/pdf/10.5555/3305890.3306006", "snippet": "<b>Learning</b> Important Features Through Propagating Activation Differences Avanti Shrikumar 1Peyton Greenside Anshul Kundaje Abstract The purported \u201cblack box\u201d nature of neural networks is a barrier to adoption in applica-tions where interpretability is essential. Here we present DeepLIFT (Deep <b>Learning</b> Impor-tant FeaTures), a method for decomposing the output prediction of a neural network on a spe-ci\ufb01c input by backpropagating the contributions of all neurons in the network to every ...", "dateLastCrawled": "2022-02-03T11:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Intelligent Control Systems with LabVIEW</b> - PDF Free Download - Donuts", "url": "https://epdf.pub/intelligent-control-systems-with-labview.html", "isFamilyFriendly": true, "displayUrl": "https://epdf.pub/<b>intelligent-control-systems-with-labview</b>.html", "snippet": "<b>Backpropagation can be thought of as</b> a generalization of the delta rule and can be used instead when ADALINE is implemented. Algorithm 3.2 Backpropagation Step 1 Select a <b>learning</b> rate value . Determine a data collection of q samples of inputs x and outputs y. Generate random values of weights wik where i specifies the i th neuron in the actual layer and k is the kth neuron of the previous layer. Initialize the time t D 0. Evaluate the neural network and obtain Ppthe output values oi ...", "dateLastCrawled": "2022-01-16T14:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Learning Important Features Through Propagating Activation</b> ...", "url": "https://www.researchgate.net/publication/315892529_Learning_Important_Features_Through_Propagating_Activation_Differences", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315892529_<b>Learning</b>_Important_Features_Through...", "snippet": "Moreover, the use of deep <b>learning</b> and neural networks in general has often raised concerns because of their presumed low interpretability (i.e., the black box pitfall). However, a recent and fast ...", "dateLastCrawled": "2022-01-20T14:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How is <b>softmax used in neural networks</b>? - Quora", "url": "https://www.quora.com/How-is-softmax-used-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/How-is-<b>softmax-used-in-neural-networks</b>", "snippet": "Answer (1 of 4): Softmax is often used as the final layer in the network, for a classification task. It receives the final representation of the data sample as input, and it outputs a classification prediction - giving a probability per class (all summing to one). As a metaphor, you can think ab...", "dateLastCrawled": "2022-01-20T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) FYP <b>Deep Learning with GPU Technology for</b> Image and Feature ...", "url": "https://www.academia.edu/12845258/FYP_Deep_Learning_with_GPU_Technology_for_Image_and_Feature_Recognition", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/12845258/FYP_<b>Deep_Learning_with_GPU_Technology_for</b>_Image_and...", "snippet": "An empirical study of the use of deep <b>learning</b> (DL) neural networks powered by NVIDIA graphical processing units (GPU), to recognise features in images. The report is aimed at fellow students and researchers to assist them to run convolutional neural", "dateLastCrawled": "2022-01-31T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is there <b>any other deep learning classifier such as softmax</b>? - Quora", "url": "https://www.quora.com/Is-there-any-other-deep-learning-classifier-such-as-softmax", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-there-<b>any-other-deep-learning-classifier-such-as-softmax</b>", "snippet": "Answer (1 of 5): First of all, your question is incomplete. Softmax isn\u2019t a classifier. It produces probabilities for each possible labels/classes, and based on those we decide which class our test sample belongs to. Added, softmax is nothing new. It\u2019s a multi-class logistic regression. for exa...", "dateLastCrawled": "2022-01-18T15:34:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(backpropagation)  is like +(teaching someone how to do something)", "+(backpropagation) is similar to +(teaching someone how to do something)", "+(backpropagation) can be thought of as +(teaching someone how to do something)", "+(backpropagation) can be compared to +(teaching someone how to do something)", "machine learning +(backpropagation AND analogy)", "machine learning +(\"backpropagation is like\")", "machine learning +(\"backpropagation is similar\")", "machine learning +(\"just as backpropagation\")", "machine learning +(\"backpropagation can be thought of as\")", "machine learning +(\"backpropagation can be compared to\")"]}