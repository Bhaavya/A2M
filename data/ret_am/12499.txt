{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Long short-term memory network for learning sentences similarity using ...", "url": "https://link.springer.com/article/10.1007/s41870-021-00686-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s41870-021-00686-y", "snippet": "<b>BERT</b> calls for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. The prepared <b>BERT</b> model can also be significantly simplified to a large variety of NLP activities with just one additional yield layer. The <b>BERT</b> design expands on the Transformer. We right now have two variations: first, <b>BERT</b> Base consists of 12 attention heads, 12 layers (transformer blocks), and 110 million parameters. Second, <b>BERT</b> Large consists of 16 attention heads and 24 layers (transformer blocks) and 340 million ...", "dateLastCrawled": "2022-01-09T18:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Abstracts of <b>BERT</b>-Related Papers - ayaka14732.github.io", "url": "https://ayaka14732.github.io/bert-related-paper-abstracts/", "isFamilyFriendly": true, "displayUrl": "https://ayaka14732.github.io/<b>bert</b>-related-paper-abstracts", "snippet": "Recently a new language representation model, <b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>), facilitates pre-training deep <b>bidirectional</b> <b>representations</b> on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring <b>BERT</b> for natural language understanding. In this work, we propose a joint intent classification and slot filling ...", "dateLastCrawled": "2022-01-30T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Comparing pre-trained language models for Spanish <b>hate speech</b> detection ...", "url": "https://www.sciencedirect.com/science/article/pii/S095741742030868X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S095741742030868X", "snippet": "In this paper, we explore two pre-trained LMs based on <b>transformers</b>: <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) (Devlin et al., 2018) and XLM (Conneau &amp; Lample, 2019). As far as we know, they are the only two multilingual models currently available. The <b>BERT</b> model was proposed by Devlin et al. (2018). It is a <b>bidirectional</b> ...", "dateLastCrawled": "2022-01-01T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Proceedings of the 15th International Workshop on Semantic Evaluation ...", "url": "https://aclanthology.org/volumes/2021.semeval-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2021.semeval-1", "snippet": "We developed a system for this task using a pre-trained language representation model called <b>BERT</b> that stands for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, and achieved good results. The average F1-score for Evaluation Phase 2, Part 1 was 0.4562 and ranked 7th, and the average F1-score for Evaluation Phase 2, Part 2 was 0.6541, and also ranked 7th.", "dateLastCrawled": "2022-02-02T18:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Authorship Attribution of Social Media and Literary Russian ...", "url": "https://www.researchgate.net/publication/357272762_Authorship_Attribution_of_Social_Media_and_Literary_Russian-Language_Texts_Using_Machine_Learning_Methods_and_Feature_Selection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357272762_Authorship_Attribution_of_Social...", "snippet": "short-term memory (LSTM), <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from T ransformers</b> (<b>BERT</b>), and fastT ext, that have not been used in previous studies, were applied to solve the problem. A particular", "dateLastCrawled": "2021-12-29T08:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A review on compound-protein interaction prediction methods: Data ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8008185/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8008185", "snippet": "Transformer has both an <b>encoder</b> and a decoder, unlike <b>BERT</b> only with an <b>encoder</b>, so that training can be possible to improve prediction accuracies. Convolutional Neural Networks. Inspired by the success in the computer vision domain, CNN was used to make structure-based binding affinity prediction in . Ragoza et al. used CNN to score CPI with the structural information of protein\u2013ligand complexes. In addition, CNN is also used for feature extraction: 1D protein-sequence-encoded vector ...", "dateLastCrawled": "2022-01-24T19:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "softwaresoftwaresystems \u2013 Software world", "url": "http://softwaresoftwaresystems.com/", "isFamilyFriendly": true, "displayUrl": "softwaresoftwaresystems.com", "snippet": "<b>BERT</b>, brief for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, can refine language a lot more naturally, understanding far better how humans encode details in a passage. Read our deep dive on <b>BERT</b> here. May 2020 Core Update. Google rolled out its relatively strong May 2020 broad core algorithm update over a few weeks in early May of 2020.", "dateLastCrawled": "2022-01-30T02:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Blog Archives - life stylee blog", "url": "https://www.lifestyleeblog.com/category/blog/blog-blog/", "isFamilyFriendly": true, "displayUrl": "https://www.lifestyleeblog.com/category/blog/blog-blog", "snippet": "<b>BERT</b>. <b>Transformers</b> <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> (BERTs) are the latest huge upgrade to how semantic search works. This affects around 10% of all queries since the end of 2019. Do not worry; it also took me a while to remember what <b>BERT</b> stands for.", "dateLastCrawled": "2022-01-14T22:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "XPRESSENGINE \u2013 SHARING, PUBLISHING. &amp; PLEASURE.", "url": "http://kfakokoji.com/", "isFamilyFriendly": true, "displayUrl": "kfakokoji.com", "snippet": "<b>BERT</b>, short for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, can refine language a lot more normally, comprehending better how people encode info in a passage. Review our deep dive on <b>BERT</b> here. May 2020 Core Update. Google turned out its reasonably strong May 2020 broad core formula update over a couple of weeks in very early May ...", "dateLastCrawled": "2022-02-03T07:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Language model pre-training method in NLP - Programmer Sought", "url": "https://www.programmersought.com/article/8681127806/", "isFamilyFriendly": true, "displayUrl": "https://www.programmersought.com/article/8681127806", "snippet": "The paper &quot;Deep Contextualized Word <b>Representations</b>&quot; comes from the work of the University of Washington, and finally was published at this year&#39;s NAACL conference and won the best paper. In fact, the predecessor of this work came from the same team published in ACL2017 &quot;Semi-supervised sequence tagging with <b>bidirectional</b> language models&quot; [4], but in this paper, they made the model more general. First, let&#39;s look at the motivation of their work. They think that a pre-trained word ...", "dateLastCrawled": "2021-11-05T07:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Long short-term memory network for learning sentences similarity using ...", "url": "https://link.springer.com/article/10.1007/s41870-021-00686-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s41870-021-00686-y", "snippet": "<b>BERT</b> calls for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. The prepared <b>BERT</b> model can also be significantly simplified to a large variety of NLP activities with just one additional yield layer. The <b>BERT</b> design expands on the Transformer. We right now have two variations: first, <b>BERT</b> Base consists of 12 attention heads, 12 layers (transformer blocks), and 110 million parameters. Second, <b>BERT</b> Large consists of 16 attention heads and 24 layers (transformer blocks) and 340 million ...", "dateLastCrawled": "2022-01-09T18:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Abstracts of <b>BERT</b>-Related Papers - ayaka14732.github.io", "url": "https://ayaka14732.github.io/bert-related-paper-abstracts/", "isFamilyFriendly": true, "displayUrl": "https://ayaka14732.github.io/<b>bert</b>-related-paper-abstracts", "snippet": "Recently a new language representation model, <b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>), facilitates pre-training deep <b>bidirectional</b> <b>representations</b> on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring <b>BERT</b> for natural language understanding. In this work, we propose a joint intent classification and slot filling ...", "dateLastCrawled": "2022-01-30T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Comparing pre-trained language models for Spanish <b>hate speech</b> detection ...", "url": "https://www.sciencedirect.com/science/article/pii/S095741742030868X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S095741742030868X", "snippet": "In this paper, we explore two pre-trained LMs based on <b>transformers</b>: <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) (Devlin et al., 2018) and XLM (Conneau &amp; Lample, 2019). As far as we know, they are the only two multilingual models currently available. The <b>BERT</b> model was proposed by Devlin et al. (2018). It is a <b>bidirectional</b> transformer pre-trained using two training strategies, Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) on a large corpus comprised of ...", "dateLastCrawled": "2022-01-01T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Proceedings of the 15th International Workshop on Semantic Evaluation ...", "url": "https://aclanthology.org/volumes/2021.semeval-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2021.semeval-1", "snippet": "We developed a system for this task using a pre-trained language representation model called <b>BERT</b> that stands for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, and achieved good results. The average F1-score for Evaluation Phase 2, Part 1 was 0.4562 and ranked 7th, and the average F1-score for Evaluation Phase 2, Part 2 was 0.6541, and also ranked 7th.", "dateLastCrawled": "2022-02-02T18:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "AutoDiscern: Rating the Quality of Online Health Information with ...", "url": "https://www.researchgate.net/publication/338291804_AutoDiscern_Rating_the_Quality_of_Online_Health_Information_with_Hierarchical_Encoder_Attention-based_Neural_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338291804_AutoDiscern_Rating_the_Quality_of...", "snippet": "Results: We introduce BioBERT (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> for Biomedical Text Mining), which is a domain specific language representation model pre-trained on large ...", "dateLastCrawled": "2021-12-21T21:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A review on compound-protein interaction prediction methods: Data ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8008185/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8008185", "snippet": "Transformer has both an <b>encoder</b> and a decoder, unlike <b>BERT</b> only with an <b>encoder</b>, so that training can be possible to improve prediction accuracies. Convolutional Neural Networks. Inspired by the success in the computer vision domain, CNN was used to make structure-based binding affinity prediction in . Ragoza et al. used CNN to score CPI with the structural information of protein\u2013ligand complexes. In addition, CNN is also used for feature extraction: 1D protein-sequence-encoded vector ...", "dateLastCrawled": "2022-01-24T19:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) RDMMFET: Representation of Dense Multimodality Fusion <b>Encoder</b> ...", "url": "https://www.researchgate.net/publication/355420376_RDMMFET_Representation_of_Dense_Multimodality_Fusion_Encoder_Based_on_Transformer", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355420376_RDMMFET_Representation_of_Dense...", "snippet": "PDF | Visual question answering (VQA) is the natural language question-answering of visual images. The model of VQA needs to make corresponding answers... | Find, read and cite all the research ...", "dateLastCrawled": "2022-01-22T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "softwaresoftwaresystems \u2013 Software world", "url": "http://softwaresoftwaresystems.com/", "isFamilyFriendly": true, "displayUrl": "softwaresoftwaresystems.com", "snippet": "<b>BERT</b>, brief for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, can refine language a lot more naturally, understanding far better how humans encode details in a passage. Read our deep dive on <b>BERT</b> here. May 2020 Core Update. Google rolled out its relatively strong May 2020 broad core algorithm update over a few weeks in early May of 2020.", "dateLastCrawled": "2022-01-30T02:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Blog Archives - life stylee blog", "url": "https://www.lifestyleeblog.com/category/blog/blog-blog/", "isFamilyFriendly": true, "displayUrl": "https://www.lifestyleeblog.com/category/blog/blog-blog", "snippet": "<b>BERT</b>. <b>Transformers</b> <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> (BERTs) are the latest huge upgrade to how semantic search works. This affects around 10% of all queries since the end of 2019. Do not worry; it also took me a while to remember what <b>BERT</b> stands for.", "dateLastCrawled": "2022-01-14T22:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Professional Web Design \u2013 SQL App", "url": "https://www.professionalwebdesignuk.com/", "isFamilyFriendly": true, "displayUrl": "https://www.professionalwebdesignuk.com", "snippet": "<b>BERT</b>, short for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, can refine language a lot more normally, comprehending far better how people encode details in a flow. Review our deep dive on <b>BERT</b> right here. May 2020 Core Update. Google turned out its relatively strong May 2020 wide core formula update over a few weeks in early May of ...", "dateLastCrawled": "2022-02-01T04:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Long short-term memory network for learning sentences similarity using ...", "url": "https://link.springer.com/article/10.1007/s41870-021-00686-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s41870-021-00686-y", "snippet": "<b>BERT</b> calls for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. The prepared <b>BERT</b> model <b>can</b> also be significantly simplified to a large variety of NLP activities with just one additional yield layer. The <b>BERT</b> design expands on the Transformer. We right now have two variations: first, <b>BERT</b> Base consists of 12 attention heads, 12 layers (transformer blocks), and 110 million parameters. Second, <b>BERT</b> Large consists of 16 attention heads and 24 layers (transformer blocks) and 340 million ...", "dateLastCrawled": "2022-01-09T18:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Abstracts of <b>BERT</b>-Related Papers - ayaka14732.github.io", "url": "https://ayaka14732.github.io/bert-related-paper-abstracts/", "isFamilyFriendly": true, "displayUrl": "https://ayaka14732.github.io/<b>bert</b>-related-paper-abstracts", "snippet": "Recently a new language representation model, <b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>), facilitates pre-training deep <b>bidirectional</b> <b>representations</b> on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring <b>BERT</b> for natural language understanding. In this work, we propose a joint intent classification and slot filling ...", "dateLastCrawled": "2022-01-30T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "softwaresoftwaresystems \u2013 Software world", "url": "http://softwaresoftwaresystems.com/", "isFamilyFriendly": true, "displayUrl": "softwaresoftwaresystems.com", "snippet": "<b>BERT</b>, brief for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, <b>can</b> refine language a lot more naturally, understanding far better how humans encode details in a passage. Read our deep dive on <b>BERT</b> here. May 2020 Core Update. Google rolled out its relatively strong May 2020 broad core algorithm update over a few weeks in early May of 2020.", "dateLastCrawled": "2022-01-30T02:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "antreasites \u2013 Tailor Made Service", "url": "https://www.antreasites.net/", "isFamilyFriendly": true, "displayUrl": "https://www.antreasites.net", "snippet": "<b>BERT</b>, brief for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, <b>can</b> process language extra naturally, comprehending far better how people encode info in a flow. Read our deep dive on <b>BERT</b> here. May 2020 Core Update. Google turned out its relatively solid May 2020 broad core algorithm update over a couple of weeks in very early May of 2020.", "dateLastCrawled": "2022-02-03T13:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Websolution \u2013 SEO &amp; Web hosting", "url": "https://www.ultimate-websolutions.com/", "isFamilyFriendly": true, "displayUrl": "https://www.ultimate-websolutions.com", "snippet": "<b>BERT</b>, brief for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, <b>can</b> refine language extra naturally, recognizing better just how people inscribe details in a flow. Read our deep dive on <b>BERT</b> here. May 2020 Core Update. Google presented its reasonably strong May 2020 wide core formula update over a couple of weeks in early May of 2020.", "dateLastCrawled": "2022-01-17T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "XPRESSENGINE \u2013 SHARING, PUBLISHING. &amp; PLEASURE.", "url": "http://kfakokoji.com/", "isFamilyFriendly": true, "displayUrl": "kfakokoji.com", "snippet": "<b>BERT</b>, short for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, <b>can</b> refine language a lot more normally, comprehending better how people encode info in a passage. Review our deep dive on <b>BERT</b> here. May 2020 Core Update. Google turned out its reasonably strong May 2020 broad core formula update over a couple of weeks in very early May ...", "dateLastCrawled": "2022-02-03T07:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Professional Web Design \u2013 SQL App", "url": "http://www.professionalwebdesignuk.com/", "isFamilyFriendly": true, "displayUrl": "www.professionalwebdesignuk.com", "snippet": "<b>BERT</b>, short for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, <b>can</b> refine language a lot more normally, comprehending far better how people encode details in a flow. Review our deep dive on <b>BERT</b> right here. May 2020 Core Update. Google turned out its relatively strong May 2020 wide core formula update over a few weeks in early May of ...", "dateLastCrawled": "2022-01-25T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Annual Meeting of the Association for Computational Linguistics (2020 ...", "url": "https://arne.chark.eu/anthology/events/acl-2020/", "isFamilyFriendly": true, "displayUrl": "https://arne.chark.eu/anthology/events/acl-2020", "snippet": "The T-TA computes contextual language <b>representations</b> without repetition and displays the benefits of a deep <b>bidirectional</b> architecture, such as that of <b>BERT</b>. In computation time experiments in a CPU environment, the proposed T-TA performs over six times faster than the <b>BERT</b>-like model on a reranking task and twelve times faster on a semantic similarity task. Furthermore, the T-TA shows competitive or even better accuracies than those of <b>BERT</b> on the above tasks. Code is available at https ...", "dateLastCrawled": "2022-01-11T22:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Blog Archives - life stylee blog", "url": "https://www.lifestyleeblog.com/category/blog/blog-blog/", "isFamilyFriendly": true, "displayUrl": "https://www.lifestyleeblog.com/category/blog/blog-blog", "snippet": "We <b>can</b> think of RankBrain as an upgrade to Hummingbird, not a standalone search algorithm. This is one of the strongest ranking signals , but the only thing you <b>can</b> proactively do to optimize it is to satisfy search intent. <b>BERT</b>. <b>Transformers</b> <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> (BERTs) are the latest huge upgrade to how semantic search works.", "dateLastCrawled": "2022-01-14T22:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Conference on Empirical Methods in Natural Language Processing (and ...", "url": "https://aclanthology.org/events/emnlp-2021/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/events/emnlp-2021", "snippet": "Previous work mainly focuses on improving cross-lingual transfer for NLU tasks with a multilingual pretrained <b>encoder</b> (MPE), or improving the performance on supervised machine translation with <b>BERT</b>. However, it is under-explored that whether the MPE <b>can</b> help to facilitate the cross-lingual transferability of NMT model. In this paper, we focus on a zero-shot cross-lingual transfer task in NMT. In this task, the NMT model is trained with parallel dataset of only one language pair and an off ...", "dateLastCrawled": "2022-01-29T06:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Long short-term memory network for learning sentences similarity using ...", "url": "https://link.springer.com/article/10.1007/s41870-021-00686-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s41870-021-00686-y", "snippet": "<b>BERT</b> calls for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. The prepared <b>BERT</b> model <b>can</b> also be significantly simplified to a large variety of NLP activities with just one additional yield layer. The <b>BERT</b> design expands on the Transformer. We right now have two variations: first, <b>BERT</b> Base consists of 12 attention heads, 12 layers (transformer blocks), and 110 million parameters. Second, <b>BERT</b> Large consists of 16 attention heads and 24 layers (transformer blocks) and 340 million ...", "dateLastCrawled": "2022-01-09T18:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Abstracts of <b>BERT</b>-Related Papers - ayaka14732.github.io", "url": "https://ayaka14732.github.io/bert-related-paper-abstracts/", "isFamilyFriendly": true, "displayUrl": "https://ayaka14732.github.io/<b>bert</b>-related-paper-abstracts", "snippet": "Recently a new language representation model, <b>BERT</b> (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>), facilitates pre-training deep <b>bidirectional</b> <b>representations</b> on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring <b>BERT</b> for natural language understanding. In this work, we propose a joint intent classification and slot filling ...", "dateLastCrawled": "2022-01-30T18:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Proceedings of the 15th International Workshop on Semantic Evaluation ...", "url": "https://aclanthology.org/volumes/2021.semeval-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/2021.semeval-1", "snippet": "We developed a system for this task using a pre-trained language representation model called <b>BERT</b> that stands for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, and achieved good results. The average F1-score for Evaluation Phase 2, Part 1 was 0.4562 and ranked 7th, and the average F1-score for Evaluation Phase 2, Part 2 was 0.6541 ...", "dateLastCrawled": "2022-02-02T18:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Comparing pre-trained language models for Spanish <b>hate speech</b> detection ...", "url": "https://www.sciencedirect.com/science/article/pii/S095741742030868X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S095741742030868X", "snippet": "In this paper, we explore two pre-trained LMs based on <b>transformers</b>: <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) (Devlin et al., 2018) and XLM (Conneau &amp; Lample, 2019). As far as we know, they are the only two multilingual models currently available. The <b>BERT</b> model was proposed by Devlin et al. (2018). It is a <b>bidirectional</b> ...", "dateLastCrawled": "2022-01-01T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "AutoDiscern: Rating the Quality of Online Health Information with ...", "url": "https://www.researchgate.net/publication/338291804_AutoDiscern_Rating_the_Quality_of_Online_Health_Information_with_Hierarchical_Encoder_Attention-based_Neural_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338291804_AutoDiscern_Rating_the_Quality_of...", "snippet": "Results: We introduce BioBERT (<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> for Biomedical Text Mining), which is a domain specific language representation model pre-trained on large ...", "dateLastCrawled": "2021-12-21T21:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "antreasites \u2013 Tailor Made Service", "url": "https://www.antreasites.net/", "isFamilyFriendly": true, "displayUrl": "https://www.antreasites.net", "snippet": "<b>BERT</b>, brief for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, <b>can</b> process language extra naturally, comprehending far better how people encode info in a flow. Read our deep dive on <b>BERT</b> here. May 2020 Core Update. Google turned out its relatively solid May 2020 broad core algorithm update over a couple of weeks in very early May of 2020.", "dateLastCrawled": "2022-02-03T13:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "XPRESSENGINE \u2013 SHARING, PUBLISHING. &amp; PLEASURE.", "url": "http://kfakokoji.com/", "isFamilyFriendly": true, "displayUrl": "kfakokoji.com", "snippet": "<b>BERT</b>, short for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, <b>can</b> refine language a lot more normally, comprehending better how people encode info in a passage. Review our deep dive on <b>BERT</b> here. May 2020 Core Update. Google turned out its reasonably strong May 2020 broad core formula update over a couple of weeks in very early May ...", "dateLastCrawled": "2022-02-03T07:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Professional Web Design \u2013 SQL App", "url": "https://www.professionalwebdesignuk.com/", "isFamilyFriendly": true, "displayUrl": "https://www.professionalwebdesignuk.com", "snippet": "<b>BERT</b>, short for <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>, <b>can</b> refine language a lot more normally, comprehending far better how people encode details in a flow. Review our deep dive on <b>BERT</b> right here. May 2020 Core Update. Google turned out its relatively strong May 2020 wide core formula update over a few weeks in early May of ...", "dateLastCrawled": "2022-02-01T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Language model pre-training method in NLP - Programmer Sought", "url": "https://www.programmersought.com/article/8681127806/", "isFamilyFriendly": true, "displayUrl": "https://www.programmersought.com/article/8681127806", "snippet": "The paper &quot;Deep Contextualized Word <b>Representations</b>&quot; comes from the work of the University of Washington, and finally was published at this year&#39;s NAACL conference and won the best paper. In fact, the predecessor of this work came from the same team published in ACL2017 &quot;Semi-supervised sequence tagging with <b>bidirectional</b> language models&quot; [4], but in this paper, they made the model more general. First, let&#39;s look at the motivation of their work. They think that a pre-trained word ...", "dateLastCrawled": "2021-11-05T07:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Annual Meeting of the Association for Computational Linguistics (2020 ...", "url": "https://arne.chark.eu/anthology/events/acl-2020/", "isFamilyFriendly": true, "displayUrl": "https://arne.chark.eu/anthology/events/acl-2020", "snippet": "The T-TA computes contextual language <b>representations</b> without repetition and displays the benefits of a deep <b>bidirectional</b> architecture, such as that of <b>BERT</b>. In computation time experiments in a CPU environment, the proposed T-TA performs over six times faster than the <b>BERT</b>-like model on a reranking task and twelve times faster on a semantic similarity task. Furthermore, the T-TA shows competitive or even better accuracies than those of <b>BERT</b> on the above tasks. Code is available at https ...", "dateLastCrawled": "2022-01-11T22:16:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "14.8. <b>Bidirectional Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b> ...", "url": "https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_natural-language-processing-pretraining/<b>bert</b>.html", "snippet": "Combining the best of both worlds, <b>BERT</b> (<b>Bidirectional Encoder</b> <b>Representations</b> <b>from Transformers</b>) encodes context bidirectionally and requires minimal architecture changes for a wide range of natural language processing tasks [Devlin et al., 2018]. Using a pretrained transformer <b>encoder</b>, <b>BERT</b> is able to represent any token based on its <b>bidirectional</b> context. During supervised <b>learning</b> of downstream tasks, <b>BERT</b> is similar to GPT in two aspects. First, <b>BERT</b> <b>representations</b> will be fed into an ...", "dateLastCrawled": "2022-02-02T10:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "This hampers <b>learning</b> unnecessarily, they argue, and they proposed a <b>bidirectional</b> variant instead: <b>BERT</b>, or <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. It is covered in this article. Firstly, we\u2019ll briefly take a look at finetuning-based approaches in NLP, which is followed by <b>BERT</b> as well.", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Med-BERT: pretrained contextualized embeddings on large</b>-scale ...", "url": "https://www.nature.com/articles/s41746-021-00455-y", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41746-021-00455-y", "snippet": "Recently, <b>bidirectional</b> <b>encoder</b> <b>representations</b> <b>from transformers</b> (<b>BERT</b>) and related models have achieved tremendous successes in the natural language processing domain. The pretraining of <b>BERT</b> on ...", "dateLastCrawled": "2022-01-28T20:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "HIBERT: Document Level Pre-training of Hierarchical <b>Bidirectional</b> ...", "url": "https://aclanthology.org/P19-1499.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/P19-1499.pdf", "snippet": "<b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>) for document encoding and a method to pre-train it using unlabeled data. We apply the pre-trained HIBERT to our summa-rization model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times dataset. We also achieve the state-of-the-art performance on these two datasets. 1 Introduction Automatic document summarization is the task of rewriting a ...", "dateLastCrawled": "2022-02-02T06:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "DNABERT: <b>pre-trained Bidirectional Encoder Representations from</b> ...", "url": "https://www.researchgate.net/publication/349060790_DNABERT_pre-trained_Bidirectional_Encoder_Representations_from_Transformers_model_for_DNA-language_in_genome", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349060790_DNA<b>BERT</b>_pre-trained_<b>Bidirectional</b>...", "snippet": "<b>Bidirectional</b> <b>encoder</b> <b>representations</b> from Transformer (<b>BERT</b>) is a language-based deep <b>learning</b> model that is highly interpretable. Therefore, a model based on <b>BERT</b> architecture can potentially ...", "dateLastCrawled": "2022-01-29T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The Ultimate Guide To Different Word Embedding Techniques In NLP ...", "url": "https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html", "snippet": "<b>BERT</b> \u2014 <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b>. Introduced by Google in 2019, <b>BERT</b> belongs to a class of NLP-based language algorithms known as <b>transformers</b>. <b>BERT</b> is a massive pre-trained deeply <b>bidirectional</b> <b>encoder</b>-based transformer model that comes in two variants. <b>BERT</b>-Base has 110 million parameters, and <b>BERT</b>-Large has ...", "dateLastCrawled": "2022-02-03T07:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "LawBERT: Towards a Legal Domain-Specific <b>BERT</b>? | by Erin Yijie Zhang ...", "url": "https://towardsdatascience.com/lawbert-towards-a-legal-domain-specific-bert-716886522b49", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/law<b>bert</b>-towards-a-legal-domain-specific-<b>bert</b>-716886522b49", "snippet": "Google\u2019s <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> (<b>BERT</b>) is a large-scale pre-trained autoencoding language model developed in 2018. Its development has been described as the NLP community\u2019s \u201cImageNet moment\u201d, largely because of how adept <b>BERT</b> is at performing downstream NLP language understanding tasks with very little backpropagation and fine-tuning needed (usually only 2\u20134 epochs).", "dateLastCrawled": "2022-01-27T06:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to perform Text Summarization with Python, HuggingFace <b>Transformers</b> ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2020/12/21/easy-text-summarization-with-hugging...", "snippet": "The <b>Bidirectional</b> <b>Encoder</b> <b>Representations</b> <b>from Transformers</b> by Devlin et al. (2018) takes the <b>encoder</b> segment from the classic (or vanilla) Transformer, slightly changes how the inputs are generated (by means of WordPiece rather than learned embeddings) and changes the <b>learning</b> task into a Masked Language Model plus Next Sentence Prediction (NSP) rather than training a simple language model. They also follow the argument for pretraining and subsequent fine-tuning: by taking the <b>encoder</b> ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Introduction to Text <b>Representations</b> for Language Processing \u2014 Part 2 ...", "url": "https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-text-<b>representations</b>-for-language...", "snippet": "<b>BERT</b>. <b>BERT</b> is a paper from the Google AI team in the name of <b>BERT</b>: Pre-training of Deep <b>Bidirectional</b> <b>Transformers</b> for Language Understanding which came out of May 2019. It is a new self-supervised <b>learning</b> task for pre-training <b>transformers</b> in order to fine-tune them for downstream tasks", "dateLastCrawled": "2022-01-31T04:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Ultimate Guide To Different Word Embedding Techniques In NLP ...", "url": "https://machinelearningmastery.in/2021/11/10/the-ultimate-guide-to-different-word-embedding-techniques-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.in/2021/11/10/the-ultimate-guide-to-different-word...", "snippet": "Let\u2019s have a look at some of the most promising word embedding techniques in NLP. 1. TF-IDF \u2014 Term Frequency-Inverse Document Frequency. TF-IDF is a <b>machine</b> <b>learning</b> (ML) algorithm based on a statistical measure of finding the relevance of words in the text.", "dateLastCrawled": "2022-01-09T14:18:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bert (bidirectional encoder representations from transformers))  is like +(two-way mirror)", "+(bert (bidirectional encoder representations from transformers)) is similar to +(two-way mirror)", "+(bert (bidirectional encoder representations from transformers)) can be thought of as +(two-way mirror)", "+(bert (bidirectional encoder representations from transformers)) can be compared to +(two-way mirror)", "machine learning +(bert (bidirectional encoder representations from transformers) AND analogy)", "machine learning +(\"bert (bidirectional encoder representations from transformers) is like\")", "machine learning +(\"bert (bidirectional encoder representations from transformers) is similar\")", "machine learning +(\"just as bert (bidirectional encoder representations from transformers)\")", "machine learning +(\"bert (bidirectional encoder representations from transformers) can be thought of as\")", "machine learning +(\"bert (bidirectional encoder representations from transformers) can be compared to\")"]}