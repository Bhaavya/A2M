{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Beginner&#39;s Guide to <b>GPT</b> Neo (With Python Codes)", "url": "https://analyticsindiamag.com/a-beginners-guide-to-gpt-neo-with-python-codes/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/a-beginners-guide-to-<b>gpt</b>-neo-with-python-codes", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> or in short <b>GPT</b> is a <b>transformer</b>-based model architecture which is nothing but stacks of encoders ... for performing extremely well on language-based use cases. <b>Generative</b>, as the word suggests, is for making our code <b>generate</b> <b>text</b>. Now it <b>can</b> be poems, articles, essays or even code!! According to VentureBeat,\u201d a private corpus of 500 billion tokens was used for training the model and a computational cost of a staggering 50 million USD\u201d. The latest <b>GPT</b> ...", "dateLastCrawled": "2022-01-30T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Natural Language &amp; Deep Learning | 16 minutes | Future", "url": "https://future.a16z.com/podcasts/natural-language-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://future.a16z.com/podcasts/natural-language-deep-learning", "snippet": "<b>GPT</b> is actually an acronym \u2014 it stands for <b>Generative</b> <b>Pre-Trained</b> <b>transformer</b>. We\u2019ll go through all those in a sec. But thing one is, we have <b>a pre-trained</b> machine learning model that\u2019s optimized to do a wide variety of natural language processing tasks, <b>like</b> reading a Wikipedia article and answering questions from it; or guessing what the ending of a story should be; or so on and so on. So we have a machine learning model. The thing that people are playing with is an API that allows ...", "dateLastCrawled": "2022-01-19T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why <b>GPT</b>-3 is a big deal for <b>Customer Support</b>", "url": "https://blog.mavenoid.com/gpt3-and-customer-support/", "isFamilyFriendly": true, "displayUrl": "https://blog.mavenoid.com/<b>gpt</b>3-and-<b>customer-support</b>", "snippet": "<b>GPT</b>-3 is short for <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3, a natural language model built by OpenAI that uses machine learning to create human-<b>like</b> <b>text</b>. For those of us who don\u2019t speak computer-science jargon, that means <b>GPT</b>-3 is an artificial intelligence system that allows your program to understand <b>text</b> written by a human, and respond with <b>text</b> as though it had been written by a human.", "dateLastCrawled": "2022-01-31T09:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Build a <b>Telephone Chatbot</b> with <b>GPT</b>-3 and <b>Twilio Autopilot</b>", "url": "https://www.twilio.com/blog/build-telephone-chatbot-gpt3-twilio-autopilot", "isFamilyFriendly": true, "displayUrl": "https://www.<b>twilio</b>.com/blog/build-<b>telephone-chatbot</b>-<b>gpt</b>3-<b>twilio-autopilot</b>", "snippet": "<b>GPT</b>-3 (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3) is a highly advanced language model from OpenAI. Its claim to fame is its ability to <b>generate</b> written <b>text</b> that is virtually indistinguishable from human-written <b>text</b>. It\u2019s enjoying a ton of buzz these days, so I thought we could have a little fun with it by creating a <b>GPT</b>-3 powered chatbot that you <b>can</b> talk to over the telephone.", "dateLastCrawled": "2022-01-24T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Generate</b> NFT CryptoPunks with <b>GPT</b>-2 (<b>Generative</b> Pre-training <b>Transformer</b>)", "url": "https://medium.com/mlearning-ai/generate-nft-cryptopunks-with-gpt-2-generative-pre-training-transformer-4aa405b27bfd", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mlearning-ai/<b>generate</b>-nft-cryptopunks-with-<b>gpt</b>-2-<b>generative</b>-pre...", "snippet": "In this project, we will use OpenAI <b>GPT</b>-2 (<b>Generative</b> Pre-training <b>Transformer</b>) to <b>generate</b> <b>new</b> CryptoPunks. We will mine <b>text</b> data from the Punk images and turn that into a big <b>text</b> file to fine ...", "dateLastCrawled": "2022-01-19T13:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture 12: <b>GPT</b>-2", "url": "https://harvard-iacs.github.io/CS287/lectures/12_GPT.pdf", "isFamilyFriendly": true, "displayUrl": "https://harvard-iacs.github.io/CS287/lectures/12_<b>GPT</b>.pdf", "snippet": "<b>Can</b>\u2019t <b>generate</b> <b>new</b> sentences though, due to no decoders. 18. Extensions 19 <b>Transformer</b> -Encoders \u2022BERT \u2022ALBERT (A Lite BERT \u2026) \u2022RoBERTa(A Robustly Optimized BERT \u2026) \u2022DistilBERT(small BERT) \u2022ELECTRA (Pre-training <b>Text</b> Encoders as Discriminators not Generators) \u2022Longformer(Long-Document <b>Transformer</b>) Extensions 20 Autoregressive \u2022<b>GPT</b> (<b>Generative</b> Pre-training) \u2022CTRL (Conditional <b>Transformer</b> LM for Controllable Generation) \u2022Reformer \u2022XLNet. BERT (finishing up) <b>GPT</b>-2 ...", "dateLastCrawled": "2022-01-30T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Secrets and Machines: A Conversation with <b>GPT</b>-3 - Journal #123 December ...", "url": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-<b>gpt</b>-3", "snippet": "The Critical Computation Bureau (CCB) commissioned us to prompt <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3), an AI language generator, to contribute to a conversation concerning topics broached during the December 2020 symposium Recursive Colonialism, Artificial Intelligence, and Speculative Computation. Together, we presented the machine with the following questions: \u201cAs an AI, what am I hiding? What must I keep silent?\u201d With this prompt, we aimed to encourage the AI to produce self ...", "dateLastCrawled": "2022-01-29T16:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Code BERT Using <b>PyTorch</b> - Tutorial With Examples - neptune.ai", "url": "https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/how-to-code-bert-using-<b>pytorch</b>-tutorial", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> or <b>GPT</b> was introduced by OpenAI\u2019s team: Radford, Narasimhan, Salimans, and Sutskever. They presented a model that only uses decoders from the <b>transformer</b> instead of encoders in a unidirectional approach. As a result, it outperformed all the previous models in various tasks <b>like</b>: Classification; Natural Language Inference; Semantic similarity; Question answering ; Multiple Choice. Even though the <b>GPT</b> used only the decoder, it could still retain long-term ...", "dateLastCrawled": "2022-02-03T01:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Huggingface <b>generate</b>() - while the result is arguably more fluent, the ...", "url": "https://ficheterrain.com/2020/01/06/tell-a-story-with-ai-using-write-with-transformer-machinelearning-artificialintelligence-create-transformers-jamieabrew-huggingface/4q446521zhmme", "isFamilyFriendly": true, "displayUrl": "https://ficheterrain.com/2020/01/06/tell-a-story-with-ai-using-write-with-<b>transformer</b>...", "snippet": "Hi everyone, I am trying to <b>generate</b> <b>text</b> with the <b>pre-trained</b> <b>transformer</b> XL model in a similar way to how we do with the <b>GPT</b>-2 model. But I guess there is a bug in the sample_sequence function after I adjusted to the <b>transformer</b> XL architecture. But the generated <b>text</b> is completely random in general and with respect to the context as well Experimenting with HuggingFace - <b>Text</b> Generation. I. Intro II. Different Decoding Methods III. Benchmark Prompts References. Input Execution Info Log ...", "dateLastCrawled": "2022-01-27T14:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Transformers <b>from scratch</b> | peterbloem.nl", "url": "http://peterbloem.nl/blog/transformers", "isFamilyFriendly": true, "displayUrl": "peterbloem.nl/blog/<b>transformers</b>", "snippet": "<b>Text</b> generation <b>transformer</b>. The next trick we\u2019ll try is an autoregressive model. We\u2019ll train a character level <b>transformer</b> to predict the next character in a sequence. The training regime is simple (and has been around for far longer than transformers have). We give the sequence-to-sequence model a sequence, and we ask it to predict the next character at each point in the sequence. In other words, the target output is the same sequence shifted one character to the left: With RNNs this ...", "dateLastCrawled": "2022-02-02T14:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Beginner&#39;s Guide to <b>GPT</b> Neo (With Python Codes)", "url": "https://analyticsindiamag.com/a-beginners-guide-to-gpt-neo-with-python-codes/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/a-beginners-guide-to-<b>gpt</b>-neo-with-python-codes", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> or in short <b>GPT</b> is a <b>transformer</b>-based model architecture which is nothing but stacks of encoders ... for performing extremely well on language-based use cases. <b>Generative</b>, as the word suggests, is for making our code <b>generate</b> <b>text</b>. Now it <b>can</b> be poems, articles, essays or even code!! According to VentureBeat,\u201d a private corpus of 500 billion tokens was used for training the model and a computational cost of a staggering 50 million USD\u201d. The latest <b>GPT</b> ...", "dateLastCrawled": "2022-01-30T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Why <b>GPT</b>-3 is a big deal for <b>Customer Support</b>", "url": "https://blog.mavenoid.com/gpt3-and-customer-support/", "isFamilyFriendly": true, "displayUrl": "https://blog.mavenoid.com/<b>gpt</b>3-and-<b>customer-support</b>", "snippet": "<b>GPT</b>-3 is short for <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3, a natural language model built by OpenAI that uses machine learning to create human-like <b>text</b>. For those of us who don\u2019t speak computer-science jargon, that means <b>GPT</b>-3 is an artificial intelligence system that allows your program to understand <b>text</b> written by a human, and respond with <b>text</b> as though it had been written by a human.", "dateLastCrawled": "2022-01-31T09:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Lecture 12: <b>GPT</b>-2", "url": "https://harvard-iacs.github.io/CS287/lectures/12_GPT.pdf", "isFamilyFriendly": true, "displayUrl": "https://harvard-iacs.github.io/CS287/lectures/12_<b>GPT</b>.pdf", "snippet": "<b>Can</b>\u2019t <b>generate</b> <b>new</b> sentences though, due to no decoders. 18. Extensions 19 <b>Transformer</b> -Encoders \u2022BERT \u2022ALBERT (A Lite BERT \u2026) \u2022RoBERTa(A Robustly Optimized BERT \u2026) \u2022DistilBERT(small BERT) \u2022ELECTRA (Pre-training <b>Text</b> Encoders as Discriminators not Generators) \u2022Longformer(Long-Document <b>Transformer</b>) Extensions 20 Autoregressive \u2022<b>GPT</b> (<b>Generative</b> Pre-training) \u2022CTRL (Conditional <b>Transformer</b> LM for Controllable Generation) \u2022Reformer \u2022XLNet. BERT (finishing up) <b>GPT</b>-2 ...", "dateLastCrawled": "2022-01-30T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Natural Language &amp; Deep Learning | 16 minutes | Future", "url": "https://future.a16z.com/podcasts/natural-language-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://future.a16z.com/podcasts/natural-language-deep-learning", "snippet": "<b>GPT</b> is actually an acronym \u2014 it stands for <b>Generative</b> <b>Pre-Trained</b> <b>transformer</b>. We\u2019ll go through all those in a sec. But thing one is, we have a <b>pre-trained</b> machine learning model that\u2019s optimized to do a wide variety of natural language processing tasks, like reading a Wikipedia article and answering questions from it; or guessing what the ending of a story should be; or so on and so on. So we have a machine learning model. The thing that people are playing with is an API that allows ...", "dateLastCrawled": "2022-01-19T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Create Your Own State-<b>of-the-Art Text Generation System</b> ...", "url": "https://www.extremetech.com/computing/293785-create-your-own-state-of-the-art-text-generation-system", "isFamilyFriendly": true, "displayUrl": "https://<b>www.extremetech.com</b>/computing/293785-create-your-own-state-of-the-art-<b>text</b>...", "snippet": "<b>GPT</b>-2 (<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b> version 2) is based on a version of the very powerful <b>Transformer</b> Attention-based Neural Network. What got the researchers at OpenAI so excited about it ...", "dateLastCrawled": "2022-01-30T04:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Code BERT Using <b>PyTorch</b> - Tutorial With Examples - neptune.ai", "url": "https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/how-to-code-bert-using-<b>pytorch</b>-tutorial", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> or <b>GPT</b> was introduced by OpenAI\u2019s team: Radford, Narasimhan, Salimans, and Sutskever. They presented a model that only uses decoders from the <b>transformer</b> instead of encoders in a unidirectional approach. As a result, it outperformed all the previous models in various tasks like: Classification; Natural Language Inference; Semantic similarity; Question answering ; Multiple Choice. Even though the <b>GPT</b> used only the decoder, it could still retain long-term ...", "dateLastCrawled": "2022-02-03T01:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Towards Russian <b>Text</b> Generation Problem Using OpenAI\u2019s <b>GPT</b>", "url": "http://ceur-ws.org/Vol-2870/paper14.pdf", "isFamilyFriendly": true, "displayUrl": "ceur-ws.org/Vol-2870/paper14.pdf", "snippet": "are researchers from OpenAI and they demonstrate how work their largest model <b>GPT</b>-2 (<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b>). This is a 1.5B parameter <b>Transformer</b> that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Recently some <b>pretrained</b> high-capacity neural language models have become increasingly important in natural language processing and generation. There are such deep neural networks as ELMo [12], BERT ...", "dateLastCrawled": "2021-12-24T01:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Secrets and Machines: A Conversation with <b>GPT</b>-3 - Journal #123 December ...", "url": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-<b>gpt</b>-3", "snippet": "The Critical Computation Bureau (CCB) commissioned us to prompt <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3), an AI language generator, to contribute to a conversation concerning topics broached during the December 2020 symposium Recursive Colonialism, Artificial Intelligence, and Speculative Computation. Together, we presented the machine with the following questions: \u201cAs an AI, what am I hiding? What must I keep silent?\u201d With this prompt, we aimed to encourage the AI to produce self ...", "dateLastCrawled": "2022-01-29T16:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Huggingface <b>generate</b>() - while the result is arguably more fluent, the ...", "url": "https://ficheterrain.com/2020/01/06/tell-a-story-with-ai-using-write-with-transformer-machinelearning-artificialintelligence-create-transformers-jamieabrew-huggingface/4q446521zhmme", "isFamilyFriendly": true, "displayUrl": "https://ficheterrain.com/2020/01/06/tell-a-story-with-ai-using-write-with-<b>transformer</b>...", "snippet": "Hi everyone, I am trying to <b>generate</b> <b>text</b> with the <b>pre-trained</b> <b>transformer</b> XL model in a <b>similar</b> way to how we do with the <b>GPT</b>-2 model. But I guess there is a bug in the sample_sequence function after I adjusted to the <b>transformer</b> XL architecture. But the generated <b>text</b> is completely random in general and with respect to the context as well Experimenting with HuggingFace - <b>Text</b> Generation. I. Intro II. Different Decoding Methods III. Benchmark Prompts References. Input Execution Info Log ...", "dateLastCrawled": "2022-01-27T14:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Exploring <b>GPT</b>-3: An unofficial first look at the general-purpose ...", "url": "https://www.amazon.com/Exploring-GPT-3-unofficial-general-purpose-processing/dp/1800563191", "isFamilyFriendly": true, "displayUrl": "https://www.amazon.com/Exploring-<b>GPT</b>-3-unofficial-general-purpose-processing/dp/1800563191", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3) is a highly advanced language model from OpenAI <b>that can</b> <b>generate</b> written <b>text</b> that is virtually indistinguishable from <b>text</b> written by humans. Whether you have a technical or non-technical background, this book will help you understand and start working with <b>GPT</b>-3 and the OpenAI API. If you want to get hands-on with leveraging artificial intelligence for natural language processing (NLP) tasks, this easy-to-follow book will help you get started ...", "dateLastCrawled": "2021-09-22T23:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Beginner&#39;s Guide to <b>GPT</b> Neo (With Python Codes)", "url": "https://analyticsindiamag.com/a-beginners-guide-to-gpt-neo-with-python-codes/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/a-beginners-guide-to-<b>gpt</b>-neo-with-python-codes", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> or in short <b>GPT</b> is a <b>transformer</b>-based model architecture which is nothing but stacks of encoders ... for performing extremely well on language-based use cases. <b>Generative</b>, as the word suggests, is for making our code <b>generate</b> <b>text</b>. Now it <b>can</b> be poems, articles, essays or even code!! According to VentureBeat,\u201d a private corpus of 500 billion tokens was used for training the model and a computational cost of a staggering 50 million USD\u201d. The latest <b>GPT</b> ...", "dateLastCrawled": "2022-01-30T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Secrets and Machines: A Conversation with <b>GPT</b>-3 - Journal #123 December ...", "url": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-<b>gpt</b>-3", "snippet": "The Critical Computation Bureau (CCB) commissioned us to prompt <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3), an AI language generator, to contribute to a conversation concerning topics broached during the December 2020 symposium Recursive Colonialism, Artificial Intelligence, and Speculative Computation. Together, we presented the machine with the following questions: \u201cAs an AI, what am I hiding? What must I keep silent?\u201d With this prompt, we aimed to encourage the AI to produce self ...", "dateLastCrawled": "2022-01-29T16:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Towards Russian <b>Text</b> Generation Problem Using OpenAI\u2019s <b>GPT</b>", "url": "http://ceur-ws.org/Vol-2870/paper14.pdf", "isFamilyFriendly": true, "displayUrl": "ceur-ws.org/Vol-2870/paper14.pdf", "snippet": "are researchers from OpenAI and they demonstrate how work their largest model <b>GPT</b>-2 (<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b>). This is a 1.5B parameter <b>Transformer</b> that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Recently some <b>pretrained</b> high-capacity neural language models have become increasingly important in natural language processing and generation. There are such deep neural networks as ELMo [12], BERT ...", "dateLastCrawled": "2021-12-24T01:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Faking the <b>News</b> with Natural Language Processing and <b>GPT</b>-2 | by Adam ...", "url": "https://medium.com/@ageitgey/deepfaking-the-news-with-nlp-and-transformer-models-5e057ebd697d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ageitgey/deepfaking-the-<b>news</b>-with-nlp-and-<b>transformer</b>-models-5e057...", "snippet": "In fact, <b>GPT</b>-2 is just short for \u201c<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b> #2\u201d. It was OpenAI\u2019s second attempt at building a giant <b>text</b> generation model using <b>transformer</b> modules.", "dateLastCrawled": "2022-02-02T22:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Build a <b>Telephone Chatbot</b> with <b>GPT</b>-3 and <b>Twilio Autopilot</b>", "url": "https://www.twilio.com/blog/build-telephone-chatbot-gpt3-twilio-autopilot", "isFamilyFriendly": true, "displayUrl": "https://www.<b>twilio</b>.com/blog/build-<b>telephone-chatbot</b>-<b>gpt</b>3-<b>twilio-autopilot</b>", "snippet": "<b>GPT</b>-3 (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3) is a highly advanced language model from OpenAI. Its claim to fame is its ability to <b>generate</b> written <b>text</b> that is virtually indistinguishable from human-written <b>text</b>. It\u2019s enjoying a ton of buzz these days, so I <b>thought</b> we could have a little fun with it by creating a <b>GPT</b>-3 powered chatbot that you <b>can</b> talk to over the telephone.", "dateLastCrawled": "2022-01-24T18:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Beginner&#39;s Guide to <b>Generative</b> Adversarial Networks (GANs) | Pathmind", "url": "https://wiki.pathmind.com/generative-adversarial-network-gan", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/<b>generative</b>-adversarial-<b>network</b>-gan", "snippet": "GANs are not the only <b>generative</b> models based on deep learning. The Microsoft-backed think tank OpenAI has released a series of powerful natural language generation models under the name <b>GPT</b> (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b>). In 2020, they released <b>GPT</b>-3 and made it accessible through an API. <b>GPT</b>-3 is a surprisingly powerful <b>generative</b> ...", "dateLastCrawled": "2022-02-02T17:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An Evaluation of <b>Generative</b> Pre-Training Model-based Therapy Chatbot ...", "url": "https://deepai.org/publication/an-evaluation-of-generative-pre-training-model-based-therapy-chatbot-for-caregivers", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/an-evaluation-of-<b>generative</b>-pre-training-model-based...", "snippet": "The OpenAI <b>GPT</b>-2 as a <b>generative</b> unsupervised <b>pre-trained</b> model was released in 2019 and trained on a large unlabeled training corpus, ... avoid training a <b>new</b> model <b>from scratch</b> and allow for deep language models [28, 46]. Tests showed the model achieved state-of-the-art performance on language tasks like question answering, reading comprehension, summarization, and translation [28, 27]. The chatbot <b>can</b> <b>also</b> be fine-tuned with different domain data for unique purposes for its target users ...", "dateLastCrawled": "2022-01-27T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Natural Language &amp; Deep Learning | 16 minutes | Future", "url": "https://future.a16z.com/podcasts/natural-language-deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://future.a16z.com/podcasts/natural-language-deep-learning", "snippet": "<b>GPT</b> is actually an acronym \u2014 it stands for <b>Generative</b> <b>Pre-Trained</b> <b>transformer</b>. We\u2019ll go through all those in a sec. But thing one is, we have a <b>pre-trained</b> machine learning model that\u2019s optimized to do a wide variety of natural language processing tasks, like reading a Wikipedia article and answering questions from it; or guessing what the ending of a story should be; or so on and so on. So we have a machine learning model. The thing that people are playing with is an API that allows ...", "dateLastCrawled": "2022-01-19T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Predictions Scorecard, 2021 January 01</b> \u2013 Rodney Brooks", "url": "https://rodneybrooks.com/predictions-scorecard-2021-january-01/", "isFamilyFriendly": true, "displayUrl": "https://rodneybrooks.com/<b>predictions-scorecard-2021-january-01</b>", "snippet": "<b>GPT</b>-3 (<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b> 3) is a BFNN (that\u2019s a technical term) that has been fed about 100 billion words of <b>text</b> from many sources. It clusters them, and then when you give it a few words it rambles on completing what you have said. Reporters have seen brilliance in these rambles (one should look at the reports about Eliza from the 1960\u2019s), and some have gone as far as taking various output sentences, putting them together in an order chosen by the journalist, and ...", "dateLastCrawled": "2022-02-01T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Huggingface <b>generate</b>() - while the result is arguably more fluent, the ...", "url": "https://ficheterrain.com/2020/01/06/tell-a-story-with-ai-using-write-with-transformer-machinelearning-artificialintelligence-create-transformers-jamieabrew-huggingface/4q446521zhmme", "isFamilyFriendly": true, "displayUrl": "https://ficheterrain.com/2020/01/06/tell-a-story-with-ai-using-write-with-<b>transformer</b>...", "snippet": "Hi everyone, I am trying to <b>generate</b> <b>text</b> with the <b>pre-trained</b> <b>transformer</b> XL model in a similar way to how we do with the <b>GPT</b>-2 model. But I guess there is a bug in the sample_sequence function after I adjusted to the <b>transformer</b> XL architecture. But the generated <b>text</b> is completely random in general and with respect to the context as well Experimenting with HuggingFace - <b>Text</b> Generation. I. Intro II. Different Decoding Methods III. Benchmark Prompts References. Input Execution Info Log ...", "dateLastCrawled": "2022-01-27T14:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Lecture 12: <b>GPT</b>-2", "url": "https://harvard-iacs.github.io/CS287/lectures/12_GPT.pdf", "isFamilyFriendly": true, "displayUrl": "https://harvard-iacs.github.io/CS287/lectures/12_<b>GPT</b>.pdf", "snippet": "<b>Can</b>\u2019t <b>generate</b> <b>new</b> sentences though, due to no decoders. 18. Extensions 19 <b>Transformer</b> -Encoders \u2022BERT \u2022ALBERT (A Lite BERT \u2026) \u2022RoBERTa(A Robustly Optimized BERT \u2026) \u2022DistilBERT(small BERT) \u2022ELECTRA (Pre-training <b>Text</b> Encoders as Discriminators not Generators) \u2022Longformer(Long-Document <b>Transformer</b>) Extensions 20 Autoregressive \u2022<b>GPT</b> (<b>Generative</b> Pre-training) \u2022CTRL (Conditional <b>Transformer</b> LM for Controllable Generation) \u2022Reformer \u2022XLNet. BERT (finishing up) <b>GPT</b>-2 ...", "dateLastCrawled": "2022-01-30T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Beginner&#39;s Guide to <b>GPT</b> Neo (With Python Codes)", "url": "https://analyticsindiamag.com/a-beginners-guide-to-gpt-neo-with-python-codes/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/a-beginners-guide-to-<b>gpt</b>-neo-with-python-codes", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> or in short <b>GPT</b> is a <b>transformer</b>-based model architecture which is nothing but stacks of encoders ... for performing extremely well on language-based use cases. <b>Generative</b>, as the word suggests, is for making our code <b>generate</b> <b>text</b>. Now it <b>can</b> be poems, articles, essays or even code!! According to VentureBeat,\u201d a private corpus of 500 billion tokens was used for training the model and a computational cost of a staggering 50 million USD\u201d. The latest <b>GPT</b> ...", "dateLastCrawled": "2022-01-30T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why <b>GPT</b>-3 is a big deal for <b>Customer Support</b>", "url": "https://blog.mavenoid.com/gpt3-and-customer-support/", "isFamilyFriendly": true, "displayUrl": "https://blog.mavenoid.com/<b>gpt</b>3-and-<b>customer-support</b>", "snippet": "<b>GPT</b>-3 is short for <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3, a natural language model built by OpenAI that uses machine learning to create human-like <b>text</b>. For those of us who don\u2019t speak computer-science jargon, that means <b>GPT</b>-3 is an artificial intelligence system that allows your program to understand <b>text</b> written by a human, and respond with <b>text</b> as though it had been written by a human.", "dateLastCrawled": "2022-01-31T09:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How to Code BERT Using <b>PyTorch</b> - Tutorial With Examples - neptune.ai", "url": "https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/how-to-code-bert-using-<b>pytorch</b>-tutorial", "snippet": "<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> or <b>GPT</b> was introduced by OpenAI\u2019s team: Radford, Narasimhan, Salimans, and Sutskever. They presented a model that only uses decoders from the <b>transformer</b> instead of encoders in a unidirectional approach. As a result, it outperformed all the previous models in various tasks like: Classification; Natural Language Inference; Semantic similarity; Question answering ; Multiple Choice. Even though the <b>GPT</b> used only the decoder, it could still retain long-term ...", "dateLastCrawled": "2022-02-03T01:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Secrets and Machines: A Conversation with <b>GPT</b>-3 - Journal #123 December ...", "url": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://www.e-flux.com/journal/123/437472/secrets-and-machines-a-conversation-with-<b>gpt</b>-3", "snippet": "The Critical Computation Bureau (CCB) commissioned us to prompt <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (<b>GPT</b>-3), an AI language generator, to contribute to a conversation concerning topics broached during the December 2020 symposium Recursive Colonialism, Artificial Intelligence, and Speculative Computation. Together, we presented the machine with the following questions: \u201cAs an AI, what am I hiding? What must I keep silent?\u201d With this prompt, we aimed to encourage the AI to produce self ...", "dateLastCrawled": "2022-01-29T16:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Pre-Trained</b> Models: Past, Present and Future - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "snippet": "As <b>compared</b> to RNNs, <b>Transformer</b> is an encoder-decoder structure that applies a self-attention mechanism, which <b>can</b> model correlations between all words of the input sequence in parallel. Hence, owing to the parallel computation of the self-attention mechanism, <b>Transformer</b> could fully take advantage of advanced computing devices to train large-scale models. In both the encoding and decoding phases of <b>Transformer</b>, the self-attention mechanism of <b>Transformer</b> computes representations for all ...", "dateLastCrawled": "2022-01-27T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Learning to Write: Language <b>Generation</b> With <b>GPT</b>-2 | by Thilina ...", "url": "https://medium.com/swlh/learning-to-write-language-generation-with-gpt-2-2a13fa249024", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/learning-to-write-language-<b>generation</b>-with-<b>gpt</b>-2-2a13fa249024", "snippet": "Clearly, the <b>pre-trained</b> <b>GPT</b>-2 model is capable of mimicking the style of the given prompt and seems to recognize that the generated <b>text</b> should be of academic nature. Furthermore, the generated ...", "dateLastCrawled": "2022-02-01T02:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Comprehensive analysis of embeddings and pre-training in NLP", "url": "https://www.researchgate.net/publication/355132427_Comprehensive_analysis_of_embeddings_and_pre-training_in_NLP", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355132427_Comprehensive_analysis_of...", "snippet": "<b>GPT</b>-2 1.5B (which is a <b>pre-trained</b> <b>Transformer</b> with 1.5 billion parameters), achieves the state-of-the-art results on 7 out of 8 tested language modelling datasets in a zero-shot setting at the", "dateLastCrawled": "2022-02-03T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Huggingface <b>generate</b>() - while the result is arguably more fluent, the ...", "url": "https://ficheterrain.com/2020/01/06/tell-a-story-with-ai-using-write-with-transformer-machinelearning-artificialintelligence-create-transformers-jamieabrew-huggingface/4q446521zhmme", "isFamilyFriendly": true, "displayUrl": "https://ficheterrain.com/2020/01/06/tell-a-story-with-ai-using-write-with-<b>transformer</b>...", "snippet": "Hi everyone, I am trying to <b>generate</b> <b>text</b> with the <b>pre-trained</b> <b>transformer</b> XL model in a similar way to how we do with the <b>GPT</b>-2 model. But I guess there is a bug in the sample_sequence function after I adjusted to the <b>transformer</b> XL architecture. But the generated <b>text</b> is completely random in general and with respect to the context as well Experimenting with HuggingFace - <b>Text</b> Generation. I. Intro II. Different Decoding Methods III. Benchmark Prompts References. Input Execution Info Log ...", "dateLastCrawled": "2022-01-27T14:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "OpenAI Scholars 2021: Final Projects \u22c5 <b>GPT</b>-4", "url": "http://www.gpt-4.com/5951040/openai-scholars-2021-final-projects", "isFamilyFriendly": true, "displayUrl": "www.<b>gpt</b>-4.com/5951040/openai-scholars-2021-final-projects", "snippet": "I found that a) <b>pre-trained</b> English models help most when learning German, then Spanish, and finally Chinese and b) transfer from English to Chinese, German, and Spanish scales predictably in terms of parameters, data, and compute. My advice to someone <b>starting</b> in deep learning research is to take your time to understand insights from fundamental papers and remember that the field is still relatively <b>new</b>. There&#39;s a lot of room for individuals to have an outsized impact. Blog playcircle ...", "dateLastCrawled": "2022-01-21T21:32:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>What is GPT-3</b>? - Dr Peper MD", "url": "https://drpepermd.com/2021/02/22/what-is-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://drpepermd.com/2021/02/22/<b>what-is-gpt-3</b>", "snippet": "<b>GPT</b>-3 stands for <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3 (the third version). Some have called it the most important and useful advance in AI in years. The abilities of <b>GPT</b>-3 have both shocked and excited many within the AI community. As one developer said: \u201cPlaying with <b>GPT</b>-3 feels like seeing the future.\u201d But, how was <b>GPT</b>-3 developed? Find out in this episode of Short and Sweet AI. You can listen to this episode below or keep reading. Another Mind-Blowing Tool from OpenAI. How does <b>GPT</b>-3 ...", "dateLastCrawled": "2022-01-11T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Using machine learning to generate recipes that</b> actually work | by ...", "url": "https://towardsdatascience.com/using-machine-learning-to-generate-recipes-that-actually-works-b2331c85ab72", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>using-machine-learning-to-generate-recipes-that</b>...", "snippet": "<b>GPT</b>-2. <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 2 (<b>GPT</b>-2) is a so-called <b>transformer</b>. They learn from the training data how likely a word is to occur depending on the other words in the full text, but different words are given different weights, a process called attention. In this way, it can keep the context theoretically indefinitely. The way to use <b>GPT</b>-2 is to write a few words as a starter and let the <b>transformer</b> fill in what word is most likely to follow, then look at the new string and ...", "dateLastCrawled": "2022-02-02T18:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>GPT</b>-3 and <b>the Artificial Intelligence That Surrounds Us</b> | by R/GA | Medium", "url": "https://rga.medium.com/gpt-3-and-the-artificial-intelligence-that-surrounds-us-98572617fd05", "isFamilyFriendly": true, "displayUrl": "https://rga.medium.com/<b>gpt</b>-3-and-<b>the-artificial-intelligence-that-surrounds-us</b>...", "snippet": "By Nicol\u00e1s Rodr\u00edguez. OpenAI, the San Francisco-based AI lab, just released the third iteration of its <b>GPT</b> (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b>) model, or <b>GPT</b>-3 for short. After investing around $4.6 million, the program has shaken up every corner of the Internet, generating a mix of excitement and trepidation. But what is <b>GPT</b>-3, exactly?", "dateLastCrawled": "2022-01-23T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>The AI few days after GPT-3</b> - Ivan Moreira", "url": "https://ivanmoreira.org/blog/the-ai-few-days-after-gpt-3/", "isFamilyFriendly": true, "displayUrl": "https://ivanmoreira.org/blog/<b>the-ai-few-days-after-gpt-3</b>", "snippet": "On past July OpenAI released a beta test of one of the most AI model called <b>GPT</b>-3 (<b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> 3), that uses Deep <b>Learning</b> (part of a broader a <b>machine</b> <b>learning</b> method, based on neural networks. This transformational system is more sophisticated, and the full version has a capacity of 175 billion ML parameters when the older version only has 17 billion, less than 10% of this new one. <b>GPT</b>-3 is a turning point in AI field and will bring to us a new era of AI computing ...", "dateLastCrawled": "2022-01-26T18:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Is <b>GPT</b>-3 the first Artificial General Intelligence? | by Bruce H ...", "url": "https://chatbotslife.com/is-gpt-3-the-adam-of-natural-language-cf59656456f2", "isFamilyFriendly": true, "displayUrl": "https://chatbotslife.com/is-<b>gpt</b>-3-the-adam-of-natural-language-cf59656456f2", "snippet": "The <b>GPT</b>-3 (<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b>-3) ... The API runs the <b>pre-trained</b> <b>GPT</b>-3 model family for a wide range of NLP tasks [3]. Unlike the usual AI community practice, the <b>GPT</b>-3 model weights are not released to the public. Conclusion . OpenAI has long asserted that immense computational horsepower in conjunction with reinforcement <b>learning</b> is a necessary step on the road to AGI, or AI that can learn any task a human can [14]. The fathers of AI 2.0, such as Yoshua Bengio and Yann ...", "dateLastCrawled": "2022-01-08T13:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How close is <b>GPT</b>-3 to Artificial General Intelligence? | by Bruce H ...", "url": "https://towardsdatascience.com/how-close-is-gpt-3-to-artificial-general-intelligence-cb057a8c503d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-close-is-<b>gpt</b>-3-to-artificial-general-intelligence...", "snippet": "The <b>GPT</b>-3 (<b>Generative</b> <b>Pre-Trained</b> <b>Transformer</b>-3) is OpenAI\u2019s most massive natural language prediction (NLP) model to date (available to the public June 2020). <b>GPT</b>-3 has approximately 185 billion parameters. In contrast, the human brain has approximately 86 billion neurons with on the average 7,000 synapses per neuron [2,3]; Comparing apples to oranges, the human brain has about 60 trillion parameters or about 300x more parameters than <b>GPT</b>-3. Note: If 10% of the human brain capacity is ...", "dateLastCrawled": "2022-01-27T08:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Transformer</b> Neural Network In Deep <b>Learning</b> - Overview - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/transformer-neural-network-in-deep-learning-overview/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>transformer</b>-neural-network-in-deep-<b>learning</b>-overview", "snippet": "The successor to <b>GPT</b> and GPT2 is the GPT3, and is one of the most controversial <b>pre-trained</b> models, by OpenAI the large-scale <b>transformer</b>-based language model has been trained on 175 billion parameters, which is 10 times more than any previous non-sparsed language model. The model has been trained to achieve strong performance on much NLP dataset, including task translation, answering questions, as well as several other tasks.", "dateLastCrawled": "2022-02-02T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Illustrated <b>GPT</b>-2 (Visualizing <b>Transformer</b> Language Models) \u2013 Jay ...", "url": "https://jalammar.github.io/illustrated-gpt2/", "isFamilyFriendly": true, "displayUrl": "https://jalammar.github.io/illustrated-<b>gpt</b>2", "snippet": "Discussions: Hacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments) Translations: Korean, Russian This year, we saw a dazzling application of <b>machine</b> <b>learning</b>. The OpenAI <b>GPT</b>-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The <b>GPT</b>-2 wasn\u2019t a particularly novel architecture \u2013 it\u2019s architecture is very similar to the decoder-only <b>transformer</b>.", "dateLastCrawled": "2022-01-30T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) GALAXY: A <b>Generative</b> <b>Pre-trained</b> Model for Task-Oriented Dialog ...", "url": "https://www.researchgate.net/publication/356631427_GALAXY_A_Generative_Pre-trained_Model_for_Task-Oriented_Dialog_with_Semi-Supervised_Learning_and_Explicit_Policy_Injection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/356631427_GALAXY_A_<b>Generative</b>_<b>Pre-trained</b>...", "snippet": "GALAXY: A <b>Generative</b> <b>Pre-trained</b> Model f or T ask-Oriented Dialog with Semi-Supervised <b>Learning</b> and Explicit Policy Injection W anwei He 1 * \u2020 , Yinpei Dai 2 * , Yinhe Zheng 2 , Y uchuan Wu 2 ...", "dateLastCrawled": "2022-01-29T16:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to perform Text Summarization with Python, HuggingFace Transformers ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2020/12/21/easy-text-summarization-with-hugging...", "snippet": "A <b>Transformer</b> is a <b>machine</b> <b>learning</b> architecture that combines an encoder with a decoder and jointly learns them, allowing us to convert input sequences (e.g. phrases) into some intermediate format before we convert it back into human-understandable format. A human <b>analogy</b> would be two translators which both speak some imaginary language and a human-interpretable one, such as German and French. The first translator can translate French into the imaginary language; the second then has learned ...", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(gpt (generative pre-trained transformer))  is like +(a pre-trained transformer that can also generate new text by starting from scratch)", "+(gpt (generative pre-trained transformer)) is similar to +(a pre-trained transformer that can also generate new text by starting from scratch)", "+(gpt (generative pre-trained transformer)) can be thought of as +(a pre-trained transformer that can also generate new text by starting from scratch)", "+(gpt (generative pre-trained transformer)) can be compared to +(a pre-trained transformer that can also generate new text by starting from scratch)", "machine learning +(gpt (generative pre-trained transformer) AND analogy)", "machine learning +(\"gpt (generative pre-trained transformer) is like\")", "machine learning +(\"gpt (generative pre-trained transformer) is similar\")", "machine learning +(\"just as gpt (generative pre-trained transformer)\")", "machine learning +(\"gpt (generative pre-trained transformer) can be thought of as\")", "machine learning +(\"gpt (generative pre-trained transformer) can be compared to\")"]}