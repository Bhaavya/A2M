{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Log Odds</b>: Definition and Worked <b>Statistics</b> Problems", "url": "https://www.statisticshowto.com/log-odds/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>statistics</b>howto.com/<b>log-odds</b>", "snippet": "Probability is the probability an <b>event</b> happens. For example, there might be an 80% chance of rain today. <b>Odds</b> (more technically the <b>odds</b> of success) is defined as probability of success/probability of failure. So the <b>odds</b> of a success (80% chance of rain) has an accompanying <b>odds</b> of failure (20% chance it doesn\u2019t rain); as an equation (the \u201c<b>odds</b> ratio\u201c), that\u2019s .8/.2 = 4. <b>Log odds</b> is <b>the logarithm</b> <b>of the odds</b>. Ln(4) = 1.38629436 \u2245 1.386. Conversion to <b>log odds</b> results in symmetry ...", "dateLastCrawled": "2022-02-02T15:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Log odds - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/log-odds/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>log-odds</b>", "snippet": "<b>Like</b> Article. <b>Log odds</b>. Last Updated : 21 Jan, 2022. <b>Odds</b> (<b>odds</b> of success): It is defined as the chances of success divided by the chances of failure. Say, there is a 90% chance that winning a wager implies that the \u2018<b>odds</b> are in our favour\u2019 as the winning <b>odds</b> are 90% while the losing <b>odds</b> are just 10%. It is also known defined as <b>odds</b> ratio as it is in the form of a ratio. So the odd ratio of the above discussed example would be: Formally, the <b>odds</b> ratio <b>of an event</b> A is defined as the ...", "dateLastCrawled": "2022-02-02T15:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Logistic Regression", "url": "https://theintactone.com/2021/11/28/logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://theintactone.com/2021/11/28/logistic-regression", "snippet": "In the logistic model, the <b>log-odds</b> (<b>the logarithm</b> <b>of the odds</b>) for the value labeled \u201c1\u201d is a linear combination of one or more independent variables (\u201cpredictors\u201d); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \u201c1\u201d can vary between 0 (certainly the value \u201c0\u201d) and 1 (certainly the value \u201c1\u201d), hence the labeling; the ...", "dateLastCrawled": "2022-02-01T15:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "FAQ: How do I interpret <b>odds</b> ratios in logistic regression?", "url": "https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-<b>odds</b>...", "snippet": "The transformation from <b>odds</b> to log of <b>odds</b> is the log transformation (In statistics, in general, when we use log almost always it means natural <b>logarithm</b>). Again this is a monotonic transformation. That is to say, the greater the <b>odds</b>, the greater the log of <b>odds</b> and vice versa. The table below shows the relationship among the probability, <b>odds</b> and log of <b>odds</b>. We have also shown the plot of <b>log odds</b> against <b>odds</b>.", "dateLastCrawled": "2022-02-03T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "WHAT and WHY of <b>Log Odds</b>. WHAT are <b>Log Odds</b> and WHY are they\u2026 | by ...", "url": "https://towardsdatascience.com/https-towardsdatascience-com-what-and-why-of-log-odds-64ba988bf704", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/https-towardsdatascience-com-what-and-why-of-<b>log-odds</b>...", "snippet": "When I first began working in Data Science, I was so confused about <b>Log Odds</b>. I would have questions <b>like</b> What is <b>Log Odds</b>, Why do we need them, etc. When trying to unde r stand any concept, I <b>like</b> to use the Divide and Understand strategy, i.e., break it into smaller pieces, understand their meanings separately, and then combine this knowledge to get hold of the concept as a whole. So here, let\u2019s first learn what is meant by <b>Odds</b> and then try to work our way towards understanding <b>Log Odds</b> ...", "dateLastCrawled": "2022-02-03T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Logistic Regression with Stata Chapter 1: Introduction to Logistic ...", "url": "https://stats.oarc.ucla.edu/stata/webbooks/logistic/chapter1/logistic-regression-with-statachapter-1-introduction-to-logistic-regression-with-stata/", "isFamilyFriendly": true, "displayUrl": "https://stats.oarc.ucla.edu/stata/webbooks/logistic/chapter1/logistic-regression-with...", "snippet": "The <b>odds</b> ratio would be 3/1.5 = 2, meaning that the <b>odds</b> are 2 to 1 that a woman will make the team compared to men. Another term that needs some explaining is <b>log odds</b>, also known as logit. <b>Log odds</b> are the natural <b>logarithm</b> <b>of the odds</b>. The coefficients in the output of the logistic regression are given in units of <b>log odds</b>. Therefore, the ...", "dateLastCrawled": "2022-02-03T01:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Logistic Regression Model | Economics and Econometrics", "url": "https://ceaeconomics.wordpress.com/2012/09/20/logistic-regression-model/", "isFamilyFriendly": true, "displayUrl": "https://ceaeconomics.wordpress.com/2012/09/20/logistic-regression-model", "snippet": "The fundamental mathematical concept that underlies in logistic regression is what\u2019s called, logit \u2013 the natural <b>logarithm</b> of an <b>odds</b> ratio. The simplest logistic regression model has the form: Logit(Y) = natural <b>log (odds</b>) = ln (\u03c0/1- \u03c0) = \u03b1+ \u03b2x (1) The equation (1) requires us to predict the probability of the occurrence\u2026", "dateLastCrawled": "2022-01-18T18:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Interpreting Logistic Regression Coefficients - Odds Ratios</b> | Science ...", "url": "https://sciphy-stats.com/post/interpreting-logistic-regression-coefficients-odds-ratios/", "isFamilyFriendly": true, "displayUrl": "https://sciphy-stats.com/post/<b>interpreting-logistic-regression-coefficients-odds-ratios</b>", "snippet": "The <b>log odds</b> are modeled as a linear combinations of the predictors and regression coefficients: \\(\\beta_0 + \\beta_1x_i\\) The complete model looks <b>like</b> this: \\(Logit = ln(\\frac{p(x)}{1 - p(x)}) = \\beta_0 + \\beta_1x_i\\) This equation shows, that the linear combination models the Logit and model coefficients \\(\\beta\\) descibe the influence of the predictors \\(X\\) on the logit, e.g. a one unit increase in \\(x_i\\) changes the Logit by the amout of \\(\\beta_i\\). Unfortunatly, we do not have a ...", "dateLastCrawled": "2022-02-03T00:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Defence from extinction and log-odds</b> - Adam Howes", "url": "https://athowes.github.io/2020/12/27/log-odds-extinction/", "isFamilyFriendly": true, "displayUrl": "https://athowes.github.io/2020/12/27/<b>log-odds</b>-extinction", "snippet": "<b>Log-odds</b>. Owen mentions how the <b>log-odds</b> operation, given by <b>the logarithm</b> <b>of the odds</b> ratio \\(p / (1 - p)\\) as \\[\\begin{equation} \\text{logit}(p) = \\log \\left( \\frac{p}{1 - p} \\right), \\end{equation}\\] \u201cstretches out\u201d probabilities near zero and one. It can be useful to think of probabilities this way in situations <b>like</b> the above where a small difference between two probabilities is actually really important \u2013 at least more important than \\(|p - q|\\) would have you believe. An easy ...", "dateLastCrawled": "2022-01-13T20:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Logistic Regression Quiz Questions &amp; Answers</b> - Data ... - Data Analytics", "url": "https://vitalflux.com/logistic-regression-quiz-questions-answers/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/<b>logistic-regression-quiz-questions-answers</b>", "snippet": "Logistic function is a sigmoid function which takes a real value as input and output the value between 0 and 1. Here is how the equation looks <b>like</b>: \u03c3 ( z) = 1 1 + e x p ( \u2212 z) In the above equation, exp represents exponential (e). In case of logistic regression, Z represents the logit of probability of <b>event</b> happening or <b>log-odds</b> of an ...", "dateLastCrawled": "2022-02-03T05:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Log Odds</b>: Definition and Worked <b>Statistics</b> Problems", "url": "https://www.statisticshowto.com/log-odds/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>statistics</b>howto.com/<b>log-odds</b>", "snippet": "<b>Odds</b> (more technically the <b>odds</b> of success) is defined as probability of success/probability of failure. So the <b>odds</b> of a success (80% chance of rain) has an accompanying <b>odds</b> of failure (20% chance it doesn\u2019t rain); as an equation (the \u201c <b>odds</b> ratio \u201c), that\u2019s .8/.2 = 4. <b>Log odds</b> is the <b>logarithm</b> <b>of the odds</b>. Ln (4) = 1.38629436 \u2245 1.386.", "dateLastCrawled": "2022-02-02T15:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Log odds - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/log-odds/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>log-odds</b>", "snippet": "<b>Log odds</b>: It is the <b>logarithm</b> <b>of the odds</b> ratio. (As shown by the equation given below) ... The <b>log odds</b> of a certain <b>event</b> gives a normal distribution when plotted on a histogram! This is what makes <b>log odds</b> so useful. Real-life example . <b>Log odds</b> are used in medical research for predicting the likelihood of symptoms the subject may get based on his/her previous symptoms. Consider the following example. A medical research is done on 1000 test subjects showing symptoms of fever and cough ...", "dateLastCrawled": "2022-02-02T15:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Logistic regression 1: from <b>odds</b> to probability | Dr. Yury Zablotski", "url": "https://yury-zablotski.netlify.app/post/from-odds-to-probability/", "isFamilyFriendly": true, "displayUrl": "https://yury-zablotski.netlify.app/post/from-<b>odds</b>-to-probability", "snippet": "It\u2019s <b>similar</b> to time, where 1 day equals to 24 hours. Probability (of success) is the chance <b>of an event</b> happening. For example, there might be an 80% chance of rain today. <b>Odds</b> are the probability of success (80% chance of rain) divided by the probability of failure (20% chance of no-rain) = 0.8/0.2 = 4, or 4 to 1. <b>Log-odds</b> is simply the <b>logarithm</b> of <b>odds</b> 1. But in order to understand them properly, let\u2019s express each of them in terms of two others! <b>Odds</b>. <b>Odds</b> are NOT probabilities! If ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Logistic Regression with Stata Chapter 1: Introduction to Logistic ...", "url": "https://stats.oarc.ucla.edu/stata/webbooks/logistic/chapter1/logistic-regression-with-statachapter-1-introduction-to-logistic-regression-with-stata/", "isFamilyFriendly": true, "displayUrl": "https://stats.oarc.ucla.edu/stata/webbooks/logistic/chapter1/logistic-regression-with...", "snippet": "The <b>odds</b> ratio would be 3/1.5 = 2, meaning that the <b>odds</b> are 2 to 1 that a woman will make the team compared to men. Another term that needs some explaining is <b>log odds</b>, also known as logit. <b>Log odds</b> are the natural <b>logarithm</b> <b>of the odds</b>. The coefficients in the output of the logistic regression are given in units of <b>log odds</b>. Therefore, the ...", "dateLastCrawled": "2022-02-03T01:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Logistic Regression in R (part 1) | by Oscar Rojo | Medium", "url": "https://medium.com/@zumaia/logistical-regression-in-r-part-1-cd676aeb060d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@zumaia/logistical-regression-in-r-part-1-cd676aeb060d", "snippet": "In the logistic model, the <b>log-odds</b> (the <b>logarithm</b> <b>of the odds</b>) for the value labeled \u201c1\u201d is a linear combination of one or more independent variables (\u201cpredictors\u201d); the independent ...", "dateLastCrawled": "2022-01-30T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Common pitfalls in statistical analysis: Logistic regression", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5543767/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5543767", "snippet": "The equation provides a model which can be used to predict the probability <b>of an event</b> happening for a particular individual, given his/her profile of predictor factors. Table 2 . Different methods of representing results of a multivariate logistic analysis: (a) As a table showing regression coefficients and significance levels, (b) as an equation for <b>log (odds</b>) containing regression coefficients for each variable, and (c) as an equation for <b>odds</b> using coefficients (or anti-loge) of ...", "dateLastCrawled": "2022-01-30T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "12.1 - <b>Logistic Regression</b> | STAT 462", "url": "https://online.stat.psu.edu/stat462/node/207/", "isFamilyFriendly": true, "displayUrl": "https://online.stat.psu.edu/stat462/node/207", "snippet": "<b>Odds</b>, <b>Log Odds</b>, and <b>Odds</b> Ratio. There are algebraically equivalent ways to write the <b>logistic regression</b> model: The first is \\[\\begin{equation}\\label{logmod1} \\frac{\\pi}{1-\\pi}=\\exp(\\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{k}X_{k}), \\end{equation}\\] which is an equation that describes the <b>odds</b> of being in the current category of interest. By definition, the <b>odds</b> for an <b>event</b> is \u03c0 / (1 - \u03c0) such that P is the probability of the <b>event</b>. For example, if you are at the racetrack and there is a 80 ...", "dateLastCrawled": "2022-02-02T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Odds</b> Ratios\u2014Current Best Practice and Use | Research, Methods ...", "url": "https://jamanetwork.com/journals/jama/fullarticle/2686777", "isFamilyFriendly": true, "displayUrl": "https://jamanetwork.com/journals/jama/fullarticle/2686777", "snippet": "However, the results from a logistic regression are converted easily into <b>odds</b> ratios because logistic regression estimates a parameter, known as the <b>log odds</b>, which is the natural <b>logarithm</b> <b>of the odds ratio</b>. For example, if a <b>log odds</b> estimated by logistic regression is 0.4 then the <b>odds ratio</b> can be derived by exponentiating the <b>log odds</b> (exp(0.4) = 1.5). It is the <b>odds ratio</b> that is usually reported in the medical literature. The <b>odds ratio</b> is always positive, although the estimated log ...", "dateLastCrawled": "2022-01-29T19:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "FAQ: How do I interpret <b>odds</b> ratios in logistic regression?", "url": "https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-<b>odds</b>...", "snippet": "The transformation from <b>odds</b> to log of <b>odds</b> is the log transformation (In statistics, in general, when we use log almost always it means natural <b>logarithm</b>). Again this is a monotonic transformation. That is to say, the greater the <b>odds</b>, the greater the log of <b>odds</b> and vice versa. The table below shows the relationship among the probability, <b>odds</b> and log of <b>odds</b>. We have also shown the plot of <b>log odds</b> against <b>odds</b>.", "dateLastCrawled": "2022-02-03T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How to Conduct Logistic Regression</b> - <b>Statistics Solutions</b>", "url": "https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/how-to-conduct-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.statisticssolutions.com</b>/free-resources/directory-of-statistical-analyses/...", "snippet": "The <b>log odds</b> is not an intuitive concept, but since it is the log <b>of the odds</b> ratio = log (p/(1-p)) we simply can translate this result back into <b>odds</b> ratios with exp(x). That is in our case exp(2.0), which is 7.39. Therefore increasing the Lethane concentration by one unit the <b>odds</b> of killing the bug are multiplied by 7.39. This is the same as saying that for two configurations of our spray the one with the higher concentration of Lethane has a +639% higher probability of killing the bug ...", "dateLastCrawled": "2022-01-25T18:53:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Logistic Regression", "url": "https://theintactone.com/2021/11/28/logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://theintactone.com/2021/11/28/logistic-regression", "snippet": "In the logistic model, the <b>log-odds</b> (the <b>logarithm</b> <b>of the odds</b>) for the value labeled \u201c1\u201d is a linear combination of one or more independent variables (\u201cpredictors\u201d); the independent variables <b>can</b> each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \u201c1\u201d <b>can</b> vary between 0 (certainly the value \u201c0\u201d) and 1 (certainly the value \u201c1\u201d), hence the labeling; the ...", "dateLastCrawled": "2022-02-01T15:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Categorical Data Analysis: Away from ANOVAs (transformation or not) and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2613284/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2613284", "snippet": "The natural <b>logarithm</b> of <b>odds</b> is called the logit (or <b>log-odds</b>). ... The random effect vector b <b>can</b> <b>be thought</b> of as the coefficients for the random effects. It is characterized by a multivariate normal distribution, centered around 0 and with the variance-covariance matrix \u03a3 (for details, see Agresti, 2002: 492). A mixed logit model then has the form (for linear mixed models, cf. Baayen et al., this issue): logit(p) = x \u2032 \u03b2 + z \u2032 b, b \u223c N(0, \u03c3 2 \u2211) (14) Just as for ordinary logit ...", "dateLastCrawled": "2022-02-02T19:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Construction and Use of <b>Log-Odds</b> Substitution Scores for Multiple ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2904766/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2904766", "snippet": "They <b>can</b> be understood simply as the sum of <b>log-odds</b> scores for the individual letters observed in a column, with the \u201ctarget frequency\u201d for each letter calculated based upon the prior distribution , and the \u201cpreviously observed\u201d letters through . Even though, by this formula, the <b>log-odds</b> score for a letter varies with its position in the column, the total column score is nevertheless invariant under permutation of the column&#39;s letters.", "dateLastCrawled": "2022-01-06T04:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Amino Acid Substitution Matrices from Information Theoretic Perspective", "url": "http://compbio.berkeley.edu/class/c246/Reading/altschul-1991-jmb.pdf", "isFamilyFriendly": true, "displayUrl": "compbio.berkeley.edu/class/c246/Reading/altschul-1991-jmb.pdf", "snippet": "likelihood or <b>odds</b> ratio. Scores that are the <b>logarithm</b> of <b>odds</b> ratios are called <b>log-odds</b> scores. Adding such scores <b>can</b> <b>be thought</b> of as multiplying the corresponding probabilities, which is appro- priate for independent events. so that the total score rtmains a <b>log-odds</b> score. <b>Log-odds</b> matrices have been advocated in a", "dateLastCrawled": "2021-09-13T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "regression - What does the <b>logit</b> value actually mean ... - Cross Validated", "url": "https://stats.stackexchange.com/questions/52825/what-does-the-logit-value-actually-mean", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/52825", "snippet": "The natural <b>logarithm</b> <b>of the odds</b> is known as <b>log-odds</b> or <b>logit</b>. The inverse function is. p = 1 1 + e \u2212 L. Probabilities range from zero to one, i.e., p \u2208 [ 0, 1], whereas logits <b>can</b> be any real number ( R, from minus infinity to infinity; L \u2208 ( \u2212 \u221e, \u221e) ). A probability of 0.5 corresponds to a <b>logit</b> of 0.", "dateLastCrawled": "2022-01-27T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Defence from extinction and log-odds</b> - Adam Howes", "url": "https://athowes.github.io/2020/12/27/log-odds-extinction/", "isFamilyFriendly": true, "displayUrl": "https://athowes.github.io/2020/12/27/<b>log-odds</b>-extinction", "snippet": "<b>Log-odds</b>. Owen mentions how the <b>log-odds</b> operation, given by the <b>logarithm</b> <b>of the odds</b> ratio \\(p / (1 - p)\\) as \\[\\begin{equation} \\text{logit}(p) = \\log \\left( \\frac{p}{1 - p} \\right), \\end{equation}\\] \u201cstretches out\u201d probabilities near zero and one. It <b>can</b> be useful to think of probabilities this way in situations like the above where a small difference between two probabilities is actually really important \u2013 at least more important than \\(|p - q|\\) would have you believe. An easy ...", "dateLastCrawled": "2022-01-13T20:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fitting a Model to Data", "url": "https://www.ccs.neu.edu/home/criedl/MISM6203/LESSONS/Module04-Lesson1-Fitting-a-Model-to-Data.html", "isFamilyFriendly": true, "displayUrl": "https://www.ccs.neu.edu/home/criedl/MISM6203/LESSONS/Module04-Lesson1-Fitting-a-Model...", "snippet": "The problem <b>can</b> be solved by taking the <b>logarithm</b> <b>of the odds</b> (<b>log-odds</b>) because for any number zero to \\(\\infty\\), its log odd will be between \\(-\\infty\\) and \\(\\infty\\). See below: <b>Log Odds</b> Linear Function: \\(log (\\frac{p+(x)}{1-p+(x)}) = f(x) = w_0 + w_1x_1 + w_2x_2 + \\ldots\\) Where \\(p + (x)\\) is the estimate of a particular <b>event</b> occuring. The Logistic Function: \\(p+(x) = \\frac{1}{1+e^{-f(x)}}\\) Graph of a Logistic Function: Note: The further away from the decision boundary \\((x=0 ...", "dateLastCrawled": "2022-01-25T22:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Logistic Regression</b> in SAS - IDRE Stats", "url": "https://stats.oarc.ucla.edu/stat/data/logistic_regression_sas/logistic_regression_sas.html", "isFamilyFriendly": true, "displayUrl": "https://stats.oarc.ucla.edu/stat/data/<b>logistic_regression</b>_sas/<b>logistic_regression</b>_sas.html", "snippet": "So, the \\(<b>odds</b>\\) <b>of an event</b> is the ratio of the probability of the <b>event</b> occuring to the probability of it not occuring. If \\(p=.5\\), then \\(<b>odds</b>=\\frac{.5}{.5}=\\frac{1}{1}\\), which expresses that the <b>event</b> has 1-to-1 <b>odds</b>, or equal <b>odds</b> of occurring versus not occurring. If \\(p=.75\\), then \\(<b>odds</b>=\\frac{.75}{.25}=\\frac{3}{1}\\), and the <b>event</b> has 3-to-1 <b>odds</b>, or the <b>event</b> should occur 3 times for each 1 time that it does not occur. <b>Odds</b> have a monotonic relationship with probabilities, so ...", "dateLastCrawled": "2022-02-02T09:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Different ways of Performing Logistic Regression in <b>SAS</b>", "url": "https://www.sas.com/content/dam/SAS/en_ca/User%20Group%20Presentations/TASS/Lerner-LogisticRegression-Sep2010.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.sas.com</b>/content/dam/<b>SAS</b>/en_ca/User Group Presentations/TASS/Lerner...", "snippet": "Logistic regression is a statistical technique that estimates the natural base <b>logarithm</b> of the probability of one discrete <b>event</b> (e.g., passing) occurring as opposed to another <b>event</b> (failing) or more other events. The <b>log-odds</b> of the <b>event</b> (broadly referred to as the logit here) are the predicted values. Exponents of parameters in a logistic regression yield the <b>odds</b> <b>of an event</b> occurring. The probability <b>of an event</b> occurring is equal to the <b>odds</b> divided by the sum <b>of the odds</b> plus 1 ...", "dateLastCrawled": "2022-02-02T05:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why do we need natural log of <b>Odds</b> in <b>Logistic Regression</b>? - Cross ...", "url": "https://stats.stackexchange.com/questions/271511/why-do-we-need-natural-log-of-odds-in-logistic-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/271511/why-do-we-need-natural-log-of-<b>odds</b>-in...", "snippet": "I know what an <b>odds</b> is. It&#39;s a ratio of the probability of some <b>event</b> happening to the probability it not happening. So, in the context of classification, the probability that an input feature vect...", "dateLastCrawled": "2022-01-16T18:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Logistic Regression with Stata Chapter 1: Introduction to Logistic ...", "url": "https://stats.oarc.ucla.edu/stata/webbooks/logistic/chapter1/logistic-regression-with-statachapter-1-introduction-to-logistic-regression-with-stata/", "isFamilyFriendly": true, "displayUrl": "https://stats.oarc.ucla.edu/stata/webbooks/logistic/chapter1/logistic-regression-with...", "snippet": "<b>Log odds</b> are the natural <b>logarithm</b> <b>of the odds</b>. The coefficients in the output of the logistic regression are given in units of <b>log odds</b>. Therefore, the coefficients indicate the amount of change expected in the <b>log odds</b> when there is a one unit change in the predictor variable with all of the other variables in the model held constant. In a while we will explain why the coefficients are given in <b>log odds</b>. Please be aware that any time a <b>logarithm</b> is discussed in this chapter, we mean the ...", "dateLastCrawled": "2022-02-03T01:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Logistic regression 1: from <b>odds</b> to probability | Dr. Yury Zablotski", "url": "https://yury-zablotski.netlify.app/post/from-odds-to-probability/", "isFamilyFriendly": true, "displayUrl": "https://yury-zablotski.netlify.app/post/from-<b>odds</b>-to-probability", "snippet": "<b>Log-odds</b> is simply the <b>logarithm</b> of <b>odds</b> 1. But in order to understand them properly, let\u2019s express each of them in terms of two others! <b>Odds</b>. <b>Odds</b> are NOT probabilities! If you are usually late 1 time out of 5, then the <b>odds</b> of you being late to the party are 1 to 4, or \\(\\frac{1}{4}\\), or 0.25. If you are late 3 times out of 5, then the <b>odds</b> of you being late are \\(\\frac{3}{2}\\), or 1.5. The <b>odds</b> are ratios of something happening, to something not happening (i.e. 3/2 = 1.5). The ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Odds</b> Ratios\u2014Current Best Practice and Use | Research, Methods ...", "url": "https://jamanetwork.com/journals/jama/fullarticle/2686777", "isFamilyFriendly": true, "displayUrl": "https://jamanetwork.com/journals/jama/fullarticle/2686777", "snippet": "However, the results from a logistic regression are converted easily into <b>odds</b> ratios because logistic regression estimates a parameter, known as the <b>log odds</b>, which is the natural <b>logarithm</b> <b>of the odds ratio</b>. For example, if a <b>log odds</b> estimated by logistic regression is 0.4 then the <b>odds ratio</b> <b>can</b> be derived by exponentiating the <b>log odds</b> (exp(0.4) = 1.5). It is the <b>odds ratio</b> that is usually reported in the medical literature. The <b>odds ratio</b> is always positive, although the estimated log ...", "dateLastCrawled": "2022-01-29T19:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Logistic Regression in R (part 1) | by Oscar Rojo | Medium", "url": "https://medium.com/@zumaia/logistical-regression-in-r-part-1-cd676aeb060d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@zumaia/logistical-regression-in-r-part-1-cd676aeb060d", "snippet": "In the logistic model, the <b>log-odds</b> (the <b>logarithm</b> <b>of the odds</b>) for the value labeled \u201c1\u201d is a linear combination of one or more independent variables (\u201cpredictors\u201d); the independent ...", "dateLastCrawled": "2022-01-30T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "FAQ: How do I interpret <b>odds</b> ratios in logistic regression?", "url": "https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-<b>odds</b>...", "snippet": "More explicitly, we <b>can</b> say that for male students, a one-unit increase in math score yields a change in <b>log odds</b> of 0.13. On the other hand, for the female students, a one-unit increase in math score yields a change in <b>log odds</b> of (.13 + .067) = 0.197. In terms of <b>odds</b> ratios, we <b>can</b> say that for male students, the <b>odds</b> ratio is exp(.13) = 1.14 for a one-unit increase in math score and the <b>odds</b> ratio for female students is exp(.197) = 1.22 for a one-unit increase in math score. The ratio of ...", "dateLastCrawled": "2022-02-03T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A30: Logistic Regression (Part-2)&gt;&gt; Behind the Scene! | by Junaid Qazi ...", "url": "https://medium.com/mlearning-ai/a30-logistic-regression-part-2-behind-the-scene-38a98b70192a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mlearning-ai/a30-logistic-regression-part-2-behind-the-scene-38a98b...", "snippet": "<b>Log-odds</b> <b>can</b> take any positive or negative value, and the purpose of the logit link is to take a linear combination of the covariate (dependent, or response) values, between \u2212\u221e and \u221e, and ...", "dateLastCrawled": "2022-02-03T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "WHAT and WHY of <b>Log Odds</b>. WHAT are <b>Log Odds</b> and WHY are they\u2026 | by ...", "url": "https://towardsdatascience.com/https-towardsdatascience-com-what-and-why-of-log-odds-64ba988bf704", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/https-towardsdatascience-com-what-and-why-of-<b>log-odds</b>...", "snippet": "Figure-2: <b>Odds</b> as a fraction. <b>Odds</b> should NOT be confused with Probabilities. <b>Odds</b> are the ratio of something happening to something not happening.In our scenario above, the <b>odds</b> are 4 to 6. Whereas, Probability is the ratio of something happening to everything that could happen.So in the case of our chess example, probability is 4 to 10 (as there were 10 games played in total).", "dateLastCrawled": "2022-02-03T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Logistic Models</b>: How to Interpret - <b>pi: predict/infer</b>", "url": "https://blog.methodsconsultants.com/posts/interpreting-logistic-models/", "isFamilyFriendly": true, "displayUrl": "https://blog.methodsconsultants.com/posts/<b>interpreting-logistic-models</b>", "snippet": "The logit model <b>can</b> also be derived as a model of <b>log odds</b>, which does not require setting values for all predictors. <b>Odds</b> versus probability: Probability ranges from 0 (impossible) to 1 (happens with certainty). <b>Odds</b> is the ratio of the probability something happens to the probability it won\u2019t happen. Example: The probability I roll a 7 on the first roll of two dice is 0.167. This means the probability of rolling anything else is 1 - 0.167 = 0.833; The <b>odds</b> of rolling anything other than ...", "dateLastCrawled": "2022-02-01T18:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Logistic regression: Why we often</b> <b>can</b> do what we think we <b>can</b> do", "url": "http://www.maartenbuis.nl/wp/odds_ratio.pdf", "isFamilyFriendly": true, "displayUrl": "www.maartenbuis.nl/wp/<b>odds</b>_ratio.pdf", "snippet": "The <b>odds</b> <b>can</b> be interpreted as the expected number of \u201csuccesses\u201d per \u201cfailure\u201d, and the <b>log-odds</b> is the <b>logarithm</b> of that. The <b>log-odds</b> has no upper or lower bound, so a linear model cannot give invalid predictions. Moreover, the exponentiated coefficients <b>can</b> be interpreted directly as <b>odds</b>", "dateLastCrawled": "2021-09-21T00:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>How to Conduct Logistic Regression</b> - <b>Statistics Solutions</b>", "url": "https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/how-to-conduct-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.statisticssolutions.com</b>/free-resources/directory-of-statistical-analyses/...", "snippet": "The <b>log odds</b> is not an intuitive concept, but since it is the log <b>of the odds</b> ratio = log (p/(1-p)) we simply <b>can</b> translate this result back into <b>odds</b> ratios with exp(x). That is in our case exp(2.0), which is 7.39. Therefore increasing the Lethane concentration by one unit the <b>odds</b> of killing the bug are multiplied by 7.39. This is the same as saying that for two configurations of our spray the one with the higher concentration of Lethane has a +639% higher probability of killing the bug ...", "dateLastCrawled": "2022-01-25T18:53:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Algorithms And Their Applications | Basic ML Algorithms", "url": "https://codinghero.ai/10-commonly-used-machine-learning-algorithms-explained-to-kids/", "isFamilyFriendly": true, "displayUrl": "https://codinghero.ai/10-commonly-used-<b>machine</b>-<b>learning</b>-algorithms-explained-to-kids", "snippet": "The best <b>analogy</b> is to think of the <b>machine</b> <b>learning</b> model as a ... In the logistic model, the <b>log-odds</b> (the logarithm of the odds) for the value labeled \u201c1\u201d is a linear combination of one or more independent variables (\u201cpredictors\u201d); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \u201c1\u201d can vary between 0 (certainly the value \u201c0 ...", "dateLastCrawled": "2022-01-26T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Log-odds</b>, i.e., log (p/(1-p)) = WX, is a linear function of parameters W. ... The <b>analogy</b> is many low-level features are coalesce into fewer high-level features. A simple approach is to pick a complex model with early stopping to prevent from overfitting. References: [1] Hands on <b>machine</b> <b>learning</b> with Scikit-Learn and TensorFlow p271. 4.5 How does batch size influence training speed and model accuracy ? Batch gradient descent. slow; may converge to local minimum, and yield worse performance ...", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Logistic Regression</b>. Simplified.. After the basics of Regression, it\u2019s ...", "url": "https://medium.com/data-science-group-iitr/logistic-regression-simplified-9b4efe801389", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>logistic-regression</b>-simplified-9b4efe801389", "snippet": "where, the left hand side is called the logit or <b>log-odds</b> function, and p(x)/(1-p(x)) ... <b>Machine</b> <b>Learning</b> Mastery Blog; Footnotes. You are aware of the most common ML Algorithms in the industry ...", "dateLastCrawled": "2022-01-31T00:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Logistic Regression</b>. By Neeta Ganamukhi | by Neeta Ganamukhi | The ...", "url": "https://medium.com/swlh/logistic-regression-7791655bc480", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>logistic-regression</b>-7791655bc480", "snippet": "In <b>machine</b> <b>learning</b>, we use sigmoid to map predictions to probabilities. The sigmoid curve can be represented with the help of following graph. We can see the values of y-axis lie between 0 and 1 ...", "dateLastCrawled": "2022-02-01T21:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interpret your Regression</b>. A walk through Logistic Regression | by ...", "url": "https://towardsdatascience.com/interpret-your-regression-d5f93908327b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpret-your-regression</b>-d5f93908327b", "snippet": "Logistic Curve. Let\u2019s come to the most interesting part now. Consider a value \u2018p\u2019 which lies between 0 and 1. So, f(p) = log { p/(1-p) }.If \u2018p\u2019 is assumed to be the probability that a woman has cervical cancer, then p/(1-p) is the \u2018odds\u2019 that a woman might have cervical cancer, where \u2019odds\u2019 is just another way of defining the probability of an event. Hence, f(p) can be considered to be the <b>log-odds</b> that a woman might have cancer. Now the range of f(p) lies between \u2212\u221e ...", "dateLastCrawled": "2022-02-01T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Tutorial on Logistic Regression using Gradient Descent with</b> Python - DPhi", "url": "https://dphi.tech/blog/tutorial-on-logistic-regression-using-python/", "isFamilyFriendly": true, "displayUrl": "https://dphi.tech/blog/<b>tutorial-on-logistic-regression-using</b>-python", "snippet": "Thus ln(p/(1\u2212p)) is known as the <b>log odds</b> and is simply used to map the probability that lies between 0 and 1 to a range between (\u2212\u221e,+\u221e). The terms b0, b1, b2\u2026 are parameters (or weights) that we will estimate during training. So this is just the basic math behind what we are going to do. We are interested in the probability p in this equation. So we simplify the equation to obtain the value of p: 1. The log term ln on the LHS can be removed by raising the RHS as a power of e: 2 ...", "dateLastCrawled": "2022-01-29T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Logistic Regression as Neural Networks</b> - Exploring <b>Machine</b> <b>Learning</b> ...", "url": "https://datascienceintuition.wordpress.com/2018/01/16/logistic-regression-as-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://datascienceintuition.wordpress.com/2018/01/16/logistic-regression-as-neural...", "snippet": "Exploring <b>Machine</b> <b>Learning</b> Algorithms. Menu Home; Contact; <b>Logistic Regression as Neural Networks</b>. ankitapaunikar Uncategorized January 16, 2018 January 19, 2018 7 Minutes. In our previous post, we understood in detail about Linear Regression where we predict a continuous variable as a linear function of input variables. But in case of the binomial variable, we follow another approach called Logistic regression where we predict the probability of the output variable as a logistic function of ...", "dateLastCrawled": "2022-01-29T02:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "CHAPTER <b>Logistic Regression</b> - Stanford University", "url": "https://www.web.stanford.edu/~jurafsky/slp3/5.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.web.stanford.edu/~jurafsky/slp3/5.pdf", "snippet": "line supervised <b>machine</b> <b>learning</b> algorithm for classi\ufb01cation, and also has a very close relationship with neural networks. As we will see in Chapter 7, a neural net-work can be viewed as a series of <b>logistic regression</b> classi\ufb01ers stacked on top of each other. Thus the classi\ufb01cation and <b>machine</b> <b>learning</b> techniques introduced here will play an important role throughout the book. <b>Logistic regression</b> can be used to classify an observation into one of two classes (like \u2018positive sentiment ...", "dateLastCrawled": "2022-02-02T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Section 8 Logistic Regression | Statistics <b>Learning</b>", "url": "https://ndleah.github.io/stat-learning/logistic-regression.html", "isFamilyFriendly": true, "displayUrl": "https://ndleah.github.io/stat-<b>learning</b>/logistic-regression.html", "snippet": "Table above shows the coefficient estimates and related information that result from fitting a logistic regression model on the Default data in order to predict the probability of default=Yes using balance.We see that \\(\\hat\\beta_1\\) = 0.0055; this indicates that an increase in balance is associated with an increase in the probability of default.To be precise, a one-unit increase in balance is associated with an increase in the <b>log odds</b> of default by 0.0055 units.", "dateLastCrawled": "2022-01-31T23:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "50 Data Scientist Interview Questions (ANSWERED with PDF) To Crack Next ...", "url": "https://www.mlstack.cafe/blog/data-scientist-interview-questions", "isFamilyFriendly": true, "displayUrl": "https://www.mlstack.cafe/blog/data-scientist-interview-questions", "snippet": "Essentially, <b>Machine</b> <b>Learning</b> is a method of teaching computers to make and improve predictions or behaviors based on some data. <b>Machine</b> <b>Learning</b> introduces a class of algorithms which is data-driven, i.e. unlike &quot;normal&quot; algorithms it is the data that &quot;tells&quot; what the &quot;good answer&quot; is. <b>Machine</b> <b>learning</b> creates a model based on sample data and ...", "dateLastCrawled": "2022-02-03T06:02:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(log-odds)  is like +(the logarithm of the odds of an event)", "+(log-odds) is similar to +(the logarithm of the odds of an event)", "+(log-odds) can be thought of as +(the logarithm of the odds of an event)", "+(log-odds) can be compared to +(the logarithm of the odds of an event)", "machine learning +(log-odds AND analogy)", "machine learning +(\"log-odds is like\")", "machine learning +(\"log-odds is similar\")", "machine learning +(\"just as log-odds\")", "machine learning +(\"log-odds can be thought of as\")", "machine learning +(\"log-odds can be compared to\")"]}