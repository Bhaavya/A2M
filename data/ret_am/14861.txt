{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Protected Attributes and &quot;Fairness through <b>Unawareness</b>,&quot; Exploring ...", "url": "https://ocw.mit.edu/resources/res-ec-001-exploring-fairness-in-machine-learning-for-international-development-spring-2020/module-three-framework/protected-attributes/protected-attributes-and-fairness-through-unawareness-exploring-fairness-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/resources/res-ec-001-<b>exploring-fairness-in-machine-learning</b>-for...", "snippet": "X Exclude words from your search Put - in front of a word you want to leave out. For example, jaguar speed -car Search for an exact match Put a word or phrase inside quotes.", "dateLastCrawled": "2021-12-10T01:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Bias and Fairness in <b>Machine</b> <b>Learning</b>, Part 1: introducing our dataset ...", "url": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-1-introducing-our-dataset-and-the-problem/", "isFamilyFriendly": true, "displayUrl": "https://freecontent.manning.com/bias-and-fairness-in-<b>machine</b>-<b>learning</b>-part-1...", "snippet": "<b>Unawareness</b>. <b>Unawareness</b> is likely the easiest definition of fairness. It states that a model should not include <b>sensitive</b> attributes as a feature in the training data. This way our model will not have access to the <b>sensitive</b> values when training. This definition aligns well with the idea of disparate treatment in that we are literally not ...", "dateLastCrawled": "2022-01-29T20:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Fairness in <b>Machine</b> <b>Learning</b> MITOCW | Protected Attributes and ...", "url": "https://ocw.mit.edu/resources/res-ec-001-exploring-fairness-in-machine-learning-for-international-development-spring-2020/module-three-framework/protected-attributes/protected-attributes-and-fairness-through-unawareness-exploring-fairness-in-machine-learning/6EPDzvUNCd0.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/resources/res-ec-001-exploring-fairness-in-<b>machine</b>-<b>learning</b>-for...", "snippet": "used a <b>machine</b> <b>learning</b> <b>algorithm</b> to screen resumes, and later found out that the <b>algorithm</b> was discriminating against female applicants. In another example, this time from the criminal justice system, <b>machine</b> <b>learning</b> is used to determine the risk of recidivism. This system has been questioned in a variety of studies, in particular, with reference to the protected <b>attribute</b> of race and gender. In another example of a large organization employing <b>machine</b> <b>learning</b>, this time, to display ads ...", "dateLastCrawled": "2022-01-27T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Google AI Blog: Equality of Opportunity in <b>Machine</b> <b>Learning</b>", "url": "https://ai.googleblog.com/2016/10/equality-of-opportunity-in-machine.html", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com/2016/10/equality-of-opportunity-in-<b>machine</b>.html", "snippet": "Despite the need, a vetted methodology in <b>machine</b> <b>learning</b> for preventing this kind of discrimination based on <b>sensitive</b> attributes has been lacking. A naive approach might require a set of <b>sensitive</b> attributes to be removed from the data before doing anything else with it. This idea of \u201cfairness through <b>unawareness</b>,\u201d however, fails due to the existence of \u201credundant encodings.\u201d Even if a particular <b>attribute</b> is not present in the data, combinations of other attributes can act as a ...", "dateLastCrawled": "2022-02-02T13:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Fix <b>Bias in Machine Learning Algorithms</b>? - Yields.io", "url": "https://www.yields.io/blog/how-to-fix-bias-in-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.yields.io/blog/how-to-fix-<b>bias-in-machine-learning-algorithms</b>", "snippet": "We would <b>like</b> to start this post by defining \u201cWhat is bias in <b>machine</b> <b>learning</b>?\u201d \u201cBias\u201d is the \u201cinclination or prejudice for/or against one person or group, especially in a way to be considered unfair\u201d. In the context of algorithms, there are especially two aspects that require our attention: Bias in algorithms is often driven by the data on which the <b>algorithm</b> is trainedMeasuring something to be unfair requires quantification in order to address this problem in <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-25T05:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Awareness in practice: tensions in access to <b>sensitive</b> <b>attribute</b> data ...", "url": "https://www.researchgate.net/publication/347462368_Awareness_in_practice_tensions_in_access_to_sensitive_attribute_data_for_antidiscrimination", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/347462368_Awareness_in_practice_tensions_in...", "snippet": "In this work, we tackle the problem of measuring group fairness under <b>unawareness</b> of <b>sensitive</b> attributes, by using techniques from quantification, a supervised <b>learning</b> task concerned with ...", "dateLastCrawled": "2022-01-14T04:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Equality of Opportunity in Supervised <b>Learning</b>", "url": "https://home.ttic.edu/~nati/Publications/HardtPriceSrebro2016.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Publications/HardtPriceSrebro2016.pdf", "snippet": "We propose a criterion for discrimination against a speci\ufb01ed <b>sensitive</b> <b>attribute</b> in su-pervised <b>learning</b>, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are avail- able, we show how to optimally adjust any learned predictor so as to remove discrimination according to our de\ufb01nition. Our framework also improves incentives by shifting the cost of poor classi\ufb01cation from disadvantaged ...", "dateLastCrawled": "2022-01-31T14:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Create <b>Trustworthy AI in Your Organization with ALTAI</b> - Yields.io", "url": "https://www.yields.io/blog/how-to-create-trustworthy-ai-in-your-organization-with-altai/", "isFamilyFriendly": true, "displayUrl": "https://www.yields.io/blog/how-to-create-<b>trustworthy-ai-in-your-organization-with-altai</b>", "snippet": "As explained in our previous article \u201chow to fix bias in <b>machine</b> <b>learning</b> algorithms?\u201d, there are three possible answers for this question: <b>unawareness</b>, demography parity and equalized odds or opportunities. <b>Unawareness</b> happens when the protected <b>attribute</b> is eliminated. A protected <b>attribute</b> is a data <b>attribute</b> against which the model needs to be fair, for instance, the language that you speak, your gender, and so on. By removing the <b>attribute</b>, we expect the model to develop sensitivity ...", "dateLastCrawled": "2021-12-19T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Two-stage <b>Algorithm</b> for <b>Fairness-aware</b> <b>Machine</b> <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/two-stage-algorithm-for-fairness-aware-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/two-stage-<b>algorithm</b>-for-<b>fairness-aware</b>-<b>machine</b>-<b>learning</b>", "snippet": "In other words, a <b>machine</b> <b>learning</b> <b>algorithm</b> that utilizes <b>sensitive</b> attributes is subject to biases in the existing data. This could be viewed as an algorithmic version of disparate treatment [], where decisions are made on the basis of these <b>sensitive</b> attributes.However, removing <b>sensitive</b> attributes from the dataset is not sufficient solution as it has a disparate impact.Disparate impact is a notion that was born in the 1970s.", "dateLastCrawled": "2021-12-10T00:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bias and Fairness in <b>Machine</b> <b>Learning</b>, Part 1: introducing our dataset ...", "url": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-1-introducing-our-dataset-and-the-problem/", "isFamilyFriendly": true, "displayUrl": "https://freecontent.manning.com/bias-and-fairness-in-<b>machine</b>-<b>learning</b>-part-1...", "snippet": "<b>Unawareness</b>. <b>Unawareness</b> is likely the easiest definition of fairness. It states that a model should not include <b>sensitive</b> attributes as a feature in the training data. This way our model will not have access to the <b>sensitive</b> values when training. This definition aligns well with the idea of disparate treatment in that we are literally not ...", "dateLastCrawled": "2022-01-29T20:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Pilot Study on Detecting Unfairness in Human Decisions With <b>Machine</b> ...", "url": "https://deepai.org/publication/a-pilot-study-on-detecting-unfairness-in-human-decisions-with-machine-learning-algorithmic-bias-detection", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-pilot-study-on-detecting-unfairness-in-human...", "snippet": "Fairness Through <b>Unawareness</b>. An <b>algorithm</b> is fair so long as any <b>sensitive</b> attributes A are not explicitly used in the decision-making process [grgic2016case]. C (X, A = 0) = C (X, A = 1) \u2200 X. (3) Individual Fairness. An <b>algorithm</b> is fair if it gives <b>similar</b> predictions to <b>similar</b> individuals [dwork2012fairness]. Given a distance metric d, if individuals i and j are <b>similar</b> under this metric (i.e., d (i, j) is small), their predictions should be <b>similar</b> too: C (X i, A i) \u2248 C (X j, A j ...", "dateLastCrawled": "2022-01-24T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "[PDF] Measuring Fairness under <b>Unawareness</b> via Quantification ...", "url": "https://www.semanticscholar.org/paper/Measuring-Fairness-under-Unawareness-via-Fabris-Esuli/c7625013e5016afd3e739553ce95b54318929cfd", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/Measuring-Fairness-under-<b>Unawareness</b>-via-Fabris...", "snippet": "This work tackles the problem of measuring group fairness under <b>unawareness</b> of <b>sensitive</b> attributes, by using techniques from quantification, a supervised <b>learning</b> task concerned with directly providing group-level prevalence estimates (rather than individual-level class labels). Models trained by means of supervised <b>learning</b> are increasingly deployed in high-stakes domains, and, when their predictions inform decisions about people, they inevitably impact (positively or negatively) on their ...", "dateLastCrawled": "2021-10-20T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How to Fix <b>Bias in Machine Learning Algorithms</b>? - Yields.io", "url": "https://www.yields.io/blog/how-to-fix-bias-in-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.yields.io/blog/how-to-fix-<b>bias-in-machine-learning-algorithms</b>", "snippet": "Another <b>similar</b> but more recent example was reported when the Apple credit card limits suddenly showed gender bias since married couples discovered that the husband\u2019s limit was 20 times higher when compared to his wife\u2019s. Common types of <b>bias in machine learning algorithms</b> In <b>machine</b> <b>learning</b>, we have different types of bias which can be organized in many different ways. We stick to the ones from Wikipedia: Pre-existing bias in algorithms is a consequence of underlying social and ...", "dateLastCrawled": "2022-01-25T05:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "21 fairer ML solutions - courses.cs.vt.edu", "url": "https://courses.cs.vt.edu/cs4824/Spring19/slide_pdfs/21%20fairer%20ML%20solutions.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.vt.edu/cs4824/Spring19/slide_pdfs/21 fairer ML solutions.pdf", "snippet": "\u2022 Different forms and causes of fairness in <b>machine</b> <b>learning</b> ... We propose a criterion for discrimination against a speci\ufb01ed <b>sensitive</b> <b>attribute</b> in su-pervised <b>learning</b>, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are avail-able, we show how to optimally adjust any learned predictor so as to remove discrimination according to our de\ufb01nition. Our framework also improves incentives ...", "dateLastCrawled": "2022-01-04T18:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Two-stage <b>Algorithm</b> for <b>Fairness-aware</b> <b>Machine</b> <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/two-stage-algorithm-for-fairness-aware-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/two-stage-<b>algorithm</b>-for-<b>fairness-aware</b>-<b>machine</b>-<b>learning</b>", "snippet": "In other words, a <b>machine</b> <b>learning</b> <b>algorithm</b> that utilizes <b>sensitive</b> attributes is subject to biases in the existing data. This could be viewed as an algorithmic version of disparate treatment [], where decisions are made on the basis of these <b>sensitive</b> attributes.However, removing <b>sensitive</b> attributes from the dataset is not sufficient solution as it has a disparate impact.Disparate impact is a notion that was born in the 1970s.", "dateLastCrawled": "2021-12-10T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Most Essential Python Fairness Libraries Every Data Scientist</b> Should ...", "url": "https://techairesearch.com/most-essential-python-fairness-libraries-every-data-scientist-should-know/", "isFamilyFriendly": true, "displayUrl": "https://techairesearch.com/<b>most-essential-python-fairness-libraries-every</b>-data...", "snippet": "In <b>Machine</b> <b>learning</b>/Artificial Intelligence, any given <b>algorithm</b> is said to be fair, or to have fairness, if its results are: ... based on the subject\u2019s <b>sensitive</b> <b>attribute</b>, and it has a disparate impact if its outcomes disproportionately hurt (or benefit) people with certain <b>sensitive</b> <b>attribute</b> values (e.g., females, blacks). Apart from the above-stated two principal definitions, there are many definitions of fairness that have been proposed in the literature (Gajane and Pechenizkiy, 2018 ...", "dateLastCrawled": "2022-01-28T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/interventions", "snippet": "Fairness through <b>unawareness</b> excludes protected attributes when <b>learning</b> a classifier with the aim on improving fairness. We consider the effect of applying Fairness Through <b>Unawareness</b> for a number of observational group fairness notions. How it works. This notebook contains the implementation of the common pre-processing intervention called Fairness Through <b>Unawareness</b> in which the protected <b>attribute</b> is not included as a feature in the training data. Besides being considered as an ...", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Create <b>Trustworthy AI in Your Organization with ALTAI</b> - Yields.io", "url": "https://www.yields.io/blog/how-to-create-trustworthy-ai-in-your-organization-with-altai/", "isFamilyFriendly": true, "displayUrl": "https://www.yields.io/blog/how-to-create-<b>trustworthy-ai-in-your-organization-with-altai</b>", "snippet": "Local explainability techniques explain a single AI decision by focusing on a set of <b>similar</b> samples and simplify the <b>algorithm</b> in that neighbourhood. Local explanations are relevant for small environments or for specific instances. These are mostly used by <b>machine</b> <b>learning</b> engineers and data scientists when auditing models and before deploying the models, instead of explaining it to the end-users. At last, global explainability techniques try to capture the global behaviour of the model ...", "dateLastCrawled": "2021-12-19T16:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Protected Attributes and &quot;Fairness through <b>Unawareness</b>,&quot; Exploring ...", "url": "https://ocw.mit.edu/resources/res-ec-001-exploring-fairness-in-machine-learning-for-international-development-spring-2020/module-three-framework/protected-attributes/protected-attributes-and-fairness-through-unawareness-exploring-fairness-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/resources/res-ec-001-<b>exploring-fairness-in-machine-learning</b>-for...", "snippet": "X Exclude words from your search Put - in front of a word you want to leave out. For example, jaguar speed -car Search for an exact match Put a word or phrase inside quotes.", "dateLastCrawled": "2021-12-10T01:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Fairness in <b>Machine</b> <b>Learning</b> MITOCW | Protected Attributes and ...", "url": "https://ocw.mit.edu/resources/res-ec-001-exploring-fairness-in-machine-learning-for-international-development-spring-2020/module-three-framework/protected-attributes/protected-attributes-and-fairness-through-unawareness-exploring-fairness-in-machine-learning/6EPDzvUNCd0.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/resources/res-ec-001-exploring-fairness-in-<b>machine</b>-<b>learning</b>-for...", "snippet": "is called fairness through <b>unawareness</b>. The use of <b>machine</b> <b>learning</b> presents both risks and opportunities. <b>Machine</b> <b>learning</b> <b>can</b> reduce costs by automating repetitive tasks, but could also increase biases. Certain individual attributes are commonly labeled as protected attributes, as they <b>can</b> be sources of social bias. These are race, religion, national origin, gender, marital status, age, and socioeconomic status. In the United States, discrimination based on these protected attributes in ...", "dateLastCrawled": "2022-01-27T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Accuracy and Fairness Trade-offs in <b>Machine</b> <b>Learning</b>: A Stochastic ...", "url": "http://www.optimization-online.org/DB_FILE/2020/08/7950.pdf", "isFamilyFriendly": true, "displayUrl": "www.optimization-online.org/DB_FILE/2020/08/7950.pdf", "snippet": "Accuracy and Fairness Trade-offs in <b>Machine</b> <b>Learning</b>: A Stochastic Multi-Objective Approach Suyun Liu* 1 Luis Nunes Vicente* 1 2 Abstract In the application of <b>machine</b> <b>learning</b> to real- life decision-making systems, e.g., credit scor-ing and criminal justice, the prediction outcomes might discriminate against people with <b>sensi-tive</b> attributes, leading to unfairness. The com-monly used strategy in fair <b>machine</b> <b>learning</b> is to include fairness as a constraint or a penal-ization term in the ...", "dateLastCrawled": "2021-11-22T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Accuracy and Fairness Trade-o s in <b>Machine</b> <b>Learning</b>: A Stochastic Multi ...", "url": "https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/tech-papers/20/20T_016.pdf", "isFamilyFriendly": true, "displayUrl": "https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/ise/pdf/...", "snippet": "fronts, and by doing so we <b>can</b> handle training data arriving in a streaming way. 1. Introduction <b>Machine</b> <b>learning</b> (ML) plays an increasingly signicant role in data-driven decision making, e.g., credit scoring, col-lege admission, hiring decisions, and criminal justice. As the <b>learning</b> models became more and more sophisticated,", "dateLastCrawled": "2022-01-28T20:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fairness in Machine Learning with Tractable Models</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-with-tractable-models", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-with-tractable-models</b>", "snippet": "<b>Fairness in Machine Learning with Tractable Models</b>. 05/16/2019 \u2219 by Michael Varley, et al. \u2219 0 \u2219 share . <b>Machine</b> <b>Learning</b> techniques have become pervasive across a range of different applications, and are now widely used in areas as disparate as recidivism prediction, consumer credit-risk analysis and insurance pricing. The prevalence of <b>machine</b> <b>learning</b> techniques has raised concerns about the potential for learned algorithms to become biased against certain groups.", "dateLastCrawled": "2021-11-27T09:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fairness metrics and bias mitigation strategies for rating predictions ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001369", "snippet": "Fairness Through <b>Unawareness</b>: This states that an <b>algorithm</b> or model <b>can</b> be considered fair as long as it does not use any of the unprivileged group defining attributes or the protected attributes explicitly in the decision-making process. Chen et al., 2019, Gajane and Pechenizkiy, 2017, Grgic-Hlaca et al., 2016, Kusner et al., 2017 and Pedreshi et al. (2008) Counterfactual Fairness: As per this the outcome should remain the same in both the real/actual world and a counter-factual world ...", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "ACCURACY AND FAIRNESS TRADE-OFFS IN <b>MACHINE</b> APPROACH", "url": "https://www.mat.uc.pt/preprints/ps/p2037.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mat.uc.pt/preprints/ps/p2037.pdf", "snippet": "ACCURACY AND FAIRNESS TRADE-OFFS IN <b>MACHINE</b> <b>LEARNING</b>: A STOCHASTIC MULTI-OBJECTIVE APPROACH S. LIU AND L. N. VICENTE Abstract: In the application of <b>machine</b> <b>learning</b> to real-life decision-making sys- tems, e.g., credit scoring and criminal justice, the prediction outcomes might dis-criminate against people with <b>sensitive</b> attributes, leading to unfairness. The com-monly used strategy in fair <b>machine</b> <b>learning</b> is to include fairness as a constraint or a penalization term in the minimization of ...", "dateLastCrawled": "2022-01-09T06:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "InfoGram and admissible <b>machine</b> <b>learning</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10994-021-06121-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-021-06121-4", "snippet": "Imagine that a <b>machine</b> <b>learning</b> <b>algorithm</b> is used by a bank to accurately predict whether to approve or deny a loan application based on the probability of default. This ML-based risk-assessing tool has access to the following historical data: Y: {0, 1} Loan status variable\u20131 whether the loan was approved and 0 if denied. \\({\\mathbf {X}}\\): Feature matrix {income, loan amount, education, credit history, zip code} \\({\\mathbf {S}}\\): Collection of protected attributes {gender, marital status ...", "dateLastCrawled": "2022-01-31T17:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) On Fair Representation in <b>Machine</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/341736051_On_Fair_Representation_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../341736051_On_Fair_Representation_in_<b>Machine</b>_<b>Learning</b>", "snippet": "With recent developments in the application of <b>Machine</b> <b>Learning</b> (ML) to <b>sensitive</b> areas such as financial, legal, and medical institutions as well as applications with high social impact such as ...", "dateLastCrawled": "2022-01-26T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to <b>Recognize and Change Human &amp; Machine Discrimination</b>", "url": "https://citizensandtech.org/2020/02/recognize-and-change-human-machine-discrimination/", "isFamilyFriendly": true, "displayUrl": "https://citizensandtech.org/2020/02/<b>recognize-and-change-human-machine-discrimination</b>", "snippet": "Fairness in <b>Machine</b> <b>Learning</b>. How <b>can</b> psychologists collaborate with computer scientists on fairness in <b>machine</b> <b>learning</b>? Speaking next is Moritz Hardt, an assistant professor in computer science at UC Berkeley. His research aims to make the practice of <b>machine</b> <b>learning</b> more robust, reliable, and aligned with societal values. Moritz Hardt at SPSP 2020 speaking about fairness in <b>Machine</b> <b>Learning</b>. Moritz tells us about research in computer science on fairness in <b>machine</b> <b>learning</b>. Much of this ...", "dateLastCrawled": "2021-12-26T18:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to Fix <b>Bias in Machine Learning Algorithms</b>? - Yields.io", "url": "https://www.yields.io/blog/how-to-fix-bias-in-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.yields.io/blog/how-to-fix-<b>bias-in-machine-learning-algorithms</b>", "snippet": "In <b>machine</b> <b>learning</b>, we have different types of bias which <b>can</b> be organized in many different ways. We stick to the ones from Wikipedia: We stick to the ones from Wikipedia: Pre-existing bias in algorithms is a consequence of underlying social and institutional ideologies, which <b>can</b> have an impact on the designers or programmers of the software \u2013 human bias in <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2022-01-25T05:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Fairness without the <b>sensitive</b> <b>attribute</b> via Causal Variational ...", "url": "https://www.researchgate.net/publication/354542106_Fairness_without_the_sensitive_attribute_via_Causal_Variational_Autoencoder", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354542106_Fairness_without_the_<b>sensitive</b>...", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.", "dateLastCrawled": "2022-01-04T17:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Fairness without the <b>sensitive</b> <b>attribute</b> via Causal Variational ...", "url": "https://deepai.org/publication/fairness-without-the-sensitive-attribute-via-causal-variational-autoencoder", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fairness-without-the-<b>sensitive</b>-<b>attribute</b>-via-causal...", "snippet": "Throughout this document, we consider a supervised <b>machine</b> <b>learning</b> <b>algorithm</b> for classification problems. The training data consists of n examples (x i, s i, y i) n i = 1, where x i \u2208 R p. is the feature vector with . p predictors of the i-th example, s i is its binary discrete <b>sensitive</b> <b>attribute</b> and y i its binary discrete outcome true value.", "dateLastCrawled": "2022-01-21T18:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Interventions | <b>Machine Learning Bias Mitigation</b>", "url": "https://cdeiuk.github.io/bias-mitigation/interventions/", "isFamilyFriendly": true, "displayUrl": "https://cdeiuk.github.io/bias-mitigation/interventions", "snippet": "The <b>algorithm</b> is very widely applicable, as it only needs access to the model outputs and the protected <b>attribute</b>. Moreover Hardt et al. show that their <b>algorithm</b> is optimal among post-processing algorithms for equalised odds. However, the possible randomness present in predictions may not be satisfactory when individual fairness is a concern, as two identical individuals could receive different predictions due to the stochasticity.", "dateLastCrawled": "2022-02-02T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "IBM researchers investigate ways to help reduce bias in healthcare AI ...", "url": "https://researchweb.draco.res.ibm.com/blog/ibm-reduce-bias-in-healthcare-ai", "isFamilyFriendly": true, "displayUrl": "https://researchweb.draco.res.ibm.com/blog/ibm-reduce-bias-in-healthcare-ai", "snippet": "Fairness is often defined with respect to the relationship between a <b>sensitive</b> <b>attribute</b>, such as a demographic ... we wanted to demonstrate how recent advances in fairness-aware <b>machine</b> <b>learning</b> approaches <b>can</b> be applied to clinical use cases so that people <b>can</b> learn and use those methods in practice. Debiasing with Prejudice Remover and reweighing. PPD affects one in nine women in the US who give birth, and early detection has significant implications for maternal and child health ...", "dateLastCrawled": "2022-01-30T18:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "with <b>Machine</b> <b>Learning</b>", "url": "https://aisecure.github.io/TEACHING/CS562/Fall2021_files/student_pres/1118JiaweiZhang.pdf", "isFamilyFriendly": true, "displayUrl": "https://aisecure.github.io/TEACHING/CS562/Fall2021_files/student_pres/1118JiaweiZhang.pdf", "snippet": "De\ufb01nition 1: (fairness through <b>unawareness</b>) A predictor is said to achieve fairness through <b>unawareness</b> if protected attributes are not explicitly used in the prediction process. Weakness: the remaining attributes may be highly correlated with the protected <b>attribute</b>\u2026", "dateLastCrawled": "2022-01-11T17:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Machine</b> <b>learning</b> models <b>can</b> also be a source of disparate impact in their implementation, through unconscious human biases that affect the fair interpretation or use of the model&#39;s results. This reference does not cover measurement of fairness at implementation. However, if you are interested in fair implementation, we recommend looking at Google&#39;s Fairness Indicators. Harms. In evaluating the potential impact of an ML model, it <b>can</b> be helpful to first clarify what specific harm(s) <b>can</b> be ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b> \u2013 The Results Are Not the only Thing that Matters ...", "url": "https://link.springer.com/chapter/10.1007/978-3-030-50423-6_46", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-50423-6_46", "snippet": "In summary, the definition of unfair <b>machine</b> <b>learning</b> process <b>can</b> be formulated as a situation in which an output tends to be disproportionately benefitting (or unfair) towards the group characterized by certain <b>sensitive</b> <b>attribute</b> values. However, this commonly used definition is too abstract to reach a consensus on the mathematical formulations of fairness. The majority of definitions of fairness in ML include the following elements", "dateLastCrawled": "2021-12-22T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Accuracy and Fairness Trade-offs in <b>Machine</b> <b>Learning</b>: A Stochastic ...", "url": "http://www.optimization-online.org/DB_FILE/2020/08/7950.pdf", "isFamilyFriendly": true, "displayUrl": "www.optimization-online.org/DB_FILE/2020/08/7950.pdf", "snippet": "<b>Machine</b> <b>learning</b> (ML) plays an increasingly signi\ufb01cant role in data-driven decision making, e.g., credit scoring, col-lege admission, hiring decisions, and criminal justice. As the <b>learning</b> models became more and more sophisticated, concern regarding fairness started receiving more and more attention. In 2014, the Obama Administration\u2019s Big Data Report (Podesta et al.,2014) claimed that discrimination against individuals and groups might be the \u201cinadvertent outcome of the way big data ...", "dateLastCrawled": "2021-11-22T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "arXiv:2012.04800v1 [cs.LG] 9 Dec 2020", "url": "https://www.researchgate.net/publication/346858234_A_Statistical_Test_for_Probabilistic_Fairness/fulltext/5fd191d792851c00f862295a/A-Statistical-Test-for-Probabilistic-Fairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/346858234_A_Statistical_Test_for...", "snippet": "a <b>machine</b> <b>learning</b> <b>algorithm</b> to simultaneously satisfy multiple notions of fairness [5,37]. Therefore, the Therefore, the choice of the fairness notion is likely to remain more an art than a science.", "dateLastCrawled": "2021-12-30T21:01:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "No fairness through <b>unawareness</b>. Some have hoped that removing or ignoring <b>sensitive</b> attributes would somehow ensure the impartiality of the resulting classifier. Unfortunately, this practice is usually somewhere on the spectrum between ineffective and harmful. In a typical data set, we have many features that are slightly correlated with the <b>sensitive</b> <b>attribute</b>. Visiting the website pinterest.com, for example, has a small statistical correlation with being female. As of August 2017, 58.9% ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Fairness without the <b>sensitive</b> <b>attribute</b> via Causal Variational ...", "url": "https://www.researchgate.net/publication/354542106_Fairness_without_the_sensitive_attribute_via_Causal_Variational_Autoencoder", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354542106_Fairness_without_the_<b>sensitive</b>...", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will ...", "dateLastCrawled": "2022-01-04T17:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) On Fair Representation in <b>Machine</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/341736051_On_Fair_Representation_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../341736051_On_Fair_Representation_in_<b>Machine</b>_<b>Learning</b>", "snippet": "<b>Machine</b> <b>Learning</b> (ML) to <b>sensitive</b> areas such as \ufb01nancial, legal, and medical institutions as well as applications with high social impact such as employment procedures, fairness", "dateLastCrawled": "2022-01-26T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Survey on Bias and Fairness in <b>Machine</b> <b>Learning</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1908.09635/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.09635", "snippet": "With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many <b>sensitive</b> environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in <b>machine</b> <b>learning</b>, natural language ...", "dateLastCrawled": "2021-11-15T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Survey on Bias and <b>Fairness in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/a-survey-on-bias-and-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-on-bias-and-<b>fairness-in-machine-learning</b>", "snippet": "A Survey on Bias and <b>Fairness in Machine Learning</b>. 08/23/2019 \u2219 by Ninareh Mehrabi, et al. \u2219 1 \u2219 share. With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many <b>sensitive</b> ...", "dateLastCrawled": "2022-01-22T23:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Manipulation by algorithms. Exploring the triangle of unfair commercial ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/eulj.12389", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/eulj.12389", "snippet": "Hence, the <b>machine</b> <b>learning</b> model will scrutinise large amounts of ... If the latter fuses economic transactions and privacy-<b>sensitive</b> data processing, the law should, to the extent that it is doctrinally possible and methodologically sound, mirror this tendency by building further bridges between different legal fields once considered separate.117 117 See the references in n. 14. The effects of privacy and GDPR breaches. This link is important because, for potential plaintiffs, it would ...", "dateLastCrawled": "2022-01-08T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Hiding Incompetence</b> <b>By Being Polite, Positive</b> and Praising", "url": "https://omegazadvisors.com/2020/03/16/hiding-incompetence/", "isFamilyFriendly": true, "displayUrl": "https://omegazadvisors.com/2020/03/16/<b>hiding-incompetence</b>", "snippet": "Tags competence, Disruptive Innovation &amp; People <b>Analogy</b>, employees, politeness, positivity, praise , sycophant. Employees work to keep their jobs. It\u2019s not only about doing the job though. It\u2019s about influencing how bosses see them too. They know that if bosses like them they are more likely to see their work favorably. If bosses don\u2019t, they won\u2019t. Thus, this becomes a strategy for <b>hiding incompetence</b> too, getting the boss to like them. Gaining The Boss\u2019 Favor. Even if objective ...", "dateLastCrawled": "2022-01-16T16:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Ultimate Data Science Flashcards | Quizlet", "url": "https://quizlet.com/474731310/ultimate-data-science-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/474731310/ultimate-data-science-flash-cards", "snippet": "A distributed <b>machine</b> <b>learning</b> approach that trains <b>machine</b> <b>learning</b> models using decentralized examples residing on devices such as smartphones. In federated <b>learning</b>, a subset of devices downloads the current model from a central coordinating server. The devices use the examples stored on the devices to make improvements to the model. The devices then upload the model improvements (but not the training examples) to the coordinating server, where they are aggregated with other updates to ...", "dateLastCrawled": "2021-06-24T10:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Program for Monday, April 15th", "url": "https://easychair.org/smart-program/TEAP2019/2019-04-15.html", "isFamilyFriendly": true, "displayUrl": "https://easychair.org/smart-program/TEAP2019/2019-04-15.html", "snippet": "In a series of three experiment (N1 = 126, N2 = 76, N3 = 76), we compared predetermined <b>learning</b> to <b>learning</b> from free information sampling. Across the three experiments we manipulated whether participants in the information sampling condition were allowed to select the option, the context, or both for each <b>learning</b> trial. Furthermore, we tested whether pseudocontingencies are still inferred when engaging in self-determined information sampling during <b>learning</b>. The results revealed the ...", "dateLastCrawled": "2022-01-01T15:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Bangalore Water-Supply</b> &amp; ... vs R. Rajappa &amp; Others on 21 February, 1978", "url": "https://indiankanoon.org/doc/1149369/", "isFamilyFriendly": true, "displayUrl": "https://indiankanoon.org/doc/1149369", "snippet": "It is perhaps difficult to <b>attribute</b> to a legislative body functioning in a static society that its intention was couched in terms of considerable breadth so as to take within its sweep the future developments comprehended by the phraseology used.. It is more reasonable to confine its intention only to the circumstances obtaining at the time the law was made. But in a modern progressive society it would be unreasonable to confine the intention of a Legislature to the meaning attributable to ...", "dateLastCrawled": "2022-01-30T22:46:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(unawareness (to a sensitive attribute))  is like +(machine learning algorithm)", "+(unawareness (to a sensitive attribute)) is similar to +(machine learning algorithm)", "+(unawareness (to a sensitive attribute)) can be thought of as +(machine learning algorithm)", "+(unawareness (to a sensitive attribute)) can be compared to +(machine learning algorithm)", "machine learning +(unawareness (to a sensitive attribute) AND analogy)", "machine learning +(\"unawareness (to a sensitive attribute) is like\")", "machine learning +(\"unawareness (to a sensitive attribute) is similar\")", "machine learning +(\"just as unawareness (to a sensitive attribute)\")", "machine learning +(\"unawareness (to a sensitive attribute) can be thought of as\")", "machine learning +(\"unawareness (to a sensitive attribute) can be compared to\")"]}