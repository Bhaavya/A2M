{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Precision-Recall Curve | ML - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/precision-recall-curve-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/precision-recall-<b>curve</b>-ml", "snippet": "However, most machine <b>learning</b> algorithms often involve a trade-off between the two. A good <b>PR</b> <b>curve</b> has greater AUC (<b>area</b> <b>under</b> <b>curve</b>). In the figure above, the classifier corresponding to the blue line has better performance than the classifier corresponding to the green line. It is important to note that the classifier that has a higher AUC on the ROC <b>curve</b> will always have a higher AUC on <b>the PR</b> <b>curve</b> as well. Consider an algorithm that classifies whether or not a document belongs to the ...", "dateLastCrawled": "2022-02-02T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Complete Guide to Understanding Precision and Recall Curves", "url": "https://analyticsindiamag.com/complete-guide-to-understanding-precision-and-recall-curves/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/complete-guide-to-<b>under</b>standing-precision-and-recall-<b>curves</b>", "snippet": "The <b>Area</b> <b>under</b> <b>the PR</b> <b>Curve</b> is a metric that helps us compare 2 similar looking curves. Higher the AUC, the better the performance. The baseline of <b>PR</b> <b>Curve</b>. Fig 2. Fig 3 . The Baseline of a <b>PR</b> <b>Curve</b> changes with class imbalance, unlike ROC <b>Curve</b>. This is due to the fact that Precision of a No-Skill model (which gives 0.5 score for every output) directly depends on the class imbalance. Fig 2 shows the baseline corresponding to a balanced dataset, whereas, Fig 3 shows the baseline ...", "dateLastCrawled": "2022-01-31T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "machine <b>learning</b> - <b>Interpretation of the area under</b> <b>the PR</b> <b>curve</b> ...", "url": "https://stats.stackexchange.com/questions/99916/interpretation-of-the-area-under-the-pr-curve", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/.../99916/<b>interpretation-of-the-area-under</b>-<b>the-pr</b>-<b>curve</b>", "snippet": "So the expected <b>PR</b> <b>curve</b> for a random classifier is just a rectangle with side lengths &quot;proportion of true positives&quot; x 1. For example, if your dataset contains 10% positive cases and 90% negative cases, the expected auPR <b>under</b> chance is 0.1. $\\endgroup$ \u2013", "dateLastCrawled": "2022-01-21T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Measuring Performance: AUPRC</b> and Average Precision \u2013 Glass Box", "url": "https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/", "isFamilyFriendly": true, "displayUrl": "https://glassboxmedicine.com/2019/03/02/<b>measuring-performance-auprc</b>", "snippet": "The AUPRC is calculated as the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. A <b>PR</b> <b>curve</b> shows the trade-off between precision and recall across different decision thresholds. (Note that \u201crecall\u201d is another name for the true positive rate (TPR). Thus, AUPRC and AUROC both make use of the TPR. For a review of TPR, precision, and decision thresholds, see Measuring Performance: The Confusion Matrix.) The x-axis of a <b>PR</b> <b>curve</b> is the recall and the y-axis is the precision. This is in contrast to ROC curves, where ...", "dateLastCrawled": "2022-02-03T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "r - <b>Area under</b> the ROC <b>curve</b> or <b>area under</b> <b>the PR</b> <b>curve</b> for imbalanced ...", "url": "https://stats.stackexchange.com/questions/90779/area-under-the-roc-curve-or-area-under-the-pr-curve-for-imbalanced-data", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/90779", "snippet": "I have some doubts about which performance measure to use, <b>area under</b> the ROC <b>curve</b> (TPR as a function of FPR) or <b>area under</b> the precision-recall <b>curve</b> (precision as a function of recall). My data is imbalanced, i.e., the number of negative instances is much larger than positive instances. I am using the output prediction of weka, a sample is:", "dateLastCrawled": "2022-02-03T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Use ROC Curves and <b>Precision-Recall Curves for Classification</b> in ...", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "<b>Area</b> <b>Under</b> <b>Curve</b>: <b>like</b> the AUC, summarizes the integral or an approximation of the <b>area</b> <b>under</b> the precision-recall <b>curve</b>. In terms of model selection, F-Measure summarizes model skill for a specific probability threshold (e.g. 0.5), whereas the <b>area</b> <b>under</b> <b>curve</b> summarize the skill of a model across thresholds, <b>like</b> ROC AUC.", "dateLastCrawled": "2022-02-03T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Ml Roc <b>Pr</b> - Plotly", "url": "https://plotly.com/python/roc-and-pr-curves/", "isFamilyFriendly": true, "displayUrl": "https://plotly.com/python/<b>roc-and-pr-curves</b>", "snippet": "Python &gt; Artificial Intelligence and Machine <b>Learning</b> &gt; <b>ROC and PR Curves</b>. Suggest an edit to this page . <b>ROC and PR Curves</b> in Python ... which is an alternative scoring method to the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. In [6]: import plotly.graph_objects as go import plotly.express as px import numpy as np import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.metrics import precision_recall_<b>curve</b>, average_precision_score np. random. seed (0) # Artificially add noise to ...", "dateLastCrawled": "2022-01-31T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "This is called the ROC <b>area</b> <b>under</b> <b>curve</b> or ROC AUC or sometimes ROCAUC. The score is a value between 0.0 and 1.0 for a perfect classifier. AUCROC can be interpreted as the probability that the scores given by a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. \u2014 Page 54, <b>Learning</b> from Imbalanced Data Sets, 2018. This single score can be used to compare binary classifier models directly. As such, this score might be the most commonly used ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "F1 <b>Score</b> vs ROC AUC vs Accuracy vs <b>PR</b> AUC: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-roc-auc-<b>pr</b>-auc", "snippet": "AUC means <b>area</b> <b>under</b> the <b>curve</b> so to speak about ROC AUC <b>score</b> we need to define ROC <b>curve</b> first. It is a chart that visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart.", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Precision-recall</b> curves \u2013 what are they and how are they used?", "url": "https://acutecaretesting.org/en/articles/precision-recall-curves-what-are-they-and-how-are-they-used", "isFamilyFriendly": true, "displayUrl": "https://acutecaretesting.org/en/articles/<b>precision-recall</b>-<b>curves</b>-what-are-they-and-how...", "snippet": "Summary This article demonstrates that adding a lot of patients without disease and with low test results to a study may improve the ROC <b>curve</b> significantly without any improvement in sensitivity or in positive predictive value of the parameter evaluated. The <b>precision-recall</b> curves are not impacted by the addition of patients without disease and with low test results. It is highly recommended to use <b>precision-recall</b> curves as a supplement to the routinely used ROC curves to get the full ...", "dateLastCrawled": "2022-02-02T06:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Complete Guide to Understanding Precision and Recall Curves", "url": "https://analyticsindiamag.com/complete-guide-to-understanding-precision-and-recall-curves/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/complete-guide-to-<b>under</b>standing-precision-and-recall-<b>curves</b>", "snippet": "The <b>Area</b> <b>under</b> <b>the PR</b> <b>Curve</b> is a metric that helps us compare 2 <b>similar</b> looking curves. Higher the AUC, the better the performance. The baseline of <b>PR</b> <b>Curve</b>. Fig 2. Fig 3. The Baseline of a <b>PR</b> <b>Curve</b> changes with class imbalance, unlike ROC <b>Curve</b>. This is due to the fact that Precision of a No-Skill model (which gives 0.5 score for every output ...", "dateLastCrawled": "2022-01-31T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "machine <b>learning</b> - <b>Interpretation of the area under</b> <b>the PR</b> <b>curve</b> ...", "url": "https://stats.stackexchange.com/questions/99916/interpretation-of-the-area-under-the-pr-curve", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/.../99916/<b>interpretation-of-the-area-under</b>-<b>the-pr</b>-<b>curve</b>", "snippet": "<b>Area</b> <b>under</b> the ROC <b>curve</b> or <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b> for imbalanced data? 10. An intuitive meaning of the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>? 8. outlier detection: <b>area</b> <b>under</b> precision recall <b>curve</b>. 1. Machine <b>learning</b> model metrics vs predicted probability? 3. <b>Area</b> <b>Under</b> the Precision Recall <b>curve</b> -<b>similar</b> interpretation to AUROC? 1. How to improve the accuracy of an ARIMA model. 2. <b>Area</b> <b>under</b> the <b>curve</b> score training/validation set. 1. Why does the best model in the training set have the worst test ...", "dateLastCrawled": "2022-01-21T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Measuring Performance: AUPRC</b> and Average Precision \u2013 Glass Box", "url": "https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/", "isFamilyFriendly": true, "displayUrl": "https://glassboxmedicine.com/2019/03/02/<b>measuring-performance-auprc</b>", "snippet": "The AUPRC is calculated as the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. A <b>PR</b> <b>curve</b> shows the trade-off between precision and recall across different decision thresholds. (Note that \u201crecall\u201d is another name for the true positive rate (TPR). Thus, AUPRC and AUROC both make use of the TPR. For a review of TPR, precision, and decision thresholds, see Measuring Performance: The Confusion Matrix.) The x-axis of a <b>PR</b> <b>curve</b> is the recall and the y-axis is the precision. This is in contrast to ROC curves, where ...", "dateLastCrawled": "2022-02-03T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Unachievable Region in Precision-Recall Space and Its Effect on ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3858955/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3858955", "snippet": "A <b>similar</b> normalization approach for summarizing <b>the PR</b> <b>curve</b> leads to a non-linear transformation of <b>PR</b> space that can change the <b>area</b> <b>under</b> the curves in unexpected ways. An effective method for generating a summary <b>PR</b> <b>curve</b> that preserves measures of <b>area</b> in a satisfying way and accounts for the unachievable region would be useful and is a promising <b>area</b> of future research.", "dateLastCrawled": "2021-12-22T11:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Ml Roc <b>Pr</b> - plotly.com", "url": "https://plotly.com/r/roc-and-pr-curves/", "isFamilyFriendly": true, "displayUrl": "https://plotly.com/r/roc-and-<b>pr</b>-<b>curves</b>", "snippet": "Basic binary ROC <b>curve</b>. We display the <b>area</b> <b>under</b> the ROC <b>curve</b> (ROC AUC). While ROC shows how the TPR and FPR vary with the threshold, the ROC AUC is a measure of the classification model&#39;s ability to distinguish one class from the other. An ideal classifier will have ROC AUC = 1. In our example, we see that the ROC AUC is fairly high, thus ...", "dateLastCrawled": "2022-02-02T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Ml Roc <b>Pr</b> - Plotly", "url": "https://plotly.com/python/roc-and-pr-curves/", "isFamilyFriendly": true, "displayUrl": "https://plotly.com/python/<b>roc-and-pr-curves</b>", "snippet": "Python &gt; Artificial Intelligence and Machine <b>Learning</b> &gt; <b>ROC and PR Curves</b>. Suggest an edit to this page . <b>ROC and PR Curves</b> in Python ... Notice how this ROC <b>curve</b> looks <b>similar</b> to the True Positive Rate <b>curve</b> from the previous plot. This is because they are the same <b>curve</b>, except the x-axis consists of increasing values of FPR instead of threshold, which is why the line is flipped and distorted. We also display the <b>area</b> <b>under</b> the ROC <b>curve</b> (ROC AUC), which is fairly high, thus consistent ...", "dateLastCrawled": "2022-01-31T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "F1 <b>Score</b> vs ROC AUC vs Accuracy vs <b>PR</b> AUC: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-roc-auc-<b>pr</b>-auc", "snippet": "Similarly to ROC AUC <b>score</b> you can calculate the <b>Area</b> <b>Under</b> the Precision-Recall <b>Curve</b> to get one number that describes model performance. You can also think of <b>PR</b> AUC as the average of precision scores calculated for each recall threshold. You can also adjust this definition to suit your business needs by choosing/clipping recall thresholds if ...", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>precision and recall</b> - GitHub Pages", "url": "https://bbabenko.github.io/prs/", "isFamilyFriendly": true, "displayUrl": "https://bbabenko.github.io/prs", "snippet": "it\u2019s easy to see that a perfect <b>PR</b> <b>curve</b> (and a perfect ROC <b>curve</b>) would end up with an <b>area</b> of 1.0. this is usually referred to as \u201caverage precision\u201d when talking about <b>PR</b>, and \u201c<b>area</b> <b>under</b> the <b>curve</b>\u201d when talking about ROC. i personally don\u2019t like these metrics, especially for <b>PR</b> curves, for two reasons: 1) there\u2019s a surprising amount of nuance in implementing average precision \u2013 see section 4 of", "dateLastCrawled": "2022-02-03T01:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "This is called the ROC <b>area</b> <b>under</b> <b>curve</b> or ROC AUC or sometimes ROCAUC. The score is a value between 0.0 and 1.0 for a perfect classifier. AUCROC can be interpreted as the probability that the scores given by a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. \u2014 Page 54, <b>Learning</b> from Imbalanced Data Sets, 2018. This single score can be used to compare binary classifier models directly. As such, this score might be the most commonly used ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to Use ROC Curves and <b>Precision-Recall Curves for Classification</b> in ...", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "<b>Area</b> <b>Under</b> <b>Curve</b>: like the AUC, summarizes the integral or an approximation of the <b>area</b> <b>under</b> the precision-recall <b>curve</b>. In terms of model selection, F-Measure summarizes model skill for a specific probability threshold (e.g. 0.5), whereas the <b>area</b> <b>under</b> <b>curve</b> summarize the skill of a model across thresholds, like ROC AUC.", "dateLastCrawled": "2022-02-03T04:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "machine <b>learning</b> - <b>Interpretation of the area under</b> <b>the PR</b> <b>curve</b> ...", "url": "https://stats.stackexchange.com/questions/99916/interpretation-of-the-area-under-the-pr-curve", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/.../99916/<b>interpretation-of-the-area-under</b>-<b>the-pr</b>-<b>curve</b>", "snippet": "<b>PR</b> <b>curve</b> is <b>thought</b> to be more informative when there is a high class imbalance in the data, ... <b>Area</b> <b>under</b> the ROC <b>curve</b> or <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b> for imbalanced data? 10. An intuitive meaning of the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>? 8. outlier detection: <b>area</b> <b>under</b> precision recall <b>curve</b>. 1. Machine <b>learning</b> model metrics vs predicted probability? 3. <b>Area</b> <b>Under</b> the Precision Recall <b>curve</b> -similar interpretation to AUROC? 1. How to improve the accuracy of an ARIMA model. 2. <b>Area</b> <b>under</b> the <b>curve</b> ...", "dateLastCrawled": "2022-01-21T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Measuring Performance: AUPRC</b> and Average Precision \u2013 Glass Box", "url": "https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/", "isFamilyFriendly": true, "displayUrl": "https://glassboxmedicine.com/2019/03/02/<b>measuring-performance-auprc</b>", "snippet": "The AUPRC is calculated as the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. A <b>PR</b> <b>curve</b> shows the trade-off between precision and recall across different decision thresholds. (Note that \u201crecall\u201d is another name for the true positive rate (TPR). Thus, AUPRC and AUROC both make use of the TPR. For a review of TPR, precision, and decision thresholds, see Measuring Performance: The Confusion Matrix.) The x-axis of a <b>PR</b> <b>curve</b> is the recall and the y-axis is the precision. This is in contrast to ROC curves, where ...", "dateLastCrawled": "2022-02-03T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "F1 <b>Score</b> vs ROC AUC vs Accuracy vs <b>PR</b> AUC: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-roc-auc-<b>pr</b>-auc", "snippet": "Similarly to ROC AUC <b>score</b> you <b>can</b> calculate the <b>Area</b> <b>Under</b> the Precision-Recall <b>Curve</b> to get one number that describes model performance. You <b>can</b> also think of <b>PR</b> AUC as the average of precision scores calculated for each recall threshold. You <b>can</b> also adjust this definition to suit your business needs by choosing/clipping recall thresholds if ...", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Intro to Deep <b>Learning</b> \u2014 performance metrics(Precision, Recall, F1, ROC ...", "url": "https://hk3342.medium.com/intro-to-deep-learning-performance-metrics-precision-recall-f1-roc-pr-prg-87f5073f9354", "isFamilyFriendly": true, "displayUrl": "https://hk3342.medium.com/intro-to-deep-<b>learning</b>-performance-metrics-precision-recall...", "snippet": "<b>PR</b> <b>Curve</b> of logistic regression and adaboost. PRG(<b>PR</b> Gain) Recently, a new metric PRG(\u2018<b>PR</b> Gain <b>curve</b>\u2019) is introduced. Peter et al. claimed that <b>PR</b> Gain <b>curve</b> provides a better comparison between different models and enables the identification of inferior ones. To empirically check this claim, let\u2019s compare AUROC (<b>Area</b> <b>under</b> ROC), AUPR ...", "dateLastCrawled": "2022-01-30T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Area</b> <b>under</b> the ROC <b>curve \u2013 assessing discrimination in logistic</b> ...", "url": "https://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://thestatsgeek.com/2014/05/05/<b>area</b>-<b>under</b>-the-roc-<b>curve</b>-assessing-discrimination...", "snippet": "Thus the <b>area</b> <b>under</b> the <b>curve</b> ranges from 1, corresponding to perfect discrimination, to 0.5, corresponding to a model with no discrimination ability. The <b>area</b> <b>under</b> the ROC <b>curve</b> is also sometimes referred to as the c-statistic (c for concordance). The <b>area</b> <b>under</b> the estimated ROC <b>curve</b> (AUC) is reported when we plot the ROC <b>curve</b> in R&#39;s Console.", "dateLastCrawled": "2022-01-29T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "classification - What is a <b>good</b> AUC for a precision-recall <b>curve</b> ...", "url": "https://stats.stackexchange.com/questions/113326/what-is-a-good-auc-for-a-precision-recall-curve", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/113326", "snippet": "An ideal <b>PR</b>-<b>curve</b> goes from the topleft corner horizontically to the topright corner and straight down to the bottomright corner, resulting in a <b>PR</b>-AUC of 1. In some applications, <b>the PR</b>-<b>curve</b> shows instead a strong spike at the beginning to quickly drop again close to the &quot;random estimator line&quot; (the horizontal line at 0.09 precision in your case). This would indicate a <b>good</b> detection of &quot;strong&quot; positive outcomes, but poor performance on the less clear candidates.", "dateLastCrawled": "2022-02-02T22:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>Area Under</b> an ROC <b>Curve</b>", "url": "http://gim.unmc.edu/dxtests/RoC3.htm", "isFamilyFriendly": true, "displayUrl": "gim.unmc.edu/dxtests/RoC3.htm", "snippet": "The <b>area under</b> the <b>curve</b> is the percentage of randomly drawn pairs for which this is true (that is, the test correctly classifies the two patients in the random pair). Computing the <b>area</b> is more difficult to explain and beyond the scope of this introductory material. Two methods are commonly used: a non-parametric method based on constructing trapeziods <b>under</b> the <b>curve</b> as an approximation of <b>area</b> and a parametric method using a maximum likelihood estimator to fit a smooth <b>curve</b> to the data ...", "dateLastCrawled": "2022-01-28T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "machine <b>learning</b> - Advantages of AUC vs standard <b>accuracy</b> - Data ...", "url": "https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/806", "snippet": "Really great question, and one that I find that most people don&#39;t really understand on an intuitive level. AUC is in fact often preferred over <b>accuracy</b> for binary classification for a number of different reasons. First though, let&#39;s talk about exactly what AUC is. Honestly, for being one of the most widely used efficacy metrics, it&#39;s surprisingly obtuse to figure out exactly how AUC works.. AUC stands for <b>Area</b> <b>Under</b> the <b>Curve</b>, which <b>curve</b> you ask?Well, that would be the ROC <b>curve</b>.ROC stands ...", "dateLastCrawled": "2022-01-27T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A system is taken from state A to <b>B thought three different paths 1</b>, 2 ...", "url": "https://www.toppr.com/ask/en-pk/question/a-system-is-taken-from-state-a-to-b-thought-three-different-paths-1-2/", "isFamilyFriendly": true, "displayUrl": "https://www.toppr.com/ask/en-pk/question/a-system-is-taken-from-state-a-to-b-<b>thought</b>...", "snippet": "The work done <b>can</b> be calculated as the work done <b>under</b> the P-V <b>curve</b>. The <b>area</b> <b>under</b> the P-V <b>curve</b> is maximum for process 1 and is minimum for process 3 . Therefore, work done is maximum for process 1 .", "dateLastCrawled": "2021-04-09T21:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the value of the <b>area</b> <b>under</b> the roc <b>curve</b> (AUC) to conclude ...", "url": "https://www.researchgate.net/post/What-is-the-value-of-the-area-under-the-roc-curve-AUC-to-conclude-that-a-classifier-is-excellent", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/What-is-the-value-of-the-<b>area</b>-<b>under</b>-the-roc-<b>curve</b>...", "snippet": "&quot;Accuracy is measured by the <b>area</b> <b>under</b> the ROC <b>curve</b>. An <b>area</b> of 1 represents a perfect test; an <b>area</b> of .5 represents a worthless test. A rough guide for classifying the accuracy of a diagnostic ...", "dateLastCrawled": "2022-02-01T16:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "machine <b>learning</b> - <b>Interpretation of the area under</b> <b>the PR</b> <b>curve</b> ...", "url": "https://stats.stackexchange.com/questions/99916/interpretation-of-the-area-under-the-pr-curve", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/.../99916/<b>interpretation-of-the-area-under</b>-<b>the-pr</b>-<b>curve</b>", "snippet": "<b>Area</b> <b>under</b> the ROC <b>curve</b> or <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b> for imbalanced data? 10. An intuitive meaning of the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>? 8. outlier detection: <b>area</b> <b>under</b> precision recall <b>curve</b>. 1. Machine <b>learning</b> model metrics vs predicted probability? 3. <b>Area</b> <b>Under</b> the Precision Recall <b>curve</b> -similar interpretation to AUROC? 1. How to improve the accuracy of an ARIMA model . 2. <b>Area</b> <b>under</b> the <b>curve</b> score training/validation set. 1. Why does the best model in the training set have the worst test ...", "dateLastCrawled": "2022-01-21T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Precision-Recall Curve | ML - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/precision-recall-curve-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/precision-recall-<b>curve</b>-ml", "snippet": "However, most machine <b>learning</b> algorithms often involve a trade-off between the two. A good <b>PR</b> <b>curve</b> has greater AUC (<b>area</b> <b>under</b> <b>curve</b>). In the figure above, the classifier corresponding to the blue line has better performance than the classifier corresponding to the green line. It is important to note that the classifier that has a higher AUC on the ROC <b>curve</b> will always have a higher AUC on <b>the PR</b> <b>curve</b> as well. Consider an algorithm that classifies whether or not a document belongs to the ...", "dateLastCrawled": "2022-02-02T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Complete Guide to Understanding Precision and Recall Curves", "url": "https://analyticsindiamag.com/complete-guide-to-understanding-precision-and-recall-curves/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/complete-guide-to-<b>under</b>standing-precision-and-recall-<b>curves</b>", "snippet": "The <b>Area</b> <b>under</b> <b>the PR</b> <b>Curve</b> is a metric that helps us compare 2 similar looking curves. Higher the AUC, the better the performance. The baseline of <b>PR</b> <b>Curve</b>. Fig 2. Fig 3. The Baseline of a <b>PR</b> <b>Curve</b> changes with class imbalance, unlike ROC <b>Curve</b>. This is due to the fact that Precision of a No-Skill model (which gives 0.5 score for every output ...", "dateLastCrawled": "2022-01-31T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "F1 <b>Score</b> vs ROC AUC vs Accuracy vs <b>PR</b> AUC: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-roc-auc-<b>pr</b>-auc", "snippet": "In order to get one number that tells us how good our <b>curve</b> is, we <b>can</b> calculate the <b>Area</b> <b>Under</b> the ROC <b>Curve</b>, or ROC AUC <b>score</b>. The more top-left your <b>curve</b> is the higher the <b>area</b> and hence higher ROC AUC <b>score</b>. Alternatively, it <b>can</b> be shown that ROC AUC <b>score</b> is equivalent to calculating the rank correlation between predictions and targets.", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "Instead, the <b>area</b> <b>under</b> the <b>curve</b> <b>can</b> be calculated to give a single score for a classifier model across all threshold values. This is called the ROC <b>area</b> <b>under</b> <b>curve</b> or ROC AUC or sometimes ROCAUC. The score is a value between 0.0 and 1.0 for a perfect classifier. AUCROC <b>can</b> be interpreted as the probability that the scores given by a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. \u2014 Page 54, <b>Learning</b> from Imbalanced Data Sets, 2018 ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Use ROC Curves and <b>Precision-Recall Curves for Classification</b> in ...", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "The curves of different models <b>can</b> <b>be compared</b> directly in general or for different thresholds. The <b>area</b> <b>under</b> the <b>curve</b> (AUC) <b>can</b> be used as a summary of the model skill. The shape of the <b>curve</b> contains a lot of information, including what we might care about most for a problem, the expected false positive rate, and the false negative rate. To make this clear: Smaller values on the x-axis of the plot indicate lower false positives and higher true negatives. Larger values on the y-axis of ...", "dateLastCrawled": "2022-02-03T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Classification: <b>ROC</b> <b>Curve</b> and AUC | Machine <b>Learning</b> Crash Course ...", "url": "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-<b>learning</b>/crash-course/classification/<b>roc</b>-and-auc", "snippet": "To compute the points in an <b>ROC</b> <b>curve</b>, we could evaluate a logistic regression model many times with different classification thresholds, but this would be inefficient. Fortunately, there&#39;s an efficient, sorting-based algorithm that <b>can</b> provide this information for us, called AUC. AUC: <b>Area</b> <b>Under</b> the <b>ROC</b> <b>Curve</b>. AUC stands for &quot;<b>Area</b> <b>under</b> the ...", "dateLastCrawled": "2022-02-02T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "machine <b>learning</b> - <b>ROC</b> vs precision-and-recall curves - Cross Validated", "url": "https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/7207", "snippet": "Since domination means &quot;at least as high&quot; at every point, the higher <b>curve</b> also has &quot;at least as high&quot; an <b>Area</b> <b>under</b> the <b>Curve</b> (AUC) as it includes also the <b>area</b> between the curves. The reverse is not true: if curves intersect, as opposed to touch, there is no dominance, but one AUC <b>can</b> still be bigger than the other.", "dateLastCrawled": "2022-01-27T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Area</b> <b>Under</b> Curves: <b>Simple Curves</b>, Definition, Calculation, Videos, Q&amp;As", "url": "https://www.toppr.com/guides/maths/application-of-integrals/area-under-simple-curves/", "isFamilyFriendly": true, "displayUrl": "https://www.toppr.com/guides/maths/application-of-integrals/<b>area-under-simple-curves</b>", "snippet": "In such cases, the <b>area</b> <b>under</b> a <b>curve</b> would be the one with respect to the y-axis. The figure given below would make things clear to you. You <b>can</b> see that here by constructing horizontal rectangular strips of length f(y 0) and breadth dy, one <b>can</b> derive another form of the formula for the <b>area</b> <b>under</b> a <b>curve</b>. $$ {A = \\int_{y = b}^{y = a} f(y)dy} $$", "dateLastCrawled": "2022-02-02T19:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine <b>learning</b> - Advantages of AUC vs standard <b>accuracy</b> - Data ...", "url": "https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/806", "snippet": "Really great question, and one that I find that most people don&#39;t really understand on an intuitive level. AUC is in fact often preferred over <b>accuracy</b> for binary classification for a number of different reasons. First though, let&#39;s talk about exactly what AUC is. Honestly, for being one of the most widely used efficacy metrics, it&#39;s surprisingly obtuse to figure out exactly how AUC works.. AUC stands for <b>Area</b> <b>Under</b> the <b>Curve</b>, which <b>curve</b> you ask?Well, that would be the ROC <b>curve</b>.ROC stands ...", "dateLastCrawled": "2022-01-27T15:14:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. See <b>PR</b> AUC (<b>Area</b> <b>under</b> <b>the PR</b> <b>Curve</b>). <b>area</b> <b>under</b> the ROC <b>curve</b> . See AUC (<b>Area</b> <b>under</b> the ROC <b>curve</b>). artificial general intelligence. A non-human mechanism that demonstrates a broad range of problem solving, creativity, and adaptability. For example, a program demonstrating artificial general intelligence could translate text, compose symphonies, and excel at games that have not yet been invented. artificial intelligence. A non-human program or model that can solve ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Learning</b> Curves in <b>Machine</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/247934703_Learning_Curves_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/247934703_<b>Learning</b>_<b>Curves</b>_in_<b>Machine</b>_<b>Learning</b>", "snippet": "The <b>area</b> <b>under</b> the receiver operating characteristic (ROC) <b>curve</b> (AUC) was 0.62 (95% confidence interval [CI]: 0.57, 0.68) and the <b>area</b> <b>under</b> the precision\u2010recall <b>curve</b> was 0.58. <b>Learning</b> <b>curve</b> ...", "dateLastCrawled": "2021-12-15T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding <b>AUC</b> - ROC <b>Curve</b> | by Sarang Narkhede | Towards Data Science", "url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>under</b>standing-<b>auc</b>-roc-<b>curve</b>-68b2303cc9c5", "snippet": "In <b>Machine</b> <b>Learning</b>, performance measurement is an essential task. So when it comes to a classification problem, we can count on an <b>AUC</b> - ROC <b>Curve</b>. When we need to check or visualize the performance of the multi-class classification problem, we use the <b>AUC</b> <b>Area</b> <b>Under</b> The <b>Curve</b>) ROC (Receiver Operating Characteristics) <b>curve</b>. It is one of the most important evaluation metrics for checking any classification model\u2019s performance. It is also written as AUROC (<b>Area</b> <b>Under</b> the Receiver Operating ...", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Differential and Integral Calculus - Differentiate with Respect to Anything", "url": "https://machinelearningmastery.com/differential-and-integral-calculus-differentiate-with-respect-to-anything/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/differential-and-integral-calculus-differentiate...", "snippet": "The Sweeping <b>Area</b> <b>Analogy</b>. Perhaps a simpler <b>analogy</b> to help us relate integration to differentiation, is to imagine holding one of the thinly cut slices and dragging it rightwards <b>under</b> the <b>curve</b> in infinitesimally small steps. As it moves rightwards, the thinly cut slice will sweep a larger <b>area</b> <b>under</b> the <b>curve</b>, while its height will change according to the shape of the <b>curve</b>. The question that we would like to answer is, at which rate does the <b>area</b> accumulate as the thin slice sweeps ...", "dateLastCrawled": "2022-01-28T21:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>What is</b> AUC - <b>ROC</b> in <b>Machine</b> <b>Learning</b> | Overview of <b>ROC</b>", "url": "https://www.mygreatlearning.com/blog/roc-curve/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/blog/<b>roc</b>-<b>curve</b>", "snippet": "By <b>analogy</b>, Higher the AUC, better the model is at distinguishing between patients with the disease and no disease. The <b>ROC</b> <b>curve</b> is plotted with TPR against the FPR where TPR is on the y-axis and FPR is on the x-axis. Defining terms used in AUC and <b>ROC</b> <b>Curve</b>. Consider a two-class prediction problem, in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is ...", "dateLastCrawled": "2022-01-30T19:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Applying <b>machine</b> <b>learning</b> algorithms to predict default probability in ...", "url": "https://www.sciencedirect.com/science/article/pii/S1057521921002878", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1057521921002878", "snippet": "The results show that, first, based on the AUC (<b>area</b> <b>under</b> the ROC <b>curve</b>) value, accuracy rate and Brier score, the <b>machine</b> <b>learning</b> models can accurately predict the default risk of online borrowers. Second, the integrated discrimination improvement (IDI) test results show that the prediction performance of the <b>machine</b> <b>learning</b> algorithms is significantly better than that of the logistic model. Third, after constructing the investor profit function with misclassification cost, we find that ...", "dateLastCrawled": "2022-01-27T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Driving skill analysis using <b>machine</b> <b>learning</b> The full <b>curve</b> and ...", "url": "https://www.researchgate.net/publication/261398439_Driving_skill_analysis_using_machine_learning_The_full_curve_and_curve_segmented_cases", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261398439_Driving_skill_analysis_using...", "snippet": "In the full <b>curve</b> driving scene, principal component analysis and a support vector <b>machine</b>-based method accurately classified drivers in 95.7 % of cases when using driving data about high- and low ...", "dateLastCrawled": "2022-01-07T09:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "AUC ROC <b>curve</b> - auc: <b>area</b> <b>under</b> the roc <b>curve</b>", "url": "https://haar-t.com/questions/25009284/how-to-plot-roc-curve-in-python5q3cww3348a8y8", "isFamilyFriendly": true, "displayUrl": "https://haar-t.com/questions/25009284/how-to-plot-roc-<b>curve</b>-in-python5q3cww3348a8y8", "snippet": "The <b>area</b> <b>under</b> the ROC <b>curve</b> is called as AUC -<b>Area</b> <b>Under</b> <b>Curve</b>. AUC ranges between 0 and 1 and is used for successful classification of the logistics model. This recipe demonstrates how to plot AUC ROC <b>curve</b> in R. In the following example, a &#39;**Healthcare case study**&#39; is taken, logistic regression had to be applied on a data set A receiver operating characteristic <b>curve</b>, or ROC <b>curve</b>, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its ...", "dateLastCrawled": "2022-01-25T23:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why is AUC (<b>Area</b> <b>under</b> <b>ROC) insensitive to class distribution changes</b> ...", "url": "https://www.quora.com/Why-is-AUC-Area-under-ROC-insensitive-to-class-distribution-changes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-AUC-<b>Area</b>-<b>under</b>-<b>ROC-insensitive-to-class-distribution-changes</b>", "snippet": "Answer (1 of 2): Well, the ROC <b>curve</b> is sensitive to the the class conditional likelihoods P(X|C_1) and P(X|C_2) because you are plotting P(miss) and P(FA) as a function of a decision threshold applied to the the likelihood ratio P(X|C_1)/P(X|C_2). The class prior probabilities P(C_1) and P(C_2)...", "dateLastCrawled": "2022-01-12T21:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Why is ROC insensitive to class distributions ...", "url": "https://stats.stackexchange.com/questions/545273/why-is-roc-insensitive-to-class-distributions", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/545273/why-is-roc-insensitive-to-class...", "snippet": "The only matter with ROC <b>Curve</b> is the percentage of FP compared to the percentage of TP, wether the model is balanced or not.--- Edit after comment question : This depends how you use AUC (<b>Area</b> <b>under</b> ROC <b>curve</b>, what you might call ROC metric). AUC measures the performance of 1 model on 1 set. So if you apply it on Train, it&#39;ll measure how your ...", "dateLastCrawled": "2022-01-29T03:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Performance Evaluation of Machine Learning Algorithms in Apache</b> ...", "url": "https://www.researchgate.net/publication/330478085_Performance_Evaluation_of_Machine_Learning_Algorithms_in_Apache_Spark_for_Intrusion_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/330478085_Performance_Evaluation_of_<b>Machine</b>...", "snippet": "The <b>area under the PR curve is like</b> the ROC. The . difference is that instead of it being a ratio bet ween the true and . false positive rates, it is a r atio between precision and t rue ...", "dateLastCrawled": "2021-11-04T12:46:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(area under the pr curve)  is like +(learning curve)", "+(area under the pr curve) is similar to +(learning curve)", "+(area under the pr curve) can be thought of as +(learning curve)", "+(area under the pr curve) can be compared to +(learning curve)", "machine learning +(area under the pr curve AND analogy)", "machine learning +(\"area under the pr curve is like\")", "machine learning +(\"area under the pr curve is similar\")", "machine learning +(\"just as area under the pr curve\")", "machine learning +(\"area under the pr curve can be thought of as\")", "machine learning +(\"area under the pr curve can be compared to\")"]}