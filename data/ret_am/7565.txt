{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Co-Training</b> for Deep Object Detection: Comparing Single-Modal and Multi ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8125436/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8125436", "snippet": "Indeed, multi-modal <b>co-training</b> is effective for self-labeling object BBs under <b>different</b> settings, namely, for standard SSL (no domain shift, a few human-labeled <b>data</b>) and when <b>using</b> virtual-world <b>data</b> (many virtual-world labeled <b>data</b>, but no human-labeled <b>data</b>) both under domain shift and after reducing it by GAN-based virtual-to-real image translation. The achieved improvement over the lower bound configurations is significant, allowing to be almost in pair with upper bound configurations ...", "dateLastCrawled": "2021-11-15T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Co-training</b> <b>Using Prosodic and Lexical Information</b> for Sentence ...", "url": "https://www.academia.edu/4804807/Co_training_Using_Prosodic_and_Lexical_Information_for_Sentence_Segmentation", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/4804807/<b>Co_training</b>_<b>Using_Prosodic_and_Lexical_Information</b>...", "snippet": "<b>Co-training</b> <b>algorithms</b> work by generating <b>using</b> self-training and <b>co-training</b> with the ICSI Meeting <b>two</b> <b>or more</b> classifiers trained on <b>different</b> views of the input Recorder Dialog Act (MRDA) corpus in Section 4. labeled <b>data</b> that are then used to label the unlabeled <b>data</b> separately. The most confidently labeled examples of the 2. Related Work automatically labeled <b>data</b> can then be added to the set of manually labeled <b>data</b>. The process may continue for several The <b>co-training</b> approach was ...", "dateLastCrawled": "2022-01-25T19:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Co-Training</b>-based Algorithm <b>Using</b> Con\ufb01dence Values to Select Instances", "url": "http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_WCCI_2020/IJCNN/Papers/N-20785.pdf", "isFamilyFriendly": true, "displayUrl": "vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_WCCI_2020/IJCNN/Papers/N...", "snippet": "Each dataset was trained <b>using</b> four <b>different</b> classi\ufb01-cation <b>algorithms</b> (Naive Bayes, Decision tree, Ripper and k-NN) as basis for the <b>co-training</b> training procedure. The obtained results are promising and they indicate that, in most cases, the proposed method performs better than the <b>co-training</b> method originally proposed in the literature. Index Terms\u2014<b>Data</b> classi\ufb01cation, Semi-supervised learning, <b>Co-training</b> algorithm. I. INTRODUCTION Machine learning is a research \ufb01eld that aims ...", "dateLastCrawled": "2021-08-05T21:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Co-Training</b> for Domain Adaptation", "url": "https://www.cs.cornell.edu/~kilian/papers/coda_nips2011.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/~kilian/papers/coda_nips2011.pdf", "snippet": "So far, we have not addressed that the <b>two</b> <b>data</b> sets Uand Lare not sampled from the same dis-tribution. In domain adaptation, the training <b>data</b> is no longer representative of the test <b>data</b>. <b>More</b> explicitly, P S(YjX= x) is <b>different</b> from P T(YjX= x). For illustration, consider the sentiment", "dateLastCrawled": "2022-01-31T15:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>co-training algorithm for multi-view data with applications</b> in <b>data</b> ...", "url": "https://www.researchgate.net/publication/45449401_A_co-training_algorithm_for_multi-view_data_with_applications_in_data_fusion", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/45449401_A_<b>co-training</b>_algorithm_for_multi...", "snippet": "The methods are compared <b>using</b> <b>two</b> real <b>data</b> sets and <b>using</b> simulated <b>data</b>. The results show that the incorporation of external information from spectroscopic measurements gives <b>more</b> information ...", "dateLastCrawled": "2022-01-11T21:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) A <b>review of research on co\u2010training</b>", "url": "https://www.researchgate.net/publication/350194568_A_review_of_research_on_co-training", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350194568_A_review_of_research_on_<b>co-training</b>", "snippet": "T ak e <b>co-training</b> under <b>two</b> views as an example, and the standard algorithm is shown in Figure 4: Given the labeled <b>data</b> x i ( i = 1, 2, 3, \u2026 ) and the unlabeled <b>data</b> x j ( j = 1, 2, 3, \u2026", "dateLastCrawled": "2022-02-03T12:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Multi-view <b>Co-training</b> for microRNA Prediction | Scientific Reports", "url": "https://www.nature.com/articles/s41598-019-47399-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-019-47399-8", "snippet": "Comparing <b>co-training</b> with benchmark <b>algorithms</b>. Multi-view <b>co-training</b> was compared with <b>two</b> benchmark <b>algorithms</b> (see Methods): 1) passive learning, where samples are randomly added to the ...", "dateLastCrawled": "2021-08-30T23:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Self-Paced</b> <b>Co-training</b>", "url": "http://proceedings.mlr.press/v70/ma17b/ma17b.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v70/ma17b/ma17b.pdf", "snippet": "<b>two</b> views in <b>co-training</b> implementation instead of the par-allel mode commonly adopted by previous methods. Under such amelioration, the new method can be proved to still guarantee the theoretical effectiveness under -expansion assumption in the traditional <b>co-training</b> theory, while <b>more</b> importantly, such series implementation exactly complies with the alternative optimization algorithm on solving a concise optimization model. Besides, it is substantiated that the new method can attain ...", "dateLastCrawled": "2022-02-03T16:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Multi-view</b> Clustering <b>Algorithms</b>: A Survey", "url": "https://ukdiss.com/examples/multi-view-clustering-algorithms-survey.php", "isFamilyFriendly": true, "displayUrl": "https://ukdiss.com/examples/<b>multi-view</b>-clustering-<b>algorithms</b>-survey.php", "snippet": "<b>Co-training</b> algorithm trains alternately to maximize the mutual consistency on <b>two</b> distinct views of the unlabeled <b>data</b> by <b>using</b> the learning or providing labeled <b>data</b> from one another. In terms of clustering, De Sa [ 19 ] pioneered a <b>two</b>-view spectral clustering algorithm, which creates a bipartite graph based on the minimizing- disagreement (the same concept to maximizing-consistency) idea.", "dateLastCrawled": "2021-12-08T21:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Do you know any simple criterion to split features into <b>two</b> views as in ...", "url": "https://www.quora.com/Do-you-know-any-simple-criterion-to-split-features-into-two-views-as-in-co-training", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Do-you-know-any-simple-criterion-to-split-features-into-<b>two</b>...", "snippet": "Answer: This is an extremely nontrivial task. One can do plenty of research and related work on it. If I&#39;d be doing it, I would try the following things, on the order of difficulty and excitement: 1) Compute how some features correlate with others. First, pick a <b>data</b> set and rank-order the va...", "dateLastCrawled": "2022-01-14T14:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Co-Training</b> for Deep Object Detection: Comparing Single-Modal and Multi ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8125436/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8125436", "snippet": "Indeed, multi-modal <b>co-training</b> is effective for self-labeling object BBs under <b>different</b> settings, namely, for standard SSL (no domain shift, a few human-labeled <b>data</b>) and when <b>using</b> virtual-world <b>data</b> (many virtual-world labeled <b>data</b>, but no human-labeled <b>data</b>) both under domain shift and after reducing it by GAN-based virtual-to-real image translation. The achieved improvement over the lower bound configurations is significant, allowing to be almost in pair with upper bound configurations ...", "dateLastCrawled": "2021-11-15T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Bayesian <b>Co-Training</b>", "url": "https://jmlr.csail.mit.edu/papers/volume12/yu11a/yu11a.pdf", "isFamilyFriendly": true, "displayUrl": "https://jmlr.csail.mit.edu/papers/volume12/yu11a/yu11a.pdf", "snippet": "<b>Co-training</b> (<b>or more</b> generally, co-regularization) has been a popular algorithm for semi-supervised learning in <b>data</b> with <b>two</b> feature representations (or views), but the fundamental assumptions un-derlying this type of models are still unclear. In this paper we propose a Bayesian undirected graphical model for <b>co-training</b>, <b>or more</b> generally for semi-supervised multi-view learning. This makes explicit the previously unstated assumptions of a large class of <b>co-training</b> type <b>algorithms</b>, and ...", "dateLastCrawled": "2022-02-01T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Analyzing Co-training Style Algorithms</b> | Request PDF", "url": "https://www.researchgate.net/publication/221112492_Analyzing_Co-training_Style_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221112492_<b>Analyzing_Co-training_Style_Algorithms</b>", "snippet": "However, it has been proven that <b>co-training</b> does not require <b>data</b> to have <b>two</b> <b>or more</b> <b>different</b> views, as long as the models are <b>different</b> [56, 57] . Basic <b>co-training</b> <b>algorithms</b> have <b>two</b> models ...", "dateLastCrawled": "2022-01-28T13:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Co-Training</b> for Domain Adaptation", "url": "https://www.cs.cornell.edu/~kilian/papers/coda_nips2011.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/~kilian/papers/coda_nips2011.pdf", "snippet": "very <b>different</b> vocabulary to describe <b>similar</b> concepts. For example, in our experiments we use the sentiment <b>data</b> of Blitzer et al. [4], where a breeze to use is a way to express positive sentiment about kitchen appliances, but not about books. In this situation, most domain adaptation <b>algorithms</b> seek to eliminate the difference between source and target distributions, either by re-weighting source instances [15, 19] or learning a new feature representation [6, 29]. We present an algorithm ...", "dateLastCrawled": "2022-01-31T15:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Multi-view <b>Co-training</b> for microRNA Prediction | Scientific Reports", "url": "https://www.nature.com/articles/s41598-019-47399-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-019-47399-8", "snippet": "Comparing <b>co-training</b> with benchmark <b>algorithms</b>. Multi-view <b>co-training</b> was compared with <b>two</b> benchmark <b>algorithms</b> (see Methods): 1) passive learning, where samples are randomly added to the ...", "dateLastCrawled": "2021-08-30T23:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Improving <b>data</b> and model quality in crowdsourcing <b>using</b> <b>co-training</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0020025521011415", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0020025521011415", "snippet": "Finally, CTNC employs a <b>co-training</b> framework to train <b>two</b> classifiers to relabel the instances <b>using</b> a consensus voting strategy. The comprehensive experiment results on both simulated and real-world datasets indicate that our proposed CTNC algorithm outperforms all compared state-of-the-art label noise correction <b>algorithms</b> in terms of the noise ratio and model quality. In CTNC, only <b>two</b> attribute views are used, and we plan to apply <b>more</b> views in a future study.", "dateLastCrawled": "2022-01-25T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Multi-view</b> Clustering <b>Algorithms</b>: A Survey", "url": "https://ukdiss.com/examples/multi-view-clustering-algorithms-survey.php", "isFamilyFriendly": true, "displayUrl": "https://ukdiss.com/examples/<b>multi-view</b>-clustering-<b>algorithms</b>-survey.php", "snippet": "<b>Co-training</b> algorithm trains alternately to maximize the mutual consistency on <b>two</b> distinct views of the unlabeled <b>data</b> by <b>using</b> the learning or providing labeled <b>data</b> from one another. In terms of clustering, De Sa [ 19 ] pioneered a <b>two</b>-view spectral clustering algorithm, which creates a bipartite graph based on the minimizing- disagreement (the same concept to maximizing-consistency) idea.", "dateLastCrawled": "2021-12-08T21:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Multiple Feature Fusion Based on <b>Co-Training</b> Approach and Time ...", "url": "https://www.hindawi.com/journals/am/2013/175064/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/am/2013/175064", "snippet": "The standard <b>Co-Training</b> is an algorithm that iteratively trains <b>two</b> classifiers on <b>two</b> view <b>data</b> by feeding the highest confidence score estimates from the testing set in another view classifier. In this semisupervised approach, the discriminatory power of each classifier is improved by another classifier\u2019s complementary knowledge. The testing set is gradually labeled round by round <b>using</b> only the highest confidence estimates. The pseudocode is presented in Algorithm", "dateLastCrawled": "2022-01-23T00:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Reinforced Co-Training</b> | DeepAI", "url": "https://deepai.org/publication/reinforced-co-training", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>reinforced-co-training</b>", "snippet": "04/17/18 - <b>Co-training</b> is a popular semi-supervised learning framework to utilize a large amount of unlabeled <b>data</b> in addition to a small lab...", "dateLastCrawled": "2022-01-11T16:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Enhanced multi\u2010dataset transfer learning method for unsupervised person ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-cvi.2018.5103", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-cvi.2018.5103", "snippet": "This is due to the fact that the <b>co-training</b> strategy will make both models <b>more</b> and <b>more</b> <b>similar</b> to each other as the increase of training iterations []. This is also the reason why PUCL models initialised on <b>different</b> source datasets have closer performances than PUL models. When the performances of models converge, the <b>two</b> models will select <b>similar</b> samples and predict <b>similar</b> labels on these samples, which makes the models not to benefit <b>more</b> from the training with the soft labelled <b>data</b> ...", "dateLastCrawled": "2022-01-13T13:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Analyzing Co-training Style Algorithms</b> | Request PDF", "url": "https://www.researchgate.net/publication/221112492_Analyzing_Co-training_Style_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221112492_<b>Analyzing_Co-training_Style_Algorithms</b>", "snippet": "However, it has been proven that <b>co-training</b> does not require <b>data</b> to have <b>two</b> <b>or more</b> <b>different</b> views, as long as the models are <b>different</b> [56, 57] . Basic <b>co-training</b> <b>algorithms</b> have <b>two</b> models ...", "dateLastCrawled": "2022-01-28T13:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Vertical Ensemble Co-Training for Text Classification</b> | Request PDF", "url": "https://www.researchgate.net/publication/320640010_Vertical_Ensemble_Co-Training_for_Text_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/320640010_<b>Vertical_Ensemble_Co-Training_for</b>...", "snippet": "Generally, <b>co-training</b> <b>algorithms</b> work by <b>using</b> <b>two</b> classifiers, trained on <b>two</b> <b>different</b> views of the <b>data</b>, to label large amounts of unlabeled <b>data</b>. Doing so <b>can</b> help minimize the human effort ...", "dateLastCrawled": "2022-01-03T23:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Combining Labeled and Unlabeled Data with</b> <b>Co-Training</b>", "url": "https://www.researchgate.net/publication/2457211_Combining_Labeled_and_Unlabeled_Data_with_Co-Training", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2457211_<b>Combining_Labeled_and_Unlabeled_Data</b>...", "snippet": "The <b>co-training</b> algorithm, initially proposed by [13], is similar to self-training, given that it increases the set of labeled <b>data</b> by iteratively classifying the set of unlabeled <b>data</b> and moving ...", "dateLastCrawled": "2022-01-29T16:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Uncertainty-aware multi-view <b>co-training</b> for semi-supervised medical ...", "url": "https://www.sciencedirect.com/science/article/pii/S1361841520301304", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1361841520301304", "snippet": "The <b>co-training</b> assumption encourages models to make similar predictions on both S and U, which potentially <b>can</b> lead to collapsed neural networks mentioned in Qiao et al. (2018), a phenomenon that results in a sudden and significant drop in validation accuracy during training of <b>co-training</b> <b>algorithms</b>. In our multi-view settings, this could also happen when the models from <b>different</b> views only <b>learn</b> the permutation or rotation of the kernels, resulting in exactly the same learned feature ...", "dateLastCrawled": "2022-01-27T16:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Democratic co-learning</b>", "url": "https://www.researchgate.net/publication/4114558_Democratic_co-learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/4114558_<b>Democratic_co-learning</b>", "snippet": "<b>Co-training</b> [Blum and Mitchell, 1998, Nigam and Ghani, 2000 exploit <b>two</b> (<b>or more</b>) views of the <b>data</b>, i.e., <b>different</b> feature sets representing the same <b>data</b> in order to let models produce labeled ...", "dateLastCrawled": "2021-11-15T16:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Uncertainty-aware multi-view <b>co-training</b> for semi-supervised medical ...", "url": "https://deepai.org/publication/uncertainty-aware-multi-view-co-training-for-semi-supervised-medical-image-segmentation-and-domain-adaptation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/uncertainty-aware-multi-view-<b>co-training</b>-for-semi...", "snippet": "The <b>co-training</b> assumption encourages models to make similar predictions on both S and U, which potentially <b>can</b> lead to collapsed neural networks mentioned in , a phenomenon that results in a sudden and significant drop in validation accuracy during training of <b>co-training</b> <b>algorithms</b>. In our multi-view settings, this could also happen when the models from <b>different</b> views only <b>learn</b> the permutation or rotation of the kernels, resulting in exactly the same learned feature representation ...", "dateLastCrawled": "2021-12-25T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Using Weighted Nearest Neighbor to Benefit from Unlabeled</b> <b>Data</b> ...", "url": "https://www.academia.edu/14700574/Using_Weighted_Nearest_Neighbor_to_Benefit_from_Unlabeled_Data", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/14700574/<b>Using_Weighted_Nearest_Neighbor_to_Benefit</b>_from...", "snippet": "Current research in semi-supervised learning <b>using</b> <b>algorithms</b> such as <b>Co-Training</b> [2] <b>or more</b> recent approaches based on graph representations [3] confirms that this is indeed possible. Most of the semi-supervised learning approaches use the labeled and unla- beled <b>data</b> <b>simultaneously</b> or at least in close collaboration. Roughly speaking, the unlabeled <b>data</b> provides information about the structure of the domain, i.e. it helps to capture the underlying distribution of the <b>data</b>. The challenge ...", "dateLastCrawled": "2022-01-13T04:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>Survey on Multi-view Learning</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1304.5634/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1304.5634", "snippet": "Wang and Zhou studied why <b>co-training</b> style <b>algorithms</b> <b>can</b> succeed when there are no redundant views. They used <b>different</b> configurations of the same base learner, which <b>can</b> be seen as another kind of view, to describe the <b>data</b> in <b>different</b> approaches, and showed that when the diversity between the <b>two</b> learners is greater than the amount of errors, the performance of the learners <b>can</b> be improved by <b>co-training</b> style <b>algorithms</b>. The <b>two</b> classifiers which have <b>different</b> biases will label some ...", "dateLastCrawled": "2022-01-17T23:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Social relationship prediction across networks using</b> tri-training BP ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231220302472", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231220302472", "snippet": "Inspired by <b>co-training</b> and tri-training, Saito et al. ... Three neural networks <b>learn</b> from <b>different</b> views during training process. As described in Fig. 2, the neural network N 1, N 2 obtain <b>different</b> classifiers h 1, h 2 by training labeled samples in source network and pseudo-labels samples in target network. Then the both classifiers h 1, h 2 will give labels for the i th sample with y i 1 or y i 2 in the target network respectively, these labels are named pseudo-label. One of the above ...", "dateLastCrawled": "2021-12-27T10:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The <b>blind men and the elephant</b>: on meeting the problem of multiple ...", "url": "https://link.springer.com/article/10.1007/s10994-013-5334-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-013-5334-y", "snippet": "In this position paper, we discuss how <b>different</b> branches of research on clustering and pattern mining, while rather <b>different</b> at first glance, in fact have a lot in common and <b>can</b> <b>learn</b> a lot from each other\u2019s solutions and approaches. We give brief introductions to the fundamental problems of <b>different</b> sub-fields of clustering, especially focusing on subspace clustering, ensemble clustering, alternative (as a variant of constraint) clustering, and multiview clustering (as a variant of ...", "dateLastCrawled": "2022-01-26T14:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Co-Training</b> for Deep Object Detection: Comparing Single-Modal and Multi ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8125436/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8125436", "snippet": "Indeed, multi-modal <b>co-training</b> is effective for self-labeling object BBs under <b>different</b> settings, namely, for standard SSL (no domain shift, a few human-labeled <b>data</b>) and when <b>using</b> virtual-world <b>data</b> (many virtual-world labeled <b>data</b>, but no human-labeled <b>data</b>) both under domain shift and after reducing it by GAN-based virtual-to-real image translation. The achieved improvement over the lower bound configurations is significant, allowing to be almost in pair with upper bound configurations ...", "dateLastCrawled": "2021-11-15T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Co-training</b> <b>Using Prosodic and Lexical Information</b> for Sentence ...", "url": "https://www.academia.edu/4804807/Co_training_Using_Prosodic_and_Lexical_Information_for_Sentence_Segmentation", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/4804807/<b>Co_training</b>_<b>Using_Prosodic_and_Lexical_Information</b>...", "snippet": "<b>Co-training</b> <b>algorithms</b> work by generating <b>using</b> self-training and <b>co-training</b> with the ICSI Meeting <b>two</b> <b>or more</b> classifiers trained on <b>different</b> views of the input Recorder Dialog Act (MRDA) corpus in Section 4. labeled <b>data</b> that are then used to label the unlabeled <b>data</b> separately. The most confidently labeled examples of the 2. Related Work automatically labeled <b>data</b> <b>can</b> then be added to the set of manually labeled <b>data</b>. The process may continue for several The <b>co-training</b> approach was ...", "dateLastCrawled": "2022-01-25T19:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Analyzing Co-training Style Algorithms</b> | Request PDF", "url": "https://www.researchgate.net/publication/221112492_Analyzing_Co-training_Style_Algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221112492_<b>Analyzing_Co-training_Style_Algorithms</b>", "snippet": "However, it has been proven that <b>co-training</b> does not require <b>data</b> to have <b>two</b> <b>or more</b> <b>different</b> views, as long as the models are <b>different</b> [56, 57] . Basic <b>co-training</b> <b>algorithms</b> have <b>two</b> models ...", "dateLastCrawled": "2022-01-28T13:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A <b>Co-Training</b>-based Algorithm <b>Using</b> Con\ufb01dence Values to Select Instances", "url": "http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_WCCI_2020/IJCNN/Papers/N-20785.pdf", "isFamilyFriendly": true, "displayUrl": "vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_WCCI_2020/IJCNN/Papers/N...", "snippet": "Each dataset was trained <b>using</b> four <b>different</b> classi\ufb01-cation <b>algorithms</b> (Naive Bayes, Decision tree, Ripper and k-NN) as basis for the <b>co-training</b> training procedure. The obtained results are promising and they indicate that, in most cases, the proposed method performs better than the <b>co-training</b> method originally proposed in the literature. Index Terms\u2014<b>Data</b> classi\ufb01cation, Semi-supervised learning, <b>Co-training</b> algorithm. I. INTRODUCTION Machine learning is a research \ufb01eld that aims ...", "dateLastCrawled": "2021-08-05T21:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Multi-view <b>Co-training</b> for microRNA Prediction | Scientific Reports", "url": "https://www.nature.com/articles/s41598-019-47399-8", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41598-019-47399-8", "snippet": "As <b>can</b> be observed from Fig. 1, we used a slightly <b>different</b> approach <b>compared</b> to the standard <b>co-training</b> approach originally proposed by Blum and Mitchell 19.In most <b>co-training</b> models, a pool ...", "dateLastCrawled": "2021-08-30T23:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Multiple Feature Fusion Based on <b>Co-Training</b> Approach and Time ...", "url": "https://www.hindawi.com/journals/am/2013/175064/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/am/2013/175064", "snippet": "The standard <b>Co-Training</b> is an algorithm that iteratively trains <b>two</b> classifiers on <b>two</b> view <b>data</b> by feeding the highest confidence score estimates from the testing set in another view classifier. In this semisupervised approach, the discriminatory power of each classifier is improved by another classifier\u2019s complementary knowledge. The testing set is gradually labeled round by round <b>using</b> only the highest confidence estimates. The pseudocode is presented in Algorithm", "dateLastCrawled": "2022-01-23T00:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Multi-view</b> Clustering <b>Algorithms</b>: A Survey", "url": "https://ukdiss.com/examples/multi-view-clustering-algorithms-survey.php", "isFamilyFriendly": true, "displayUrl": "https://ukdiss.com/examples/<b>multi-view</b>-clustering-<b>algorithms</b>-survey.php", "snippet": "<b>Co-training</b> algorithm trains alternately to maximize the mutual consistency on <b>two</b> distinct views of the unlabeled <b>data</b> by <b>using</b> the learning or providing labeled <b>data</b> from one another. In terms of clustering, De Sa [ 19 ] pioneered a <b>two</b>-view spectral clustering algorithm, which creates a bipartite graph based on the minimizing- disagreement (the same concept to maximizing-consistency) idea.", "dateLastCrawled": "2021-12-08T21:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Improving <b>data</b> and model quality in crowdsourcing <b>using</b> <b>co-training</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0020025521011415", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0020025521011415", "snippet": "Finally, CTNC employs a <b>co-training</b> framework to train <b>two</b> classifiers to relabel the instances <b>using</b> a consensus voting strategy. The comprehensive experiment results on both simulated and real-world datasets indicate that our proposed CTNC algorithm outperforms all <b>compared</b> state-of-the-art label noise correction <b>algorithms</b> in terms of the noise ratio and model quality. In CTNC, only <b>two</b> attribute views are used, and we plan to apply <b>more</b> views in a future study.", "dateLastCrawled": "2022-01-25T16:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "MiCo: Mixup <b>Co-Training</b> <b>for Semi-Supervised Domain Adaptation</b> | DeepAI", "url": "https://deepai.org/publication/mico-mixup-co-training-for-semi-supervised-domain-adaptation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/mico-mixup-<b>co-training</b>-for-semi-supervised-domain...", "snippet": "<b>Co-training</b> works, because the <b>two</b> classifiers are trained on <b>different</b> views and therefore classify <b>different</b> samples correctly. We show that by de-composing SSDA settings into an SSL and UDA problem, we also obtain <b>two</b> classifiers that, although trained in the same feature space, differ sufficiently in predictions that they qualify for <b>co-training</b> to work well. We also successfully incorporate self-training by incorporating the pseudo-labels of one classifier into its own training set.", "dateLastCrawled": "2022-01-12T04:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Do you know any simple criterion to split features into <b>two</b> views as in ...", "url": "https://www.quora.com/Do-you-know-any-simple-criterion-to-split-features-into-two-views-as-in-co-training", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Do-you-know-any-simple-criterion-to-split-features-into-<b>two</b>...", "snippet": "Answer: This is an extremely nontrivial task. One <b>can</b> do plenty of research and related work on it. If I&#39;d be doing it, I would try the following things, on the order of difficulty and excitement: 1) Compute how some features correlate with others. First, pick a <b>data</b> set and rank-order the va...", "dateLastCrawled": "2022-01-14T14:58:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Supervised Learning and Co-training</b> | Request PDF", "url": "https://www.researchgate.net/publication/268809884_Supervised_Learning_and_Co-training", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/268809884_<b>Supervised_Learning_and_Co-training</b>", "snippet": "\u2022 <b>Co-Training</b> [2]: It is a <b>machine</b> <b>learning</b> algorithm used when there are only some labeled data and large amounts of unlabeled data. One of its uses is in text mining for search engines. ...", "dateLastCrawled": "2021-10-24T01:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Self-Supervised Graph <b>Co-Training</b> for Session-based Recommendation | DeepAI", "url": "https://deepai.org/publication/self-supervised-graph-co-training-for-session-based-recommendation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/self-supervised-graph-<b>co-training</b>-for-session-based...", "snippet": "<b>Co-Training</b> is a classical semi-supervised <b>learning</b> paradigm to exploit unlabeled data (Blum and Mitchell, 1998; Da Costa et al., 2018; Han et al., 2020). Under this regime, two classifiers are separately trained on two views and then exchange confident pseudo labels of unlabeled instances to construct additional labeled training data for each other. Typically, the two views are two disjoint sets of features and can provide complementary information to each other. Blum", "dateLastCrawled": "2022-02-01T08:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Cooperative <b>Learning</b> of Energy-Based Model and Latent Variable Model ...", "url": "http://www.stat.ucla.edu/~ywu/CoopNets/doc/CoopNets_AAAI.pdf", "isFamilyFriendly": true, "displayUrl": "www.stat.ucla.edu/~ywu/CoopNets/doc/CoopNets_AAAI.pdf", "snippet": "3Amazon RSML (Retail System <b>Machine</b> <b>Learning</b>) Group Abstract This paper proposes a cooperative <b>learning</b> algorithm to train both the undirected energy-based model and the directed latent variable model jointly. The <b>learning</b> algorithm interweaves the maximum likelihood algorithms for <b>learning</b> the two models, and each iteration consists of the following two steps: (1) Modi\ufb01ed contrastive divergence for energy-based model: The <b>learning</b> of the energy-based model is based on the contrastive ...", "dateLastCrawled": "2022-02-03T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Classification Algorithm</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/classification-algorithm", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>classification-algorithm</b>", "snippet": "Two of the most popular methods for semi-supervised <b>learning</b> are <b>Co-Training</b> (Blum and Mitchell, 1998) and Semi-Supervised Support Vector Machines (S3VM) (Sindhwani and Keerthi, 2006). <b>Co-Training</b> assumes the presence of multiple views for each feature and uses the confident samples in one view to update the other. However, in applications such as image classification, one often has just a single feature vector and hence it is difficult to apply <b>Co-Training</b>. Semi-supervised support vector ...", "dateLastCrawled": "2022-01-18T09:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interacting meaningfully with machine learning systems</b>: Three ...", "url": "https://dl.acm.org/doi/10.1016/j.ijhcs.2009.03.004", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1016/j.ijhcs.2009.03.004", "snippet": "Although <b>machine</b> <b>learning</b> is becoming commonly used in today&#39;s software, there has been little research into how end users might interact with <b>machine</b> <b>learning</b> systems, beyond communicating simple &#39;&#39;right/wrong&#39;&#39; judgments. If the users themselves could work hand-in-hand with <b>machine</b> <b>learning</b> systems, the users&#39; understanding and trust of the system could improve and the accuracy of <b>learning</b> systems could be improved as well. We conducted three experiments to understand the potential for ...", "dateLastCrawled": "2022-01-28T03:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) CoSpa: A <b>Co-training</b> <b>Approach for Spam Review Identification</b> with ...", "url": "https://www.researchgate.net/publication/297724912_CoSpa_A_Co-training_Approach_for_Spam_Review_Identification_with_Support_Vector_Machine", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/297724912_CoSpa_A_<b>Co-training</b>_Approach_for...", "snippet": "<b>Co-training</b> is one of the semi-supervised techniques, \ufb01rst proposed by Blum and Mitchell 15 and has recently been extended into three categories: <b>co-training</b> with multiple views, <b>co-training</b> with", "dateLastCrawled": "2021-12-22T23:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>VICTORIA&#39;s MACHINE LEARNING NOTES</b> - Persagen Consulting", "url": "https://persagen.com/files/ml.html", "isFamilyFriendly": true, "displayUrl": "https://persagen.com/files/ml.html", "snippet": "<b>Machine</b> <b>learning</b> (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration ...", "dateLastCrawled": "2022-02-01T17:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Enhancement to <b>Selective Incremental Approach for Transductive</b> ...", "url": "https://www.grin.com/document/205436", "isFamilyFriendly": true, "displayUrl": "https://www.grin.com/document/205436", "snippet": "2.5 <b>Co-Training</b> and Multi view <b>Learning</b> 2.5.1 <b>Co-Training</b>. <b>Co-training</b> (Blum &amp; Mitchell, 1998) (Mitchell, 1999) assumes that (i) features can be split into two sets; (ii) each sub-feature set is sufficient to train a good classifier; (iii) the two sets are conditionally independent given the class. Initially two separate classifiers are trained ...", "dateLastCrawled": "2020-05-16T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>100 Must-Read</b> NLP Papers | This is a list of 100 important natural ...", "url": "http://masatohagiwara.net/100-nlp-papers/", "isFamilyFriendly": true, "displayUrl": "masatohagiwara.net/100-nlp-papers", "snippet": "<b>Machine</b> <b>Learning</b>. Avrim Blum and Tom Mitchell: Combining Labeled and Unlabeled Data with <b>Co-Training</b>, 1998. John Lafferty, Andrew McCallum, Fernando C.N. Pereira: Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data, ICML 2001. Charles Sutton, Andrew McCallum. An Introduction to Conditional Random Fields for Relational <b>Learning</b>. Kamal Nigam, et al.: Text Classification from Labeled and Unlabeled Documents using EM. <b>Machine</b> <b>Learning</b>, 1999. Kevin Knight ...", "dateLastCrawled": "2022-01-31T22:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Hui&#39;s Homepage", "url": "https://layneins.github.io/", "isFamilyFriendly": true, "displayUrl": "https://layneins.github.io", "snippet": "My main research interests include natural language processing, text mining and <b>machine</b> <b>learning</b>. News [2021.12] ... Unsupervised Conversation Disentanglement through <b>Co-Training</b> Hui Liu, Zhan Shi, Xiaodan Zhu EMNLP 2021 main conference, long paper Retrieval, <b>Analogy</b>, and Composition: A framework for Compositional Generalization in Image Captioning Zhan Shi, Hui Liu, Martin Renqiang Min, Christopher Malon, Li Erran Li and Xiaodan Zhu Findings of EMNLP 2021, long paper Enhancing Descriptive ...", "dateLastCrawled": "2022-02-02T14:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Deep <b>Learning</b> for Sensor-based Human Activity Recognition ...", "url": "https://www.researchgate.net/publication/338737352_Deep_Learning_for_Sensor-based_Human_Activity_Recognition_Overview_Challenges_and_Opportunities", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338737352_Deep_<b>Learning</b>_for_Sensor-based...", "snippet": "Many <b>machine</b> <b>learning</b> methods have been employed in human activity recognition. However ... The process of <b>co-training is like</b> the process of human <b>learning</b>. People can learn new knowledge. from ...", "dateLastCrawled": "2022-01-09T11:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) A <b>literature survey of active machine learning</b> in the context of ...", "url": "https://www.researchgate.net/publication/228682097_A_literature_survey_of_active_machine_learning_in_the_context_of_natural_language_processing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228682097_A_literature_survey_of_active...", "snippet": "Active <b>machine</b> <b>learning</b> is a supervised <b>learning</b> method in which the learner. is in control of the data from which it learns. That control is used by. the learner to ask an oracle, a teacher ...", "dateLastCrawled": "2022-02-01T01:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep <b>Learning</b> for Sensor-based Human Activity Recognition: Overview ...", "url": "https://deepai.org/publication/deep-learning-for-sensor-based-human-activity-recognition-overview-challenges-and-opportunities", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/deep-<b>learning</b>-for-sensor-based-human-activity...", "snippet": "Transfer <b>learning</b> is a common <b>machine</b> <b>learning</b> technique that transfers the classification ability of the <b>learning</b> model from one predefined setting to a dynamic setting. Transfer <b>learning</b> is particularly effective in solving heterogeneity problems. It avoids the decline in the performance of <b>learning</b> models when the training data and the test data follow different distributions. In the activity recognition context, this problem appears when activity recognition models are deployed for ...", "dateLastCrawled": "2022-01-11T03:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Instance labeling in semi-supervised <b>learning</b> with meaning values of ...", "url": "https://www.sciencedirect.com/science/article/pii/S0952197617300672", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0952197617300672", "snippet": "In <b>machine</b> <b>learning</b> applications, especially in the field of text classification there are two conventional strategies; supervised <b>learning</b> and unsupervised <b>learning</b>. A sufficient amount of labeled data is required as training corpus to build the classifier in conventional supervised classification methods, which will be helpful to guess the class labels of the unlabeled instances. Conversely, unsupervised <b>learning</b>, only depends on unlabeled instances, and doesn\u2019t require class labels to ...", "dateLastCrawled": "2022-01-11T19:09:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(co-training)  is like +(using two or more different algorithms to learn from data simultaneously)", "+(co-training) is similar to +(using two or more different algorithms to learn from data simultaneously)", "+(co-training) can be thought of as +(using two or more different algorithms to learn from data simultaneously)", "+(co-training) can be compared to +(using two or more different algorithms to learn from data simultaneously)", "machine learning +(co-training AND analogy)", "machine learning +(\"co-training is like\")", "machine learning +(\"co-training is similar\")", "machine learning +(\"just as co-training\")", "machine learning +(\"co-training can be thought of as\")", "machine learning +(\"co-training can be compared to\")"]}