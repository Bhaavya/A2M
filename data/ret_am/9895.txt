{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Distributed</b> Deep <b>Learning</b>: Concepts, Methods ... - <b>federated</b>-ml.org", "url": "http://www.federated-ml.org/tutorials/globecom2020/part4.pdf", "isFamilyFriendly": true, "displayUrl": "www.<b>federated</b>-ml.org/tutorials/globecom2020/part4.pdf", "snippet": "2. <b>Distributed</b> &amp; <b>Federated</b> <b>Learning</b>: Concepts &amp; Methods (WS) 3. <b>Distributed</b> &amp; <b>Federated</b> <b>Learning</b> in Wireless Networks (DG) 4. <b>Distributed</b> <b>Learning</b> &amp; <b>Neural</b> <b>Network</b> Compression (WS) Deniz Gunduz (Fraunhofer HHI) IEEE GLOBECOM 2020 Tutorial on <b>Distributed</b> Deep <b>Learning</b>: Concepts, Methods &amp; Applications in Wireless Networks", "dateLastCrawled": "2022-01-18T23:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Federated</b> <b>Learning</b>. <b>Federated</b> <b>learning</b> is a response to the\u2026 | by ...", "url": "https://medium.datadriveninvestor.com/federated-learning-c401f98fcdbf", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/<b>federated</b>-<b>learning</b>-c401f98fcdbf", "snippet": "<b>Federated</b> <b>Learning</b>, a new framework for Artificial Intelligence (AI) model development that is <b>distributed</b> over millions of mobile devices. It enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on the device, decoupling the ability to do machine <b>learning</b> from the need to store the data in the cloud.", "dateLastCrawled": "2022-01-15T09:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Federated Learning</b> with PySyft. The new era of training Machine\u2026 | by ...", "url": "https://towardsdatascience.com/federated-learning-3097547f8ca3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>federated-learning</b>-3097547f8ca3", "snippet": "<b>Federated Learning</b> is a <b>distributed</b> machine <b>learning</b> approach which enables model training on a large corpus of decentralised data. <b>Federated Learning</b> enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine <b>learning</b> from the need to store the data in the cloud. This goes beyond the use of local models that make predictions on mobile devices (<b>like</b> the Mobile Vision API and On-Device Smart ...", "dateLastCrawled": "2022-01-29T18:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Federated</b> <b>Learning</b> as a <b>New Approach to Machine Learning</b> - Avenga", "url": "https://www.avenga.com/magazine/federated-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.avenga.com/magazine/<b>federated</b>-<b>learning</b>", "snippet": "In the case of DNNs, <b>network</b> weights and hyper-parameters are exchanged between nodes with specialized coordinators and synchronization routines. <b>Distributed</b> <b>learning</b> or <b>federated</b> <b>learning</b>? Does it sound a little bit <b>like</b> <b>distributed</b> <b>learning</b> which we can apply in a classical centralized machine <b>learning</b> flow? It actually uses the evergreen idea of dividing and conquering the larger problem by separating it into smaller units and then combining the results together. Partially this is true ...", "dateLastCrawled": "2022-01-26T20:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "SpreadGNN: Serverless Multi-task <b>Federated</b> <b>Learning</b> for Graph <b>Neural</b> ...", "url": "https://fl-icml.github.io/2021/papers/FL-ICML21_paper_18.pdf", "isFamilyFriendly": true, "displayUrl": "https://fl-icml.github.io/2021/papers/FL-ICML21_paper_18.pdf", "snippet": "<b>Federated</b> <b>Learning</b> (FL) is a <b>distributed</b> <b>learning</b> paradigm that addresses this data isolation problem via collaborative training. In this paradigm, training is an act of collabora- tion between multiple clients (such as research institutions) without requiring centralized local data while providing a certain degree of user-level privacy (McMahan et al., 2017; Kairouz et al., 2019). However there are still challenges and shortcomings to training GNNs in a <b>federated</b> setting. As. SpreadGNN ...", "dateLastCrawled": "2022-01-21T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "From <b>federated learning</b> to <b>federated</b> <b>neural</b> architecture search: a ...", "url": "https://link.springer.com/article/10.1007/s40747-020-00247-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40747-020-00247-z", "snippet": "<b>Federated</b> NAS aims to optimize the architecture of <b>neural</b> <b>network</b> models in the <b>federated learning</b> environment. As discussed in section \u201c<b>Federated Learning</b>\u201d, <b>distributed</b> model training is intrinsically more difficult than centralized training, and it becomes even more challenging for NAS problems. In this section, we would <b>like</b> to introduce ...", "dateLastCrawled": "2022-02-02T17:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>FedGraphNN: A Federated Learning System and Benchmark</b> for Graph <b>Neural</b> ...", "url": "https://gnnsys.github.io/papers/GNNSys21_paper_3.pdf", "isFamilyFriendly": true, "displayUrl": "https://gnnsys.github.io/papers/GNNSys21_paper_3.pdf", "snippet": "<b>Federated</b> <b>Learning</b> (FL) is a <b>distributed</b> <b>learning</b> paradigm that addresses this data isolation problem. In FL, training *Equal contribution 1Viterbi School of Engineering , Univer-sity of Southern California, California, USA 2Machine <b>Learn-ing</b> Center, Tencent AI Lab. Correspondence to: Chaoyang He, Keshav Balasubramanian, Emir Ceyani &lt;chaoyang.he@usc.edu, keshavba@usc.edu, ceyani@usc.edu&gt;. Proceedings of the First MLSys Workshop on Graph <b>Neural</b> Net-works and Systems (GNNSys\u201921), https ...", "dateLastCrawled": "2022-02-02T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Brief Study Note on <b>Three Privacy Privacy-Preserving Distributed</b> Deep ...", "url": "https://yumiaomiao0908.medium.com/brief-study-note-on-three-privacy-privacy-preserving-distributed-deep-learning-methods-16744381bd45", "isFamilyFriendly": true, "displayUrl": "https://yumiaomiao0908.medium.com/brief-study-note-on-three-privacy-privacy-preserving...", "snippet": "<b>Federated</b> <b>learning</b> (FL) (Kone\u010dn\u00fd, Jakub, et al. arXiv preprint arXiv:1610.02527 (2016)) is a <b>distributed</b> <b>learning</b> method that enables training of <b>neural</b> <b>network</b> models across multiple devices or servers without the need for movement of data. Instead of storing data from various sources at a centralized processing site and preform a centralized training, FL is involved with multiple <b>federated</b> rounds to obtain a robust model. A <b>federated</b> round is defined as below:", "dateLastCrawled": "2022-01-05T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Federated</b> Reconnaissance: Efficient, <b>Distributed</b>, Class-Incremental ...", "url": "https://deepai.org/publication/federated-reconnaissance-efficient-distributed-class-incremental-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>federated</b>-reconnaissance-efficient-<b>distributed</b>-class...", "snippet": "In this work, we present <b>federated</b> reconnaissance, a new a class of <b>learning</b> problems in which <b>distributed</b> models should be able to learn new concepts independently and share that knowledge efficiently.Typically in <b>federated</b> <b>learning</b>, a single static set of classes is learned by each client (McMahan et al., 2017).In contrast, <b>federated</b> reconnaissance requires that each client can individually learn a growing set of classes and communicate knowledge of previously observed and new classes ...", "dateLastCrawled": "2022-01-29T12:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Federated</b> <b>Learning</b> using Peer-to-peer <b>Network</b> for Decentralized ...", "url": "https://www.techrxiv.org/articles/preprint/Federated_Learning_using_Peer-to-peer_Network_for_Decentralized_Orchestration_of_Model_Weights/14267468", "isFamilyFriendly": true, "displayUrl": "https://www.techrxiv.org/articles/preprint/<b>Federated</b>_<b>Learning</b>_using_Peer-to-peer...", "snippet": "Such implementations of privacy preserving <b>federated</b> <b>learning</b> find applicability in various ecosystems <b>like</b> finance, health care, legal, research and other fields that require preservation of privacy. However, many such implementations are driven by a centralized architecture in the <b>network</b>, where the aggregator node becomes the single point of failure, and is also expected with lots of computing resources at its disposal. In this paper, we propose an approach of implementing a decentralized ...", "dateLastCrawled": "2022-01-31T23:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Federated learning</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Federated_learning", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Federated_learning</b>", "snippet": "<b>Federated learning</b> (also known as collaborative <b>learning</b>) is a machine <b>learning</b> technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them.This approach stands in contrast to traditional centralized machine <b>learning</b> techniques where all the local datasets are uploaded to one server, as well as to more classical decentralized approaches which often assume that local data samples are identically <b>distributed</b> ...", "dateLastCrawled": "2022-02-02T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Decentralized <b>Federated</b> Graph <b>Neural</b> Networks", "url": "https://federated-learning.org/fl-ijcai-2021/FTL-IJCAI21_paper_20.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>federated</b>-<b>learning</b>.org/fl-ijcai-2021/FTL-IJCAI21_paper_20.pdf", "snippet": "<b>federated</b> <b>learning</b> and graph <b>neural</b> networks [Zheng et al., 2021], [Wu et al., 2021]. These models, according to their studies, perform similarly with popular centralized models in terms of model accuracy, while providing data privacy protec-tion. In particular, the work [Mei et al., 2019] uses a <b>similar</b>-ity matrix construction method to hide the private structural information of nodes. The work [Lalitha et al., 2019] uses a peer-to-peer <b>distributed</b> <b>federated</b> <b>learning</b> framework and ...", "dateLastCrawled": "2022-01-30T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Feature <b>distributed</b> <b>learning</b> (a) and vertical <b>federated</b> <b>learning</b> (b ...", "url": "https://researchgate.net/figure/Feature-distributed-learning-a-and-vertical-federated-learning-b_fig2_348224791", "isFamilyFriendly": true, "displayUrl": "https://researchgate.net/figure/Feature-<b>distributed</b>-<b>learning</b>-a-and-vertical-<b>federated</b>...", "snippet": "Different from horizontal <b>federated</b> <b>learning</b>, vertical <b>federated</b> <b>learning</b> <b>is similar</b> to feature <b>distributed</b> <b>learning</b> [4] to some extent which &#39;vertically&#39; partitions the training data, as shown in ...", "dateLastCrawled": "2021-07-23T12:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Federated</b> <b>Learning</b>: A <b>Distributed</b> Shared Machine <b>Learning</b> Method", "url": "https://www.hindawi.com/journals/complexity/2021/8261663/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/complexity/2021/8261663", "snippet": "<b>Federated</b> <b>learning</b> (FL) is a <b>distributed</b> machine <b>learning</b> (ML) framework. In FL, multiple clients collaborate to solve traditional <b>distributed</b> ML problems under the coordination of the central server without sharing their local private data with others. This paper mainly sorts out FLs based on machine <b>learning</b> and deep <b>learning</b>. First of all, this paper introduces the development process, definition, architecture, and classification of FL and explains the concept of FL by comparing it with ...", "dateLastCrawled": "2022-02-03T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>What is Federated Learning</b>? - Unite.AI", "url": "https://www.unite.ai/what-is-federated-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.unite.ai/<b>what-is-federated-learning</b>", "snippet": "In a <b>federated</b> <b>learning</b> system, the various devices that are part of the <b>learning</b> <b>network</b> each have a copy of the model on the device. The different devices/clients train their own copy of the model using the client\u2019s local data, and then the parameters/weights from the individual models are sent to a master device, or server, that aggregates the parameters and updates the global model.", "dateLastCrawled": "2022-02-02T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "From <b>federated learning</b> to <b>federated</b> <b>neural</b> architecture search: a ...", "url": "https://link.springer.com/article/10.1007/s40747-020-00247-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40747-020-00247-z", "snippet": "In instance <b>distributed</b> <b>learning</b>, such as multi-GPU training (GPUs are always embedded inside a server, communication effects can thus be ignored), a deep <b>neural</b> <b>network</b> tends to synchronously update the global model once the local gradients of the mini-batch data are calculated, to ensure the correct <b>distributed</b> model <b>learning</b> direction. This global model updating approach is intrinsically not suited for horizontal <b>federated learning</b> because frequent upload and download of data are not ...", "dateLastCrawled": "2022-02-02T17:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "FedGraphNN: A <b>Federated Learning</b> System and Benchmark for Graph <b>Neural</b> ...", "url": "https://deepai.org/publication/fedgraphnn-a-federated-learning-system-and-benchmark-for-graph-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fedgraphnn-a-<b>federated-learning</b>-system-and-benchmark...", "snippet": "<b>Federated Learning</b> (FL) is a <b>distributed</b> <b>learning</b> paradigm that addresses this data isolation problem. In FL, training is an act of collaboration between multiple clients without requiring centralized local data while providing a certain degree of user-level privacy (McMahan et al., 2017; Kairouz et al., 2019). (a) Graph-level FL (b) Subgraph-level FL (c) Node-level FL (d) Edge-level FL: Figure 1: Four Types of <b>Federated</b> Graph <b>Neural</b> Networks (FedGraphNN): (a) Graph-level FL: molecule ...", "dateLastCrawled": "2022-01-25T15:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Federated Learning</b> - GitHub Pages", "url": "https://florian.github.io/federated-learning/", "isFamilyFriendly": true, "displayUrl": "https://florian.github.io/<b>federated-learning</b>", "snippet": "Theoretically, this can be done arbitrarily, by using any of the common <b>neural</b> <b>network</b> initialization strategies or the equivalent for other model types. In practice, it is a good idea to use publicly available data to pretrain the model. For the example given above, this could be done by using text from Wikipedia. Although this does not produce the best possible model, it is a good starting point and can reduce the time until the <b>Federated Learning</b> process converges. After the model is ...", "dateLastCrawled": "2022-01-30T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Federated</b> <b>Learning</b>: Massively <b>distributed</b> <b>learning</b> | by Adit Deshmukh ...", "url": "https://aditdeshmukh.medium.com/federated-learning-massively-distributed-learning-f57ea1a6cecc", "isFamilyFriendly": true, "displayUrl": "https://aditdeshmukh.medium.com/<b>federated</b>-<b>learning</b>-massively-<b>distributed</b>-<b>learning</b>-f57...", "snippet": "When the data is so massively <b>distributed</b> as in <b>federated</b> <b>learning</b>, the computation is no longer the bottleneck. The bottleneck is the communication. All the different devices in the federation have to send updates to the parameter server. Communication efficiency of utmost importance. The authors put forward two ways of reducing communication costs: structured updates and sketched updates. They show that it is possible to reduce communication costs by two orders of magnitude without ...", "dateLastCrawled": "2022-01-16T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Federated</b> <b>learning</b> over wireless backhaul for <b>distributed</b> micro\u2010Doppler ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rsn2.12227", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rsn2.12227", "snippet": "Instead, the <b>federated</b> <b>learning</b> approach can be applied to the <b>distributed</b> system as an alternative <b>distributed</b> machine <b>learning</b> method [15, 16]. Rather than sharing the locally collected data, each <b>distributed</b> node trains the <b>neural</b> <b>network</b> locally and transfers its trained model updates to the server. The server then aggregates the updates from <b>distributed</b> nodes into a global <b>network</b> iteratively. Furthermore, in Ref.", "dateLastCrawled": "2022-02-02T12:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "GitHub - amitgoswami1027/FederatedLearningMaster: <b>Federated</b> <b>Learning</b> or ...", "url": "https://github.com/amitgoswami1027/FederatedLearningMaster", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/amitgoswami1027/<b>FederatedLearning</b>Master", "snippet": "<b>Federated</b> <b>Learning</b> - <b>Distributed</b> Machine <b>Learning</b> !! Motivation : <b>Federated</b> <b>Learning</b>. A technique for training ML Models on data to which you don\u2019t have access.", "dateLastCrawled": "2022-02-03T05:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>What is Federated Learning</b>? - Unite.AI", "url": "https://www.unite.ai/what-is-federated-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.unite.ai/<b>what-is-federated-learning</b>", "snippet": "Model convergence time is another challenge for <b>federated</b> <b>learning</b>, as <b>federated</b> <b>learning</b> models typically take longer to converge than locally trained models. The number of devices involved in the training adds an element of unpredictability to the model training, as connection issues, irregular updates, and even different application use times <b>can</b> contribute to increased convergence time and decreased reliability. For this reason, <b>federated</b> <b>learning</b> solutions are typically most useful when ...", "dateLastCrawled": "2022-02-02T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Performance Analysis of <b>Distributed</b> and <b>Federated</b> <b>Learning</b> Models on ...", "url": "https://d22z49uojj1i5r.cloudfront.net/performance-analysis-of-distributed-and-federated-learning-models/fulltext/pdf/performance-analysis-of-distributed-and-federated-learning-models.pdf", "isFamilyFriendly": true, "displayUrl": "https://d22z49uojj1i5r.cloudfront.net/performance-analysis-of-<b>distributed</b>-and...", "snippet": "existing model training methods (basic and <b>distributed</b>) with a recent model training method (<b>federated</b> <b>learning</b>). We build a <b>neural</b> <b>network</b> which plays the role of a classifier in all the three training mechanisms. Thus, while the underlying model being trained is the same, the way in which the parameter updations occur is different. This", "dateLastCrawled": "2022-01-18T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Federated</b> Reconnaissance: Efficient, <b>Distributed</b>, Class-Incremental ...", "url": "https://deepai.org/publication/federated-reconnaissance-efficient-distributed-class-incremental-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>federated</b>-reconnaissance-efficient-<b>distributed</b>-class...", "snippet": "In this work, we present <b>federated</b> reconnaissance, a new a class of <b>learning</b> problems in which <b>distributed</b> models should be able to learn new concepts independently and share that knowledge efficiently.Typically in <b>federated</b> <b>learning</b>, a single static set of classes is learned by each client (McMahan et al., 2017).In contrast, <b>federated</b> reconnaissance requires that each client <b>can</b> individually learn a growing set of classes and communicate knowledge of previously observed and new classes ...", "dateLastCrawled": "2022-01-29T12:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Federated Learning</b>: Rewards &amp; Challenges of <b>Distributed</b> Private ML", "url": "https://www.infoq.com/presentations/federated-learning-distributed-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.infoq.com/presentations/<b>federated-learning</b>-<b>distributed</b>-ml", "snippet": "<b>Federated learning</b> is a real crucible because it brings together even more, so it&#39;s really an interface between data science, machine <b>learning</b>, engineering, DevOps, software data, and security ...", "dateLastCrawled": "2022-01-24T05:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Performance Analysis of Distributed and Federated Learning Models</b> on ...", "url": "https://www.sciencedirect.com/science/article/pii/S1877050920300478", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1877050920300478", "snippet": "While the basic and <b>distributed</b> methods of training are widely known, <b>federated</b> <b>learning</b> is a more recent category of <b>distributed</b> machine <b>learning</b>, which is able to train on a large corpus of decentralized data while preserving its confidentiality. <b>Federated</b> <b>Learning</b> is a flourishing research topic which was first proposed by Google [12,13,14] in 2016. The humongous volumes of data <b>can</b> be very useful in training models, but new regulations on data localization and privacy prevent companies ...", "dateLastCrawled": "2022-01-14T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Parallel, Distributed, and Federated Learning</b>", "url": "https://www.linkedin.com/pulse/parallel-distributed-federated-learning-michael-kamp", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/parallel-<b>distributed</b>-<b>federated</b>-<b>learning</b>-michael-kamp", "snippet": "The workshop aims to foster discussion, discovery, and dissemination of novel ideas and approaches for parallel, <b>distributed</b>, and <b>federated</b> machine <b>learning</b>. We invite participation in the 3rd ...", "dateLastCrawled": "2021-08-25T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Federated Learning through Distance-Based Clustering</b> | by Phani Rohith ...", "url": "https://towardsdatascience.com/federated-learning-through-distance-based-clustering-5b09c3700b3c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>federated-learning-through-distance-based-clustering</b>-5b...", "snippet": "<b>Federated</b> <b>learning</b> is a paradigm where a <b>distributed</b> system of devices is set up to collaborate to train a model. Traditional <b>federated</b> <b>learning</b> involves having a centralized server that contains model weights with devices that contribute to the training of that model by occasionally sending their weights back to the server. When the weights get sent back to the server, all devices are given an equal chance to update the main/server model using a process called <b>Federated</b> Averaging . In ...", "dateLastCrawled": "2022-01-28T11:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Federated learning of deep networks using model averaging</b>", "url": "https://www.slideshare.net/NguynNhng2/federated-learning-of-deep-networks-using-model-averaging", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/NguynNhng2/<b>federated-learning-of-deep-networks-using</b>-model...", "snippet": "<b>Federated Learning of Deep Networks</b> non-IID data distributions, and <b>can</b> reduce the rounds of communication needed to train a deep <b>network</b> by one to two orders of magnitude. 1.1. <b>Federated</b> <b>Learning</b> What tasks are best suited to <b>federated</b> <b>learning</b>? The ideal problems have the following properties: \u2022 Training on real-world data from mobile devices pro- vides a distinct advantage over training on proxy data that is generally available in the data-center. \u2022 This data is privacy sensitive or ...", "dateLastCrawled": "2022-01-21T18:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Federated Learning and Privacy</b> \u2013 Blog", "url": "https://dudeperf3ct.github.io/federated/learning/privacy/2019/02/08/Federated-Learning-and-Privacy/", "isFamilyFriendly": true, "displayUrl": "https://dudeperf3ct.github.io/<b>federated</b>/<b>learning</b>/privacy/2019/02/08/<b>Federated</b>-<b>Learning</b>...", "snippet": "Due to varying upload and download speed across different regions and different countries, the uploads required in <b>federated</b> <b>learning</b> will be very slow compared to traditional <b>distributed</b> machine <b>learning</b> in datacenters where the communications among the nodes is very quick and messages don\u2019t get lost (Remember, Imagenet training in 5 mintues).", "dateLastCrawled": "2022-02-01T18:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Decentralized <b>Federated</b> Graph <b>Neural</b> Networks", "url": "https://federated-learning.org/fl-ijcai-2021/FTL-IJCAI21_paper_20.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>federated</b>-<b>learning</b>.org/fl-ijcai-2021/FTL-IJCAI21_paper_20.pdf", "snippet": "\ufb01cient <b>distributed</b> <b>learning</b> algorithm. The work [Heged\u02ddus et al., 2019] introduces the decentralized <b>learning</b> frame- work Gossip [Liu et al., 2018] into <b>federated</b> <b>learning</b>, and proposed an alternative optimization method for <b>federated</b> <b>learning</b>. The work [Hu et al., 2019] proposed a new de-centralized <b>federated</b> <b>learning</b> algorithm based on both the Gossip algorithm and the model segmentation, where local models are propagated over a peer-to-peer <b>network</b> topology through a sum-weight gossip ...", "dateLastCrawled": "2022-01-30T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Federated</b> <b>Learning</b>: Massively <b>distributed</b> <b>learning</b> | by Adit Deshmukh ...", "url": "https://aditdeshmukh.medium.com/federated-learning-massively-distributed-learning-f57ea1a6cecc", "isFamilyFriendly": true, "displayUrl": "https://aditdeshmukh.medium.com/<b>federated</b>-<b>learning</b>-massively-<b>distributed</b>-<b>learning</b>-f57...", "snippet": "<b>Federated</b> <b>Learning</b>: Massively <b>distributed</b> <b>learning</b>. Adit Deshmukh. Apr 10, 2021 \u00b7 10 min read. Modern mobile devices have access to a vast amount of information. This information <b>can</b> be leveraged using machine <b>learning</b> to enhance user experience. For example, a model <b>can</b> learn how a user types and suggest the next word accordingly.", "dateLastCrawled": "2022-01-16T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Federated</b> <b>Learning</b>: A <b>Distributed</b> Shared Machine <b>Learning</b> Method", "url": "https://www.hindawi.com/journals/complexity/2021/8261663/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/complexity/2021/8261663", "snippet": "<b>Federated</b> <b>learning</b> (FL) is a <b>distributed</b> machine <b>learning</b> (ML) framework. In FL, multiple clients collaborate to solve traditional <b>distributed</b> ML problems under the coordination of the central server without sharing their local private data with others. This paper mainly sorts out FLs based on machine <b>learning</b> and deep <b>learning</b>. First of all, this paper introduces the development process, definition, architecture, and classification of FL and explains the concept of FL by comparing it with ...", "dateLastCrawled": "2022-02-03T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Stochastic Channel-Based <b>Federated</b> <b>Learning</b> With <b>Neural</b> <b>Network</b> Pruning ...", "url": "https://pubmed.ncbi.nlm.nih.gov/33350391/", "isFamilyFriendly": true, "displayUrl": "https://<b>pubmed</b>.ncbi.nlm.nih.gov/33350391", "snippet": "The proposed method, termed stochastic channel-based <b>federated</b> <b>learning</b> (SCBFL), enables participants to train a high-performance model cooperatively and in a <b>distributed</b> manner without sharing their inputs. Methods: We designed, implemented, and evaluated a channel-based update algorithm for a central server in a <b>distributed</b> system. The update ...", "dateLastCrawled": "2020-12-28T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Federated</b> <b>learning</b> over wireless backhaul for <b>distributed</b> micro\u2010Doppler ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rsn2.12227", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rsn2.12227", "snippet": "Instead, the <b>federated</b> <b>learning</b> approach <b>can</b> be applied to the <b>distributed</b> system as an alternative <b>distributed</b> machine <b>learning</b> method [15, 16]. Rather than sharing the locally collected data, each <b>distributed</b> node trains the <b>neural</b> <b>network</b> locally and transfers its trained model updates to the server. The server then aggregates the updates from <b>distributed</b> nodes into a global <b>network</b> iteratively. Furthermore, in Ref.", "dateLastCrawled": "2022-02-02T12:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[2112.03465] <b>Federated</b> Deep Reinforcement <b>Learning</b> for the <b>Distributed</b> ...", "url": "https://arxiv.org/abs/2112.03465", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2112.03465", "snippet": "Zero-touch data-driven approaches <b>can</b> improve the ability of the <b>network</b> to adapt to the current operating conditions. Tools such as reinforcement <b>learning</b> (RL) algorithms <b>can</b> build optimal control policy solely based on a history of observations. Specifically, deep RL (DRL), which uses a deep <b>neural</b> <b>network</b> (DNN) as a predictor, has been shown to achieve good performance even in complex environments and with high dimensional inputs. However, the training of DRL models require a large amount ...", "dateLastCrawled": "2022-01-21T20:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "FedSGC: <b>Federated</b> Simple Graph Convolution for Node Classi\ufb01cation", "url": "https://federated-learning.org/fl-ijcai-2021/FTL-IJCAI21_paper_5.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>federated</b>-<b>learning</b>.org/fl-ijcai-2021/FTL-IJCAI21_paper_5.pdf", "snippet": "dress this, <b>federated</b> <b>learning</b>, as a <b>distributed</b> <b>learn-ing</b> mechanism, is proposed to train models with decentralized data owned by different data parties without sharing or leaking the raw data. In this work, we study the vertical and horizontal settings for <b>federated</b> <b>learning</b> on graph data. We pro-pose FedSGC to train the Simple Graph Convo-lution model under three data split scenarios. We also demonstrate that the prediction performance of FedSGC is closely aligned with the non-<b>federated</b> ...", "dateLastCrawled": "2022-01-29T15:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Federated</b> <b>Learning</b> of <b>Neural Network Models with Heterogeneous</b> ...", "url": "https://ieeexplore.ieee.org/document/9356244/", "isFamilyFriendly": true, "displayUrl": "https://ieeexplore.ieee.org/document/9356244", "snippet": "<b>Federated</b> <b>Learning</b> of <b>Neural Network Models with Heterogeneous Structures</b> Abstract: <b>Federated</b> <b>learning</b> trains a model on a centralized server using datasets <b>distributed</b> over a large number of edge devices. Applying <b>federated</b> <b>learning</b> ensures data privacy because it does not transfer local data from edge devices to the server. Existing <b>federated</b> <b>learning</b> algorithms assume that all deployed models share the same structure. However, it is often infeasible to distribute the same model to every ...", "dateLastCrawled": "2021-09-07T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Network</b> Coding for <b>Federated</b> <b>Learning</b> Systems | SpringerLink", "url": "https://link.springer.com/chapter/10.1007/978-3-030-63833-7_46", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-63833-7_46", "snippet": "<b>Compared</b> with the ordinary machine <b>learning</b>, <b>federated</b> <b>learning</b> (FL) enables multiple participants to collaboratively learn a shared machine <b>learning</b> model while keeping all the training data on local devices. However, most of the current secured <b>federated</b> <b>learning</b> systems (FLSs) are built up with high computational and communication costs. On the other hand, optimizing the <b>network</b> structure of <b>federated</b> <b>learning</b> systems <b>can</b> reduce communication complexity by considering the correlation of ...", "dateLastCrawled": "2022-01-03T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Efficient <b>federated</b> convolutional <b>neural</b> <b>network</b> with information ...", "url": "https://www.sciencedirect.com/science/article/pii/S0967066121001908", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0967066121001908", "snippet": "<b>Federated</b> <b>Learning</b> (FL) (Yang, Liu, Chen, &amp; Tong, 2019b) as an emerging <b>distributed</b> <b>learning</b> approach, <b>can</b> enable multiple industrial participants to collaboratively train a deep <b>learning</b> model through information fusion (sharing the gradient parameters of their local models). Since the local models of the participants may learn different knowledge from their local datasets, the cloud server <b>can</b> obtain an excellent global deep <b>learning</b> model by aggregating the local models. Due to its ...", "dateLastCrawled": "2022-01-26T17:05:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A brief introduction to <b>Federated</b> <b>Learning</b> \u2014 FL Series Part 1 | by OPEN ...", "url": "https://openzone.medium.com/a-brief-introduction-to-federated-learning-fl-series-part-1-b81c6ec15fb8", "isFamilyFriendly": true, "displayUrl": "https://openzone.medium.com/a-brief-introduction-to-<b>federated</b>-<b>learning</b>-fl-series-part...", "snippet": "<b>Federated</b> <b>learning</b> was first introduced by Google in 2017 (1) to improve text prediction in mobile keyboard using <b>machine</b> <b>learning</b> models trained by data across multiple devices. The new technology branch of <b>machine</b> <b>learning</b> has been sought-after ever since because it doesn\u2019t require uploading personal data to a central server to train the models, which was a breakthrough in traditional <b>machine</b> <b>learning</b> to address data privacy issues.", "dateLastCrawled": "2022-01-24T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Why Is <b>Federated</b> <b>Learning</b> Getting So Popular", "url": "https://analyticsindiamag.com/why-federated-learning-getting-so-popular/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/why-<b>federated</b>-<b>learning</b>-getting-so-popular", "snippet": "<b>Federated</b> <b>Learning</b> leverages techniques from multiple research areas such as distributed systems, <b>machine</b> <b>learning</b>, and privacy. FL is best applied in situations where the on-device data is more relevant than the data that exists on servers. However, FLS faces various challenges such as effectiveness, efficiency, and privacy. FL enables multiple parties to jointly train a <b>machine</b> <b>learning</b> model without exchanging the local data. According to", "dateLastCrawled": "2022-01-29T22:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "[2107.03770] <b>Federated</b> <b>Learning</b> as a Mean-Field Game", "url": "https://arxiv.org/abs/2107.03770", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2107.03770", "snippet": "Abstract: We establish a connection between <b>federated</b> <b>learning</b>, a concept from <b>machine</b> <b>learning</b>, and mean-field games, a concept from game theory and control theory. In this <b>analogy</b>, the local <b>federated</b> learners are considered as the players and the aggregation of the gradients in a central server is the mean-field effect. We present <b>federated</b> <b>learning</b> as a differential game and discuss the properties of the equilibrium of this game. We hope this novel view to <b>federated</b> <b>learning</b> brings ...", "dateLastCrawled": "2021-08-31T21:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Federated</b> <b>Learning</b> as a Mean-Field Game | DeepAI", "url": "https://deepai.org/publication/federated-learning-as-a-mean-field-game", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>federated</b>-<b>learning</b>-as-a-mean-field-game", "snippet": "We establish a connection between <b>federated</b> <b>learning</b>, a concept from <b>machine</b> <b>learning</b>, and mean-field games, a concept from game theory and control theory. In this <b>analogy</b>, the local <b>federated</b> learners are considered as the players and the aggregation of the gradients in a central server is the mean-field effect. We present <b>federated</b> <b>learning</b> as a differential game and discuss the properties of the equilibrium of this game. We hope this novel view to <b>federated</b> <b>learning</b> brings together ...", "dateLastCrawled": "2022-01-28T18:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How will <b>federated</b> <b>learning</b> influence your everyday life? | by ...", "url": "https://medium.com/cybervein/how-will-federal-learning-influence-your-everyday-life-271250697378", "isFamilyFriendly": true, "displayUrl": "https://medium.com/cybervein/how-will-federal-<b>learning</b>-influence-your-everyday-life...", "snippet": "<b>Federated</b> <b>learning</b> is a type of distributed <b>learning</b> method that has the same modeling effect as traditional ML algorithms. However, rather than centralizing all the raw data like traditional ML ...", "dateLastCrawled": "2022-01-24T20:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Federated Learning at Scale</b> - Part I - Random Notes", "url": "https://xzhu0027.gitbook.io/blog/ml-system/sys-ml-index/towards-federated-learning-at-scale-system-design", "isFamilyFriendly": true, "displayUrl": "https://xzhu0027.gitbook.io/blog/ml-system/sys-ml-index/towards-<b>federated</b>-<b>learning</b>-at...", "snippet": "<b>Federated</b> <b>learning</b> (FL) is a <b>machine</b> <b>learning</b> setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL allows for smarter models, lower latency, less bandwidth usage, and less power consumption, all while ensuring privacy and user experiences.", "dateLastCrawled": "2022-02-01T09:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "terminology - <b>What is Federated Learning</b>? - Artificial Intelligence ...", "url": "https://ai.stackexchange.com/questions/26421/what-is-federated-learning", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/26421/<b>what-is-federated-learning</b>", "snippet": "The <b>analogy</b> is to a federal system of government. In a federation, smaller pieces follow the direction of a higher piece. In <b>federated</b> <b>machine</b> <b>learning</b>, you give your data for processing to the higher <b>machine</b>. The federation in this <b>analogy</b> is a collection of smaller computers. The central computer breaks up your data and gives portions of it to each smaller computer. When those computers are done they return the results and the central computer reassembles them into a single model. The main ...", "dateLastCrawled": "2022-01-26T03:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Privacy and Machine Learning</b>. In today\u2019s world of technology, it can ...", "url": "https://medium.com/dotstar/privacy-and-machine-learning-a0effbbdc658", "isFamilyFriendly": true, "displayUrl": "https://medium.com/dotstar/<b>privacy-and-machine-learning</b>-a0effbbdc658", "snippet": "<b>Federated</b> <b>Learning</b>. In <b>analogy</b>, FL is like ordering dinner instead going to the restaurant yourself (what?). So, for example , if Alice refuses to send her data to train Bob\u2019s model on a sitting ...", "dateLastCrawled": "2021-08-14T18:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Overcoming Forgetting in <b>Federated Learning</b> on Non-IID Data | DeepAI", "url": "https://deepai.org/publication/overcoming-forgetting-in-federated-learning-on-non-iid-data", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/overcoming-forgetting-in-<b>federated-learning</b>-on-non-iid-data", "snippet": "There is a deep parallel between the <b>Federated Learning</b> problem and another fundamental <b>machine</b> <b>learning</b> problem called Lifelong <b>Learning</b> (and the related Multi-Task <b>Learning</b>).In Lifelong <b>Learning</b>, the challenge is to learn task A, and continue on to learn task B using the same model, but without &quot;forgetting&quot;, without severely hurting the performance on, task A; or in general, <b>learning</b> tasks A 1, A 2 \u2026 in sequence without forgetting previously-learnt tasks for which samples are not ...", "dateLastCrawled": "2022-01-11T12:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Federated</b> <b>Learning</b> Game Theory", "url": "https://www.learning-study.info/federated-learning-game-theory/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>learning</b>-study.info/<b>federated</b>-<b>learning</b>-game-theory", "snippet": "(8 days ago) <b>Federated</b> <b>Learning</b> (FL) is a distributed <b>learning</b> framework that can deal with the distributed issue in <b>machine</b> <b>learning</b> and still guarantee high <b>learning</b> performance. However, it is impractical that all users will sacrifice their resources to join the FL algorithm. This motivates us to study the incentive mechanism design for FL.", "dateLastCrawled": "2022-01-27T11:14:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A survey on <b>federated learning</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0950705121000381", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705121000381", "snippet": "<b>Federated learning is similar</b> to multi-party computing and distributed <b>machine</b> <b>learning</b>. There are many types of distributed <b>machine</b> <b>learning</b>, including distributed publishing model results, distributed storage training data, and distributed computing tasks. The parameter server in distributed <b>machine</b> <b>learning</b> is one of the tools to accelerate the training speed of <b>machine</b> <b>learning</b> models. It stores data on different working nodes in a distributed manner and allocates resources through a ...", "dateLastCrawled": "2022-01-28T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Federated <b>Learning</b>: A Distributed Shared <b>Machine</b> <b>Learning</b> Method", "url": "https://www.hindawi.com/journals/complexity/2021/8261663/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/complexity/2021/8261663", "snippet": "Federated <b>learning</b> (FL) is a distributed <b>machine</b> <b>learning</b> (ML) framework. In FL, multiple clients collaborate to solve traditional distributed ML problems under the coordination of the central server without sharing their local private data with others. This paper mainly sorts out FLs based on <b>machine</b> <b>learning</b> and deep <b>learning</b>. First of all, this paper introduces the development process, definition, architecture, and classification of FL and explains the concept of FL by comparing it with ...", "dateLastCrawled": "2022-02-03T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A novel federated <b>learning</b> approach based on the confidence of ...", "url": "https://link.springer.com/article/10.1007%2Fs13042-021-01410-9", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13042-021-01410-9", "snippet": "<b>Federated learning is similar</b> to but not equal to federated Kalman filtering, so it is impossible to obtain federated filtering gains according to variance as the FKF generally does. Therefore, this paper chooses cross-entropy loss, which has been proven to be excellent in <b>machine</b> <b>learning</b> and deep <b>learning</b> algorithms, instead of variance. The traditional FKF allocates the total variance \\({P}_{f}\\) of the system noise in the main filter and partial filters through the allocation factor and ...", "dateLastCrawled": "2022-01-29T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Performance Analysis of Distributed and Federated Learning Models</b> on ...", "url": "https://www.sciencedirect.com/science/article/pii/S1877050920300478", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1877050920300478", "snippet": "The initial implementation of <b>federated learning is similar</b> to the distributed <b>machine</b> <b>learning</b> classifier. However, there are two major differences, which are as follows: First, we use the federated-average-optimizer, an API using which we optimize the federated averaging process. Second, we use a variable called interval-steps. This variable keeps a count of the pre-defined number of steps after which the workers have to update their weights in the parameter server so that the chief worker ...", "dateLastCrawled": "2022-01-14T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Federated <b>Learning</b>: A Distributed Shared <b>Machine</b> <b>Learning</b> Method", "url": "https://www.researchgate.net/publication/354244192_Federated_Learning_A_Distributed_Shared_Machine_Learning_Method", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354244192_Federated_<b>Learning</b>_A_Distributed...", "snippet": "Federated <b>learning</b> (FL) is a distributed <b>machine</b> <b>learning</b> (ML) framework. In FL, multiple clients collaborate to solve traditional distributed ML problems under the coordination of the central ...", "dateLastCrawled": "2021-11-08T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>Privacy</b>-Protection Model for Patients - <b>Hindawi</b>", "url": "https://www.hindawi.com/journals/scn/2020/6647562/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/scn/2020/6647562", "snippet": "Vertical <b>federated learning is similar</b> to horizontal federated <b>learning</b>, but is different in data feature extraction, where vertical federated <b>learning</b> extracts data of the same user in different dimensions. For example, two organizations with different businesses conduct data analysis on users in the same area, which can analyse the characteristic values of the same users to achieve better data results. (3) Federated Transfer <b>Learning</b>. When the user characteristics and samples overlap in ...", "dateLastCrawled": "2022-01-26T22:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "From <b>federated learning</b> to federated neural architecture search: a ...", "url": "https://link.springer.com/article/10.1007/s40747-020-00247-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40747-020-00247-z", "snippet": "<b>Federated learning</b> is a recently proposed distributed <b>machine</b> <b>learning</b> paradigm for privacy preservation, which has found a wide range of applications where data privacy is of primary concern. Meanwhile, neural architecture search has become very popular in deep <b>learning</b> for automatically tuning the architecture and hyperparameters of deep neural networks. While both <b>federated learning</b> and neural architecture search are faced with many open challenges, searching for optimized neural ...", "dateLastCrawled": "2022-02-02T17:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Private <b>federated learning on vertically partitioned</b> data via entity ...", "url": "https://www.researchgate.net/publication/321374782_Private_federated_learning_on_vertically_partitioned_data_via_entity_resolution_and_additively_homomorphic_encryption", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321374782_Private_federated_<b>learning</b>_on...", "snippet": "The main idea of federated <b>learning</b> is to perform an on-device collaborative training of a single <b>machine</b> <b>learning</b> model without having to share the raw training data with any third-party entity ...", "dateLastCrawled": "2022-01-25T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Assisted <b>Learning</b>: A Framework for Multi-Organization <b>Learning</b>", "url": "https://proceedings.neurips.cc/paper/2020/file/a7b23e6eefbe6cf04b8e62a6f0915550-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/a7b23e6eefbe6cf04b8e62a6f0915550-Paper.pdf", "snippet": "clients hold different features over the same group of subjects) in vertical <b>Federated Learning is similar</b> to that in Assisted <b>Learning</b>. Nevertheless, these two types of <b>learning</b> are fundamentally different. Conceptually, the objective of Federated <b>Learning</b> is to exploit resources of massive edge devices for achieving a global objective. At the same time, the general goal of Assisted <b>Learning</b> is to facilitate multiple participants (possibly with rich resources) to autonomously assist each ...", "dateLastCrawled": "2021-11-02T00:46:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why the Future of AI is Federated | Richard Cornelius Suwandi", "url": "https://richardcsuwandi.github.io/post/federated-learning/", "isFamilyFriendly": true, "displayUrl": "https://richardcsuwandi.github.io/post/federated-<b>learning</b>", "snippet": "Thus, <b>federated learning can be thought of as</b> a new approach to <b>machine</b> <b>learning</b> that allows you to train models across devices without pooling the data. Instead of bringing users&#39; data to the server, federated <b>learning</b> brings the <b>machine</b> <b>learning</b> model to the users&#39; devices.", "dateLastCrawled": "2022-01-26T23:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Posts on Richard Cornelius Suwandi", "url": "https://richardcsuwandi.github.io/post/index.xml", "isFamilyFriendly": true, "displayUrl": "https://richardcsuwandi.github.io/post/index.xml", "snippet": "Thus, <b>federated learning can be thought of as</b> a new approach to <b>machine</b> <b>learning</b> that allows you to train models across devices without pooling the data. Instead of bringing users&#39; data to the server, federated <b>learning</b> brings the <b>machine</b> <b>learning</b> model to the users&#39; devices.", "dateLastCrawled": "2021-12-22T04:55:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(federated learning)  is like +(distributed neural network)", "+(federated learning) is similar to +(distributed neural network)", "+(federated learning) can be thought of as +(distributed neural network)", "+(federated learning) can be compared to +(distributed neural network)", "machine learning +(federated learning AND analogy)", "machine learning +(\"federated learning is like\")", "machine learning +(\"federated learning is similar\")", "machine learning +(\"just as federated learning\")", "machine learning +(\"federated learning can be thought of as\")", "machine learning +(\"federated learning can be compared to\")"]}