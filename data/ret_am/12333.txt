{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "rlLect.pdf - Reinforcement <b>Learning</b> People commonly experience ...", "url": "https://www.coursehero.com/file/125676420/rlLectpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/125676420/rlLectpdf", "snippet": "11/30/21 1 Reinforcement <b>Learning</b> People commonly experience reinforcement <b>learning</b> during life. For example: Remember <b>learning</b> <b>to ride</b> <b>a bike</b>? Fall, skinned knee \u2192 negative reward Mastery \u2192 positive reward Other examples of a \u201cnegative reward\u201d: - touching a hot stove burner with your hand - spilling coffee on your open laptop Other examples of a \u201cpositive reward\u201d:-receiving a big tip when rendering a service-eating an ice cream cone when one is hungry 1 Spectrum of Machine ...", "dateLastCrawled": "2022-01-14T18:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning</b> to Drive <b>a Bicycle Using Reinforcement Learning and Shaping</b>.", "url": "https://www.researchgate.net/publication/221346431_Learning_to_Drive_a_Bicycle_Using_Reinforcement_Learning_and_Shaping", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221346431_<b>Learning</b>_to_Drive_a_Bicycle_Using...", "snippet": "Then we solve the compos- ite problem of <b>learning</b> to balance a bicycle and then drive to &#39;It goal. In our approach the rein- forcement function is independent of the task the agent tries to learn ...", "dateLastCrawled": "2021-12-02T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Curriculum <b>Learning</b> via Reward Shaping", "url": "https://kshitijsachan.com/files/csci2951f_final.pdf", "isFamilyFriendly": true, "displayUrl": "https://kshitijsachan.com/files/csci2951f_final.pdf", "snippet": "with increasing complexity, building up to the target task (such as <b>learning</b> <b>to ride</b> a tricycle before <b>a bike</b>). The source tasks should be simple enough to be learned with a sparse reward. This approach is known as curriculum <b>learning</b>. We explore using reward shaping\u2014a specific way to transfer knowledge between tasks\u2014with cur- riculum <b>learning</b> to speed up <b>learning</b>. We specifically focus on how to design a good curriculum and how to learn with multiple source tasks. 1. 2 Background ...", "dateLastCrawled": "2022-01-04T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Markov Decision Process</b> in Reinforcement <b>Learning</b>: Everything You Need ...", "url": "https://neptune.ai/blog/markov-decision-process-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>markov-decision-process</b>-in-reinforcement-<b>learning</b>", "snippet": "Maybe <b>ride</b> <b>a bike</b>, or buy an airplane ticket? Making this choice, you incorporate probability into your decision-making process. Perhaps there\u2019s a 70% chance of rain or a car crash, which can cause traffic jams. If your <b>bike</b> tire is old, it may break down \u2013 this is certainly a large probabilistic factor. On the other hand, there are deterministic costs \u2013 for instance, the cost of gas or an airplane ticket \u2013 as well as deterministic rewards \u2013 <b>like</b> much faster travel times taking an ...", "dateLastCrawled": "2022-01-26T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Reinforcement Learning Vs. Deep Reinforcement Learning</b>", "url": "https://www.techopedia.com/reinforcement-learning-vs-deep-reinforcement-learning-whats-the-difference/2/34039", "isFamilyFriendly": true, "displayUrl": "https://<b>www.techopedia.com</b>/<b>reinforcement-learning-vs-deep-reinforcement-learning</b>-whats...", "snippet": "Brandon Haynie, chief data scientist at Babel Street in Washington, DC, compares it to a human <b>learning</b> <b>to ride</b> a bicycle. \u201cIf you\u2019re stationary and lift your feet without pedaling, a fall \u2013 or penalty \u2013 is imminent.\u201d However, if you start to pedal, then you will remain on the <b>bike</b> \u2013 reward \u2013 and progress to the next state. Haynie says: \u201c<b>Reinforcement learning</b> has applications spanning several sectors, including financial decisions, chemistry, manufacturing, and of course ...", "dateLastCrawled": "2022-02-01T06:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Large-Scale Order Dispatch in On-Demand <b>Ride</b>-Hailing Platforms: A ...", "url": "https://www.researchgate.net/publication/326495826_Large-Scale_Order_Dispatch_in_On-Demand_Ride-Hailing_Platforms_A_Learning_and_Planning_Approach", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/326495826_Large-Scale_Order_Dispatch_in_On...", "snippet": "Individual <b>Q-learning</b> is a heuristic <b>learning</b> algorithm that, even in small multi-agent problems, does not provide convergence guarantees. Nonetheless, we observe good performance of this ...", "dateLastCrawled": "2021-12-14T02:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep <b>Reinforcement Learning Techniques</b> in Diversified Domains: A Survey ...", "url": "https://link.springer.com/article/10.1007/s11831-021-09552-3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11831-021-09552-3", "snippet": "The use of max operation in <b>Q learning</b> led to such overestimation. Reducing sample size Training in deep reinforcement <b>learning</b> needs many samples. For problems related to the real-world, it might not be possible to obtain that quantity of data. Detecting and preventing overfitting Overfitting happens, when the agent is sensitive to little perturbation in the environment. The use of high capacity models of deep <b>learning</b> in DRL for applications, where limited data is available, might cause ...", "dateLastCrawled": "2022-01-26T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>News</b> - Sanghani Center for Artificial Intelligence and Data Analytics", "url": "https://dac.cs.vt.edu/news/news-person/", "isFamilyFriendly": true, "displayUrl": "https://dac.cs.vt.edu/<b>news</b>/<b>news</b>-person", "snippet": "The title of her thesis is \u201cPredicting Mutational Pathways of Influenza A H1N1 Virus using <b>Q-learning</b>. Raghuraman has joined LexisNexis Legal and Professional in Raleigh, North Carolina, as a data scientist. Esther Robb, advised by Jia-Bin Huang, has earned a master\u2019s degree in computer engineering. Her primary research interests lie in reinforcement <b>learning</b> and data-efficient <b>learning</b>. The title of her thesis is \u201cData-Efficient <b>Learning</b> in Image Synthesis and Instance Segmentation ...", "dateLastCrawled": "2022-01-18T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>the function approximation in reinforcement learning</b>? - Quora", "url": "https://www.quora.com/What-is-the-function-approximation-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-function-approximation-in-reinforcement-learning</b>", "snippet": "Answer: Consider a gridworld domain: Say the goal for the agent is to reach the top-right cell. In such a setting, you can represent a policy using a table \u2014 for each cell (state), the policy is a table that tells you what action to take. Similarly, you can represent the value functions (V and Q...", "dateLastCrawled": "2022-01-15T05:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The AI Lifecycle: Orchestrate \u2014 Dataiku Knowledge Base", "url": "https://knowledge.dataiku.com/latest/courses/project-walk/o16n/o16n.html", "isFamilyFriendly": true, "displayUrl": "https://knowledge.dataiku.com/latest/courses/project-walk/o16n/o16n.html", "snippet": "The AI Lifecycle: Orchestrate\u00b6. For an enterprise looking to scale, the ability to deploy a model into production is not enough. Realizing the full potential of a model requires orchestration \u2014 a repeatable, efficient process for creating and effectively deploying models into production.. In the case of the NY Taxi Fares project, we have deployed a model for real-time scoring, but managing the model lifecycle remains a manual process.", "dateLastCrawled": "2022-01-22T22:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "TDM: From Model-Free to <b>Model-Based Deep Reinforcement Learning</b> \u2013 The ...", "url": "https://bair.berkeley.edu/blog/2018/04/26/tdm/", "isFamilyFriendly": true, "displayUrl": "https://bair.berkeley.edu/blog/2018/04/26/tdm", "snippet": "Eventually, your <b>bike</b>-riding skills will be so proficient that you can start to plan in very abstract terms (\u201c<b>Bike</b> to the end of the road.\u201d) to the point that all there is left to do is planning and you no longer need to worry about the nitty-gritty details of riding <b>a bike</b>. We see that there is a gradual transition from the model-free (trial-and-error) strategy to a model-based (planning) strategy. If we could develop artificial intelligence algorithms--and specifically RL algorithms ...", "dateLastCrawled": "2022-01-29T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Curriculum <b>Learning</b> via Reward Shaping", "url": "https://kshitijsachan.com/files/csci2951f_final.pdf", "isFamilyFriendly": true, "displayUrl": "https://kshitijsachan.com/files/csci2951f_final.pdf", "snippet": "with increasing complexity, building up to the target task (such as <b>learning</b> <b>to ride</b> a tricycle before <b>a bike</b>). The source tasks should be simple enough to be learned with a sparse reward. This approach is known as curriculum <b>learning</b>. We explore using reward shaping\u2014a specific way to transfer knowledge between tasks\u2014with cur- riculum <b>learning</b> to speed up <b>learning</b>. We specifically focus on how to design a good curriculum and how to learn with multiple source tasks. 1. 2 Background ...", "dateLastCrawled": "2022-01-04T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Reinforcement Learning Vs. Deep Reinforcement Learning</b>", "url": "https://www.techopedia.com/reinforcement-learning-vs-deep-reinforcement-learning-whats-the-difference/2/34039", "isFamilyFriendly": true, "displayUrl": "https://<b>www.techopedia.com</b>/<b>reinforcement-learning-vs-deep-reinforcement-learning</b>-whats...", "snippet": "Brandon Haynie, chief data scientist at Babel Street in Washington, DC, compares it to a human <b>learning</b> <b>to ride</b> a bicycle. \u201cIf you\u2019re stationary and lift your feet without pedaling, a fall \u2013 or penalty \u2013 is imminent.\u201d However, if you start to pedal, then you will remain on the <b>bike</b> \u2013 reward \u2013 and progress to the next state. Haynie says: \u201c<b>Reinforcement learning</b> has applications spanning several sectors, including financial decisions, chemistry, manufacturing, and of course ...", "dateLastCrawled": "2022-02-01T06:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning</b> to Drive <b>a Bicycle Using Reinforcement Learning and Shaping</b>.", "url": "https://www.researchgate.net/publication/221346431_Learning_to_Drive_a_Bicycle_Using_Reinforcement_Learning_and_Shaping", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221346431_<b>Learning</b>_to_Drive_a_Bicycle_Using...", "snippet": "Matthew E. Taylor. Michael Bowling. Reinforcement <b>learning</b> (RL) is a powerful <b>learning</b> paradigm in which agents can learn to maximize sparse and delayed reward signals. Although RL has had many ...", "dateLastCrawled": "2021-12-02T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "My_<b>Bibliography_for_Research_on_Autonomous_Driving</b>/6_model_free_rl.md ...", "url": "https://github.com/chauvinSimon/My_Bibliography_for_Research_on_Autonomous_Driving/blob/master/sections/6_model_free_rl.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/chauvinSimon/My_<b>Bibliography_for_Research_on_Autonomous_Driving</b>/...", "snippet": "Main inspiration: Deep <b>Q-learning</b> from Demonstrations (Hester et al. 2017) at DeepMind. &quot;We present an algorithm, ... &quot;While the <b>ride</b> comfort was not ready for a production vehicle yet, it achieved speeds of over 60 km\u2215h while staying within lane and taking turns.&quot; I really like the lessons learned section. 1-Generalization. &quot;We believe that the cause of the agents high generalization capabilities stems from how the agent was trained [state, reward, scenarios], as opposed to what algorithm ...", "dateLastCrawled": "2022-01-10T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "2021.06.04 | AI Research Trends", "url": "https://mavenlin.github.io/ai_research_trends/2021/06/04/2021.06.04.html", "isFamilyFriendly": true, "displayUrl": "https://mavenlin.github.io/ai_research_trends/2021/06/04/2021.06.04.html", "snippet": "In this paper, we develop a radically uncoupled <b>Q-learning</b> dynamics that is both rational and convergent: the <b>learning</b> dynamics converges to the best response to the opponent&#39;s strategy when the opponent follows an asymptotically stationary strategy; when both agents adopt the <b>learning</b> dynamics, they converge to the Nash equilibrium of the game. The key challenge in this decentralized setting is the non-stationarity of the environment from an agent&#39;s perspective, since both her own payoffs ...", "dateLastCrawled": "2022-02-02T15:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "In Copyright - Non-Commercial Use Permitted Rights / License: Research ...", "url": "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/304517/root.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/304517/root.pdf?...", "snippet": "<b>learning</b> for station-based <b>bike</b>-sharing [17]. Contribution: We introduce reinforcement <b>learning</b> to the problem of rebalancing. Our system is trained to learn a policy that is independent of any hand-crafted control law. Secondly, a cascade-based reinforcement <b>learning</b> model is derived that both captures the essential spatio-temporal char-acteristics of the rebalancing problem and ef\ufb01ciently de\ufb01nes a state-action space that promises reasonably fast and stable convergence. Finally, this ...", "dateLastCrawled": "2022-01-04T00:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Modular and Transferable Reinforcement <b>Learning</b> Framework for the ...", "url": "https://www.researchgate.net/publication/355305702_A_Modular_and_Transferable_Reinforcement_Learning_Framework_for_the_Fleet_Rebalancing_Problem", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355305702_A_Modular_and_Transferable...", "snippet": "As a complex and critical cyber-physical system ( CPS ), the hybrid electric powertrain is significant to mitigate air pollution and improve fuel economy. Energy management strate", "dateLastCrawled": "2021-12-22T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>the function approximation in reinforcement learning</b>? - Quora", "url": "https://www.quora.com/What-is-the-function-approximation-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-function-approximation-in-reinforcement-learning</b>", "snippet": "Answer: Consider a gridworld domain: Say the goal for the agent is to reach the top-right cell. In such a setting, you can represent a policy using a table \u2014 for each cell (state), the policy is a table that tells you what action to take. Similarly, you can represent the value functions (V and Q...", "dateLastCrawled": "2022-01-15T05:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>News</b> - Sanghani Center for Artificial Intelligence and Data Analytics", "url": "https://dac.cs.vt.edu/news/news-person/", "isFamilyFriendly": true, "displayUrl": "https://dac.cs.vt.edu/<b>news</b>/<b>news</b>-person", "snippet": "The title of her thesis is \u201cPredicting Mutational Pathways of Influenza A H1N1 Virus using <b>Q-learning</b>. Raghuraman has joined LexisNexis Legal and Professional in Raleigh, North Carolina, as a data scientist. Esther Robb, advised by Jia-Bin Huang, has earned a master\u2019s degree in computer engineering. Her primary research interests lie in reinforcement <b>learning</b> and data-efficient <b>learning</b>. The title of her thesis is \u201cData-Efficient <b>Learning</b> in Image Synthesis and Instance Segmentation ...", "dateLastCrawled": "2022-01-18T22:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "TDM: From Model-Free to <b>Model-Based Deep Reinforcement Learning</b> \u2013 The ...", "url": "https://bair.berkeley.edu/blog/2018/04/26/tdm/", "isFamilyFriendly": true, "displayUrl": "https://bair.berkeley.edu/blog/2018/04/26/tdm", "snippet": "Eventually, your <b>bike</b>-riding skills will be so proficient that you <b>can</b> start to plan in very abstract terms (\u201c<b>Bike</b> to the end of the road.\u201d) to the point that all there is left to do is planning and you no longer need to worry about the nitty-gritty details of riding <b>a bike</b>. We see that there is a gradual transition from the model-free (trial-and-error) strategy to a model-based (planning) strategy. If we could develop artificial intelligence algorithms--and specifically RL algorithms ...", "dateLastCrawled": "2022-01-29T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) A Modular and Transferable Reinforcement <b>Learning</b> Framework for ...", "url": "https://www.researchgate.net/publication/351925954_A_Modular_and_Transferable_Reinforcement_Learning_Framework_for_the_Fleet_Rebalancing_Problem", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/351925954_A_Modular_and_Transferable...", "snippet": "PDF | Mobility on demand (MoD) systems show great promise in realizing flexible and efficient urban transportation. However, significant technical... | Find, read and cite all the research you ...", "dateLastCrawled": "2021-11-09T22:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Modular and Transferable Reinforcement <b>Learning</b> Framework for the ...", "url": "https://www.researchgate.net/publication/355305702_A_Modular_and_Transferable_Reinforcement_Learning_Framework_for_the_Fleet_Rebalancing_Problem", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355305702_A_Modular_and_Transferable...", "snippet": "As a complex and critical cyber-physical system ( CPS ), the hybrid electric powertrain is significant to mitigate air pollution and improve fuel economy. Energy management strate", "dateLastCrawled": "2021-12-22T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>News</b> - Sanghani Center for Artificial Intelligence and Data Analytics", "url": "https://dac.cs.vt.edu/news/news-person/", "isFamilyFriendly": true, "displayUrl": "https://dac.cs.vt.edu/<b>news</b>/<b>news</b>-person", "snippet": "The title of her thesis is \u201cPredicting Mutational Pathways of Influenza A H1N1 Virus using <b>Q-learning</b>. Raghuraman has joined LexisNexis Legal and Professional in Raleigh, North Carolina, as a data scientist. Esther Robb, advised by Jia-Bin Huang, has earned a master\u2019s degree in computer engineering. Her primary research interests lie in reinforcement <b>learning</b> and data-efficient <b>learning</b>. The title of her thesis is \u201cData-Efficient <b>Learning</b> in Image Synthesis and Instance Segmentation ...", "dateLastCrawled": "2022-01-18T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Practical Reinforcement Learning</b> | PDF | Machine <b>Learning</b> | Statistical ...", "url": "https://www.scribd.com/document/391488300/Practical-Reinforcement-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/391488300/<b>Practical-Reinforcement-Learning</b>", "snippet": "Machine <b>learning</b> <b>can</b> be divided into three categories: Figure 1.1: Types of machine <b>learning</b>. Supervised <b>learning</b> Supervised <b>learning</b> is a type of machine <b>learning</b> where we have input and output variables and [3] we use an algorithm to learn the mapping from the input variable to the output variable. Y = f(X)", "dateLastCrawled": "2022-01-16T03:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>the function approximation in reinforcement learning</b>? - Quora", "url": "https://www.quora.com/What-is-the-function-approximation-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-function-approximation-in-reinforcement-learning</b>", "snippet": "Answer: Consider a gridworld domain: Say the goal for the agent is to reach the top-right cell. In such a setting, you <b>can</b> represent a policy using a table \u2014 for each cell (state), the policy is a table that tells you what action to take. Similarly, you <b>can</b> represent the value functions (V and Q...", "dateLastCrawled": "2022-01-15T05:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "2021.06.04 | AI Research Trends", "url": "https://mavenlin.github.io/ai_research_trends/2021/06/04/2021.06.04.html", "isFamilyFriendly": true, "displayUrl": "https://mavenlin.github.io/ai_research_trends/2021/06/04/2021.06.04.html", "snippet": "We show highly competitive results on <b>tabular</b> data, early results on CIFAR-10, and give insight into how the model makes use of the interactions between points. Active Covering arxiv:2106.02552 115 . Heinrich Jiang, Afshin Rostamizadeh **Abstract:** We analyze the problem of active covering, where the learner is given an unlabeled dataset and <b>can</b> sequentially label query examples. The objective is to label query all of the positive examples in the fewest number of total label queries. We ...", "dateLastCrawled": "2022-02-02T15:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "CIKM <b>Proceedings</b> - CIKM 2020", "url": "https://www.cikm2020.org/index.html@p=1283.html", "isFamilyFriendly": true, "displayUrl": "https://www.cikm2020.org/index.html@p=1283.html", "snippet": "Moreover, the <b>learning</b> process of GNNVis is designed as an end-to-end manner and <b>can</b> easily be extended to arbitrary Dimension Reduction methods if the corresponding objective function is given. Based on GNNVis, several typical dimension reduction methods t-SNE, LargeVis, and UMAP are investigated. As a parametric framework, GNNVis is an inherently efficient Visualizer capable of computing the embeddings of large-scale unseen data. To guarantee its scalability in the Training Stage, a novel ...", "dateLastCrawled": "2022-01-26T00:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Solid Edge 2021 - As last saved option | GoLectures | Online Lectures", "url": "https://www.golectures.com/index.php?go=search&q=Solid%20Edge%202021%20-%20As%20last%20saved%20option", "isFamilyFriendly": true, "displayUrl": "https://www.golectures.com/index.php?go=search&amp;q=Solid Edge 2021 - As last saved option", "snippet": "Join us to discover how Solid Edge 2022 provides you with the tools to design at the speed of creative <b>thought</b>, and engage with Siemens experts via live chat. ... The Solid Edge portfolio is an integrated set of powerful, comprehensive, and accessible tools that advance all aspects of the product development process. Solid Edge addresses today\u2019s complexity challenges with automated digital solutions that cultivate creativity and collaboration. By harnessing the latest innovative ...", "dateLastCrawled": "2022-01-22T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Global+marketing+(8th+edition) Chap1-5 | Mariana Nu\u00e3\u00b1ez ... - ID ...", "url": "https://baixardoc.com/documents/globalmarketing8thedition-chap1-5-mariana-nuaez--5d06a71e7b5b7", "isFamilyFriendly": true, "displayUrl": "https://baixardoc.com/documents/globalmarketing8thedition-chap1-5-mariana-nuaez--5d06a...", "snippet": "Global+marketing+(8th+edition) Chap1-5 | Mariana Nu\u00e3\u00b1ez ... - ID:5d06a71e7b5b7. GLOBAL MARKETING EIGHTH EDITION Warren J. Keegan Lubin Graduate School of Business Pace University New York City and Wes...", "dateLastCrawled": "2022-01-22T06:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Process</b> in Reinforcement <b>Learning</b>: Everything You Need ...", "url": "https://neptune.ai/blog/markov-decision-process-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>markov-decision-process</b>-in-reinforcement-<b>learning</b>", "snippet": "It <b>can</b> be used to efficiently calculate the value of a policy and to solve not only <b>Markov Decision</b> Processes, but many other recursive problems. <b>Q-Learning</b> is the <b>learning</b> of Q-values in an environment, which often resembles a <b>Markov Decision Process</b>. It is suitable in cases where the specific probabilities, rewards, and penalties are not ...", "dateLastCrawled": "2022-01-26T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Curriculum <b>Learning</b> via Reward Shaping", "url": "https://kshitijsachan.com/files/csci2951f_final.pdf", "isFamilyFriendly": true, "displayUrl": "https://kshitijsachan.com/files/csci2951f_final.pdf", "snippet": "with increasing complexity, building up to the target task (such as <b>learning</b> <b>to ride</b> a tricycle before <b>a bike</b>). The source tasks should be simple enough to be learned with a sparse reward. This approach is known as curriculum <b>learning</b>. We explore using reward shaping\u2014a specific way to transfer knowledge between tasks\u2014with cur- riculum <b>learning</b> to speed up <b>learning</b>. We specifically focus on how to design a good curriculum and how to learn with multiple source tasks. 1. 2 Background ...", "dateLastCrawled": "2022-01-04T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep <b>Reinforcement Learning Techniques</b> in Diversified Domains: A Survey ...", "url": "https://link.springer.com/article/10.1007/s11831-021-09552-3", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11831-021-09552-3", "snippet": "The value-based <b>Q learning</b> is exploration insensitive (<b>can</b> learn without necessarily following current policy), while the policy-based actor-critic methods <b>can</b> respond to smooth changes in actions on smoothly varying states (it follows current policy). The authors of defined eight criteria that are necessary and sufficient for <b>Q-learning</b> based methods in the continuous domain. The earlier methods failed to fulfil one or more criteria. They proposed a method that uses a neural network and ...", "dateLastCrawled": "2022-01-26T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Learning</b> to Drive <b>a Bicycle Using Reinforcement Learning and Shaping</b>.", "url": "https://www.researchgate.net/publication/221346431_Learning_to_Drive_a_Bicycle_Using_Reinforcement_Learning_and_Shaping", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221346431_<b>Learning</b>_to_Drive_a_Bicycle_Using...", "snippet": "Matthew E. Taylor. Michael Bowling. Reinforcement <b>learning</b> (RL) is a powerful <b>learning</b> paradigm in which agents <b>can</b> learn to maximize sparse and delayed reward signals. Although RL has had many ...", "dateLastCrawled": "2021-12-02T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Modular and Transferable Reinforcement <b>Learning</b> Framework for the ...", "url": "https://www.researchgate.net/publication/355305702_A_Modular_and_Transferable_Reinforcement_Learning_Framework_for_the_Fleet_Rebalancing_Problem", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/355305702_A_Modular_and_Transferable...", "snippet": "As a complex and critical cyber-physical system ( CPS ), the hybrid electric powertrain is significant to mitigate air pollution and improve fuel economy. Energy management strate", "dateLastCrawled": "2021-12-22T04:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An Overview of Reinforcement <b>Learning</b> Methods for Variable Speed Limit ...", "url": "https://www.mdpi.com/2076-3417/10/14/4917/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2076-3417/10/14/4917/htm", "snippet": "Variable Speed Limit (VSL) control systems are widely studied as solutions for improving safety and throughput on urban motorways. Machine <b>learning</b> techniques, specifically Reinforcement <b>Learning</b> (RL) methods, are a promising alternative for setting up VSL since they <b>can</b> learn and react to different traffic situations without knowing the explicit model of the motorway dynamics. However, the efficiency of combined RL-VSL is highly related to the class of the used RL algorithm, and description ...", "dateLastCrawled": "2021-12-20T09:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is <b>the function approximation in reinforcement learning</b>? - Quora", "url": "https://www.quora.com/What-is-the-function-approximation-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-function-approximation-in-reinforcement-learning</b>", "snippet": "Answer: Consider a gridworld domain: Say the goal for the agent is to reach the top-right cell. In such a setting, you <b>can</b> represent a policy using a table \u2014 for each cell (state), the policy is a table that tells you what action to take. Similarly, you <b>can</b> represent the value functions (V and Q...", "dateLastCrawled": "2022-01-15T05:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>News</b> - Sanghani Center for Artificial Intelligence and Data Analytics", "url": "https://dac.cs.vt.edu/news/news-person/", "isFamilyFriendly": true, "displayUrl": "https://dac.cs.vt.edu/<b>news</b>/<b>news</b>-person", "snippet": "The title of her thesis is \u201cPredicting Mutational Pathways of Influenza A H1N1 Virus using <b>Q-learning</b>. Raghuraman has joined LexisNexis Legal and Professional in Raleigh, North Carolina, as a data scientist. Esther Robb, advised by Jia-Bin Huang, has earned a master\u2019s degree in computer engineering. Her primary research interests lie in reinforcement <b>learning</b> and data-efficient <b>learning</b>. The title of her thesis is \u201cData-Efficient <b>Learning</b> in Image Synthesis and Instance Segmentation ...", "dateLastCrawled": "2022-01-18T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "2021.06.04 | AI Research Trends", "url": "https://mavenlin.github.io/ai_research_trends/2021/06/04/2021.06.04.html", "isFamilyFriendly": true, "displayUrl": "https://mavenlin.github.io/ai_research_trends/2021/06/04/2021.06.04.html", "snippet": "We show highly competitive results on <b>tabular</b> data, early results on CIFAR-10, and give insight into how the model makes use of the interactions between points. Active Covering arxiv:2106.02552 115 . Heinrich Jiang, Afshin Rostamizadeh **Abstract:** We analyze the problem of active covering, where the learner is given an unlabeled dataset and <b>can</b> sequentially label query examples. The objective is to label query all of the positive examples in the fewest number of total label queries. We ...", "dateLastCrawled": "2022-02-02T15:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "CIKM <b>Proceedings</b> - CIKM 2020", "url": "https://www.cikm2020.org/index.html@p=1283.html", "isFamilyFriendly": true, "displayUrl": "https://www.cikm2020.org/index.html@p=1283.html", "snippet": "Using five publicly-available datasets spanning three sequential <b>learning</b> settings, we demonstrate that our method consistently achieves higher accuracy with fewer updates <b>compared</b> to state-of-the-art alternatives. We also show the benefits of <b>learning</b> to sparsely-update a large hidden state as opposed to densely-update a small hidden state. As an added benefit, our method <b>can</b> be directly applied to a wide variety of models containing RNN architectures.", "dateLastCrawled": "2022-01-26T00:17:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>introduction to Q-Learning: Reinforcement Learning</b>", "url": "https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/an-<b>introduction-to-q-learning-reinforcement-learning</b>", "snippet": "Reinforcement <b>learning</b> solves a particular kind of problem where decision making is sequential, and the goal is long-term, such as game playing, robotics, resource management, or logistics. For a robot, an environment is a place where it has been put to use. Remember this robot is itself the agent.", "dateLastCrawled": "2022-01-31T09:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Reinforcement <b>Q-Learning</b> from Scratch in Python with OpenAI Gym ...", "url": "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/", "isFamilyFriendly": true, "displayUrl": "https://www.learndatasci.com/tutorials/reinforcement-<b>q-learning</b>-scratch-python-openai-gym", "snippet": "<b>Q-learning</b> is one of the easiest Reinforcement <b>Learning</b> algorithms. The problem with <b>Q-learning</b> however is, once the number of states in the environment are very high, it becomes difficult to implement them with Q table as the size would become very, very large. State of the art techniques uses Deep neural networks instead of the Q-table (Deep Reinforcement <b>Learning</b>). The neural network takes in state information and actions to the input layer and learns to output the right action over the ...", "dateLastCrawled": "2022-02-03T03:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) A comparison of <b>Q-learning</b> and Classifier Systems", "url": "https://www.researchgate.net/publication/2712709_A_comparison_of_Q-learning_and_Classifier_Systems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2712709_A_comparison_of_<b>Q-learning</b>_and...", "snippet": "Watkin&#39;s <b>tabular</b> <b>Q-learning</b> or other more efficient kinds of discrete partition of the state space like Chapman and Kaelbling (1991) or Munos et al. (1994)), to continuous", "dateLastCrawled": "2022-01-29T21:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>GAN Q-learning</b> | DeepAI", "url": "https://deepai.org/publication/gan-q-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>gan-q-learning</b>", "snippet": "Distributional reinforcement <b>learning</b> (distributional RL) has seen empirical success in complex Markov Decision Processes (MDPs) in the setting of nonlinear function approximation. However, there are many different ways in which one can leverage the distributional approach to reinforcement <b>learning</b>. In this paper, we propose <b>GAN Q-learning</b>, a novel distributional RL method based on generative adversarial networks (GANs) and analyze its performance in simple <b>tabular</b> environments, as well as ...", "dateLastCrawled": "2022-01-09T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Branch Prediction as a Reinforcement <b>Learning</b> Problem: Why, How and ...", "url": "https://ease-lab.github.io/ease_website/pubs/RL-BP_MLArchSys21.pdf", "isFamilyFriendly": true, "displayUrl": "https://ease-lab.github.io/ease_website/pubs/RL-BP_MLArchSys21.pdf", "snippet": "A. <b>Tabular</b> Methods: <b>Q-Learning</b> A number of <b>tabular</b> RL methods exist; most popular ones include TD-<b>learning</b> [15], SARSA [14], <b>Q-Learning</b> [17] and double <b>Q-Learning</b> [6]. Here we focus on the <b>Q-Learning</b> algorithm that provides speci\ufb01c convergence guarantees [17]3. <b>Q-Learning</b> stores the Q-values Q(s;a) for every state and action pair in a \ufb01xed-sized table. Given a state sfrom the environment, <b>Q-Learning</b> predicts the action greedily using the policy \u02c7 greedy (s). The <b>Q-Learning</b> update rule ...", "dateLastCrawled": "2021-11-20T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Reinforcement <b>learning</b>: <b>Temporal-Difference</b>, SARSA, <b>Q-Learning</b> ...", "url": "https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/reinforcement-<b>learning</b>-<b>temporal-difference</b>-sarsa-q...", "snippet": "Source: Introduction to Reinforcement <b>learning</b> by Sutton and Barto \u2014Chapter 6. The action A\u2019 in the above algorithm is given by following the same policy (\u03b5-greedy over the Q values) because SARSA is an on-policy method.. \u03b5-greedy policy. Epsilon-greedy policy is this: Generate a random number r \u2208[0,1]; If r&lt;\u03b5 choose an action derived from the Q values (which yields the maximum utility); Else choose a random action", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Q-learning</b> with Logarithmic Regret | DeepAI", "url": "https://deepai.org/publication/q-learning-with-logarithmic-regret", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>q-learning</b>-with-logarithmic-regret", "snippet": "<b>Q-learning</b> (Watkins and Dayan, 1992) is one of the most popular classes of methods for solving reinforcement <b>learning</b> (RL) problems. <b>Q-learning</b> tries to estimate the optimal state-action value function (. Q-function).With a Q-function, at every state, one can greedily choose the action with the largest Q value to interact with the RL environment while achieving near optimal expected cumulative rewards in the long run. Compared to another popular classes of methods, e.g., model-based RL, Q ...", "dateLastCrawled": "2022-01-27T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>PyTorch Tabular \u2013 A Framework for Deep Learning for Tabular Data</b> \u2013 Deep ...", "url": "https://deep-and-shallow.com/2021/01/27/pytorch-tabular-a-framework-for-deep-learning-for-tabular-data/", "isFamilyFriendly": true, "displayUrl": "https://deep-and-shallow.com/2021/01/27/<b>pytorch-tabular-a-framework-for</b>-deep-<b>learning</b>...", "snippet": "It is common knowledge that Gradient Boosting models, more often than not, kick the asses of every other <b>machine</b> <b>learning</b> models when it comes to <b>Tabular</b> Data.I have written extensively about Gradient Boosting, the theory behind and covered the different implementations like XGBoost, LightGBM, CatBoost, NGBoost etc. in detail. The unreasonable effectiveness of Deep <b>Learning</b> that was displayed in many other modalities \u2013 like text and image- haven not been demonstrated in <b>tabular</b> data.", "dateLastCrawled": "2022-01-29T08:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On using Huber loss in (Deep) <b>Q-learning</b> | \u30e4\u30ed\u30df\u30eb", "url": "https://jaromiru.wordpress.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://jaromiru.wordpress.com/2017/05/27/on-using-huber-loss-in-deep-<b>q-learning</b>", "snippet": "<b>MACHINE</b> <b>LEARNING</b> &amp; AI. Menu. Let\u2019s make a DQN. Theory; Implementation; Debugging; Full DQN; Double <b>Learning</b> and Prioritized Experience Replay; Let\u2019s make an A3C. Theory; Implementation; About me; On using Huber loss in (Deep) <b>Q-learning</b>. Posted on May 27, 2017 May 30, 2017 by \u30e4\u30ed\u30df\u30eb. I\u2019ve been recently working on a problem where I put a plain DQN to use. The problem is very simple, deterministic, partially observable and states are quite low-dimensional. The agent however can\u2019t ...", "dateLastCrawled": "2021-12-26T09:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why doesn&#39;t <b>Q-learning</b> converge when using function approximation ...", "url": "https://ai.stackexchange.com/questions/11679/why-doesnt-q-learning-converge-when-using-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/11679/why-doesnt-<b>q-learning</b>-converge-when-using...", "snippet": "In <b>tabular</b> <b>Q-learning</b>, when we update a Q-value, other Q-values in the table don&#39;t get affected by this. But in neural networks, one update to the weights aiming to alter one Q-value ends up affecting other Q-values whose states look similar (since neural networks learn a continuous function that is smooth)", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(tabular q-learning)  is like +(learning to ride a bike)", "+(tabular q-learning) is similar to +(learning to ride a bike)", "+(tabular q-learning) can be thought of as +(learning to ride a bike)", "+(tabular q-learning) can be compared to +(learning to ride a bike)", "machine learning +(tabular q-learning AND analogy)", "machine learning +(\"tabular q-learning is like\")", "machine learning +(\"tabular q-learning is similar\")", "machine learning +(\"just as tabular q-learning\")", "machine learning +(\"tabular q-learning can be thought of as\")", "machine learning +(\"tabular q-learning can be compared to\")"]}