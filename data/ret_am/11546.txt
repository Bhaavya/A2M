{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Path-Specific Counterfactual Fairness</b> | DeepAI", "url": "https://deepai.org/publication/path-specific-counterfactual-fairness", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>path-specific-counterfactual-fairness</b>", "snippet": "For example, in the Berkeley alleged sex <b>bias</b> case, a and a \u2032 would correspond to female and male applicants respectively. Under these assumptions, <b>path-specific counterfactual fairness</b> is achieved when the difference between the causal effects along the unfair pathways for A = a and A = a \u2032 is small. In the remainder of the paper, we will ...", "dateLastCrawled": "2022-01-19T14:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Snapshot of the Frontiers <b>of Fairness in Machine Learning</b> | May 2020 ...", "url": "https://cacm.acm.org/magazines/2020/5/244336-a-snapshot-of-the-frontiers-of-fairness-in-machine-learning/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2020/5/244336-a-snapshot-of-the-frontiers-of-<b>fairness</b>...", "snippet": "<b>Fairness</b> concerns typically surface precisely in settings where the available <b>training</b> <b>data</b> is already contaminated by <b>bias</b>. The <b>data</b> itself is often a product of social and historical process that operated to the disadvantage of certain groups. When trained in such <b>data</b>, off-the-shelf machine learning techniques may reproduce, reinforce, and potentially exacerbate existing biases. Understanding how <b>bias</b> arises in the <b>data</b>, and how to correct for it, are fundamental challenges in the study ...", "dateLastCrawled": "2022-01-29T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Snapshot of the Frontiers of Fairness in Machine Learning</b> | May 2020 ...", "url": "https://m-cacm.acm.org/magazines/2020/5/244336-a-snapshot-of-the-frontiers-of-fairness-in-machine-learning/fulltext?mobile=true", "isFamilyFriendly": true, "displayUrl": "https://m-cacm.acm.org/magazines/2020/5/244336-a-<b>snapshot-of-the-frontiers-of</b>-<b>fairness</b>...", "snippet": "<b>Fairness</b> concerns typically surface precisely in settings where the available <b>training</b> <b>data</b> is already contaminated by <b>bias</b>. The <b>data</b> itself is often a product of social and historical process that operated to the disadvantage of certain groups. When trained in such <b>data</b>, off-the-shelf machine learning techniques may reproduce, reinforce, and potentially exacerbate existing biases. Understanding how <b>bias</b> arises in the <b>data</b>, and how to correct for it, are fundamental challenges in the study ...", "dateLastCrawled": "2021-12-04T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fairness and Bias in</b> Machine Learning Algorithms | District <b>Data</b> Labs", "url": "https://www.districtdatalabs.com/fairness-and-bias-in-algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.district<b>data</b>labs.com/<b>fairness-and-bias-in-algorithms</b>", "snippet": "In this post, I want to explore whether we can use the tools in Yellowbrick to \u201caudit\u201d a black-box algorithm and assess claims about <b>fairness</b> and <b>bias</b>. At the same time, I prefer R for most visualization tasks. Fortunately, the new reticulate package has allowed Python part-timers, <b>like</b> me, to get something close to the best of both worlds.", "dateLastCrawled": "2022-01-23T03:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The <b>Frontiers of Fairness in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-frontiers-of-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>frontiers-of-fairness-in-machine-learning</b>", "snippet": "<b>Fairness</b> concerns typically surface precisely in settings where the available <b>training</b> <b>data</b> is already contaminated by <b>bias</b>. The <b>data</b> itself is often a product of social and historical process that operated to the disadvantage of certain groups. When trained in such <b>data</b>, off-the-shelf machine learning techniques may reproduce, reinforce, and potentially exacerbate existing biases. Understanding how <b>bias</b> arises in the <b>data</b>, and how to correct for it, are fundamental challenges in the study ...", "dateLastCrawled": "2022-01-19T04:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Frontiers | Addressing <b>Fairness</b>, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "Address and Respect <b>Bias</b>: <b>Bias</b> should be examined at each level of the computation, and the individual features used <b>in the training</b> <b>data</b> should themselves be examined <b>for bias</b>. While the <b>bias</b> found in other application domains of machine learning (finance, employment, law enforcement, etc.) may often be due to sampling <b>bias</b> or implicit cultural <b>bias</b>, the domain of health also contains true systematic <b>bias</b> inherent in biological processes which may not be possible to mitigate or \u201crepair ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Clicks can be Cheating: <b>Counterfactual</b> Recommendation for Mitigating ...", "url": "https://dl.acm.org/doi/10.1145/3404835.3462962", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3404835.3462962", "snippet": "Marco Morik, Ashudeep Singh, Jessica Hong, and Thorsten Joachims. 2020. Controlling <b>Fairness</b> and <b>Bias</b> in Dynamic Learning-to-Rank. In SIGIR. ACM, 429--438. Google Scholar; Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. 2020. <b>Counterfactual</b> VQA: A Cause-Effect Look at Language <b>Bias</b>. In arXiv:2006.04315 . Google Scholar; Zohreh Ovaisi, Ragib Ahsan, Yifan Zhang, Kathryn Vasilaky, and Elena Zheleva. 2020. <b>Correcting</b> for Selection <b>Bias</b> in Learning-to-Rank ...", "dateLastCrawled": "2022-01-30T19:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Trustworthy Machine Learning - Kush R. Varshney - Chapter 10: <b>Fairness</b>", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "snippet": "The <b>training</b> <b>data</b> is society.\u201d \u2014 M. C. Hammer, musician and technology consultant. Social <b>bias</b> enters claims <b>data</b> in a few ways. First, you might think that patients who visit doctors a lot and get many prescriptions filled, i.e. utilize the health care system a lot, are sicker and thus more appropriate candidates for care management. While it is directionally true that greater health care utilization implies a sicker patient, it is not true when comparing patients across populations ...", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Counterfactual</b> Attention Learning for Fine-Grained Visual ...", "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Rao_Counterfactual_Attention_Learning_for_Fine-Grained_Visual_Categorization_and_Re-Identification_ICCV_2021_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content/ICCV2021/papers/Rao_<b>Counterfactual</b>_Attention...", "snippet": "tion to mitigate the effects of <b>data</b> biases. Because of the lack of effective tool to evaluate the quality of attentions quantitatively, <b>correcting</b> misleading attentions is a very challenging task. One straightforward solution is to use extra annotations <b>like</b> bounding boxes or segmentation masks to obtain the regions of interest explicitly such ...", "dateLastCrawled": "2022-01-31T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "4. <b>Fairness</b> Pre-Processing - Practical <b>Fairness</b> [Book]", "url": "https://www.oreilly.com/library/view/practical-fairness/9781492075721/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/practical-<b>fairness</b>/9781492075721/ch04.html", "snippet": "Chapter 4. <b>Fairness</b> Pre-Processing. As discussed in the previous chapter, <b>fairness</b> can affect three stages of the <b>data</b> modeling pipeline. This chapter focuses on the earliest stage, adjusting the way that <b>data</b> is translated into inputs for a machine learning <b>training</b> process, also called pre-processing the <b>data</b>.. The advantages of pre-processing a <b>data</b> set are numerous.", "dateLastCrawled": "2022-01-13T02:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness and Bias in</b> Machine Learning Algorithms | District <b>Data</b> Labs", "url": "https://www.districtdatalabs.com/fairness-and-bias-in-algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.district<b>data</b>labs.com/<b>fairness-and-bias-in-algorithms</b>", "snippet": "In situations with pre-existing structural inequities, group <b>fairness</b> can be <b>similar</b> to the concept of \u201c<b>fairness</b> through awareness\u201d \u2014 the only way to treat groups equally is to take people\u2019s membership in those groups into account and correct for an existing unleveled playing field. At the other end of the spectrum, there\u2019s individual ...", "dateLastCrawled": "2022-01-23T03:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Frontiers | Facing the Challenges of Developing Fair Risk Scoring ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2021.681915/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2021.681915", "snippet": "A definition of <b>counterfactual</b> <b>fairness</b> is given together with an algorithm that results in a fair scoring model. The concepts are illustrated by means of a transparent simulation and a popular real world example, the German Credit <b>data</b> using traditional scorecard models based on logistic regression and weight of evidence variable pre-transform. In contrast to previous studies in the field for our study a corrected version of the <b>data</b> is presented and used. With help of the simulation the ...", "dateLastCrawled": "2022-01-14T13:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Addressing <b>Fairness</b>, <b>Bias</b>, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "Address and Respect <b>Bias</b>: <b>Bias</b> should be examined at each level of the computation, and the individual features used <b>in the training</b> <b>data</b> should themselves be examined <b>for bias</b>. While the <b>bias</b> found in other application domains of machine learning (finance, employment, law enforcement, etc.) may often be due to sampling <b>bias</b> or implicit cultural <b>bias</b>, the domain of health also contains true systematic <b>bias</b> inherent in biological processes which may not be possible to mitigate or \u201crepair ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bias</b> and ethical considerations in machine learning and the automation ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7442146/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7442146", "snippet": "Such tools can assist with <b>data</b> preparation to debias labelling 20 or word embeddings 21; post hoc assessment of <b>bias</b> utilising newer methods such as <b>counterfactual</b> <b>fairness</b>; and re-weighing of <b>data</b> during <b>training</b> to mitigate algorithmic <b>bias</b>. 22 The work of Buolamwini and colleagues, including the work of the Algorithmic Justice League, deserves specific highlighting and commendation. 9 The US National Institute of Health&#39;s All of Us Research Program has been especially designed to address ...", "dateLastCrawled": "2021-11-16T04:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Snapshot of the Frontiers <b>of Fairness in Machine Learning</b> | May 2020 ...", "url": "https://cacm.acm.org/magazines/2020/5/244336-a-snapshot-of-the-frontiers-of-fairness-in-machine-learning/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2020/5/244336-a-snapshot-of-the-frontiers-of-<b>fairness</b>...", "snippet": "<b>Fairness</b> concerns typically surface precisely in settings where the available <b>training</b> <b>data</b> is already contaminated by <b>bias</b>. The <b>data</b> itself is often a product of social and historical process that operated to the disadvantage of certain groups. When trained in such <b>data</b>, off-the-shelf machine learning techniques may reproduce, reinforce, and potentially exacerbate existing biases. Understanding how <b>bias</b> arises in the <b>data</b>, and how to correct for it, are fundamental challenges in the study ...", "dateLastCrawled": "2022-01-29T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>Snapshot of the Frontiers of Fairness in Machine Learning</b> | May 2020 ...", "url": "https://m-cacm.acm.org/magazines/2020/5/244336-a-snapshot-of-the-frontiers-of-fairness-in-machine-learning/fulltext?mobile=true", "isFamilyFriendly": true, "displayUrl": "https://m-cacm.acm.org/magazines/2020/5/244336-a-<b>snapshot-of-the-frontiers-of</b>-<b>fairness</b>...", "snippet": "<b>Fairness</b> concerns typically surface precisely in settings where the available <b>training</b> <b>data</b> is already contaminated by <b>bias</b>. The <b>data</b> itself is often a product of social and historical process that operated to the disadvantage of certain groups. When trained in such <b>data</b>, off-the-shelf machine learning techniques may reproduce, reinforce, and potentially exacerbate existing biases. Understanding how <b>bias</b> arises in the <b>data</b>, and how to correct for it, are fundamental challenges in the study ...", "dateLastCrawled": "2021-12-04T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Cyberbullying Detection with Fairness Constraints</b> | DeepAI", "url": "https://deepai.org/publication/cyberbullying-detection-with-fairness-constraints", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>cyberbullying-detection-with-fairness-constraints</b>", "snippet": "Identifying and <b>Correcting</b> Label <b>Bias</b> in Machine Learning ... Once the model is trained to satisfy the required <b>fairness</b> constraints on the <b>training</b> <b>data</b> for the desired groups, access to group attributes is not needed during inference. All trainings (in total eight - one baseline and one constrained for each dataset) have been performed in a mini-batch manner for 75 epochs with a batch size of 128 with Adam optimizer (learning rate of . 5 \u00d7 10 \u2212 4). In order to avoid overfitting, models ...", "dateLastCrawled": "2022-01-31T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Trustworthy Machine Learning - Kush R. Varshney - Chapter 10: <b>Fairness</b>", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "snippet": "The <b>training</b> <b>data</b> is society.\u201d \u2014 M. C. Hammer, musician and technology consultant. Social <b>bias</b> enters claims <b>data</b> in a few ways. First, you might think that patients who visit doctors a lot and get many prescriptions filled, i.e. utilize the health care system a lot, are sicker and thus more appropriate candidates for care management. While it is directionally true that greater health care utilization implies a sicker patient, it is not true when comparing patients across populations ...", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Mitigating <b>Bias</b> in AI Using Debias-GAN - WWT", "url": "https://www.wwt.com/article/mitigating-bias-in-ai-using-debias-gan", "isFamilyFriendly": true, "displayUrl": "https://www.wwt.com/article/mitigating-<b>bias</b>-in-ai-using-de<b>bias</b>-gan", "snippet": "After the classifier is trained, we calculated the <b>fairness</b> metrics on real tweets that are not included <b>in the training</b> <b>data</b>. In most cases, we observed improved <b>fairness</b> metrics (Figure 13, BA, DEOs) with different mix ratios and moderate decrease in the model classification performance (Figure 13, AUC: area under the curve). Interestingly, the impact of the number of synthetic tweets in the input <b>data</b> on the <b>fairness</b> metrics seems to be nonmonotonic and differs from metric to metric ...", "dateLastCrawled": "2022-02-03T03:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Understanding and Mitigating Annotation <b>Bias</b> in Facial Expression ...", "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Understanding_and_Mitigating_Annotation_Bias_in_Facial_Expression_Recognition_ICCV_2021_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Understanding_and...", "snippet": "In the case where the <b>data</b> labels are historically biased, <b>data</b> massaging is the most commonly used technique. This includes directly <b>correcting</b> the labels by changing them prior to <b>training</b> [55, 39], or use some weights or sampling techniques during <b>training</b> [40, 41]. Annotation <b>bias</b>. For tabulated <b>data</b>, historical label <b>bias</b> is a well-known ...", "dateLastCrawled": "2022-02-01T06:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "When Worlds Collide: Integrating Different <b>Counterfactual</b> Assumptions ...", "url": "https://www.researchgate.net/publication/324600594_When_Worlds_Collide_Integrating_Different_Counterfactual_Assumptions_in_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600594_When_Worlds_Collide_Integrating...", "snippet": "Recently, a few works extend <b>counterfactual</b> <b>fairness</b> to graph <b>data</b>, but most of them neglect the following facts that <b>can</b> lead to biases: 1) the sensitive attributes of each node&#39;s neighbors may ...", "dateLastCrawled": "2022-01-19T23:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness</b> in <b>Risk Assessment Instruments: Post-Processing</b> to Achieve ...", "url": "https://www.researchgate.net/publication/349697758_Fairness_in_Risk_Assessment_Instruments_Post-Processing_to_Achieve_Counterfactual_Equalized_Odds", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349697758_<b>Fairness</b>_in_Risk_Assessment...", "snippet": "Post-processing in algorithmic <b>fairness</b> is a versatile approach for <b>correcting</b> <b>bias</b> in ML systems that are already used in production. The main appeal of post-processing is that it avoids ...", "dateLastCrawled": "2022-01-24T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Trustworthy Machine Learning - Kush R. Varshney - Chapter 10: <b>Fairness</b>", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "snippet": "The most obvious source of unfairness is unwanted <b>bias</b>, specifically social <b>bias</b> in the measurement process (going from the construct space to the observed space) and representation <b>bias</b> in the sampling process (going from the observed space to the raw <b>data</b> space), in figure 10.1. (This is a repetition of figure 9.2 and an extension of figure 4.3 where the concepts of construct space and observed space were first introduced.)", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Frontiers | Addressing <b>Fairness</b>, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "While the eventual optimal tuning of an algorithm <b>can</b> thus depend on many factors, including the local system of laws and community values, the overall goal of this paper is to introduce and define the notions of <b>bias</b>, <b>fairness</b>, and appropriate use, as it pertains to machine learning in global health, and also to illustrate how a given machine learning model <b>can</b> be analyzed to identify and quantify issues of <b>bias</b> and <b>fairness</b>. With this goal in mind, this paper is intended for the audience ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Affirmative</b> Algorithms: The Legal Grounds for <b>Fairness</b> as Awareness ...", "url": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang/", "isFamilyFriendly": true, "displayUrl": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang", "snippet": "Understanding the source of <b>bias</b> <b>can</b> clarify which definitions, if any, are most appropriate given the historical source of discrimination. In the CS courses example above, if we did not have any knowledge about the historical sources of discrimination, we might simply use different score thresholds for different groups as a way to address the hiring gaps. Knowing that \u201cnumber of CS courses\u201d is less predictive of success as a software engineer for certain groups because of the alienation ...", "dateLastCrawled": "2022-01-31T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Addressing Fairness, Bias, and Appropriate</b> Use of Artificial ...", "url": "https://europepmc.org/article/PMC/PMC8107824", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8107824", "snippet": "While the eventual optimal tuning of an algorithm <b>can</b> thus depend on many factors, including the local system of laws and community values, the overall goal of this paper is to introduce and define the notions of <b>bias</b>, <b>fairness</b>, and appropriate use, as it pertains to machine learning in global health, and also to illustrate how a given machine learning model <b>can</b> be analyzed to identify and quantify issues of <b>bias</b> and <b>fairness</b>. With this goal in mind, this paper is intended for the audience ...", "dateLastCrawled": "2021-06-01T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "4. <b>Fairness</b> Pre-Processing - Practical <b>Fairness</b> [Book]", "url": "https://www.oreilly.com/library/view/practical-fairness/9781492075721/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/practical-<b>fairness</b>/9781492075721/ch04.html", "snippet": "Chapter 4. <b>Fairness</b> Pre-Processing. As discussed in the previous chapter, <b>fairness</b> <b>can</b> affect three stages of the <b>data</b> modeling pipeline. This chapter focuses on the earliest stage, adjusting the way that <b>data</b> is translated into inputs for a machine learning <b>training</b> process, also called pre-processing the <b>data</b>.. The advantages of pre-processing a <b>data</b> set are numerous.", "dateLastCrawled": "2022-01-13T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "An Adaptive Boosting Technique to Mitigate Popularity <b>Bias</b> in ...", "url": "https://deepai.org/publication/an-adaptive-boosting-technique-to-mitigate-popularity-bias-in-recommender-system", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/an-adaptive-boosting-technique-to-mitigate-popularity...", "snippet": "Model-Agnostic <b>Counterfactual</b> Reasoning for Eliminating Popularity <b>Bias</b> in Recommender System ... This paper considers the most important <b>fairness</b> issue, namely the popularity <b>bias</b> at the item level. The items rated by most of the users or have received high ratings are known as popular items, and the items rated by very few users or not rated at all are known as non-popular items. The underlying notion behind popularity <b>bias</b> is that people are more inclined to offer comments on mainstream ...", "dateLastCrawled": "2022-01-22T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Accessing Artificial Intelligence for Clinical Decision-Making", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8521931/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8521931", "snippet": "Broadly, such <b>bias</b> <b>can</b> originate from the <b>data</b> used for model <b>training</b> and testing, as well as the mechanics of the model itself . <b>Bias</b> originating from <b>data</b> <b>can</b> be pernicious; for instance, work by Weber et al. found that simply filtering for \u201ccomplete\u201d EHRs, a common strategy for managing missing <b>data</b>, introduced a <b>bias</b> toward older patients who were more likely female ( 50 ).", "dateLastCrawled": "2021-12-07T10:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Artificial Intelligence in Hiring: Problem</b> or Solution? | CLS Blue Sky Blog", "url": "https://clsbluesky.law.columbia.edu/2020/08/07/artificial-intelligence-in-hiring-problem-or-solution/", "isFamilyFriendly": true, "displayUrl": "https://clsbluesky.law.columbia.edu/2020/08/07/artificial-intelligence-in-hiring...", "snippet": "In my paper, I argue that you <b>can</b> create better results by using known sources for <b>data</b> and looking for and <b>correcting</b> <b>bias</b> in the <b>data</b> set. For example, if a <b>data</b> set is skewed in favor of one gender or one race, it <b>can</b> be balanced through boosting and reduction. Boosting would involve replicating the records of the lesser represented group ...", "dateLastCrawled": "2022-01-12T01:43:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Counterfactual risk assessments, evaluation, and fairness</b> | Request PDF", "url": "https://www.researchgate.net/publication/338841792_Counterfactual_risk_assessments_evaluation_and_fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/338841792_<b>Counterfactual</b>_risk_assessments...", "snippet": "A novel method for estimating risk assessment based on <b>counterfactual</b> definition of <b>fairness</b> is presented in Coston et al. (2020) preprocessing methodology that <b>can</b> be used to mitigate <b>bias</b> in ...", "dateLastCrawled": "2022-01-14T06:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness and Bias in</b> Machine Learning Algorithms | District <b>Data</b> Labs", "url": "https://www.districtdatalabs.com/fairness-and-bias-in-algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.district<b>data</b>labs.com/<b>fairness-and-bias-in-algorithms</b>", "snippet": "In this post, I want to explore whether we <b>can</b> use the tools in Yellowbrick to \u201caudit\u201d a black-box algorithm and assess claims about <b>fairness</b> and <b>bias</b>. At the same time, I prefer R for most visualization tasks. Fortunately, the new reticulate package has allowed Python part-timers, like me, to get something close to the best of both worlds.", "dateLastCrawled": "2022-01-23T03:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "When Worlds Collide: Integrating Different <b>Counterfactual</b> Assumptions ...", "url": "https://www.researchgate.net/publication/324600594_When_Worlds_Collide_Integrating_Different_Counterfactual_Assumptions_in_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600594_When_Worlds_Collide_Integrating...", "snippet": "Recently, a few works extend <b>counterfactual</b> <b>fairness</b> to graph <b>data</b>, but most of them neglect the following facts that <b>can</b> lead to biases: 1) the sensitive attributes of each node&#39;s neighbors may ...", "dateLastCrawled": "2022-01-19T23:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Snapshot of the Frontiers <b>of Fairness in Machine Learning</b> | May 2020 ...", "url": "https://cacm.acm.org/magazines/2020/5/244336-a-snapshot-of-the-frontiers-of-fairness-in-machine-learning/fulltext", "isFamilyFriendly": true, "displayUrl": "https://cacm.acm.org/magazines/2020/5/244336-a-snapshot-of-the-frontiers-of-<b>fairness</b>...", "snippet": "<b>Fairness</b> concerns typically surface precisely in settings where the available <b>training</b> <b>data</b> is already contaminated by <b>bias</b>. The <b>data</b> itself is often a product of social and historical process that operated to the disadvantage of certain groups. When trained in such <b>data</b>, off-the-shelf machine learning techniques may reproduce, reinforce, and potentially exacerbate existing biases. Understanding how <b>bias</b> arises in the <b>data</b>, and how to correct for it, are fundamental challenges in the study ...", "dateLastCrawled": "2022-01-29T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The <b>Frontiers of Fairness in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-frontiers-of-fairness-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>frontiers-of-fairness-in-machine-learning</b>", "snippet": "<b>Fairness</b> concerns typically surface precisely in settings where the available <b>training</b> <b>data</b> is already contaminated by <b>bias</b>. The <b>data</b> itself is often a product of social and historical process that operated to the disadvantage of certain groups. When trained in such <b>data</b>, off-the-shelf machine learning techniques may reproduce, reinforce, and potentially exacerbate existing biases. Understanding how <b>bias</b> arises in the <b>data</b>, and how to correct for it, are fundamental challenges in the study ...", "dateLastCrawled": "2022-01-19T04:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "4. <b>Fairness</b> Pre-Processing - Practical <b>Fairness</b> [Book]", "url": "https://www.oreilly.com/library/view/practical-fairness/9781492075721/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/practical-<b>fairness</b>/9781492075721/ch04.html", "snippet": "Chapter 4. <b>Fairness</b> Pre-Processing. As discussed in the previous chapter, <b>fairness</b> <b>can</b> affect three stages of the <b>data</b> modeling pipeline. This chapter focuses on the earliest stage, adjusting the way that <b>data</b> is translated into inputs for a machine learning <b>training</b> process, also called pre-processing the <b>data</b>.. The advantages of pre-processing a <b>data</b> set are numerous.", "dateLastCrawled": "2022-01-13T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Trustworthy Machine Learning - Kush R. Varshney - Chapter 10: <b>Fairness</b>", "url": "http://www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "isFamilyFriendly": true, "displayUrl": "www.trustworthymachinelearning.com/trustworthymachinelearning-10.htm", "snippet": "The most obvious source of unfairness is unwanted <b>bias</b>, specifically social <b>bias</b> in the measurement process (going from the construct space to the observed space) and representation <b>bias</b> in the sampling process (going from the observed space to the raw <b>data</b> space), in figure 10.1. (This is a repetition of figure 9.2 and an extension of figure 4.3 where the concepts of construct space and observed space were first introduced.)", "dateLastCrawled": "2022-01-07T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Cyberbullying Detection with Fairness Constraints</b> | DeepAI", "url": "https://deepai.org/publication/cyberbullying-detection-with-fairness-constraints", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>cyberbullying-detection-with-fairness-constraints</b>", "snippet": "Identifying and <b>Correcting</b> Label <b>Bias</b> in Machine Learning ... Once the model is trained to satisfy the required <b>fairness</b> constraints on the <b>training</b> <b>data</b> for the desired groups, access to group attributes is not needed during inference. All trainings (in total eight - one baseline and one constrained for each dataset) have been performed in a mini-batch manner for 75 epochs with a batch size of 128 with Adam optimizer (learning rate of . 5 \u00d7 10 \u2212 4). In order to avoid overfitting, models ...", "dateLastCrawled": "2022-01-31T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Frontiers | Addressing <b>Fairness</b>, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "While the eventual optimal tuning of an algorithm <b>can</b> thus depend on many factors, including the local system of laws and community values, the overall goal of this paper is to introduce and define the notions of <b>bias</b>, <b>fairness</b>, and appropriate use, as it pertains to machine learning in global health, and also to illustrate how a given machine learning model <b>can</b> be analyzed to identify and quantify issues of <b>bias</b> and <b>fairness</b>. With this goal in mind, this paper is intended for the audience ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Mitigating <b>Bias</b> in AI Using Debias-GAN - WWT", "url": "https://www.wwt.com/article/mitigating-bias-in-ai-using-debias-gan", "isFamilyFriendly": true, "displayUrl": "https://www.wwt.com/article/mitigating-<b>bias</b>-in-ai-using-de<b>bias</b>-gan", "snippet": "After the classifier is trained, we calculated the <b>fairness</b> metrics on real tweets that are not included <b>in the training</b> <b>data</b>. In most cases, we observed improved <b>fairness</b> metrics (Figure 13, BA, DEOs) with different mix ratios and moderate decrease in the model classification performance (Figure 13, AUC: area under the curve). Interestingly, the impact of the number of synthetic tweets in the input <b>data</b> on the <b>fairness</b> metrics seems to be nonmonotonic and differs from metric to metric ...", "dateLastCrawled": "2022-02-03T03:23:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Responsible AI practices \u2013 Google AI", "url": "https://ai.google/responsibilities/responsible-ai-practices/?category=fairness", "isFamilyFriendly": true, "displayUrl": "https://ai.google/responsibilities/responsible-ai-practices/?category=<b>fairness</b>", "snippet": "A case-study on the application of <b>fairness</b> in <b>machine</b> <b>learning</b> research to a production classification system, and new insights in how to measure and address algorithmic <b>fairness</b> issues. Research paper <b>Counterfactual</b> <b>fairness</b> in text classification through robustness Provides and compares multiple approaches for addressing <b>counterfactual</b> <b>fairness</b> issues in text models. Research paper Model Cards for Model Reporting Proposes a framework to encourage transparent model reporting. Research ...", "dateLastCrawled": "2022-02-02T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Counterfactual Generation and Fairness Evaluation</b> Using Adversarially ...", "url": "https://www.researchgate.net/publication/344294835_Counterfactual_Generation_and_Fairness_Evaluation_Using_Adversarially_Learned_Inference", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344294835_<b>Counterfactual</b>_Generation_and...", "snippet": "<b>Counterfactual</b> examples for an input---perturbations that change specific features but not others---have been shown to be useful for evaluating explainability and <b>fairness</b> of <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-04T04:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Counterfactual</b> Explanation of <b>Machine</b> <b>Learning</b> Survival Models - IOS Press", "url": "https://content.iospress.com/articles/informatica/infor468", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/informatica/infor468", "snippet": "A method for <b>counterfactual</b> explanation of <b>machine</b> <b>learning</b> survival models is proposed. One of the difficulties of solving the <b>counterfactual</b> explanation problem is that the classes of examples are implicitly defined through outcomes of a <b>machine</b> <b>learning</b> survival model in the form of survival functions. A condition that establishes the difference between survival functions of the original example and the <b>counterfactual</b> is introduced. This condition is based on using a distance between mean ...", "dateLastCrawled": "2022-01-15T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "A lot of what is discussed in the <b>machine</b> <b>learning</b> literature touches on <b>fairness</b> (or rather equivalence in certain outcomes) between groups, yet this narrowly constricts <b>fairness</b> to the notion of equality. Of course, we should think about <b>fairness</b> in the context of prejudiced groups, but we should also ask whether it is fair to an individual. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[PDF] A Survey on Bias and <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | Semantic Scholar", "url": "https://www.semanticscholar.org/paper/A-Survey-on-Bias-and-Fairness-in-Machine-Learning-Mehrabi-Morstatter/0090023afc66cd2741568599057f4e82b566137c", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/A-Survey-on-Bias-and-<b>Fairness</b>-in-<b>Machine</b>...", "snippet": "This survey investigated different real-world applications that have shown biases in various ways, and created a taxonomy for <b>fairness</b> definitions that <b>machine</b> <b>learning</b> researchers have defined to avoid the existing bias in AI systems. With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for <b>fairness</b> has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive ...", "dateLastCrawled": "2022-01-29T11:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "dimensions of <b>machine</b> Causality and the normative", "url": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "isFamilyFriendly": true, "displayUrl": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "snippet": "dimensions of <b>machine</b> <b>learning</b> Joshua Loftus (LSE Statistics) High level intro Causality, what is it good for? Causal <b>fairness</b> In prediction and ranking tasks, and with intersectionality Designing interventions Optimal fair policies, causal interference Concluding thoughts 2 / 27. Tech solutionism, using ML/AI in every situation 3 / 27. Imagination Albert Einstein: Imagination is more important than knowledge. For knowledge is limited, whereas imagination [...] stimulat[es] progress, giving ...", "dateLastCrawled": "2022-01-11T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Stable <b>Learning</b> and its Causal Implication", "url": "http://pengcui.thumedialab.com/papers/Stable%20Learning-tutorial-valse2021.pdf", "isFamilyFriendly": true, "displayUrl": "pengcui.thumedialab.com/papers/Stable <b>Learning</b>-tutorial-valse2021.pdf", "snippet": "Application --- <b>counterfactual</b> visual explanations ... Goyal, Yash, et al. &quot;<b>Counterfactual</b> visual explanations.&quot; International Conference on <b>Machine</b> <b>Learning</b>. PMLR, 2019. Explainability with Causality Application --- causal recommendation 17 He et al. \u201dCollaborative Causal Filtering for Out-of-Distribution Recommendation.&quot; Under review. Caual structure among user features and item features Example . Explainability and OOD \u2022Explainability would be a side product when pursuing OOD with ...", "dateLastCrawled": "2022-01-28T03:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Mitigating Political Bias in Language Models Through Reinforced Calibration", "url": "https://www.cs.dartmouth.edu/~rbliu/aaai_copy.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.dartmouth.edu/~rbliu/aaai_copy.pdf", "snippet": "\ufb01rst proposed <b>counterfactual</b> <b>fairness</b>, which treats data samples equally in actual and <b>counterfactual</b> demographic groups. Zhao et al. mitigated gender bias by augmenting original data with gender-swapping and training a unbiased system on the union of two datasets. Other augmentation techniques have reduced gender bias in hate speech detec-tion (Park, Shin, and Fung 2018; Liu et al. 2020), knowledge graph building (Mitchell et al. 2019) and <b>machine</b> transla-tion (Stanovsky, Smith, and ...", "dateLastCrawled": "2022-01-28T20:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Counterfactual Fairness \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1703.06856/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.06856", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however, previous decisions have been made that are unfairly biased against certain subpopulations (e.g., those of a particular race, gender, or sexual orientation). Because this past data is often biased, <b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating discriminatory practices (or ...", "dateLastCrawled": "2021-12-15T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Abstract - arXiv", "url": "https://arxiv.org/pdf/1703.06856v3.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1703.06856v3.pdf", "snippet": "<b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our de\ufb01nition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real ...", "dateLastCrawled": "2020-08-09T05:32:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(counterfactual fairness)  is like +(\"correcting\" for bias in the training data)", "+(counterfactual fairness) is similar to +(\"correcting\" for bias in the training data)", "+(counterfactual fairness) can be thought of as +(\"correcting\" for bias in the training data)", "+(counterfactual fairness) can be compared to +(\"correcting\" for bias in the training data)", "machine learning +(counterfactual fairness AND analogy)", "machine learning +(\"counterfactual fairness is like\")", "machine learning +(\"counterfactual fairness is similar\")", "machine learning +(\"just as counterfactual fairness\")", "machine learning +(\"counterfactual fairness can be thought of as\")", "machine learning +(\"counterfactual fairness can be compared to\")"]}