{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Disparate</b> Impact in <b>Machine</b> <b>Learning</b> \u00bb Dome | Blog Archive | Boston ...", "url": "https://sites.bu.edu/dome/2020/06/08/disparate-impact-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://sites.bu.edu/dome/2020/06/08/<b>disparate</b>-impact-in-<b>machine</b>-<b>learning</b>", "snippet": "Thus, instead of relying on either <b>disparate</b> impact or <b>disparate</b> <b>treatment</b> theory, perhaps legal analysis of discrimination in <b>machine</b> <b>learning</b> should be entirely outcomes-driven. If, in fact, an <b>algorithm</b> wrongly predicts the likelihood of an event occurring, and that <b>algorithm</b> is less accurate for protected class members than unprotected class members, the <b>algorithm</b> should be considered prima facie discriminatory. Such a solution is viable for examining recidivism, interest rates and loan ...", "dateLastCrawled": "2021-12-09T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Disparate</b> <b>Treatment</b> \u2014 Involves classifying someone in an impermissible way. It involves the intent to discriminate, evidenced by explicit reference to group membership. <b>Disparate</b> Impact \u2014 Looks at the consequences of classification/decision making on certain groups. No intent is required and it is facially neutral. <b>Disparate</b> impact is often referred to as unintentional <b>discrimination</b>, whereas <b>disparate</b> <b>treatment</b> is intentional. Practices with a disproportionate impact on a particular ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Discrimination</b> in the Age of Algorithms | Journal of Legal Analysis ...", "url": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "snippet": "We focus on one kind of <b>machine</b>-<b>learning</b> <b>algorithm</b> often applied to such problems, ... With this in mind, consider what <b>disparate</b> <b>treatment</b> looks <b>like</b>. For hiring, an HR manager might do something <b>like</b> rank-order a less-productive white applicant over a more-productive minority applicant. But with a training and a screening <b>algorithm</b>, if we have accounted for the possible sources of <b>discrimination</b> in the human\u2019s choices for the training <b>algorithm</b> (outcome, candidate predictors, and ...", "dateLastCrawled": "2022-01-28T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Does mitigating ML&#39;s <b>disparate impact require disparate treatment</b>?", "url": "https://arxiv.org/abs/1711.07076v1", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/1711.07076v1", "snippet": "The natural way to reduce <b>disparate</b> impact would be to apply <b>disparate</b> <b>treatment</b> in favor of the disadvantaged group, i.e. to apply affirmative action. However, owing to the practice&#39;s contested legal status, several papers have proposed trying to eliminate both forms of unfairness simultaneously, introducing a family of algorithms that we denote <b>disparate</b> <b>learning</b> processes (DLPs). These processes incorporate the protected characteristic as an input to the <b>learning</b> <b>algorithm</b> (e.g.~via a ...", "dateLastCrawled": "2021-12-18T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> in Drug Discovery: A Review", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8356896/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8356896", "snippet": "<b>Machine</b> <b>learning</b> techniques improve the decision-making in pharmaceutical data across various applications <b>like</b> QSAR analysis, hit discoveries, de novo drug architectures to retrieve accurate outcomes. Target validation, prognostic biomarkers, digital pathology are considered under problem statements in this review. ML challenges must be applicable for the main cause of inadequacy in interpretability outcomes that may restrict the applications in drug discovery. In clinical trials, absolute ...", "dateLastCrawled": "2022-01-27T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Differentially Private Fair <b>Learning</b> - Proceedings of <b>Machine</b> <b>Learning</b> ...", "url": "http://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf", "snippet": "can be viewed as a form of \u201c<b>disparate</b> <b>treatment</b>\u201d. Our second <b>algorithm</b> is a differentially private version of the oracle-ef\ufb01cient in-processing ap-proach of (Agarwal et al.,2018) which is more complex but need not have access to protected group membership at test time. We identify new tradeoffs between fairness, accuracy, and privacy that emerge only when requiring all three proper-ties, and show that these tradeoffs can be milder if group membership may be used at test time. We ...", "dateLastCrawled": "2022-01-22T22:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Disparate</b> impact in a <b>machine</b> <b>learning</b> model originates from bias in either the data or the algorithms. A popular example is the prejudicially biased data used for recidivism prediction. Due to <b>disparate</b> socioeconomic factors and systemic racism in the United States, blacks have historically been (and continue to be) incarcerated at higher rates than whites . Not coincidentally, blacks are also exonerated due to wrongful accusation at a considerably higher rate than whites . A recidivism ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Fairness in Machine Learning is Tricky</b> | by John Dickerson | Arthur AI ...", "url": "https://medium.com/arthur-ai/fairness-in-machine-learning-is-tricky-1ea47111b847", "isFamilyFriendly": true, "displayUrl": "https://medium.com/arthur-ai/<b>fairness-in-machine-learning-is-tricky</b>-1ea47111b847", "snippet": "Then, one goal that a <b>fairness in machine learning</b> practitioner might have is to mathematically certify that an <b>algorithm</b> does not suffer from <b>disparate</b> <b>treatment</b> or <b>disparate</b> impact, perhaps ...", "dateLastCrawled": "2021-04-29T13:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b> by Runshan Fu, Manmohan Aseri ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "snippet": "<b>Machine</b> <b>Learning</b> algorithms are becoming widely deployed in real world decision-making. Ensuring fairness in algorithmic decision-making is a crucial policy issue. Current legislation ensures fairness by barring <b>algorithm</b> designers from using demographic information in their decision-making. As a result, the algorithms need to ensure equal <b>treatment</b> to be legally compliant. However, in many cases, ensuring equal <b>treatment</b> leads to <b>disparate</b> impact particularly when there are differences ...", "dateLastCrawled": "2022-01-29T16:56:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning and Discrimination</b> - GitHub Pages", "url": "https://harvard-ml-courses.github.io/cs181-web-2020/files/lecture12.pdf", "isFamilyFriendly": true, "displayUrl": "https://harvard-ml-courses.github.io/cs181-web-2020/files/lecture12.pdf", "snippet": "<b>Machine Learning and Discrimination</b> Diana Acosta-Navas PhD candidate, Harvard Philosophy Department Adjunct Lecturer in Ethics and Public Policy, Harvard Kennedy School . For Today\u2026 \u2022Discrimination/ wrongful discrimination \u2022Case Study: PredPol \u2022<b>Disparate</b> <b>treatment</b> vs. <b>Disparate</b> impact \u2022How predictive policing could wrongfully discriminate \u2022What contextual considerations are important to determine whether an <b>algorithm</b> wrongfully discriminates? For Today\u2026 \u2022Content warning ...", "dateLastCrawled": "2021-09-15T23:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "AI Fairness \u2014 Explanation of <b>Disparate Impact</b> Remover | by Stacey ...", "url": "https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/ai-fairness-explanation-of-<b>disparate-impact</b>-remover-ce0...", "snippet": "The <b>algorithm</b> requires the user to specify a repair_level, this indicates how much you wish for the distributions of the groups to overlap. Let\u2019s explore the impact of two different repair levels, 1.0 and 0.8. Repair value = 1.0. This diagram shows the repaired values for Feature for the unprivileged group Blue and privileged group Orange after using DisparateImpactRemover with a repair level of 1.0. You are no longer able to select a point and infer which group it belongs to. This would ...", "dateLastCrawled": "2022-01-29T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>AI Fairness \u2014 Explanation of Disparate Impact Remover</b> - Adolfo Eliaz\u00e0t ...", "url": "https://adolfoeliazat.com/2021/05/06/ai-fairness-explanation-of-disparate-impact-remover/", "isFamilyFriendly": true, "displayUrl": "https://adolfoeliazat.com/2021/05/06/<b>ai-fairness-explanation-of-disparate-impact-remover</b>", "snippet": "<b>Disparate</b> Impact Remover preserves rank-ordering within groups; if an individual has the highest score for group Blue, it will still have the highest score among Blues after repair. Building <b>Machine</b> <b>Learning</b> Models. Once <b>Disparate</b> Impact Remover has been implemented, a <b>machine</b> <b>learning</b> model can be built using the repaired data. The <b>Disparate</b> ...", "dateLastCrawled": "2022-01-22T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Legal and Ethical Challenges for HR in <b>Machine</b> <b>Learning</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10672-021-09377-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10672-021-09377-z", "snippet": "Such an example of <b>disparate</b> <b>treatment</b> on the part of the software would be obviously wrong to HR. However, <b>disparate</b> (adverse) impact discrimination issues are likely to be less apparent and may serve to exacerbate previous discriminatory patterns. For example, suppose that a <b>machine</b> <b>learning</b> <b>algorithm</b> determines that breaks in previous employment records are related to absenteeism or turnover. This might seem to be a reasonable basis on which to reject an applicant, but it is likely to ...", "dateLastCrawled": "2022-01-12T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "6.S897 <b>Machine</b> <b>Learning</b> in Healthcare, Lecture 23: Fairness", "url": "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-notes/MIT6_S897S19_lec23.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/electrical-engineering-and-computer-science/6-s897-<b>machine</b>...", "snippet": "<b>algorithm</b>, starting in October 2019. A county o\ufb03cial will then take that grade and use it to recommend whether the accused should be released or remain in jail. \u2022 \u201c\u2026 the <b>machine</b> <b>learning</b> systems used to calculate these risk scores throughout the criminal justice system, have been shown to hold severe", "dateLastCrawled": "2022-01-28T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Disparate</b> impact in a <b>machine</b> <b>learning</b> model originates from bias in either the data or the algorithms. A popular example is the prejudicially biased data used for recidivism prediction. Due to <b>disparate</b> socioeconomic factors and systemic racism in the United States, blacks have historically been (and continue to be) incarcerated at higher rates than whites . Not coincidentally, blacks are also exonerated due to wrongful accusation at a considerably higher rate than whites . A recidivism ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Equal Protection Under the <b>Algorithm</b>: A Legal-Inspired Framework for ...", "url": "https://www.fatml.org/media/documents/equal_protection_under_the_algorithm.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.fatml.org/media/documents/equal_protection_under_the_<b>algorithm</b>.pdf", "snippet": "<b>machine</b> <b>learning</b>. There are many types of applications where this work may be relevant, and it is not possible to create a single <b>algorithm</b> that is applicable to all of them. Rather, we provide a high-level description of how one can de\ufb01ne an appropriate <b>algorithm</b>. 4.1. Terminology A <b>treatment</b> is a targeting action, such as showing an ad. A", "dateLastCrawled": "2022-02-02T07:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Automated Feature Engineering for Algorithmic Fairness", "url": "https://www.vldb.org/pvldb/vol14/p1694-neutatz.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.vldb.org/pvldb/vol14/p1694-neutatz.pdf", "snippet": "perpetuation and amplification of discrimination through <b>machine</b> <b>learning</b> applications. In particular, it is desired to exclude the in-fluence of attributes with sensitive information, such as gender or race, and other causally related attributes on the <b>machine</b> <b>learning</b> task. The state-of-the-art bias reduction <b>algorithm</b> Capuchin breaks", "dateLastCrawled": "2022-01-26T07:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Fall 2020 Journal | Algorithms and Child Welfare: The <b>Disparate</b> Impact ...", "url": "https://bppj.berkeley.edu/2021/02/02/algorithms-and-child-welfare-the-disparate-impact-of-family-surveillance-in-risk-assessment-technologies/", "isFamilyFriendly": true, "displayUrl": "https://bppj.berkeley.edu/2021/02/02/<b>algorithms</b>-and-child-welfare-the-<b>disparate</b>-impact...", "snippet": "We believe that if <b>machine</b> <b>learning</b> is to continue to be used in social services, the history of the data must be considered [34]. Through our literature review, we did not find evidence of regulation over the child welfare data used in <b>machine</b> <b>learning</b> technologies. At the time of writing, Pennsylvania\u2019s statutes on Child Protective Services did not include any guidance on the use of <b>machine</b> <b>learning</b> or artificial intelligence. Searches for the words \u201cautomated\u201d and \u201c<b>algorithm</b> ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2,", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Disparate</b> <b>Treatment</b> \u2014 Involves classifying someone in an impermissible way. It involves the intent to discriminate, evidenced by explicit reference to group membership. <b>Disparate</b> Impact \u2014 Looks at the consequences of classification/decision making on certain groups. No intent is required and it is facially neutral. <b>Disparate</b> impact is often referred to as unintentional <b>discrimination</b>, whereas <b>disparate</b> <b>treatment</b> is intentional. Practices with a disproportionate impact on a particular ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A primer on AI <b>fairness</b>. What it is and the tradeoffs to be made | by ...", "url": "https://towardsdatascience.com/artificial-intelligence-fairness-and-tradeoffs-ce11ac284b63", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/artificial-intelligence-<b>fairness</b>-and-tradeoffs-ce11ac284b63", "snippet": "Discrimination Law: <b>Disparate</b> <b>Treatment</b> (formal vs intentional) vs <b>Disparate</b> Impact (20% rule, legal rule of thumb). <b>Disparate</b> <b>treatment</b> <b>can</b> <b>be thought</b> of as procedural <b>fairness</b>. The underlying philosophy is equality of opportunity. <b>Disparate</b> impact is distributive justice. There is tension between these two goals.", "dateLastCrawled": "2022-02-02T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> (CS 181) - 2018 Spring | Embedded EthiCS", "url": "https://embeddedethics.seas.harvard.edu/classes/cs-181-2018-spring", "isFamilyFriendly": true, "displayUrl": "https://embeddedethics.seas.harvard.edu/classes/cs-181-2018-spring", "snippet": "<b>machine</b> <b>learning</b> CS Module Overview: In this module, we probe the ways that <b>machine</b> <b>learning</b> models <b>can</b> be discriminatory and examine different methods for preventing discriminatory outcomes. We begin by introducing two concepts of discrimination: <b>disparate</b> <b>treatment</b> and <b>disparate</b> impact. We then use those concepts to argue that there are at ...", "dateLastCrawled": "2022-01-06T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Algorithmic bias detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/<b>algorithm</b>ic-bias-detection-and-mitigation-best-", "snippet": "For example, the demonstration of <b>disparate</b> <b>treatment</b> does not describe the ways in which an <b>algorithm</b> <b>can</b> learn to treat similarly situated groups differently, as will be discussed later in the ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Review of Challenges and Opportunities in <b>Machine</b> <b>Learning</b> for Health", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7233077/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7233077", "snippet": "Existing reviews of <b>machine</b> <b>learning</b> in the medical space have focused narrowly on biomedical applications 5, deep <b>learning</b> tasks well suited for healthcare 6, the need for transparency 7, and use of big data in precision medicine 8. Here, we emphasize the broad opportunities present in <b>machine</b> <b>learning</b> for healthcare and the careful considerations that must be made. We focus on the electronic health record (EHR), which documents the process of healthcare delivery and operational needs such ...", "dateLastCrawled": "2022-01-25T00:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Algorithms as discrimination detectors | <b>PNAS</b>", "url": "https://www.pnas.org/content/117/48/30096", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/48/30096", "snippet": "Such algorithms are generally developed using the methods of <b>machine</b> <b>learning</b>; beyond this level of generality there is nothing specific about our argument that hinges on any particular formalism within <b>machine</b> <b>learning</b>\u2014for example, whether we are working with, say, a neural network or some other <b>machine</b>-<b>learning</b> technique. Regardless of the complexity of the function class that the <b>learning</b> procedure allows, all standard techniques share the property that the <b>algorithm</b> builder will need ...", "dateLastCrawled": "2022-01-28T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Discover Artificial Intelligence and <b>Machine</b> <b>Learning</b> <b>Thought</b> ...", "url": "https://www.himss.org/news/discover-artificial-intelligence-and-machine-learning-thought-leadership-himss21", "isFamilyFriendly": true, "displayUrl": "https://<b>www.himss.org</b>/news/discover-artificial-intelligence-and-<b>machine</b>-<b>learning</b>...", "snippet": "Ethical <b>Machine</b> <b>Learning</b>: 11:30 a.m. - 12:30 p.m. This session explores how to build ethical considerations into <b>machine</b> <b>learning</b> projects and tools. Having an ethical framework will help to provide transparency and build public trust. The speakers will draw upon the experience of New Zealand, where they have implemented a national <b>algorithm</b> ...", "dateLastCrawled": "2021-12-31T20:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Affirmative</b> Algorithms: The Legal Grounds for Fairness as Awareness ...", "url": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang/", "isFamilyFriendly": true, "displayUrl": "https://lawreviewblog.uchicago.edu/2020/10/30/aa-ho-xiang", "snippet": "A substantial literature has emerged within <b>machine</b> <b>learning</b> around remedies for algorithmic bias, with the consensus position being that <b>machine</b> bias is best addressed through awareness of such \u201cprotected groups.\u201d 3 We use \u201cprotected groups\u201d to refer to groups conventionally protected under various antidiscrimination provisions, such as the Constitution\u2019s Equal Protection Clause, Title VII of the Civil Rights Act, the Fair Housing Act, and the like. An important debate concerns ...", "dateLastCrawled": "2022-01-31T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "TOP: A Compiler-Based Framework for Optimizing <b>Machine</b> <b>Learning</b> ...", "url": "https://sites.cs.ucsb.edu/~yufeiding/publication/SysML18.pdf", "isFamilyFriendly": true, "displayUrl": "https://sites.cs.ucsb.edu/~yufeiding/publication/SysML18.pdf", "snippet": "<b>treatment</b> to various ML algorithms, and TOP, a compiler-based optimizer for e ectively applying TI to optimize <b>ma-chine</b> <b>learning</b> algorithms. Experiments show that TOP is able to automatically produce optimized algorithms that ei-ther matches or outperforms manually designed algorithms, giving up to 237x speedups and 2.5X on average1. 1. INTRODUCTION Vector dot products and point-to-point distance calcula-tions are essential to many important algorithms across var-ious domains. Vector dot ...", "dateLastCrawled": "2021-09-18T13:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The ethics of <b>algorithms</b>: key problems and solutions", "url": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "snippet": "Actions (1) and (2) may be performed by (semi-)autonomous <b>algorithms</b>\u2014such as <b>machine</b> <b>learning</b> (ML) <b>algorithms</b>\u2014and this complicates, (3) the attribution of responsibility for the effects of actions that an <b>algorithm</b> may trigger. Here, ML is of particular interest, as a field which includes deep <b>learning</b> architectures. Computer systems deploying ML <b>algorithms</b> may be described as \u201cautonomous\u201d or \u201csemi-autonomous\u201d, to the extent that their outputs are induced from data and thus, non ...", "dateLastCrawled": "2022-01-30T20:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "<b>Disparate</b> <b>Treatment</b> \u2014 Involves classifying someone in an impermissible way. It involves the intent to discriminate, evidenced by explicit reference to group membership. <b>Disparate</b> Impact \u2014 Looks at the consequences of classification/decision making on certain groups. No intent is required and it is facially neutral. <b>Disparate</b> impact is often referred to as unintentional <b>discrimination</b>, whereas <b>disparate</b> <b>treatment</b> is intentional. Practices with a disproportionate impact on a particular ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b>", "url": "https://www.ftc.gov/system/files/documents/public_events/1567421/fuaserisinghsrinivasan_updated2.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ftc.gov</b>/system/files/documents/public_events/1567421/fuaserisinghsriniva...", "snippet": "<b>Compared</b> to the current law, which requires <b>treatment</b> parity, these \u201cfair\u201d algorithms, which require impact parity, limit the bene\ufb01ts of a more accurate <b>algorithm</b> for a \ufb01rm. As a result, pro\ufb01t maximizing \ufb01rms could under-invest in <b>learning</b>, i.e., improving the accuracy of their <b>machine</b> <b>learning</b> algorithms. We show that the investment in <b>learning</b> decreases when misclassi\ufb01cation is costly, which is exactly the case when greater accuracy is otherwise desired. Our paper highlights ...", "dateLastCrawled": "2022-02-03T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding and Reducing Bias in <b>Machine Learning</b> | by Jaspreet ...", "url": "https://towardsdatascience.com/understanding-and-reducing-bias-in-machine-learning-6565e23900ac", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-and-reducing-bias-in-<b>machine-learning</b>...", "snippet": "Even if we remove <b>disparate</b> <b>treatment</b> by removing the sensitive feature, discrimination <b>can</b> still happen through other correlated features such as zip code. Measuring and correcting <b>disparate</b> impact makes sure that this is corrected. This requirement should be used when the training dataset is biased. While being considered a controversial measure by many, notably by critics who hold that some scenarios cannot be freed from disproportionate outcomes.", "dateLastCrawled": "2022-01-28T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "\u201cUn\u201dFair <b>Machine</b> <b>Learning</b> Algorithms | Management Science", "url": "https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4065", "isFamilyFriendly": true, "displayUrl": "https://pubsonline.informs.org/doi/10.1287/mnsc.2021.4065", "snippet": "However, in many cases, ensuring equal <b>treatment</b> leads to <b>disparate</b> impact particularly when there are differences among groups based on demographic classes. In response, several \u201cfair\u201d <b>machine</b> <b>learning</b> (ML) algorithms that require impact parity (e.g., equal opportunity) at the cost of equal <b>treatment</b> have recently been proposed to adjust for the societal inequalities. Advocates of fair ML propose changing the law to allow the use of protected class-specific decision rules. We show that ...", "dateLastCrawled": "2022-02-01T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> in Drug Discovery: A Review", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8356896/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8356896", "snippet": "In <b>machine</b> <b>learning</b>, mapping ability features <b>can</b> yield great accomplishment to extract physical, geometric, and chemical features (Khamis et al. ) to retrieve scores. Based on scores, data-driven black box models which are considered to predict interactions in binding affinities and furthermore avoiding few concepts in docking like physical function are very hard to study (Ain et al. 2015 ).", "dateLastCrawled": "2022-01-27T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b> by Runshan Fu, Manmohan Aseri ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "snippet": "<b>Machine</b> <b>Learning</b> algorithms are becoming widely deployed in real world decision-making. Ensuring fairness in algorithmic decision-making is a crucial policy issue. Current legislation ensures fairness by barring <b>algorithm</b> designers from using demographic information in their decision-making. As a result, the algorithms need to ensure equal <b>treatment</b> to be legally compliant. However, in many cases, ensuring equal <b>treatment</b> leads to <b>disparate</b> impact particularly when there are differences ...", "dateLastCrawled": "2022-01-29T16:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The promise of <b>machine learning in predicting treatment outcomes</b> in ...", "url": "https://onlinelibrary.wiley.com/doi/10.1002/wps.20882", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1002/wps.20882", "snippet": "Predicting <b>treatment</b> response is just one relatively narrow use case where <b>machine</b> <b>learning</b> <b>can</b> add value and improve mental health care. Prediction <b>can</b> help with so many more clinical decisions and clinical processes. We could predict barriers that prevent an individual from engaging in care initially, or non-adherence or dropout from care after initiation. We could streamline patients to the appropriate level of care, such as self-guided programs vs. outpatient care, or intensive ...", "dateLastCrawled": "2022-01-22T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Two-stage <b>Algorithm</b> for <b>Fairness-aware</b> <b>Machine</b> <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/two-stage-algorithm-for-fairness-aware-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/two-stage-<b>algorithm</b>-for-<b>fairness-aware</b>-<b>machine</b>-<b>learning</b>", "snippet": "In other words, a <b>machine</b> <b>learning</b> <b>algorithm</b> that utilizes sensitive attributes is subject to biases in the existing data. This could be viewed as an algorithmic version of <b>disparate</b> <b>treatment</b> , where decisions are made on the basis of these sensitive attributes. However, removing sensitive attributes from the dataset is not sufficient solution as it has a <b>disparate</b> impact. <b>Disparate</b> impact is a notion that was born in the 1970s. The U.S. Supreme Court ruled that the hiring decision at the ...", "dateLastCrawled": "2021-12-10T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Disparate</b> Impact Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-impact-analysis", "snippet": "<b>Machine</b> <b>learning</b> models <b>can</b> make drastically differing predictions for only minor changes in input variable values. For example, when looking at predictions that determine financial decisions, SA <b>can</b> be used to help you understand the impact of changing the most important input variables and the impact of changing socially sensitive variables (such as Sex, Age, Race, etc.) in the model. If the model changes in reasonable and expected ways when important variable values are changed, this <b>can</b> ...", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Machine</b> <b>learning</b> models <b>can</b> also be a source of <b>disparate</b> impact in their implementation, through unconscious human biases that affect the fair interpretation or use of the model&#39;s results. This reference does not cover measurement of fairness at implementation. However, if you are interested in fair implementation, we recommend looking at Google&#39;s Fairness Indicators. Harms. In evaluating the potential impact of an ML model, it <b>can</b> be helpful to first clarify what specific harm(s) <b>can</b> be ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>learning</b> and applications in microbiology", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8498514/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8498514", "snippet": "<b>Machine</b> <b>learning</b> has two main <b>learning</b> modes: supervised (also known as predictive) to make future predictions from training data, and unsupervised (descriptive), which is exploratory in nature without training data, defined target or output (Mitchell 1997). Training data are the initial information used to teach supervised ML algorithms in the process of developing a model, from which the model creates and refines its rules required for prediction. Typically, training data comprises a set ...", "dateLastCrawled": "2021-12-06T07:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Realistically Integrating <b>Machine</b> <b>Learning</b> Into Clinical Pra ...", "url": "https://journals.lww.com/anesthesia-analgesia/fulltext/2020/05000/realistically_integrating_machine_learning_into.4.aspx", "isFamilyFriendly": true, "displayUrl": "https://<b>journals.lww.com</b>/.../05000/realistically_integrating_<b>machine</b>_<b>learning</b>_into.4.aspx", "snippet": "<b>Machine</b>-<b>learning</b> models have been created to predict an increasing number of clinical outcomes, such as diagnoses and mortality, with applications including C. difficile infection in the inpatient hospital setting, 2 identifying molecular markers for cancer treatments, 3 and postoperative surgical outcomes. 4 Examples of <b>machine</b> <b>learning</b> include a cardiologist using an automated interpretation of an ECG and a radiologist using an automated detection of a lung nodule in a chest x-ray. In both ...", "dateLastCrawled": "2021-11-22T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b>: Science and Technology, Volume 2, Number 1, March ...", "url": "https://iopscience.iop.org/issue/2632-2153/2/1", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/issue/2632-2153/2/1", "snippet": "<b>Machine</b> <b>learning</b> has been used in high energy physics (HEP) for a long time, primarily at the analysis level with supervised classification. Quantum computing was postulated in the early 1980s as way to perform computations that would not be tractable with a classical computer. With the advent of noisy intermediate-scale quantum computing devices, more quantum algorithms are being developed with the aim at exploiting the capacity of the hardware for <b>machine</b> <b>learning</b> applications. An ...", "dateLastCrawled": "2021-12-16T00:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The Immune System Computes the State of the Body: Crowd Wisdom, <b>Machine</b> ...", "url": "https://www.frontiersin.org/articles/10.3389/fimmu.2019.00010/full", "isFamilyFriendly": true, "displayUrl": "https://www.<b>frontiers</b>in.org/articles/10.3389/fimmu.2019.00010", "snippet": "The correlations between the components comprising a set of data can be very subtle and obscure to the human observer, yet such correlations are detectable by <b>machine</b> <b>learning</b> algorithms, and, by <b>analogy</b>, by networks of cells and antibodies in the immune system. As a consequence of exposure to training sets of input, the computer algorithm\u2014and the immune system\u2014can accumulate a bank of learned correlations. These formative correlations can then be used by the computer or the repertoire ...", "dateLastCrawled": "2022-01-30T04:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine learning</b>, artificial neural networks and social research ...", "url": "https://link.springer.com/article/10.1007/s11135-020-01037-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11135-020-01037-y", "snippet": "<b>Machine learning</b> (ML), and particularly algorithms based on artificial neural networks (ANNs), constitute a field of research lying at the intersection of different disciplines such as mathematics, statistics, computer science and neuroscience. This approach is characterized by the use of algorithms to extract knowledge from large and heterogeneous data sets. In addition to offering a brief introduction to ANN algorithms-based ML, in this paper we will focus our attention on its possible ...", "dateLastCrawled": "2022-01-27T04:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Protein function in precision medicine: deep understanding with <b>machine</b> ...", "url": "https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.12307", "isFamilyFriendly": true, "displayUrl": "https://febs.onlinelibrary.wiley.com/doi/pdf/10.1002/1873-3468.12307", "snippet": "<b>disparate</b> parts may ultimately lead to the same observable effect. In this <b>analogy</b>, we might argue that medicine has so far been often investing into mitigat- ing the inconvenience with lemons and much less into improving and augmenting the protocols for \ufb01nding the individual causes of problems. In his recent State-of-the-Union address, the US Pres-ident Barack Obama announced the Precision Medicine Initiative, making this challenge a national and interna-tional priority. Precision ...", "dateLastCrawled": "2021-12-12T19:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Solution Manual Alpaydin Introduction To <b>Machine</b> <b>Learning</b>", "url": "https://se.rangle.io/solution+manual+alpaydin+introduction+to+machine+learning+pdf", "isFamilyFriendly": true, "displayUrl": "https://se.rangle.io/solution+manual+alpaydin+introduction+to+<b>machine</b>+<b>learning</b>+pdf", "snippet": "With in-depth Python and MATLAB/OCTAVE-based computational exercises and a complete <b>treatment</b> of cutting edge numerical optimization techniques, this is an essential resource for students and an ideal reference for researchers and practitioners working in <b>machine</b> <b>learning</b>, computer science, electrical engineering, signal processing, and numerical optimization. Handbook of Research on Innovations in Database Technologies and Applications One of the currently most active research areas within ...", "dateLastCrawled": "2022-01-11T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Latent bias and the implementation of <b>artificial intelligence</b> in ...", "url": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jamia/article/27/12/2020/5859726", "snippet": "<b>Artificial intelligence</b> (AI) in general, and <b>machine</b> <b>learning</b> in particular, by all accounts, appear poised to revolutionize medicine. 1\u20133 With a wide spectrum of potential uses across translational research (from bench to bedside to health policy), clinical medicine (including diagnosis, <b>treatment</b>, prediction, and healthcare resource allocation), and public health, every area of medicine will be affected.", "dateLastCrawled": "2022-01-28T13:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "47 <b>Analogy</b> Examples To Make You As Sharp As A Tack (and then some)", "url": "https://www.greetingcardpoet.com/good-analogy-examples-and-definition/", "isFamilyFriendly": true, "displayUrl": "https://www.greetingcardpoet.com/<b>good-analogy-examples-and-definition</b>", "snippet": "The essence of this literary device is to set up a comparison that highlights similarities between two seemingly <b>disparate</b> items. It is by examining how the two items are alike in some way that leads to a clear understanding. Differences between Similes, Metaphors, and Analogies . While similes, metaphors, and analogies are similar in that they all compare two different things, similes and metaphors are figures of speech. In contrast, an <b>analogy</b> is more akin to a logical argument. A writer ...", "dateLastCrawled": "2022-02-02T03:57:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A primer on AI <b>fairness</b>. What it is and the tradeoffs to be made | by ...", "url": "https://towardsdatascience.com/artificial-intelligence-fairness-and-tradeoffs-ce11ac284b63", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/artificial-intelligence-<b>fairness</b>-and-tradeoffs-ce11ac284b63", "snippet": "A <b>machine</b> <b>learning</b> algorithms value is being able to increase the number of true positives and true negatives, which each have a value attached. Each false positive and false negative is costly. The value assigned to each depends on each context. A false negative is more costly in medical situations while a false positive is costlier in death penalty decisions. Expected value is profits that businesses can expect from using the algorithm. The more accurate the model, the higher the profits.", "dateLastCrawled": "2022-02-02T02:42:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(disparate treatment)  is like +(machine learning algorithm)", "+(disparate treatment) is similar to +(machine learning algorithm)", "+(disparate treatment) can be thought of as +(machine learning algorithm)", "+(disparate treatment) can be compared to +(machine learning algorithm)", "machine learning +(disparate treatment AND analogy)", "machine learning +(\"disparate treatment is like\")", "machine learning +(\"disparate treatment is similar\")", "machine learning +(\"just as disparate treatment\")", "machine learning +(\"disparate treatment can be thought of as\")", "machine learning +(\"disparate treatment can be compared to\")"]}