{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Read <b>Like</b> Humans: Autonomous, <b>Bidirectional</b> and Iterative <b>Language</b> ...", "url": "https://www.semanticscholar.org/paper/Read-Like-Humans%3A-Autonomous%2C-Bidirectional-and-for-Fang-Xie/e3d81ed56fad9bd348712c480e9651007d6c2226", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/Read-<b>Like</b>-<b>Human</b>s:-Autonomous,-<b>Bidirectional</b>-and...", "snippet": "This paper argues that the limited capacity of <b>language</b> models comes from: 1) implicitly <b>language</b> modeling; 2) unidirectional feature representation; 3) <b>language</b> <b>model</b> with noise input, and proposes an autonomous, <b>bidirectional</b> and iterative ABINet for scene text recognition. Linguistic knowledge is of great benefit to scene text recognition. However, how to effectively <b>model</b> linguistic rules in end-to-end deep networks remains a research challenge. In this paper, we argue that the limited ...", "dateLastCrawled": "2022-01-28T20:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What are <b>Language</b> Models in NLP? - Daffodil", "url": "https://insights.daffodilsw.com/blog/what-are-language-models-in-nlp", "isFamilyFriendly": true, "displayUrl": "https://insights.daffodilsw.com/blog/what-are-<b>language</b>-<b>models</b>-in-nlp", "snippet": "A <b>language</b> <b>model</b> is the core component of modern Natural <b>Language</b> Processing (NLP). It\u2019s a statistical tool that analyzes the pattern of <b>human</b> <b>language</b> for the prediction of words. NLP-based applications use <b>language</b> models for a variety of tasks, such as audio to text conversion, speech recognition, sentiment analysis, summarization, spell ...", "dateLastCrawled": "2022-02-03T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Generalized <b>Language</b> <b>Models</b> - Lil&#39;Log", "url": "https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html", "isFamilyFriendly": true, "displayUrl": "https://lilianweng.github.io/lil-log/2019/01/31/generalized-<b>language</b>-<b>models</b>.html", "snippet": "<b>Bidirectional</b> <b>Language</b> <b>Model</b>. The <b>bidirectional</b> <b>Language</b> <b>Model</b> (biLM) is the foundation for ELMo. While the input is a sequence of \\(n\\) tokens, \\((x_1, \\dots, x_n)\\), the <b>language</b> <b>model</b> learns to predict the probability of next token given the history. In the forward pass, the history contains words before the target token, \\[p(x_1, \\dots, x_n) = \\prod_{i=1}^n p(x_i \\mid x_1, \\dots, x_{i-1})\\] In the backward pass, the history contains words after the target token, \\[p(x_1, \\dots, x_n ...", "dateLastCrawled": "2022-01-30T14:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "<b>Like</b> BERT, XLNet uses a <b>bidirectional</b> context, which means it looks at the words before and after a given token to predict what it should be. To this end, XLNet maximizes the expected log-likelihood of a sequence with respect to all possible permutations of the factorization order. As an autoregressive <b>language</b> <b>model</b>, XLNet doesn\u2019t rely on data corruption, and thus avoids BERT\u2019s limitations due to masking \u2013 i.e., pretrain-finetune discrepancy and the assumption that unmasked tokens are ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Comparison between <b>BERT</b>, GPT-2 and ELMo | by Gaurav Ghati | Medium", "url": "https://medium.com/@gauravghati/comparison-between-bert-gpt-2-and-elmo-9ad140cd1cda", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@gauravghati/comparison-between-<b>bert</b>-gpt-2-and-elmo-9ad140cd1cda", "snippet": "ELMo u s es <b>bidirectional</b> <b>language</b> <b>model</b> (biLM) which is pre-trained on a large text corpus, to learn both words (e.g., syntax and semantics) and linguistic context (i.e., to <b>model</b> polysemy).", "dateLastCrawled": "2022-02-02T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>BERT (Language Model</b>) and How Does It Work?", "url": "https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model", "isFamilyFriendly": true, "displayUrl": "https://www.techtarget.com/searchenterpriseai/definition/<b>BERT-language-model</b>", "snippet": "Using this <b>bidirectional</b> capability, BERT is pre-trained on two different, but related, NLP tasks: Masked <b>Language</b> Modeling and Next Sentence Prediction. The objective of Masked <b>Language</b> <b>Model</b> (MLM) training is to hide a word in a sentence and then have the program predict what word has been hidden (masked) based on the hidden word&#39;s context ...", "dateLastCrawled": "2022-02-02T23:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 6, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "BERT (<b>language</b> <b>model</b>) - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/BERT_(Language_model)", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/BERT_(<b>Language</b>_<b>model</b>)", "snippet": "<b>Bidirectional</b> Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for natural <b>language</b> processing (NLP) pre-training developed by Google.BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. In 2019, Google announced that it had begun leveraging BERT in its search engine, and by late 2020 it was using BERT in almost every English-<b>language</b> query.A 2020 literature survey concluded that &quot;in a little over a year ...", "dateLastCrawled": "2022-02-02T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning a bidirectional mapping between human whole</b>-body motion and ...", "url": "https://www.sciencedirect.com/science/article/pii/S0921889017306280", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0921889017306280", "snippet": "In this paper we propose a generative <b>model</b> that learns <b>a bidirectional mapping between human whole</b>-body motion and natural <b>language</b> using deep recurrent neural networks (RNNs) and sequence-to-sequence learning. Our approach does not require any segmentation or manual feature engineering and learns a distributed representation, which is shared for all motions and descriptions. We evaluate our approach on 2", "dateLastCrawled": "2021-10-24T09:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The <b>language</b> of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "<b>Like</b> <b>human</b> <b>language</b>, protein sequences can be naturally represented as strings of letters (Fig. 1A). The protein alphabet consists of 20 common amino acids (AAs) (excluding unconventional and rare amino acids). Furthermore, <b>like</b> natural <b>language</b>, naturally evolved proteins are typically composed of reused modular elements exhibiting slight variations that can be rearranged and assembled in a hierarchical fashion. By this analogy, common protein motifs and domains, which are the basic ...", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Language</b> <b>Translation</b> with RNNs. Build a recurrent neural network (RNN ...", "url": "https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>language</b>-<b>translation</b>-with-rnns-d84d43b40571", "snippet": "The ability to communicate with one another is a fundamental part of <b>being</b> <b>human</b>. There are nearly 7,000 different languages worldwide. As our world becomes increasingly connected, <b>language</b> <b>translation</b> provides a critical cultural and economic bridge between people from different countries and ethnic groups. Some of the more obvious use-cases include: business: international trade, investment, contracts, finance; commerce: travel, purchase of foreign goods and services, customer support ...", "dateLastCrawled": "2022-02-02T17:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bidirectional Language Modeling: A Systematic Literature Review</b>", "url": "https://www.hindawi.com/journals/sp/2021/6641832/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/sp/2021/6641832", "snippet": "<b>Similar</b> content shared by more than one research study is discarded. 2.2. Search Process. Four scientific repositories and two conferences mentioned in Section 2.1 initiated the search process. Five defined keywords, i.e., (1) <b>bidirectional</b> <b>language</b> modeling, (2) pretrained <b>language</b> modeling, (3) biLM, (4) BERT, and (5) transformer, are used to search the research studies as shown in Table 2. We only use AND operator while searching because without AND operator, some keywords produced ...", "dateLastCrawled": "2022-01-31T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Comparison between <b>BERT</b>, GPT-2 and ELMo | by Gaurav Ghati | Medium", "url": "https://medium.com/@gauravghati/comparison-between-bert-gpt-2-and-elmo-9ad140cd1cda", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@gauravghati/comparison-between-<b>bert</b>-gpt-2-and-elmo-9ad140cd1cda", "snippet": "ELMo u s es <b>bidirectional</b> <b>language</b> <b>model</b> (biLM) which is pre-trained on a large text corpus, to learn both words (e.g., syntax and semantics) and linguistic context (i.e., to <b>model</b> polysemy).", "dateLastCrawled": "2022-02-02T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bidirectional</b> RNN - Devopedia", "url": "https://devopedia.org/bidirectional-rnn", "isFamilyFriendly": true, "displayUrl": "https://devopedia.org/<b>bidirectional</b>-rnn", "snippet": "Peters et al. publish details of a <b>language</b> <b>model</b> called Embeddings from <b>Language</b> Models (ELMo). ELMo representations are deep, ... they use the term <b>Bidirectional</b> <b>Language</b> <b>Model</b> (BiLM). References. Bergmanis, Toms, and Sharon Goldwater. 2018. &quot;Context Sensitive Neural Lemmatization with Lematus.&quot; Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: <b>Human</b> <b>Language</b> Technologies, Volume 1 (Long Papers), pp. 1391-1400, June. Accessed ...", "dateLastCrawled": "2022-01-30T00:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Top 10 <b>Pre-Trained</b> NLP <b>Language</b> Models - Daffodil", "url": "https://insights.daffodilsw.com/blog/top-5-nlp-language-models", "isFamilyFriendly": true, "displayUrl": "https://insights.daffodilsw.com/blog/top-5-nlp-<b>language</b>-<b>models</b>", "snippet": "Denoising autoencoding based <b>language</b> models such as BERT helps in achieving better performance than an autoregressive <b>model</b> for <b>language</b> modeling. That is why there is XLNet that introduces the auto-regressive pre-training method which offers the following benefits- it enables learning <b>bidirectional</b> context and helps overcome the limitations of BERT with its autoregressive formula. XLNet is known to outperform BERT on 20 tasks, which includes natural <b>language</b> inference, document ranking ...", "dateLastCrawled": "2022-02-02T17:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "We introduce a new <b>language</b> representation <b>model</b> called BERT, which stands for <b>Bidirectional</b> Encoder Representations from Transformers. Unlike recent <b>language</b> representation models, BERT is designed to pre-train deep <b>bidirectional</b> representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning a bidirectional mapping between human whole</b>-body motion and ...", "url": "https://www.sciencedirect.com/science/article/pii/S0921889017306280", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0921889017306280", "snippet": "This property makes sequence-to-sequence learning an excellent fit for the purpose of learning a mapping between <b>human</b> motion and natural <b>language</b>. We <b>model</b> each direction, that is from <b>human</b> motion to natural <b>language</b> as well as from natural <b>language</b> <b>to human</b> motion, individually. Fig. 4 depicts the details of both models. In both cases, the input sequence is first transformed into a latent context vector c by a recurrent neural network (RNN) or a stack thereof (meaning that many recurrent ...", "dateLastCrawled": "2021-10-24T09:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Is <b>Bidirectional</b> Sentence Parsing the Future of Natural <b>Language</b> ...", "url": "https://ysjournal.com/is-bidirectional-sentence-parsing-the-future-of-natural-language-processing/", "isFamilyFriendly": true, "displayUrl": "https://ysjournal.com/is-<b>bidirectional</b>-sentence-parsing-the-future-of-natural-<b>language</b>...", "snippet": "But <b>bidirectional</b> parsing is quickly becoming more prevalent in Natural <b>Language</b> Processing (NLP) and deep learning. Deep learning methods often require large amounts of data to extract patterns but in many NLP tasks, labelled data is scarce, so usually, a <b>model</b> is pre-trained on unsupervised data to learn universal <b>language</b> representations. The Transformer is a deep learning <b>model</b> proposed in the paper Attention is All You Need [1] by researchers at Google and the University of Toronto in ...", "dateLastCrawled": "2022-02-02T05:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Review \u2014 BERT: Pre-training of Deep <b>Bidirectional</b> Transformers for ...", "url": "https://sh-tsang.medium.com/review-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-59b1684882db", "isFamilyFriendly": true, "displayUrl": "https://sh-tsang.medium.com/review-bert-pre-training-of-deep-<b>bidirectional</b>...", "snippet": "BERT\u2019s <b>model</b> architecture is a multi-layer <b>bidirectional</b> Transformer encoder based on Transformer.The implementation is almost identical to the original one. The number of layers (i.e., Transformer blocks) is denoted as L, the hidden size is denoted as H, and the number of self-attention heads as A.3; Two <b>model</b> sizes are evaluated: BERTBASE (L=12, H=768, A=12, Total Parameters=110M), which has the same <b>model</b> size as OpenAI GPT for comparison purposes. BERT Transformer uses <b>bidirectional</b> ...", "dateLastCrawled": "2022-02-02T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Explanation of BERT <b>Model</b> - NLP - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/explanation-of-bert-model-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/explanation-of-bert-<b>model</b>-nlp", "snippet": "BERT (<b>Bidirectional</b> Encoder Representations from Transformers) is a Natural <b>Language</b> Processing <b>Model</b> proposed by researchers at Google Research in 2018. When it was proposed it achieve state-of-the-art accuracy on many NLP and NLU tasks such as: General <b>Language</b> Understanding Evaluation; Stanford Q/A dataset SQuAD v1.1 and v2.0; Situation With Adversarial Generations. Soon after few days of release the published open-sourced the code with two versions of pre-trained <b>model</b> BERT BASE and BERT ...", "dateLastCrawled": "2022-02-02T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Deep and Bi-directional RNNs</b> - Recurrent Neural Networks for <b>Language</b> ...", "url": "https://www.coursera.org/lecture/sequence-models-in-nlp/deep-and-bi-directional-rnns-xHrTe", "isFamilyFriendly": true, "displayUrl": "https://<b>www.coursera.org</b>/lecture/sequence-<b>models</b>-in-nlp/<b>deep-and-bi-directional-rnns</b>-xHrTe", "snippet": "In Course 3 of the Natural <b>Language</b> Processing Specialization, you will: a) Train a neural network with GLoVe word embeddings to perform sentiment analysis of tweets, b) Generate synthetic Shakespeare text using a Gated Recurrent Unit (GRU) <b>language</b> <b>model</b>, c) Train a recurrent neural network to perform named entity recognition (NER) using LSTMs with linear layers, and d) Use so-called \u2018Siamese\u2019 LSTM models to compare questions in a corpus and identify those that are worded differently ...", "dateLastCrawled": "2022-01-12T14:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Review \u2014 BERT: Pre-training of Deep <b>Bidirectional</b> Transformers for ...", "url": "https://sh-tsang.medium.com/review-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-59b1684882db", "isFamilyFriendly": true, "displayUrl": "https://sh-tsang.medium.com/review-bert-pre-training-of-deep-<b>bidirectional</b>...", "snippet": "BERT\u2019s <b>model</b> architecture is a multi-layer <b>bidirectional</b> Transformer encoder based on Transformer.The implementation is almost identical to the original one. The number of layers (i.e., Transformer blocks) is denoted as L, the hidden size is denoted as H, and the number of self-attention heads as A.3; Two <b>model</b> sizes are evaluated: BERTBASE (L=12, H=768, A=12, Total Parameters=110M), which has the same <b>model</b> size as OpenAI GPT for comparison purposes. BERT Transformer uses <b>bidirectional</b> ...", "dateLastCrawled": "2022-02-02T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bidirectional</b> Cognitive Computing <b>Model</b> for Uncertain Concepts ...", "url": "https://link.springer.com/article/10.1007/s12559-019-09666-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12559-019-09666-8", "snippet": "<b>Language</b> and words are powerful tools for <b>human</b> thinking, but there is a fundamental difference between <b>human</b> intelligence and the other creatures\u2019 intelligence in the usage of the <b>language</b> and words . Take cognition an uncertain concept \u201cThe Young\u201d for example, there is a significant difference between the <b>human</b> brain and computer. The <b>human</b> brain is to understand \u201cThe Young\u201d from the intension (although different people could have different understanding), while the computer is ...", "dateLastCrawled": "2021-12-23T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 2, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "BERT (<b>language</b> <b>model</b>) - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/BERT_(Language_model)", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/BERT_(<b>Language</b>_<b>model</b>)", "snippet": "<b>Bidirectional</b> Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for natural <b>language</b> processing (NLP) pre-training developed by Google.BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. In 2019, Google announced that it had begun leveraging BERT in its search engine, and by late 2020 it was using BERT in almost every English-<b>language</b> query.A 2020 literature survey concluded that &quot;in a little over a year ...", "dateLastCrawled": "2022-02-02T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Quantifying <b>Human</b> Decision-Making: Implications for <b>Bidirectional</b> ...", "url": "https://www.academia.edu/68449624/Quantifying_Human_Decision_Making_Implications_for_Bidirectional_Communication_in_Human_Robot_Teams", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/68449624/Quantifying_<b>Human</b>_Decision_Making_Implications_for...", "snippet": "This is important for <b>bidirectional</b> communica- tion, as it shows that more complex decision-making algorithms may produce only mar- ginal performance gains over simpler algorithms, at a cost of <b>being</b> far more difficult to explain to <b>human</b> teammates and the computational complexity of the algorithm it- self. Predictability of decisions. An important part of <b>human</b>-agent teams is the extent to which agents <b>can</b> predict one another\u2019s actions. This <b>can</b> be viewed as a function of the number of ...", "dateLastCrawled": "2022-01-18T03:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "We introduce a new <b>language</b> representation <b>model</b> called BERT, which stands for <b>Bidirectional</b> Encoder Representations from Transformers. Unlike recent <b>language</b> representation models, BERT is designed to pre-train deep <b>bidirectional</b> representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations <b>can</b> be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What Is GPT-3: How It <b>Works and Why You Should Care</b>", "url": "https://www.twilio.com/blog/what-is-gpt-3", "isFamilyFriendly": true, "displayUrl": "https://www.twilio.com/blog/what-is-gpt-3", "snippet": "GPT-3 is a <b>language</b> <b>model</b> from OpenAI that generates AI-written text that has the potential to be indistinguishable from <b>human</b> writing. Learn more about GPT-3. Learn more about GPT-3. Level up your Twilio API skills in TwilioQuest , an educational game for Mac, Windows, and Linux.", "dateLastCrawled": "2022-02-02T16:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Frontiers | The <b>Bidirectional</b> Social-Cognitive Mechanisms of the Social ...", "url": "https://www.frontiersin.org/articles/10.3389/fpsyt.2021.752274/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyt.2021.752274", "snippet": "Initiating joint attention behaviors, such as alternating gaze (see Figure 2), may be more specific to <b>human</b> development and <b>can</b> be reliably measured by 6- and 10-months of age (21, 23, 25, 107). Moreover, atypical IJA becomes measurable in infant siblings between 6- and 12-months of age ( 23 ) in the same timeframe that social orienting problems in ASD were most clearly observed by Jones and Klin [( 34 ), see Figure 3 ].", "dateLastCrawled": "2022-02-03T01:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "BERT: Google\u2019s <b>bidirectional</b> transformer | by Jan Perina | knowledge ...", "url": "https://medium.com/knowledge-engineering-seminar/bert-googles-bidirectional-transformer-f34e513f54fe", "isFamilyFriendly": true, "displayUrl": "https://medium.com/knowledge-engineering-seminar/bert-googles-<b>bidirectional</b>...", "snippet": "Transformer is a <b>model</b> that is used primarily in natural <b>language</b> processing. It is designed to solve sentence-to-sentence tasks and it learns the representation of both input and output whilst it ...", "dateLastCrawled": "2020-12-29T06:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[R] BERT: Pre-training of Deep <b>Bidirectional</b> Transformers for <b>Language</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/9nfqxz/r_bert_pretraining_of_deep_<b>bidirectional</b>", "snippet": "It&#39;s unfortunately impossible to train a deep <b>bidirectional</b> <b>model</b> like a normal LM, because that would create cycles where words <b>can</b> indirectly &quot;see themselves,&quot; and the predictions become trivial. What we <b>can</b> do instead is the very simple trick that&#39;s used in de-noising auto-encoders, where we mask some percent of words from the input and have to reconstruct those words from context.", "dateLastCrawled": "2021-11-17T05:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "GPT-2: How to Build &quot;The AI That&#39;s Too Dangerous to Release\u201d", "url": "https://blog.floydhub.com/gpt2/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/gpt2", "snippet": "Here\u2019s where we\u2019re at \u2014 we have a really good <b>language</b> <b>model</b> that (hopefully) has learned dynamics of the English <b>language</b> after <b>being</b> trained on a vast text corpus. Now, in theory, if we stick a task-specific layer or two on top of the <b>language</b> <b>model</b>, we should **get something that leverages the <b>language</b> <b>model</b>\u2019s linguistic capabilities while also adapting to the task at hand.", "dateLastCrawled": "2022-01-31T11:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bidirectional Language Modeling: A Systematic Literature Review</b>", "url": "https://www.hindawi.com/journals/sp/2021/6641832/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/sp/2021/6641832", "snippet": "(2) Every <b>model</b> in this paper is <b>compared</b> with RoBERTa (replicated BERT <b>model</b>) having large dataset and batch size but with a small step size. It is concluded that seven (7) out of thirty-one (31) models in this SLR outperforms RoBERTa in which three were trained on a larger dataset while the other four models are trained on a smaller dataset. Besides, among these seven models, six models shared both feedforward network (FFN) and attention across the layers. Rest of the twenty-four (24 ...", "dateLastCrawled": "2022-01-31T02:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What are <b>Language</b> Models in NLP? - Daffodil", "url": "https://insights.daffodilsw.com/blog/what-are-language-models-in-nlp", "isFamilyFriendly": true, "displayUrl": "https://insights.daffodilsw.com/blog/what-are-<b>language</b>-<b>models</b>-in-nlp", "snippet": "A <b>language</b> <b>model</b> is the core component of modern Natural <b>Language</b> Processing (NLP). It\u2019s a statistical tool that analyzes the pattern of <b>human</b> <b>language</b> for the prediction of words. NLP-based applications use <b>language</b> models for a variety of tasks, such as audio to text conversion, speech recognition, sentiment analysis, summarization, spell ...", "dateLastCrawled": "2022-02-03T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Review \u2014 BERT: Pre-training of Deep <b>Bidirectional</b> Transformers for ...", "url": "https://sh-tsang.medium.com/review-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-59b1684882db", "isFamilyFriendly": true, "displayUrl": "https://sh-tsang.medium.com/review-bert-pre-training-of-deep-<b>bidirectional</b>...", "snippet": "BERT\u2019s <b>model</b> architecture is a multi-layer <b>bidirectional</b> Transformer encoder based on Transformer.The implementation is almost identical to the original one. The number of layers (i.e., Transformer blocks) is denoted as L, the hidden size is denoted as H, and the number of self-attention heads as A.3; Two <b>model</b> sizes are evaluated: BERTBASE (L=12, H=768, A=12, Total Parameters=110M), which has the same <b>model</b> size as OpenAI GPT for comparison purposes. BERT Transformer uses <b>bidirectional</b> ...", "dateLastCrawled": "2022-02-02T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Comparison between <b>BERT</b>, GPT-2 and ELMo | by Gaurav Ghati | Medium", "url": "https://medium.com/@gauravghati/comparison-between-bert-gpt-2-and-elmo-9ad140cd1cda", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@gauravghati/comparison-between-<b>bert</b>-gpt-2-and-elmo-9ad140cd1cda", "snippet": "ELMo u s es <b>bidirectional</b> <b>language</b> <b>model</b> (biLM) which is pre-trained on a large text corpus, to learn both words (e.g., syntax and semantics) and linguistic context (i.e., to <b>model</b> polysemy).", "dateLastCrawled": "2022-02-02T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Extending <b>Bidirectional</b> <b>Language</b> <b>Model</b> for Enhancing the Performance of ...", "url": "https://link.springer.com/chapter/10.1007%2F978-981-15-3125-5_15", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-981-15-3125-5_15", "snippet": "In order to improve the performance of sentiment analyser, we have used a <b>bidirectional</b> <b>language</b> <b>model</b>\u2014embeddings from <b>language</b> <b>model</b> layer (ELMo). We have merged the Natural <b>language</b> processing and Deep learning techniques to analyse and achieve an important emotion from the long sentences fed to the system. The proposed system is <b>compared</b> with state-of-art approaches such as\u2014Simple Multilayer Perceptron, recurrent neural network (RNN) and LSTM. These techniques are thus <b>being</b> applied ...", "dateLastCrawled": "2022-01-29T09:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "10 <b>Leading Language Models For NLP</b> In 2021 - TOPBOTS", "url": "https://www.topbots.com/leading-nlp-language-models-2020/", "isFamilyFriendly": true, "displayUrl": "https://www.topbots.com/leading-nlp-<b>language</b>-<b>models</b>-2020", "snippet": "We introduce a new <b>language</b> representation <b>model</b> called BERT, which stands for <b>Bidirectional</b> Encoder Representations from Transformers. Unlike recent <b>language</b> representation models, BERT is designed to pre-train deep <b>bidirectional</b> representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations <b>can</b> be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as ...", "dateLastCrawled": "2022-02-02T10:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is <b>bidirectional</b> communication? \u2013 Easierwithpractice.com", "url": "https://easierwithpractice.com/what-is-bidirectional-communication/", "isFamilyFriendly": true, "displayUrl": "https://easierwithpractice.com/what-is-<b>bidirectional</b>-communication", "snippet": "Which communication <b>model</b> is <b>bidirectional</b>? The two-way (or <b>bidirectional</b>) relaying network <b>model</b> has attracted considerable research interest <b>compared</b> with the various traditional cooperative communication protocols. In particular, the two-way DF relaying network consists of a transmission phase and a broadcast phase. What is <b>bidirectional</b> support? What does the <b>Bidirectional</b> Support checkbox do? The setting, found on the Ports tab when looking at the Printer Properties menu in Windows ...", "dateLastCrawled": "2022-01-25T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>Bidirectional</b> Transformer Based Alignment <b>Model</b> for Unsupervised Word ...", "url": "https://aclanthology.org/2021.acl-long.24.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/2021.acl-long.24.pdf", "snippet": "A <b>Bidirectional</b> Transformer Based Alignment <b>Model</b> for Unsupervised Word Alignment Jingyi Zhang 1and Josef van Genabith;2 1German Research Center for Arti\ufb01cial Intelligence (DFKI), Saarland Informatics Campus, Saarbrucken, Germany\u00a8 2Department of <b>Language</b> Science and Technology, Saarland University, Saarland Informatics Campus, Saarbrucken, Germany\u00a8 Jingyi.Zhang@dfki.de,Josef.Van Genabith@dfki.de Abstract Word alignment and machine translation are two closely related tasks. Neural transla ...", "dateLastCrawled": "2022-01-28T21:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Explanation of BERT <b>Model</b> - NLP - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/explanation-of-bert-model-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/explanation-of-bert-<b>model</b>-nlp", "snippet": "ELMo gained its <b>language</b> understanding from <b>being</b> trained to predict the next word in a sequence of words \u2013 a task called <b>Language</b> Modeling. This is convenient because we have vast amounts of text data that such a <b>model</b> <b>can</b> learn from without labels <b>can</b> be trained. ULM-Fit: Transfer Learning In NLP: ULM-Fit introduces a new <b>language</b> <b>model</b> and process to effectively fine-tuned that <b>language</b> <b>model</b> for the specific task. This enables NLP architecture to perform transfer learning on a pre ...", "dateLastCrawled": "2022-02-02T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Mixed Reality as a <b>Bidirectional</b> Communication Interface for <b>Human</b> ...", "url": "https://h2r.cs.brown.edu/wp-content/uploads/rosen20pvdpomdp.pdf", "isFamilyFriendly": true, "displayUrl": "https://h2r.cs.brown.edu/wp-content/uploads/rosen20pvdpomdp.pdf", "snippet": "Mixed Reality as a <b>Bidirectional</b> Communication Interface for <b>Human</b>-Robot Interaction Eric Rosen , David Whitney , Michael Fishman , Daniel Ullman and Stefanie Tellex Department of Computer Science, Brown University These authors contributed equally Abstract\u2014We present a decision-theoretic <b>model</b> and robot system that interprets multimodal <b>human</b> communication to disambiguate item references by asking questions via a mixed reality (MR) interface. Existing approaches have either chosen to use ...", "dateLastCrawled": "2021-09-09T16:49:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Language</b> Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>language</b>", "snippet": "<b>bidirectional</b> <b>language</b> <b>model</b>. #<b>language</b>. A <b>language</b> <b>model</b> that determines the probability that a given token is present at a given location in an excerpt of text based on the preceding and following text. BLEU (Bilingual Evaluation Understudy) #<b>language</b>. A score between 0.0 and 1.0, inclusive, indicating the quality of a translation between two human languages (for example, between English and Russian). A BLEU score of 1.0 indicates a perfect translation; a BLEU score of 0.0 indicates a ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Introduction to Text Representations for <b>Language</b> Processing \u2014 Part 2 ...", "url": "https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-text-<b>representation</b>s-for-<b>language</b>...", "snippet": "It is a new self-supervised <b>learning</b> task for pre-training transformers in order to fine-tune them for downstream tasks. BERT uses the <b>bidirectional</b> context of <b>language</b> <b>model</b> i:e it tries to mask both left-to-right &amp; right-to-left to create intermediate tokens to be used for the prediction tasks hence the term <b>bidirectional</b>.", "dateLastCrawled": "2022-01-31T04:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "8.3. <b>Language</b> Models and the Dataset \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://www.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html", "isFamilyFriendly": true, "displayUrl": "https://www.d2l.ai/chapter_recurrent-neural-networks/<b>language</b>-<b>models</b>-and-dataset.html", "snippet": "For instance, an ideal <b>language</b> <b>model</b> would be able to generate natural text just on its own, simply by drawing one token at a time \\(x_t \\sim P(x_t \\mid x_{t-1}, \\ldots, x_1)\\). Quite unlike the monkey using a typewriter, all text emerging from such a <b>model</b> would pass as natural <b>language</b>, e.g., English text. Furthermore, it would be sufficient for generating a meaningful dialog, simply by conditioning the text on previous dialog fragments. Clearly we are still very far from designing such a ...", "dateLastCrawled": "2022-01-31T05:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Intuitive Introduction to BERT \u2013 MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/2021/01/04/intuitive-introduction-to-bert/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2021/01/04/<b>intuitive-introduction-to-bert</b>", "snippet": "We do however often want to create a <b>machine</b> <b>learning</b> <b>model</b> that can perform one task really well. This is where finetuning comes in: using a labeled corpus, which is often smaller, we can then train the pretrained <b>model</b> further, with an additional or replacing NLP task. The end result is a <b>model</b> that has been pretrained on the large unlabeled corpus and which is finetuned to a specific <b>language</b> task, such as summarization, text generation in a particular domain, or translation.", "dateLastCrawled": "2022-01-30T22:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Overview of Word Embedding using Embeddings from <b>Language</b> Models (ELMo ...", "url": "https://www.geeksforgeeks.org/overview-of-word-embedding-using-embeddings-from-language-models-elmo/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/overview-of-word-embedding-using-embeddings-from...", "snippet": "Embeddings from <b>Language</b> Models(ELMo) : ELMo is an NLP framework developed by AllenNLP. ELMo word vectors are calculated using a two-layer <b>bidirectional</b> <b>language</b> <b>model</b> (biLM). Each layer comprises forward and backward pass. Unlike Glove and Word2Vec, ELMo represents embeddings for a word using the complete sentence containing that word. Therefore, ELMo embeddings are able to capture the context of the word used in the sentence and can generate different embeddings for the same word used in a ...", "dateLastCrawled": "2022-02-02T13:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An introduction to Deep <b>Learning</b> in Natural <b>Language</b> Processing: Models ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221010997", "snippet": "Similarly to common classification problems in <b>Machine</b> <b>Learning</b>, ... ELMo looks at the entire sentence producing a contextualized word embedding through a <b>bidirectional</b> <b>language</b> <b>model</b>. The network is a multilayer LSTM (see Fig. 7) pre-trained on unlabeled data. Most important, the authors showed mechanisms to use internal representations in downstream tasks by fine-tuning the network, improving results on several benchmarks. Download : Download high-res image (83KB) Download : Download full ...", "dateLastCrawled": "2022-01-04T17:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>language</b> of <b>proteins: NLP, machine learning &amp; protein sequences</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2001037021000945", "snippet": "Natural <b>language</b> processing (NLP) is a field of computer science concerned with automated text and <b>language</b> analysis. In recent years, following a series of breakthroughs in deep and <b>machine</b> <b>learning</b>, NLP methods have shown overwhelming progress. Here, we review the success, promise and pitfalls of applying NLP algorithms to the study of proteins.", "dateLastCrawled": "2022-02-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Bidirectional</b> <b>Deep Learning of Context Representation for</b> Joint ...", "url": "https://www.researchgate.net/publication/318167037_Bidirectional_Deep_Learning_of_Context_Representation_for_Joint_Word_Segmentation_and_POS_Tagging", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/318167037_<b>Bidirectional</b>_Deep_<b>Learning</b>_of...", "snippet": "This paper aims to study the effect of applying deep <b>learning</b> in <b>machine</b> translation processes including word segmentation and translation <b>model</b> generation. We compare the results of the process ...", "dateLastCrawled": "2022-01-02T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "9.5. <b>Machine Translation</b> and the Dataset \u2014 Dive into Deep <b>Learning</b> 0.17 ...", "url": "https://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/<b>machine-translation</b>-and-dataset.html", "snippet": "Different from our <b>language</b> <b>model</b> problem in Section 8.3 whose corpus is in one single <b>language</b>, <b>machine translation</b> datasets are composed of pairs of text sequences that are in the source <b>language</b> and the target <b>language</b>, respectively. Thus, instead of reusing the preprocessing routine for <b>language</b> modeling, we need a different way to preprocess <b>machine translation</b> datasets. In the following, we show how to load the preprocessed data into minibatches for training.", "dateLastCrawled": "2022-01-29T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Andrew-NG-Notes/andrewng-p-5-sequence-models.md at master ... - <b>GitHub</b>", "url": "https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-models.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/.../Andrew-NG-Notes/blob/master/andrewng-p-5-sequence-<b>models</b>.md", "snippet": "What is a <b>language</b> <b>model</b>. Let&#39;s say we are solving a speech recognition problem and someone says a sentence that can be interpreted into to two sentences: The apple and pair salad; The apple and pear salad; Pair and pear sounds exactly the same, so how would a speech recognition application choose from the two. That&#39;s where the <b>language</b> <b>model</b> comes in. It gives a probability for the two sentences and the application decides the best based on this probability. The job of a <b>language</b> <b>model</b> is ...", "dateLastCrawled": "2022-02-03T05:14:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bidirectional language model)  is like +(human being)", "+(bidirectional language model) is similar to +(human being)", "+(bidirectional language model) can be thought of as +(human being)", "+(bidirectional language model) can be compared to +(human being)", "machine learning +(bidirectional language model AND analogy)", "machine learning +(\"bidirectional language model is like\")", "machine learning +(\"bidirectional language model is similar\")", "machine learning +(\"just as bidirectional language model\")", "machine learning +(\"bidirectional language model can be thought of as\")", "machine learning +(\"bidirectional language model can be compared to\")"]}