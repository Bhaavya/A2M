{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Process</b> | AI Strategy &amp; Policy Blog", "url": "https://aistrategyblog.com/tag/markov-decision-process/", "isFamilyFriendly": true, "displayUrl": "https://aistrategyblog.com/tag/<b>markov-decision-process</b>", "snippet": "The survey, \u201cCorporate data-driven <b>decision</b> making and the role of Artificial Intelligence in the <b>decision</b> making <b>process</b>\u201d, reveals the general perception of the corporate data-driven environment available to corporate <b>decision maker</b>, e.g., the structure and perceived quality of available data. Furthermore, the survey explores the <b>decision</b> makers\u2019 opinions about bias in available data and applied tooling, as well as their own and their peers biases and possible impact on their ...", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>MDP</b> | AI Strategy &amp; Policy Blog", "url": "https://aistrategyblog.com/category/mdp/", "isFamilyFriendly": true, "displayUrl": "https://aistrategyblog.com/category/<b>mdp</b>", "snippet": "We all make use of a <b>Markov</b> Chain and a <b>Markov Decision Process</b> pretty much every day, many many times a day. The pages of the world wide web, with its 1.5+ billion indexed pages, can be designated states of a humongous huge <b>Markov</b> chain. And the in excess of 150+ billion hyperlinks, between those web pages, can be seen as the equivalent of <b>Markov</b> state transitions, taken us from one State (web page) to another State (another web page). The <b>Markov Decision Process</b> value and policy iteration ...", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Markov Decision Processes with Multiple Objectives</b>", "url": "https://www.researchgate.net/publication/220994894_Markov_Decision_Processes_with_Multiple_Objectives", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220994894_<b>Markov</b>_<b>Decision</b>_<b>Process</b>es_with...", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) is an appropriate mathematical framework for analysis and modeling a large class of sequential <b>decision</b>\u2010making problems. Real\u2010world applications necessitate the ...", "dateLastCrawled": "2021-11-14T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "MITx_6.86x/Unit 05 - Reinforcement Learning.md at master - <b>GitHub</b>", "url": "https://github.com/sylvaticus/MITx_6.86x/blob/master/Unit%2005%20-%20Reinforcement%20Learning/Unit%2005%20-%20Reinforcement%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/sylvaticus/MITx_6.86x/blob/master/Unit 05 - Reinforcement Learning...", "snippet": "Indeed: &quot;A <b>Markov decision process</b> (<b>MDP</b>) is a discrete time stochastic control <b>process</b>. It provides a mathematical framework for modeling <b>decision</b> making in situations where outcomes are partly random and partly under the control of a <b>decision maker</b>. <b>Markov</b> <b>decision</b> processes are an extension of <b>Markov</b> chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. &quot;wait&quot;) and all rewards are the ...", "dateLastCrawled": "2021-11-21T01:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Modelling</b> Methods for Pharmacoeconomics and Health Technology ...", "url": "https://www.academia.edu/26845273/Modelling_Methods_for_Pharmacoeconomics_and_Health_Technology_Assessment_An_Overview_and_Guide", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/26845273/<b>Modelling</b>_Methods_for_Pharmacoeconomics_and_Health...", "snippet": "Time A <b>Markov Decision Process</b> (<b>MDP</b>) [see figure 5] horizon refers to the time period in which the prob- is a way to describe and analyse sequential decisions lem is framed. For example, a problem examining under conditions of uncertainty. These are structur- the prevention of MI through cholesterol reduction ally similar to <b>Markov</b> Chains except that the transi- may have a lifetime time horizon, whereas a prob- lem examining a new hospital guideline to reduce iatrogenic infections may have a ...", "dateLastCrawled": "2022-01-30T20:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Logically-Correct Reinforcement Learning</b> | DeepAI", "url": "https://deepai.org/publication/logically-correct-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>logically-correct-reinforcement-learning</b>", "snippet": "Definition 1 (<b>Markov Decision Process</b> (<b>MDP</b>)) An <b>MDP</b> M = ( \\allowbreak S , \\allowbreak A , \\allowbreak s 0 , \\allowbreak P , \\allowbreak A P , \\allowbreak L ) is a tuple over a finite set of states S where A is a finite set of actions, s 0 is the initial state and P : S \u00d7 A \u00d7 S \u2192 [ 0 , 1 ] is the transition probability function which determines probability of moving from a given state to another by taking an action.", "dateLastCrawled": "2022-01-22T11:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>markov decision process</b> example code - Engage Headlines", "url": "http://engageheadlines.com/topic/f98023-markov-decision-process-example-code", "isFamilyFriendly": true, "displayUrl": "engageheadlines.com/topic/f98023-<b>markov-decision-process</b>-example-code", "snippet": "For an overview of <b>Markov</b> chains in general state space, see <b>Markov</b> chains on a measurable state space. <b>Markov Decision Process</b> : It is <b>Markov</b> Reward <b>Process</b> with a decisions.Everything is same <b>like</b> MRP but now we have actual agency that makes decisions or take actions. P and R will have slight change w.r.t actions as follows : Now, our reward function is dependent on the action. For example, in racing games, we start the game (start the race) and play it until the game is over (race ends ...", "dateLastCrawled": "2021-12-11T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Sensors | Free Full-Text | Machine Learning Aided Scheme for Load ...", "url": "https://www.mdpi.com/1424-8220/18/11/3779/html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>mdp</b>i.com/1424-8220/18/11/3779/html", "snippet": "Additionally, our method employs a <b>Markov Decision Process</b> (<b>MDP</b>) to determine whether a BS needs to be balanced or not. For both techniques, the data are obtained from a real IoT LoRaWAN network deployed in an urban area, which is the use case scenario for our solution. To the best of our knowledge, this paper is the first one that presents a solution to the load balancing problem applied to a LoRaWAN network.", "dateLastCrawled": "2022-01-06T15:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>review On reinforcement learning: Introduction and applications</b> in ...", "url": "https://www.sciencedirect.com/science/article/pii/S0098135420300557", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0098135420300557", "snippet": "Relating to <b>process</b> control, the agent would be the controller logic and everything else would make up the system. Reinforcement learning\u2019s <b>decision</b> making <b>process</b> is formalized in the <b>Markov decision process</b> (<b>MDP</b>). Download : Download high-res image (53KB) Download : Download full-size image; Fig. 3. The general <b>Markov decision process</b> ...", "dateLastCrawled": "2022-01-14T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Program</b> for Wednesday, November 21st", "url": "https://easychair.org/smart-program/PGMODAYS2018/2018-11-21.html", "isFamilyFriendly": true, "displayUrl": "https://easychair.org/smart-<b>program</b>/PGMODAYS2018/2018-11-21.html", "snippet": "Influence Diagrams (ID) are a flexible tool to represent discrete stochastic optimization problems, including <b>Markov Decision Process</b> (<b>MDP</b>) and Partially Observable <b>MDP</b> as standard examples. More precisely, given random variables considered as vertices of an acyclic digraph, a probabilistic graphical model defines a joint distribution via the conditional distributions of vertices given their parents. In ID, the random variables are represented by a probabilistic graphical model whose ...", "dateLastCrawled": "2022-01-12T13:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Markov decision process</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Markov_decision_process", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Markov_decision_process</b>", "snippet": "In mathematics, a <b>Markov decision process</b> (<b>MDP</b>) is a discrete-time stochastic control <b>process</b>. It provides a mathematical framework for modeling <b>decision</b> making in situations where outcomes are partly random and partly under the control of a <b>decision maker</b>. MDPs are useful for studying optimization problems solved via dynamic programming.MDPs were known at least as early as the 1950s; a core body of research on <b>Markov</b> <b>decision</b> processes resulted from Ronald Howard&#39;s 1960 book, Dynamic ...", "dateLastCrawled": "2022-02-07T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b> | AI Strategy &amp; Policy Blog", "url": "https://aistrategyblog.com/tag/markov-decision-process/", "isFamilyFriendly": true, "displayUrl": "https://aistrategyblog.com/tag/<b>markov-decision-process</b>", "snippet": "The survey, \u201cCorporate data-driven <b>decision</b> making and the role of Artificial Intelligence in the <b>decision</b> making <b>process</b>\u201d, reveals the general perception of the corporate data-driven environment available to corporate <b>decision maker</b>, e.g., the structure and perceived quality of available data. Furthermore, the survey explores the <b>decision</b> makers\u2019 opinions about bias in available data and applied tooling, as well as their own and their peers biases and possible impact on their ...", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>MDP</b> | AI Strategy &amp; Policy Blog", "url": "https://aistrategyblog.com/category/mdp/", "isFamilyFriendly": true, "displayUrl": "https://aistrategyblog.com/category/<b>mdp</b>", "snippet": "We all make use of a <b>Markov</b> Chain and a <b>Markov Decision Process</b> pretty much every day, many many times a day. The pages of the world wide web, with its 1.5+ billion indexed pages, can be designated states of a humongous huge <b>Markov</b> chain. And the in excess of 150+ billion hyperlinks, between those web pages, can be seen as the equivalent of <b>Markov</b> state transitions, taken us from one State (web page) to another State (another web page). The <b>Markov Decision Process</b> value and policy iteration ...", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Modelling</b> Methods <b>for Pharmacoeconomics and Health Technology</b> ...", "url": "https://link.springer.com/article/10.2165/00019053-200826020-00004", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.2165/00019053-200826020-00004", "snippet": "A <b>Markov Decision Process</b> (<b>MDP</b>) [see figure 5] is a way to describe and analyse sequential decisions under conditions of uncertainty. These are structurally <b>similar</b> to <b>Markov</b> Chains except that the transition matrix depends on the actions or policy of the <b>decision maker</b> at each time increment. <b>MDP</b> models are also known as sequential stochastic optimizations, discrete-time stochastic control processes, stochastic dynamic programming, etc. Another way to think of MDPs is as a series of ...", "dateLastCrawled": "2022-01-25T06:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "MITx_6.86x/Unit 05 - Reinforcement Learning.md at master - <b>GitHub</b>", "url": "https://github.com/sylvaticus/MITx_6.86x/blob/master/Unit%2005%20-%20Reinforcement%20Learning/Unit%2005%20-%20Reinforcement%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/sylvaticus/MITx_6.86x/blob/master/Unit 05 - Reinforcement Learning...", "snippet": "Indeed: &quot;A <b>Markov decision process</b> (<b>MDP</b>) is a discrete time stochastic control <b>process</b>. It provides a mathematical framework for modeling <b>decision</b> making in situations where outcomes are partly random and partly under the control of a <b>decision maker</b>. <b>Markov</b> <b>decision</b> processes are an extension of <b>Markov</b> chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. &quot;wait&quot;) and all rewards are the ...", "dateLastCrawled": "2021-11-21T01:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Modelling</b> Methods for Pharmacoeconomics and Health Technology ...", "url": "https://www.academia.edu/26845273/Modelling_Methods_for_Pharmacoeconomics_and_Health_Technology_Assessment_An_Overview_and_Guide", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/26845273/<b>Modelling</b>_Methods_for_Pharmacoeconomics_and_Health...", "snippet": "Time A <b>Markov Decision Process</b> (<b>MDP</b>) [see figure 5] horizon refers to the time period in which the prob- is a way to describe and analyse sequential decisions lem is framed. For example, a problem examining under conditions of uncertainty. These are structur- the prevention of MI through cholesterol reduction ally <b>similar</b> to <b>Markov</b> Chains except that the transi- may have a lifetime time horizon, whereas a prob- lem examining a new hospital guideline to reduce iatrogenic infections may have a ...", "dateLastCrawled": "2022-01-30T20:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Sensors | Free Full-Text | Machine Learning Aided Scheme for Load ...", "url": "https://www.mdpi.com/1424-8220/18/11/3779/html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>mdp</b>i.com/1424-8220/18/11/3779/html", "snippet": "Additionally, our method employs a <b>Markov Decision Process</b> (<b>MDP</b>) to determine whether a BS needs to be balanced or not. For both techniques, the data are obtained from a real IoT LoRaWAN network deployed in an urban area, which is the use case scenario for our solution. To the best of our knowledge, this paper is the first one that presents a solution to the load balancing problem applied to a LoRaWAN network.", "dateLastCrawled": "2022-01-06T15:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>review On reinforcement learning: Introduction and applications</b> in ...", "url": "https://www.sciencedirect.com/science/article/pii/S0098135420300557", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0098135420300557", "snippet": "Relating to <b>process</b> control, the agent would be the controller logic and everything else would make up the system. Reinforcement learning\u2019s <b>decision</b> making <b>process</b> is formalized in the <b>Markov decision process</b> (<b>MDP</b>). Download : Download high-res image (53KB) Download : Download full-size image; Fig. 3. The general <b>Markov decision process</b> ...", "dateLastCrawled": "2022-01-14T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ICML Beijing", "url": "https://icml.cc/Conferences/2014/icml2014keywords/index/article/all.html", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/Conferences/2014/icml2014keywords/index/article/all.html", "snippet": "We consider the problem of controlling a <b>Markov decision process</b> (<b>MDP</b>) with a large state space, so as to minimize average cost. Since it is intractable to compete with the optimal policy for large scale problems, we pursue the more modest goal of competing with a low-dimensional family of policies. We use the dual linear programming formulation of the <b>MDP</b> average cost problem, in which the variable is a stationary distribution over state-action pairs, and we consider a neighborhood of a low ...", "dateLastCrawled": "2022-01-10T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Program</b> for Wednesday, November 21st", "url": "https://easychair.org/smart-program/PGMODAYS2018/2018-11-21.html", "isFamilyFriendly": true, "displayUrl": "https://easychair.org/smart-<b>program</b>/PGMODAYS2018/2018-11-21.html", "snippet": "Influence Diagrams (ID) are a flexible tool to represent discrete stochastic optimization problems, including <b>Markov Decision Process</b> (<b>MDP</b>) and Partially Observable <b>MDP</b> as standard examples. More precisely, given random variables considered as vertices of an acyclic digraph, a probabilistic graphical model defines a joint distribution via the conditional distributions of vertices given their parents. In ID, the random variables are represented by a probabilistic graphical model whose ...", "dateLastCrawled": "2022-01-12T13:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Process</b> | AI Strategy &amp; Policy Blog", "url": "https://aistrategyblog.com/tag/markov-decision-process/", "isFamilyFriendly": true, "displayUrl": "https://aistrategyblog.com/tag/<b>markov-decision-process</b>", "snippet": "The survey, \u201cCorporate data-driven <b>decision</b> making and the role of Artificial Intelligence in the <b>decision</b> making <b>process</b>\u201d, reveals the general perception of the corporate data-driven environment available to corporate <b>decision maker</b>, e.g., the structure and perceived quality of available data. Furthermore, the survey explores the <b>decision</b> makers\u2019 opinions about bias in available data and applied tooling, as well as their own and their peers biases and possible impact on their ...", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Processes with Multiple Objectives</b>", "url": "https://www.researchgate.net/publication/220994894_Markov_Decision_Processes_with_Multiple_Objectives", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220994894_<b>Markov</b>_<b>Decision</b>_<b>Process</b>es_with...", "snippet": "RL <b>can</b> also be employed to develop a harmonic and efficient traffic system. Wang et al [187] formulate lanechanging for each vehicle as a <b>Markov decision process</b> (<b>MDP</b>) [208]. This <b>MDP</b> is based on ...", "dateLastCrawled": "2021-11-14T08:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Chapter 51 <b>Structural estimation of markov decision</b> processes ...", "url": "https://www.sciencedirect.com/science/article/pii/S1573441205800200", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1573441205800200", "snippet": "Ch. 51: <b>Structural Estimation of Markov Decision</b> Processes 3095 follows that limN_, o~ flNE~ = O, guaranteeing the invertibility of [I- flEo] for any Markovian <b>decision</b> rule 6 and all fie [O, 1).15 2.5. Overview of solution methods This section provides a brief review of solution methods for <b>MDP</b>&#39;s.", "dateLastCrawled": "2022-01-13T18:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Everything you need to know about Reinforcement Learning in 80 minutes ...", "url": "https://srianumakonda.medium.com/everything-you-need-to-know-about-reinforcement-learning-in-80minutes-4cd5a365e340", "isFamilyFriendly": true, "displayUrl": "https://srianumakonda.medium.com/everything-you-need-to-know-about-reinforcement...", "snippet": "The agent state = environment state = information state. O\u209c = S\u1d43\u209c = S\u1d49\u209c. This is formally defined as the <b>Markov Decision Process</b> (<b>MDP</b>). Partially observable environments. The agent indirectly observes the environment. Think of it like a robot with camera vision; it isn\u2019t told where it actually is [location]. Agent state \u2260environment state. This is formally defined as the partially observable <b>Markov Decision process</b> (POMDP). In this environment, the agent has to construct its ...", "dateLastCrawled": "2022-01-30T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>MDP</b> | AI Strategy &amp; Policy Blog", "url": "https://aistrategyblog.com/category/mdp/", "isFamilyFriendly": true, "displayUrl": "https://aistrategyblog.com/category/<b>mdp</b>", "snippet": "Based on those insights the <b>decision maker</b> <b>can</b> then form ideas or hypotheses that may support in formulating relevant data-driven decisions. In this <b>process</b>, the consequences of a made <b>decision</b> may or may not be directly measured missing out on the opportunity to close-the-loop on the business data-driven <b>decision</b> <b>process</b>. In fact, it may not even be meaningful to attempt to close-the-loop due to the structure of data required or vagueness of the <b>decision</b>-foundation. The \u201cbig-data-<b>decision</b> ...", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A simple method for dealing with large state spaces - Schapaugh - 2012 ...", "url": "https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00242.x", "isFamilyFriendly": true, "displayUrl": "https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00242.x", "snippet": "<b>Markov</b> <b>decision</b> processes (Bellman 1957) provide a mathematical framework for <b>modelling</b> sequential <b>decision</b>-making problems. As the name implies, MDPs are an extension of <b>Markov</b> chains; the difference is the addition of actions (to influence the state of the system) and rewards (giving motivation). An <b>MDP</b> is defined by the following components: a set of states:", "dateLastCrawled": "2021-12-10T17:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Modelling</b> Methods for Pharmacoeconomics and Health Technology ...", "url": "https://www.academia.edu/26845273/Modelling_Methods_for_Pharmacoeconomics_and_Health_Technology_Assessment_An_Overview_and_Guide", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/26845273/<b>Modelling</b>_Methods_for_Pharmacoeconomics_and_Health...", "snippet": "Time A <b>Markov Decision Process</b> (<b>MDP</b>) [see figure 5] horizon refers to the time period in which the prob- is a way to describe and analyse sequential decisions lem is framed. For example, a problem examining under conditions of uncertainty. These are structur- the prevention of MI through cholesterol reduction ally similar to <b>Markov</b> Chains except that the transi- may have a lifetime time horizon, whereas a prob- lem examining a new hospital guideline to reduce iatrogenic infections may have a ...", "dateLastCrawled": "2022-01-30T20:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Bisimulation Metrics for Continuous Markov Decision Processes</b> ...", "url": "https://www.academia.edu/2750715/Bisimulation_Metrics_for_Continuous_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2750715", "snippet": "In recent years, various metrics have been developed for measuring the behavioral similarity of states in probabilistic transition systems [J. Desharnais et al., Proceedings of CONCUR&#39;99, Springer-Verlag, London, 1999, pp. 258-273; F. van Breugel", "dateLastCrawled": "2022-01-17T02:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Formalization of Continuous Time <b>Markov</b> Chains with Applications in ...", "url": "https://tu.research-learning.net.ru/93", "isFamilyFriendly": true, "displayUrl": "https://tu.research-learning.net.ru/93", "snippet": "The <b>markov</b> <b>process</b> consists of a sequence of states that strictly obey the <b>markov</b> property. The future depends only on the current state s and action a but not on the past, it is formulated as a <b>markov decision process</b> (<b>mdp</b>). Continuous time <b>markov</b> chain (dtmc and ctmc), is the solution of a difference or differential equation respectively ...", "dateLastCrawled": "2022-01-26T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Modeling, simulation, and <b>decision</b> support - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/B9780128200742000174", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/B9780128200742000174", "snippet": "In an environment that <b>can</b> be stated in the form of a finite <b>Markov decision process</b> (<b>MDP</b>), RL algorithms <b>can</b> be utilized to discover an optimal action-selection policy. Krause and Andersson [78] analyzed multiple congestion management mechanisms employing an agent-based model, in which GenCos learn according to the Q-learning algorithm.", "dateLastCrawled": "2021-12-11T15:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Process</b> | AI Strategy &amp; Policy Blog", "url": "https://aistrategyblog.com/tag/markov-decision-process/", "isFamilyFriendly": true, "displayUrl": "https://aistrategyblog.com/tag/<b>markov-decision-process</b>", "snippet": "The survey, \u201cCorporate data-driven <b>decision</b> making and the role of Artificial Intelligence in the <b>decision</b> making <b>process</b>\u201d, reveals the general perception of the corporate data-driven environment available to corporate <b>decision maker</b>, e.g., the structure and perceived quality of available data. Furthermore, the survey explores the <b>decision</b> makers\u2019 opinions about bias in available data and applied tooling, as well as their own and their peers biases and possible impact on their ...", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Logically-Correct Reinforcement Learning</b> | DeepAI", "url": "https://deepai.org/publication/logically-correct-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>logically-correct-reinforcement-learning</b>", "snippet": "Definition 1 (<b>Markov Decision Process</b> (<b>MDP</b>)) An <b>MDP</b> M = ( \\allowbreak S , \\allowbreak A , \\allowbreak s 0 , \\allowbreak P , \\allowbreak A P , \\allowbreak L ) is a tuple over a finite set of states S where A is a finite set of actions, s 0 is the initial state and P : S \u00d7 A \u00d7 S \u2192 [ 0 , 1 ] is the transition probability function which determines probability of moving from a given state to another by taking an action.", "dateLastCrawled": "2022-01-22T11:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "MITx_6.86x/Unit 05 - Reinforcement Learning.md at master - <b>GitHub</b>", "url": "https://github.com/sylvaticus/MITx_6.86x/blob/master/Unit%2005%20-%20Reinforcement%20Learning/Unit%2005%20-%20Reinforcement%20Learning.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/sylvaticus/MITx_6.86x/blob/master/Unit 05 - Reinforcement Learning...", "snippet": "Indeed: &quot;A <b>Markov decision process</b> (<b>MDP</b>) is a discrete time stochastic control <b>process</b>. It provides a mathematical framework for modeling <b>decision</b> making in situations where outcomes are partly random and partly under the control of a <b>decision maker</b>. <b>Markov</b> <b>decision</b> processes are an extension of <b>Markov</b> chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. &quot;wait&quot;) and all rewards are the ...", "dateLastCrawled": "2021-11-21T01:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Embedding a state space model into a Markov decision process</b>", "url": "https://www.researchgate.net/publication/220461617_Embedding_a_state_space_model_into_a_Markov_decision_process", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220461617_<b>Embedding_a_state_space_model</b>_into...", "snippet": "A hierar chical <b>MDP</b> (HMDP) is an in\ufb01nite stage <b>Markov decision process</b> with parameters de\ufb01ned in a special way , but nev ertheless in accordance with all usual rules and conditions relating to ...", "dateLastCrawled": "2022-01-18T23:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>MDP</b> | AI Strategy &amp; Policy Blog", "url": "https://aistrategyblog.com/category/mdp/", "isFamilyFriendly": true, "displayUrl": "https://aistrategyblog.com/category/<b>mdp</b>", "snippet": "Based on those insights the <b>decision maker</b> <b>can</b> then form ideas or hypotheses that may support in formulating relevant data-driven decisions. In this <b>process</b>, the consequences of a made <b>decision</b> may or may not be directly measured missing out on the opportunity to close-the-loop on the business data-driven <b>decision</b> <b>process</b>. In fact, it may not even be meaningful to attempt to close-the-loop due to the structure of data required or vagueness of the <b>decision</b>-foundation. The \u201cbig-data-<b>decision</b> ...", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Modelling</b> Methods for Pharmacoeconomics and Health Technology ...", "url": "https://www.academia.edu/26845273/Modelling_Methods_for_Pharmacoeconomics_and_Health_Technology_Assessment_An_Overview_and_Guide", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/26845273/<b>Modelling</b>_Methods_for_Pharmacoeconomics_and_Health...", "snippet": "Time A <b>Markov Decision Process</b> (<b>MDP</b>) [see figure 5] horizon refers to the time period in which the prob- is a way to describe and analyse sequential decisions lem is framed. For example, a problem examining under conditions of uncertainty. These are structur- the prevention of MI through cholesterol reduction ally similar to <b>Markov</b> Chains except that the transi- may have a lifetime time horizon, whereas a prob- lem examining a new hospital guideline to reduce iatrogenic infections may have a ...", "dateLastCrawled": "2022-01-30T20:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A <b>review On reinforcement learning: Introduction and applications</b> in ...", "url": "https://www.sciencedirect.com/science/article/pii/S0098135420300557", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0098135420300557", "snippet": "Relating to <b>process</b> control, the agent would be the controller logic and everything else would make up the system. Reinforcement learning\u2019s <b>decision</b> making <b>process</b> is formalized in the <b>Markov decision process</b> (<b>MDP</b>). Download : Download high-res image (53KB) Download : Download full-size image; Fig. 3. The general <b>Markov decision process</b> ...", "dateLastCrawled": "2022-01-14T17:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Sensors | Free Full-Text | Machine Learning Aided Scheme for Load ...", "url": "https://www.mdpi.com/1424-8220/18/11/3779/html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>mdp</b>i.com/1424-8220/18/11/3779/html", "snippet": "Additionally, our method employs a <b>Markov Decision Process</b> (<b>MDP</b>) to determine whether a BS needs to be balanced or not. For both techniques, the data are obtained from a real IoT LoRaWAN network deployed in an urban area, which is the use case scenario for our solution. To the best of our knowledge, this paper is the first one that presents a solution to the load balancing problem applied to a LoRaWAN network.", "dateLastCrawled": "2022-01-06T15:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ICML Beijing", "url": "https://icml.cc/Conferences/2014/icml2014keywords/index/article/all.html", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/Conferences/2014/icml2014keywords/index/article/all.html", "snippet": "We consider the problem of controlling a <b>Markov decision process</b> (<b>MDP</b>) with a large state space, so as to minimize average cost. Since it is intractable to compete with the optimal policy for large scale problems, we pursue the more modest goal of competing with a low-dimensional family of policies. We use the dual linear programming formulation of the <b>MDP</b> average cost problem, in which the variable is a stationary distribution over state-action pairs, and we consider a neighborhood of a low ...", "dateLastCrawled": "2022-01-10T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Program</b> for Wednesday, November 21st", "url": "https://easychair.org/smart-program/PGMODAYS2018/2018-11-21.html", "isFamilyFriendly": true, "displayUrl": "https://easychair.org/smart-<b>program</b>/PGMODAYS2018/2018-11-21.html", "snippet": "Influence Diagrams (ID) are a flexible tool to represent discrete stochastic optimization problems, including <b>Markov Decision Process</b> (<b>MDP</b>) and Partially Observable <b>MDP</b> as standard examples. More precisely, given random variables considered as vertices of an acyclic digraph, a probabilistic graphical model defines a joint distribution via the conditional distributions of vertices given their parents. In ID, the random variables are represented by a probabilistic graphical model whose ...", "dateLastCrawled": "2022-01-12T13:49:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does <b>Markov Decision Process</b> matter in Reinforcement <b>Learning</b>? | by ...", "url": "https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-does-malkov-<b>decision</b>-<b>process</b>-matter-in...", "snippet": "It is named by <b>analogy</b> to \u201cone-armed bandit\u201d(= a slot <b>machine</b>) although the framework has k levers instead of one. ... we introduce <b>Markov Decision Process</b>(<b>MDP</b>) to solve such a problem. An <b>MDP</b> consists of two elements; the agent and the environment. The agent is a learner or <b>decision</b>-maker. In the above example, the agent is the rabbit. The environment is everything surrounding the agent. In the example, the environment includes everything in the field where the rabbit is with food and ...", "dateLastCrawled": "2022-01-31T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov decision process</b>: value iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-value-iteration-2d161d50a6ff", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-value-iteration-2d161d50a6ff", "snippet": "<b>Markov decision process</b>, <b>MDP</b>, value iteration, policy iteration, policy evaluation, policy improvement, sweep, iterative policy evaluation, policy, optimal policy ...", "dateLastCrawled": "2022-01-08T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Markov Decision Process</b>: How Does Value Iteration Work? | Baeldung on ...", "url": "https://www.baeldung.com/cs/mdp-value-iteration", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>mdp</b>-value-iteration", "snippet": "From this point, we can make an <b>analogy</b> with the <b>Markov</b> model since the solution for this problem is a sequence of actions. A <b>Markov Decision Process</b> is used to model the agent, considering that the agent itself generates a series of actions. In the real world, we can have observable, hidden, or partially observed states, depending on the ...", "dateLastCrawled": "2022-01-30T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Reinforcement <b>Learning</b> | Ioannis Anifantakis | Analytics Vidhya", "url": "https://medium.com/analytics-vidhya/reinforcement-learning-basic-understanding-4fcb91ba4e4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/reinforcement-<b>learning</b>-basic-understanding-4fcb91ba4e4", "snippet": "<b>Markov Decision Process</b> (<b>MDP</b>) The States, Actions, Rewards, their mechanics (known as One-Step Dynamics ), together with the discount rate (\u03b3) define a <b>Markov Decision Process</b> (<b>MDP</b>) .", "dateLastCrawled": "2021-08-02T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>", "url": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "snippet": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>. <b>Machine</b> <b>Learning</b> [cs.LG]. Uni-versit\u00e9 Paris Saclay (COmUE), 2019. English. \uffffNNT: 2019SACLS588\uffff. \ufffftel-02422144v2\uffff I would like to dedicate this thesis to my loving parents doctorat CLS588 Meta-<b>Learning</b> as a <b>Markov Decision Process</b> Th\u00e8se de doctorat de l\u2019Universit\u00e9 Paris-Saclay pr\u00e9par\u00e9e \u00e0 l\u2019Universit\u00e9 Paris-Sud Ecole doctorale n 580 Sciences et Technologies de l\u2019Information et de la Communication (STIC) Sp\u00e9cialit\u00e9 de doctorat ...", "dateLastCrawled": "2022-01-13T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "AI and Reinforcement <b>Learning</b> \u2014 Machines that Learn through Experience ...", "url": "https://www.cantorsparadise.com/ai-and-reinforcement-learning-machines-that-learn-through-experience-e7eea7bb6765", "isFamilyFriendly": true, "displayUrl": "https://www.cantorsparadise.com/ai-and-reinforcement-<b>learning</b>-<b>machines</b>-that-learn...", "snippet": "This <b>process</b> can be mathematically represented as a <b>Markov Decision Process</b> (<b>MDP</b>). \u201cMDPs are a mathematically idealized form of the reinforcement <b>learning</b> problem for which precise theoretical statements can be made.\u201d \u2014 Richard S. Sutton . The <b>MDP</b> framework is an abstraction of the problem of goal-directed <b>learning</b> from interaction. It proposes that any problem of <b>learning</b> goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment ...", "dateLastCrawled": "2022-01-25T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Reinforcement <b>Learning</b> \u2014 Controversy over Reward | by OperAI ...", "url": "https://operai.medium.com/reinforcement-learning-reward-controversy-issue-e9b88167d238", "isFamilyFriendly": true, "displayUrl": "https://operai.medium.com/reinforcement-<b>learning</b>-reward-controversy-issue-e9b88167d238", "snippet": "Example of RL algorithm is the <b>Markov Decision Process</b> (<b>MDP</b>) and there is a package for applying <b>MDP</b>. Other algorithms and packages are also under development such as the \u201cReinforcementLearning\u201d package, which is intended to partially close this gap and offers the ability to perform model-free reinforcement <b>learning</b> in a highly customizable framework.", "dateLastCrawled": "2022-02-01T20:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Why is the <b>optimal policy</b> in <b>Markov Decision Process</b> ...", "url": "https://stats.stackexchange.com/questions/132890/why-is-the-optimal-policy-in-markov-decision-process-mdp-independent-of-the-i", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132890", "snippet": "The intuition behind the argument saying that the <b>optimal policy</b> is independent of initial state is the following: The <b>optimal policy</b> is defined by a function that selects an action for every possible state and actions in different states are independent.. Formally speaking, for an unknown initial distribution, the value function to maximize would be the following (not conditioned on initial state)", "dateLastCrawled": "2022-01-25T23:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Overview: Representation Techniques", "url": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66fffb5d0bd4c20697922f5ffbf9a602b66bec3f74ac83fb77c/DecisionMaking.pdf", "isFamilyFriendly": true, "displayUrl": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66...", "snippet": "<b>Markov Decision Process MDP is like</b> a Markov process, except every round we make a decision Transition probabilities depend on actions taken P(St+1 = S&#39; | St = s, At = a) = P(S, a, S&#39;) Rewards for every state, action pair u(St = s, At = a) Discount factor \u03b4 Example. A <b>machine</b> can be in one of three states: good, deteriorating, broken Can take ...", "dateLastCrawled": "2022-01-21T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(markov decision process (mdp))  is like +(modelling how a decision-maker might behave)", "+(markov decision process (mdp)) is similar to +(modelling how a decision-maker might behave)", "+(markov decision process (mdp)) can be thought of as +(modelling how a decision-maker might behave)", "+(markov decision process (mdp)) can be compared to +(modelling how a decision-maker might behave)", "machine learning +(markov decision process (mdp) AND analogy)", "machine learning +(\"markov decision process (mdp) is like\")", "machine learning +(\"markov decision process (mdp) is similar\")", "machine learning +(\"just as markov decision process (mdp)\")", "machine learning +(\"markov decision process (mdp) can be thought of as\")", "machine learning +(\"markov decision process (mdp) can be compared to\")"]}