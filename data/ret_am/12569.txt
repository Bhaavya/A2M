{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness Constraints: Mechanisms for Fair Classi\ufb01cation</b>", "url": "http://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v54/zafar17a/zafar17a.pdf", "snippet": "<b>Fairness Constraints: Mechanisms for Fair Classi\ufb01cation</b> Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna P. Gummadi Max Planck Institute for Software Systems (MPI-SWS), Germany Abstract Algorithmic decision making systems are ubiquitous across a wide variety of online as well as o ine services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and pro\ufb01tabil-ity. However, there is ...", "dateLastCrawled": "2022-01-27T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Tutorial on <b>Fairness</b> in Machine Learning | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-machine-learning-3ff8ba1040cb", "snippet": "The notion of individual <b>fairness</b> emphasizes on that: similar individuals should be treated similarly. Formulation: Denote O to be a measurable space and \u0394(O) to be the space of the distribution over O. Denote M:X\u2192 \u0394(O) to be a <b>map</b> that maps each individuals to a distribution of outcomes. The formulation is then: D(M(X),M(X\u2019))\u2264 d(X,X\u2019)", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "On Consequentialism and <b>Fairness</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861221/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861221", "snippet": "While research in machine learning <b>fairness</b> is ongoing, most proposals can be classified into two types, which to some extent <b>map</b> onto the two legal doctrines mentioned above. Some definitions are specified without reference to outcomes (section 5.1). Others are specified exclusively with regard to a particular set of outcomes (which must be evaluated using real data; section 5.2). We summarize the dominant proposals of each type below.", "dateLastCrawled": "2022-01-28T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Classification - <b>Fairness</b> and machine learning", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Object recognition might not even seem <b>like</b> a statistical problem, yet statistical methods came to be the method of choice for many important pattern recognition tasks in computer vision. Supervised learning. A classifier is a mapping from the space of possible values for X to the space of values that the target variable Y can assume. Supervised learning is the prevalent method for constructing classifiers from observed data. The essential idea is very simple. Suppose we have labeled data ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "Saxena et al. (2019), and Srivastava et al. (2019) conducted experiments to find out how public attitudes <b>map</b> to <b>fairness</b> definitions for machine learning. The first paper found calibrated <b>fairness</b> tends to be preferred in the context of loan decisions while the second study found that demographic parity most closely matches people&#39;s idea of <b>fairness</b> as applied to criminal risk prediction and skin cancer risk prediction.", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Reading papers on <b>fairness</b> of recommendation system (4) | Develop Paper", "url": "https://developpaper.com/reading-papers-on-fairness-of-recommendation-system-4/", "isFamilyFriendly": true, "displayUrl": "https://developpaper.com/reading-papers-on-<b>fairness</b>-of-recommendation-system-4", "snippet": "Modeling on the framework of recommendation system based on reinforcement learning can ensure long-term <b>fairness</b>, rather than only short-term <b>fairness</b> <b>like</b> most papers; The second is to treat the <b>fairness</b> <b>constraint</b> as a <b>constraint</b> of the well-known confidence region optimization algorithm in reinforcement learning, which is concise and clear;", "dateLastCrawled": "2022-01-14T14:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Bias and <b>Fairness</b> in Machine Learning, Part 3: building a bias-aware ...", "url": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-3-building-a-bias-aware-model/", "isFamilyFriendly": true, "displayUrl": "https://freecontent.manning.com/bias-and-<b>fairness</b>-in-machine-learning-part-3-building...", "snippet": "This will help to remove the disparate impact that this column would have on our group <b>fairness</b>. As pseudocode, it would look <b>like</b> For each group label: Get the subset of priors_count values for that group Apply the yeo-johnson transformation to the subset Modify the column in place for that group label with the new values By applying the transformation on each subset of values rather than applying to the column as a whole, we are forcing each group\u2019s set of values to be normal with a mean ...", "dateLastCrawled": "2022-02-02T03:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Controllable <b>Fairness</b> in Machine Learning | SAIL Blog", "url": "https://ai.stanford.edu/blog/controllable-fairness/", "isFamilyFriendly": true, "displayUrl": "https://ai.stanford.edu/blog/controllable-<b>fairness</b>", "snippet": "TLDR: How do we finely control the <b>fairness</b> of machine learning systems? In our AISTATS 2019 paper, we introduce a theoretically grounded method for learning controllable fair representations. Using our method, a party who is concerned with <b>fairness</b> (<b>like</b> a data collector, community organizer, or regulatory body) can convert data to representations with controllable limits on unfairness, then release only the representations. This controls how much downstream machine learning models can ...", "dateLastCrawled": "2022-01-30T20:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Fairness</b> in Ranking &amp; Wrap-Up - Cornell University", "url": "https://www.cs.cornell.edu/courses/cs4780/2019fa/lectures/27-fairrank.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs4780/2019fa/lectures/27-fairrank.pdf", "snippet": "<b>Fairness</b> of Exposure Endogenous Factors How to allocate exposure based on merit in order to \u2022 Satisfy legal requirements \u2022 Shape marketplace dynamics (e.g. Spotify, superstar economics) \u2022 Spam, Polarization Exogenous Factors How to estimate merit without biases <b>like</b> \u2022 Position bias \u2022 Trust bias \u2022 Uncertainty bias \u2022 Historical actions", "dateLastCrawled": "2021-12-02T05:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Toward Fair Recommendation in Two-sided Platforms | ACM Transactions on ...", "url": "https://dl.acm.org/doi/10.1145/3503624", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3503624", "snippet": "Moreover, this additional feasibility <b>constraint</b> makes it difficult to decide how many copies of which product should be made available for a total of allocations, satisfying the feasibility constraints as well as the <b>fairness</b> requirement. Thus, unlike the fair allocation problem, we consider no restriction on the number of copies of each product that are made available. All these contrast points, along with the two-sided <b>fairness</b> guarantees make fair recommendation an interesting extension ...", "dateLastCrawled": "2022-01-16T19:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness</b> Through Awareness", "url": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "snippet": "classi\ufb01cation task at hand; (2) an algorithm for maximizing utility subject to the <b>fairness</b> <b>constraint</b>, that <b>similar</b> individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of \u201cfair a rmative action,\u201d which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classi\ufb01cation are the same as the demographics of the underlying population), while treating <b>similar</b> individuals as similarly as ...", "dateLastCrawled": "2022-01-29T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness</b> through awareness", "url": "https://www.cs.toronto.edu/~toni/Papers/awareness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~toni/Papers/awareness.pdf", "snippet": "ing utility subject to the <b>fairness</b> <b>constraint</b>, that <b>similar</b> individuals are treated similarly. We also present an adapta-tion of our approach to achieve the complementary goal of \\fair a rmative action,&quot;which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classi cation are the same as the demographics of the underlying population), while treating <b>similar</b> individuals as similarly as possible. Finally, we discuss the relationship of <b>fairness</b> to ...", "dateLastCrawled": "2021-12-06T13:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "On Consequentialism and <b>Fairness</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861221/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861221", "snippet": "As the name suggests, rule consequentialism <b>is similar</b> to act consequentialism, ... and this is not meant to suggest that any particular <b>fairness</b> <b>constraint</b> is likely to lead to disaster. Nevertheless, it is important to remember that <b>fairness</b> criteria which are specified only in terms of a narrow set of short term metrics do not guarantee positive outcomes beyond what they measure, and may in some cases lead to overall greater harm. In sum, adopting a consequentialist perspective reveals ...", "dateLastCrawled": "2022-01-28T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Learning Certi\ufb01ed Individually Fair Representations", "url": "https://proceedings.neurips.cc/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf", "snippet": "The key idea is <b>to map</b> <b>similar</b> individuals to close latent representations and leverage this latent proximity to certify individual <b>fairness</b>. That is, our method enables the data producer to learn and certify a representation where for a data point all <b>similar</b> individuals are at \u2018 1-distance at most , thus allowing data consumers to certify individual <b>fairness</b> by proving -robustness of their classi\ufb01er. Our experimental evaluation on \ufb01ve real-world datasets and several <b>fairness</b> ...", "dateLastCrawled": "2022-01-22T14:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Reading papers on <b>fairness</b> of recommendation system (4) | Develop Paper", "url": "https://developpaper.com/reading-papers-on-fairness-of-recommendation-system-4/", "isFamilyFriendly": true, "displayUrl": "https://developpaper.com/reading-papers-on-<b>fairness</b>-of-recommendation-system-4", "snippet": "Thirdly, the <b>fairness</b> <b>constraint</b> is linear, and the analytical solution can be obtained for the parameter optimization of the model, which can greatly reduce the time complexity of the algorithm. Background knowledge reserve. First of all, I read the classic book reinforcement learning: an introduction [2] Have a preliminary understanding of many basic concepts in reinforcement learning. The key idea of reinforcement learning is Markov decision processes (MDPs). An MDP can use a tuple \\(M ...", "dateLastCrawled": "2022-01-14T14:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Bias and <b>Fairness</b> in Machine Learning, Part 3: building a bias-aware ...", "url": "https://freecontent.manning.com/bias-and-fairness-in-machine-learning-part-3-building-a-bias-aware-model/", "isFamilyFriendly": true, "displayUrl": "https://freecontent.manning.com/bias-and-<b>fairness</b>-in-machine-learning-part-3-building...", "snippet": "Quantifying <b>fairness</b> through various metrics Applying feature engineering techniques to remove bias from our model without sacrificing model performance . Take 35% off Feature Engineering Bookcamp by entering fccozdemir into the discount code box at checkout at manning.com. Building a Bias-aware Model. For information on the dataset, the mechanics and importance of model bias and <b>fairness</b>, and building the basic model, check out part 1 and part 2. Let\u2019s begin to construct a more bias-aware ...", "dateLastCrawled": "2022-02-02T03:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fairness</b> for Users - NEEDS | Mind <b>Map</b> - EdrawMind", "url": "https://www.edrawmind.com/mind-maps/13710/fairness-for-users-needs", "isFamilyFriendly": true, "displayUrl": "https://www.edrawmind.com/mind-<b>map</b>s/13710/<b>fairness</b>-for-users-needs", "snippet": "A mind <b>map</b> about <b>fairness</b> for users - needs. You can edit this mind <b>map</b> or create your own using our free cloud based mind <b>map</b> maker. Wondershare EdrawMind Product Tour; Resources; Gallery; Pricing; Log in; Sign up; Return to Mind <b>Map</b> Gallery <b>Fairness</b> for Users - NEEDS Duplicate. <b>Fairness</b> for Users - NEEDS. Accessibility. Bikes available. Safety/Security . Social <b>Constraint</b>. Weather &amp; Topography. Design. Lighter Bikes. Unisex bike design. Detachable child seats. Side mirrors. Wider bike ...", "dateLastCrawled": "2022-01-12T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning Fairness-aware Relational Structures</b> | DeepAI", "url": "https://deepai.org/publication/learning-fairness-aware-relational-structures", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-fairness-aware-relational-structures</b>", "snippet": "Farnadi et al.\u2019s work on developing <b>fairness</b> metrics for relational domains and <b>fairness</b>-aware <b>MAP</b> inference for hinge-loss Markov random fields (HL-MRFs) is the first work in this direction. Farnadi et al. [ 10 ] note that in many social contexts, discrimination is the result of complex interactions and cannot be described solely in terms of attributes of an individual.", "dateLastCrawled": "2021-12-18T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "While this is in the sphere of AI <b>fairness</b>, there are many <b>similar</b> parallels across both domains. This post is written as part of our participation in the Monetary Authority of Singapore (MAS) Veritas challenge on applying <b>fairness</b>, ethics, accountability and <b>fairness</b> (FEAT) in artificial intelligence and data analytics (AIDA) use cases. 1. As one of the shortlisted finalist, I thought it would be good to blog about the approach we are taking and why we believe it to be the right one. In the ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Toward Fair Recommendation in Two-sided Platforms | ACM Transactions on ...", "url": "https://dl.acm.org/doi/10.1145/3503624", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/10.1145/3503624", "snippet": "A general version of the category-wise upper bound <b>constraint</b>, namely, laminar matroid <b>constraint</b>, is studied by Biswas et al. and the existence of is proved for identical valuations. A different problem is considered by Gourv\u00e8s et al. [ 34 , 35 ] where the goal is to find fair allocation that union of all the allocated goods is an independent set of a given matroid.", "dateLastCrawled": "2022-01-16T19:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On Consequentialism and <b>Fairness</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861221/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7861221", "snippet": "While research in machine learning <b>fairness</b> is ongoing, most proposals <b>can</b> be classified into two types, which to some extent <b>map</b> onto the two legal doctrines mentioned above. Some definitions are specified without reference to outcomes (section 5.1). Others are specified exclusively with regard to a particular set of outcomes (which must be evaluated using real data; section 5.2). We summarize the dominant proposals of each type below.", "dateLastCrawled": "2022-01-28T22:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Fair and diverse allocation of scarce resources", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8597936/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8597936", "snippet": "For example, the <b>fairness</b> requirement <b>can</b> <b>be thought</b> of as the US Equal Employment Opportunity Commission&#39;s \u201cfour-fifths rule,\u201d which requires that \u201cthe selection rate for any race, sex, or ethnic group [must be at least] four-fifths (4/5) (or eighty percent) of the rate for the group with the highest rate\u201d. 1 We consider a similar requirement for diversity as well.", "dateLastCrawled": "2022-01-28T13:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fairness</b> Through Awareness \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1104.3913/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1104.3913", "snippet": "We <b>can</b> think of these categories as outcomes, as they determine which ads will be shown to an individual. In order to comply with our <b>fairness</b> requirement, the mapping from individuals into categories (or outcomes) will have to be randomized and satisfy the Lipschitz property introduced above. Subject to the Lipschitz <b>constraint</b>, the vendor <b>can</b> ...", "dateLastCrawled": "2022-01-01T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "In the spirit of openness, I decided to document my <b>thought</b> process in designing the described framework. This is the first post in a series of three. For the first post, I will focus on the issue of <b>fairness</b> and argue that the right approach that should be adopted by the industry is a human-centric one. Attempts to distil <b>fairness</b> down to a single metric are unhelpful and counter-productive. As business owners and model developers we should embrace the struggle in trying to define <b>fairness</b> ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fairness</b>, Individuality, and Free Riding | The Philosophical Quarterly ...", "url": "https://academic.oup.com/pq/advance-article/doi/10.1093/pq/pqab075/6500929", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/pq/advance-article/doi/10.1093/pq/pqab075/6500929", "snippet": "<b>Fairness</b>, on such views, is not simply a consideration that is relevant to how good outcomes are, or a practice that <b>can</b> produce or sustain other goods, it is a <b>constraint</b> that prevents our achieving good outcomes in immoral ways. So if consequentialism is not the right moral theory, Mill&#39;s assessment of the consequences will not be decisive, even if it is accurate.", "dateLastCrawled": "2022-02-03T02:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "promela - SPIN: Visit statements infinitely often - Stack Overflow", "url": "https://stackoverflow.com/questions/19157373/spin-visit-statements-infinitely-often", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/19157373", "snippet": "I&#39;m wondering whether it&#39;s possible to verify an LTL property in a program with a <b>fairness</b> <b>constraint</b> that states that multiple statements must be executed infinitely often. E.g.: bool no_flip; bool flag; active[1] proctype node() { do :: skip -&gt; progress00: no_flip = 1 :: skip -&gt; progress01: flag = 1 :: !no_flip -&gt; flag = 0 od; } // eventually we have flag==1 forever ltl p0 { &lt;&gt; ([] (flag == 1)) } This program is correct iff eventually the no_flip flag becomes true and flag becomes true ...", "dateLastCrawled": "2022-01-14T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "This article seeks to provide an overview of the different schools of <b>thought</b> and approaches to mitigating (social) biases and increase <b>fairness</b> in the Machine Learning literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of <b>fairness</b> in regression, recommender systems, unsupervised ...", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "You <b>Can</b> <b>Have Your Cake and Redistrict It Too</b>", "url": "http://www.gerdusbenade.com/files/21_gt.pdf", "isFamilyFriendly": true, "displayUrl": "www.gerdusbenade.com/files/21_gt.pdf", "snippet": "As the <b>fairness</b> <b>constraint</b> we adopt the geometric target, which requires the number of seats won by each party to be at least the average (rounded down) of its outcomes under the worst and best partitions of the state; but we extend this notion to allow the two parties to compute their targets with respect to different election datasets. Our theoretical contribution is twofold: we introduce a new model of redistricting that closely mirrors the classic model of cake-cutting and we prove the ...", "dateLastCrawled": "2021-09-19T06:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Real-Time Shape Editing using Radial Basis Functions", "url": "https://www.graphics.rwth-aachen.de/media/papers/rbf-modeling1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.graphics.rwth-aachen.de/media/papers/rbf-modeling1.pdf", "snippet": "<b>constraint</b> modeling (BCM). Most existing BCM approaches are surface-based, i.e., they <b>can</b> <b>be thought</b> of as computing a fair deformation \ufb01eld on the surface S. If the underlying surface representation is a triangle mesh, computing the deformation \ufb01eld usually re-quires to solve a linear Laplacian system on S. An apparent", "dateLastCrawled": "2022-01-31T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Exceptions To The Principles of Natural Justice</b>", "url": "http://www.legalservicesindia.com/article/1529/Exceptions-To-The-Principles-of-Natural-Justice.html", "isFamilyFriendly": true, "displayUrl": "www.legalservicesindia.com/article/1529/<b>Exceptions-To-The-Principles-of</b>-Natural...", "snippet": "<b>Principles of Natural Justice</b> are ultimately weighed in the balance of <b>fairness</b> and hence the Courts have been circumspect in extending <b>principles of natural justice</b> to situations where it would cause more injustice rather than justice so, where a right to be fairly heard has been denied, it is more probably a case of bad decision than of true exception, then <b>principles of natural justice</b> <b>can</b> be discarded. Application of <b>the principles of natural justice</b> <b>can</b> be excluded either expressly or ...", "dateLastCrawled": "2022-02-02T04:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Fair and diverse allocation of scarce resources", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8597936/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8597936", "snippet": "<b>Map</b> of race and ethnicity by neighborhood in Chicago. Next, ... As we described in \u00a7 2, Equation is the <b>fairness</b> <b>constraint</b> that is defined for each social group. Solving P2 with and without the <b>fairness</b> constraints, we <b>can</b> obtain different allocation solutions and consequently different <b>fairness</b> and diversity gaps. This will allow us to compare fair-diverse allocation performance with the Diverse-only allocation to analyze the impact of <b>fairness</b> constraints, namely PoF. The PoF metric ...", "dateLastCrawled": "2022-01-28T13:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Learning Certi\ufb01ed Individually Fair Representations", "url": "https://proceedings.neurips.cc/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf", "snippet": "Given the <b>fairness</b> <b>constraint</b> \u02da, we <b>can</b> now train an individually fair representation and use it to obtain a certi\ufb01cate of individual <b>fairness</b> for the end-to-end model. For training, the data producer <b>can</b> employ our framework to learn an encoder f with the goal that two individuals satisfying \u02dashould be mapped close together in \u2018 1-distance in latent space. As a consequence, individual <b>fairness</b> <b>can</b> then be certi\ufb01ed for a data point in two steps: \ufb01rst, the data producer computes a ...", "dateLastCrawled": "2022-01-22T14:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fairness</b> <b>in Examination Timetabling: Student Preferences and</b> Extended ...", "url": "https://www.eventmapsolutions.com/wp-content/uploads/2017/01/fairness-examination-timetabling.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.event<b>map</b>solutions.com/wp-content/uploads/2017/01/<b>fairness</b>-examination...", "snippet": "considers <b>fairness</b> with respect to the examination timetables of their immedi-ate peers, as highly important. Considerations such as providing an equitable distribution of preparation time between all student cohort examinations, not just a majority, are used to form a measure of <b>fairness</b>. In order to satisfy this requirement, we propose an extension to the state-of-the-art examination timetabling problem models widely used in the scienti c literature. <b>Fairness</b> is introduced as a new ...", "dateLastCrawled": "2021-11-19T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Fair Feature Distillation for Visual Recognition", "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Jung_Fair_Feature_Distillation_for_Visual_Recognition_CVPR_2021_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content/CVPR2021/papers/Jung_Fair_Feature_Distillation...", "snippet": "high accuracy and <b>fairness</b> <b>can</b> be achieved. already deployed model has been identi\ufb01ed as unfair. The usual approach of the so-called in-processing methods to mitigate the unfair bias is to re-train the model from scratch with an additional <b>fairness</b> <b>constraint</b> [1, 18, 37]. However, such approaches typically do not utilize any predictive in-formation already learned out by the deployed model, and hence, would lead to sacri\ufb01cing the accuracy for the im-provedfairness ...", "dateLastCrawled": "2022-01-30T16:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Controllable <b>Fairness</b> in Machine Learning | SAIL Blog", "url": "http://ai.stanford.edu/blog/controllable-fairness/", "isFamilyFriendly": true, "displayUrl": "ai.stanford.edu/blog/controllable-<b>fairness</b>", "snippet": "Based on our theoretical approach, we introduce a new method where the concerned party <b>can</b> control the <b>fairness</b> of representations by requesting specific limits on unfairness. <b>Compared</b> to earlier fair representations, ours <b>can</b> be learned more quickly, are able to satisfy requests for many notions of <b>fairness</b> simultaneously, and contain more useful information. A theoretical approach to fair representations. We assume we are given a set of data points (\\(x\\)), typically representing people ...", "dateLastCrawled": "2022-01-30T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "When comparing current work in ml with initial work in <b>fairness</b>, it is noteworthy that much of the early literature considers regression settings as well a correlation-based definition of <b>fairness</b> properties of an underlying mechanism due to the focus on test <b>fairness</b> with a continuous target variable (see e.g., [darlington1971]).However, the general notions transfer to (binary) classification settings and thus define essential concepts such as protected/demographic variables (e.g ...", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "A business problem <b>can</b> then be translated to a mathematical / statistical / computational problem and different methods <b>can</b> <b>be compared</b> against each other based on how far the predicted output differs from the actual output as measured by the objective function. If <b>fairness</b> could be distilled down to a single metric, a data scientist <b>can</b> include it as part of the objective function or as a <b>constraint</b> and find an ideal point that maximises overall business needs while satisfying <b>fairness</b> ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "HaSTE: Hadoop YARN Scheduling Based on Task-Dependency and Resource-Demand", "url": "https://www.cs.umb.edu/~shengbo/paper/cloud14a.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.umb.edu/~shengbo/paper/cloud14a.pdf", "snippet": "jobs. However, the precedence <b>constraint</b> or <b>fairness</b> <b>constraint</b> in current widely used scheduling policies in YARN, such as FIFO and Fair, <b>can</b> both lead to inef\ufb01cient resource allocation in the Hadoop YARN cluster. They also omit the dependency between tasks which is crucial for the ef\ufb01ciency of resource utilization. We thus propose a new YARN scheduler, named HaSTE, which <b>can</b> effectively reduce the makespan of MapReduce jobs in YARN by leveraging the information of requested resources ...", "dateLastCrawled": "2022-01-18T12:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Algorithmic <b>Fairness</b>: Tackling Bias in City Algorithms | Data-Smart ...", "url": "https://datasmart.ash.harvard.edu/news/article/algorithmic-fairness-tackling-bias-city-algorithms", "isFamilyFriendly": true, "displayUrl": "https://datasmart.ash.harvard.edu/news/article/algorithmic-<b>fairness</b>-tackling-bias-city...", "snippet": "The foundation for ensuring <b>fairness</b> in the algorithms created by an organization is a set of structural conditions that promote a culture committed to reducing bias. Without this, the policy and technical strategies that follow are irrelevant, because they will never become a priority for analysts. Create diverse teams. Building a culture of <b>fairness</b> starts with hiring a diverse group of engineers. Black, Latinx, and Native American people are underrepresented in tech by 16 to 18 percentage ...", "dateLastCrawled": "2022-02-03T10:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Solved The figure below shows Angela and Bruno\u2019s feasible | <b>Chegg</b>.com", "url": "https://www.chegg.com/homework-help/questions-and-answers/figure-shows-angela-bruno-s-feasible-frontier-angela-s-biological-survival-constraint-rese-q45546110", "isFamilyFriendly": true, "displayUrl": "https://www.<b>chegg</b>.com/homework-help/questions-and-answers/figure-shows-angela-bruno-s...", "snippet": "The figure below shows Angela and Bruno\u2019s feasible frontier, Angela\u2019s biological survival <b>constraint</b>, and her reservation indifference curve, when Angela would accept any offers strictly better than her reservation position of no work and 2.5 bushels of survival rations from the government.", "dateLastCrawled": "2022-01-01T19:03:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Classification - <b>Fairness</b> and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Dwork et al. Dwork et al., \u201c<b>Fairness</b> Through Awareness.\u201d argued that the independence criterion was inadequate as a <b>fairness</b> <b>constraint</b>. The separation criterion appeared under the name equalized odds , Hardt, Price, and Srebro, \u201cEquality of Opportunity in Supervised <b>Learning</b>.\u201d alongside the relaxation to equal false negative rates, called equality of opportunity.", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Survey on Bias and <b>Fairness</b> in <b>Machine</b> <b>Learning</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1908.09635/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1908.09635", "snippet": "With the widespread use of AI systems and applications in our everyday lives, it is important to take <b>fairness</b> issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in <b>machine</b> <b>learning</b>, natural language ...", "dateLastCrawled": "2021-11-15T17:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "A lot of what is discussed in the <b>machine</b> <b>learning</b> literature touches on <b>fairness</b> (or rather equivalence in certain outcomes) between groups, yet this narrowly constricts <b>fairness</b> to the notion of equality. Of course, we should think about <b>fairness</b> in the context of prejudiced groups, but we should also ask whether it is fair to an individual. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "dimensions of <b>machine</b> Causality and the normative", "url": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "isFamilyFriendly": true, "displayUrl": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "snippet": "dimensions of <b>machine</b> <b>learning</b> Joshua Loftus (LSE Statistics) High level intro Causality, what is it good for? Causal <b>fairness</b> In prediction and ranking tasks, and with intersectionality Designing interventions Optimal fair policies, causal interference Concluding thoughts 2 / 27. Tech solutionism, using ML/AI in every situation 3 / 27. Imagination Albert Einstein: Imagination is more important than knowledge. For knowledge is limited, whereas imagination [...] stimulat[es] progress, giving ...", "dateLastCrawled": "2022-01-11T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Matching Code and Law: Achieving Algorithmic <b>Fairness</b> with ...", "url": "https://www.academia.edu/40615359/Matching_Code_and_Law_Achieving_Algorithmic_Fairness_with_Optimal_Transport", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40615359/Matching_Code_and_Law_Achieving_Algorithmic_<b>Fairness</b>...", "snippet": "Increasingly, discrimination by algorithms is perceived as a societal and legal problem. As a response, a number of criteria for implementing algorithmic <b>fairness</b> in <b>machine</b> <b>learning</b> have been developed in the literature. This paper proposes the", "dateLastCrawled": "2021-12-22T00:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Using <b>Constraint</b> Programming to Improve Grocery Picking Efficiency at ...", "url": "https://www.torontomachinelearning.com/events/using-constraint-programming-to-improve-grocery-picking-efficiency-at-loblaws/", "isFamilyFriendly": true, "displayUrl": "https://www.toronto<b>machinelearning</b>.com/events/using-<b>constraint</b>-programming-to-improve...", "snippet": "Jaya Kawale is the Director of <b>Machine</b> <b>Learning</b> at Tubi leading all of the <b>machine</b> <b>learning</b> efforts at Tubi encompassing homepage recommendations, content understanding and ads. Prior to Tubi, she has worked on different aspects of recommender systems at Netflix and Adobe research labs. She did her PhD from the University of Minnesota, Twin cities and her thesis won several awards including the Explorations in Science using computation award. She has published many top tier conference and ...", "dateLastCrawled": "2021-12-29T16:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Fairness</b> through awareness", "url": "https://www.cs.toronto.edu/~toni/Papers/awareness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~toni/Papers/awareness.pdf", "snippet": "ing utility subject to the <b>fairness</b> <b>constraint</b>, that similar individuals are treated similarly. We also present an adapta-tion of our approach to achieve the complementary goal of \\fair a rmative action,&quot;which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classi cation are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of <b>fairness</b> to ...", "dateLastCrawled": "2021-12-06T13:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fairness</b> Through Awareness", "url": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~zemel/documents/fairAwareItcs2012.pdf", "snippet": "<b>Fairness</b> Through Awareness Cynthia Dwork Moritz Hardty Toniann Pitassiz Omer Reingoldx Richard Zemel{November 29, 2011 Abstract We study <b>fairness</b> in classi\ufb01cation, where individuals are classi\ufb01ed, e.g., admitted to a uni- versity, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classi\ufb01er (the university). The main conceptual contribution of this paper is a framework for fair classi\ufb01cation ...", "dateLastCrawled": "2022-01-29T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning</b> 2020 summary: 84 interesting papers/articles | by ...", "url": "https://towardsdatascience.com/machine-learning-2020-summary-84-interesting-papers-articles-45bd45c0d35b", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-2020-summary-84-interesting-papers...", "snippet": "8.<b>Machine learning</b> with natural sciences \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 A lot of research combining natural science and <b>machine learning</b> has been published. There is a lot of research on speeding up and improving the accuracy of numerical simulations using <b>machine learning</b> ...", "dateLastCrawled": "2022-01-29T20:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "On word analogies and negative results in NLP - Hacking semantics", "url": "https://hackingsemantics.xyz/2019/analogies/", "isFamilyFriendly": true, "displayUrl": "https://hackingsemantics.xyz/2019/analogies", "snippet": "Share of BATS <b>analogy</b> questions in which the vector the closest to the predicted vector is one of the source vectors (a,a&#39;, b), the target vector b&#39;, or some other vector. In most cases the result is simply the vector b (&quot;woman&quot;). If in most cases the predicted vector is the closest to the source vector, it means that the vector offset is simply too small to induce a meaning shift on its own. And that means that adding it will not get you somewhere significantly different. Which means you ...", "dateLastCrawled": "2021-06-02T05:25:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(fairness constraint)  is like +(map)", "+(fairness constraint) is similar to +(map)", "+(fairness constraint) can be thought of as +(map)", "+(fairness constraint) can be compared to +(map)", "machine learning +(fairness constraint AND analogy)", "machine learning +(\"fairness constraint is like\")", "machine learning +(\"fairness constraint is similar\")", "machine learning +(\"just as fairness constraint\")", "machine learning +(\"fairness constraint can be thought of as\")", "machine learning +(\"fairness constraint can be compared to\")"]}