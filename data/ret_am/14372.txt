{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> algorithms based on &quot;<b>structural</b> <b>risk</b> <b>minimization</b>&quot;?", "url": "https://cs.stackexchange.com/questions/2006/machine-learning-algorithms-based-on-structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://cs.stackexchange.com/questions/2006/<b>machine</b>-<b>learning</b>-<b>algorithms</b>-based-on...", "snippet": "The <b>structural</b> <b>risk</b> <b>minimization</b> principle is a principle that is at least partly &#39;used&#39; in all <b>machine</b> <b>learning</b> methods, since overfitting is often to be taken into account: reducing the complexity of the model is (supposedly and in practice) a good way to limit overfitting.", "dateLastCrawled": "2022-01-19T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Structural</b> <b>risk</b> <b>minimization</b> and minimum description length", "url": "https://andrewcharlesjones.github.io/journal/mdl.html", "isFamilyFriendly": true, "displayUrl": "https://andrewcharlesjones.github.io/journal/mdl.html", "snippet": "We have seen that <b>structural</b> <b>risk</b> <b>minimization</b> is a general framework for searching among a large set of hypotheses by incorporating knowledge about which types of hypotheses we prefer. Minimum description length is one possible strategy for formally assigning preferences, which as we saw is suprisingly agnostic to the langauge in which we represent the hypotheses. These ideas are deeply connected to several other well-known concepts in statistics and <b>machine</b> <b>learning</b>, some of which are ...", "dateLastCrawled": "2022-01-02T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b>", "url": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "snippet": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b> Akshay Krishnamurthy akshay@cs.umass.edu September 28, 2017 1 Recap Last time we saw our rst real <b>learning</b> <b>algorithm</b> (that wasn\u2019t obviously ERM), namely kernel regression. Recall the estimator took the form ^(x) = P n i=1 Y iK(kX i xk=h) P n i=1 K(kX i xk=h) where the data (X i;Y", "dateLastCrawled": "2021-12-31T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>machine</b> <b>learning</b> - <b>Structural</b> <b>risk</b> <b>minimization</b> in SVMs - Cross Validated", "url": "https://stats.stackexchange.com/questions/278306/structural-risk-minimization-in-svms", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/278306/<b>structural</b>-<b>risk</b>-<b>minimization</b>-in-svms", "snippet": "In <b>SRM</b> (<b>structural</b> <b>risk</b> <b>minimization</b>) as larger is the VC dimension as larger is the <b>risk</b>. I know that SVM <b>algorithm</b> selects the hyperplane with minimum VC dimension (namely with minimum margin) however if the VC-dimension is infinite (<b>like</b> in gaussian kernels) this minimum is infinite and the <b>risk</b> will be likely high. How is it possible?", "dateLastCrawled": "2022-01-23T15:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "11.1 Empirical <b>Risk</b> <b>Minimization</b> - Carnegie Mellon University", "url": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lec11.pdf", "snippet": "11.2 Complexity Regularized Empirical <b>Risk</b> <b>Minimization</b> aka <b>Structural</b> <b>Risk</b> <b>Minimization</b> To achieve better estimation of the true <b>risk</b>, we should minimize both the empirical <b>risk</b> and complexity, instead of only minimizing the empirical <b>risk</b>. f^<b>SRM</b> = argmin f2F fR^(f) + (f)g (11.14) where (f) = q c(f)+log 2 2n. With probability 1 , we have the ...", "dateLastCrawled": "2022-01-29T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Model Selection in <b>Machine</b> <b>Learning</b> | by ANUSHKA BAJPAI | Medium", "url": "https://medium.com/@anushkhabajpai/model-selection-in-machine-learning-c568e5a42dcc", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@anushkhabajpai/model-selection-in-<b>machine</b>-<b>learning</b>-c568e5a42dcc", "snippet": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) <b>Machine</b> <b>learning</b> models face the inevitable problem of defining a generalized theory from a set of finite data. This leads to cases of overfitting where the ...", "dateLastCrawled": "2022-01-29T03:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> for Character Recognition", "url": "https://papers.nips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "snippet": "The method of <b>Structural</b> <b>Risk</b> <b>Minimization</b> refers to tuning the capacity of the classifier to the available amount of training data. This capac\u00ad ity is influenced by several factors, including: (1) properties of the input space, (2) nature and structure of the classifier, and (3) <b>learning</b> <b>algorithm</b>.", "dateLastCrawled": "2021-11-21T16:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Understanding Machine Learning</b>: From Theory to Algorithms", "url": "https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.huji.ac.il/.../<b>understanding-machine-learning</b>-theory-<b>algorithms</b>.pdf", "snippet": "the Empirical <b>Risk</b> <b>Minimization</b> (ERM), <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>), and Minimum Description Length (MDL) <b>learning</b> rules, which shows \\how can a <b>machine</b> learn&quot;. We quantify the amount of data needed for <b>learning</b> using the ERM, <b>SRM</b>, and MDL rules and show how <b>learning</b> might fail by deriving", "dateLastCrawled": "2022-01-29T21:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>Gentle Introduction to Model Selection for Machine Learning</b>", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-model-selection-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-model-selection-for</b>...", "snippet": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>). Probabilistic measures are appropriate when using simpler linear models <b>like</b> linear regression or logistic regression where the calculating of model complexity penalty (e.g. in sample bias) is known and tractable. Resampling Methods", "dateLastCrawled": "2022-02-02T06:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - Is there an equivalent to VC-dimension for density ...", "url": "https://cstheory.stackexchange.com/questions/47744/is-there-an-equivalent-to-vc-dimension-for-density-estimation-as-opposed-to-clas", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/47744/is-there-an-equivalent-to-vc...", "snippet": "I did find this article which seems <b>like</b> it might come pretty close to answering my question, since they apply <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) to gaussian mixture models, and it looks <b>like</b> they estimate the capacity using &quot;annealed entropy&quot; applied to a class of threshold-based indicator functions associated with the log-likelihood function. However, they don&#39;t provide references to theorems/bounds to show whether this is a principled way to assess the capacity of a probability density ...", "dateLastCrawled": "2022-01-12T03:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b>", "url": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "snippet": "<b>Structural</b> <b>risk minimization</b> is a way to basically do this for free, and get the best of both worlds. The rst observation is that if we weight the classes appropriately by taking a union bound, we can get a convergence result that is fairly <b>similar</b> to the uniform convergence results we have used in the past. This is a form of non-uniform ...", "dateLastCrawled": "2021-12-31T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Statistical <b>Learning</b> Theory: The <b>Structural</b> <b>Risk</b> <b>Minimization</b> Principle", "url": "http://www.cnel.ufl.edu/courses/EEL6814/srm.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/<b>srm</b>.pdf", "snippet": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) Principle Vapnik posed four questions that need to be addressed in the design of <b>learning</b> machines (LMs): 1. What are the necessary and sufficient conditions for consistency of a <b>learning</b> process. 2. How fast is the rate of convergence to the solution. 3. How can we control the generalization ability of the LM. 4. How can we construct an <b>algorithm</b> that implement these pre requisites. <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) principle Vapnik argues that the ...", "dateLastCrawled": "2022-01-25T07:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) Task - GM-RKB", "url": "https://www.gabormelli.com/RKB/Structural_Risk_Minimization_(SRM)_Task", "isFamilyFriendly": true, "displayUrl": "https://www.gabormelli.com/RKB/<b>Structural</b>_<b>Risk</b>_<b>Minimization</b>_(<b>SRM</b>)_Task", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) is an inductive principle of use in <b>machine</b> <b>learning</b>. Commonly in <b>machine</b> <b>learning</b>, a generalized model must be selected from a finite data set, with the consequent problem of overfitting \u2013 the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The <b>SRM</b> principle addresses this problem by balancing the model&#39;s complexity against its success at fitting the training data. In practical terms ...", "dateLastCrawled": "2021-12-09T18:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Structural</b> <b>risk</b> <b>minimization</b> and minimum description length", "url": "https://andrewcharlesjones.github.io/journal/mdl.html", "isFamilyFriendly": true, "displayUrl": "https://andrewcharlesjones.github.io/journal/mdl.html", "snippet": "We have seen that <b>structural</b> <b>risk</b> <b>minimization</b> is a general framework for searching among a large set of hypotheses by incorporating knowledge about which types of hypotheses we prefer. Minimum description length is one possible strategy for formally assigning preferences, which as we saw is suprisingly agnostic to the langauge in which we represent the hypotheses. These ideas are deeply connected to several other well-known concepts in statistics and <b>machine</b> <b>learning</b>, some of which are ...", "dateLastCrawled": "2022-01-02T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Structural risk minimization and SVMs</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/27036690/structural-risk-minimization-and-svms", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/27036690/<b>structural-risk-minimization-and-svms</b>", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> is the Vapnik&#39;s concept of <b>learning</b> which is conceptually <b>similar</b> to the other &quot;minimum assumption approaches&quot;. In short words, most of the <b>learning</b> algorithms (in classification) try to find some mapping from input space to classes in such a way that this mapping behaves &quot;well&quot; on the training set (returns correct answers). It is however a known phenomena, that this is not enough to build a good model, as one could for example memorize all training examples and ...", "dateLastCrawled": "2022-01-03T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A. <b>Structural</b> <b>Risk</b> <b>Minimization</b>", "url": "https://cs.nyu.edu/~mohri/aml21/sol2.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.nyu.edu/~mohri/aml21/sol2.pdf", "snippet": "Advanced <b>Machine</b> <b>Learning</b> 2021 Courant Institute of Mathematical Sciences Homework assignment 2 April 20, 2021 Due: May 04, 2021 A. <b>Structural</b> <b>Risk</b> <b>Minimization</b> As discussed in class, the <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) technique is based on a hypothesis set H de ned as a countable union of hypothesis sets H n with nite VC-dimension or favorable Rademacher complexity. In this problem, we study several questions related to such countable union hypothesis sets. 1.Let H = S +1 n=1 fh ngbe a ...", "dateLastCrawled": "2021-12-28T06:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Model Selection in <b>Machine</b> <b>Learning</b> | by ANUSHKA BAJPAI | Medium", "url": "https://medium.com/@anushkhabajpai/model-selection-in-machine-learning-c568e5a42dcc", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@anushkhabajpai/model-selection-in-<b>machine</b>-<b>learning</b>-c568e5a42dcc", "snippet": "<b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) <b>Machine</b> <b>learning</b> models face the inevitable problem of defining a generalized theory from a set of finite data. This leads to cases of overfitting where the ...", "dateLastCrawled": "2022-01-29T03:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "An Efficient <b>Machine</b> <b>Learning</b> Regression Model for Rainfall Prediction", "url": "https://research.ijcaonline.org/volume115/number23/pxc3902681.pdf", "isFamilyFriendly": true, "displayUrl": "https://research.ijcaonline.org/volume115/number23/pxc3902681.pdf", "snippet": "SVM is fundamentally a linear <b>machine</b>, which can be seen as a statistical tool that procedures the problem identical to Artificial Neural Networks (ANN). <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) is used to relay wellbeing on unseen data.SVM is a <b>similar</b> implementation principle of <b>Structural</b> <b>Risk</b> <b>minimization</b>. While on one hand it has all the strengths", "dateLastCrawled": "2021-12-08T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_<b>Machine</b>s", "snippet": "hypothesis spaces is known as <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) (Vapnik, 1998). An important question that arises in SLT is that of meas uring the &quot;complexity&quot; of a hypothesis space - which, as ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Support Vector Machines for Classification</b>", "url": "https://www.researchgate.net/publication/300723807_Support_Vector_Machines_for_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/300723807_Support_Vector_<b>Machines</b>_for...", "snippet": "SVM uses <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) and satisfies the duality and convexity requirements. <b>SRM</b> <b>SRM</b> (Vapnik 1964) is an inductive principle that selects a model for <b>learning</b> from a finite ...", "dateLastCrawled": "2022-02-03T04:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Structural Risk Minimization</b> - AI Alignment Forum", "url": "https://www.alignmentforum.org/posts/5bd75cc58225bf0670374f81/structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://www.alignmentforum.org/posts/5bd75cc58225bf0670374f81/<b>structural-risk-minimization</b>", "snippet": "<b>Structural risk minimization</b>, a concept from computational <b>learning</b> theory, is proposed as a satisficing framework. ----- The goal of this post is similar to Creating a Satisficer and Minimax as an approach to reduced-impact AI: constructing agents which are safe even with a utility function which isn&#39;t quite what is actually wanted. The approach is similar to Paul Christiano&#39;s Model-Free Decisions. The philosophy behind this solution is to treat it as an example of the optimizer&#39;s curse ...", "dateLastCrawled": "2022-01-15T11:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "svm - difference between <b>empirical</b> <b>risk</b> <b>minimization</b> and <b>structural</b> ...", "url": "https://datascience.stackexchange.com/questions/66729/difference-between-empirical-risk-minimization-and-structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/66729", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) is an inductive principle of use in <b>machine</b> <b>learning</b>. Commonly in <b>machine</b> <b>learning</b>, a generalized model must be selected from a finite data set, with the consequent problem of overfitting \u2013 the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The <b>SRM</b> principle addresses this problem by balancing the model&#39;s complexity against its success at fitting the training data.", "dateLastCrawled": "2022-01-24T04:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>Stochastic Gradient Descent Algorithm for Structural Risk Minimisation</b>", "url": "https://www.researchgate.net/publication/225132871_A_Stochastic_Gradient_Descent_Algorithm_for_Structural_Risk_Minimisation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/225132871_A_Stochastic_Gradient_Descent...", "snippet": "The paper introduces a framework for studying <b>structural</b> <b>risk</b> minimisation. The model views <b>structural</b> <b>risk</b> minimisation in a PAC context. It then considers the more general case when the hi ...", "dateLastCrawled": "2021-09-30T08:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "VC-<b>dimension and structural risk minimization for</b> the analysis of ...", "url": "https://www.sciencedirect.com/science/article/pii/S0096300305007824", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0096300305007824", "snippet": "Meir has outlined an approach that extends Vapnik\u2019s method of <b>Structural</b> <b>Risk</b> <b>Minimization</b> to time series generated by an underlying mixing stochastic process. However, this approach requires the knowledge of the mixing rate of the process, which is not at all easy to estimate. Therefore, we will straightforwardly use the standard <b>SRM</b> for nonlinear regressors and our heuristic approach will be justified only a posteriori by the results for the problem of model choice. If density ...", "dateLastCrawled": "2021-10-24T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Structural Return Maximization for Reinforcement Learning</b> | DeepAI", "url": "https://deepai.org/publication/structural-return-maximization-for-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>structural-return-maximization-for-reinforcement-learning</b>", "snippet": "We overcome this problem by applying the principle of <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) (Vapnik, 1998), which, in terms of RL, states that instead of choosing the policy which maximizes the estimated return we should instead maximize a bound on return. In <b>SRM</b> the policy class size is treated as a controlling variable in the optimization of the bound, allowing us to naturally trade-off between estimated performance and estimation confidence. By controlling policy class size in this ...", "dateLastCrawled": "2022-01-26T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Recommended system <b>algorithm</b> related notes | Develop Paper", "url": "https://developpaper.com/recommended-system-algorithm-related-notes/", "isFamilyFriendly": true, "displayUrl": "https://developpaper.com/recommended-system-<b>algorithm</b>-related-notes", "snippet": "A <b>machine</b> <b>learning</b> process that provides data and does not provide data corresponding results. ... <b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) isEmpirical <b>risk</b> (ERM) is added with a regularizer term or penalty term representing the complexity of the model\u3002 The regularization term is generally a monotonic increasing function of model complexity, that is, the more complex the model is, the greater the regularization value is. A typical implementation of <b>structural</b> <b>risk</b> <b>minimization</b> is regularization ...", "dateLastCrawled": "2022-01-30T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Support Vector Machines \u2013 An Introduction</b>", "url": "https://www.researchgate.net/publication/226743605_Support_Vector_Machines_-_An_Introduction", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/226743605_Support_Vector_<b>Machines</b>_-_An...", "snippet": "This is a basic paradigm of the <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) introduced by V apnik and Chervonenkis and their co workers that led to the new <b>learning</b> <b>algorithm</b>.", "dateLastCrawled": "2022-02-01T23:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - Is there an equivalent to VC-dimension for density ...", "url": "https://cstheory.stackexchange.com/questions/47744/is-there-an-equivalent-to-vc-dimension-for-density-estimation-as-opposed-to-clas", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/47744/is-there-an-equivalent-to-vc...", "snippet": "I did find this article which seems like it might come pretty close to answering my question, since they apply <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) to gaussian mixture models, and it looks like they estimate the capacity using &quot;annealed entropy&quot; applied to a class of threshold-based indicator functions associated with the log-likelihood function. However, they don&#39;t provide references to theorems/bounds to show whether this is a principled way to assess the capacity of a probability density ...", "dateLastCrawled": "2022-01-12T03:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Understanding Machine Learning: From Theory</b> to Algorithms [pdf] [PDF ...", "url": "https://authorzilla.com/JjpMG/understanding-machine-learning-from-theory-to-algorithms-pdf.html", "isFamilyFriendly": true, "displayUrl": "https://authorzilla.com/JjpMG/<b>understanding-machine-learning-from-theory</b>-to-<b>algorithms</b>...", "snippet": "We describe the Empirical <b>Risk</b> <b>Minimization</b> (ERM), <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>), and Minimum Description Length (MDL) <b>learning</b> rules, which shows how <b>can</b> a <b>machine</b> learn. We quantify the amount of data needed for <b>learning</b> using the ERM, <b>SRM</b>, and MDL rules and show how <b>learning</b> might fail by deriving . 7 viii a no-free-lunch theorem. We also discuss how much computation time is re- quired for <b>learning</b>. In the second part of the book we describe various <b>learning</b> algorithms. For some of ...", "dateLastCrawled": "2022-02-03T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>understanding-machine-learning</b>-theory-algorithms[1] Pages 1 - 50 - Flip ...", "url": "https://fliphtml5.com/flqg/grxi/basic", "isFamilyFriendly": true, "displayUrl": "https://fliphtml5.com/flqg/grxi/basic", "snippet": "We describethe Empirical <b>Risk</b> <b>Minimization</b> (ERM), <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>),and Minimum Description Length (MDL) <b>learning</b> rules, which shows \u201chow cana <b>machine</b> learn\u201d. We quantify the amount of data needed for <b>learning</b> usingthe ERM, <b>SRM</b>, and MDL rules and show how <b>learning</b> might fail by deriving viii a \u201cno-free-lunch\u201d theorem. We also discuss how much computation time is re- quired for <b>learning</b>. In the second part of the book we describe various <b>learning</b> algorithms. For some ...", "dateLastCrawled": "2021-12-18T06:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) A Comparative Analysis of <b>One-class Structural Risk Minimization</b> ...", "url": "https://www.academia.edu/11049393/A_Comparative_Analysis_of_One_class_Structural_Risk_Minimization_by_Support_Vector_Machines_and_Nearest_Neighbor_Rule", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/11049393/A_Comparative_Analysis_of_One_class_<b>Structural</b>_<b>Risk</b>...", "snippet": "One-class classification is an important problem with applications in several different areas such as outlier detection and <b>machine</b> monitoring. In this paper we propose a novel method for one-class classification, referred to as kernel k-NNDDSRM.", "dateLastCrawled": "2021-02-17T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> algorithms based on &quot;<b>structural</b> <b>risk</b> <b>minimization</b>&quot;?", "url": "https://cs.stackexchange.com/questions/2006/machine-learning-algorithms-based-on-structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://cs.stackexchange.com/questions/2006/<b>machine</b>-<b>learning</b>-<b>algorithms</b>-based-on...", "snippet": "The <b>structural</b> <b>risk</b> <b>minimization</b> principle is a principle that is at least partly &#39;used&#39; in all <b>machine</b> <b>learning</b> methods, since overfitting is often to be taken into account: reducing the complexity of the model is (supposedly and in practice) a good way to limit overfitting.", "dateLastCrawled": "2022-01-19T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b>", "url": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.cs.umass.edu/~akshay/courses/cs690m/files/lec8.pdf", "snippet": "Lecture 8: Model selection and <b>structural</b> <b>risk minimization</b> Akshay Krishnamurthy akshay@cs.umass.edu September 28, 2017 1 Recap Last time we saw our rst real <b>learning</b> <b>algorithm</b> (that wasn\u2019t obviously ERM), namely kernel regression. Recall the estimator took the form ^(x) = P n i=1 Y iK(kX i xk=h) P n i=1 K(kX i xk=h) where the data (X i;Y", "dateLastCrawled": "2021-12-31T02:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Structural Risk Minimization for Character Recognition</b>", "url": "https://proceedings.neurips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "snippet": "<b>Structural Risk Minimization for Character Recognition</b> I. Guyon, V. Vapnik, B. Boser, L. Bottou, and S. A. Solla AT&amp;T Bell Laboratories Holmdel, NJ 07733, USA Abstract The method of <b>Structural</b> <b>Risk</b> <b>Minimization</b> refers to tuning the capacity of the classifier to the available amount of training data. This capac\u00ad", "dateLastCrawled": "2022-02-01T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>learning</b> theory - Nonuniform learnability", "url": "http://ce.sharif.edu/courses/98-99/2/ce718-1/resources/root/Slides/Lect-10.pdf", "isFamilyFriendly": true, "displayUrl": "ce.sharif.edu/courses/98-99/2/ce718-1/resources/root/Slides/Lect-10.pdf", "snippet": "<b>Structural</b> <b>risk</b> <b>minimization</b> The bound that the <b>SRM</b> rule wishes to minimize is given in the following theorem. Theorem Let w : N 7![0;1] be a function such that P1 n=1 w(n) 1. Let H be a hypothesis class that <b>can</b> be written as H = S n2N H n, where for each n, H n satis es the uniform convergence property with a sample complexity function mUC H ...", "dateLastCrawled": "2021-07-11T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "In-sample and Out-of-sample Model Selection and Error Estimation for ...", "url": "http://www.smartlab.ws/files/pubblications/IJP/In-sample%20and%20Out-of-sample%20Model%20Selection%20and%20Error%20Estimation%20for%20Support%20Vector%20Machines%20+%20%20Disclaimer.pdf", "isFamilyFriendly": true, "displayUrl": "www.smartlab.ws/files/pubblications/IJP/In-sample and Out-of-sample Model Selection and...", "snippet": "data\u2013dependent <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) framework and propose a proper re-formulation of the SVM <b>learning</b> <b>algorithm</b>, so that the in\u2013sample approach <b>can</b> be effectively applied. The experiments, performed both on simulated and real\u2013world datasets, show that our in\u2013sample approach <b>can</b> be favorably <b>compared</b> to out\u2013of\u2013sample methods, especially in cases where the last ones provide questionable results. In particular, when the number of samples is small <b>compared</b> to their ...", "dateLastCrawled": "2022-01-13T14:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Learning</b> Gaussian Mixture Models by <b>Structural</b> <b>Risk</b> <b>Minimization</b>", "url": "http://www.cis.pku.edu.cn/faculty/vision/wangliwei/pdf/Learning_Gaussian_Mixture.pdf", "isFamilyFriendly": true, "displayUrl": "www.cis.pku.edu.cn/faculty/vision/wangliwei/pdf/<b>Learning</b>_Gaussian_Mixture.pdf", "snippet": "<b>Learning</b> Gaussian Mixture Models by <b>Structural</b> <b>Risk</b> <b>Minimization</b> Liwei Wang, and Jufu Feng Center for Information Sciences, School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China, fwanglw, fjf g@cis.pku.edu.cn Abstract Gaussian mixture models are often used for probability density estimation in pattern recognition and <b>machine</b> <b>learning</b> systems. Selecting an optimal number of components in mixture model is important to ensure an accurate and ef\ufb01cient ...", "dateLastCrawled": "2022-01-13T04:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Optimization for Cutting Conditions of Surface Roughness in Machining ...", "url": "http://ijens.org/Vol_19_I_05/190701-1905-3939-IJMME-IJENS.pdf", "isFamilyFriendly": true, "displayUrl": "ijens.org/Vol_19_I_05/190701-1905-3939-IJMME-IJENS.pdf", "snippet": "it <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>), to obtain a nonlinearity relationship between the cutting conditions and its surface roughness. The alternative framework gave more accurate prediction model <b>compared</b> to RSM and soft-computing techniques based regression model in a machining dataset, and has less complex structure of regression <b>compared</b> to KPCR and GA. Index Terms\u2014Surface roughness, support vector regression, nonlinear regression, genetic algorithms. I. INTRODUCTION REGRESSION models ...", "dateLastCrawled": "2021-09-17T22:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "CS-E4710 <b>Machine</b> <b>Learning</b>: Supervised Methods", "url": "https://mycourses.aalto.fi/pluginfile.php/1628003/mod_resource/content/2/MLSM_Lecture4_051021.pdf", "isFamilyFriendly": true, "displayUrl": "https://mycourses.aalto.fi/pluginfile.php/1628003/mod_resource/content/2/MLSM_Lecture4...", "snippet": "<b>SRM</b> model selection: pros and cons <b>Structural</b> <b>risk</b> <b>minimization</b> bene ts from strong <b>learning</b> guarantees However, the assumption of a countable decomposition of the hypothesis class is a restrictive one The computational price to pay is large, especially when a large number of hypothesis classes H k has to be processed 17", "dateLastCrawled": "2022-01-17T18:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>Learning</b> Algorithms (CS260) Cheat Sheet", "url": "https://web.cs.ucla.edu/~patricia.xiao/files/CS_260_Cheatsheet_version2.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.cs.ucla.edu/~patricia.xiao/files/CS_260_Cheatsheet_version2.pdf", "snippet": "Described as hypothesis class Hin PAC <b>learning</b> and uniform <b>learning</b>. However there are other ways of expressing it, such as bias to shorter expressions. Generally, bias could be denoted as a weight w(h) as-signed to each hypothesis in a countable hypothesis class H. The weight re ects prior knowledge on the importance of each h. X h2H w(h) 1 An example is the description length. Prior Knowledge Description language is denoted by d(h ) The term pre x-free means that 8h6= h0, d(h) is not a pre ...", "dateLastCrawled": "2022-01-30T20:06:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "PAC, Generalization and <b>SRM</b>", "url": "https://elearning.unipd.it/math/mod/resource/view.php?id=38143", "isFamilyFriendly": true, "displayUrl": "https://e<b>learning</b>.unipd.it/math/mod/resource/view.php?id=38143", "snippet": "Connection to <b>learning</b> Measuring the complexity of the hypotheses space (VC-Dimension) VC-Dimension of hyperplanes <b>Structural</b> <b>Risk</b> <b>Minimization</b> Exercises VC-Dimension of other hypothesis spaces, e.g. intervals in R : h(x) = +1 if a &lt;= x &lt;= b;h(x) = 1 otherwise: Fabio Aiolli PAC, Generalization and <b>SRM</b> October 6th, 202122/22", "dateLastCrawled": "2021-11-19T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Structural Risk Minimization for Character Recognition</b>", "url": "https://proceedings.neurips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf", "snippet": "<b>Structural Risk Minimization for Character Recognition</b> I. Guyon, V. Vapnik, B. Boser, L. Bottou, and S. A. Solla AT&amp;T Bell Laboratories Holmdel, NJ 07733, USA Abstract The method of <b>Structural</b> <b>Risk</b> <b>Minimization</b> refers to tuning the capacity of the classifier to the available amount of training data. This capac\u00ad", "dateLastCrawled": "2022-02-01T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 Empirical <b>risk</b> <b>minimization</b> (ERM) and <b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Tutorial On Support Vector Machines For Pattern Recognition Pdf ...", "url": "https://elizabethsid.org/for-pdf/11704-a-tutorial-on-support-vector-machines-for-pattern-recognition-pdf-565-290.php", "isFamilyFriendly": true, "displayUrl": "https://elizabethsid.org/for-pdf/11704-a-tutorial-on-support-vector-<b>machines</b>-for...", "snippet": "The paper starts with an overview of <b>structural</b> <b>risk</b> <b>minimization</b> <b>SRM</b> principle, and describes the mechanism of how to construct SVM. For a two-class pattern recognition problem, we discuss in detail the classification mechanism of SVM in three cases of linearly separable, linearly nonseparable and nonlinear. Finally, for nonlinear case, we give a new function mapping technique: By choosing an appropriate kernel function, the SVM can map the low-dimensional input space into the high ...", "dateLastCrawled": "2022-01-19T18:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 Empirical <b>risk</b> <b>minimization</b> (ERM) and <b>Structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_<b>Machine</b>s", "snippet": "hypothesis spaces is known as <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) (Vapnik, 1998). An important question that arises in SLT is that of meas uring the &quot;complexity&quot; of a hypothesis space - which, as ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Data gravitation based classification", "url": "http://www.isda03.softcomputing.net/dgc.pdf", "isFamilyFriendly": true, "displayUrl": "www.isda03.softcomputing.net/dgc.pdf", "snippet": "SVM is a relatively new <b>machine</b> <b>learning</b> method based on the statistical <b>learning</b> theory and <b>structural</b> <b>risk</b> <b>minimization</b> (<b>SRM</b>) principle. SVM is gaining popularity due to many attractive features, and promising empirical performance. SVM is based on the hypothesis that the training samples obey a certain distribution, which restricts its application scope. Rough set [17] theory has also been applied to classi\ufb01cation in recent years especially for feature selection [10] or as a ...", "dateLastCrawled": "2021-12-23T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, empirical <b>risk</b>, motivation for Empirical <b>Risk</b> <b>Minimization</b> (ERM) Further Reading, Supplementary: Jan 12: Consistency of ERM, Sufficient condition for ERM as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) A comparative <b>analysis of structural risk minimization by support</b> ...", "url": "https://www.academia.edu/10904454/A_comparative_analysis_of_structural_risk_minimization_by_support_vector_machines_and_nearest_neighbor_rule", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/10904454/A_comparative_analysis_of_<b>structural</b>_<b>risk</b>...", "snippet": "A Comparative <b>Analysis of Structural Risk Minimization by Support Vector Machines</b> and Nearest Neighbor Rule Bilge Kara\u00b8cal\u0131 , Rajeev Ramanath, Wesley E. Snyder a,b,c a Dept. of Radiology, University of Pennsylvania, Philadephia, PA 19104 b Dept. of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC 27695-7914 c Dept. of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC 27695-7911 Abstract Support Vector Machines (SVMs) are by ...", "dateLastCrawled": "2021-07-19T02:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Automatically Detecting Excavator Anomalies Based</b> on <b>Machine</b> <b>Learning</b>", "url": "https://www.readkong.com/page/automatically-detecting-excavator-anomalies-based-on-9697342", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/<b>automatically-detecting-excavator-anomalies-based</b>-on-9697342", "snippet": "Support vector <b>machine</b> is based on the principle of <b>Structural</b> <b>Risk</b> <b>Minimization</b> (<b>SRM</b>) in statistical <b>learning</b> theory, and has good generalization performance [26]. Minimizing <b>structural</b> <b>risk</b> means maximizing profits between different categories. Thus, SVM is not only a useful statistical theory, but also a way to deal with engineering problems [27]. The idea of SVM is to divide training samples into two classes using a linearly separated hyperplane. Symmetry 2019, 11, 957 11 of 18 In this ...", "dateLastCrawled": "2022-01-16T15:53:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(structural risk minimization (srm))  is like +(machine learning algorithm)", "+(structural risk minimization (srm)) is similar to +(machine learning algorithm)", "+(structural risk minimization (srm)) can be thought of as +(machine learning algorithm)", "+(structural risk minimization (srm)) can be compared to +(machine learning algorithm)", "machine learning +(structural risk minimization (srm) AND analogy)", "machine learning +(\"structural risk minimization (srm) is like\")", "machine learning +(\"structural risk minimization (srm) is similar\")", "machine learning +(\"just as structural risk minimization (srm)\")", "machine learning +(\"structural risk minimization (srm) can be thought of as\")", "machine learning +(\"structural risk minimization (srm) can be compared to\")"]}