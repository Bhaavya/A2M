{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "<b>Bias</b> in humans can be unconscious (also called <b>implicit</b> <b>bias</b>), which indicates humans can introduce <b>bias</b> without even knowing they are doing so. Let\u2019s explore how we can detect <b>bias</b> in <b>machine learning</b> models and how it can be eliminated. Types of <b>bias</b>. <b>Bias</b> in <b>machine learning</b> data sets and models is such a problem that you\u2019ll find tools ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Mitigating <b>implicit bias in machine learning</b>", "url": "https://faraday.ai/blog/implicit-bias-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://faraday.ai/blog/<b>implicit</b>-<b>bias</b>-<b>machine</b>-<b>learning</b>", "snippet": "How to minimize the effects of <b>implicit bias in machine learning</b>, from data sourcing to model predictions. An algorithm contains the biases of its builder. At Faraday, we have a handful of approaches we use to minimize these effects at each level of our <b>machine</b> <b>learning</b> pipeline. Blog. \u2261 20 Oct 2020 \u2022 3 min read The weighted scale: Mitigating <b>implicit</b> <b>bias</b> in data science. An algorithm contains the biases of its builder. At Faraday, we have a handful of approaches we use to minimize ...", "dateLastCrawled": "2021-12-24T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> Biases: how to avoid them - Codemotion Magazine", "url": "https://www.codemotion.com/magazine/ai-ml/machine-learning-bias-fantastic-data/", "isFamilyFriendly": true, "displayUrl": "https://www.codemotion.com/magazine/ai-ml/<b>machine</b>-<b>learning</b>-<b>bias</b>-fantastic-data", "snippet": "Latent/<b>implicit</b> <b>bias</b> is when a system starts to learn to correlate certain ideas or certain particular qualities with certain categories such as gender, sexuality or country of origin. For example, we can think of a system that, when searching for images of doctors, first presents images of male doctors over images of female doctors, regardless of how many images it has of male or female doctors.", "dateLastCrawled": "2022-01-31T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Understanding Racial Bias in Machine Learning Algorithms</b> | by The ...", "url": "https://betterprogramming.pub/understanding-racial-bias-in-machine-learning-algorithms-1c5afe76f8b", "isFamilyFriendly": true, "displayUrl": "https://betterprogramming.pub/<b>understanding-racial-bias-in-machine-learning-algorithms</b>...", "snippet": "<b>Implicit</b> <b>bias</b> is pervasive in the tech industry \u2014 in hiring practices, but also in the products and technologies that well-intentioned developers create. In particular, researchers identify <b>machine</b> <b>learning</b> and artificial intelligence as technologies that suffer from <b>implicit</b> racial biases. If software development is truly \u201ceating the world ...", "dateLastCrawled": "2022-01-25T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding <b>Bias</b> in <b>Machine</b> <b>Learning</b> Models - Arize AI", "url": "https://arize.com/blog/understanding-bias-in-ml-models/", "isFamilyFriendly": true, "displayUrl": "https://arize.com/blog/understanding-<b>bias</b>-in-ml-models", "snippet": "For decades, <b>bias</b> in <b>machine</b> <b>learning</b> has been recognized as a potential concern, but it remains a complex and challenging issue for <b>machine</b> <b>learning</b> researchers and engineers when deploying models into production. The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm, which is utilized in US court systems to estimate the probability that a defendant will be a reoffender, is the most prominent example of AI <b>bias</b> and the negative implications on society ...", "dateLastCrawled": "2022-02-01T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Algorithmic bias: on the implicit biases of social technology</b> ...", "url": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "snippet": "The story of Tay highlights an obstacle facing developers of <b>machine</b> <b>learning</b> programs: algorithmic <b>bias</b>. An AI <b>like</b> Tay, which uses <b>machine</b> <b>learning</b> to capitalize on (or \u201clearn\u201d from) statistical regularities in human-generated datasets, tends to pick up social patterns that manifest in human behavior and that are reflected in the data on which it is trained. In many of these cases, we have reason to suspect that programmers are not explicitly writing biases toward marginalized ...", "dateLastCrawled": "2022-01-25T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Research shows AI is often biased. Here&#39;s how to make algorithms work ...", "url": "https://www.weforum.org/agenda/2021/07/ai-machine-learning-bias-discrimination/", "isFamilyFriendly": true, "displayUrl": "https://www.weforum.org/agenda/2021/07/ai-<b>machine</b>-<b>learning</b>-<b>bias</b>-discrimination", "snippet": "Ridding AI and <b>machine</b> <b>learning</b> of <b>bias</b> involves taking their many uses into consideration Image: British Medical Journal To list some of the source of fairness and non-discrimination risks in the use of artificial intelligence, these include: <b>implicit</b> <b>bias</b>, sampling <b>bias</b>, temporal <b>bias</b>, over-fitting to training data, and edge cases and outliers.", "dateLastCrawled": "2022-01-31T15:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Measuring Bias in</b> <b>Machine Learning</b>: The <b>Statistical Bias</b> Test", "url": "https://www.datacamp.com/community/blog/measuring-bias-in-ml", "isFamilyFriendly": true, "displayUrl": "https://www.datacamp.com/community/blog/<b>measuring-bias-in</b>-ml", "snippet": "The question of <b>bias</b> in <b>machine learning</b> models has been the subject of a lot of attention in recent years. Stories of models going wrong make headlines, and humanitarian lawyers, politicians, and journalists have all contributed to the conversation about what ethics and values we want to be reflected in the models we build. While human <b>bias</b> is a thorny issue and not always easily defined, <b>bias</b> in <b>machine learning</b> is, at the end of the day, mathematical. There are many different types of ...", "dateLastCrawled": "2022-01-29T07:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Bias in Natural Language Processing (NLP): A Dangerous</b> But Fixable ...", "url": "https://towardsdatascience.com/bias-in-natural-language-processing-nlp-a-dangerous-but-fixable-problem-7d01a12cf0f7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bias-in-natural-language-processing-nlp-a-dangerous</b>-but...", "snippet": "Essentially, it\u2019s when <b>machine</b> <b>learning</b> algorithms express <b>implicit</b> biases that often pass undetected during testing because most papers test their models for raw accuracy. Take, for example, the following instances of deep <b>learning</b> models expressing gender <b>bias</b>. According to our deep <b>learning</b> models, \u201cHe is doctor\u201d has a higher likelihood than \u201cShe is doctor.\u201d Man is to woman as computer programmer is to homemaker. Sentences with female nouns are more indicative of anger ...", "dateLastCrawled": "2022-02-02T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "GitHub - lauravodden/Gender-<b>Bias</b>-in-<b>Machine</b>-<b>Learning</b>: An investigation ...", "url": "https://github.com/lauravodden/Gender-Bias-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/lauravodden/Gender-<b>Bias</b>-in-<b>Machine</b>-<b>Learning</b>", "snippet": "Finally, <b>implicit</b> cultural <b>bias</b> can result in patterns that cause <b>bias</b> in <b>machine</b> <b>learning</b> predictions (Zhao et al., 2018). Issues <b>like</b> systemic racial prejudice and gender role <b>bias</b> can and do make their way into <b>machine</b> <b>learning</b> predictions (Sun et al., 2019; Wei, 2020). An example of this <b>machine</b> <b>learning</b> <b>bias</b> is the association of \u2018black\u2019 to \u2018criminal\u2019 and \u2018Caucasian\u2019 to \u2018police\u2019, or \u2018doctor\u2019 to \u2018man\u2019 and \u2018nurse\u2019 to \u2018woman\u2019 (Ethayarajh, 2020; Wei, 2020).", "dateLastCrawled": "2022-02-02T21:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "<b>Bias</b> in humans can be unconscious (also called <b>implicit</b> <b>bias</b>), which indicates humans can introduce <b>bias</b> without even knowing they are doing so. Let\u2019s explore how we can detect <b>bias</b> in <b>machine learning</b> models and how it can be eliminated. Types of <b>bias</b>. <b>Bias</b> in <b>machine learning</b> data sets and models is such a problem that you\u2019ll find tools ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Mitigating <b>Bias</b> in <b>Machine Learning</b>: An introduction to ...", "url": "https://towardsdatascience.com/mitigating-bias-in-machine-learning-an-introduction-to-mlfairnesspipeline-42e007dce0a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/mitigating-<b>bias</b>-in-<b>machine-learning</b>-an-introduction-to...", "snippet": "<b>Bias</b> in <b>Machine Learning</b>. <b>Bias</b> takes many different forms and impact all groups of people. It can range from <b>implicit</b> to expli c it and is often very difficult to detect. In the field of <b>machine learning</b> <b>bias</b> is often subtle and hard to identify, let alone solve. Why is this a problem? <b>Implicit</b> <b>bias</b> in <b>machine learning</b> has very real consequences including denial of a loan, a lengthier prison sentence, and many other harmful outcomes for underprivileged groups. The data scientists designing ...", "dateLastCrawled": "2022-02-02T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Understanding Racial Bias in Machine Learning Algorithms</b> | by The ...", "url": "https://betterprogramming.pub/understanding-racial-bias-in-machine-learning-algorithms-1c5afe76f8b", "isFamilyFriendly": true, "displayUrl": "https://betterprogramming.pub/<b>understanding-racial-bias-in-machine-learning-algorithms</b>...", "snippet": "<b>Implicit</b> <b>bias</b> is pervasive in the tech industry \u2014 in hiring practices, but also in the products and technologies that well-intentioned developers create. In particular, researchers identify <b>machine</b> <b>learning</b> and artificial intelligence as technologies that suffer from <b>implicit</b> racial biases. If software development is truly \u201ceating the world ...", "dateLastCrawled": "2022-01-25T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Algorithmic bias: on the implicit biases of social technology</b> ...", "url": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "snippet": "Often <b>machine</b> <b>learning</b> programs inherit social patterns reflected in their training data without any directed effort by programmers to include such biases. Computer scientists call this algorithmic <b>bias</b>. This paper explores the relationship between <b>machine</b> <b>bias</b> and human cognitive <b>bias</b>. In it, I argue similarities between algorithmic and cognitive biases indicate a disconcerting sense in which sources of <b>bias</b> emerge out of seemingly innocuous patterns of information processing. The emergent ...", "dateLastCrawled": "2022-01-25T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Potential Biases in <b>Machine</b> <b>Learning</b> Algorithms Using Electronic Health ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6347576/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6347576", "snippet": "<b>Machine</b> <b>learning</b> hasgen-erated substantial advances in medical imaging, for example, through improved detection of colonic polyps, cerebral microbleeding, and diabetic retinopathy. 2 Predictive modelingwith electronic health records usingdeep <b>learning</b> can accurately predict in-hospital mortality, 30-day unplanned readmission, prolonged length of stay, and final discharge diagnoses.3 Integration of <b>machine</b> <b>learning</b> with clinical decision support tools, such as computerized alerts or ...", "dateLastCrawled": "2022-01-27T11:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bias and Fairness in Machine Learning</b> - Abhishek Tiwari", "url": "https://www.abhishek-tiwari.com/bias-and-fairness-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.abhishek-tiwari.com/<b>bias-and-fairness-in-machine-learning</b>", "snippet": "In AI and <b>machine</b> <b>learning</b>, the future resembles the past and <b>bias</b> refers to prior information. There has been a growing interest in identifying the harmful biases in the <b>machine</b> <b>learning</b>. Often these harmful biases are just the reflection or amplification of human biases which algorithms learn from training data. Some training data sets such as text, medical, criminal, educational, financial etc. are more susceptible to human biases compared to others. For example, weather data is little or ...", "dateLastCrawled": "2022-01-29T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>The Implicit Bias Challenge</b> - JobSync", "url": "https://www.jobsync.io/the-implicit-bias-challenge/", "isFamilyFriendly": true, "displayUrl": "https://www.jobsync.io/<b>the-implicit-bias-challenge</b>", "snippet": "Sexism, racism and other unrecognized biases can be built into <b>machine</b>-<b>learning</b> algorithms underlying the intelligence and shape the way people are categorized and addressed. This risks perpetuating an already vicious cycle of <b>bias</b>\u2026The truth is that most of the programming and data analytics are being created globally by white males\u2026has shown that women are less likely than men to be shown ads on Google for executive jobs\u2026these algorithmic flaws are not easy to detect. Ingrained <b>bias</b> ...", "dateLastCrawled": "2022-01-25T19:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Addressing Fairness, <b>Bias</b>, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "While the <b>bias</b> found in other application domains of <b>machine</b> <b>learning</b> (finance, employment, law enforcement, etc.) may often be due to sampling <b>bias</b> or <b>implicit</b> cultural <b>bias</b>, the domain of health also contains true systematic <b>bias</b> inherent in biological processes which may not be possible to mitigate or \u201crepair.\u201d In the domain of health, there are true genetic differences across different races and ethnic groups which affect disease prevalence, and these differences cannot (and should ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Discussion Questions About <b>Implicit</b> <b>Bias</b> and <b>Similar</b> Products and ...", "url": "https://www.listalternatives.com/discussion-questions-about-implicit-bias", "isFamilyFriendly": true, "displayUrl": "https://www.listalternatives.com/discussion-questions-about-<b>implicit</b>-<b>bias</b>", "snippet": "<b>Implicit</b> <b>Bias</b> <b>Implicit</b> <b>bias</b> exists when people unconsciously hold attitudes toward others or associate stereotypes with them. Discussion Questions 1. In a presentation, Professor Will Cox shows two news photos published in the wake of Hurricane Katrina. One shows a young black man walking through swirling water holding a carton of soda.", "dateLastCrawled": "2022-02-03T07:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Dealing With <b>Bias</b> in Artificial Intelligence - <b>The New York Times</b>", "url": "https://www.nytimes.com/2019/11/19/technology/artificial-intelligence-bias.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.nytimes.com</b>/2019/11/19/technology/artificial-intelligence-<b>bias</b>.html", "snippet": "So, if your <b>machine</b>-<b>learning</b> algorithm is one that is trained on the data from a given set of hospitals, and you will only use it in those same set of hospitals, then latching onto which hospital ...", "dateLastCrawled": "2022-01-27T07:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Artificial Intelligence Has An <b>Implicit</b> <b>Bias</b> Diversity Dilemma", "url": "https://carpenterwellington.com/post/artificial-intelligence-implicit-bias-diversity-dilemma/", "isFamilyFriendly": true, "displayUrl": "https://carpenterwellington.com/post/artificial-intelligence-<b>implicit</b>-<b>bias</b>-diversity...", "snippet": "Growing Concern About <b>Implicit</b> <b>Bias</b>. There is growing concern about <b>implicit</b> biases built into artificial intelligence systems. These biases could lead to unfair outcomes. The foundation of these systems is the data inputs and the people behind designing the technology. If the <b>implicit</b> biases of the people building the technology skew the data, that will carry forward to skewed outputs. After all, <b>machine</b> <b>learning</b> algorithms are limited to the data sets available to them. Few Women, Fewer ...", "dateLastCrawled": "2022-01-26T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bias and discrimination in machine learning</b> \u00b7 Merlin Rebrovi\u0107", "url": "https://merlin.rebrovic.net/blog/ml-bias", "isFamilyFriendly": true, "displayUrl": "https://merlin.rebrovic.net/blog/ml-<b>bias</b>", "snippet": "A man\u2019s visa application was rejected in New Zealand because the <b>machine</b> <b>thought</b> his eyes were closed on his photo. ... and held an <b>implicit</b> <b>bias</b> against those who were not in that population. The fix for this, and for the more general problem of <b>bias and discrimination in machine learning</b>, is hard for many reasons: People don\u2019t realize they aren\u2019t inclusive. It\u2019s easy to recognize a movie star in the latest blockbuster. In contrast, it\u2019s hard to notice that a small-time actor, who ...", "dateLastCrawled": "2021-12-28T23:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>The Implicit Bias Challenge</b> - JobSync", "url": "https://www.jobsync.io/the-implicit-bias-challenge/", "isFamilyFriendly": true, "displayUrl": "https://www.jobsync.io/<b>the-implicit-bias-challenge</b>", "snippet": "Sexism, racism and other unrecognized biases <b>can</b> be built into <b>machine</b>-<b>learning</b> algorithms underlying the intelligence and shape the way people are categorized and addressed. This risks perpetuating an already vicious cycle of <b>bias</b>\u2026The truth is that most of the programming and data analytics are being created globally by white males\u2026has shown that women are less likely than men to be shown ads on Google for executive jobs\u2026these algorithmic flaws are not easy to detect. Ingrained <b>bias</b> ...", "dateLastCrawled": "2022-01-25T19:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "News and Media <b>Bias</b> Detection using <b>Machine Learning</b> | by Jerry Wei ...", "url": "https://towardsdatascience.com/news-and-media-bias-detection-using-machine-learning-a-potential-way-to-find-fake-news-13c766aa3988", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/news-and-media-<b>bias</b>-detection-using-<b>machine-learning</b>-a...", "snippet": "<b>Machine learning</b> has recently seen a huge increase because of a rise in both available data and computational power. Researchers have also been working to make even more complex neural networks with more and more layers (deep <b>learning</b>), which allows them to solve even harder problems. <b>Machine learning</b> itself has a bunch of applications in almost every field imaginable; recent advances in <b>machine learning</b> include self-driving cars, language translation, and facial recognition. I previously ...", "dateLastCrawled": "2022-01-20T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Analyzing &amp; <b>Preventing Unconscious Bias in Machine Learning</b>", "url": "https://www.infoq.com/presentations/unconscious-bias-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.infoq.com/presentations/unconscious-<b>bias</b>-<b>machine</b>-<b>learning</b>", "snippet": "So, one is that <b>machine</b> <b>learning</b> <b>can</b> actually amplify <b>bias</b>. There is an interesting paper called &quot;Men also like shopping&quot; where they looked at a dataset. These are kind commonly site datasets ...", "dateLastCrawled": "2022-02-03T03:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Implicit Bias of Gradient Descent</b> on Linear Convolutional ... - NeurIPS", "url": "https://proceedings.neurips.cc/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf", "snippet": "<b>Implicit</b> biases introduced by optimization algorithms play an crucial role in <b>learning</b> deep neural net-works [Neyshabur et al., 2015b,a, Hochreiter and Schmidhuber, 1997, Keskar et al., 2016, Chaudhari et al., 2016, Dinh et al., 2017, Andrychowicz et al., 2016, Neyshabur et al., 2017, Zhang et al., 2017, Wilson et al., 2017, Hoffer et al., 2017, Smith, 2018]. Large scale neural networks used in practice are highly over-parameterized with far more trainable model parameters compared to the ...", "dateLastCrawled": "2022-01-12T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Social Bias in Machine Learning</b> | <b>Machine</b> <b>Learning</b> Medium", "url": "https://machinelearningmedium.com/2018/10/09/algorithmic-fairness/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>medium.com/2018/10/09/algorithmic-fairness", "snippet": "Recently <b>machine</b> <b>learning</b> has seen its utilitization in a lot of important decision making pipelines such as predicting time of recidivism, college acceptance, loan approvals etc., and hence it becomes increasingly important to question the <b>machine</b> <b>learning</b> models being developed in terms of <b>implicit</b> <b>bias</b> that they might be inheriting from the data that they train on. In order to do away with such biases in a <b>machine</b> <b>learning</b> algorithm one needs to understand how exactly does <b>bias</b> creep in ...", "dateLastCrawled": "2021-12-28T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Survivorship bias</b> in Data Science and <b>Machine</b> <b>Learning</b> | by Gonzalo ...", "url": "https://towardsdatascience.com/survivorship-bias-in-data-science-and-machine-learning-4581419b3bca", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>survivorship-bias</b>-in-data-science-and-<b>machine</b>-<b>learning</b>...", "snippet": "Fraud prevention: <b>survivorship bias</b> within the data science, <b>machine</b> <b>learning</b> and artificial intelligence world for fraud prevention <b>can</b> be also very dangerous. Usually, our clients at Ravelin come to us already using some kind of solution for fraud prevention. Whether it is a rules system, manual reviews of customers or other. Imagine if for building our clients\u2019 <b>machine</b> <b>learning</b> model we would take only those fraudsters that got through the client previous solution. We would be using ...", "dateLastCrawled": "2022-01-28T15:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Implicit Bias: Sharing Our Stories</b> \u2013 Ensemble <b>Learning</b>", "url": "https://ensemblelearning.org/implicit-bias-sharing-our-stories/", "isFamilyFriendly": true, "displayUrl": "https://ensemble<b>learning</b>.org/<b>implicit-bias-sharing-our-stories</b>", "snippet": "If you want to think more about your own <b>implicit</b> biases, it <b>can</b> be helpful to start with a test that measures them. We like the Harvard Project <b>Implicit</b> tests (bonus: your data is used to anonymously power Harvard\u2019s research!). If you or your staff are interested in Ensemble <b>Learning</b>\u2019s tools to identify, measure, and combat <b>implicit</b> <b>bias</b>, reach out to Dr. Leigh Mingle at lmingle@ensemblelearning.org for more information.", "dateLastCrawled": "2022-02-02T05:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "While the <b>bias</b> found in other application domains of <b>machine</b> <b>learning</b> (finance, employment, law enforcement, etc.) may often be due to sampling <b>bias</b> or <b>implicit</b> cultural <b>bias</b>, the domain of health also contains true systematic <b>bias</b> inherent in biological processes which may not be possible to mitigate or \u201crepair.\u201d In the domain of health, there are true genetic differences across different races and ethnic groups which affect disease prevalence, and these differences cannot (and should ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "<b>Bias</b> in humans <b>can</b> be unconscious (also called <b>implicit</b> <b>bias</b>), which indicates humans <b>can</b> introduce <b>bias</b> without even knowing they are doing so. Let\u2019s explore how we <b>can</b> detect <b>bias</b> in <b>machine learning</b> models and how it <b>can</b> be eliminated. Types of <b>bias</b> . <b>Bias</b> in <b>machine learning</b> data sets and models is such a problem that you\u2019ll find tools from many of the leaders in <b>machine learning</b> development. Detecting <b>bias</b> starts with the data set. A data set might not represent the problem space ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Mitigating <b>Bias</b> in <b>Machine Learning</b>: An introduction to ...", "url": "https://towardsdatascience.com/mitigating-bias-in-machine-learning-an-introduction-to-mlfairnesspipeline-42e007dce0a5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/mitigating-<b>bias</b>-in-<b>machine-learning</b>-an-introduction-to...", "snippet": "<b>Bias</b> in <b>Machine Learning</b>. <b>Bias</b> takes many different forms and impact all groups of people. It <b>can</b> range from <b>implicit</b> to expli c it and is often very difficult to detect. In the field of <b>machine learning</b> <b>bias</b> is often subtle and hard to identify, let alone solve. Why is this a problem? <b>Implicit</b> <b>bias</b> in <b>machine learning</b> has very real consequences including denial of a loan, a lengthier prison sentence, and many other harmful outcomes for underprivileged groups. The data scientists designing ...", "dateLastCrawled": "2022-02-02T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bias and Fairness in Machine Learning</b> - Abhishek Tiwari", "url": "https://www.abhishek-tiwari.com/bias-and-fairness-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.abhishek-tiwari.com/<b>bias-and-fairness-in-machine-learning</b>", "snippet": "In AI and <b>machine</b> <b>learning</b>, the future resembles the past and <b>bias</b> refers to prior information. There has been a growing interest in identifying the harmful biases in the <b>machine</b> <b>learning</b>. Often these harmful biases are just the reflection or amplification of human biases which algorithms learn from training data. Some training data sets such as text, medical, criminal, educational, financial etc. are more susceptible to human biases <b>compared</b> to others. For example, weather data is little or ...", "dateLastCrawled": "2022-01-29T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Artificial Intelligence Has An <b>Implicit</b> <b>Bias</b> Diversity Dilemma", "url": "https://carpenterwellington.com/post/artificial-intelligence-implicit-bias-diversity-dilemma/", "isFamilyFriendly": true, "displayUrl": "https://carpenterwellington.com/post/artificial-intelligence-<b>implicit</b>-<b>bias</b>-diversity...", "snippet": "Growing Concern About <b>Implicit</b> <b>Bias</b>. There is growing concern about <b>implicit</b> biases built into artificial intelligence systems. These biases could lead to unfair outcomes. The foundation of these systems is the data inputs and the people behind designing the technology. If the <b>implicit</b> biases of the people building the technology skew the data, that will carry forward to skewed outputs. After all, <b>machine</b> <b>learning</b> algorithms are limited to the data sets available to them. Few Women, Fewer ...", "dateLastCrawled": "2022-01-26T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Comparison of <b>Implicit</b> vs. Explicit Regime Identification in <b>Machine</b> ...", "url": "https://www.academia.edu/59128750/Comparison_of_Implicit_vs_Explicit_Regime_Identification_in_Machine_Learning_Methods_for_Solar_Irradiance_Prediction", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/59128750/Comparison_of_<b>Implicit</b>_vs_Explicit_Regime...", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> bridge the gap between persistence and physical-based NWP models by <b>learning</b> the relationships among predictor variables and solar irradiance; however, there are many different <b>machine</b> <b>learning</b> algorithms that <b>can</b> be used and each has its strengths and weaknesses depending on the quality of the training data. The most common <b>machine</b> <b>learning</b> algorithms for solar irradiance or power prediction are support vector machines (SVM)/support vector regression (SVR) and ...", "dateLastCrawled": "2021-11-22T03:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Potential Biases in <b>Machine</b> <b>Learning</b> Algorithms Using Electronic Health ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6347576/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6347576", "snippet": "<b>Machine</b> <b>learning</b> hasgen-erated substantial advances in medical imaging, for example, through improved detection of colonic polyps, cerebral microbleeding, and diabetic retinopathy. 2 Predictive modelingwith electronic health records usingdeep <b>learning</b> <b>can</b> accurately predict in-hospital mortality, 30-day unplanned readmission, prolonged length of stay, and final discharge diagnoses.3 Integration of <b>machine</b> <b>learning</b> with clinical decision support tools, such as computerized alerts or ...", "dateLastCrawled": "2022-01-27T11:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Measuring Bias in</b> <b>Machine Learning</b>: The <b>Statistical Bias</b> Test", "url": "https://www.datacamp.com/community/blog/measuring-bias-in-ml", "isFamilyFriendly": true, "displayUrl": "https://www.datacamp.com/community/blog/<b>measuring-bias-in</b>-ml", "snippet": "The question of <b>bias</b> in <b>machine learning</b> models has been the subject of a lot of attention in recent years. Stories of models going wrong make headlines, and humanitarian lawyers, politicians, and journalists have all contributed to the conversation about what ethics and values we want to be reflected in the models we build. While human <b>bias</b> is a thorny issue and not always easily defined, <b>bias</b> in <b>machine learning</b> is, at the end of the day, mathematical. There are many different types of ...", "dateLastCrawled": "2022-01-29T07:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>What is unconscious bias</b>? How does it affect you?", "url": "https://caseguard.com/articles/what-is-unconscious-bias/", "isFamilyFriendly": true, "displayUrl": "https://caseguard.com/articles/<b>what-is-unconscious-bias</b>", "snippet": "The first step to combating unconscious <b>bias</b> in <b>machine</b> <b>learning</b> is identify what <b>bias</b> in the data is being fed <b>to machine</b> <b>learning</b>. The algorithms that <b>machine</b> <b>learning</b> is predicated upon <b>can</b> only function based on the data being given. If a data set has inherently marginalized a certain demographic of people, that will obviously be made apparent through <b>machine</b> <b>learning</b> as well. Another angle is to analyze the processes designed to catch unconscious <b>bias</b> present in <b>machine</b> <b>learning</b> and ...", "dateLastCrawled": "2022-01-30T06:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Fix <b>Bias in Machine Learning Algorithms</b>? - Yields.io", "url": "https://www.yields.io/blog/how-to-fix-bias-in-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://www.yields.io/blog/how-to-fix-<b>bias-in-machine-learning-algorithms</b>", "snippet": "Pre-existing <b>bias</b> in algorithms is a consequence of underlying social and institutional ideologies, which <b>can</b> have an impact on the designers or programmers of the software \u2013 human <b>bias</b> in <b>machine</b> <b>learning</b>. These preconceptions <b>can</b> be explicit and conscious, or <b>implicit</b> and unconscious. In general, <b>bias</b> <b>can</b> appear in algorithms through both the modeling approach but also through the use of poor data, or data from a biased source.", "dateLastCrawled": "2022-01-25T05:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Human <b>Bias</b> in <b>Machine Learning</b>. A brief exploration on the impacts that ...", "url": "https://towardsdatascience.com/bias-what-it-means-in-the-big-data-world-6e64893e92a1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bias</b>-what-it-means-in-the-big-data-world-6e64893e92a1", "snippet": "The classic example used to describe this <b>bias</b> is a <b>machine learning</b> model that\u2019s designed to differentiate between men and women in pictures. The training data contains more pictures of women in kitchens than men in kitchens, or more pictures of men coding than women, then the algorithm is trained to make incorrect inferences about the gender of people engaged in those activities due to prejudices that occur in the real world, represented in the data. Sentiment Analysis is the use of ...", "dateLastCrawled": "2022-02-02T02:18:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "<b>Bias</b> in humans can be unconscious (also called <b>implicit</b> <b>bias</b>), which indicates humans can introduce <b>bias</b> without even knowing they are doing so. Let\u2019s explore how we can detect <b>bias</b> in <b>machine learning</b> models and how it can be eliminated. Types of <b>bias</b>. <b>Bias</b> in <b>machine learning</b> data sets and models is such a problem that you\u2019ll find tools ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Implicit/Machine learning gender bias</b> | ReberLab", "url": "https://www.reberlab.psych.northwestern.edu/2018/10/11/implicitmachine-learning-gender-bias/", "isFamilyFriendly": true, "displayUrl": "https://www.reberlab.psych.northwestern.edu/.../10/11/<b>implicitmachine-learning-gender-bias</b>", "snippet": "First, I have found myself describing on a few recent occasions that it is reasonable to think of <b>implicit</b> <b>learning</b> (IL) as the brain\u2019s <b>machine</b> <b>learning</b> (ML) algorithm. ML is a super-hot topic in AI and data science research, so this might be a useful <b>analogy</b> to help people understand what we mean by studying IL. We characterize IL as the statistical extraction of patterns in the environment and the shaping of cognitive processing to maximize efficiency and effectiveness to these patterns ...", "dateLastCrawled": "2020-11-16T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Algorithmic bias: on the implicit biases of social technology</b> ...", "url": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "snippet": "Often <b>machine</b> <b>learning</b> programs inherit social patterns reflected in their training data without any directed effort by programmers to include such biases. Computer scientists call this algorithmic <b>bias</b>. This paper explores the relationship between <b>machine</b> <b>bias</b> and human cognitive <b>bias</b>. In it, I argue similarities between algorithmic and cognitive biases indicate a disconcerting sense in which sources of <b>bias</b> emerge out of seemingly innocuous patterns of information processing. The emergent ...", "dateLastCrawled": "2022-01-25T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Harnessing data for unbiased <b>Learning</b> of Machines | by Vivek Gupta | Medium", "url": "https://gvivek.medium.com/harnessing-data-for-unbiased-learning-of-machines-dc614497a29c", "isFamilyFriendly": true, "displayUrl": "https://gvivek.medium.com/harnessing-data-for-un<b>bias</b>ed-<b>learning</b>-of-<b>machines</b>-dc614497a29c", "snippet": "In <b>Machine</b> <b>Learning</b>, the input data reflects stereotypes and biases of the broader society, then the output of the <b>learning</b> algorithm also captures these stereotypes. Here, we will discuss the gender stereotypes in word embedding. word-embedding encode semantic information, they also exhibit hidden biases inherent in the dataset they are trained on associations such as: father:doctor :: mother:nurse. man:computer programmer :: woman:homemaker. The prejudices and stereotypes in these ...", "dateLastCrawled": "2022-01-12T17:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning</b> | Gendered Innovations", "url": "http://genderedinnovations.stanford.edu/case-studies/machinelearning.html", "isFamilyFriendly": true, "displayUrl": "genderedinnovations.stanford.edu/case-studies/<b>machinelearning</b>.html", "snippet": "As <b>machine learning</b> becomes increasingly ubiquitous in everyday lives, such <b>bias</b>, if uncorrected, can lead to social inequities. Researchers need to understand how gender and ethnicity operate within the context of their algorithm in order to enhance or, at least, not reinforce social equalities. Here we suggest avenues for reducing <b>bias</b> in training data and algorithms in efforts to produce AI that enhances social equalities.", "dateLastCrawled": "2022-01-25T22:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Introduction to Natural Language Processing</b> (NLP) and <b>Bias</b> in AI | by ...", "url": "https://towardsdatascience.com/introduction-to-natural-language-processing-nlp-and-bias-in-ai-877d3f3ee680", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>introduction-to-natural-language-processing</b>-nlp-and...", "snippet": "In this post, I will introduce key concepts of NLP such as word embeddings, and we will see how an algorithm can become biased, and how we can remove that <b>bias</b>. Le t \u2019s get started! For hands-on video tutorials on <b>machine</b> <b>learning</b>, deep <b>learning</b>, and artificial intelligence, checkout my YouTube channel.", "dateLastCrawled": "2022-01-20T19:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fairness Metrics - Data Analytics - Data Science, <b>Machine</b> <b>Learning</b>, AI", "url": "https://vitalflux.com/fairness-metrics-ml-model-sensitivity-bias-detection/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/<b>fairness-metrics-ml-model-sensitivity</b>-<b>bias</b>-detection", "snippet": "There are many different ways in which <b>machine</b> <b>learning</b> ... Instinctively (based on <b>implicit</b> human <b>bias</b>), it is one of the specific \u201cgender\u201d (male or female) against which the model could get biased as like in the real world in predicting whether those with a specific gender could re-offend or not. Thus, one of the protected attributes becomes gender. For the current example, in the real world, one could get biased in favor of female and not (or fail to) classify them as the ones who ...", "dateLastCrawled": "2022-01-20T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word ...", "url": "https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf", "snippet": "signi\ufb01cant risk and challenge for <b>machine</b> <b>learning</b> and its applications. The analogies generated from these embeddings spell out the <b>bias</b> <b>implicit</b> in the data on which they were trained. Hence, word embeddings may serve as a means to extract <b>implicit</b> gender associations from a large text corpus", "dateLastCrawled": "2022-02-02T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning is Requirements Engineering</b> \u2014 On the Role of Bugs ...", "url": "https://medium.com/analytics-vidhya/machine-learning-is-requirements-engineering-8957aee55ef4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>machine-learning-is-requirements-engineering</b>-8957...", "snippet": "A <b>machine</b>-learned model is not an attempt at implementing an <b>implicit</b> specification; a <b>machine</b>-learned model is a specification! It is a learned description of how the system shall behave. The ...", "dateLastCrawled": "2022-01-19T17:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Man is to Computer Programmer as Woman is</b> to Homemaker? | by Sheldon ...", "url": "https://towardsdatascience.com/man-is-to-computer-programmer-as-woman-is-to-homemaker-e57b07cbde96", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>man-is-to-computer-programmer-as-woman-is</b>-to-homemaker...", "snippet": "The paper discusses gender <b>bias</b> in <b>machine</b> <b>learning</b> as a result of using biased training data and proposes a solution to debias the model. This article contains an overview of the paper and discusses key findings in the python implementation of the paper. Overview. The authors of the paper used Word Embe d ding Model to demonstrate gender <b>bias</b> in the training data. The Word Embedding Model was trained on Google News articles and the vector representation of all words were stored in the ...", "dateLastCrawled": "2022-01-30T22:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CSWA: <b>Unconscious Bias</b> | American Astronomical Society", "url": "https://aas.org/comms/cswa/resources/unconsciousbias", "isFamilyFriendly": true, "displayUrl": "https://aas.org/comms/cswa/resources/<b>unconsciousbias</b>", "snippet": "The intervention is based on the premise that <b>implicit bias is like</b> a habit that can be broken through a combination of awareness of implicit bias, concern about the effects of that bias, and the application of strategies to reduce bias. In a 12-week longitudinal study, people who received the intervention showed dramatic reductions in implicit race bias. People who were concerned about discrimination or who reported using the strategies showed the greatest reductions. The intervention also ...", "dateLastCrawled": "2022-02-03T07:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Patrick Forscher</b> \u2013 Research Lead \u2013 Busara Center for Behavioral ...", "url": "https://ch.linkedin.com/in/patrick-forscher-91163854", "isFamilyFriendly": true, "displayUrl": "https://ch.linkedin.com/in/<b>patrick-forscher</b>-91163854", "snippet": "The intervention is based on the premise that <b>implicit bias is like</b> a habit that can be broken through a combination of awareness of implicit bias, concern about the effects of that bias, and the application of strategies to reduce bias. In a 12-week longitudinal study, people who received the intervention showed dramatic reductions in implicit race bias. People who were\u2026 We developed a multi-faceted prejudice habit-breaking intervention to produce long-term reductions in implicit race ...", "dateLastCrawled": "2022-02-03T15:10:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Those designing healthcare algorithms must become actively</b> anti-racist ...", "url": "https://www.nature.com/articles/s41591-020-1020-3", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41591-020-1020-3", "snippet": "<b>Just as \u2018implicit bias</b>\u2019 training for police does little to change racist behavior 10 \u2014in large part because departmental cultures do not fully support the lessons of anti-racism\u2014healthcare ...", "dateLastCrawled": "2021-11-15T00:12:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(implicit bias)  is like +(machine learning)", "+(implicit bias) is similar to +(machine learning)", "+(implicit bias) can be thought of as +(machine learning)", "+(implicit bias) can be compared to +(machine learning)", "machine learning +(implicit bias AND analogy)", "machine learning +(\"implicit bias is like\")", "machine learning +(\"implicit bias is similar\")", "machine learning +(\"just as implicit bias\")", "machine learning +(\"implicit bias can be thought of as\")", "machine learning +(\"implicit bias can be compared to\")"]}