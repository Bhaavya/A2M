{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[2011.11284] <b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep Learning ...", "url": "https://arxiv.org/abs/2011.11284", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2011.11284", "snippet": "Deep learning algorithms are growing in popularity in the field of exoplanetary science due to their ability to model highly non-linear relations and solve interesting problems in a data-driven manner. Several works have attempted to perform fast retrievals of atmospheric parameters with the use of machine learning algorithms <b>like</b> deep neural networks (DNNs). Yet, despite their high predictive power, DNNs are also infamous for being &#39;<b>black</b> boxes&#39;. It is their apparent lack of explainability ...", "dateLastCrawled": "2021-12-28T21:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep-learning Models for ...", "url": "https://ui.adsabs.harvard.edu/abs/2021AJ....162..195Y/abstract", "isFamilyFriendly": true, "displayUrl": "https://ui.adsabs.harvard.edu/abs/2021AJ....162..195Y/abstract", "snippet": "Deep-learning algorithms are growing in popularity in the field of exoplanetary science due to their ability to model highly nonlinear relations and solve interesting problems in a data-driven manner. Several works have attempted to perform fast retrievals of atmospheric parameters with the use of machine-learning algorithms <b>like</b> deep neural networks (DNNs). Yet, despite their high predictive power, DNNs are also infamous for being &quot;<b>black</b> boxes.&quot; It is their apparent lack of explainability ...", "dateLastCrawled": "2021-11-14T05:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep Learning Models ...", "url": "https://www.academia.edu/69799916/Peeking_inside_the_Black_Box_Interpreting_Deep_Learning_Models_for_Exoplanet_Atmospheric_Retrievals", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69799916/<b>Peeking</b>_<b>inside</b>_the_<b>Black</b>_<b>Box</b>_Interpreting_Deep...", "snippet": "Draft version July 26, 2021 Typeset using LATEX twocolumn style in AASTeX62 <b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep Learning Models for Exoplanet Atmospheric Retrievals Kai Hou Yip,1 Quentin Changeat,1 Nikolaos Nikolaou,1 Mario Morvan,1 Billy Edwards,1 Ingo P. Waldmann,1 and Giovanna Tinetti1 1 Departmentof Physics and Astronomy University College London arXiv:2011.11284v2 [astro-ph.EP] 23 Jul 2021 Gower Street,WC1E 6BT London, United Kingdom (Accepted July 20, 2021) Submitted to AJ ...", "dateLastCrawled": "2022-02-05T04:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Peeking</b> <b>Inside the Black Box: Techniques</b> for Making AI Models More ...", "url": "https://www.rtinsights.com/peeking-inside-the-black-box-techniques-for-making-ai-models-more-easily-interpretable/", "isFamilyFriendly": true, "displayUrl": "https://www.rtinsights.com/<b>peeking</b>-<b>inside-the-black-box-techniques</b>-for-making-ai...", "snippet": "<b>Peeking</b> <b>Inside the Black Box: Techniques</b> for Making <b>AI Models More Easily Interpretable</b> By Dan Capellupo | May 18, 2020 with 0 Comments. Tweet. Considering the increasing consequences of faulty AI, explainable AI is going to be a critical issue facing data science for many years. When training a machine learning or AI model, typically the main goal is to make the most accurate prediction possible. Data scientists and machine learning engineers will transform their data in myriad ways and ...", "dateLastCrawled": "2022-01-22T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep Learning Models for ...", "url": "https://deepai.org/publication/peeking-inside-the-black-box-interpreting-deep-learning-models-for-exoplanet-atmospheric-retrievals", "isFamilyFriendly": true, "displayUrl": "https://<b>deepai</b>.org/publication/<b>peeking</b>-<b>inside</b>-the-<b>black</b>-<b>box</b>-interpreting-deep-learning...", "snippet": "<b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep <b>Learning Models for Exoplanet Atmospheric Retrievals</b> . 11/23/2020 . \u2219. by Kai Hou Yip, et al. \u2219. 16 \u2219. share Deep learning algorithms are growing in popularity in the field of exoplanetary science due to their ability to model highly non-linear relations and solve interesting problems in a data-driven manner. Several works have attempted to perform fast retrievals of atmospheric parameters with the use of machine learning algorithms <b>like</b> ...", "dateLastCrawled": "2021-12-24T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep-learning Models for ...", "url": "https://iopscience.iop.org/article/10.3847/1538-3881/ac1744/meta", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.3847/1538-3881/ac1744/meta", "snippet": "<b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep-learning Models for Exoplanet Atmospheric Retrievals Kai Hou Yip 1 , Quentin Changeat 1 , Nikolaos Nikolaou 1 , Mario Morvan 1 , Billy Edwards 1 , Ingo P. Waldmann 1 , and Giovanna Tinetti 1", "dateLastCrawled": "2021-12-02T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep-learning Models for ...", "url": "https://discovery.ucl.ac.uk/id/eprint/10136651/", "isFamilyFriendly": true, "displayUrl": "https://discovery.ucl.ac.uk/id/eprint/10136651", "snippet": "UCL Discovery is UCL&#39;s open access repository, showcasing and providing access to UCL research outputs from all UCL disciplines.", "dateLastCrawled": "2021-11-05T21:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Interpretability: Cracking open the black</b> <b>box</b> \u2013 Part II \u2013 Deep &amp; Shallow", "url": "https://deep-and-shallow.com/2019/11/16/interpretability-cracking-open-the-black-box-part-ii/", "isFamilyFriendly": true, "displayUrl": "https://deep-and-shallow.com/2019/11/16/<b>interpretability-cracking-open-the-black</b>-<b>box</b>...", "snippet": "<b>Interpretability: Cracking open the black</b> <b>box</b> \u2013 Part II . In the last post in the series, we defined what <b>interpretability</b> is and looked at a few interpretable models and the quirks and \u2018gotchas\u2019 in it. Now let\u2019s dig deeper into the post-hoc interpretation techniques which is useful when you model itself is not transparent. This resonates with most real world use cases, because whether we <b>like</b> it or not, we get better performance with <b>a black</b> <b>box</b> model. Data Set. For this exercise, I ...", "dateLastCrawled": "2022-01-29T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Peeking</b> <b>Inside</b> the <b>Black</b>-<b>Box</b>: A Survey on Explainable Artificial ...", "url": "https://www.researchgate.net/publication/327709435_Peeking_Inside_the_Black-Box_A_Survey_on_Explainable_Artificial_Intelligence_XAI", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/327709435_<b>Peeking</b>_<b>Inside</b>_the_<b>Black</b>-<b>Box</b>_A...", "snippet": "A. ADADI et al: <b>Peeking</b> <b>inside</b> the <b>black</b>-<b>box</b>: A <b>survey on Explainable Artificial Intelligence</b> (XA I) VOLUME XX, 2018 3 remarkab le resurgence of XAI term research interest using", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "awesome_deep_learning_<b>interpretability</b>/sort_cite.md at master ...", "url": "https://github.com/oneTaken/awesome_deep_learning_interpretability/blob/master/sort_cite.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/oneTaken/awesome_deep_learning_<b>interpretability</b>/blob/master/sort...", "snippet": "Visual <b>interpretability</b> for deep learning: a survey: 140: 2016: arxiv: Understanding neural networks through representation erasure: 137: 2018: Access: <b>Peeking</b> <b>inside</b> the <b>black</b>-<b>box</b>: A survey on Explainable Artificial Intelligence (XAI) 131: 2016: arxiv", "dateLastCrawled": "2021-12-27T14:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Peeking</b> <b>inside</b> the <b>black</b>-<b>box</b>: Explainable machine learning applied to ...", "url": "https://www.sciencedirect.com/science/article/pii/S0198971521000545", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0198971521000545", "snippet": "This is because AI models are mostly of high complexity and low <b>interpretability</b>; in other words, they are <b>black</b>-<b>box</b> models. This study presents a case study of how model transparency and explanation can be generated using the Local Interpretable Model-Agnostic Explanation (LIME) to support advanced machine learning techniques in the transportation energy field. The methodology has been implemented based on the Household Travel Survey (HTS) data, which is used to train the artificial neural ...", "dateLastCrawled": "2021-11-29T09:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep-learning Models for ...", "url": "https://ui.adsabs.harvard.edu/abs/2021AJ....162..195Y/abstract", "isFamilyFriendly": true, "displayUrl": "https://ui.adsabs.harvard.edu/abs/2021AJ....162..195Y/abstract", "snippet": "Deep-learning algorithms are growing in popularity in the field of exoplanetary science due to their ability to model highly nonlinear relations and solve interesting problems in a data-driven manner. Several works have attempted to perform fast retrievals of atmospheric parameters with the use of machine-learning algorithms like deep neural networks (DNNs). Yet, despite their high predictive power, DNNs are also infamous for being &quot;<b>black</b> boxes.&quot; It is their apparent lack of explainability ...", "dateLastCrawled": "2021-11-14T05:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpretable Machine Learning and ...", "url": "https://eres.architexturez.net/doc/oai-eres-id-eres2021-104", "isFamilyFriendly": true, "displayUrl": "https://eres.architexturez.net/doc/oai-eres-id-eres2021-104", "snippet": "<b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpretable Machine Learning and Hedonic Rental Estimation To estimate real estate prices and rents, ML represents a promising extension to the hedonic literature since it is able to increase predictive accuracy and is more flexible than the standard regression-based hedonic approach in handling a variety of quantitative and qualitative inputs.", "dateLastCrawled": "2022-01-27T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep Learning Models for ...", "url": "https://ui.adsabs.harvard.edu/abs/2020EPSC...14...66H/abstract", "isFamilyFriendly": true, "displayUrl": "https://ui.adsabs.harvard.edu/abs/2020EPSC...14...66H/abstract", "snippet": "Deep learning algorithms are growing in popularity in the field of exoplanetary science due to their ability to model highly non-linear relations and solve interesting problems in a data-driven manner. Several works have attempted to perform fast retrieval of atmospheric parameters with the use of machine learning algorithms or deep neural networks (DNNs). Yet, despite their high predictive power, DNNs are also infamous for being `<b>black</b> boxes&quot;. It is their apparent lack of explainability ...", "dateLastCrawled": "2021-03-14T13:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Peeking</b> <b>Inside the Black Box: Techniques</b> for Making AI Models More ...", "url": "https://www.rtinsights.com/peeking-inside-the-black-box-techniques-for-making-ai-models-more-easily-interpretable/", "isFamilyFriendly": true, "displayUrl": "https://www.rtinsights.com/<b>peeking</b>-<b>inside-the-black-box-techniques</b>-for-making-ai...", "snippet": "<b>Peeking</b> <b>Inside the Black Box: Techniques</b> for Making <b>AI Models More Easily Interpretable</b> By Dan Capellupo | May 18, 2020 with 0 Comments. Tweet . Considering the increasing consequences of faulty AI, explainable AI is going to be a critical issue facing data science for many years. When training a machine learning or AI model, typically the main goal is to make the most accurate prediction possible. Data scientists and machine learning engineers will transform their data in myriad ways and ...", "dateLastCrawled": "2022-01-22T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Peeking</b> <b>Inside</b> the <b>Black</b> <b>Box</b>: Visualizing Statistical Learning With ...", "url": "https://www.jstor.org/stable/43304932", "isFamilyFriendly": true, "displayUrl": "https://www.jstor.org/stable/43304932", "snippet": "<b>interpretability</b> for improved predictive accuracy, any window into <b>black</b> <b>box</b>&#39;s internals can be beneficial. Authors have devised a variety of algorithm-specific techniques targeted at improving the <b>interpretability</b> of a particular statistical learning procedure&#39;s output. Rao and Potts (1997) offered a technique for visualizing the decision boundary produced by bagging decision trees. Although applicable to high-dimensional settings, their work primarily focuses on the low-dimensional case of ...", "dateLastCrawled": "2022-01-26T01:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Explainable AI: A Review of Machine Learning <b>Interpretability</b> Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "3.1.2. <b>Interpretability</b> Methods to Explain any <b>Black</b>-<b>Box</b> Model . This section focuses on <b>interpretability</b> techniques, which can be applied to any <b>black</b>-<b>box</b> model. First introduced in , the local interpretable model-agnostic explanations (LIME) method is one of the most popular <b>interpretability</b> methods for <b>black</b>-<b>box</b> models. Following a simple ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Peeking</b> <b>Inside</b> the <b>Black Box: Visualizing Statistical Learning</b> with ...", "url": "https://dm-gatech.github.io/CS8803-Fall2018-DML-Papers/ice.pdf", "isFamilyFriendly": true, "displayUrl": "https://dm-gatech.github.io/CS8803-Fall2018-DML-Papers/ice.pdf", "snippet": "<b>Peeking</b> <b>Inside</b> the <b>Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation</b> Alex Goldstein , Adam Kapelnery, Justin Bleich z, and Emil Pitkinx The Wharton School of the University of Pennsylvania March 21, 2014 Abstract This article presents Individual Conditional Expectation (ICE) plots, a tool for vi-sualizing the model estimated by any supervised learning algorithm. Classical partial dependence plots (PDPs) help visualize the average partial ...", "dateLastCrawled": "2021-12-10T15:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On the <b>interpretability</b> of machine learning-based model for predicting ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6664803/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6664803", "snippet": "<b>Interpretability</b> of <b>black</b>-<b>box</b> models via visualization has been used extensively [42 ... Due to the nature of this study and the unlimited availability of <b>similar</b> comparable cohorts. Generalizing the findings and explanations of this study would require the inclusion of multiple datasets representing multiple cohorts. Conclusion. Explaining the predictions of <b>black</b>-<b>box</b> machine learning models have become a crucial issue which is gaining increasing momentum. In particular, achieving optimal ...", "dateLastCrawled": "2022-01-22T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "XAI Methods \u2014 The Introduction. What are XAI methods? | by Kemal Erdem ...", "url": "https://towardsdatascience.com/xai-methods-the-introduction-5b1b81427c9c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/xai-methods-the-introduction-5b1b81427c9c", "snippet": "Figure 1: Taxonomy of the model <b>interpretability</b>. There are two major types of models: white-<b>box</b> models and <b>black</b>-<b>box</b> models. <b>Interpretability</b> of the first type is defined as the Intrinsic . This type of <b>interpretability</b> covers all models which have an interpretable internal structure. As an example, the structure of a decision tree is ...", "dateLastCrawled": "2022-02-03T01:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Peeking</b> <b>Inside</b> the <b>Black</b>-<b>Box</b>: A Survey on Explainable Artificial ...", "url": "https://www.researchgate.net/publication/327709435_Peeking_Inside_the_Black-Box_A_Survey_on_Explainable_Artificial_Intelligence_XAI", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/327709435_<b>Peeking</b>_<b>Inside</b>_the_<b>Black</b>-<b>Box</b>_A...", "snippet": "<b>Peeking</b> <b>Inside</b> the <b>Black</b>-<b>Box</b>: A <b>Survey on Explainable Artificial Intelligence (XAI</b>) ... <b>interpretability</b> could suffice, as the consequences of it . going wrong are negligible. On the other hand ...", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A peek <b>inside</b> the &#39;<b>Black</b> <b>Box</b>&#39; - interpreting neural networks | Patrick ...", "url": "https://www.paltmeyer.com/post/2021-02-01-a-peek-inside-the-black-box-interpreting-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.paltmeyer.com/post/2021-02-01-a-peek-<b>inside</b>-the-<b>black</b>-<b>box</b>-interpreting...", "snippet": "Interpretable DL - a whistle-stop tour. Before delving further into how the intrinsics of deep neural networks <b>can</b> be disentangled we should first clarify what <b>interpretability</b> in the context of algorithms actually means. Fan, Xiong, and Wang describes model <b>interpretability</b> simply as the extent to which humans <b>can</b> \u201cunderstand and reason\u201d the model. This may concern an understanding of both the ad-hoc workings of the algorithm as well as the post-hoc <b>interpretability</b> of its output. In ...", "dateLastCrawled": "2022-01-25T22:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "AI Generates Hypotheses Human Scientists Have Not <b>Thought</b> Of - Osce Master", "url": "https://oscemaster.com/ai-generates-hypotheses-human-scientists-have-not-thought-of/", "isFamilyFriendly": true, "displayUrl": "https://oscemaster.com/ai-generates-hypotheses-human-scientists-have-not-<b>thought</b>-of", "snippet": "Although <b>peeking</b> <b>inside</b> the <b>black</b> <b>box</b> <b>can</b> help humans construct novel scientific hypotheses, \u201cwe still have a long way to go,\u201d says Soumik Sarkar, an associate professor of mechanical engineering at Iowa State University. <b>Interpretability</b> techniques <b>can</b> hint at correlations that pop up in the machine-learning process, but they cannot prove causation or offer explanations. They still rely on subject matter experts to derive meaning from the network. Machine learning also often uses data ...", "dateLastCrawled": "2022-01-15T05:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Peeking</b> <b>Inside</b> The <b>Black</b>-<b>Box</b> A Survey On Explainable Artificial ...", "url": "https://www.scribd.com/document/473613935/Peeking-Inside-the-Black-Box-A-Survey-on-Explainable-Artificial-Intelligence-XAI", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/473613935/<b>Peeking</b>-<b>Inside</b>-the-<b>Black</b>-<b>Box</b>-A-Survey-on...", "snippet": "Adadi, M. Berrada: <b>Peeking</b> <b>Inside</b> the <b>Black</b>-<b>Box</b>: Survey on XAI. Explainable Artificial Intelligence [7], XCI 2017 on Explain- and explainability. On the other hand, Cognilytica has exam- able Computational Intelligence [8] and IJCNN 2017 on ined in its \u2018\u2018AI Positioning Matrix\u2019\u2019 (CAPM) the market of Explainability of Learning Machines [9]. This year (2018) is AI products. It proposed a chart where XAI technologies flourishing by a wide range of dedicated workshops to the are arguably ...", "dateLastCrawled": "2021-12-30T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "AI Generates Hypotheses Human Scientists Have Not <b>Thought</b> Of ...", "url": "https://www.scientificamerican.com/article/ai-generates-hypotheses-human-scientists-have-not-thought-of/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.scientificamerican.com</b>/article/ai-generates-hypotheses-human-scientists...", "snippet": "Although <b>peeking</b> <b>inside</b> the <b>black</b> <b>box</b> <b>can</b> help humans construct novel scientific hypotheses, \u201cwe still have a long way to go,\u201d says Soumik Sarkar, an associate professor of mechanical ...", "dateLastCrawled": "2022-01-28T22:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Frontiers | <b>Illuminating the Black Box: Interpreting</b> Deep Neural ...", "url": "https://www.frontiersin.org/articles/10.3389/fpsyt.2020.551299/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyt.2020.551299", "snippet": "In our view, the question of <b>interpretability</b> <b>can</b> be approached from two different perspectives: (1) the perspective of science, in which a precise, formulated definition is required, and (2) the perspective of the interpreter, which arises from the psychological need to construct meaning out of things. Although most previous works discuss the issue from the former 7\u201314), we begin our discussion with the latter. In this paper, we define <b>interpretability</b> as the capability of a subject ...", "dateLastCrawled": "2022-02-03T17:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Unwrapping The <b>Black</b> <b>Box</b> of Deep ReLU Networks: <b>Interpretability</b> ...", "url": "https://www.arxiv-vanity.com/papers/2011.04041/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2011.04041", "snippet": "Unwrapping The <b>Black</b> <b>Box</b> of Deep ReLU Networks: <b>Interpretability</b>, Diagnostics, and Simplification. Agus Sudjianto 1, William Knauth 2, Rahul Singh 1 Zebin Yang 3, and Aijun Zhang 3, \u2217. 1 Wells Fargo, 2 Carnegie Mellon University, 3 The University of Hong Kong. Abstract. The deep neural networks (DNNs) have achieved great success in learning complex patterns with strong predictive power, but they are often <b>thought</b> of as \u201c<b>black</b> <b>box</b>\u201d models without a sufficient level of transparency and ...", "dateLastCrawled": "2022-01-17T15:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Foundations of Ethical AI: Concepts and Principles of Explainability ...", "url": "https://www.niceactimize.com/blog/foundations-of-ethical-ai-concepts-and-principles-of-explainability-and-trust/", "isFamilyFriendly": true, "displayUrl": "https://www.niceactimize.com/blog/foundations-of-ethical-ai-concepts-and-principles-of...", "snippet": "However, the <b>black</b>-<b>box</b> nature of these systems means it isn\u2019t straightforward for business users to understand the logic behind the decision. FSOs are leveraging data and artificial intelligence to create scalable and automated solutions \u2014 but at the same time they are also scaling their reputational, regulatory and legal risks. AI stimulates unprecedented business gains, but along the way, government and industry leaders have to address a variety of ethical challenges and dilemmas such ...", "dateLastCrawled": "2022-01-31T16:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A mental models approach for defining explainable artificial ...", "url": "https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-021-01703-7", "isFamilyFriendly": true, "displayUrl": "https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-021-01703-7", "snippet": "Wide-ranging concerns exist regarding the use of <b>black</b>-<b>box</b> modelling methods in sensitive contexts such as healthcare. Despite performance gains and hype, uptake of artificial intelligence (AI) is hindered by these concerns. Explainable AI is <b>thought</b> to help alleviate these concerns. However, existing definitions for explainable are not forming a solid foundation for this work. We critique recent reviews on the literature regarding: the agency of an AI within a team; mental models ...", "dateLastCrawled": "2022-01-30T12:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Explainability in human\u2013agent systems</b> | SpringerLink", "url": "https://link.springer.com/article/10.1007/s10458-019-09408-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-019-09408-y", "snippet": "The Random Forest <b>can</b> be considered <b>a black</b> <b>box</b> that determines the class of a given feature set. \\({\\mathcal {L}}\\) \u2019s interpretabity is obtained by determining how the different features contribute to the classification of a feature set , or even which features should be changed, and how, in order to obtain a different classification . This type of interpretation is extremely valuable. For example, consider a set of medical features, such as weight, blood pressure, age etc. and a model ...", "dateLastCrawled": "2022-01-25T01:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Peeking</b> <b>inside</b> the <b>black</b>-<b>box</b>: Explainable machine learning applied to ...", "url": "https://www.sciencedirect.com/science/article/pii/S0198971521000545", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0198971521000545", "snippet": "This is because AI models are mostly of high complexity and low <b>interpretability</b>; in other words, they are <b>black</b>-<b>box</b> models. This study presents a case study of how model transparency and explanation <b>can</b> be generated using the Local Interpretable Model-Agnostic Explanation (LIME) to support advanced machine learning techniques in the transportation energy field. The methodology has been implemented based on the Household Travel Survey (HTS) data, which is used to train the artificial neural ...", "dateLastCrawled": "2021-11-29T09:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "On the <b>interpretability</b> of machine learning-based model for predicting ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6664803/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6664803", "snippet": "However, in practice, even though <b>black</b>-<b>box</b> models such as Neural Networks <b>can</b> achieve better performance than white-<b>box</b> models (e.g. linear regression, decision tree), they are less interpretable. In general, methods for machine learning <b>interpretability</b> <b>can</b> be classified as either Model-Specific or Model-Agnostic. In principle, model-specific ...", "dateLastCrawled": "2022-01-22T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Explainable AI: A Review of Machine Learning <b>Interpretability</b> Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "3.1.2. <b>Interpretability</b> Methods to Explain any <b>Black</b>-<b>Box</b> Model . This section focuses on <b>interpretability</b> techniques, which <b>can</b> be applied to any <b>black</b>-<b>box</b> model. First introduced in , the local interpretable model-agnostic explanations (LIME) method is one of the most popular <b>interpretability</b> methods for <b>black</b>-<b>box</b> models. Following a simple ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Guidance - <b>interpretability evaluation</b> - Assuring Autonomy ...", "url": "https://www.york.ac.uk/assuring-autonomy/guidance/body-of-knowledge/implementation/2-8/interpretability-evaluation/", "isFamilyFriendly": true, "displayUrl": "https://www.york.ac.uk/.../implementation/2-8/<b>interpretability-evaluation</b>", "snippet": "Between different types of <b>interpretability</b>; Between <b>interpretability</b> and accuracy; Between the fidelity of explanations and their comprehensibility; References [1] Amina Adadi and Mohammed Berrada. \u201c<b>Peeking</b> <b>Inside</b> the <b>Black</b>-<b>Box</b>: A Survey on Explainable Artificial Intelligence (XAI)\u201d. In: IEEE (2018). [2] Finale Doshi-Velez and Been Kim ...", "dateLastCrawled": "2022-01-05T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep Learning Models for ...", "url": "https://deepai.org/publication/peeking-inside-the-black-box-interpreting-deep-learning-models-for-exoplanet-atmospheric-retrievals", "isFamilyFriendly": true, "displayUrl": "https://<b>deepai</b>.org/publication/<b>peeking</b>-<b>inside</b>-the-<b>black</b>-<b>box</b>-interpreting-deep-learning...", "snippet": "<b>Peeking</b> <b>inside</b> the <b>Black</b> <b>Box</b>: Interpreting Deep <b>Learning Models for Exoplanet Atmospheric Retrievals</b> . 11/23/2020 . \u2219. by Kai Hou Yip, et al. \u2219. 16 \u2219. share Deep learning algorithms are growing in popularity in the field of exoplanetary science due to their ability to model highly non-linear relations and solve interesting problems in a data-driven manner. Several works have attempted to perform fast retrievals of atmospheric parameters with the use of machine learning algorithms like ...", "dateLastCrawled": "2021-12-24T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "XAI Methods for Neural Time Series Classification: A Brief Review | DeepAI", "url": "https://deepai.org/publication/xai-methods-for-neural-time-series-classification-a-brief-review", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/xai-methods-for-neural-time-series-classification-a...", "snippet": "Arnout et al. <b>compared</b> multiple existing <b>interpretability</b> methods, namely saliency maps Simonyan et al. , Layer ... <b>Peeking</b> <b>inside</b> the <b>black</b>-<b>box</b>: a survey on explainable artificial intelligence (xai). IEEE Access 6, pp. 52138\u201352160. Cited by: \u00a74. H. Arnout, M. El-Assady, D. Oelke, and D. A. Keim (2019) Towards a rigorous evaluation of xai methods on time series. In . 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pp. 4197\u20134201. Cited by: \u00a74, \u00a75.1.2, \u00a75.3 ...", "dateLastCrawled": "2022-01-26T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "XAI Methods \u2014 The Introduction. What are XAI methods? | by Kemal Erdem ...", "url": "https://towardsdatascience.com/xai-methods-the-introduction-5b1b81427c9c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/xai-methods-the-introduction-5b1b81427c9c", "snippet": "The structure of the <b>interpretability</b> <b>can</b> be defined in many ways (either by the purpose, by the method, or by the application). Model-Agnostic and Model-Specific . As shown in the taxonomy (see Fig. 1), post-hoc <b>interpretability</b> is divided into model-agnostic and model-specific . The model-agnostic methods are the methods that <b>can</b> be applied to any <b>black</b>-<b>box</b> model without the concern about the internal structure of the model. These methods are usually less precise but because they explain ...", "dateLastCrawled": "2022-02-03T01:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Interpretability: Cracking open the black</b> <b>box</b> \u2013 Part II \u2013 Deep &amp; Shallow", "url": "https://deep-and-shallow.com/2019/11/16/interpretability-cracking-open-the-black-box-part-ii/", "isFamilyFriendly": true, "displayUrl": "https://deep-and-shallow.com/2019/11/16/<b>interpretability-cracking-open-the-black</b>-<b>box</b>...", "snippet": "<b>Interpretability: Cracking open the black</b> <b>box</b> \u2013 Part II . In the last post in the series, we defined what <b>interpretability</b> is and looked at a few interpretable models and the quirks and \u2018gotchas\u2019 in it. Now let\u2019s dig deeper into the post-hoc interpretation techniques which is useful when you model itself is not transparent. This resonates with most real world use cases, because whether we like it or not, we get better performance with <b>a black</b> <b>box</b> model. Data Set. For this exercise, I ...", "dateLastCrawled": "2022-01-29T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Unwrapping The <b>Black</b> <b>Box</b> of Deep ReLU Networks: <b>Interpretability</b> ...", "url": "https://www.researchgate.net/publication/345654541_Unwrapping_The_Black_Box_of_Deep_ReLU_Networks_Interpretability_Diagnostics_and_Simplification", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/345654541_Unwrapping_The_<b>Black</b>_<b>Box</b>_of_Deep...", "snippet": "The purported &quot;<b>black</b> <b>box</b>&quot;&#39; nature of neural networks is a barrier to adoption in applications where <b>interpretability</b> is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a ...", "dateLastCrawled": "2022-02-02T01:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Opening the <b>black</b> <b>box</b> of <b>artificial intelligence</b> for clinical decision ...", "url": "https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0231166", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231166", "snippet": "Such methods <b>can</b> be tailored to one specific original <b>black</b> <b>box</b> algorithm, or <b>can</b> be generalized like the LIME algorithm . We would like to stress that no standardization of these terms currently exists. Thus, in the presented work, explainability is mainly examined from a clinical point-of-view, highlighting the ability of humans to understand which clinical features drive the prediction. This is important, as a major goal of clinical predictive modeling is the development of clinical ...", "dateLastCrawled": "2021-08-27T17:42:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretability</b> in <b>Machine</b> <b>Learning</b>: An Overview", "url": "https://thegradient.pub/interpretability-in-ml-a-broad-overview/", "isFamilyFriendly": true, "displayUrl": "https://thegradient.pub/<b>interpretability</b>-in-ml-a-broad-overview", "snippet": "First, <b>interpretability</b> in <b>machine</b> <b>learning</b> is useful because it can aid in trust. As humans, we may be reluctant to rely on <b>machine</b> <b>learning</b> models for certain critical tasks, e.g., medical diagnosis, unless we know &quot;how they work.&quot; There&#39;s often a fear of the unknown when trusting in something opaque, which we see when people confront new technology, and this can slow down adoption. Approaches to <b>interpretability</b> that focus on transparency could help mitigate some of these fears.", "dateLastCrawled": "2022-02-01T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "6 \u2013 <b>Interpretability</b> \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "Figure 1: <b>Interpretability</b> for <b>machine</b> <b>learning</b> models bridges the concrete objectives models optimize for and the real-world (and less easy to define) desiderata that ML applications aim to achieve. Introduction . The objectives <b>machine</b> <b>learning</b> models optimize for do not always reflect the actual desiderata of the task at hand. <b>Interpretability</b> in models allows us to evaluate their decisions and obtain information that the objective alone cannot confer. <b>Interpretability</b> takes many forms ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interpretability</b> in <b>Machine</b> <b>Learning</b>", "url": "https://www.cl.cam.ac.uk/teaching/1819/P230/IWML-Lecture-4.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cl.cam.ac.uk/teaching/1819/P230/IWML-Lecture-4.pdf", "snippet": "This provides a novel <b>analogy</b> between data compression and regularization. Qualitative and quantitative state-of-the-art results on three datasets. 20 / 33. Interpretable Lens Variable Model (ILVM) 21 / 33. Interactive <b>Interpretability</b> via Active <b>Learning</b> Interactive \u2018human-in-the-loop\u2019 <b>interpretability</b> Choose the point with index j that maximizes : ^j = argmax jI(s ; ) = H(s ) E q \u02da(z js)[H(s jz j)] = Z p(s j)log p(s j)ds + E q \u02da(z js) Z p (s jjz)log p (s jjz)ds : (5) Choose the point ...", "dateLastCrawled": "2022-01-19T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Towards <b>Analogy</b>-Based Explanations in <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/chapter/10.1007/978-3-030-57524-3_17", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-57524-3_17", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-16T03:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Confusion Matrices</b> &amp; <b>Interpretable ML</b> | by andrea b | high stakes ...", "url": "https://medium.com/high-stakes-design/interpretability-techniques-explained-in-simple-terms-f5e1573674f3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/high-stakes-design/<b>interpretability</b>-techniques-explained-in-simple...", "snippet": "The best [<b>analogy</b>] I can think of is an indicator light in your car \u2014 [and the] <b>machine</b> that you plug in to tell you more about the readout. ANDREA: Do you see <b>interpretability</b>, primarily, as ...", "dateLastCrawled": "2021-03-22T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Economic Methodology Meets Interpretable Machine Learning</b> - Part I ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes", "snippet": "In this series of posts, we will develop an <b>analogy</b> between the realistic assumptions debate in economic methodology and the current discussion over <b>interpretability</b> when using <b>machine</b> <b>learning</b> models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for <b>machine</b> <b>learning</b> theorists and practitioners to avoid analysis paralysis and make progress on the <b>interpretability</b> issue - one way or the other. Intro - Part ...", "dateLastCrawled": "2022-01-22T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Economic Methodology Meets Interpretable <b>Machine</b> <b>Learning</b> ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_intro/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_intro", "snippet": "In this series of posts, we will develop an <b>analogy</b> between the realistic assumptions debate in economic methodology and the current discussion over <b>interpretability</b> when using <b>machine</b> <b>learning</b> models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for <b>machine</b> <b>learning</b> theorists and practitioners to avoid analysis paralysis and make progress on the <b>interpretability</b> issue - one way or the other. But first ...", "dateLastCrawled": "2022-01-05T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Analogies between Biology and Deep <b>Learning</b> [rough note] -- colah&#39;s blog", "url": "https://colah.github.io/notes/bio-analogies/", "isFamilyFriendly": true, "displayUrl": "https://colah.github.io/notes/bio-analogies", "snippet": "Neuroscience \u2194 <b>Interpretability</b>. <b>Analogy</b>: model=brain. Artificial neural networks are historically inspired by neuroscience, but I used to be pretty skeptical that the connection was anything more than superficial. I&#39;ve since come around: I now think this is a very deep connection. The thing that personally persuaded me was that, in my own investigations of what goes on inside neural networks, we kept finding things that were previously discovered by neuroscientists. The most recent ...", "dateLastCrawled": "2022-01-21T13:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "9.6 <b>SHAP</b> (SHapley Additive exPlanations) | Interpretable <b>Machine</b> <b>Learning</b>", "url": "https://christophm.github.io/interpretable-ml-book/shap.html", "isFamilyFriendly": true, "displayUrl": "https://christophm.github.io/interpretable-ml-book/<b>shap</b>.html", "snippet": "9.6 <b>SHAP</b> (SHapley Additive exPlanations). This chapter is currently only available in this web version. ebook and print will follow. <b>SHAP</b> (SHapley Additive exPlanations) by Lundberg and Lee (2017) 69 is a method to explain individual predictions. <b>SHAP</b> is based on the game theoretically optimal Shapley values.. There are two reasons why <b>SHAP</b> got its own chapter and is not a subchapter of Shapley values.First, the <b>SHAP</b> authors proposed KernelSHAP, an alternative, kernel-based estimation ...", "dateLastCrawled": "2022-02-03T01:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Chris Olah on what the hell is going on inside neural networks - 80,000 ...", "url": "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/", "isFamilyFriendly": true, "displayUrl": "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research", "snippet": "Chris is a <b>machine</b> <b>learning</b> researcher currently focused on neural network interpretability. Until last December he led OpenAI\u2019s interpretability team but along with some colleagues he recently moved on to help start a new AI lab focussed on large models and safety called Anthropic. Rob Wiblin: Before OpenAI he spent 4 years at Google Brain developing tools to visualize what\u2019s going on in neural networks. Chris was hugely impactful at Google Brain. He was second author on the launch of ...", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Marketing AI: Interpretability and Explainability</b> - Christopher S. Penn ...", "url": "https://www.christopherspenn.com/2021/03/marketing-ai-interpretability-and-explainability/", "isFamilyFriendly": true, "displayUrl": "https://www.christopherspenn.com/2021/03/<b>marketing-ai-interpretability-and-explainability</b>", "snippet": "<b>Interpretability is like</b> inspecting the baker\u2019s recipe for the cake. We look at the list of ingredients and the steps taken to bake the cake, and we verify that the recipe makes sense and the ingredients were good. This is a much more rigorous way of validating our results, but it\u2019s the most complete \u2013 and if we\u2019re in a high-stakes situation where we need to remove all doubt, this is the approach we take. Interpretability in AI is like that \u2013 we step through the code itself that ...", "dateLastCrawled": "2022-01-29T12:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Causal <b>Learning</b> From Predictive Modeling for Observational Data", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7931928/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7931928", "snippet": "Given the recent success of <b>machine</b> <b>learning</b>, specifically deep <b>learning</b>, in several applications (Goodfellow et al., ... This statistical <b>interpretability is similar</b> in spirit to traditional interpretability. This allows to answer questions, such as \u201cdoes BMI influence susceptibility to Covid?\u201d Moreover, it has been argued that developing an effective CBN for practical applications requires expert knowledge when data collection is cumbersome (Fenton and Neil, 2012). This applies to ...", "dateLastCrawled": "2021-12-09T23:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Optimal <b>Predictive Clustering</b> - Dimitris Bertsimas", "url": "https://dbertsim.mit.edu/pdfs/papers/2020-sobiesk-optimal-predictive-clustering.pdf", "isFamilyFriendly": true, "displayUrl": "https://dbertsim.mit.edu/pdfs/papers/2020-sobiesk-optimal-<b>predictive-clustering</b>.pdf", "snippet": "Table 1 Comparison of major <b>machine</b> <b>learning</b> methods relative to each other across the metrics of performance (out-of-sample R2), scalability and interpretability. 1 is the best, while 5 is the worst. Optimal <b>Predictive Clustering</b> 3 From Table 1, we observe all existing methods have weakness in at least one category. We therefore seek to design a method that has strong performance in all three categories at the same time. Optimal <b>Predictive Clustering</b> (OPC) is an algorithm that uses mixed ...", "dateLastCrawled": "2021-11-24T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reviewing Challenges of Predicting Protein Melting Temperature Change ...", "url": "https://link.springer.com/article/10.1007/s12033-021-00349-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12033-021-00349-0", "snippet": "Predicting the effects of mutations on protein stability is a key problem in fundamental and applied biology, still unsolved even for the relatively simple case of small, soluble, globular, monomeric, two-state-folder proteins. Many articles discuss the limitations of prediction methods and of the datasets used to train them, which result in low reliability for actual applications despite globally capturing trends. Here, we review these and other issues by analyzing one of the most detailed ...", "dateLastCrawled": "2022-02-03T02:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretable Machine Learning: Advantages and Disadvantages</b> | by ...", "url": "https://towardsdatascience.com/interpretable-machine-learning-advantages-and-disadvantages-901769f48c43", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpretable-machine-learning-advantages-and</b>...", "snippet": "In my view, a shor t coming of interpretable <b>machine</b> <b>learning</b> is that it assumes to a degree that the data being fed into the model is always going to be suitable for human interpretation. This is not necessarily the case. For instance, let\u2019s say that a company is trying to implement interpretable <b>machine</b> <b>learning</b> to devise a credit scoring model, whereby prospective credit card applications are classified as approved or rejected based on numerous features. It is often the case that such ...", "dateLastCrawled": "2022-01-19T03:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Breaking the interpretability barrier - a</b> method for interpreting deep ...", "url": "http://www.di.uniba.it/~loglisci/NFMCP2019/NFMCP/nfMCP2019_paper_17.pdf", "isFamilyFriendly": true, "displayUrl": "www.di.uniba.it/~loglisci/NFMCP2019/NFMCP/nfMCP2019_paper_17.pdf", "snippet": "Last, but not least, <b>interpretability can be thought of as</b> a useful tool for understanding and correcting model errors. In general, we are faced with a trade-o between performance and inter-pretability. Graph classi cation is normally a domain which requires the ap-plication of complex <b>learning</b> models, such as deep neural networks, which are not interpretable by nature. Several relevant attempts have been made to in-terpret complex models post-hoc (brie y reviewed in section2). However, most ...", "dateLastCrawled": "2021-09-22T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Discovering Discriminative Nodes for Classi\ufb01cation with Deep Graph ...", "url": "http://muresanlab.tins.ro/publications/preprints/Palcu_et_al_LNAI_2020.pdf", "isFamilyFriendly": true, "displayUrl": "muresanlab.tins.ro/publications/preprints/Palcu_et_al_LNAI_2020.pdf", "snippet": "Last, but not least, <b>interpretability can be thought of as</b> a useful tool for understanding and correcting model errors. In general, we are faced with a trade-o\ufb00 between performance and inter-pretability. Graph classi\ufb01cation is normally a domain which requires the appli-cation of complex <b>learning</b> models, such as deep neural networks, which are not interpretable by nature. Several relevant attempts have been made to interpret complex models post-hoc (brie\ufb02y reviewed in Sect.2). However ...", "dateLastCrawled": "2021-09-02T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Confronting Abusive Language Online: A Survey from the Ethical and ...", "url": "https://www.arxiv-vanity.com/papers/2012.12305/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2012.12305", "snippet": "The pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. Significant effort in Natural Language Processing (NLP) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. Although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this ...", "dateLastCrawled": "2021-10-13T19:21:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(interpretability)  is like +(peeking inside a black box)", "+(interpretability) is similar to +(peeking inside a black box)", "+(interpretability) can be thought of as +(peeking inside a black box)", "+(interpretability) can be compared to +(peeking inside a black box)", "machine learning +(interpretability AND analogy)", "machine learning +(\"interpretability is like\")", "machine learning +(\"interpretability is similar\")", "machine learning +(\"just as interpretability\")", "machine learning +(\"interpretability can be thought of as\")", "machine learning +(\"interpretability can be compared to\")"]}