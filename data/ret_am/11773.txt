{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to use <b>categorical / multiclass hinge with TensorFlow</b> 2 and Keras ...", "url": "https://www.machinecurve.com/index.php/2019/10/17/how-to-use-categorical-multiclass-hinge-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/.../10/17/how-to-use-categorical-multiclass-<b>hinge</b>-with-keras", "snippet": "From binary <b>hinge</b> to multiclass <b>hinge</b>. In that previous blog, we looked at <b>hinge</b> <b>loss</b> and squared <b>hinge</b> <b>loss</b> \u2013 which actually helped us to generate <b>a decision</b> <b>boundary</b> <b>between</b> <b>two</b> <b>classes</b> and hence a classifier, but yep \u2013 <b>two</b> <b>classes</b> only.. <b>Hinge</b> <b>loss</b> and squared <b>hinge</b> <b>loss</b> can be used for binary classification problems.. Unfortunately, many of today\u2019s problems aren\u2019t binary, but rather, multiclass: the number of possible target <b>classes</b> is \\(&gt; 2\\).", "dateLastCrawled": "2022-01-29T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss Function</b>(Part III): Support Vector Machine - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-iii-5dff33fa015d", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/optimization-<b>loss-function</b>-under-the-hood-part-iii-5dff...", "snippet": "For example, in the plot on the left as below, the ideal <b>decision</b> <b>boundary</b> should be <b>like</b> green line, by adding the orange orange triangle (outlier), with a vey big C, the <b>decision</b> <b>boundary</b> will shift to the orange line to satisfy the the rule of large margin. On the other hand, C also plays a role to adjust the width of margin which enables margin violation. See the plot below on the right. When C is small, the margin is wider shown as green line. The pink <b>data</b> points have violated the ...", "dateLastCrawled": "2022-02-02T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "machine learning - What are the impacts of choosing different <b>loss</b> ...", "url": "https://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing-different-loss-functions-in-classification-to-a", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing...", "snippet": "However one important property of <b>hinge</b> <b>loss</b> is, <b>data</b> points far away from the <b>decision</b> <b>boundary</b> contribute nothing to the <b>loss</b>, the solution will be the same with those points removed. The remaining points are called support vectors in the context of SVM. Whereas SVM uses a regularizer term to ensure the maximum margin property and a unique solution. Share. Cite. Improve this answer. Follow edited Jul 29 &#39;16 at 1:52. answered Jul 27 &#39;16 at 9:46. dontloo dontloo. 13.4k 7 7 gold badges 49 49 ...", "dateLastCrawled": "2022-01-22T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Function For Multiclass Classification - XpCourse", "url": "https://www.xpcourse.com/loss-function-for-multiclass-classification", "isFamilyFriendly": true, "displayUrl": "https://www.xpcourse.com/<b>loss</b>-function-for-multiclass-classification", "snippet": "From binary <b>hinge</b> to multiclass <b>hinge</b>. In that previous blog, we looked at <b>hinge</b> <b>loss</b> and squared <b>hinge</b> <b>loss</b> - which actually helped us to generate <b>a decision</b> <b>boundary</b> <b>between</b> <b>two</b> <b>classes</b> and hence a classifier, but yep - <b>two</b> <b>classes</b> only.. <b>Hinge</b> <b>loss</b> and squared <b>hinge</b> <b>loss</b> can be used for binary classification problems.. Unfortunately, many of ...", "dateLastCrawled": "2022-01-03T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "10-701/15-781 Machine Learning - Midterm Exam, Fall 2010", "url": "https://cs.cmu.edu/~aarti/Class/10701/exams/midterm2010f_sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.cmu.edu/~aarti/Class/10701/exams/midterm2010f_sol.pdf", "snippet": "The functional form of f\u2019s <b>decision</b> <b>boundary</b> is the same as h\u2019s, but with di erent parameters. (e.g., if hwas a linear classi er, then fis also a linear classi er). False: For example, the functional form of <b>a decision</b> stump is a single axis-aligned split of the input space, but the functional form of the boosted classi\ufb01er is linear combinations of <b>decision</b> stumps which can form a more complex (piecewise linear) <b>decision</b> <b>boundary</b>. 2. 8. The depth of a learned <b>decision</b> tree can be ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Binary &amp; categorical crossentropy <b>loss</b> with TensorFlow 2 and Keras ...", "url": "https://www.machinecurve.com/index.php/2019/10/22/how-to-use-binary-categorical-crossentropy-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinecurve</b>.com/index.php/2019/10/22/how-to-use-binary-categorical-cross...", "snippet": "As you can see, with binary crossentropy, the Keras model has learnt to generate <b>a decision</b> <b>boundary</b> that allows us to distinguish <b>between</b> both <b>classes</b> accurately. This is unsurprising, since we allowed the circles to be very well separable, and this is represented in model history: \u2026 when 30 epochs passed, the model was still improving, also when tested with validation <b>data</b>. Hence, it was not overfitting yet \u2013 unsurprising again given the separability of our circles. This was once again ...", "dateLastCrawled": "2022-02-02T01:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Machine Learning Algorithms from Start to Finish ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/machine-learning-algorithms-from-start-to-finish-in...", "snippet": "The third hyperplane, H3, represents the <b>decision</b> <b>boundary</b> of the SVM classifier; this line not only separates the <b>two</b> <b>classes</b> but also keeps the widest distance <b>between</b> the most extreme points of the <b>two</b> <b>classes</b>. You can think of the SVM as fitting the widest possible margin <b>between</b> the <b>two</b> <b>classes</b>. This is known as large margin classification. Large Margin Classification. Photo by Wikipedia. <b>Like</b> I said, a large margin SVM Classifier essentially tries to fit the widest possible street ...", "dateLastCrawled": "2022-02-03T14:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Data Science- CSE 160</b> Flashcards | Quizlet", "url": "https://quizlet.com/325386741/data-science-cse-160-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/325386741/<b>data-science-cse-160</b>-flash-cards", "snippet": "Named <b>hinge</b> <b>loss</b> because it looks <b>like</b> a <b>hinge</b> (see diagram in notes) lists in R. A more general ordered collection of items is a list \u2022 Unlike a vector, the items in a list can be of different types (or modes, which is the term used in R) Logistic Regression. For logistic regression the model produces a numeric estimate However, the values of the target variable in the <b>data</b> are categorical The linear model corresponds to the log-odds (log(probability of scenario happening/probability of ...", "dateLastCrawled": "2021-03-27T20:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Support Vector Machine Algorithm - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/support-vector-machine-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/support-vector-machine-algorithm", "snippet": "One reasonable choice as the best hyperplane is the one that represents the largest separation or margin <b>between</b> the <b>two</b> <b>classes</b>. So we choose the hyperplane whose distance from it to the nearest <b>data</b> point on each side is maximized. If such a hyperplane exists it is known as the maximum-margin hyperplane/hard margin. So from the above figure, we choose L2. Let\u2019s consider a scenario <b>like</b> shown below. Here we have one blue ball in the <b>boundary</b> of the red ball. So how does SVM classify the ...", "dateLastCrawled": "2022-02-02T05:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning Interview Questions</b> (2022) - InterviewBit", "url": "https://www.interviewbit.com/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.interviewbit.com/<b>machine-learning-interview-questions</b>", "snippet": "Suppose we have given some <b>data</b> points that each belong to one of <b>two</b> <b>classes</b>, and the goal is to separate <b>two</b> <b>classes</b> based on a set of examples. In SVM, a <b>data</b> point is viewed as a p-dimensional vector (a list of p numbers), and we wanted to know whether we can separate such points with a (p-1)-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that classify the <b>data</b>. To choose the best hyperplane that represents the largest separation or margin <b>between</b> ...", "dateLastCrawled": "2022-02-03T00:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to use <b>categorical / multiclass hinge with TensorFlow</b> 2 and Keras ...", "url": "https://www.machinecurve.com/index.php/2019/10/17/how-to-use-categorical-multiclass-hinge-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/.../10/17/how-to-use-categorical-multiclass-<b>hinge</b>-with-keras", "snippet": "From binary <b>hinge</b> to multiclass <b>hinge</b>. In that previous blog, we looked at <b>hinge</b> <b>loss</b> and squared <b>hinge</b> <b>loss</b> \u2013 which actually helped us to generate <b>a decision</b> <b>boundary</b> <b>between</b> <b>two</b> <b>classes</b> and hence a classifier, but yep \u2013 <b>two</b> <b>classes</b> only.. <b>Hinge</b> <b>loss</b> and squared <b>hinge</b> <b>loss</b> can be used for binary classification problems.. Unfortunately, many of today\u2019s problems aren\u2019t binary, but rather, multiclass: the number of possible target <b>classes</b> is \\(&gt; 2\\).", "dateLastCrawled": "2022-01-29T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss Function</b>(Part III): Support Vector Machine - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-iii-5dff33fa015d", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/optimization-<b>loss-function</b>-under-the-hood-part-iii-5dff...", "snippet": "When <b>data</b> points are just right on the margin, \u03b8\u1d40x = 1, when <b>data</b> points are <b>between</b> <b>decision</b> <b>boundary</b> and margin, 0&lt; \u03b8\u1d40x &lt;1. I will explain why some <b>data</b> points appear inside of margin later. As for why removing non-support vectors won\u2019t affect model performance, we are able to answer it now. Remember model fitting process is to minimize the cost function. Since there is no cost for non-support vectors at all, the total value of cost function won\u2019t be changed by adding or removing ...", "dateLastCrawled": "2022-02-02T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Binary &amp; categorical crossentropy <b>loss</b> with TensorFlow 2 and Keras ...", "url": "https://www.machinecurve.com/index.php/2019/10/22/how-to-use-binary-categorical-crossentropy-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinecurve</b>.com/index.php/2019/10/22/how-to-use-binary-categorical-cross...", "snippet": "As you can see, with binary crossentropy, the Keras model has learnt to generate <b>a decision</b> <b>boundary</b> that allows us to distinguish <b>between</b> both <b>classes</b> accurately. This is unsurprising, since we allowed the circles to be very well separable, and this is represented in model history: \u2026 when 30 epochs passed, the model was still improving, also when tested with validation <b>data</b>. Hence, it was not overfitting yet \u2013 unsurprising again given the separability of our circles. This was once again ...", "dateLastCrawled": "2022-02-02T01:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Classification \u2013 Min Liang&#39;s blog", "url": "https://liangminblog.wordpress.com/2017/02/21/some-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://liangminblog.wordpress.com/2017/02/21/some-machine-learning", "snippet": "The <b>decision</b> <b>boundary</b> is a margin, which is defined by the support vectors, ... <b>hinge</b> <b>loss</b> \u2013 unboundedness of the convex <b>loss</b> causes the sensitivity to outliers; <b>decision</b> <b>boundary</b> only depends on support vectors. Pros: Accuracy, Works well on smaller cleaner datasets, It can be more efficient because it uses a subset of training points, more features (help build feature space, prevent overfitting by kernel) Less effective on noisier datasets with overlapping <b>classes</b>, use small number of ...", "dateLastCrawled": "2022-01-31T15:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "128 <b>Data</b> science terms from A-Z: The updated glossary of Machine ...", "url": "https://medium.com/@actiondatas/128-data-science-terms-from-a-z-the-updated-glossary-of-machine-learning-definitions-75bb5b7d08d2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@action<b>data</b>s/128-<b>data</b>-science-terms-from-a-z-the-updated-g<b>loss</b>ary...", "snippet": "A <b>hinge</b> <b>loss</b> is a <b>loss</b> function designed for classification models, to find the <b>decision</b> <b>boundary</b> as far as possible from each training example. A <b>hinge</b> <b>loss</b> function maximizes the margin <b>between</b> ...", "dateLastCrawled": "2021-08-29T03:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "10-701/15-781 Machine Learning - Midterm Exam, Fall 2010", "url": "https://cs.cmu.edu/~aarti/Class/10701/exams/midterm2010f_sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.cmu.edu/~aarti/Class/10701/exams/midterm2010f_sol.pdf", "snippet": "The functional form of f\u2019s <b>decision</b> <b>boundary</b> is the same as h\u2019s, but with di erent parameters. (e.g., if hwas a linear classi er, then fis also a linear classi er). False: For example, the functional form of <b>a decision</b> stump is a single axis-aligned split of the input space, but the functional form of the boosted classi\ufb01er is linear combinations of <b>decision</b> stumps which can form a more complex (piecewise linear) <b>decision</b> <b>boundary</b>. 2. 8. The depth of a learned <b>decision</b> tree can be ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Support Vector Machines(<b>SVM</b>) - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/https-medium-com-pupalerushikesh-<b>svm</b>-f4b42800e989", "snippet": "At first approximation what SVMs do is to find a separating line(or hyperplane) <b>between</b> <b>data</b> of <b>two</b> <b>classes</b>. <b>SVM</b> is an algorithm that takes the <b>data</b> as an input and outputs a line that separates those <b>classes</b> if possible. Lets begin with a problem. Suppose you have a dataset as shown below and you need to classify the red rectangles from the blue ellipses(let\u2019s say positives from the negatives). So your task is to find an ideal line that separates this dataset in <b>two</b> <b>classes</b> (say red and ...", "dateLastCrawled": "2022-02-03T04:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning Glossary</b> \u2013 Rishabh Misra \u2013 ML Engineer", "url": "https://rishabhmisra.github.io/Machine-Learning-Glossary/", "isFamilyFriendly": true, "displayUrl": "https://rishabhmisra.github.io/<b>Machine-Learning-Glossary</b>", "snippet": "Support Vector Machine: Support Vector Machine (SVM), in simplest terms, is a classification algorithm which aims to find <b>a decision</b> <b>boundary</b> that separates <b>two</b> <b>classes</b> such that the closest <b>data</b> points from either class are as far as possible. Having a good margin <b>between</b> the <b>two</b> <b>classes</b> contributes to robustness and generalizability of SVM.", "dateLastCrawled": "2021-10-17T14:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning Interview Questions</b> (2022) - InterviewBit", "url": "https://www.interviewbit.com/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.interviewbit.com/<b>machine-learning-interview-questions</b>", "snippet": "Suppose we have given some <b>data</b> points that each belong to one of <b>two</b> <b>classes</b>, and the goal is to separate <b>two</b> <b>classes</b> based on a set of examples. In SVM, a <b>data</b> point is viewed as a p-dimensional vector (a list of p numbers), and we wanted to know whether we can separate such points with a (p-1)-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that classify the <b>data</b>. To choose the best hyperplane that represents the largest separation or margin <b>between</b> ...", "dateLastCrawled": "2022-02-03T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "GitHub - arthursjkim/dsc-building-an-svm-using-scikit-learn-lab-1", "url": "https://github.com/arthursjkim/dsc-building-an-svm-using-scikit-learn-lab-1", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/arthursjkim/dsc-building-an-svm-using-scikit-learn-lab-1", "snippet": "Now it&#39;s time to fit a simple linear support vector machine model on this <b>data</b>. The process is very <b>similar</b> to other scikit-learn models you have built so far: import the class, instantiate, fit, and predict. Import SVC from scikit-learn&#39;s svm module; Instantiate SVC (which stands for Support Vector Classification) with kernel=&#39;linear&#39; as the only argument; Call the .fit() method with the <b>data</b> as the first argument and the labels as the second. Note: Typically you should scale <b>data</b> when ...", "dateLastCrawled": "2021-09-18T23:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "4. Fitting a Model to <b>Data</b> - <b>Data Science for Business</b> [Book]", "url": "https://www.oreilly.com/library/view/data-science-for/9781449374273/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/<b>data</b>-science-for/9781449374273/ch04.html", "snippet": "Support vector machines use <b>hinge</b> <b>loss</b>, so called because the <b>loss</b> graph looks like a <b>hinge</b>. <b>Hinge</b> <b>loss</b> incurs no penalty for an example that is not on the wrong side of the margin. The <b>hinge</b> <b>loss</b> only becomes positive when an example is on the wrong side of the <b>boundary</b> and beyond the margin. <b>Loss</b> then increases linearly with the example\u2019s distance from the margin, thereby penalizing points more the farther they are from the separating <b>boundary</b>.", "dateLastCrawled": "2022-01-26T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Novel Decompositions of Proper Scoring Rules for Classi\ufb01cation: Score ...", "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-23528-8_5.pdf", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/content/pdf/10.1007/978-3-319-23528-8_5.pdf", "snippet": "<b>decision</b> <b>boundary</b>. In this case, surrogate losses such as quadratic <b>loss</b>, <b>hinge</b> <b>loss</b> or log-<b>loss</b> enable SVMs, logistic regression or boosting to converge towards better models. c Springer International Publishing Switzerland 2015 A. Appice et al. (Eds.): ECML PKDD 2015, Part I, LNAI 9284, pp. 68\u201385, 2015. DOI: 10.1007/978-3-319-23528-8 5. Novel Decompositions of Proper Scoring Rules for Classi\ufb01cation 69 The second situation where the choice of evaluation measure is non-trivial is when ...", "dateLastCrawled": "2022-01-22T20:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "GitHub - jeevu94/ml-notes: Personal notes on ML", "url": "https://github.com/jeevu94/ml-notes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/jeevu94/ml-notes", "snippet": "The <b>hinge</b> <b>loss</b> is used for &quot;maximum-margin&quot; classification, most notably for support vector machines (SVMs). To summarize, when working with an SVM, if a computed value gives a correct classification and is larger than the margin, there is no <b>hinge</b> <b>loss</b>. If a computed value gives a correct classification but is too close to zero (where too close is defined by a margin) there is a small <b>hinge</b> <b>loss</b>. If a computed value gives an incorrect classification there will always be a <b>hinge</b> <b>loss</b> ...", "dateLastCrawled": "2022-01-20T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Classification \u2013 Min Liang&#39;s blog", "url": "https://liangminblog.wordpress.com/2017/02/21/some-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://liangminblog.wordpress.com/2017/02/21/some-machine-learning", "snippet": "The <b>decision</b> <b>boundary</b> is a margin, which is defined by the support vectors, ... <b>hinge</b> <b>loss</b> \u2013 unboundedness of the convex <b>loss</b> causes the sensitivity to outliers; <b>decision</b> <b>boundary</b> only depends on support vectors. Pros: Accuracy, Works well on smaller cleaner datasets, It <b>can</b> be more efficient because it uses a subset of training points, more features (help build feature space, prevent overfitting by kernel) Less effective on noisier datasets with overlapping <b>classes</b>, use small number of ...", "dateLastCrawled": "2022-01-31T15:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Statistical Mechanics of Deep Learning | Annual Review of Condensed ...", "url": "https://www.annualreviews.org/doi/10.1146/annurev-conmatphys-031119-050745", "isFamilyFriendly": true, "displayUrl": "https://www.annualreviews.org/doi/10.1146/annurev-conmatphys-031119-050745", "snippet": "Whereas the <b>decision</b> <b>boundary</b> in this hidden layer must be a linear hyperplane, ... used in classification settings in which the neural network output is a single real number whose sign indicates one of <b>two</b> <b>classes</b>. The <b>hinge</b> <b>loss</b> as a function of weight space then distinguishes each of the P training examples as either satisfied (i.e., classified with the correct sign with a threshold margin) or unsatisfied. Each point in N-dimensional network parameter space then yields a fraction of ...", "dateLastCrawled": "2022-02-02T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Comparison <b>Between</b> Svm And Logistic Regression Which One", "url": "https://mrciweb-test.mrci.com/v/pdf/F5J0I3/comparison-between-svm-and-logistic-regression-which-one_pdf", "isFamilyFriendly": true, "displayUrl": "https://mrciweb-test.mrci.com/v/pdf/F5J0I3/comparison-<b>between</b>-svm-and-logistic...", "snippet": "The SVM model tries to enlarge the distance <b>between</b> the <b>two</b> <b>classes</b> by <b>creating</b> a well-defined <b>decision</b> <b>boundary</b>. Examples \u2014 scikit-learn 1.1.dev0 documentation Multiclass sparse logistic regression on 20newgroups Polynomial and Spline interpolation \u00b6 Early stopping of Stochastic Gradient Descent \u00b6 One-Class SVM versus One-Class SVM using Stochastic Gradient Descent Comparison <b>between</b> grid search and successive halving Applications of Support Vector Machine (SVM) Learning in Dec 26, 2017 ...", "dateLastCrawled": "2022-01-16T19:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Support Vector Machine</b> - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/<b>support-vector-machine</b>-introduction-to-machine-learning...", "snippet": "To separate the <b>two</b> <b>classes</b> <b>of data</b> points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the maximum distance <b>between</b> <b>data</b> points of both <b>classes</b>. Maximizing the margin distance provides some reinforcement so that future <b>data</b> points <b>can</b> be classified with more confidence. Hyperplanes and Support Vectors. Hyperplanes in 2D and 3D feature space. Hyperplanes are <b>decision</b> boundaries that help classify the <b>data</b> points ...", "dateLastCrawled": "2022-02-03T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What I Learned From The Future <b>of Data</b>-Centric AI 2021 \u2014 James Le", "url": "https://jameskle.com/writes/snorkel-data-centric-ai-2021", "isFamilyFriendly": true, "displayUrl": "https://jameskle.com/writes/snorkel-<b>data</b>-centric-ai-2021", "snippet": "For example, we need algorithms for learning a more compact <b>decision</b> <b>boundary</b> <b>between</b> ID and OOD <b>data</b>. There are also great opportunities for a more realistic <b>data</b> model. The current evaluation is too simplified to capture real-world OOD <b>data</b>. Previous work has relied on small-scale, low-resolution datasets (10-100 <b>classes</b>). As it turns out ...", "dateLastCrawled": "2022-01-31T18:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>IS 300 (Analytics) Midterm</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/273111798/is-300-analytics-midterm-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/273111798/<b>is-300-analytics-midterm</b>-flash-cards", "snippet": "Discriminates <b>between</b> <b>classes</b> and the function of the <b>decision</b> <b>boundary</b> is a linear combination\u2014a weighted sum\u2014of the attributes. A linear discriminant function is a numeric classification model. (i.e. Age = (-1.5) x Balance + 60) Linear discriminant function. The weights of the linear function (wi) are the parameters. The <b>data</b> mining is going to &quot;fit&quot; this parameterized model to a dataset\u2014meaning specifically, to find a good set of weights on the features. General Linear Model. A ...", "dateLastCrawled": "2021-11-21T06:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Business Analytics MidTerm</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/98662555/business-analytics-midterm-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/98662555/<b>business-analytics-midterm</b>-flash-cards", "snippet": "<b>Two</b> main sub <b>classes</b> of supervised <b>data</b> mining. Classification and Regression Distinguished by type of target Regression involves a numeric target Classification involves categorical (often binary) target. Unsupervised Task. Clustering, co-occurrence, profiling No guarantee results will be useful. Supervised or Unsupervised Task. Similarity matching, link prediction, <b>data</b> reduction &quot;Will this customer purchase service S1 if given incentive I?&quot; This is a classification problem because it has ...", "dateLastCrawled": "2018-11-01T20:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "How to use <b>categorical / multiclass hinge with TensorFlow</b> 2 and Keras ...", "url": "https://www.machinecurve.com/index.php/2019/10/17/how-to-use-categorical-multiclass-hinge-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/.../10/17/how-to-use-categorical-multiclass-<b>hinge</b>-with-keras", "snippet": "From binary <b>hinge</b> to multiclass <b>hinge</b>. In that previous blog, we looked at <b>hinge</b> <b>loss</b> and squared <b>hinge</b> <b>loss</b> \u2013 which actually helped us to generate <b>a decision</b> <b>boundary</b> <b>between</b> <b>two</b> <b>classes</b> and hence a classifier, but yep \u2013 <b>two</b> <b>classes</b> only.. <b>Hinge</b> <b>loss</b> and squared <b>hinge</b> <b>loss</b> <b>can</b> be used for binary classification problems.. Unfortunately, many of today\u2019s problems aren\u2019t binary, but rather, multiclass: the number of possible target <b>classes</b> is \\(&gt; 2\\).", "dateLastCrawled": "2022-01-29T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "machine learning - What are the impacts of choosing different <b>loss</b> ...", "url": "https://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing-different-loss-functions-in-classification-to-a", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing...", "snippet": "However one important property of <b>hinge</b> <b>loss</b> is, <b>data</b> points far away from the <b>decision</b> <b>boundary</b> contribute nothing to the <b>loss</b>, the solution will be the same with those points removed. The remaining points are called support vectors in the context of SVM. Whereas SVM uses a regularizer term to ensure the maximum margin property and a unique solution. Share. Cite. Improve this answer. Follow edited Jul 29 &#39;16 at 1:52. answered Jul 27 &#39;16 at 9:46. dontloo dontloo. 13.4k 7 7 gold badges 49 49 ...", "dateLastCrawled": "2022-01-22T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "If none of those occur, it\u2019s likely that the tomato <b>can</b> be sold. We now have <b>two</b> <b>classes</b>: ... more precisely at the \u2018<b>boundary</b>\u2019 <b>between</b> no <b>loss</b> / minimum <b>loss</b>. Fortunately, a subgradient of the <b>hinge</b> <b>loss</b> function <b>can</b> be optimized, so it <b>can</b> (albeit in a different form) still be used in today\u2019s deep learning models (Wikipedia, 2011). For example, <b>hinge</b> <b>loss</b> is available as a <b>loss</b> function in Keras. Squared <b>hinge</b>. The squared <b>hinge</b> <b>loss</b> is like the <b>hinge</b> formula displayed above, but ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "10-701/15-781 Machine Learning - Midterm Exam, Fall 2010", "url": "https://cs.cmu.edu/~aarti/Class/10701/exams/midterm2010f_sol.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.cmu.edu/~aarti/Class/10701/exams/midterm2010f_sol.pdf", "snippet": "The functional form of f\u2019s <b>decision</b> <b>boundary</b> is the same as h\u2019s, but with di erent parameters. (e.g., if hwas a linear classi er, then fis also a linear classi er). False: For example, the functional form of <b>a decision</b> stump is a single axis-aligned split of the input space, but the functional form of the boosted classi\ufb01er is linear combinations of <b>decision</b> stumps which <b>can</b> form a more complex (piecewise linear) <b>decision</b> <b>boundary</b>. 2. 8. The depth of a learned <b>decision</b> tree <b>can</b> be ...", "dateLastCrawled": "2022-02-02T21:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Function For Multiclass Classification - XpCourse", "url": "https://www.xpcourse.com/loss-function-for-multiclass-classification", "isFamilyFriendly": true, "displayUrl": "https://www.xpcourse.com/<b>loss</b>-function-for-multiclass-classification", "snippet": "From binary <b>hinge</b> to multiclass <b>hinge</b>. In that previous blog, we looked at <b>hinge</b> <b>loss</b> and squared <b>hinge</b> <b>loss</b> - which actually helped us to generate <b>a decision</b> <b>boundary</b> <b>between</b> <b>two</b> <b>classes</b> and hence a classifier, but yep - <b>two</b> <b>classes</b> only.. <b>Hinge</b> <b>loss</b> and squared <b>hinge</b> <b>loss</b> <b>can</b> be used for binary classification problems.. Unfortunately, many of ...", "dateLastCrawled": "2022-01-03T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning using Python Interview Questions</b> <b>Data</b> ... - Knowledgehut", "url": "https://www.zeolearn.com/interview-questions/machine-learning-using-python", "isFamilyFriendly": true, "displayUrl": "https://www.zeolearn.com/interview-questions/machine-learning-using-python", "snippet": "SVM uses <b>hinge</b> <b>loss</b> function: Where w^2 is the regularize and is the <b>loss</b> function. Add Bookmark 10. Discuss some of the pre-processing techniques used to prepare the <b>data</b> in python? Mean removal - It involves removing the mean from each feature so that it is centred on zero. Mean removal helps in removing any bias from the features. Feature scaling - The values of every feature in a <b>data</b> point <b>can</b> vary <b>between</b> random values. So, it is important to scale them so that this matches specified ...", "dateLastCrawled": "2022-02-02T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Can</b> <b>data</b> points lie on <b>decision boundary or between support vectors</b> in ...", "url": "https://www.quora.com/Can-data-points-lie-on-decision-boundary-or-between-support-vectors-in-support-Vector-Machine", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Can</b>-<b>data</b>-points-lie-on-<b>decision-boundary-or-between-support</b>...", "snippet": "Answer: Yes, Most of the time it happens. In real-world cases, your <b>data</b> is not linearly separable. Actually, the algorithm\u2019s mathematical formulation has been formed based on the consideration that your <b>data</b> won&#39;t be linearly separable. In lin SVM we try to do margin maximization. margin mean...", "dateLastCrawled": "2022-01-23T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Fine-grained <b>Sentiment Analysis</b> in Python (Part 1) - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/fine-grained-<b>sentiment-analysis</b>-in-python-part-1-2697bb...", "snippet": "Support Vector Machines (SVMs) are very similar to logistic regression in terms of how they optimize a <b>loss</b> function to generate <b>a decision</b> <b>boundary</b> <b>between</b> <b>data</b> points. The primary difference, however, is the use of \u201ckernel functions\u201d, i.e. functions that transform a complex, nonlinear <b>decision</b> space to one that has higher dimensionality, so that an appropriate hyperplane separating the <b>data</b> points <b>can</b> be found. The SVM classifier looks to maximize the distance of each <b>data</b> point from ...", "dateLastCrawled": "2022-01-30T23:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "SVM <b>From Scratch</b> \u2014 Python. Important Concepts ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/svm-implementation-<b>from-scratch</b>-python-2db2fc52e5c2", "snippet": "It creates separation <b>between</b> examples of <b>two</b> <b>classes</b> with a maximum margin; Its equation (w.x + b = 0) yields a value \u2265 1 for examples from+ve class and \u2264-1 for examples from -ve class; How does it find this hyperplane? By finding the optimal values w* (weights/normal) and b* (intercept) which define this hyperplane. The optimal values are found by minimizing a cost function. Once the algorithm identifies these optimal values, the SVM model f(x) is then defined as shown below: SVM Model ...", "dateLastCrawled": "2022-02-02T15:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Which is better for binary classification, SVM or logistic ... - Quora", "url": "https://www.quora.com/Which-is-better-for-binary-classification-SVM-or-logistic-regression", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Which-is-better-for-binary-classification-SVM-or-logistic-regression", "snippet": "Answer (1 of 2): If you restrict yourself to linear kernels, both SVMs and LR will give almost identical performance and in some cases, LR will beat SVM. If the <b>data</b> is linearly separable in the input space, then LR is usually preferred as it outputs probabilities instead of hard labels and you c...", "dateLastCrawled": "2022-01-21T08:52:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Main concepts behind <b>Machine</b> <b>Learning</b> | by Leven.co.in | Medium", "url": "https://in-leven.medium.com/main-concepts-behind-machine-learning-848ec516ef94", "isFamilyFriendly": true, "displayUrl": "https://in-leven.medium.com/main-concepts-behind-<b>machine</b>-<b>learning</b>-848ec516ef94", "snippet": "The two most common <b>loss</b> function are <b>hinge</b>-<b>loss</b> and cross-entropy. The first one is used in SVM (Supported Vector Machines) classifiers and it concerns in getting the correct class score greater than the other scores by a margin \u0394. Formula for <b>hinge</b>-<b>loss</b>. s\u1d62 is the correct score category. The second one is used in Softmax classifiers which interprets the scores as probabilities, always trying to get the correct class close to 1. Formula for cross-entropy. s\u1d62 the correct category score ...", "dateLastCrawled": "2022-01-14T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Main concepts behind Machine Learning</b> | by Bruno Eidi Nishimoto ...", "url": "https://medium.com/neuronio/main-concepts-behind-machine-learning-22cd81d68a11", "isFamilyFriendly": true, "displayUrl": "https://medium.com/neuronio/<b>main-concepts-behind-machine-learning</b>-22cd81d68a11", "snippet": "The two most common <b>loss</b> function are <b>hinge</b>-<b>loss</b> and cross-entropy. The first one is used in SVM (Supported Vector Machines) classifiers and it concerns in getting the correct class score greater ...", "dateLastCrawled": "2022-01-19T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, squared <b>hinge</b> <b>loss</b> function (as against <b>hinge</b> <b>loss</b> function) and l2 penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "In contrast, in <b>machine</b> <b>learning</b> methodology, log <b>loss</b> will be minimized with respect to ... <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the following stages occur: Stage 1 ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Metrics to Evaluate Classification and Regression Algorithms | by ...", "url": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and-regression-algorithms-1554f1e00a75", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and...", "snippet": "<b>Hinge</b> <b>Loss</b>(Binary Classification): An alternative to cross-entropy for binary classification problems is the <b>hinge</b> <b>loss</b> function, primarily developed for use with support vector <b>machine</b> (SVM ...", "dateLastCrawled": "2022-01-17T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Models 1.1 Support vector <b>machine</b> 1.1.1 Principle 1.1.2 Kernel 1.1.3 Soft margin SVM 1.1.4 <b>Hinge</b> <b>loss</b> view 1.1.5 Multi-class SVM 1.1.6 Extensions 1.2 Tree-based models 1.2.1 Decision tree 1.2.2 Random forest 1.2.3 Gradient boosted decision trees 1.2.4 Tools 1.3 EM Principle 1.4 MaxEnt 1.4.1 Entropy 1.5 Model selection 1.5.1 Under-fitting / Over-fitting 1.5.2 Model ensemble, sklearn 2.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "Optimization methods are applied to minimize the <b>loss</b> function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one <b>loss</b> is L0-1 = 1 (m &lt;= 0); in zero-one <b>loss</b>, value of <b>loss</b> is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this <b>loss</b> is it is not differentiable, non-convex, and also NP-hard. Hence, in order to make optimization feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Statistical <b>Learning</b> Theory and the C-<b>Loss</b> cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/c<b>loss</b>.pdf", "snippet": "Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. Empirical Risk Minimization (ERM) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the Risk functional as L(.) is called the <b>Loss</b> function, and minimize it w.r.t. w achieving the best possible <b>loss</b>. But we can not do this integration because the joint is normally not ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "50 Data Scientist Interview Questions (ANSWERED with PDF) To Crack Next ...", "url": "https://www.mlstack.cafe/blog/data-scientist-interview-questions", "isFamilyFriendly": true, "displayUrl": "https://www.mlstack.cafe/blog/data-scientist-interview-questions", "snippet": "Companies need data scientists. They need people who are able to take large amounts of data and make it usable. The national average salary for a Data Scientist in the United States is $117,212. Data Scientist roles in Australia were typically advertised between $110k and $140k in the last 3 months. Follow along and learn the 50 most common and advanced Data Scientist Interview Questions and Answers (PDF download ready) you must know before your next <b>Machine</b> <b>Learning</b> and Data Science interview.", "dateLastCrawled": "2022-02-03T06:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "We\u2019re then using <b>machine</b> <b>learning</b> for ... The squared <b>hinge loss is like</b> the hinge formula displayed above, but then the \\(max()\\) function output is squared. This helps achieving two things: Firstly, it makes the loss value more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add to the loss more significantly than smaller errors. Note that simiarly, this may also mean that you\u2019ll need to inspect your dataset for the presence of such outliers first. Secondly ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - <b>hinge loss</b> vs logistic loss advantages and ...", "url": "https://stats.stackexchange.com/questions/146277/hinge-loss-vs-logistic-loss-advantages-and-disadvantages-limitations", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/146277/<b>hinge-loss</b>-vs-logistic-loss...", "snippet": "<b>machine</b>-<b>learning</b> svm loss-functions computer-vision. Share. Cite. Improve this question. Follow edited Jul 23 &#39;18 at 15:41. DHW. 644 3 3 silver badges 13 13 bronze badges. asked Apr 14 &#39;15 at 11:18. user570593 user570593. 1,059 2 2 gold badges 12 12 silver badges 19 19 bronze badges $\\endgroup$ Add a comment | 3 Answers Active Oldest Votes. 31 $\\begingroup$ Logarithmic loss minimization leads to well-behaved probabilistic outputs. <b>Hinge loss</b> leads to some (not guaranteed) sparsity on the ...", "dateLastCrawled": "2022-01-26T09:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>A Course in Machine Learning</b> | AZERTY UIOP - Academia.edu", "url": "https://www.academia.edu/11902068/A_Course_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/11902068/<b>A_Course_in_Machine_Learning</b>", "snippet": "<b>A Course in Machine Learning</b>. \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset password. Enter the email address you signed up with and we&#39;ll email you a reset link. Need an account? Click here to sign up. Log In Sign ...", "dateLastCrawled": "2022-01-23T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Course in <b>Machine</b> <b>Learning</b>", "url": "http://ciml.info/dl/v0_8/ciml-v0_8-ch12.pdf", "isFamilyFriendly": true, "displayUrl": "ciml.info/dl/v0_8/ciml-v0_8-ch12.pdf", "snippet": "160 a course in <b>machine</b> <b>learning</b> fortunately, not only is the zero-norm non-convex, it\u2019s also discrete. Optimizing it is NP-hard. A reasonable middle-ground is the one-norm: jjwjj 1 = \u00e5 djw j. It is indeed convex: in fact, it is the tighest \u2018p norm that is convex. Moreover, its gradients do not go to zero as in the two-norm. <b>Just as hinge-loss</b> is the tightest convex upper bound on zero-one error, the one-norm is the tighest convex upper bound on the zero-norm. At this point, you should ...", "dateLastCrawled": "2021-09-07T20:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Course in <b>Machine</b> <b>Learning</b> | PDF | <b>Machine</b> <b>Learning</b> | Prediction", "url": "https://www.scribd.com/document/346469890/a-course-in-machine-learning-pdf", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/346469890/a-course-in-<b>machine</b>-<b>learning</b>-pdf", "snippet": "The <b>machine</b> <b>learning</b> algorithm has succeeded if its performance on the test data is high. 1.2 Some Canonical <b>Learning</b> Problems. There are a large number of typical inductive <b>learning</b> problems. The primary difference between them is in what type of thing theyre trying to predict. Here are some examples: Regression: trying to predict a real value. For instance, predict the value of a stock tomorrow given its past performance. Or predict Alices score on the <b>machine</b> <b>learning</b> final exam based on ...", "dateLastCrawled": "2021-12-06T00:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Course in <b>Machine</b> <b>Learning</b>", "url": "http://ciml.info/dl/v0_9/ciml-v0_9-ch12.pdf", "isFamilyFriendly": true, "displayUrl": "ciml.info/dl/v0_9/ciml-v0_9-ch12.pdf", "snippet": "162 a course in <b>machine</b> <b>learning</b> pect the algorithm to converge. Unfortunately, in comparisong to gradient descent, stochastic gradient is quite sensitive to the selection of a good <b>learning</b> rate. There is one more practical issues related to the use of SGD as a <b>learning</b> algorithm: do you really select a random point (or subset of random points) at each step, or do you stream through the data in order. The answer is akin to the answer of the same question for the perceptron algorithm ...", "dateLastCrawled": "2021-09-20T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "- <b>A Course in Machine Learning</b> - Studylib", "url": "https://studylib.net/doc/8792694/--a-course-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://studylib.net/doc/8792694/--<b>a-course-in-machine-learning</b>", "snippet": "Free essays, homework help, flashcards, research papers, book reports, term papers, history, science, politics", "dateLastCrawled": "2021-12-27T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Ciml <b>v0 - 8 All Machine Learning</b> | <b>Machine Learning</b> | Prediction", "url": "https://www.scribd.com/document/172987143/Ciml-v0-8-All-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/172987143/Ciml-<b>v0-8-All-Machine-Learning</b>", "snippet": "The <b>machine learning</b> algorithm has succeeded if its performance on the test data is high. 1.2 Some Canonical <b>Learning</b> Problems. There are a large number of typical inductive <b>learning</b> problems. The primary difference between them is in what type of thing theyre trying to predict. Here are some examples: Regression: trying to predict a real value. For instance, predict the value of a stock tomorrow given its past performance. Or predict Alices score on the <b>machine learning</b> nal exam based on ...", "dateLastCrawled": "2022-01-19T05:02:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(hinge loss)  is like +(creating a decision boundary between two classes of data)", "+(hinge loss) is similar to +(creating a decision boundary between two classes of data)", "+(hinge loss) can be thought of as +(creating a decision boundary between two classes of data)", "+(hinge loss) can be compared to +(creating a decision boundary between two classes of data)", "machine learning +(hinge loss AND analogy)", "machine learning +(\"hinge loss is like\")", "machine learning +(\"hinge loss is similar\")", "machine learning +(\"just as hinge loss\")", "machine learning +(\"hinge loss can be thought of as\")", "machine learning +(\"hinge loss can be compared to\")"]}