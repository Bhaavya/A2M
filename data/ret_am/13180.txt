{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>BLEU</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/BLEU", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>BLEU</b>", "snippet": "<b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is an <b>algorithm</b> for evaluating the quality of text which has been <b>machine</b>-translated from one natural language to another. Quality is considered to be the correspondence between a <b>machine</b>&#39;s output and that of a human: &quot;the closer a <b>machine</b> translation is to a professional human translation, the better it is&quot; \u2013 this is the central idea behind <b>BLEU</b>. <b>BLEU</b> was one of the first metrics to claim a high correlation with human judgements of quality, and ...", "dateLastCrawled": "2022-02-02T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise <b>machine</b> translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching <b>algorithm</b> that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching <b>algorithm</b> that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to Calculating the <b>BLEU</b> Score for Text in Python", "url": "https://machinelearningmastery.com/calculate-bleu-score-for-text-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/calculate-<b>bleu</b>-score-for-text-pyth", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for comparing a candidate translation of text to one or more reference translations. Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks. In this tutorial, you will discover the <b>BLEU</b> score for evaluating and scoring candidate text using the NLTK library in", "dateLastCrawled": "2022-01-30T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "<b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical <b>machine</b> translation to make it more adjustable and robust . We in tend to adapt it to resemble human <b>evaluatio n</b> mor e.", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Choosing the <b>Right Metric for Evaluating Machine Learning</b> Models \u2013 Part ...", "url": "https://www.kdnuggets.com/2018/04/right-metric-evaluating-machine-learning-models-1.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2018/04/right-metric-evaluating-<b>machine</b>-<b>learning</b>-models-1.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) It is mostly used to measure the quality of <b>machine</b> translation with respect to the human translation. It uses a modified form of precision metric. Steps to compute <b>BLEU</b> score: 1. Convert the sentence into unigrams, bigrams, trigrams, and 4-grams 2. Compute precision for n-grams of size 1 to 4 3. Take the exponential of the weighted average of all those precision values 4. Multiply it with brevity penalty (will explain later) Here BP is the brevity ...", "dateLastCrawled": "2022-01-25T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "This measure, looking at n-grams overlap between the output and reference translations with a penalty for shorter outputs, is known as <b>BLEU</b> (short for \u201c<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>\u201d which people literally only ever say when explaining the acronym) and was developed by Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu at IBM in 2002. It\u2019s a very popular metric in NLP, particularly for tasks where the output of a system is a text string rather than a classification. This ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Automatic Assessment of Open Ended</b> Questions with a <b>Bleu</b>-Inspired ...", "url": "https://www.researchgate.net/publication/221418794_Automatic_Assessment_of_Open_Ended_Questions_with_a_Bleu-Inspired_Algorithm_and_Shallow_NLP", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221418794_<b>Automatic_Assessment_of_Open_Ended</b>...", "snippet": "The most important of these studies are presented below; Alfonseca and Perez (2004) used an <b>algorithm</b> which is called <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>), based on n-gram and normalization ...", "dateLastCrawled": "2021-12-24T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning Interview Questions</b> - <b>Machine</b> <b>Learning</b> Interviews", "url": "https://machinelearninginterview.com/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>interview.com/<b>machine-learning-interview-questions</b>", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>) score is the most common metric used during <b>machine</b> translation. Typically, it is used to measure a candidate translation against a set of reference translations available ...", "dateLastCrawled": "2022-01-31T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Automatic question generation and answer assessment</b>: a survey ...", "url": "https://telrp.springeropen.com/articles/10.1186/s41039-021-00151-1", "isFamilyFriendly": true, "displayUrl": "https://telrp.springeropen.com/articles/10.1186/s41039-021-00151-1", "snippet": "Noorbehbahani and Kardan (2011) introduced a method for judging free text answers of students using a modified <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (M-<b>BLEU</b>) <b>algorithm</b>. The M-<b>BLEU</b> recognized the most similar reference answer to a student answer and estimated a score to judge the answers. Their method achieved higher <b>accuracy</b> than the other <b>evaluation</b> methods, <b>like</b> latent semantic analysis and n-gram co-occurrence. Dhokrat et al. (2012) proposed an appraisal system for evaluating the student\u2019s ...", "dateLastCrawled": "2022-01-31T20:35:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 0, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>BLEU</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/BLEU", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>BLEU</b>", "snippet": "<b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is an <b>algorithm</b> for evaluating the quality of text which has been <b>machine</b>-translated from one natural language to another. Quality is considered to be the correspondence between a <b>machine</b>&#39;s output and that of a human: &quot;the closer a <b>machine</b> translation is to a professional human translation, the better it is&quot; \u2013 this is the central idea behind <b>BLEU</b>. <b>BLEU</b> was one of the first metrics to claim a high correlation with human judgements of quality, and ...", "dateLastCrawled": "2022-02-02T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching <b>algorithm</b> that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise <b>machine</b> translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching <b>algorithm</b> that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to Calculating the <b>BLEU</b> Score for Text in Python", "url": "https://machinelearningmastery.com/calculate-bleu-score-for-text-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/calculate-<b>bleu</b>-score-for-text-pyth", "snippet": "<b>BLEU</b>, or the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>, is a score for comparing a candidate translation of text to one or more reference translations. Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks. In this tutorial, you will discover the <b>BLEU</b> score for evaluating and scoring candidate text using the NLTK library in", "dateLastCrawled": "2022-01-30T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "Abstract and Figures. Our research extends the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical <b>machine</b> translation to make it more adjustable and robust. We intend to ...", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "This measure, looking at n-grams overlap between the output and reference translations with a penalty for shorter outputs, is known as <b>BLEU</b> (short for \u201c<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>\u201d which people literally only ever say when explaining the acronym) and was developed by Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu at IBM in 2002. It\u2019s a very popular metric in NLP, particularly for tasks where the output of a system is a text string rather than a classification. This ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Choosing the <b>Right Metric for Evaluating Machine Learning</b> Models \u2013 Part ...", "url": "https://www.kdnuggets.com/2018/04/right-metric-evaluating-machine-learning-models-1.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2018/04/right-metric-evaluating-<b>machine</b>-<b>learning</b>-models-1.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) It is mostly used to measure the quality of <b>machine</b> translation with respect to the human translation. It uses a modified form of precision metric. Steps to compute <b>BLEU</b> score: 1. Convert the sentence into unigrams, bigrams, trigrams, and 4-grams 2. Compute precision for n-grams of size 1 to 4 3. Take the exponential of the weighted average of all those precision values 4. Multiply it with brevity penalty (will explain later) Here BP is the brevity ...", "dateLastCrawled": "2022-01-25T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Hugging Face Pre-trained Models: Find the Best One for Your Task ...", "url": "https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best", "snippet": "This is the reason, we will go with the below metrics for <b>evaluation</b> purposes. <b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) \u201cIt is an <b>algorithm</b> for evaluating the quality of text which has been <b>machine</b>-translated from one natural language to another. Quality is considered to be the correspondence between a <b>machine</b>\u2019s output and that of a human: \u201cthe closer a <b>machine</b> translation is to a professional human translation, the better it is\u201d \u2013 this is the central idea behind <b>BLEU</b>. It was one of ...", "dateLastCrawled": "2022-02-02T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Automatic Assessment of Open Ended</b> Questions with a <b>Bleu</b>-Inspired ...", "url": "https://www.researchgate.net/publication/221418794_Automatic_Assessment_of_Open_Ended_Questions_with_a_Bleu-Inspired_Algorithm_and_Shallow_NLP", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221418794_<b>Automatic_Assessment_of_Open_Ended</b>...", "snippet": "The most important of these studies are presented below; Alfonseca and Perez (2004) used an <b>algorithm</b> which is called <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>), based on n-gram and normalization ...", "dateLastCrawled": "2021-12-24T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine Learning Interview Questions</b> - <b>Machine</b> <b>Learning</b> Interviews", "url": "https://machinelearninginterview.com/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>interview.com/<b>machine-learning-interview-questions</b>", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>) score is the most common metric used during <b>machine</b> translation. Typically, it is used to measure a candidate translation against a set of reference translations available ...", "dateLastCrawled": "2022-01-31T09:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "Abstract and Figures. Our research extends the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical <b>machine</b> translation to make it more adjustable and robust. We intend to ...", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Automatic Assessment of Open Ended</b> Questions with a <b>Bleu</b>-Inspired ...", "url": "https://www.researchgate.net/publication/221418794_Automatic_Assessment_of_Open_Ended_Questions_with_a_Bleu-Inspired_Algorithm_and_Shallow_NLP", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221418794_<b>Automatic_Assessment_of_Open_Ended</b>...", "snippet": "The most important of these studies are presented below; Alfonseca and Perez (2004) used an <b>algorithm</b> which is called <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>), based on n-gram and normalization ...", "dateLastCrawled": "2021-12-24T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Information | Free Full-Text | A Literature Survey of Recent Advances ...", "url": "https://www.mdpi.com/2078-2489/13/1/41/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2078-2489/13/1/41/htm", "snippet": "<b>bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) is widely used to assess various NLP tasks, even though it was first implemented to measure <b>machine</b> translation outputs. The <b>BLEU</b> metric assigns a value to a translation on a scale of 0 to 1, however it is typically expressed as a percentage. The closer the translation is to 1, the more closely it resembles a human translation. Simply said, the <b>BLEU</b> metric counts the number of words that overlap in a translation when compared to a reference translation ...", "dateLastCrawled": "2022-02-01T05:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How <b>Machine</b> <b>Learning</b> and Artificial Intelligence has changed Language ...", "url": "https://www.langminds.com/blog/2019/12/31/how-machine-learning-and-artificial-intelligence-has-changed-language-industry/", "isFamilyFriendly": true, "displayUrl": "https://www.langminds.com/blog/2019/12/31/how-<b>machine</b>-<b>learning</b>-and-artificial...", "snippet": "The industry measures the translation quality by an <b>algorithm</b> called as <b>BLEU</b> (the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>). It matters because human\u2019s translation <b>accuracy</b> scores up to 4.636. The <b>BLEU</b> matrices check how close the <b>machine</b> translation is to human version of the translation.", "dateLastCrawled": "2021-12-08T14:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Evaluation</b> of <b>Text Generation</b>: A Survey | DeepAI", "url": "https://deepai.org/publication/evaluation-of-text-generation-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>evaluation</b>-of-<b>text-generation</b>-a-survey", "snippet": "The <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>bleu</b>) is one of the first metrics used to measure the similarity between <b>two</b> sentences (Papineni et al., 2002). Originally proposed for <b>machine</b> translation, it compares a candidate translation of text to one or more reference translations. <b>bleu</b>. is a weighted geometric mean of . n-gram precision scores ...", "dateLastCrawled": "2022-02-03T15:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Translating Sentimental Statements Using Deep Learning Techniques</b>", "url": "https://www.mdpi.com/2079-9292/10/2/138/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2079-9292/10/2/138/htm", "snippet": "<b>BLEU</b> is a text <b>evaluation</b> <b>algorithm</b> originally used to evaluate the correspondence between <b>machine</b> translation and professional human translation. In general, the closer the <b>machine</b> translation is to the professional human translation, the better the <b>machine</b> translation. Thus, the <b>BLEU</b> <b>algorithm</b>\u2019s score <b>can</b> be used as an indicator of STM quality. The higher the <b>BLEU</b> scores, the higher the word similarity to the target sentences. Here, the <b>BLEU</b> scores do not consider grammar correctness ...", "dateLastCrawled": "2022-01-18T17:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "GitHub - rdewey1/Python-Neural-<b>Machine</b>-Translation: The goal of this ...", "url": "https://github.com/rdewey1/Python-Neural-Machine-Translation", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rdewey1/Python-Neural-<b>Machine</b>-Translation", "snippet": "We used <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) and Recall Oriented <b>Understudy</b> for Gisting <b>Evaluation</b> (ROUGE) to evaluate the quality of our NMT models. We chose to use <b>BLEU</b> as a primary <b>evaluation</b> metric for a number of reasons. <b>BLEU</b> is designed specifically to evaluate <b>Machine</b> Translations by comparing strings of words and <b>can</b> <b>be thought</b> of as a measure of similarity. In our case, the <b>BLEU</b> scores are a number between 0 and 1 that suggests the quality of the <b>machine</b> translation as compared ...", "dateLastCrawled": "2021-11-18T23:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "CERC: an <b>interactive content extraction, recognition, and construction</b> ...", "url": "https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-01330-8", "isFamilyFriendly": true, "displayUrl": "https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-01330-8", "snippet": "We evaluate the performance using the <b>bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) <b>algorithm</b> . Scores are calculated for individual translated segments (sentences) by comparing them with a set of good quality reference translations. This approximates the human judgement at a corpus level. The output <b>BLEU</b> value is between 0 and 1, with values closer to 1 indicating more similar (thus a better translation).", "dateLastCrawled": "2022-01-23T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Generative Adversarial Networks in Text Generation", "url": "https://kth.diva-portal.org/smash/get/diva2:1374343/FULLTEXT01.pdf", "isFamilyFriendly": true, "displayUrl": "https://kth.diva-portal.org/smash/get/diva2:1374343/FULLTEXT01.pdf", "snippet": "<b>BLEU</b> score <b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>) is an <b>algorithm</b> for evaluating the quality of text which has been <b>machine</b>-translated from one natural language to another. Quality is considered to be the correspondence between a <b>machine</b>\u2019s output and that of a human [12].", "dateLastCrawled": "2022-01-29T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Interpreting quality assessment re-imagined: The synergy between human ...", "url": "https://journals.sagepub.com/doi/10.1177/27523810211033670", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/10.1177/27523810211033670", "snippet": "A number of metrics have been developed and used to automatically evaluate <b>machine</b>-translated product, including <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>, see Papineni et al., 2002), National Institute of Standards and Technology (NIST, see Doddington, 2002), Metric for <b>Evaluation</b> of Translation With Explicit Ordering (METEOR, see Banerjee &amp; Lavie, 2005), and Translation Edit Rate (TER, see Snover et al., 2006). 1", "dateLastCrawled": "2022-02-03T17:50:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching <b>algorithm</b> that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise <b>machine</b> translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching <b>algorithm</b> that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Enhanced <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>", "url": "https://www.researchgate.net/publication/264043325_Enhanced_Bilingual_Evaluation_Understudy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../264043325_Enhanced_<b>Bilingual</b>_<b>Evaluation</b>_<b>Understudy</b>", "snippet": "Abstract and Figures. Our research extends the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) <b>evaluation</b> technique for statistical <b>machine</b> translation to make it more adjustable and robust. We intend to ...", "dateLastCrawled": "2021-12-11T10:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Evaluating Text Output in NLP: <b>BLEU</b> at your own risk | by Rachael ...", "url": "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/evaluating-text-output-in-nlp-<b>bleu</b>-at-your-own-risk-e...", "snippet": "This measure, looking at n-grams overlap between the output and reference translations with a penalty for shorter outputs, is known as <b>BLEU</b> (short for \u201c<b>Bilingual</b> <b>evaluation</b> <b>understudy</b>\u201d which people literally only ever say when explaining the acronym) and was developed by Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu at IBM in 2002. It\u2019s a very popular metric in NLP, particularly for tasks where the output of a system is a text string rather than a classification. This ...", "dateLastCrawled": "2022-02-02T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Automatic Assessment of Open Ended</b> Questions with a <b>Bleu</b>-Inspired ...", "url": "https://www.researchgate.net/publication/221418794_Automatic_Assessment_of_Open_Ended_Questions_with_a_Bleu-Inspired_Algorithm_and_Shallow_NLP", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221418794_<b>Automatic_Assessment_of_Open_Ended</b>...", "snippet": "The most important of these studies are presented below; Alfonseca and Perez (2004) used an <b>algorithm</b> which is called <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>), based on n-gram and normalization ...", "dateLastCrawled": "2021-12-24T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Master Thesis Using <b>Machine</b> <b>Learning</b> Methods for Evaluating the Quality ...", "url": "http://www.diva-portal.org/smash/get/diva2:920202/FULLTEXT01.pdf", "isFamilyFriendly": true, "displayUrl": "www.diva-portal.org/smash/get/diva2:920202/FULLTEXT01.pdf", "snippet": "original text and the newly generated text are <b>compared</b> [1]. On this basis, the \u201c<b>bilingual</b> <b>evaluation</b> <b>understudy</b>\u201d (<b>BLEU</b>) <b>algorithm</b> was developed and presented by Kishore Pa-pineni et al. in 2002. <b>BLEU</b> de\ufb01nes document quality as a strong correlation between <b>machine</b> translations and the work of professional human translators. It is based on the", "dateLastCrawled": "2022-01-25T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b> <b>Learning</b> Translation and the Google Translate <b>Algorithm</b> - DZone AI", "url": "https://dzone.com/articles/machine-learning-translation-and-the-google-transl", "isFamilyFriendly": true, "displayUrl": "https://dzone.com/articles/<b>machine</b>-<b>learning</b>-translation-and-the-google-transl", "snippet": "There are a lot of approaches that partly solve this problem, but the most popular and effective metric is <b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>). Imagine that we have <b>two</b> candidates from <b>machine</b> ...", "dateLastCrawled": "2022-01-12T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Deep Sequence Modeling. Whether it\u2019s an audio waveform of your\u2026 | by ...", "url": "https://medium.com/nwamaka-imasogie/deep-sequence-modeling-bdd9d26a5d6e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/nwamaka-imasogie/deep-sequence-modeling-bdd9d26a5d6e", "snippet": "<b>Bleu</b> Score \u2014 <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> One of the challenges of <b>machine</b> translation is that, given a French sentence, there could be multiple English translations that are equally good ...", "dateLastCrawled": "2021-08-16T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Setting up <b>a neural machine translation system for English</b> to Indian ...", "url": "https://www.sciencedirect.com/science/article/pii/B9780128194430000118", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/B9780128194430000118", "snippet": "We have evaluated our system using the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) score . In each configuration the <b>BLEU</b> score of the translation scores are <b>different</b>, and Table 11.7 shows the <b>BLEU</b> score for each <b>different</b> configuration. The same mechanism is used to evaluate English to other 5 Indian language NMT systems as well. The <b>BLEU</b> scores ...", "dateLastCrawled": "2021-12-20T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Optimization of paraphrase generation and identification using language ...", "url": "https://www.sciencedirect.com/science/article/pii/S2667096821000185", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2667096821000185", "snippet": "<b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>): <b>BLEU</b> counts the matching N-grams in the generated translation to N-grams in the gold or reference text. Here the unigram is token-wise and the bi-gram is word pair. To penalize a generated translation or paraphrase which generates a lot of reasonable words, the n-gram counting is modified. In this paper, BLUE-1/2/3/4 is used for <b>evaluation</b>.", "dateLastCrawled": "2022-01-04T11:34:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language <b>Evaluation</b> | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) #language. A score between 0.0 and 1.0, inclusive, indicating the quality of a translation between two human languages (for example, between English and Russian). A <b>BLEU</b> score of 1.0 indicates a perfect translation; a <b>BLEU</b> score of 0.0 indicates a terrible translation. C. causal language model. #language. Synonym for unidirectional language model. See bidirectional language model to contrast different directional approaches in language modeling. crash ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Evaluation</b> of an <b>NLP</b> model \u2014 latest benchmarks | by Ria Kulshrestha ...", "url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>evaluation</b>-of-an-<b>nlp</b>-model-latest-benchmarks-90fd8ce6fae5", "snippet": "<b>BLEU</b> Score \u2014 <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>. As the name suggests, it was originally used to evaluate translations from one language to another. How to calculate <b>BLEU</b> score? Calculating unigram precision: Step 1: Look at each word in the output sentence and assign it a score of 1 if it shows up in any of the reference sentences and 0 if it doesn\u2019t. Step 2: Normalize that count, so that it\u2019s always between 0 and 1, by dividing the number of words that showed up in one of the reference ...", "dateLastCrawled": "2022-01-28T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Natrual language processing basic concepts - language model - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "<b>BLEU</b> stands for <b>bilingual</b> <b>evaluation</b> <b>understudy</b>. It&#39;s an automatic metric to evaluate how close a sequence of text generated by a language model is to a reference. At first, it&#39;s used to evaluate the quality of <b>machine</b> translation text. Now other natural language processing tasks such as task-oriented dialogue generation adopt it as well. For a reference &quot;The man returned to the store&quot;, a generated text &quot;the the man the&quot; would get a BLUE score as below. For each word in the generated text ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) pathak2019.pdf | Aditya Kumar Pathak and Priyankit Acharya ...", "url": "https://www.academia.edu/38228943/pathak2019_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38228943/pathak2019_pdf", "snippet": "The standard metric people are using for <b>evaluation</b> of MT systems is <b>BLEU</b> score.<b>Bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) is the algorithm to determine the quality of text translated by a <b>machine</b> translation. Quality is the comparison between <b>machine</b>-translated output to that of human-generated output; the closer <b>machine</b> translation is to human-generated translation, the better is the <b>BLEU</b> score. <b>BLEU</b> score is a n-gram overlap of <b>machine</b> translation to that of reference translation.<b>BLEU</b> \u00bc min ...", "dateLastCrawled": "2021-02-16T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "9.7. <b>Sequence</b> to <b>Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>), though originally proposed for evaluating <b>machine</b> translation results [Papineni et al., 2002], has been extensively used in measuring the quality of output sequences for different applications.", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The <b>Computational</b> Limits of Deep <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-computational-limits-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>computational</b>-limits-of-deep-<b>learning</b>", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) [papineni2002bleu] score is a metric for translation and computes the similarity between human translation and <b>machine</b> translation based on n-gram. An n-gram is a continuous sequence of n items from a given text. The score is based on precision, brevity penalty, and clipping. The modified n-gram precision ...", "dateLastCrawled": "2022-01-28T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Sequence Models - Deep <b>Learning</b> Specialization 5 - Yuet&#39;s Blog", "url": "https://yestinyang.github.io/2018/02/19/DLS-5-Sequence-Models.html", "isFamilyFriendly": true, "displayUrl": "https://yestinyang.github.io/2018/02/19/DLS-5-Sequence-Models.html", "snippet": "<b>Bleu</b> Score: <b>bilingual</b> <b>evaluation</b> <b>understudy</b>. Evaluate \u2018accuracy\u2019 of a model predicting multiply equally good answers, being a substitute for human evaluating each output Attention Model. Counter the problem of long sentence, which requires the ability of memory but not badly need a NN to do this kind of job. Instead of \u2018remembering\u2019 the ...", "dateLastCrawled": "2022-01-22T18:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Arti cial Intelligence Master Thesis", "url": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "snippet": "2:87 <b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>) score over the baseline; attention model, for German-English translation, and 0:34 <b>BLEU</b> score improvement for Catalan-Spanish trans-lation. Keywords <b>Machine</b> <b>Learning</b>, Deep <b>Learning</b>, Natural Language Processing, Neural <b>Machine</b> Transla-tion", "dateLastCrawled": "2021-12-27T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b> translation from text to sign language: a systematic review ...", "url": "https://link.springer.com/article/10.1007/s10209-021-00823-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10209-021-00823-1", "snippet": "The SMT component of the system uses MOSES for generating a language model and decodes the input sentence. The approach uses the <b>BLEU</b> metric for <b>evaluation</b> and reports the scores as <b>BLEU</b>-4 12.64% <b>BLEU</b>-3 19.28% <b>BLEU</b>-2 31.48% <b>BLEU</b>-1 53.17%. The results reported are satisfactory; however, the system needs a virtual avatar tool for completeness.", "dateLastCrawled": "2022-01-30T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Evaluation of machine translation systems and related procedures</b>", "url": "https://www.researchgate.net/publication/326320090_Evaluation_of_machine_translation_systems_and_related_procedures", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/326320090_<b>Evaluation</b>_of_<b>machine</b>_translation...", "snippet": "<b>Evaluation of machine translation systems and related procedures</b>. June 2018 ; Journal of Engineering and Applied Sciences 13(12):3961-3972; Project: <b>Machine</b> <b>learning</b>; Authors: Musatafa Albadr ...", "dateLastCrawled": "2022-01-15T04:04:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bleu (bilingual evaluation understudy))  is like +(machine learning algorithm accuracy on two different languages)", "+(bleu (bilingual evaluation understudy)) is similar to +(machine learning algorithm accuracy on two different languages)", "+(bleu (bilingual evaluation understudy)) can be thought of as +(machine learning algorithm accuracy on two different languages)", "+(bleu (bilingual evaluation understudy)) can be compared to +(machine learning algorithm accuracy on two different languages)", "machine learning +(bleu (bilingual evaluation understudy) AND analogy)", "machine learning +(\"bleu (bilingual evaluation understudy) is like\")", "machine learning +(\"bleu (bilingual evaluation understudy) is similar\")", "machine learning +(\"just as bleu (bilingual evaluation understudy)\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be thought of as\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be compared to\")"]}