{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Epsilon</b>-<b>greedy</b> babbling | Request PDF", "url": "https://www.researchgate.net/publication/324256720_Epsilon-greedy_babbling", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324256720_<b>Epsilon</b>-<b>greedy</b>_babbling", "snippet": "<b>Epsilon</b> <b>greedy</b> is an important and widely applied <b>policy</b>-based exploration method in reinforcement learning and has also ... [Show full abstract] been employed to improve ACO algorithms as the ...", "dateLastCrawled": "2021-12-17T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Improving</b> the efficiency of reinforcement learning for a spacecraft ...", "url": "https://link.springer.com/article/10.1007/s11081-021-09687-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11081-021-09687-z", "snippet": "In \\(\\<b>epsilon</b>\\)-<b>greedy</b> policies, the amount of exploration an agent does is controlled by the parameter \\(\\<b>epsilon</b>\\), where \\ (0 \\le \\<b>epsilon</b> \\le 1\\). At each timestep the agent will take a random action with probability \\(\\<b>epsilon</b>\\), otherwise it takes a <b>greedy</b> action. Usually, random actions are sampled with equal probability from the action space, but this can be adjusted in cases where random actions could be detrimental to the agent. It is sensible for an agent to explore <b>more</b> at the ...", "dateLastCrawled": "2021-12-06T11:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Epoch-<b>Greedy</b> algorithm for contextual multi-armed bandits", "url": "https://www.researchgate.net/publication/228658511_The_Epoch-Greedy_algorithm_for_contextual_multi-armed_bandits", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228658511_The_Epoch-<b>Greedy</b>_algorithm_for...", "snippet": "Epoch-<b>Greedy</b> has the following prop-erties: 1. No knowledge of a time horizon T is necessary. 2. The regret incurred by Epoch-<b>Greedy</b> is controlled by a sample complexity bound for a hypothesis ...", "dateLastCrawled": "2022-01-31T14:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Recent advances in leveraging human guidance for sequential decision ...", "url": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "snippet": "A simple strategy (\\(\\<b>epsilon</b>\\)-<b>greedy</b>) chooses a random action with probability \\(\\<b>epsilon</b>\\) and chooses the <b>greedy</b> action (the action with the highest Q value) with probability \\(1-\\<b>epsilon</b>\\) . A <b>more</b> sophisticated strategy uses a Boltzmann distribution for selecting actions based on the current estimate of Q function : $$\\begin{aligned} P(a|s, Q, \\tau ) = \\frac{e^{Q(s,a)}/\\tau }{\\sum _{a&#39; \\in \\mathcal {A}} e^{Q(s,a&#39;)}/\\tau } \\end{aligned}$$ (4) where \\(\\tau\\) is a temperature constant ...", "dateLastCrawled": "2022-01-31T18:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Deep reinforcement learning in transportation research: A review ...", "url": "https://www.sciencedirect.com/science/article/pii/S2590198221001317", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2590198221001317", "snippet": ", the action <b>selection</b> uses <b>greedy</b> <b>policy</b> based on the Q-network with parameter \u03b8 ... DRL can be used for problems that require optimization of multiple factors through a common reward signal, which helps <b>improving</b> learning efficiency. In real-world problems, successful completion of a task may depend on several factors with a need for parameterization for the reward function. Weights of these parameters should be specified, but would involve intense tuning if done manually. For example ...", "dateLastCrawled": "2022-01-21T21:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Computational analysis and prediction of lysine malonylation sites by ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6954445/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6954445", "snippet": "<b>More</b> specifically, LightGBM outperformed all other ML methods when trained with the H. sapiens <b>data</b> set, which contains a larger number of samples, indicating that LightGBM could be suitable for large <b>data</b> modeling. Moreover, we demonstrated that the integration of single ML-based models into ensemble models could further improve the prediction performance. Then, using the optimized ensemble models, we developed an online predictor, named kmal-sp, which we have made publicly accessible at", "dateLastCrawled": "2021-12-17T00:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Reinforcement Learning: Summary and Review | Bill Mei", "url": "https://billmei.net/books/reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://billmei.net/books/reinforcement-learning", "snippet": "Because you have <b>more</b> <b>data</b> to advantage behaviors from the majority group, and you disadvantage behaviors from the minority group. This is somewhat mitigated by importance sampling and the off-<b>policy</b> behavior <b>policy</b>, described in a later chapter. Exercise 3.1. Task 1: Write a bestselling novel. Actions: What key to press next on the keyboard; States: The keys that have already been pressed; Rewards: Sales ($) of books (or views, downloads, etc.) Task 2: Gardening/Farming. Actions: Location ...", "dateLastCrawled": "2022-01-05T11:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Which machine learning algorithm should I use? - The SAS <b>Data</b> Science Blog", "url": "https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/", "isFamilyFriendly": true, "displayUrl": "https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning...", "snippet": "Once you obtain some results and become familiar with the <b>data</b>, you may spend <b>more</b> time using <b>more</b> sophisticated algorithms to strengthen your understanding of the <b>data</b>, hence further <b>improving</b> the results. Even in this stage, the best algorithms might not be the methods that have achieved the highest reported accuracy, as an algorithm usually requires careful tuning and extensive training to obtain its best achievable performance. When to use specific algorithms. Looking <b>more</b> closely at ...", "dateLastCrawled": "2022-02-01T17:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "New submissions for Tue, 19 Oct 21 \u00b7 Issue #154 \u00b7 zoq/arxiv-updates ...", "url": "https://github.com/zoq/arxiv-updates/issues/154", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zoq/arxiv-updates/issues/154", "snippet": "For <b>more</b> general matroid constraints, the GSEMO can achieve the asymptotically optimal polynomial-time approximation ratio, $1/2-\\<b>epsilon</b>/(4n)$. Furthermore, when the objective function (i.e., a linear combination of quality and diversity) changes dynamically, the GSEMO can maintain this approximation ratio in polynomial running time, addressing the open question proposed by Borodin et al. This also theoretically shows the superiority of EAs over local search for solving dynamic optimization ...", "dateLastCrawled": "2022-01-31T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "When is the first spurious variable selected by sequential regression ...", "url": "https://academic.oup.com/biomet/article-abstract/105/3/517/5046346", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/biomet/article-abstract/105/3/517/5046346", "snippet": "Compared with forward stepwise regression, the lasso and least angle regression are considered less <b>greedy</b> because at each step they <b>gradually</b> blend in a new variable instead of adding it in a discontinuous manner (Efron et al., 2004). Forward stepwise regression selects the predictor having the largest absolute correlation with the residual vector and then takes a large step in the direction of the selected predictor, whereas the other methods proceed along a direction equiangular between ...", "dateLastCrawled": "2022-01-11T05:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Epsilon</b>-<b>greedy</b> babbling | Request PDF", "url": "https://www.researchgate.net/publication/324256720_Epsilon-greedy_babbling", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324256720_<b>Epsilon</b>-<b>greedy</b>_babbling", "snippet": "<b>Epsilon</b> <b>greedy</b> is an important and widely applied <b>policy</b>-based exploration method in reinforcement learning and has also ... [Show full abstract] been employed to improve ACO algorithms as the ...", "dateLastCrawled": "2021-12-17T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A trust-aware task allocation method using deep q-learning for ...", "url": "https://hcis-journal.springeropen.com/articles/10.1186/s13673-019-0187-4", "isFamilyFriendly": true, "displayUrl": "https://hcis-journal.springeropen.com/articles/10.1186/s13673-019-0187-4", "snippet": "Therefore, the \\(\\<b>epsilon</b>\\)-<b>greedy</b> <b>policy</b> is an improvement, \\(v_{\\pi &#39;}(s) \\ge v_{\\pi }(s)\\). To maintain a good balance of exploration and exploitation, a suitable learning parameter should be selected for the \\(\\<b>epsilon</b>\\)-<b>greedy</b> strategy. In the early training time, a <b>more</b> random <b>policy</b> should be used to encourage initial exploration, and as ...", "dateLastCrawled": "2021-12-27T12:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Real-time operation of distribution network: A deep reinforcement ...", "url": "https://www.sciencedirect.com/science/article/pii/S2213138821008559", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2213138821008559", "snippet": "This module is for <b>improving</b> the action <b>selection</b> based on input state information by updating the weights of DNNs. ... The <b>epsilon</b>-<b>greedy</b> <b>policy</b> is used to select actions during the training process. This <b>policy</b> helps the learning agent make a trade-off between exploration and exploitation by adjusting the value of <b>epsilon</b>. The value of <b>epsilon</b> decreases <b>gradually</b>, as shown in Fig. 6(a). This helps the learning agent to focus on exploiting its knowledge about the environment at the end of ...", "dateLastCrawled": "2022-01-17T06:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The Epoch-<b>Greedy</b> algorithm for contextual multi-armed bandits", "url": "https://www.researchgate.net/publication/228658511_The_Epoch-Greedy_algorithm_for_contextual_multi-armed_bandits", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228658511_The_Epoch-<b>Greedy</b>_algorithm_for...", "snippet": "Epoch-<b>Greedy</b> has the following prop-erties: 1. No knowledge of a time horizon T is necessary. 2. The regret incurred by Epoch-<b>Greedy</b> is controlled by a sample complexity bound for a hypothesis ...", "dateLastCrawled": "2022-01-31T14:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Sensors | Free Full-Text | Double Deep Q-Learning and Faster R-CNN ...", "url": "https://www.mdpi.com/1424-8220/21/4/1468/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/21/4/1468/htm", "snippet": "The process in which the agent consistently determines the maximum Q-value for navigation is called the <b>epsilon</b> <b>greedy</b> strategy . This work also includes detecting and classifying obstacles along its way while navigating. It takes <b>data</b> from obstacles in rocky, rough and bumpy surfaces through the sensor. We have mainly used a vision sensor to implement the mentioned proposal. The sensory <b>data</b> are fed to the agent, and the decision is taken based on the fed conditions. Faster R-CNN is ...", "dateLastCrawled": "2021-11-29T07:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "New submissions for Tue, 19 Oct 21 \u00b7 Issue #154 \u00b7 zoq/arxiv-updates ...", "url": "https://github.com/zoq/arxiv-updates/issues/154", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zoq/arxiv-updates/issues/154", "snippet": "For <b>more</b> general matroid constraints, the GSEMO can achieve the asymptotically optimal polynomial-time approximation ratio, $1/2-\\<b>epsilon</b>/(4n)$. Furthermore, when the objective function (i.e., a linear combination of quality and diversity) changes dynamically, the GSEMO can maintain this approximation ratio in polynomial running time, addressing the open question proposed by Borodin et al. This also theoretically shows the superiority of EAs over local search for solving dynamic optimization ...", "dateLastCrawled": "2022-01-31T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Improving</b> the efficiency of reinforcement learning for a spacecraft ...", "url": "https://link.springer.com/article/10.1007/s11081-021-09687-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11081-021-09687-z", "snippet": "Automated hyperparameter <b>selection</b> is a very <b>similar</b> problem to meta-learning since it often uses a higher level learning procedure to \u201ctrain\u201d the hyperparameters of the lower level algorithm. These automated methods use a variety of intelligent approaches such as evolutionary computation (Schweighofer and Doya 2003; Young et al. 2015) and Bayesian optimisation methods (Barsce et al. 2017; Bergstra et al. 2011). This work aims to find ways of speeding up learning times in a sample ...", "dateLastCrawled": "2021-12-06T11:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reinforcement Learning: Summary and Review | Bill Mei", "url": "https://billmei.net/books/reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://billmei.net/books/reinforcement-learning", "snippet": "Because you have <b>more</b> <b>data</b> to advantage behaviors from the majority group, and you disadvantage behaviors from the minority group. This is somewhat mitigated by importance sampling and the off-<b>policy</b> behavior <b>policy</b>, described in a later chapter. Exercise 3.1. Task 1: Write a bestselling novel. Actions: What key to press next on the keyboard; States: The keys that have already been pressed; Rewards: Sales ($) of books (or views, downloads, etc.) Task 2: Gardening/Farming. Actions: Location ...", "dateLastCrawled": "2022-01-05T11:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Which machine learning algorithm should I use? - The SAS <b>Data</b> Science Blog", "url": "https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/", "isFamilyFriendly": true, "displayUrl": "https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning...", "snippet": "Clustering: Grouping a set of <b>data</b> examples so that examples in one group (or one cluster) are <b>more</b> <b>similar</b> (according to some criteria) than those in other groups. This is often used to segment the whole dataset into several groups. Analysis can be performed in each group to help users to find intrinsic patterns. Dimension reduction: Reducing the number of variables under consideration. In many applications, the raw <b>data</b> have very high dimensional features and some features are redundant or ...", "dateLastCrawled": "2022-02-01T17:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "When is the first spurious variable selected by sequential regression ...", "url": "https://academic.oup.com/biomet/article-abstract/105/3/517/5046346", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/biomet/article-abstract/105/3/517/5046346", "snippet": "Compared with forward stepwise regression, the lasso and least angle regression are considered less <b>greedy</b> because at each step they <b>gradually</b> blend in a new variable instead of adding it in a discontinuous manner (Efron et al., 2004). Forward stepwise regression selects the predictor having the largest absolute correlation with the residual vector and then takes a large step in the direction of the selected predictor, whereas the other methods proceed along a direction equiangular between ...", "dateLastCrawled": "2022-01-11T05:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Epsilon</b>-<b>greedy</b> babbling | Request PDF", "url": "https://www.researchgate.net/publication/324256720_Epsilon-greedy_babbling", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324256720_<b>Epsilon</b>-<b>greedy</b>_babbling", "snippet": "To select and combine low-level heuristics (LLHs) during the evolutionary procedure, this paper also proposes an adaptive <b>epsilon</b>-<b>greedy</b> <b>selection</b> strategy. The proposed hyper-heuristic <b>can</b> solve ...", "dateLastCrawled": "2021-12-17T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Recent advances in leveraging human guidance for sequential decision ...", "url": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10458-021-09514-w", "snippet": "A simple strategy (\\(\\<b>epsilon</b>\\)-<b>greedy</b>) chooses a random action with probability \\(\\<b>epsilon</b>\\) and chooses the <b>greedy</b> action (the action with the highest Q value) with probability \\(1-\\<b>epsilon</b>\\) . A <b>more</b> sophisticated strategy uses a Boltzmann distribution for selecting actions based on the current estimate of Q function : $$\\begin{aligned} P(a|s, Q, \\tau ) = \\frac{e^{Q(s,a)}/\\tau }{\\sum _{a&#39; \\in \\mathcal {A}} e^{Q(s,a&#39;)}/\\tau } \\end{aligned}$$ (4) where \\(\\tau\\) is a temperature constant ...", "dateLastCrawled": "2022-01-31T18:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Simple Threshold Rules Solve Explore/Exploit</b> ... - <b>Wiley Online Library</b>", "url": "https://onlinelibrary.wiley.com/doi/10.1111/cogs.12817", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/cogs.12817", "snippet": "The first stage <b>can</b> <b>be thought</b> of as gathering information about the range of possible values and setting a threshold (the highest value seen so far) for the second stage. This optimal strategy gives the searcher a 37% probability of selecting the best applicant. To see how and how well people actually solve this problem, Seale and Rapoport presented participants with fixed-length sequences of values (using ranks rather than actual values so that distributions could not be learned) and had ...", "dateLastCrawled": "2022-01-23T01:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "CS379C 2019 Class Discussion Notes", "url": "https://web.stanford.edu/class/cs379c/archive/2019/class_messages_listing/index.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs379c/archive/2019/class_messages_listing/index.html", "snippet": "The BG <b>can</b> <b>be thought</b> of as implementing such a <b>policy</b>, and the PFC as adapting that <b>policy</b>. Our approach to HRL involves the use of a composite <b>policy</b> comprised of a core-competency <b>policy</b> that is common across all contexts and a subroutine implemented as a <b>policy</b> that is specific to that subroutine and that is adapted by the PFC to suit the current circumstances. Such adapted contexts <b>can</b> <b>be thought</b> of as altering cortical landscape in which the basal ganglia normally operate.", "dateLastCrawled": "2022-02-03T14:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Google Translate", "url": "https://translate.google.ca/", "isFamilyFriendly": true, "displayUrl": "https://translate.google.ca", "snippet": "Google&#39;s free service instantly translates words, phrases, and web pages between English and over 100 other languages.", "dateLastCrawled": "2022-02-02T22:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mohamad H Danesh | Reinforcement Learning Key Papers Keynotes", "url": "https://modanesh.github.io/blog/RL-key-papers-key-notes/", "isFamilyFriendly": true, "displayUrl": "https://modanesh.github.io/blog/RL-key-papers-key-notes", "snippet": "On-<b>policy</b> learning: One of the simplest ways to learn a neural network <b>policy</b> is to collect a batch of behavior wherein the <b>policy</b> is used to act in the world, and then compute and apply a <b>policy</b> gradient update from this <b>data</b>. This is referred to as on-<b>policy</b> learning because all of the updates are made using <b>data</b> that was collected from the trajectory distribution induced by the current <b>policy</b> of the agent.", "dateLastCrawled": "2022-01-20T09:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Reinforcement Learning: Summary and Review | Bill Mei", "url": "https://billmei.net/books/reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://billmei.net/books/reinforcement-learning", "snippet": "Because you have <b>more</b> <b>data</b> to advantage behaviors from the majority group, and you disadvantage behaviors from the minority group. This is somewhat mitigated by importance sampling and the off-<b>policy</b> behavior <b>policy</b>, described in a later chapter. Exercise 3.1. Task 1: Write a bestselling novel. Actions: What key to press next on the keyboard; States: The keys that have already been pressed; Rewards: Sales ($) of books (or views, downloads, etc.) Task 2: Gardening/Farming. Actions: Location ...", "dateLastCrawled": "2022-01-05T11:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Rich <b>Sutton</b>&#39;s Publications - Richard S. <b>Sutton</b>", "url": "http://incompleteideas.net/publications.html", "isFamilyFriendly": true, "displayUrl": "incompleteideas.net/publications.html", "snippet": "Off-<b>policy</b> techniques, such as <b>Greedy</b>-GQ, enable a target <b>policy</b> to be learned while following and obtaining <b>data</b> from another (behavior) <b>policy</b>. For many problems, however, actor-critic methods are <b>more</b> practical than action value methods (like <b>Greedy</b>-GQ) because they explicitly represent the <b>policy</b>; consequently, the <b>policy</b> <b>can</b> be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-<b>policy</b> learning ...", "dateLastCrawled": "2022-01-30T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Track: <b>Poster Session 3</b>", "url": "https://nips.cc/virtual/2020/session/21244", "isFamilyFriendly": true, "displayUrl": "https://nips.cc/virtual/2020/session/21244", "snippet": "However, in practice, offline RL presents a major challenge, and standard off-<b>policy</b> RL methods <b>can</b> fail due to overestimation of values induced by the distributional shift between the dataset and the learned <b>policy</b>, especially when training on complex and multi-modal <b>data</b> distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a <b>policy</b> under this Q-function lower ...", "dateLastCrawled": "2022-01-18T04:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "GitHub - <b>rvsrnvsn/ICML2019</b>: Notes from ICML 2019 (Jun 10-15, Long Beach ...", "url": "https://github.com/rvsrnvsn/ICML2019", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/rvsrnvsn/ICML2019", "snippet": "Real off-<b>policy</b> <b>data</b> is always preferable to model-generated on-<b>policy</b> <b>data</b>! So why use model at all? Empirical model generalization motivates model usage ; MBPO uses short model rollouts to give large benefits to <b>policy</b> optimization, avoids model exploitation, and scales to long-horizon tasks; Personalized Visualization of the Impact of Climate Change ** Yoshua Bengio. Environmental applications of ML in transportation, construction, and industry Example: synthesizing new materials for ...", "dateLastCrawled": "2021-12-12T14:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Epsilon</b>-<b>greedy</b> babbling | Request PDF", "url": "https://www.researchgate.net/publication/324256720_Epsilon-greedy_babbling", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324256720_<b>Epsilon</b>-<b>greedy</b>_babbling", "snippet": "To select and combine low-level heuristics (LLHs) during the evolutionary procedure, this paper also proposes an adaptive <b>epsilon</b>-<b>greedy</b> <b>selection</b> strategy. The proposed hyper-heuristic <b>can</b> solve ...", "dateLastCrawled": "2021-12-17T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A trust-aware task allocation method using deep q-learning for ...", "url": "https://hcis-journal.springeropen.com/articles/10.1186/s13673-019-0187-4", "isFamilyFriendly": true, "displayUrl": "https://hcis-journal.springeropen.com/articles/10.1186/s13673-019-0187-4", "snippet": "Therefore, the \\(\\<b>epsilon</b>\\)-<b>greedy</b> <b>policy</b> is an improvement, \\(v_{\\pi &#39;}(s) \\ge v_{\\pi }(s)\\). To maintain a good balance of exploration and exploitation, a suitable learning parameter should be selected for the \\(\\<b>epsilon</b>\\)-<b>greedy</b> strategy. In the early training time, a <b>more</b> random <b>policy</b> should be used to encourage initial exploration, and as ...", "dateLastCrawled": "2021-12-27T12:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Real-time operation of distribution network: A deep reinforcement ...", "url": "https://www.sciencedirect.com/science/article/pii/S2213138821008559", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2213138821008559", "snippet": "The <b>epsilon</b>-<b>greedy</b> <b>policy</b> is used to select actions during the training process. This <b>policy</b> helps the learning agent make a trade-off between exploration and exploitation by adjusting the value of <b>epsilon</b>. The value of <b>epsilon</b> decreases <b>gradually</b>, as shown in Fig. 6(a). This helps the learning agent to focus on exploiting its knowledge about the environment at the end of the learning process. Download : Download high-res image (702KB) Download : Download full-size image; Fig. 6. Offline ...", "dateLastCrawled": "2022-01-17T06:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Improving</b> the efficiency of reinforcement learning for a spacecraft ...", "url": "https://link.springer.com/article/10.1007/s11081-021-09687-z", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11081-021-09687-z", "snippet": "So to learn the optimal <b>policy</b> we <b>can</b> learn the optimal value function. In practise, this method is impractical due to the requirement of a perfect model which <b>can</b> estimate all possible state transitions. Instead, from the optimal action-value function we <b>can</b> also derive an optimal <b>policy</b> which selects the action with maximal action-value in a given state. This is referred to as taking <b>greedy</b> actions\u2014only choosing actions with the highest expected value. An alternative to learning value ...", "dateLastCrawled": "2021-12-06T11:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Epoch-<b>Greedy</b> algorithm for contextual multi-armed bandits", "url": "https://www.researchgate.net/publication/228658511_The_Epoch-Greedy_algorithm_for_contextual_multi-armed_bandits", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228658511_The_Epoch-<b>Greedy</b>_algorithm_for...", "snippet": "We present Epoch-<b>Greedy</b>, an algorithm for contextual multi-armed bandits (also known as bandits with side information). Epoch-<b>Greedy</b> has the following prop-erties: 1. No knowledge of a time ...", "dateLastCrawled": "2022-01-31T14:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Simple Threshold Rules Solve Explore/Exploit</b> ... - <b>Wiley Online Library</b>", "url": "https://onlinelibrary.wiley.com/doi/10.1111/cogs.12817", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1111/cogs.12817", "snippet": "We first describe the models and then report their performance on the card search task and their fit to participants&#39; <b>data</b>. 5.1 Strategies <b>compared</b>. For all of the strategies below we describe a stochastic model used to find the parameters that best fit the model to participants&#39; <b>data</b>. We also report the performance on the card search task of the corresponding deterministic form of each model (except the <b>epsilon</b>-<b>greedy</b> baseline). 5.1.1 Random baseline models. A standard type of random model ...", "dateLastCrawled": "2022-01-23T01:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "New submissions for Tue, 26 Oct 21 \u00b7 Issue #159 \u00b7 zoq/arxiv-updates ...", "url": "https://github.com/zoq/arxiv-updates/issues/159", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zoq/arxiv-updates/issues/159", "snippet": "We train our RL <b>policy</b> using deep Q-learning, and show that this <b>policy</b> <b>can</b> result in significantly accelerated convergence (up to a 59% reduction in the number of iterations <b>compared</b> to existing, curvature-informed penalty parameter <b>selection</b> methods). Furthermore, we show that our RL <b>policy</b> demonstrates promise for generalizability, performing well under unseen loading schemes as well as under unseen losses of lines and generators (up to a 50% reduction in iterations). This work thus ...", "dateLastCrawled": "2022-01-28T19:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "New submissions for Tue, 19 Oct 21 \u00b7 Issue #154 \u00b7 zoq/arxiv-updates ...", "url": "https://github.com/zoq/arxiv-updates/issues/154", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/zoq/arxiv-updates/issues/154", "snippet": "For <b>more</b> general matroid constraints, the GSEMO <b>can</b> achieve the asymptotically optimal polynomial-time approximation ratio, $1/2-\\<b>epsilon</b>/(4n)$. Furthermore, when the objective function (i.e., a linear combination of quality and diversity) changes dynamically, the GSEMO <b>can</b> maintain this approximation ratio in polynomial running time, addressing the open question proposed by Borodin et al. This also theoretically shows the superiority of EAs over local search for solving dynamic optimization ...", "dateLastCrawled": "2022-01-31T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "When is the first spurious variable selected by sequential regression ...", "url": "https://academic.oup.com/biomet/article-abstract/105/3/517/5046346", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/biomet/article-abstract/105/3/517/5046346", "snippet": "<b>Compared</b> with forward stepwise regression, the lasso and least angle regression are considered less <b>greedy</b> because at each step they <b>gradually</b> blend in a new variable instead of adding it in a discontinuous manner (Efron et al., 2004). Forward stepwise regression selects the predictor having the largest absolute correlation with the residual vector and then takes a large step in the direction of the selected predictor, whereas the other methods proceed along a direction equiangular between ...", "dateLastCrawled": "2022-01-11T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Reinforcement Learning: Summary and Review | Bill Mei", "url": "https://billmei.net/books/reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://billmei.net/books/reinforcement-learning", "snippet": "Because you have <b>more</b> <b>data</b> to advantage behaviors from the majority group, and you disadvantage behaviors from the minority group. This is somewhat mitigated by importance sampling and the off-<b>policy</b> behavior <b>policy</b>, described in a later chapter. Exercise 3.1. Task 1: Write a bestselling novel. Actions: What key to press next on the keyboard; States: The keys that have already been pressed; Rewards: Sales ($) of books (or views, downloads, etc.) Task 2: Gardening/Farming. Actions: Location ...", "dateLastCrawled": "2022-01-05T11:49:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement <b>Learning</b> | Ioannis Anifantakis | Analytics Vidhya", "url": "https://medium.com/analytics-vidhya/reinforcement-learning-basic-understanding-4fcb91ba4e4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/reinforcement-<b>learning</b>-basic-understanding-4fcb91ba4e4", "snippet": "The <b>greedy</b>-<b>policy</b> is always following the directions of the q-table blindly, while <b>epsilon</b>-<b>greedy</b>-<b>policy</b> follows mostly the q-table, but allows for some \u201crandom choice\u201d now and then to see how ...", "dateLastCrawled": "2021-08-02T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Why <b>Machine Learning Is A Metaphor For</b> Life \u2013 Adit Deshpande ...", "url": "https://adeshpande3.github.io/Why-Machine-Learning-is-a-Metaphor-For-Life", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/Why-<b>Machine-Learning-is-a-Metaphor-For</b>-Life", "snippet": "Another cool <b>analogy</b> is that of the <b>epsilon</b> <b>greedy</b> <b>policy</b>. This is a term used in reinforcement <b>learning</b> to fight the problem of exploration vs exploitation. The basic idea is that the RL agent will take a random action (instead of the optimal action according to its current <b>policy</b>) with probability \u03b5, in hope of searching a larger area of the state space, and eventually getting a better reward.", "dateLastCrawled": "2022-01-31T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Multi-Armed <b>Bandits in Python: Epsilon Greedy, UCB1, Bayesian UCB</b>, and ...", "url": "https://jamesrledoux.com/algorithms/bandit-algorithms-epsilon-ucb-exp-python/", "isFamilyFriendly": true, "displayUrl": "https://jamesrledoux.com/algorithms/bandit-algorithms-<b>epsilon</b>-ucb-exp-python", "snippet": "Like the name suggests, the <b>epsilon</b> <b>greedy</b> algorithm follows a <b>greedy</b> arm selection <b>policy</b>, selecting the best-performing arm at each time step. However, \\(\\<b>epsilon</b>\\) percent of the time, it will go off-<b>policy</b> and choose an arm at random. The value of \\(\\<b>epsilon</b>\\) determines the fraction of the time when the algorithm explores available arms, and exploits the ones that have performed the best historically the rest of the time.", "dateLastCrawled": "2022-02-02T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reinforcement Machine Learning for Effective Clinical Trials</b>", "url": "https://www.infoq.com/articles/multi-armed-bandits-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.infoq.com/articles/multi-armed-bandits-reinforcement-<b>learning</b>", "snippet": "Now, we will run the same test using an <b>epsilon</b> <b>greedy</b> <b>policy</b>. We will explore the arms 20% of time (<b>epsilon</b> = 0.2) and rest of time we will pull the arm with the maximum rewards rate \u2013 that is ...", "dateLastCrawled": "2022-01-19T01:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Reinforcement <b>Learning</b>. Reinforcement <b>learning</b> is type of\u2026 | by Mehul ...", "url": "https://medium.com/@mehulved1503/reinforcement-learning-e743bcd00962", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@mehulved1503/reinforcement-<b>learning</b>-e743bcd00962", "snippet": "Reinforcement <b>Learning</b>:<b>Epsilon</b>-<b>Greedy</b> Strategy. Estimate the value from each action as the long term average Q(a)=(r_1+r_2+\u2026+r_k)/k where k is the number of occurrences of action a.; The <b>greedy</b> ...", "dateLastCrawled": "2021-08-04T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Multi-armed bandit</b> - Pain is inevitable. Suffering is optional.", "url": "https://changyaochen.github.io/multi-armed-bandit-mar-2020/", "isFamilyFriendly": true, "displayUrl": "https://changyaochen.github.io/<b>multi-armed-bandit</b>-mar-2020", "snippet": "You can play the 10-armed bandit with <b>greedy</b>, \\(\\<b>epsilon</b>\\)-<b>greedy</b>, and UCB polices here. For details, read on. For details, read on. Like many people, when I first learned the concept of <b>machine</b> <b>learning</b>, the first split made is to categorize the problems to supervised and unsupervised, a soundly complete grouping.", "dateLastCrawled": "2022-02-02T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Epsilon\u2013First Policies for Budget\u2013Limited Multi</b>-Armed Bandits", "url": "https://www.researchgate.net/publication/43334305_Epsilon-First_Policies_for_Budget-Limited_Multi-Armed_Bandits", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/43334305_<b>Epsilon</b>-First_Policies_for_Budget...", "snippet": "ploration <b>policy</b> and the reward\u2013cost ratio or dered <b>greedy</b> 1 A detailed survey of these algorithms can be found in An- donov , Poirriez, and Rajopadhye (2000).", "dateLastCrawled": "2021-12-09T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 7, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Multi-armed bandit</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Multi-armed_bandit", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Multi-armed_bandit</b>", "snippet": "Adaptive <b>epsilon</b>-<b>greedy</b> strategy based on Bayesian ensembles (<b>Epsilon</b>-BMC): An adaptive <b>epsilon</b> adaptation strategy for reinforcement <b>learning</b> similar to VBDE, with monotone convergence guarantees. In this framework, the <b>epsilon</b> parameter is viewed as the expectation of a posterior distribution weighting a <b>greedy</b> agent (that fully trusts the learned reward) and uniform <b>learning</b> agent (that distrusts the learned reward). This posterior is approximated using a suitable Beta distribution under ...", "dateLastCrawled": "2022-02-03T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Reinforcement <b>learning</b> algorithms seek to find a <b>policy</b> (i.e., optimal <b>policy</b>) that will yield more return to the agent than all other policies Bellman optimality equation For any state-action pair (s,a) at time t , the expected return is R_(t+1) (i.e. the expected reward we get from taking action a in state s ) + the maximum expected discounted return that can be achieved from any possible next state-action pair.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Reinforcement <b>learning</b> algorithms seek to find a <b>policy</b> (i.e., optimal <b>policy</b>) that will yield more return to the agent than all other policies Bellman optimality equation For any state-action pair (s,a) at time t , the expected return is R_(t+1) (i.e. the expected reward we get from taking action a in state s ) + the maximum expected discounted return that can be achieved from any possible next state-action pair.", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(epsilon greedy policy)  is like +(gradually improving selection by incorporating more data)", "+(epsilon greedy policy) is similar to +(gradually improving selection by incorporating more data)", "+(epsilon greedy policy) can be thought of as +(gradually improving selection by incorporating more data)", "+(epsilon greedy policy) can be compared to +(gradually improving selection by incorporating more data)", "machine learning +(epsilon greedy policy AND analogy)", "machine learning +(\"epsilon greedy policy is like\")", "machine learning +(\"epsilon greedy policy is similar\")", "machine learning +(\"just as epsilon greedy policy\")", "machine learning +(\"epsilon greedy policy can be thought of as\")", "machine learning +(\"epsilon greedy policy can be compared to\")"]}