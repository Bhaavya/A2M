{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Full article: <b>Causal Interpretations of Black-Box Models</b>", "url": "https://www.tandfonline.com/doi/full/10.1080/07350015.2019.1624293", "isFamilyFriendly": true, "displayUrl": "https://www.tandfonline.com/doi/full/10.1080/07350015.2019.1624293", "snippet": "<b>Causal</b> interpretation of <b>black</b>-<b>box</b> models, the main topic of this article, is another good example for the usefulness of <b>causal</b> <b>language</b> and theory for machine learning researchers. Pearl ( 2018 ) postulated fundamental impediments to today\u2019s machine learning algorithms and summarized \u201csparks\u201d from \u201cThe <b>Causal</b> Revolution\u201d that may help us circumvent them.", "dateLastCrawled": "2022-02-03T14:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>causal</b> framework for explaining the predictions of <b>black</b>-<b>box</b> sequence ...", "url": "https://dmelis.github.io/assets/publications/2017_blackbox_seq2seq_interpret/EMNLP2017_Causal.pdf", "isFamilyFriendly": true, "displayUrl": "https://dmelis.github.io/.../2017_<b>blackbox</b>_seq2seq_interpret/EMNLP2017_<b>Causal</b>.pdf", "snippet": "Proceedings of the 2017 Conference on Empirical Methods in Natural <b>Language</b> Processing, pages 412\u2013421 Copenhagen, Denmark, September 7\u201311, 2017. c 2017 Association for Computational Linguistics A <b>causal</b> framework for explaining the predictions of <b>black</b>-<b>box</b> sequence-to-sequence models David Alvarez-Melis and Tommi S. Jaakkola CSAIL, MIT {davidam, tommi}@csail.mit.edu Abstract We interpret the predictions of any <b>black</b>-<b>box</b> structured input-structured output <b>model</b> around a specic input ...", "dateLastCrawled": "2021-08-21T19:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>causal framework for explaining the predictions of black</b>-<b>box</b> sequence ...", "url": "https://people.csail.mit.edu/tommi/papers/AlvJaa_EMNLP2017.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.csail.mit.edu/tommi/papers/AlvJaa_EMNLP2017.pdf", "snippet": "related under the <b>black</b>-<b>box</b> <b>model</b>. <b>Causal</b> de-pendencies arise from analyzing perturbed ver-sions of inputs that are passed through the <b>black</b>-<b>box</b> <b>model</b>. Although such perturbations might be available in limited cases, we generate them auto-matically. For sentences, we adopt a variational autoencoder to produce semantically related sen-tence variations. The resulting inferred <b>causal</b> de-pendencies (interval estimates) form a dense bi-partite graph over tokens from which explanations can be ...", "dateLastCrawled": "2022-01-17T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "June 22: Yanai Elazar, <b>Causal</b> Attributions in <b>Language</b> Models ...", "url": "https://nlp.berkeley.edu/2021/06/16/june-22-yanai-elazar-causal-attributions-in-language-models/", "isFamilyFriendly": true, "displayUrl": "https://nlp.berkeley.edu/.../june-22-yanai-elazar-<b>causal</b>-attributions-in-<b>language</b>-<b>models</b>", "snippet": "In the first part of this talk, I\u2019ll propose a new interpretability method that takes inspiration from counterfactuals \u2013 what would have been the prediction if the <b>model</b> had not accessed certain information \u2013 and claim it is a more suitable method for asking <b>causal</b> questions about how certain attributes are used by models. In the second part, I\u2019ll talk about a different kind of probing that treats the <b>model</b> as a <b>black</b> <b>box</b> and uses cloze patterns to query the <b>model</b> for world knowledge ...", "dateLastCrawled": "2021-12-21T21:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Improving NLP with Causality. Potential uses of <b>Causal</b> Inference to ...", "url": "https://towardsdatascience.com/improving-nlp-with-causality-2dec1fa90b74", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/improving-nlp-with-<b>causal</b>ity-2dec1fa90b74", "snippet": "<b>Causal</b> <b>Model</b> Interpretability &amp; Explainability. While NLP models <b>like</b> BERT are currently \u201cinscrutable <b>black</b> boxes that are difficult to explain, there is a need for diagnosing errors and establishing trust with decision makers\u201d (Feder et al., (2021a).", "dateLastCrawled": "2022-02-03T11:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Review for NeurIPS paper: <b>Generative causal explanations of black</b>-<b>box</b> ...", "url": "https://papers.nips.cc/paper/2020/file/3a93a609b97ec0ab0ff5539eb79ef33a-Review.html", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2020/file/3a93a609b97ec0ab0ff5539eb79ef33a-Review.html", "snippet": "Summary and Contributions: This paper presents a generative <b>model</b> to &quot;explain&quot; any given <b>black</b>-<b>box</b> classifier and its training dataset. By &quot;explain&quot;, the authors mean that a hidden factor can be discovered to control or intervene in the output of the classifier. The discovery is based on a proposed maximization objective, which consists of two terms: 1) a proposed Information Flow that denotes the <b>causal</b> effect from the hidden factor to the classifier output and 2) a distribution similarity ...", "dateLastCrawled": "2021-09-29T17:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Unpacking the <b>Black</b> <b>Box</b> of Causality: Learning about <b>Causal</b> Mechanisms ...", "url": "https://www.cambridge.org/core/journals/american-political-science-review/article/unpacking-the-black-box-of-causality-learning-about-causal-mechanisms-from-experimental-and-observational-studies/9D2ACE9F784B99A30216D216FBF88553", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/american-political-science-review/article/...", "snippet": "The use of the potential outcomes framework is essential because it provides a formal <b>language</b> for understanding the counterfactual comparisons required to study <b>causal</b> mechanisms. As shown later, the conventional approaches based on structural equation models fail to recognize the key assumption behind <b>causal</b> mediation analysis. Potential Outcomes Framework. We first introduce the concept of potential outcomes, which has been used in the methodological literature as the formal framework of ...", "dateLastCrawled": "2022-01-12T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Pearl&#39;s Structural <b>Causal</b> Models: Assessing Causality from ...", "url": "https://blog.methodsconsultants.com/posts/pearl-causality/", "isFamilyFriendly": true, "displayUrl": "https://blog.methodsconsultants.com/posts/pearl-<b>causal</b>ity", "snippet": "The <b>black</b> <b>box</b> hides the answer we need if we want to develop effective rules that lead to socially desirable outcomes. Limitations of the Pearlian Weltanschauung At the same time, Pearl\u2019s dismissal of non-SCM approaches to modeling (potential outcomes, ML) are based on finding specific cases where these approaches fail, but he does not give a sense as to how often they fail.", "dateLastCrawled": "2022-01-31T23:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Partial dependence</b> plots are a simple way to make <b>black</b>-<b>box</b> models easy ...", "url": "https://lmc2179.github.io/posts/pdp.html", "isFamilyFriendly": true, "displayUrl": "https://lmc2179.github.io/posts/pdp.html", "snippet": "This section uses a bit of <b>language</b> from <b>causal</b> inference, particularly the idea of a \u201cback door path\u201d. Most of this content is from <b>Causal</b> interpretations of <b>black</b>-<b>box</b> models, Zhao et al 2018, which is well worth a read if you want to know more. That paper even uses the same Boston housing data set, making it a natural on-ramp after you ...", "dateLastCrawled": "2022-02-02T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Causal</b> Inference in Natural <b>Language</b> Processing: Estimation, Prediction ...", "url": "https://deepai.org/publication/causal-inference-in-natural-language-processing-estimation-prediction-interpretation-and-beyond", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>causal</b>-inference-in-natural-<b>language</b>-processing...", "snippet": "<b>Causal</b> inference with text data involves several challenges that distinct from typical <b>causal</b> inference settings: text is high-dimensional, needs sophisticated modeling to measure semantically meaningful factors <b>like</b> topic, and demands careful thought to formalize the intervention that a <b>causal</b> question corresponds to. The developments in NLP around modeling <b>language</b>, from topic models to contextual embeddings, offer promising ways to extract the information we need from text to estimate ...", "dateLastCrawled": "2022-02-03T01:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Full article: <b>Causal Interpretations of Black-Box Models</b>", "url": "https://www.tandfonline.com/doi/full/10.1080/07350015.2019.1624293", "isFamilyFriendly": true, "displayUrl": "https://www.tandfonline.com/doi/full/10.1080/07350015.2019.1624293", "snippet": "<b>Causal</b> interpretation of <b>black</b>-<b>box</b> models, the main topic of this article, is another good example for the usefulness of <b>causal</b> <b>language</b> and theory for machine learning researchers. Pearl ( 2018 ) postulated fundamental impediments to today\u2019s machine learning algorithms and summarized \u201csparks\u201d from \u201cThe <b>Causal</b> Revolution\u201d that may help us circumvent them.", "dateLastCrawled": "2022-02-03T14:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>causal framework for explaining the predictions of black</b>-<b>box</b> sequence ...", "url": "https://aclanthology.org/D17-1042.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/D17-1042.pdf", "snippet": "related under the <b>black</b>-<b>box</b> <b>model</b>. <b>Causal</b> de-pendencies arise from analyzing perturbed ver-sions of inputs that are passed through the <b>black</b>-412. <b>box</b> <b>model</b>. Although such perturbations might be available in limited cases, we generate them auto-matically. For sentences, we adopt a variational autoencoder to produce semantically related sen-tence variations. The resulting inferred <b>causal</b> de-pendencies (interval estimates) form a dense bi-partite graph over tokens from which explanations can be ...", "dateLastCrawled": "2021-11-18T22:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A <b>causal framework for explaining the predictions of black</b>-<b>box</b> sequence ...", "url": "https://people.csail.mit.edu/tommi/papers/AlvJaa_EMNLP2017.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.csail.mit.edu/tommi/papers/AlvJaa_EMNLP2017.pdf", "snippet": "related under the <b>black</b>-<b>box</b> <b>model</b>. <b>Causal</b> de-pendencies arise from analyzing perturbed ver-sions of inputs that are passed through the <b>black</b>-<b>box</b> <b>model</b>. Although such perturbations might be available in limited cases, we generate them auto-matically. For sentences, we adopt a variational autoencoder to produce semantically related sen-tence variations. The resulting inferred <b>causal</b> de-pendencies (interval estimates) form a dense bi-partite graph over tokens from which explanations can be ...", "dateLastCrawled": "2022-01-17T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Generative <b>causal</b> explanations of <b>black</b>-<b>box</b> classi\ufb01ers", "url": "https://papers.nips.cc/paper/2020/file/3a93a609b97ec0ab0ff5539eb79ef33a-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2020/file/3a93a609b97ec0ab0ff5539eb79ef33a-Paper.pdf", "snippet": "We develop a method for generating <b>causal</b> post-hoc explanations of <b>black</b>-<b>box</b> classi\ufb01ers based on a learned low-dimensional representation of the data. The explanation is <b>causal</b> in the sense that changing learned latent factors produces a change in the classi\ufb01er output statistics. To construct these explanations, we design a learning framework that leverages a generative <b>model</b> and information-theoretic measures of <b>causal</b> in\ufb02uence. Our objective function encourages both the generative ...", "dateLastCrawled": "2022-02-02T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Review for NeurIPS paper: <b>Generative causal explanations of black</b>-<b>box</b> ...", "url": "https://papers.nips.cc/paper/2020/file/3a93a609b97ec0ab0ff5539eb79ef33a-Review.html", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2020/file/3a93a609b97ec0ab0ff5539eb79ef33a-Review.html", "snippet": "The proposed <b>model</b> optimizes mutual information between the class (predicted by the <b>black</b>-<b>box</b> <b>model</b>) and the learned features and involves distance measure between the data and generator distribution D(p(g(alpha, beta)), p(X)). This places the <b>model</b> close to the works on VAEs, InfoGANs, and related generative models. In fact, if we cut the \u201c<b>black</b> <b>box</b> classifier\u201d part of the Figure 1a), we get a supervised method for discovering meaningful features, very <b>similar</b> in spirit to InfoGAN ...", "dateLastCrawled": "2021-09-29T17:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Black</b>-<b>box Brain Experiments, Causal Mathematical Logic</b>, and the ...", "url": "https://ui.adsabs.harvard.edu/abs/2013JAGI....4...10P/abstract", "isFamilyFriendly": true, "displayUrl": "https://ui.adsabs.harvard.edu/abs/2013JAGI....4...10P/abstract", "snippet": "The same input is separately supplied to a <b>causal</b> virtual machine, and the calculated output is compared with the measured output. The virtual machine, described in a previous paper, is a computer implementation of CML, fixed for all experiments and unrelated to the device in the <b>black</b> <b>box</b>. If the two outputs are equivalent, then the experiment ...", "dateLastCrawled": "2020-04-15T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Improving NLP with Causality. Potential uses of <b>Causal</b> Inference to ...", "url": "https://towardsdatascience.com/improving-nlp-with-causality-2dec1fa90b74", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/improving-nlp-with-<b>causal</b>ity-2dec1fa90b74", "snippet": "Feder et al. (2021b) tested this approach on BERT, in a recent paper they outline a framework for <b>causal</b> <b>model</b> explanations through counterfactual <b>language</b> models. Their approach is to fine-tune a deep contextualized embedding <b>model</b> like BERT, using auxiliary adversarial tasks derived from a <b>causal</b> graph of the prediction problem. They show that it is possible to choose auxiliary adversarial pre-training tasks that allow BERT to effectively learn a counterfactual representation for a concept ...", "dateLastCrawled": "2022-02-03T11:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Partial dependence</b> plots are a simple way to make <b>black</b>-<b>box</b> models easy ...", "url": "https://lmc2179.github.io/posts/pdp.html", "isFamilyFriendly": true, "displayUrl": "https://lmc2179.github.io/posts/pdp.html", "snippet": "This section uses a bit of <b>language</b> from <b>causal</b> inference, particularly the idea of a \u201cback door path\u201d. Most of this content is from <b>Causal</b> interpretations of <b>black</b>-<b>box</b> models, Zhao et al 2018, which is well worth a read if you want to know more. That paper even uses the same Boston housing data set, making it a natural on-ramp after you ...", "dateLastCrawled": "2022-02-02T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Interpretability vs Explainability: The Black</b> <b>Box</b> of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-interpretability-vs-explainability", "snippet": "The benefit a deep neural net offers to engineers is it creates a <b>black</b> <b>box</b> of parameters, like fake additional data points, that allow a <b>model</b> to base its decisions against. These fake data points go unknown to the engineer. The <b>black</b> <b>box</b>, or hidden layers, allow a <b>model</b> to make associations among the given data points to predict better results. For example, if we are deciding how long someone might have to live, and we use career data as an input, it is possible the <b>model</b> sorts the careers ...", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Causal KNN</b> - GitHub Pages", "url": "https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/blog_post_causal_knn/", "isFamilyFriendly": true, "displayUrl": "https://humboldt-wi.github.io/.../applied_predictive_<b>model</b>ing_19/blog_post_<b>causal_knn</b>", "snippet": "At this point, a distinction between the <b>Causal KNN</b> and the Two-<b>Model</b>-Approach has to be made, since both methods seem to be <b>similar</b>. According to Rzepakowski and Jaroszewicz (2012, 2), the idea behind the two <b>model</b> approach is to build two separate models, to estimate the treatment effect. One <b>Model</b> is trained, using the treatment, and the other one using the control data set. After building the two models, the treatment effect estimations are calculated by subtracting the predicted class ...", "dateLastCrawled": "2022-01-31T21:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>causal</b> framework for explaining the predictions of <b>black</b>-<b>box</b> sequence ...", "url": "https://dmelis.github.io/assets/publications/2017_blackbox_seq2seq_interpret/EMNLP2017_Causal.pdf", "isFamilyFriendly": true, "displayUrl": "https://dmelis.github.io/.../2017_<b>blackbox</b>_seq2seq_interpret/EMNLP2017_<b>Causal</b>.pdf", "snippet": "related under the <b>black</b>-<b>box</b> <b>model</b>. <b>Causal</b> de-pendencies arise from analyzing perturbed ver-sions of inputs that are passed through the <b>black</b>-412. <b>box</b> <b>model</b>. Although such perturbations might be available in limited cases, we generate them auto-matically. For sentences, we adopt a variational autoencoder to produce semantically related sen-tence variations. The resulting inferred <b>causal</b> de-pendencies (interval estimates) form a dense bi-partite graph over tokens from which explanations <b>can</b> be ...", "dateLastCrawled": "2021-08-21T19:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>causal framework for explaining the predictions of black</b>-<b>box</b> sequence ...", "url": "https://people.csail.mit.edu/tommi/papers/AlvJaa_EMNLP2017.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.csail.mit.edu/tommi/papers/AlvJaa_EMNLP2017.pdf", "snippet": "related under the <b>black</b>-<b>box</b> <b>model</b>. <b>Causal</b> de-pendencies arise from analyzing perturbed ver-sions of inputs that are passed through the <b>black</b>-<b>box</b> <b>model</b>. Although such perturbations might be available in limited cases, we generate them auto-matically. For sentences, we adopt a variational autoencoder to produce semantically related sen-tence variations. The resulting inferred <b>causal</b> de-pendencies (interval estimates) form a dense bi-partite graph over tokens from which explanations <b>can</b> be ...", "dateLastCrawled": "2022-01-17T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Review for NeurIPS paper: <b>Generative causal explanations of black</b>-<b>box</b> ...", "url": "https://papers.nips.cc/paper/2020/file/3a93a609b97ec0ab0ff5539eb79ef33a-Review.html", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2020/file/3a93a609b97ec0ab0ff5539eb79ef33a-Review.html", "snippet": "Summary and Contributions: This paper presents a generative <b>model</b> to &quot;explain&quot; any given <b>black</b>-<b>box</b> classifier and its training dataset. By &quot;explain&quot;, the authors mean that a hidden factor <b>can</b> be discovered to control or intervene in the output of the classifier. The discovery is based on a proposed maximization objective, which consists of two terms: 1) a proposed Information Flow that denotes the <b>causal</b> effect from the hidden factor to the classifier output and 2) a distribution similarity ...", "dateLastCrawled": "2021-09-29T17:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Unpacking the <b>Black</b> <b>Box</b> of Causality: Learning about <b>Causal</b> Mechanisms ...", "url": "https://www.cambridge.org/core/journals/american-political-science-review/article/unpacking-the-black-box-of-causality-learning-about-causal-mechanisms-from-experimental-and-observational-studies/9D2ACE9F784B99A30216D216FBF88553", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/ameri<b>can</b>-political-science-review/article/...", "snippet": "The use of the potential outcomes framework is essential because it provides a formal <b>language</b> for understanding the counterfactual comparisons required to study <b>causal</b> mechanisms. As shown later, the conventional approaches based on structural equation models fail to recognize the key assumption behind <b>causal</b> mediation analysis. Potential Outcomes Framework. We first introduce the concept of potential outcomes, which has been used in the methodological literature as the formal framework of ...", "dateLastCrawled": "2022-01-12T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Causal</b> Rasch models - PubMed Central (PMC)", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3750201/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3750201", "snippet": "Rasch analysis, absent construct theory and an associated specification equation, is a <b>black</b> <b>box</b> in which understanding may be more illusory than not. Finally, the quantitative hypothesis <b>can</b> be tested by comparing theory-based trade-off relations with observed trade-off relations. Only quantitative variables (as measured) support such trade-offs. Note that to test the quantitative hypothesis requires more than manipulation of the algebraic equivalencies in the Rasch <b>model</b> or descriptively ...", "dateLastCrawled": "2021-10-30T18:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) CausaLM: <b>Causal Model Explanation Through Counterfactual Language</b> ...", "url": "https://www.researchgate.net/publication/350482062_CausaLM_Causal_Model_Explanation_Through_Counterfactual_Language_Models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350482062_<b>CausaL</b>M_<b>Causal</b>_<b>Model</b>_Explanation...", "snippet": "A byproduct of our method is a <b>language</b> representation <b>model</b> that is unaffected by the tested concept, which <b>can</b> be useful in mitigating unwanted bias ingrained in the data. Three <b>causal</b> graphs ...", "dateLastCrawled": "2022-02-02T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Improving NLP with Causality. Potential uses of <b>Causal</b> Inference to ...", "url": "https://towardsdatascience.com/improving-nlp-with-causality-2dec1fa90b74", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/improving-nlp-with-<b>causal</b>ity-2dec1fa90b74", "snippet": "Feder et al. (2021b) tested this approach on BERT, in a recent paper they outline a framework for <b>causal</b> <b>model</b> explanations through counterfactual <b>language</b> models. Their approach is to fine-tune a deep contextualized embedding <b>model</b> like BERT, using auxiliary adversarial tasks derived from a <b>causal</b> graph of the prediction problem. They show that it is possible to choose auxiliary adversarial pre-training tasks that allow BERT to effectively learn a counterfactual representation for a concept ...", "dateLastCrawled": "2022-02-03T11:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "CausaLM: <b>Causal</b> <b>Model</b> Explanation Through Counterfactual <b>Language</b> Models", "url": "https://www.arxiv-vanity.com/papers/2005.13407/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2005.13407", "snippet": "Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all ML-based methods, they are as good as their training data, and <b>can</b> also capture unwanted biases. While there are tools that <b>can</b> help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high level <b>language</b> concepts. A key problem of estimating the ...", "dateLastCrawled": "2021-10-11T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Interpretability vs Explainability: The Black</b> <b>Box</b> of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-interpretability-vs-explainability", "snippet": "These fake data points go unknown to the engineer. The <b>black</b> <b>box</b>, or hidden layers, allow a <b>model</b> to make associations among the given data points to predict better results. For example, if we are deciding how long someone might have to live, and we use career data as an input, it is possible the <b>model</b> sorts the careers into high- and low-risk career options all on its own. Perhaps we inspect a node and see it relates oil rig workers, underwater welders, and boat cooks to each other. It is ...", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Computational Model: Causal Diagrams with Symmetry</b> - LessWrong 2.0 viewer", "url": "https://www.greaterwrong.com/posts/mZy6AMgCw9CPjNCoK/computational-model-causal-diagrams-with-symmetry", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/mZy6AMgCw9CPjNCoK/computational-<b>model</b>-<b>causal</b>...", "snippet": "A proper <b>causal</b> analysis would have to open the <b>black</b> <b>box</b> of the voltage or current source and reveal the circular patterns of causation within. Set up as a voltage source, it is continuously sensing its own output voltage and maintaining it close to the reference value set on the control panel. Set up as a current source, it is doing corresponding things with the sensed and reference currents.", "dateLastCrawled": "2021-12-22T00:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Review for NeurIPS paper: <b>Generative causal explanations of black</b>-<b>box</b> ...", "url": "https://proceedings.neurips.cc/paper/2020/file/3a93a609b97ec0ab0ff5539eb79ef33a-Review.html", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/3a93a609b97ec0ab0ff5539eb79ef33a-Review...", "snippet": "Summary and Contributions: This paper presents a generative <b>model</b> to &quot;explain&quot; any given <b>black</b>-<b>box</b> classifier and its training dataset. By &quot;explain&quot;, the authors mean that a hidden factor <b>can</b> be discovered to control or intervene in the output of the classifier. The discovery is based on a proposed maximization objective, which consists of two terms: 1) a proposed Information Flow that denotes the <b>causal</b> effect from the hidden factor to the classifier output and 2) a distribution similarity ...", "dateLastCrawled": "2021-09-20T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Unpacking the <b>Black</b> <b>Box</b> of Causality: Learning about <b>Causal</b> Mechanisms ...", "url": "https://www.cambridge.org/core/journals/american-political-science-review/article/unpacking-the-black-box-of-causality-learning-about-causal-mechanisms-from-experimental-and-observational-studies/9D2ACE9F784B99A30216D216FBF88553", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/ameri<b>can</b>-political-science-review/article/...", "snippet": "The use of the potential outcomes framework is essential because it provides a formal <b>language</b> for understanding the counterfactual comparisons required to study <b>causal</b> mechanisms. As shown later, the conventional approaches based on structural equation models fail to recognize the key assumption behind <b>causal</b> mediation analysis. Potential Outcomes Framework. We first introduce the concept of potential outcomes, which has been used in the methodological literature as the formal framework of ...", "dateLastCrawled": "2022-01-12T13:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Black</b>-<b>box Brain Experiments, Causal Mathematical Logic</b>, and the ...", "url": "https://ui.adsabs.harvard.edu/abs/2013JAGI....4...10P/abstract", "isFamilyFriendly": true, "displayUrl": "https://ui.adsabs.harvard.edu/abs/2013JAGI....4...10P/abstract", "snippet": "Experiments are focused on a <b>black</b>-<b>box</b> as one of the devices described above of which both the input and the output are precisely known, but not the internal implementation. The same input is separately supplied to a <b>causal</b> virtual machine, and the calculated output is <b>compared</b> with the measured output. The virtual machine, described in a previous paper, is a computer implementation of CML, fixed for all experiments and unrelated to the device in the <b>black</b> <b>box</b>. If the two outputs are ...", "dateLastCrawled": "2020-04-15T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "CausalR: <b>Causal</b> Reasoning over Natural <b>Language</b> Rulebases | OpenReview", "url": "https://openreview.net/forum?id=vuAX_4bv8A", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/forum?id=vuAX_4bv8A", "snippet": "Transformers have been shown to perform deductive reasoning on a logical rulebase containing rules and statements written in natural <b>language</b>. Recent works show that such models <b>can</b> also produce the reasoning steps (i.e., the proof graph) that emulate the <b>model</b>\u2019s logical reasoning process. But these models behave as a <b>black</b>-<b>box</b> unit that emulates the reasoning process without any <b>causal</b> constraints in the reasoning steps, thus questioning the faithfulness. In this work, we frame the ...", "dateLastCrawled": "2022-01-19T21:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "CAUSALR: <b>Causal</b> Reasoning over Natural <b>Language</b> Rulebases", "url": "https://openreview.net/pdf?id=vuAX_4bv8A", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=vuAX_4bv8A", "snippet": "CAUSALR: <b>Causal</b> Reasoning over Natural <b>Language</b> Rulebases Anonymous ACL submission Abstract 001 Transformers have been shown to be able to 002 perform deductive reasoning on a logical rule- 003 base containing rules and statements written 004 in natural <b>language</b>. Recent works show that 005 such models <b>can</b> also produce the reasoning 006 steps (i.e., the proof graph) that emulate the 007 <b>model</b>\u2019s logical reasoning process. But these 008 models behave as a <b>black</b>-<b>box</b> unit that emu- 009 lates ...", "dateLastCrawled": "2022-02-02T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Interpretability vs Explainability: The Black</b> <b>Box</b> of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-interpretability-vs-explainability", "snippet": "The benefit a deep neural net offers to engineers is it creates a <b>black</b> <b>box</b> of parameters, like fake additional data points, that allow a <b>model</b> to base its decisions against. These fake data points go unknown to the engineer. The <b>black</b> <b>box</b>, or hidden layers, allow a <b>model</b> to make associations among the given data points to predict better results. For example, if we are deciding how long someone might have to live, and we use career data as an input, it is possible the <b>model</b> sorts the careers ...", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Generative causal explanations of black-box classifiers</b> | Request PDF", "url": "https://www.researchgate.net/publication/342435716_Generative_causal_explanations_of_black-box_classifiers", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/342435716_Generative_<b>causal</b>_explanations_of...", "snippet": "The applications in which <b>black</b> <b>box</b> decision systems <b>can</b> be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence ...", "dateLastCrawled": "2021-12-24T21:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) CausaLM: <b>Causal Model Explanation Through Counterfactual Language</b> ...", "url": "https://www.researchgate.net/publication/350482062_CausaLM_Causal_Model_Explanation_Through_Counterfactual_Language_Models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350482062_<b>CausaL</b>M_<b>Causal</b>_<b>Model</b>_Explanation...", "snippet": "A byproduct of our method is a <b>language</b> representation <b>model</b> that is unaffected by the tested concept, which <b>can</b> be useful in mitigating unwanted bias ingrained in the data. Three <b>causal</b> graphs ...", "dateLastCrawled": "2022-02-02T19:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Improving NLP with Causality. Potential uses of <b>Causal</b> Inference to ...", "url": "https://towardsdatascience.com/improving-nlp-with-causality-2dec1fa90b74", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/improving-nlp-with-<b>causal</b>ity-2dec1fa90b74", "snippet": "Feder et al. (2021b) tested this approach on BERT, in a recent paper they outline a framework for <b>causal</b> <b>model</b> explanations through counterfactual <b>language</b> models. Their approach is to fine-tune a deep contextualized embedding <b>model</b> like BERT, using auxiliary adversarial tasks derived from a <b>causal</b> graph of the prediction problem. They show that it is possible to choose auxiliary adversarial pre-training tasks that allow BERT to effectively learn a counterfactual representation for a concept ...", "dateLastCrawled": "2022-02-03T11:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Causal KNN</b> - GitHub Pages", "url": "https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/blog_post_causal_knn/", "isFamilyFriendly": true, "displayUrl": "https://humboldt-wi.github.io/.../applied_predictive_<b>model</b>ing_19/blog_post_<b>causal_knn</b>", "snippet": "Treatment assignment based on the uplift values <b>can</b> <b>be compared</b> to the random assignment, where every individual gets a treatment with a probability of 2/3. Finally, one <b>can</b> look at the cumulative visits for the targeted customers. If the targeting of customers with a higher uplift also corresponds to a higher chance of visit, the curve of the <b>Causal KNN</b> predictions must be above the curve of the randomized predictions. We <b>can</b> use the same procedure to evaluate the predictions of the <b>Causal</b> ...", "dateLastCrawled": "2022-01-31T21:20:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Body language and machine learning</b> | Statistical Modeling, <b>Causal</b> ...", "url": "https://statmodeling.stat.columbia.edu/2020/10/25/body-language-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://stat<b>model</b>ing.stat.columbia.edu/2020/10/25/<b>body-language-and-machine-learning</b>", "snippet": "Anyway, the other day I was thinking about how this is an example of <b>machine</b> <b>learning</b>. No <b>causal</b> inference (sorry, Judea!), just pure prediction, but \u201c<b>machine</b> <b>learning</b>\u201d in that my brain has passively gathering data on car positioning for the past few decades, and at some point it decided to associate that with driving decisions. I guess it was motivated by me trying to figure out where to go in particular situations. So in many ways this is exactly the kind of problem we\u2019ve been ...", "dateLastCrawled": "2021-11-28T21:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Causality for Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/causality-for-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>causality-for-machine-learning</b>", "snippet": "The <b>machine</b> <b>learning</b> community\u2019s interest in causality has significantly increased in recent years. My understanding of causality has been shaped by Judea Pearl and a number of collaborators and colleagues, and much of it went into a book written with Dominik Janzing and Jonas Peters (PetJanSch17).I have spoken about this topic on various occasions, 1 1 1 e.g., (Schoelkopf2017icml), talks at ICLR, ACML, and in <b>machine</b> <b>learning</b> labs that have meanwhile developed an interest in causality (e ...", "dateLastCrawled": "2022-02-02T18:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b> Glossary: <b>Language</b> Evaluation | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/<b>language</b>", "snippet": "<b>causal</b> <b>language</b> <b>model</b>. #<b>language</b>. Synonym for unidirectional <b>language</b> <b>model</b>. See bidirectional <b>language</b> <b>model</b> to contrast different directional approaches in <b>language</b> modeling. crash blossom. #<b>language</b>. A sentence or phrase with an ambiguous meaning. Crash blossoms present a significant problem in natural <b>language</b> understanding. For example, the headline Red Tape Holds Up Skyscraper is a crash blossom because an NLU <b>model</b> could interpret the headline literally or figuratively. D. decoder. # ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>01 - Introduction To Causality</b> \u2014 <b>Causal</b> Inference for the Brave and True", "url": "https://matheusfacure.github.io/python-causality-handbook/01-Introduction-To-Causality.html", "isFamilyFriendly": true, "displayUrl": "https://matheusfacure.github.io/python-<b>causal</b>ity-handbook/<b>01-Introduction-To-Causality</b>...", "snippet": "We can do all sorts of wonderful things with <b>machine</b> <b>learning</b>. The only requirement is that we frame our problems as prediction ones. Want to translate from english to portuguese? Then build a ML <b>model</b> that predicts portuguese sentences when given english sentences. Want to recognize faces? Then build a ML <b>model</b> that predicts the presence of a face in a subsection of a picture. Want to build a self driving car? Then build one ML <b>model</b> to predict the direction of the wheel and the pressure on ...", "dateLastCrawled": "2022-01-31T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> and Theological Traditions of <b>Analogy</b> - Davison - 2021 ...", "url": "https://onlinelibrary.wiley.com/doi/full/10.1111/moth.12682", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/full/10.1111/moth.12682", "snippet": "An <b>analogy</b> of <b>causal</b> likeness, \u2018in the open\u2019, from the maker to the made, might apply more to <b>machine</b> <b>learning</b> as it was conceived in an earlier period of computing history than it does today. While a pioneering generation in the field of artificial intelligence had attempted to code the sort of properties we are talking about directly, today the approach is more likely to be emergent, arising within a network, somewhat akin to the neurons of a brain, with connections tweaked iteratively ...", "dateLastCrawled": "2021-04-16T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Towards <b>Causal</b> <b>Representation Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-causal-representation-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/towards-<b>causal</b>-<b>representation-learning</b>", "snippet": "When <b>learning</b> a <b>causal</b> <b>model</b>, one should thus require fewer examples to adapt as most knowledge, i.e., modules, can be re-used without further training. A Causality Perspective. Causation is a subtle concept that cannot be fully described using the <b>language</b> of Boolean logic [lewis1974causation] or that of probabilistic inference; it requires the additional notion of intervention [Spirtes2000, Pearl2009]. The manipulative definition of causation [Spirtes2000, Pearl2009, imbens2015causal ...", "dateLastCrawled": "2022-02-02T22:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Machine</b> <b>Learning</b> and Theological Traditions of <b>Analogy</b>", "url": "https://www.researchgate.net/publication/349470559_Machine_Learning_and_Theological_Traditions_of_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/349470559_<b>Machine</b>_<b>Learning</b>_and_Theological...", "snippet": "For <b>machine</b> <b>learning</b> by evolution, open analogies of <b>causal</b> similitude may not be the best way to analyse using human capacities we find there. As we have seen, a computer can most", "dateLastCrawled": "2021-11-04T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Causal</b> inference and <b>machine</b> <b>learning</b> problems - Refinitiv", "url": "https://www.refinitiv.com/perspectives/ai-digitalization/applying-causal-inference-to-machine-learning-problems/", "isFamilyFriendly": true, "displayUrl": "https://www.refinitiv.com/.../applying-<b>causal</b>-inference-to-<b>machine</b>-<b>learning</b>-problems", "snippet": "The research team at Refinitiv Labs is looking at applying <b>causal</b> inference to <b>machine</b> <b>learning</b> problems. How can a causality <b>model</b> be applied to data and data management use cases that test for causes and identify paths to performance gains? Narrow AI \u2014 artificial intelligence that focuses on a singular or limited task \u2014 struggles to differentiate between actions or states that appear in proximity (correlation), and actions that actually affect each other (causation). To address this ...", "dateLastCrawled": "2022-01-25T16:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "6 \u2013 Interpretability \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability", "snippet": "Explanation by example: Similar to how humans justify actions by <b>analogy</b>. Outputs nearest training examples in the latent representation learned by the <b>model</b>. Examples include k-nearest neighbors and learned representations of words after word2vec training. Figure 5: An example of text explanations that can be used to interpret what a <b>model</b> has learned: captions generated in tandem with visual recognition tasks. (Karpathy 2015) Post-hoc explanations can be useful for research and ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>All Unit MCQ questions of ML</b> \u2013 TheCodingShef", "url": "https://thecodingshef.com/all-unit-mcq-questions-of-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thecodingshef.com/<b>all-unit-mcq-questions-of</b>-<b>machine</b>-<b>learning</b>", "snippet": "It is used for prediction. It discovers <b>causal</b> relationships. It relates inputs to outputs. Correct option is C. Choose the correct option regarding <b>machine</b> <b>learning</b> (ML) and artificial intelligence (AI) ML is a set of techniques that turns a dataset into a software. AI is a software that can emulate the human mind.", "dateLastCrawled": "2022-01-30T22:09:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(causal language model)  is like +(black box)", "+(causal language model) is similar to +(black box)", "+(causal language model) can be thought of as +(black box)", "+(causal language model) can be compared to +(black box)", "machine learning +(causal language model AND analogy)", "machine learning +(\"causal language model is like\")", "machine learning +(\"causal language model is similar\")", "machine learning +(\"just as causal language model\")", "machine learning +(\"causal language model can be thought of as\")", "machine learning +(\"causal language model can be compared to\")"]}