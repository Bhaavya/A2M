{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Handling Parallelism in a Concurrency Model</b>", "url": "https://www.researchgate.net/publication/255173624_Handling_Parallelism_in_a_Concurrency_Model", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../255173624_<b>Handling_Parallelism_in_a_Concurrency_Model</b>", "snippet": "the developer would <b>like</b> to speed up with <b>parallelism</b>. On the other hand, even . simple <b>data</b>-parallel programs may su\ufb00er from concurrenc y issues such as <b>data</b>. races, atomicity violations, or ...", "dateLastCrawled": "2022-01-13T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Using Persistent <b>Data</b> Models to Automate <b>Parallelism</b> under ...", "url": "https://newtraell.cs.uchicago.edu/files/ms_paper/slaguna.pdf", "isFamilyFriendly": true, "displayUrl": "https://newtraell.cs.uchicago.edu/files/ms_paper/slaguna.pdf", "snippet": "Using Persistent <b>Data</b> Models to Automate <b>Parallelism</b> under Synchronization Requirements by Sean Laguna A thesis submitted in partial ful llment for the degree of Master of Computer Science in the Ridgway Scott Department of Computer Science November 2016. UNIVERSITY OF CHICAGO Abstract Ridgway Scott Department of Computer Science Masters Degree bySean Laguna We implement a vector, using a bit-partitioned trie, that allows for consistent concurrent reading and writing between multiple tasks ...", "dateLastCrawled": "2021-11-18T16:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "6 Types of <b>Activation Function in Neural Networks</b> You Need to Know - upGrad", "url": "https://www.upgrad.com/blog/types-of-activation-function-in-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/types-of-<b>activation-function-in-neural-networks</b>", "snippet": "Non-linearity imparts a better fit to the <b>data</b>. High <b>parallelism</b> promotes fast processing and hardware failure-tolerance. Generalization allows for the application of the <b>model</b> to unlearned <b>data</b>. Noise insensitivity that allows accurate prediction even for uncertain <b>data</b> and measurement errors. Learning and adaptivity allow the <b>model</b> to update its internal architecture according to the changing environment. ANN-based computing primarily aims to design advanced mathematical algorithms that ...", "dateLastCrawled": "2022-02-02T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Parallel actor monitors: Disentangling task-level parallelism from data</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167642313000725", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167642313000725", "snippet": "Because of the strong <b>data</b>-task entanglement in the actor <b>model</b>, the actor programmer is forced to partition her <b>data</b> in order to exploit <b>parallelism</b>. In general, the structure and choice of algorithms strongly depend on the structure of the underlying <b>data</b> [2]. When each actor possesses a partitioning of the <b>data</b>, global operations over the <b>data</b> must be implemented by a well defined protocol between those actors. These protocols have to be carefully encoded to guarantee properties <b>like</b> ...", "dateLastCrawled": "2021-11-30T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Pipeline parallelism</b> - SlideShare", "url": "https://www.slideshare.net/anniyappa/pipeline-parallelism", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/anniyappa/<b>pipeline-parallelism</b>", "snippet": "Task <b>parallelism</b> is inadequate because the <b>parallelism</b> and synchronization is not matched to the target, forcing the programmer to intervene and create un-portable code Fine-grained <b>data</b> <b>parallelism</b> has good <b>parallelism</b>, but would overwhelm the communication mechanism of a multicore Coarsening the granularity before <b>data</b>-<b>parallelism</b> is exploited and achieve great parallelization of stateless components Finally, adding software pipelining allows us to parallelize stateful components and ...", "dateLastCrawled": "2022-01-30T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Uniqueness and Reference Immutability for Safe <b>Parallelism</b>", "url": "https://www.cs.drexel.edu/~csgordon/papers/oopsla12_slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.drexel.edu/~csgordon/papers/oopsla12_slides.pdf", "snippet": "Immutability for Safe <b>Parallelism</b> Colin Gordon (UW), Matt Parkinson (MSR), Jared Parsons (MS), Aleks Bromfield (MS), Joe Duffy (MS) OOPSLA 2012 . A Prototype Extension to C# \u2022 Extend C# for safe <b>parallelism</b> \u2022 Safe task &amp; <b>data</b> <b>parallelism</b> \u2013 No locks \u2013 No mutable statics/globals \u2022 Statically enforce <b>data</b>-race freedom \u2022 Real: millions of LOC \u2013 Experience report later . <b>Data</b>-Race-Free, Parallel, Imperative OOP \u2022 Prove soundness for \u2022 <b>Data</b>-race freedom \u2022 Reference immutability ...", "dateLastCrawled": "2022-01-02T10:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "NII Lectures: Parallel Languages of Today: OpenMP to Fortress", "url": "https://www.nii.ac.jp/userimg/lectures/LawrenceSnyder/niiLec4.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.nii.ac.jp/userimg/lectures/LawrenceSnyder/niiLec4.pdf", "snippet": "unlimited <b>parallelism</b> Consider this <b>model</b> Imagine <b>data</b> shifts left one item \u2026 what\u2019s the cost for 100,000 local values? <b>Generalizing</b> \u201ctrivialized\u201d operations is hard P 0 P 1 P 2 P 3 P 4 P 5 P 6 P 7. 3 Scalable Techniques Replace \u201cunlimited <b>parallelism</b>\u201d in thinking by scalable <b>parallelism</b>: programs designed to fully use all available <b>parallelism</b> Generally \u201csmooth scaling\u201d across a practical range of values Multiple solution paradigms may be needed to span a wide range of ...", "dateLastCrawled": "2021-09-19T11:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>CIFAR-10</b> on Benchmarks.AI", "url": "https://benchmarks.ai/cifar-10", "isFamilyFriendly": true, "displayUrl": "https://benchmarks.ai/<b>cifar-10</b>", "snippet": "To address the need for efficient and task-independent <b>model</b> <b>parallelism</b>, we introduce GPipe, a pipeline <b>parallelism</b> library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a ...", "dateLastCrawled": "2022-02-03T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Classification/regression with very large</b> dataset - any thoughts?", "url": "https://www.researchgate.net/post/Classification-regression-with-very-large-dataset-any-thoughts", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Classification-regression-with-very-large</b>-<b>data</b>set...", "snippet": "Using 10 billion features to train a prediction <b>model</b> with 100 inputs seems <b>like</b> an overkill. The <b>data</b> can be better utilized by splitting into multiple batches, training several models and ...", "dateLastCrawled": "2022-02-02T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the T5 Transformer and how does it work? \u2013 <b>MachineCurve</b>", "url": "https://www.machinecurve.com/index.php/question/what-is-the-t5-transformer-and-how-does-it-work/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinecurve</b>.com/index.php/question/what-is-the-t5-transformer-and-how...", "snippet": "However, larger models trained with fewer steps &gt; smaller models that have more <b>data</b>. Ensemble models improve the <b>data</b> IF pretrained AND finetuned separately, i.e. they cannot use the same pretrained <b>model</b> during finetuning. T5 conclusions: * Text-to-text based task formulation for <b>generalizing</b> Transformer architecture across language tasks works.", "dateLastCrawled": "2022-01-31T13:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Programming Paradigms</b> - cse.iitd.ac.in", "url": "https://www.cse.iitd.ac.in/~dheerajb/parallel_paradigms.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitd.ac.in/~dheerajb/parallel_paradigms.pdf", "snippet": "on a parallel computer. <b>Parallelism</b> remains transparent to the user. ... (<b>Similar</b> to <b>data</b> parallel) l It is multithreading and asynchronous (<b>Similar</b> to message-passing <b>model</b>) l <b>Data</b> resides in single shared address space, thus does not have to be explicitly allocated l Workload can be either explicitly or implicitly allocated l Communication is done implicitly through shared reads and writes of variables. However synchronization is explicit 30 Explicit Parallel Programming Models Shared ...", "dateLastCrawled": "2022-01-22T22:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Handling Parallelism in a Concurrency Model</b>", "url": "https://www.researchgate.net/publication/255173624_Handling_Parallelism_in_a_Concurrency_Model", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../255173624_<b>Handling_Parallelism_in_a_Concurrency_Model</b>", "snippet": "<b>Data</b> Parallel Haskell [4] implements the <b>model</b> of nested <b>data</b> <b>para llelism</b> (in- spired by NESL [3]), extending par allel access also to user -de\ufb01ned <b>data</b> struc- tures.", "dateLastCrawled": "2022-01-13T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Using Persistent <b>Data</b> Models to Automate <b>Parallelism</b> under ...", "url": "https://newtraell.cs.uchicago.edu/files/ms_paper/slaguna.pdf", "isFamilyFriendly": true, "displayUrl": "https://newtraell.cs.uchicago.edu/files/ms_paper/slaguna.pdf", "snippet": "comparable scaling results and <b>similar</b> raw performance. It beats serial implementa-tions on a dual-core machine and runs only 4% to 50% slower than the best-performing parallel technique on up to 16 logical cores, without requiring manual synchronization between steps and while providing greater algorithmic exibility. Acknowledgements The original idea to use persistent and transient <b>data</b> structures in other settings came from very useful conversations with Matt Rocklin, who was my student ...", "dateLastCrawled": "2021-11-18T16:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Parallel actor monitors: Disentangling task-level parallelism from data</b> ...", "url": "https://www.researchgate.net/publication/259118332_Parallel_actor_monitors_Disentangling_task-level_parallelism_from_data_partitioning_in_the_actor_model", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/259118332_Parallel_actor_monitors...", "snippet": "The BSP <b>model</b> enables a quite different <b>parallelism</b> from the one provided by active objects : <b>data</b>-<b>parallelism</b>. This form of <b>parallelism</b> consists of cutting a task into several pieces in order to ...", "dateLastCrawled": "2021-08-10T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Generalizing</b> Knowledge in Decentralized Rule-based Models", "url": "https://dmle.iais.fraunhofer.de/2018/papers/strecht2018generalizing.pdf", "isFamilyFriendly": true, "displayUrl": "https://dmle.iais.fraunhofer.de/2018/papers/strecht2018<b>generalizing</b>.pdf", "snippet": "Such <b>parallelism</b> makes it increasingly common to generate not a single <b>model</b> but multiple models, each relating to a business unit. In the company example, each subsidiary can have a <b>model</b> to describe/predict its monthly sales level. In the university context, each course can have a <b>model</b> to describe/predict the performance of the students enrolled in it. Yet, the fact that these models are associated with only one unit makes it hard to nd generalized knowledge representative of the whole ...", "dateLastCrawled": "2021-09-02T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Intro to <b>RLlib</b>: Example Environments | by Paco Nathan | Distributed ...", "url": "https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70", "isFamilyFriendly": true, "displayUrl": "https://medium.com/distributed-computing-with-ray/intro-to-<b>rllib</b>-example-environments...", "snippet": "Train a <b>model</b> based on patterns that are associated with specific labels in <b>data</b> \u2014 <b>generalizing</b> from those patterns \u2014 then use the <b>model</b> to detect <b>similar</b> patterns within other <b>data</b>. Decisions ...", "dateLastCrawled": "2022-01-31T04:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Implicit <b>Parallelism</b> through Deep Language Embedding", "url": "http://stratosphere.eu/assets/papers/2016.SIGMODRec.ImplicitParallelism.pdf", "isFamilyFriendly": true, "displayUrl": "stratosphere.eu/assets/papers/2016.SIGMODRec.Implicit<b>Parallelism</b>.pdf", "snippet": "Implicit <b>Parallelism</b> through Deep Language Embedding Alexander Alexandrov Asterios Katsifodimos Georgi Krastev Volker Markl TU Berlin \ufb01rst.last@tu-berlin.de ABSTRACT Parallel collection processing based on second-order func-tions such as map and reduce has been widely adopted for scalable <b>data</b> analysis. Initially popularized by Google, over the past decade this programming paradigm has found its way in the core APIs of parallel <b>data</b> ow engines such as Hadoop\u2019s MapReduce, Spark\u2019s RDDs ...", "dateLastCrawled": "2021-11-19T08:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Parallel actor monitors: Disentangling task-level parallelism from data</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167642313000725", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167642313000725", "snippet": "Therefore, either <b>data</b> is encapsulated in an actor and only one task can manipulate that <b>data</b> or the <b>data</b> is distributed over multiple actors in order to exploit <b>parallelism</b>. Because of the strong <b>data</b>-task entanglement in the actor <b>model</b>, the actor programmer is forced to partition her <b>data</b> in order to exploit <b>parallelism</b>. In general, the structure and choice of algorithms strongly depend on the structure of the underlying <b>data</b> [2]. When each actor possesses a partitioning of the <b>data</b> ...", "dateLastCrawled": "2021-11-30T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Parallel Hidden Markov Model</b> - GitHub Pages", "url": "https://firebb.github.io/parahmm/", "isFamilyFriendly": true, "displayUrl": "https://firebb.github.io/parahmm", "snippet": "Multi-threading <b>parallelism</b>. Hidden Markov <b>Model</b> algorithms are <b>similar</b> to the inference procedure of the deep neural network, which is performed by a layer by layer fashion. Each layer depends on the previous layer, but in each layer, the computation is independent. Therefore, work can be easily divided for each thread and the speed up is close to linear. However, it is non-trivial to utilize SIMD <b>parallelism</b> in HMM algorithms. The challenge comes from several parts: Since all transition ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Text Classification with Few Examples using Controlled Generalization", "url": "https://aclanthology.org/N19-1319.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/N19-1319.pdf", "snippet": "<b>model</b> objectives and \ufb01ne-tuned to available train-ing <b>data</b> have recently smashed benchmark scores on a wide range of text classi\ufb01cation problems (Peters et al.,2018;Howard and Ruder,2018;De-vlin et al.,2018). Despite the strong performance of these ap-proaches for large text classi\ufb01cation datasets, chal-lenges still arise with small datasets with few, pos-sibly imbalanced, training examples per class. La-bels can be obtained cheaply from crowd workers for some languages, but there are ...", "dateLastCrawled": "2022-01-17T13:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Parallelism</b> - Examples and Definition of <b>Parallelism</b>", "url": "https://literarydevices.net/parallelism/", "isFamilyFriendly": true, "displayUrl": "https://literarydevices.net/<b>parallelism</b>", "snippet": "<b>Parallelism</b> <b>can</b> involve the repetition of words or phrases, but it also must reflect the repetition of grammatical and/or structural elements. In fact, the only requirement for <b>parallelism</b> as a literary device is the repetition of grammatical elements and/or structure in a written work\u2013apart from strictly word or phrase repetition. A good example to demonstrate the difference between <b>parallelism</b> and repetition is a soliloquy spoken by the title character in Macbeth by William Shakespeare ...", "dateLastCrawled": "2022-02-03T01:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Generalizing</b> Phylogenetic Parsimony from the Tree to the Forest", "url": "https://www.jstor.org/stable/2585249", "isFamilyFriendly": true, "displayUrl": "https://www.jstor.org/stable/2585249", "snippet": "A potentially reticulate <b>data</b> structure, the hypertree, <b>can</b> represent a forest-structured phylogenetic hypothesis and simplify the calculation of parsimony costs. A work? able parsimony criterion for hypertrees is the simultaneous minimization of mutation costs and the complexity of the forest. A method is presented to perform <b>data</b>-directed permutations on hyper? trees in a heuristic search for parsimonious solutions. For any given <b>data</b> set, parsimonious hyper? trees will range from zero ...", "dateLastCrawled": "2021-11-24T15:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Flow Algorithms for Parallel Query Optimization", "url": "https://www.cs.umd.edu/~amol/papers/cs-tr-4873.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.umd.edu/~amol/papers/cs-tr-4873.pdf", "snippet": "<b>parallelism</b> <b>can</b> exploit the available processors maximally, and should be used if the number of proces-sors exceeds the number of operators. Partitioned <b>parallelism</b>, however, suffers from higher communication overhead, is sensitive to <b>data</b> skew [12], and is typically more complicated to set up. Pipelined <b>parallelism</b> is typically considered easier to implement and reason about, and results in less communication overhead; however, it enables limited <b>parallelism</b> since the number of operators in ...", "dateLastCrawled": "2021-11-23T17:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Flow Algorithms for Parallel Query Optimization", "url": "https://www.cs.umd.edu/~amol/papers/icde08-flow.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.umd.edu/~amol/papers/icde08-flow.pdf", "snippet": "the <b>data</b> (partitioned or intra-operator <b>parallelism</b>). Typically, most systems use a combination of these, depending on the available resources, the <b>data</b> placement (in a shared-nothing system), and the execution plan itself (some execution plans are naturally more parallelizable than others). For example, partitioned <b>parallelism</b> <b>can</b> exploit the available processors maximally, and should be used if the number of processors exceeds the number of operators. Partitioned <b>parallelism</b>, how-ever ...", "dateLastCrawled": "2021-12-28T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Linearizability: A Correctness Condition for Concurrent Objects", "url": "https://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf", "snippet": "A concurrent object is a <b>data</b> object shared by concurrent processes. Linearizability is a correctness condition for concurrent objects that exploits the semantics of abstract <b>data</b> types. It permits a high degree of concurrency, yet it permits programmers to specify and reason about concurrent objects using known techniques from the sequential domain. Linearizability provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between ...", "dateLastCrawled": "2022-02-01T09:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Single-pass Parallel Prefix Scan with Decoupled Look-back", "url": "https://research.nvidia.com/sites/default/files/pubs/2016-03_Single-pass-Parallel-Prefix/nvr-2016-002.pdf", "isFamilyFriendly": true, "displayUrl": "https://research.nvidia.com/sites/default/files/pubs/2016-03_Single-pass-Parallel...", "snippet": "<b>data</b> movement: n inputs are read, n outputs are written. Our method ... defined, acyclic dataflow networks in the circuit <b>model</b> [7, 26] of parallel computation. In this <b>model</b>, prefix scan <b>can</b> <b>be thought</b> of as a forest of reduction trees, one for each output. Network size is reduced when reduction trees share intermediate partial sums. For practical use in computer software, scan networks are typically encoded as imperative algorithms in the PRAM <b>model</b> [13, 14]. The minimum circuit depth and ...", "dateLastCrawled": "2022-01-12T01:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>CIFAR-100</b> on Benchmarks.AI", "url": "https://benchmarks.ai/cifar-100", "isFamilyFriendly": true, "displayUrl": "https://benchmarks.ai/<b>cifar-100</b>", "snippet": "To address the need for efficient and task-independent <b>model</b> <b>parallelism</b>, we introduce GPipe, a pipeline <b>parallelism</b> library that allows scaling any network that <b>can</b> be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a ...", "dateLastCrawled": "2022-02-03T06:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Classification/regression with very large</b> dataset - any thoughts?", "url": "https://www.researchgate.net/post/Classification-regression-with-very-large-dataset-any-thoughts", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/<b>Classification-regression-with-very-large</b>-<b>data</b>set...", "snippet": "If there is a <b>model</b> in your <b>data</b> set, a random sample of n=10000 (say) should give you a good estimate of the <b>model</b>. You may repeat this several times to confirm the stability of the <b>model</b>. If ...", "dateLastCrawled": "2022-02-02T03:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "deep learning - <b>Data Science Stack Exchange</b>", "url": "https://datascience.stackexchange.com/questions/37281/seemingly-good-results-with-training-a-cnn-but-bad-when-testing", "isFamilyFriendly": true, "displayUrl": "https://<b>data</b>science.stackexchange.com/questions/37281", "snippet": "$\\begingroup$ I have spent the better part of the day carefully going over the code, as there is alot, the preprocessing function is called by both training and testing, so no difference there, and the mapping of labels to vectors is saved as dictionary so everything that needs it, uses the same mapping. Training and validation split was done using one of sklearns built in functions: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= (1 - self.split_ratio), random_state=42 ...", "dateLastCrawled": "2022-01-28T08:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Prettt-tty, pretty, pretty good!: Actors <b>are not a good concurrency model</b>", "url": "https://pchiusano.blogspot.com/2010/01/actors-are-not-good-concurrency-model.html", "isFamilyFriendly": true, "displayUrl": "https://pchiusano.blogspot.com/2010/01/actors-are-not-good-concurrency-<b>model</b>.html", "snippet": "And <b>generalizing</b> a bit, <b>can</b> we write the more general combining function, the one that doesn&#39;t care whether the types are &#39;heap&#39;, &#39;int&#39; and &#39;list&#39; or A, B or C? You just <b>can</b>&#39;t do this with actors since you <b>can</b>&#39;t make any assumptions about what the actor will do with the result - it might forward it to some other actor, write it to a file, extract from it the missile launch codes and launch the missile, etc.", "dateLastCrawled": "2022-01-31T15:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Handling Parallelism in a Concurrency Model</b>", "url": "https://www.researchgate.net/publication/255173624_Handling_Parallelism_in_a_Concurrency_Model", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../255173624_<b>Handling_Parallelism_in_a_Concurrency_Model</b>", "snippet": "The <b>model</b> is rigorously defined using a formal semantics, and shown to be free <b>from data</b> races. This property <b>can</b> be used to detect unsafe memory accesses when simulating the <b>model</b> together with ...", "dateLastCrawled": "2022-01-13T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Programming Paradigms</b> - cse.iitd.ac.in", "url": "https://www.cse.iitd.ac.in/~dheerajb/parallel_paradigms.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitd.ac.in/~dheerajb/parallel_paradigms.pdf", "snippet": "difficult to write as <b>compared</b> to sequential programmes. Consider the situations: \u00fc Write a sequential programme and its multiple copies on a parallel computer. <b>Parallelism</b> remains transparent to the user. \u00fc Write an Oracle application. Oracle is implicitly parallel. Run it on a parallel machine. Similarly parallel C and FORTRAN90. 2 3 Parallel Programming A parallel computer should be flexible and easy to use. This will depend upon its architecture and the way we write a parallel program ...", "dateLastCrawled": "2022-01-22T22:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "6 Types of <b>Activation Function in Neural Networks</b> You Need to Know - upGrad", "url": "https://www.upgrad.com/blog/types-of-activation-function-in-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.upgrad.com/blog/types-of-<b>activation-function-in-neural-networks</b>", "snippet": "Non-linearity imparts a better fit to the <b>data</b>. High <b>parallelism</b> promotes fast processing and hardware failure-tolerance. Generalization allows for the application of the <b>model</b> to unlearned <b>data</b>. Noise insensitivity that allows accurate prediction even for uncertain <b>data</b> and measurement errors. Learning and adaptivity allow the <b>model</b> to update its internal architecture according to the changing environment. ANN-based computing primarily aims to design advanced mathematical algorithms that ...", "dateLastCrawled": "2022-02-02T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Parallel actor monitors: Disentangling task-level parallelism from data</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167642313000725", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167642313000725", "snippet": "Therefore, either <b>data</b> is encapsulated in an actor and only one task <b>can</b> manipulate that <b>data</b> or the <b>data</b> is distributed over multiple actors in order to exploit <b>parallelism</b>. Because of the strong <b>data</b>-task entanglement in the actor <b>model</b>, the actor programmer is forced to partition her <b>data</b> in order to exploit <b>parallelism</b>. In general, the structure and choice of algorithms strongly depend on the structure of the underlying <b>data</b> [2]. When each actor possesses a partitioning of the <b>data</b> ...", "dateLastCrawled": "2021-11-30T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Turing-NLG: A <b>17-billion-parameter language model by Microsoft</b> ...", "url": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.microsoft.com</b>/en-us/research/blog/turing-nlg-a-17-billion-parameter...", "snippet": "We have observed that the bigger the <b>model</b> and the more diverse and comprehensive the pretraining <b>data</b>, the better it performs at <b>generalizing</b> to multiple downstream tasks even with fewer training examples. Therefore, we believe it is more efficient to train a large centralized multi-task <b>model</b> and share its capabilities across numerous tasks rather than train a new <b>model</b> for every task individually. Pretraining T-NLG: Hardware and software breakthroughs. Any <b>model</b> with more than 1.3 billion ...", "dateLastCrawled": "2022-02-02T23:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>CIFAR-10</b> on Benchmarks.AI", "url": "https://benchmarks.ai/cifar-10", "isFamilyFriendly": true, "displayUrl": "https://benchmarks.ai/<b>cifar-10</b>", "snippet": "To address the need for efficient and task-independent <b>model</b> <b>parallelism</b>, we introduce GPipe, a pipeline <b>parallelism</b> library that allows scaling any network that <b>can</b> be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a ...", "dateLastCrawled": "2022-02-03T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Intro to <b>RLlib</b>: Example Environments | by Paco Nathan | Distributed ...", "url": "https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70", "isFamilyFriendly": true, "displayUrl": "https://medium.com/distributed-computing-with-ray/intro-to-<b>rllib</b>-example-environments...", "snippet": "Train a <b>model</b> based on patterns that are associated with specific labels in <b>data</b> \u2014 <b>generalizing</b> from those patterns \u2014 then use the <b>model</b> to detect similar patterns within other <b>data</b>. Decisions ...", "dateLastCrawled": "2022-01-31T04:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>CIFAR-100</b> on Benchmarks.AI", "url": "https://benchmarks.ai/cifar-100", "isFamilyFriendly": true, "displayUrl": "https://benchmarks.ai/<b>cifar-100</b>", "snippet": "To address the need for efficient and task-independent <b>model</b> <b>parallelism</b>, we introduce GPipe, a pipeline <b>parallelism</b> library that allows scaling any network that <b>can</b> be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a ...", "dateLastCrawled": "2022-02-03T06:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What are the advantages and disadvantages of ANN and how do they ...", "url": "https://www.researchgate.net/post/What-are-the-advantages-and-disadvantages-of-ANN-and-how-do-they-compare-to-conventional-statistical-techniques", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/What-are-the-advantages-and-disadvantages-of-ANN-and...", "snippet": "The <b>data</b> explosion in modern drug discovery research requires sophisticated analysis methods to uncover the hidden causal relationships between single or multiple responses and a large set of ...", "dateLastCrawled": "2021-12-30T13:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Effect of <b>batch size</b> on training dynamics | by Kevin Shen | Mini ...", "url": "https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mini-distill/effect-of-<b>batch-size</b>-on-training-dynamics-21c14f7a716e", "snippet": "The <b>model</b> <b>can</b> switch to a lower <b>batch size</b> or higher learning rate anytime to achieve better test accuracy. The next interesting question to ask is whether training with large batch sizes ...", "dateLastCrawled": "2022-01-31T07:48:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Difference between instruction level <b>parallelism</b> and <b>machine</b> level ...", "url": "https://cruise4reviews.com/2022/difference-between-instruction-level-parallelism-and-machine-level-parallelism/", "isFamilyFriendly": true, "displayUrl": "https://cruise4reviews.com/2022/difference-between-instruction-level-<b>parallelism</b>-and...", "snippet": "An <b>analogy</b> is the difference between scalar of instruction-level <b>parallelism</b> otherwise conventional superscalar CPU, if the instruction stream <b>Parallelism</b> at level of instruction.. Instruction-level <b>Parallelism</b> consume all of the processing power causing individual <b>machine</b> operations to \u2022 Convert Thread-level <b>parallelism</b> to instruction-level \u2022<b>Machine</b> state registers not see the difference between SMT and real processors!) In order to understand how Jacket works, it is important to ...", "dateLastCrawled": "2022-01-24T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Savant: Automatic Parallelization of a Scheduling Heuristic with ...", "url": "https://my.ece.msstate.edu/faculty/skhan/pub/P_K_2013_NABIC.pdf", "isFamilyFriendly": true, "displayUrl": "https://my.ece.msstate.edu/faculty/skhan/pub/P_K_2013_NABIC.pdf", "snippet": "Evolutionary algorithms and <b>machine</b> <b>learning</b> were applied in [10], [11], [12]. As mentioned, our focus is not to preserve most of the source program, nor even the algorithm, but to \ufb01nd new algo-rithms and code. Genetic Programming (GP) [13] is a method to achieve such a goal. Indeed, GP aims to automatically evolve a program that displays a set of properties. <b>Parallelism</b> can be one of them. A combined evolutionary and source-to-source transformation technique was presented in [14]. There ...", "dateLastCrawled": "2021-09-02T06:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Controversy Behind Microsoft-NVIDIA\u2019s Megatron-Turing Scale", "url": "https://analyticsindiamag.com/the-controversy-behind-microsoft-nvidias-megatron-turing-scale/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/the-controversy-behind-microsoft-nvidias-megatron-turing...", "snippet": "He said, using the Megatron software to split models between different GPUs and different servers, alongside both \u2018data <b>parallelism</b> and <b>model</b> <b>parallelism</b>\u2019 and smarter networking, you are able to achieve high efficiency. \u201c50 per cent of theoretical peak performance of GPUs,\u201d added Kharya. He said it is a very high number, where you are achieving hundreds of teraFLOPs for every GPU.", "dateLastCrawled": "2022-02-03T01:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Lecture 16: Introduction to natural language processing \u2014 CPSC 330 ...", "url": "https://ubc-cs.github.io/cpsc330/lectures/16_natural-language-processing.html", "isFamilyFriendly": true, "displayUrl": "https://ubc-cs.github.io/cpsc330/lectures/16_natural-language-processing.html", "snippet": "Deep <b>learning</b> is very popular these days. &lt;-&gt; <b>Machine</b> <b>learning</b> is dominated by neural networks. 0.7564516644025884 <b>Machine</b> <b>learning</b> is dominated by neural networks. &lt;-&gt; A home-made fresh bread with butter and cheese. 0.5363564587815752", "dateLastCrawled": "2021-12-09T04:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "12.5. Training on <b>Multiple GPUs</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_computational-performance/multiple-gpus.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_computational-performance/<b>multiple-gpus</b>.html", "snippet": "Data <b>Parallelism</b>\u00b6 Assume that there are \\(k\\) GPUs on a <b>machine</b>. Given the <b>model</b> to be trained, each GPU will maintain a complete set of <b>model</b> parameters independently though parameter values across the GPUs are identical and synchronized. As an example, Fig. 12.5.3 illustrates training with data <b>parallelism</b> when \\(k=2\\).", "dateLastCrawled": "2022-02-02T06:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Distributed Machine Learning for Big</b> Data and Streaming - Guavus - Go ...", "url": "https://www.guavus.com/technical-blog/distributed-machine-learning-for-big-data-and-streaming/", "isFamilyFriendly": true, "displayUrl": "https://www.guavus.com/technical-blog/<b>distributed-machine-learning-for-big</b>-data-and...", "snippet": "The same <b>analogy</b> applies to granularity of approximation of a non-linear <b>model</b> through linear models. <b>Machine</b> <b>Learning</b> at High Speeds. There have been many advances in this area, for example, the High-Performance Computing (HPC) community has been actively researching in this area for decades. As a result, the HPC community has developed some basic building blocks for vector and matrix operations in the form of BLAS (Basic Linear Algebra Subprograms), which has existed for more than 40 years ...", "dateLastCrawled": "2022-01-21T14:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Gensim Tutorial - A Complete Beginners Guide - <b>Machine</b> <b>Learning</b> Plus", "url": "https://www.machinelearningplus.com/nlp/gensim-tutorial/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machinelearning</b>plus.com/nlp/gensim-tutorial", "snippet": "Gensim Tutorial \u2013 A Complete Beginners Guide. October 16, 2018. Selva Prabhakaran. Gensim is billed as a Natural Language Processing package that does \u2018Topic Modeling for Humans\u2019. But it is practically much more than that. It is a leading and a state-of-the-art package for processing texts, working with word vector models (such as ...", "dateLastCrawled": "2022-02-02T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "13.2. Fine-Tuning \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_computer-vision/fine-tuning.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_computer-vision/fine-tuning.html", "snippet": "13.2.1. Steps\u00b6. In this section, we will introduce a common technique in transfer <b>learning</b>: fine-tuning.As shown in Fig. 13.2.1, fine-tuning consists of the following four steps:. Pretrain a neural network <b>model</b>, i.e., the source <b>model</b>, on a source dataset (e.g., the ImageNet dataset).. Create a new neural network <b>model</b>, i.e., the target <b>model</b>.This copies all <b>model</b> designs and their parameters on the source <b>model</b> except the output layer.", "dateLastCrawled": "2022-02-02T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Do we really need <b>GPU</b> for Deep <b>Learning</b>? - CPU vs <b>GPU</b> | by ... - Medium", "url": "https://medium.com/@shachishah.ce/do-we-really-need-gpu-for-deep-learning-47042c02efe2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@shachishah.ce/do-we-really-need-<b>gpu</b>-for-deep-<b>learning</b>-47042c02efe2", "snippet": "Training a <b>model</b> in deep <b>learning</b> requires a huge amount of Dataset, hence the large computational operations in terms of memory. To compute the data efficiently,<b>GPU</b> is the optimum choice. The ...", "dateLastCrawled": "2022-01-30T11:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "300+ TOP <b>Neural Networks Multiple Choice Questions and Answers</b>", "url": "https://engineeringinterviewquestions.com/neural-networks-multiple-choice-questions-and-answers/", "isFamilyFriendly": true, "displayUrl": "https://engineeringinterviewquestions.com/<b>neural-networks-multiple-choice-questions</b>...", "snippet": "35. How many types of <b>learning</b> are available in <b>machine</b> <b>learning</b>? a) 1 b) 2 c) 3 d) 4. Answer: c Explanation: The three types of <b>machine</b> <b>learning</b> are supervised, unsupervised and reinforcement. 36. Choose from the following that are Decision Tree nodes. a) Decision Nodes b) Weighted Nodes c) Chance Nodes d) End Nodes. Answer: a, c, d. 37 ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>Power Ef\ufb01cient Neural Network Implementation on Heterogeneous FPGA</b> ...", "url": "https://users.cs.fiu.edu/~chens/PDF/IRI19_FPGA.pdf", "isFamilyFriendly": true, "displayUrl": "https://users.cs.fiu.edu/~chens/PDF/IRI19_FPGA.pdf", "snippet": "<b>Model parallelism can be thought of as</b> partitioning the neural networks into subprocesses, which are computed in different devices. Such parallelism allows a model to be trained distributively and reduces network traf\ufb01c [3]. This approach is particularly bene\ufb01cial in big data, multimedia, and/or real-time applications [15] [17] [19] [20] where the size of data inhibits \ufb01le transfers. In this paper, we propose a model parallelism architecture for DNNs that is distributively computed on ...", "dateLastCrawled": "2022-02-03T05:55:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(model parallelism)  is like +(generalizing from data)", "+(model parallelism) is similar to +(generalizing from data)", "+(model parallelism) can be thought of as +(generalizing from data)", "+(model parallelism) can be compared to +(generalizing from data)", "machine learning +(model parallelism AND analogy)", "machine learning +(\"model parallelism is like\")", "machine learning +(\"model parallelism is similar\")", "machine learning +(\"just as model parallelism\")", "machine learning +(\"model parallelism can be thought of as\")", "machine learning +(\"model parallelism can be compared to\")"]}