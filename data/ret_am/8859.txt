{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Disparate</b> <b>Impact</b> in <b>Machine</b> <b>Learning</b> \u00bb Dome | Blog Archive | Boston ...", "url": "https://sites.bu.edu/dome/2020/06/08/disparate-impact-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://sites.bu.edu/dome/2020/06/08/<b>disparate</b>-<b>impact</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "Thus, instead of relying on either <b>disparate</b> <b>impact</b> or <b>disparate</b> treatment theory, perhaps legal analysis of discrimination in <b>machine</b> <b>learning</b> should be entirely outcomes-driven. If, in fact, an <b>algorithm</b> wrongly predicts the likelihood of an event occurring, and that <b>algorithm</b> is less accurate for protected class members than unprotected class members, the <b>algorithm</b> should be considered prima facie discriminatory. Such a solution is viable for examining recidivism, interest rates and loan ...", "dateLastCrawled": "2021-12-09T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Disparate</b> <b>Impact</b> Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-<b>impact</b>-analysis", "snippet": "<b>Disparate</b> <b>Impact</b> Analysis (DIA) Sensitivity Analysis(SA) As a matter of speaking, the above two features provide a solution to a common problem in ML: the multiplicity of good models. It is well understood that for the same set of input features and prediction targets, complex <b>machine</b> <b>learning</b> algorithms can produce multiple accurate models with very similar, but not the same, internal architectures: the multiplicity of good models [1]. This alone is an obstacle to interpretation, but when ...", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Algorithmic Fairness in <b>Machine</b> <b>Learning</b>", "url": "https://www2.cs.duke.edu/courses/spring19/compsci216/lectures/11-fairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www2.cs.duke.edu/courses/spring19/compsci216/lectures/11-fairness.pdf", "snippet": "<b>Disparate</b> <b>Impact</b> \u2022Arguably this is the only good measure if you think the dataare biased and you have a strong prior belief protected status is uncorrelated with outcomes. \u2022\u201cIn Griggs v. Duke Power Co. [20], the US Supreme Court ruled a business hiring decision illegal if it resulted in <b>disparate</b> <b>impact</b> by", "dateLastCrawled": "2022-01-21T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Fall 2020 Journal | Algorithms and Child Welfare: The <b>Disparate</b> <b>Impact</b> ...", "url": "https://bppj.berkeley.edu/2021/02/02/algorithms-and-child-welfare-the-disparate-impact-of-family-surveillance-in-risk-assessment-technologies/", "isFamilyFriendly": true, "displayUrl": "https://bppj.berkeley.edu/2021/02/02/<b>algorithms</b>-and-child-welfare-the-<b>disparate</b>-<b>impact</b>...", "snippet": "We believe that if <b>machine</b> <b>learning</b> is to continue to be used in social services, the history of the data must be considered [34]. Through our literature review, we did not find evidence of regulation over the child welfare data used in <b>machine</b> <b>learning</b> technologies. At the time of writing, Pennsylvania\u2019s statutes on Child Protective Services did not include any guidance on the use of <b>machine</b> <b>learning</b> or artificial intelligence. Searches for the words \u201cautomated\u201d and \u201c<b>algorithm</b> ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Fairness in Machine Learning: Part</b> I - cs.uwaterloo.ca", "url": "https://cs.uwaterloo.ca/~xihe/cs848_f19/slides/05-module2-FairML1.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.uwaterloo.ca/~xihe/cs848_f19/slides/05-module2-FairML1.pdf", "snippet": "Certifying <b>Disparate</b> <b>Impact</b> \u2022<b>Disparate</b> <b>impact</b> is related to predictability. So what? \u2022Given D, we estimate: 1.The predictability (call it \u2018) of D. 2.B, the fraction of class X=0 predicted to have outcome 1. \u2022This yields an estimate on the possible <b>disparate</b> <b>impact</b> of any classifier built on D. \u2022How do we get these estimates?", "dateLastCrawled": "2021-09-14T22:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "When is a Chair not a Chair? Big Data Algorithms, <b>Disparate</b> <b>Impact</b>, and ...", "url": "http://users.umiacs.umd.edu/~oard/desi7/papers/JS.pdf", "isFamilyFriendly": true, "displayUrl": "users.umiacs.umd.edu/~oard/desi7/papers/JS.pdf", "snippet": "Big Data Algorithms, <b>Disparate</b> <b>Impact</b>, ... the equation,3 and bigger is better, where \u201cvery large data sets can improve even the worst <b>machine</b> <b>learning</b> algorithms.\u201d4 While there are detractors from this theory who argue that developers are sometimes too trusting of data to the detriment of <b>algorithm</b> development,5 the majority of current, conventional wisdom trusts in history and volume as proxies for quality. Quality Data comes from the Real World The historical portion of high quality ...", "dateLastCrawled": "2021-11-05T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Mitigating Bias in AI with</b> AIF360 | by Bryan Truong | Towards Data Science", "url": "https://towardsdatascience.com/mitigating-bias-in-ai-with-aif360-b4305d1f88a9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>mitigating-bias-in-ai-with</b>-aif360-b4305d1f88a9", "snippet": "As shown above, I arrived at a <b>disparate</b> <b>impact</b> ratio of .66. This <b>disparate</b> <b>impact</b> ratio is worse than the one from the actual test split, which was .83\u2013 less biased than the .66 of the model we just trained. This is not a surprise, as it has been shown time and time again that biases can easily get amplified in <b>machine</b> <b>learning</b> models.", "dateLastCrawled": "2022-02-02T22:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Differential Fairness for <b>Machine Learning and Artificial Intelligence</b> ...", "url": "http://jfoulds.informationsystems.umbc.edu/slides/2018/Foulds_Nov_14_2018_MD_AI_Differential_Fairness.pdf", "isFamilyFriendly": true, "displayUrl": "jfoulds.informationsystems.umbc.edu/slides/2018/Foulds_Nov_14_2018_MD_AI_Differential...", "snippet": "<b>Machine</b> <b>Learning</b> \u2022 <b>Machine</b> <b>learning</b> algorithms, which make predictions based on data, are having an increasing <b>impact</b> on our daily lives. \u2022 Example: credit scoring - predicting whether you will repay or default on a loan \u2022 Given the feature vector, the <b>algorithm</b> learns to predict the class label", "dateLastCrawled": "2022-01-30T16:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Big Data&#39;s Disparate Impact</b> by Solon <b>Barocas</b>, Andrew D. Selbst :: SSRN", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899", "snippet": "In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining\u2019s victims would seem to lie in <b>disparate</b> <b>impact</b> doctrine. Case law and the Equal Employment Opportunity Commission\u2019s Uniform Guidelines, though, hold that a practice can be justified as a business necessity when its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. Unless there is a reasonably practical way ...", "dateLastCrawled": "2022-01-29T21:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Disparate</b> <b>Impact</b> Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-<b>impact</b>-analysis", "snippet": "<b>Disparate</b> <b>Impact</b> Analysis (DIA) ... complex <b>machine</b> <b>learning</b> algorithms can produce multiple accurate models with very <b>similar</b>, but not the same, internal architectures: the multiplicity of good models [1]. This alone is an obstacle to interpretation, but when using these types of tools as interpretation tools or with interpretation tools, it is important to remember that details of explanations can change across multiple accurate models. This instability of explanations is a driving factor ...", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "AI Fairness \u2014 Explanation of <b>Disparate Impact</b> Remover | by Stacey ...", "url": "https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/ai-fairness-explanation-of-<b>disparate-impact</b>-remover-ce0...", "snippet": "<b>Disparate Impact</b>. <b>Disparate Impact</b> is a metric to evaluate fairness. It compares the proportion of individuals that receive a positive output for two groups: an unprivileged group and a privileged group. The calculation is the proportion of the unprivileged group that received the positive outcome divided by the proportion of the privileged group that received the positive outcome. The industry standard is a four-fifths rule: if the unprivileged group receives a positive outcome less than 80 ...", "dateLastCrawled": "2022-01-29T05:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>AI Fairness \u2014 Explanation of Disparate Impact Remover</b> - Adolfo Eliaz\u00e0t ...", "url": "https://adolfoeliazat.com/2021/05/06/ai-fairness-explanation-of-disparate-impact-remover/", "isFamilyFriendly": true, "displayUrl": "https://adolfoeliazat.com/2021/05/06/<b>ai-fairness-explanation-of-disparate-impact-remover</b>", "snippet": "<b>Disparate</b> <b>Impact</b>. <b>Disparate</b> <b>Impact</b> is a metric to evaluate fairness. It compares the proportion of individuals that receive a positive output for two groups: an unprivileged group and a privileged group. The calculation is the proportion of the unprivileged group that received the positive outcome divided by the proportion of the privileged group that received the positive outcome. The industry standard is a four-fifths rule: if the unprivileged group receives a positive outcome less than 80 ...", "dateLastCrawled": "2022-01-22T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Fall 2020 Journal | Algorithms and Child Welfare: The <b>Disparate</b> <b>Impact</b> ...", "url": "https://bppj.berkeley.edu/2021/02/02/algorithms-and-child-welfare-the-disparate-impact-of-family-surveillance-in-risk-assessment-technologies/", "isFamilyFriendly": true, "displayUrl": "https://bppj.berkeley.edu/2021/02/02/<b>algorithms</b>-and-child-welfare-the-<b>disparate</b>-<b>impact</b>...", "snippet": "We believe that if <b>machine</b> <b>learning</b> is to continue to be used in social services, the history of the data must be considered [34]. Through our literature review, we did not find evidence of regulation over the child welfare data used in <b>machine</b> <b>learning</b> technologies. At the time of writing, Pennsylvania\u2019s statutes on Child Protective Services did not include any guidance on the use of <b>machine</b> <b>learning</b> or artificial intelligence. Searches for the words \u201cautomated\u201d and \u201c<b>algorithm</b> ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fairness in Machine Learning: Part</b> I - cs.uwaterloo.ca", "url": "https://cs.uwaterloo.ca/~xihe/cs848_f19/slides/05-module2-FairML1.pdf", "isFamilyFriendly": true, "displayUrl": "https://cs.uwaterloo.ca/~xihe/cs848_f19/slides/05-module2-FairML1.pdf", "snippet": "Certifying <b>Disparate</b> <b>Impact</b> \u2022<b>Disparate</b> <b>impact</b> is related to predictability. So what? \u2022Given D, we estimate: 1.The predictability (call it \u2018) of D. 2.B, the fraction of class X=0 predicted to have outcome 1. \u2022This yields an estimate on the possible <b>disparate</b> <b>impact</b> of any classifier built on D. \u2022How do we get these estimates?", "dateLastCrawled": "2021-09-14T22:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Disparate</b> <b>impact</b> in a <b>machine</b> <b>learning</b> model originates from bias in either the data or the algorithms. A popular example is the prejudicially biased data used for recidivism prediction. Due to <b>disparate</b> socioeconomic factors and systemic racism in the United States, blacks have historically been (and continue to be) incarcerated at higher rates than whites . Not coincidentally, blacks are also exonerated due to wrongful accusation at a considerably higher rate than whites . A recidivism ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Differential Fairness for <b>Machine Learning and Artificial Intelligence</b> ...", "url": "http://jfoulds.informationsystems.umbc.edu/slides/2018/Foulds_Nov_14_2018_MD_AI_Differential_Fairness.pdf", "isFamilyFriendly": true, "displayUrl": "jfoulds.informationsystems.umbc.edu/slides/2018/Foulds_Nov_14_2018_MD_AI_Differential...", "snippet": "\u2022 <b>Machine</b> <b>learning</b> algorithms, which make predictions based on data, are having an increasing <b>impact</b> on our daily lives. \u2022 Example: credit scoring - predicting whether you will repay or default on a loan \u2022 Given the feature vector, the <b>algorithm</b> learns to predict the class label \u2022 The models are \u201ctrained\u201d on many labeled feature vectors \u2022 This is called classification, an instance of supervised <b>machine</b> <b>learning</b> 2 # Late Payments % of available credit used Previous defaults ...", "dateLastCrawled": "2022-01-30T16:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Algorithmic Bias and Regularisation in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/algorithmic-bias-and-regularisation-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>algorithmic-bias-and-regularisation-in-machine-learning</b>", "snippet": "Often, what is termed algorithmic bias in <b>machine</b> <b>learning</b> will be due to historic bias in the training data. But sometimes the bias may be introduced (or at least exacerbated) by the <b>algorithm</b> itself. The ways in which algorithms can actually accentuate bias has not received a lot of attention with researchers focusing directly on methods to eliminate bias - no matter the source.", "dateLastCrawled": "2022-01-26T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-<b>learning</b>-3ff8ba1040cb", "snippet": "<b>Machine</b> <b>Learning</b>, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective. AI is objective only in the sense of <b>learning</b> what human teaches. The data provided by human can be highly-biased. It has been found in 2016 that COMPAS, the <b>algorithm</b> used for recidivism prediction produces much higher false positive rate for black people than white people(see Fig2, Larson et al. ProPublica, 2016). Fig2: The bias in COMPAS. (from Larson ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Removing Unfair Bias in <b>Machine</b> <b>Learning</b>", "url": "https://community.ibm.com/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=618c7917-8d2b-ca78-5ecc-2286486b9c69&forceDialog=0", "isFamilyFriendly": true, "displayUrl": "https://community.ibm.com/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=...", "snippet": "Bias In the <b>Machine</b> <b>Learning</b> Pipeline dataset metric pre-processing <b>algorithm</b> in-processing <b>algorithm</b> post-processing <b>algorithm</b> classifier metric . Where Can You Intervene in the Pipeline? \u2022If you can modify the Training Data, then pre-processing can be used. \u2022If you can modify the <b>Learning</b> <b>Algorithm</b>, then in-processing can be used. \u2022If you can only treat the learned model as a black box and can\u2019t modify the training data or <b>learning</b> <b>algorithm</b>, then only post-processing can be used ...", "dateLastCrawled": "2022-01-29T04:03:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Handling Discriminatory Biases in Data for <b>Machine Learning</b> | by ...", "url": "https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning</b>-and-<b>discrimination</b>-2ed1a8b01038", "snippet": "There are two forms of <b>discrimination</b> that we will refer to as <b>disparate</b> <b>impact</b> and <b>disparate</b> treatment. ... In this case, the biases of humans are not mitigated by the <b>machine learning</b> <b>algorithm</b>. In fact, they are reproduced in the classifications that are made. Why does this happen? Recidivism scores such as those made by the Northpointe software are based on prior arrests, age of first police contact, parents\u2019 incarceration record. This information is shaped by biases in the world (such ...", "dateLastCrawled": "2022-01-29T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Algorithmic Fairness in <b>Machine</b> <b>Learning</b>", "url": "https://www2.cs.duke.edu/courses/spring19/compsci216/lectures/11-fairness.pdf", "isFamilyFriendly": true, "displayUrl": "https://www2.cs.duke.edu/courses/spring19/compsci216/lectures/11-fairness.pdf", "snippet": "<b>Disparate</b> <b>Impact</b> \u2022Arguably this is the only good measure if you think the dataare biased and you have a strong prior belief protected status is uncorrelated with outcomes. \u2022\u201cIn Griggs v. Duke Power Co. [20], the US Supreme Court ruled a business hiring decision illegal if it resulted in <b>disparate</b> <b>impact</b> by", "dateLastCrawled": "2022-01-21T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "HIRING BY <b>ALGORITHM</b> PREDICTING AND PREVENTING <b>DISPARATE</b> <b>IMPACT</b>", "url": "http://sorelle.friedler.net/papers/SSRN-id2746078.pdf", "isFamilyFriendly": true, "displayUrl": "sorelle.friedler.net/papers/SSRN-id2746078.pdf", "snippet": "Major advances in <b>machine</b> <b>learning</b> have encouraged corporations to rely on Big Data and algorithmic decision making with the presumption that such decisions are efficient and impartial. In this Essay, we show that protected information that is encoded in seemingly facially neutral data could be predicted with high accuracy by algorithms and employed in the decision-making process, thus resulting in a <b>disparate</b> <b>impact</b> on protected classes. We then demonstrate how it is possible to repair the ...", "dateLastCrawled": "2022-01-22T08:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Does mitigating ML&#39;s <b>disparate</b> <b>impact</b> require <b>disparate</b> treatment? - DeepAI", "url": "https://deepai.org/publication/does-mitigating-ml-s-disparate-impact-require-disparate-treatment", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/does-mitigating-ml-s-<b>disparate</b>-<b>impact</b>-require-<b>disparate</b>...", "snippet": "Algorithms exhibit <b>disparate</b> <b>impact</b> if they affect subgroups differently. <b>Disparate</b> <b>impact</b> <b>can</b> arise unintentionally and absent <b>disparate</b> treatment. The natural way to reduce <b>disparate</b> <b>impact</b> would be to apply <b>disparate</b> treatment in favor of the disadvantaged group, i.e. to apply affirmative action. However, owing to the practice&#39;s contested ...", "dateLastCrawled": "2021-12-15T05:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Mitigating Bias in AI with</b> AIF360 | by Bryan Truong | Towards Data Science", "url": "https://towardsdatascience.com/mitigating-bias-in-ai-with-aif360-b4305d1f88a9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>mitigating-bias-in-ai-with</b>-aif360-b4305d1f88a9", "snippet": "As shown above, I arrived at a <b>disparate</b> <b>impact</b> ratio of .66. This <b>disparate</b> <b>impact</b> ratio is worse than the one from the actual test split, which was .83\u2013 less biased than the .66 of the model we just trained. This is not a surprise, as it has been shown time and time again that biases <b>can</b> easily get amplified in <b>machine</b> <b>learning</b> models.", "dateLastCrawled": "2022-02-02T22:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fall 2020 Journal | Algorithms and Child Welfare: The <b>Disparate</b> <b>Impact</b> ...", "url": "https://bppj.berkeley.edu/2021/02/02/algorithms-and-child-welfare-the-disparate-impact-of-family-surveillance-in-risk-assessment-technologies/", "isFamilyFriendly": true, "displayUrl": "https://bppj.berkeley.edu/2021/02/02/<b>algorithms</b>-and-child-welfare-the-<b>disparate</b>-<b>impact</b>...", "snippet": "We believe that if <b>machine</b> <b>learning</b> is to continue to be used in social services, the history of the data must be considered [34]. Through our literature review, we did not find evidence of regulation over the child welfare data used in <b>machine</b> <b>learning</b> technologies. At the time of writing, Pennsylvania\u2019s statutes on Child Protective Services did not include any guidance on the use of <b>machine</b> <b>learning</b> or artificial intelligence. Searches for the words \u201cautomated\u201d and \u201c<b>algorithm</b> ...", "dateLastCrawled": "2022-02-03T01:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Does mitigating ML&#39;s <b>disparate impact require disparate treatment</b>?", "url": "https://www.researchgate.net/publication/321180707_Does_mitigating_ML's_disparate_impact_require_disparate_treatment", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321180707_Does_mitigating_ML", "snippet": "<b>Disparate</b> <b>impact</b> <b>can</b> arise unintentionally and absent <b>disparate</b> treatment. The natural way to reduce <b>disparate</b> <b>impact</b> would be to apply <b>disparate</b> treatment in favor of the disadvantaged group, i.e ...", "dateLastCrawled": "2021-10-19T15:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Fair prediction with <b>disparate</b> <b>impact</b>: A study of bias in recidivism ...", "url": "https://www.arxiv-vanity.com/papers/1703.00056/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.00056", "snippet": "Keywords: <b>disparate</b> <b>impact</b>; bias; recidivism prediction; risk assessment; fair <b>machine</b> <b>learning</b> 1 Introduction Risk assessment instruments are gaining increasing popularity within the criminal justice system, with versions of such instruments being used or considered for use in pre-trial decision-making, parole decisions, and in some states even sentencing [ 1 , 2 , 3 ] .", "dateLastCrawled": "2022-01-11T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Algorithmic bias detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/<b>algorithm</b>ic-bias-detection-and-mitigation-best-", "snippet": "If left unchecked, biased algorithms <b>can</b> lead to decisions which <b>can</b> have a collective, <b>disparate</b> <b>impact</b> on certain groups of people even without the programmer\u2019s intention to discriminate. The ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "When is a Chair not a Chair? Big Data Algorithms, <b>Disparate</b> <b>Impact</b>, and ...", "url": "http://users.umiacs.umd.edu/~oard/desi7/papers/JS.pdf", "isFamilyFriendly": true, "displayUrl": "users.umiacs.umd.edu/~oard/desi7/papers/JS.pdf", "snippet": "<b>Algorithm</b> quality is in turn intimately related to data quality and size, and as collection measures and relative data sizes increase in volume, algorithms commensurately increase in complexity. This makes insight into the inner workings of <b>algorithm</b> function more difficult, which is challenging at a time where both the effects of algorithms and the data associated with their development and utilization are undergoing additional scrutiny. This paper examines these trends, and considers both ...", "dateLastCrawled": "2021-11-05T02:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b>", "url": "https://www.ftc.gov/system/files/documents/public_events/1567421/fuaserisinghsrinivasan_updated2.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ftc.gov</b>/system/files/documents/public_events/1567421/fuaserisinghsriniva...", "snippet": "to <b>disparate</b> <b>impact</b> particularly when there are di\u21b5erences among groups based on demographic classes. In response, several \u201c<b>fair\u201d machine learning algorithms</b> that require <b>impact</b> parity (e.g., equal opportunity) have recently been proposed to adjust for the societal inequalities; advocates propose changing the law to allow the use of protected class-speci\ufb01c decision rules. We show that these \u201cfair\u201d algorithms that require <b>impact</b> parity, while conceptually appealing, <b>can</b> make ...", "dateLastCrawled": "2022-02-03T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Disparate</b> <b>Impact</b> Analysis", "url": "https://h2oai.github.io/tutorials/disparate-impact-analysis/", "isFamilyFriendly": true, "displayUrl": "https://h2oai.github.io/tutorials/<b>disparate</b>-<b>impact</b>-analysis", "snippet": "And such explanatory results <b>can</b> be accessed by the <b>Disparate</b> <b>Impact</b> Analysis and Sensitivity Analysis(SA) features/tools. With the above in mind, let us discover how we <b>can</b> better understand our models. References [1] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The Elements of Statistical <b>Learning</b>. Springer, New York, 2001. You will need the following to be able to do this self-paced course: Basic knowledge of <b>Machine</b> <b>Learning</b> and Statistics; Basic knowledge of Driverless AI or ...", "dateLastCrawled": "2022-02-03T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "LOCKED OUT BY BIG DATA: HOW BIG DATA, ALGORITHMS AND <b>MACHINE</b> <b>LEARNING</b> ...", "url": "http://blogs.law.columbia.edu/hrlr/files/2020/11/251_Schneider.pdf", "isFamilyFriendly": true, "displayUrl": "blogs.law.columbia.edu/hrlr/files/2020/11/251_Schneider.pdf", "snippet": "<b>disparate</b> <b>impact</b> analysis\u2014and has proposed to specifically immunize housing providers that rely on algorithms in decision-making from liability.4 This article proceeds in five parts. Part I describes the advent of big data, algorithmic decision-making, and <b>machine</b> <b>learning</b>. Part II describes the relevant provisions of the Fair Housing Act and", "dateLastCrawled": "2022-01-27T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Evaluating Fairness in <b>Machine</b> <b>Learning</b> - <b>github.com</b>", "url": "https://github.com/KenSciResearch/fairMLHealth/blob/integration/docs/resources/Evaluating_Fairness.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/KenSciResearch/fairMLHealth/blob/integration/docs/resources/...", "snippet": "<b>Disparate</b> <b>impact</b> in a <b>machine</b> <b>learning</b> model originates from bias in either the data or the algorithms. A popular example is the prejudicially biased data used for recidivism prediction. Due to <b>disparate</b> socioeconomic factors and systemic racism in the United States, blacks have historically been (and continue to be) incarcerated at higher rates than whites . Not coincidentally, blacks are also exonerated due to wrongful accusation at a considerably higher rate than whites . A recidivism ...", "dateLastCrawled": "2022-02-02T03:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare", "url": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "Often, the approach to fairness draws upon the legal standard of <b>disparate</b> <b>impact</b>[2][3]. <b>Disparate</b> <b>impact</b> occurs when the predicted outcomes are different for different groups. Some examples of when this metric is used are recidivism[4], hiring[5][6], and loan applications[2]. This standard metric accounts for only one factor: the rate at which the <b>algorithm</b> predicts a person should benefit from a particular classification.", "dateLastCrawled": "2022-02-01T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bias and Fairness in Machine Learning</b> - Abhishek Tiwari", "url": "https://www.abhishek-tiwari.com/bias-and-fairness-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.abhishek-tiwari.com/<b>bias-and-fairness-in-machine-learning</b>", "snippet": "Wall Street Journal investigators showed that Staples\u2019 online pricing <b>algorithm</b> discriminated against lower-income people; Black people were more likely to be assessed as having a higher risk of recidivism when using commercial prediction tools such as COMPAS ; An insurance company that used <b>machine</b> <b>learning</b> to workout insurance premiums involuntarily discriminated against elderly patients; A credit card company used tracking information to personalize offers steering minorities into ...", "dateLastCrawled": "2022-01-29T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding and Reducing Bias in <b>Machine Learning</b> | by Jaspreet ...", "url": "https://towardsdatascience.com/understanding-and-reducing-bias-in-machine-learning-6565e23900ac", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-and-reducing-bias-in-<b>machine-learning</b>...", "snippet": "A study done by ProPublica (Jeff Larson, 2016) showed that the <b>algorithm</b> was twice as likely to label black defenders as high risk who eventually did not reoffend as <b>compared</b> to white defenders. This is given by the False Positive rate (FP rate) of black defendants which is 44.85 (i.e. 44.85 percent of the black defendants classified as reoffending did not reoffend) as <b>compared</b> to 23.45 for white defendants. However, Northpointe came with the rebuttal that according to the measures they used ...", "dateLastCrawled": "2022-01-28T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "DP-SGD vs PATE: Which Has Less <b>Disparate</b> <b>Impact</b> on Model Accuracy? | DeepAI", "url": "https://deepai.org/publication/dp-sgd-vs-pate-which-has-less-disparate-impact-on-model-accuracy", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/dp-sgd-vs-pate-which-has-less-<b>disparate</b>-<b>impact</b>-on-model...", "snippet": "Recent advances in differentially private deep <b>learning</b> have demonstrated that application of differential privacy, specifically the DP-SGD <b>algorithm</b>, has a <b>disparate</b> <b>impact</b> on different sub-groups in the population, which leads to a significantly high drop-in model utility for sub-populations that are under-represented (minorities), <b>compared</b> to well-represented ones. In this work, we aim to compare PATE, another mechanism for training deep <b>learning</b> models using differential privacy, with DP ...", "dateLastCrawled": "2022-01-05T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "AI Fairness -A Brief Introduction to AI Fairness 360 | by ...", "url": "https://transformernlp.medium.com/ai-fairness-a-brief-introduction-to-ai-fairness-360-b2e39c96ca49", "isFamilyFriendly": true, "displayUrl": "https://transformernlp.medium.com/ai-fairness-a-brief-introduction-to-ai-fairness-360...", "snippet": "The choice among <b>algorithm</b> categories <b>can</b> partially be made based on the user persona\u2019s ability to intervene at different parts of a <b>machine</b> <b>learning</b> pipeline. If the user is allowed to modify the training data, then pre-processing <b>can</b> be used. If the user is allowed to change the <b>learning</b> <b>algorithm</b>, then in-processing <b>can</b> be used. If the user <b>can</b> only treat the learned model as a black box without any ability to modify the training data or <b>learning</b> <b>algorithm</b>, then only post-processing <b>can</b> ...", "dateLastCrawled": "2022-01-18T14:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "&#39;<b>Un&#39;Fair Machine Learning Algorithms</b> by Runshan Fu, Manmohan Aseri ...", "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "isFamilyFriendly": true, "displayUrl": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3408275", "snippet": "<b>Compared</b> to the current law, which requires treatment parity, these ``fair&#39;&#39; algorithms, which require <b>impact</b> parity, limit the benefits of a more accurate <b>algorithm</b> for a firm. As a result, profit maximizing firms could under-invest in <b>learning</b>, i.e., improving the accuracy of their <b>machine</b> <b>learning</b> algorithms. We show that the investment in <b>learning</b> decreases when misclassification is costly, which is exactly the case when greater accuracy is otherwise desired. Our paper highlights the ...", "dateLastCrawled": "2022-01-29T16:56:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare", "url": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.closedloop.ai/post/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "Often, the approach to fairness draws upon the legal standard of <b>disparate</b> <b>impact</b>[2][3]. <b>Disparate</b> <b>impact</b> occurs when the predicted outcomes are different for different groups. Some examples of when this metric is used are recidivism[4], hiring[5][6], and loan applications[2]. This standard metric accounts for only one factor: the rate at which the algorithm predicts a person should benefit from a particular classification.", "dateLastCrawled": "2022-02-01T00:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A New Metric for Quantifying <b>Machine</b> <b>Learning</b> Fairness in Healthcare ...", "url": "https://towardsdatascience.com/a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare-closedloop-ai-fc07b9c83487", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-new-metric-for-quantifying-<b>machine</b>-<b>learning</b>-fairness...", "snippet": "<b>Disparate</b> <b>impact</b> occurs when the predicted outcomes are different for different groups. Some examples of when this metric is used are recidivism[4], hiring[5][6], and loan applications[2]. This standard metric accounts for only one factor: the rate at which the algorithm predicts a person should benefit from a particular classification. In the context of healthcare, the standard of <b>disparate</b> <b>impact</b> is entirely inappropriate. The above examples have a common characteristic; every individual ...", "dateLastCrawled": "2022-01-17T14:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "A <b>machine learning</b> technique that iteratively combines a set of simple and not very accurate classifiers ... <b>disparate</b> <b>impact</b>. #fairness. Making decisions about people that <b>impact</b> different population subgroups disproportionately. This usually refers to situations where an algorithmic decision-making process harms or benefits some subgroups more than others. For example, suppose an algorithm that determines a Lilliputian&#39;s eligibility for a miniature-home loan is more likely to classify them ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Assessing <b>Disparate</b> <b>Impact</b> of Personalized Interventions ...", "url": "https://proceedings.neurips.cc/paper/8603-assessing-disparate-impact-of-personalized-interventions-identifiability-and-bounds.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/8603-assessing-<b>disparate</b>-<b>impact</b>-of-personalized...", "snippet": "result in <b>disparate</b> <b>impact</b> (with regards to social welfare) for the same reasons that these disparities occur in <b>machine</b> <b>learning</b> classi\ufb01cation models [21]. (See Appendix C for an expanded discussion on our use of the term \u201c<b>disparate</b> <b>impact</b>.\u201d) However, in the problem of personalized interventions, the \u201cfundamental problem of causal inference,\u201d that outcomes are not observed for interventions not administered, poses a fundamental challenge for evaluating the fairness of any ...", "dateLastCrawled": "2021-09-17T16:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Feature Engineering for Machine Learning</b>: Why and How | by ...", "url": "https://medium.com/analytics-vidhya/feature-engineering-for-machine-learning-stem-to-shtem-submission-76903112e437", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>feature-engineering-for-machine-learning</b>-stem-to...", "snippet": "Here\u2019s a simple <b>analogy</b>: a student named Timmy, analogous to a supervised <b>machine</b> <b>learning</b> model, has spent the last few weeks studying for a math test so that he can answer questions correctly ...", "dateLastCrawled": "2021-09-13T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) On the <b>Impact</b> of <b>Machine Learning Architecture without Architects</b>?", "url": "https://www.researchgate.net/publication/335175592_On_the_Impact_of_Machine_Learning_Architecture_without_Architects", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335175592_On_the_<b>Impact</b>_of_<b>Machine</b>_<b>Learning</b>...", "snippet": "3 The <b>Impact</b> of <b>Machine</b> <b>Learning</b> in Architectural Design . Current research [8-13] already illustrates some ML applications in architecture. We . complement t hose studies by hypothesizi ng on ...", "dateLastCrawled": "2021-11-05T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Algorithmic injustice: a relational ethics approach", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7892355/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7892355", "snippet": "<b>Machine</b> classification and prediction are practices that act directly upon the world and result in tangible <b>impact</b>.64 Various companies, institutes, and governments use <b>machine</b>-<b>learning</b> systems across a variety of areas. These systems process people&#39;s behaviors, actions, and the social world at large. The <b>machine</b>-detected patterns often provide \u201canswers\u201d to fuzzy, contingent, and open-ended questions. These \u201canswers\u201d neither reveal any causal relations nor provide explanation on why ...", "dateLastCrawled": "2022-01-26T01:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Papers on fairness in <b>machine</b> <b>learning</b>, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about fairness as it might apply to an ML process at hand, common language \u2013 and common sense \u2013 will do just fine. If, after analyzing your use case, you judge that the more technical results are relevant to the process in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning is Popular</b> Right Now", "url": "https://machinelearningmastery.com/machine-learning-is-popular/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>machine-learning-is-popular</b>", "snippet": "Abundant and cheap computation has driven the abundance of data we are collecting and the increase in capability of <b>machine learning</b> methods. In this post you learned that <b>machine learning is popular</b> now for three reasons: The field has matured both in terms of identity and in terms of methods and tools. There is an abundance of data to learn from.", "dateLastCrawled": "2022-02-03T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Structural disconnects between algorithmic decision-making</b> and the law ...", "url": "https://blogs.icrc.org/law-and-policy/2019/04/25/structural-disconnects-algorithmic-decision-making-law/", "isFamilyFriendly": true, "displayUrl": "https://blogs.icrc.org/law-and-policy/2019/04/25/structural-disconnects-algorithmic...", "snippet": "And the definition of \u2018works\u2019 is based on (in the case of <b>machine</b> <b>learning</b>) compliance with some prespecified examples of scenarios that \u2018work\u2019 and scenarios that \u2018don\u2019t\u2019. To use a legal <b>analogy</b>, this would be analogous to defining a fair decision by coming up with a rule based on past decisions that someone decided were \u2018right\u2019 or \u2018wrong\u2019 based on past outcomes. In one sense this is entirely circular: we are deciding what is \u2018right\u2019 based on someone deciding what ...", "dateLastCrawled": "2022-01-25T19:25:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(disparate impact)  is like +(machine learning algorithm)", "+(disparate impact) is similar to +(machine learning algorithm)", "+(disparate impact) can be thought of as +(machine learning algorithm)", "+(disparate impact) can be compared to +(machine learning algorithm)", "machine learning +(disparate impact AND analogy)", "machine learning +(\"disparate impact is like\")", "machine learning +(\"disparate impact is similar\")", "machine learning +(\"just as disparate impact\")", "machine learning +(\"disparate impact can be thought of as\")", "machine learning +(\"disparate impact can be compared to\")"]}