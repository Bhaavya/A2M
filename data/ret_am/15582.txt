{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>What Are Word Embeddings</b> for Text? - Machine <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "That you can either train <b>a new</b> <b>embedding</b> or use a pre-trained <b>embedding</b> on your natural <b>language</b> processing task. ... Deep Neural Networks with Multitask <b>Learning</b>, 2008. Continuous <b>space</b> <b>language</b> models, 2007. Efficient Estimation of Word Representations in Vector <b>Space</b>, 2013; Distributed Representations of Words and Phrases and their Compositionality, 2013. GloVe: Global Vectors for Word Representation, 2014. Projects. word2vec on Google Code; GloVe: Global Vectors for Word Representation ...", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Neural Network Embeddings Explained | by Will Koehrsen | Towards Data ...", "url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-network-<b>embeddings</b>-explained-4d028e6f0526", "snippet": "Embeddings. An <b>embedding</b> is a mapping of a discrete \u2014 categorical \u2014 variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed <b>space</b>.. Neural network embeddings have 3 primary purposes:", "dateLastCrawled": "2022-02-03T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning Word Meta-Embeddings by</b> Autoencoding", "url": "https://aclanthology.org/C18-1140.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C18-1140.pdf", "snippet": "meta-<b>embedding</b> <b>learning</b> problem as an autoencoding problem, where we would <b>like</b> to learn a meta-<b>embedding</b> <b>space</b> that can accurately reconstruct all source embeddings simultaneously. Thereby, the meta-<b>embedding</b> <b>space</b> is enforced to capture complementary information in differ-ent source embeddings via a coherent common <b>embedding</b> <b>space</b>. We propose three \ufb02avours of autoencoded meta-embeddings motivated by different requirements that must be satis\ufb01ed by a meta-<b>embedding</b>. Our experimental ...", "dateLastCrawled": "2022-02-03T15:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "deep <b>learning</b> - <b>creating a common embedding for two languages</b> - Stack ...", "url": "https://stackoverflow.com/questions/60481990/creating-a-common-embedding-for-two-languages", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/60481990/<b>creating-a-common-embedding-for-two-languages</b>", "snippet": "My task deals with multi-<b>language</b> <b>like</b> (english and hindi). For that I need a common <b>embedding</b> to represent both languages. I know there are methods for <b>learning</b> multilingual <b>embedding</b> <b>like</b> &#39;MUSE...", "dateLastCrawled": "2022-01-23T18:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Word Embeddings, WordPiece and Language-Agnostic BERT</b> (LaBSE) | by ...", "url": "https://medium.com/mlearning-ai/word-embeddings-wordpiece-and-language-agnostic-bert-labse-98c7626878c7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/<b>word-embeddings-wordpiece-and-language-agnostic-bert</b>...", "snippet": "GloVe. Stanford University came up with <b>a new</b> model for word embeddings called GloVe, Global Vectors for word representations. GloVe achieved an accuracy of 75% on word analogy dataset, also ...", "dateLastCrawled": "2022-02-03T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Word Embedding</b> - Mattia Mancassola - Mattia Mancassola", "url": "https://mett29.github.io/posts/2019/12/word_embedding/", "isFamilyFriendly": true, "displayUrl": "https://mett29.github.io/posts/2019/12/<b>word_embedding</b>", "snippet": "<b>Word embedding</b> is the collective name for a set of <b>language</b> modeling and feature <b>learning</b> techniques in natural <b>language</b> processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical <b>embedding</b> from a <b>space</b> with many dimensions per word to a continuous vector <b>space</b> with a much lower dimension. Wikipedia. The above description is pretty exaustive: we need an efficient way in which we can represent words so that they ...", "dateLastCrawled": "2021-12-23T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-<b>embeddings</b>-in-nlp", "snippet": "Word <b>Embedding</b> or Word Vector is a numeric vector input that represents a word in a lower-dimensional <b>space</b>. It allows words with similar meaning to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features. Features: Anything that relates words to one another.", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Document <b>Embedding</b> Techniques. A review of notable literature on the ...", "url": "https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/document-<b>embedding</b>-techniques-fed3e7a6a25d", "snippet": "Crosslingual reduced-rank ridge regression (Cr5) [Josifoski et al, 2019] introduce a method for <b>embedding</b> documents written in any <b>language</b> into a single, <b>language</b>-independent vector <b>space</b>. This is done by training a ridge-regression-based classifier that uses <b>language</b>-specific bag-of-word features in order to predict the concept that a given document is about. When constraining the learned weight matrix to be of low rank, the authors show it can be factored to obtain the desired mappings ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "machine <b>learning</b> - What is the difference between <b>latent</b> and <b>embedding</b> ...", "url": "https://ai.stackexchange.com/questions/11285/what-is-the-difference-between-latent-and-embedding-spaces", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/11285/what-is-the-difference-between-<b>latent</b>-and...", "snippet": "<b>Embedding</b> vs <b>Latent</b> <b>Space</b>. Due to Machine <b>Learning</b>&#39;s recent and rapid renaissance, and the fact that it draws from many distinct areas of mathematics, statistics, and computer science, it often has a number of different terms for the same or similar concepts. &quot;<b>Latent</b> <b>space</b>&quot; and &quot;<b>embedding</b>&quot; both refer to an (often lower-dimensional) representation of high-dimensional data: <b>Latent</b> <b>space</b> refers specifically to the <b>space</b> from which the low-dimensional representation is drawn. <b>Embedding</b> refers to ...", "dateLastCrawled": "2022-02-03T03:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] NLP <b>models with output in the embedding space</b>? : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/7kvv68/d_nlp_models_with_output_in_the_embedding_space/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../comments/7kvv68/d_nlp_<b>models_with_output_in_the_embedding_space</b>", "snippet": "NLP deep <b>learning</b> models, <b>like</b> <b>Language</b> Models or Machine Translation models, usually output the probability distribution over the token <b>space</b>, normally having a softmax as last layer. I am interested in knowing if there are any publications that propose models where the output is not a softmax but a vector in the <b>embedding</b> <b>space</b> or, alternatively, suggestions on how to do it, <b>like</b> what loss function to use (e.g. MSE between output and expected embedded vectors) or not to use, and why. I ...", "dateLastCrawled": "2021-05-27T21:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Introduction to Word Embeddings</b>. Word <b>embedding</b> is one of the most ...", "url": "https://medium.com/analytics-vidhya/introduction-to-word-embeddings-c2ba135dce2f", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>introduction-to-word-embeddings</b>-c2ba135dce2f", "snippet": "Word <b>embedding</b> is the collective name for a set of <b>language</b> modeling and feature <b>learning</b> techniques in <b>language</b> modeling ... <b>space</b> almost having <b>similar</b> vector representations. So, When ...", "dateLastCrawled": "2021-01-12T01:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "HowTo100M: <b>Learning</b> a Text-Video <b>Embedding</b> by Watching Hundred Million ...", "url": "https://www.cs.toronto.edu/~makarand/papers/ICCV2019_HowTo100M.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~makarand/papers/ICCV2019_HowTo100M.pdf", "snippet": "ural <b>language</b> [15,26] or video summarization with natural <b>language</b> [38]. Vision, <b>language</b> and speech. A common approach to model vision and <b>language</b> is <b>learning</b> a joint <b>embedding</b> <b>space</b> where visual and textual cues are adjacent if and only if they are semantically <b>similar</b> [6,8,10,11,25,32,35,37, 38,59,54,55,57]. Most of these works rely on medium", "dateLastCrawled": "2022-02-03T08:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>What Are Word Embeddings</b> for Text? - Machine <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "Word embeddings are a type of word representation that allows words with <b>similar</b> meaning to have a <b>similar</b> representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep <b>learning</b> methods on challenging natural <b>language</b> processing problems. In this post, you will discover the word <b>embedding</b> approach for representing text data. After", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Word <b>Embedding</b> and Vector <b>Space</b> Models | by Jiaqi (Karen) Fang ...", "url": "https://medium.com/analytics-vidhya/word-embedding-and-vector-space-models-11c9b76f58e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/word-<b>embedding</b>-and-vector-<b>space</b>-models-11c9b76f58e", "snippet": "image from week 3 of Natural <b>Language</b> Processing with Classification and Vector Spaces course. When using vector <b>space</b> models, the way that representations are made is by identifying the context ...", "dateLastCrawled": "2022-01-31T03:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Introduction to Embedding in Natural Language Processing</b>", "url": "https://blogs.oracle.com/ai-and-datascience/post/introduction-to-embedding-in-natural-language-processing", "isFamilyFriendly": true, "displayUrl": "https://blogs.oracle.com/.../post/<b>introduction-to-embedding-in-natural-language-processing</b>", "snippet": "Building mono-lingual word embeddings <b>is similar</b> for all languages, given a tokenization schemes, especially for non-<b>space</b> separated languages. Due to random initialization during training time, the vector for a word and its counterpart in another <b>language</b> would not be the same. To embed them in the same vector <b>space</b>, having a large corpus containing rich interactions in desired languages is straight forward. In case, such corpus is not available, other popular techniques that have been ...", "dateLastCrawled": "2022-01-29T11:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-<b>embeddings</b>-in-nlp", "snippet": "Word <b>Embedding</b> or Word Vector is a numeric vector input that represents a word in a lower-dimensional <b>space</b>. It allows words with <b>similar</b> meaning to have a <b>similar</b> representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features. Features: Anything that relates words to one another.", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Neural Network Embeddings Explained | by Will Koehrsen | Towards Data ...", "url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-network-<b>embeddings</b>-explained-4d028e6f0526", "snippet": "Embeddings. An <b>embedding</b> is a mapping of a discrete \u2014 categorical \u2014 variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed <b>space</b>.. Neural network embeddings have 3 primary purposes:", "dateLastCrawled": "2022-02-03T06:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Guide To Word2vec Using Skip Gram Model", "url": "https://analyticsindiamag.com/guide-to-word2vec-using-skip-gram-model/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/guide-to-word2vec-using-skip-gram-model", "snippet": "The words that are closed in vector <b>space</b> are expected to have a <b>similar</b> meaning. Word <b>embedding</b> uses <b>language</b> modeling and feature <b>learning</b> techniques where words from the vocabulary are mapped to vectors of real numbers. Let\u2019s take an example, text = \u201cThe match between India and <b>New</b>-Zealand delayed due to rain\u201d From the above text, we can form the dictionary of unique words as following. [\u2018The\u2019,\u2019match\u2019,\u2019between\u2019,\u2019India\u2019,\u2019and\u2019,\u2019<b>New</b>-Zealand\u2019,\u2019delayed ...", "dateLastCrawled": "2022-02-03T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A <b>complete guide to transfer learning from English to</b> other Languages ...", "url": "https://towardsdatascience.com/a-complete-guide-to-transfer-learning-from-english-to-other-languages-using-sentence-embeddings-8c427f8804a9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-<b>complete-guide-to-transfer-learning-from-english-to</b>...", "snippet": "The idea of sentence <b>embedding</b> using Siamese BERT-Networks \ud83d\udccb. W ord Vector using Neural vector representations which has become ubiquitous in all sub fields of Natural <b>Language</b> Processing (NLP) is obviously a familiar field with a lot of people [1]. Among those techniques, Sentence <b>embedding</b> idea holds various application potential, which attempts to encode a sentence or short text paragraphs into a fixed length vector (dense vector <b>space</b>) and then the vector is used to evaluate how well ...", "dateLastCrawled": "2022-01-25T06:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "deep <b>learning</b> - <b>creating a common embedding for two languages</b> - Stack ...", "url": "https://stackoverflow.com/questions/60481990/creating-a-common-embedding-for-two-languages", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/60481990/<b>creating-a-common-embedding-for-two-languages</b>", "snippet": "My task deals with multi-<b>language</b> like (english and hindi). For that I need a common <b>embedding</b> to represent both languages. I know there are methods for <b>learning</b> multilingual <b>embedding</b> like &#39;MUSE...", "dateLastCrawled": "2022-01-23T18:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Document <b>Embedding</b> Techniques. A review of notable literature on the ...", "url": "https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/document-<b>embedding</b>-techniques-fed3e7a6a25d", "snippet": "This, of course, <b>can</b> be taken as an <b>embedding</b> <b>space</b> for these documents, and \u2014 depending on the choice of K \u2014 it <b>can</b> be of a significantly smaller dimension than vocabulary-based ones. Indeed, while a main use case for LDA is unsupervised topic/community discovery, other cases include the use of the resulting latent topic <b>space</b> as an <b>embedding</b> <b>space</b> for the document corpus. Also, note that other topic modeling techniques \u2014 such as non-negative matrix factorization (NMF) and ...", "dateLastCrawled": "2022-02-03T01:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>A Tutorial on Network Embeddings</b> - ResearchGate", "url": "https://www.researchgate.net/publication/326913014_A_Tutorial_on_Network_Embeddings", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/326913014_<b>A_Tutorial_on_Network_Embeddings</b>", "snippet": "<b>embedding</b> <b>space</b>, metric MDS minimizes ... DeepW alk is that nodes in a network <b>can</b> <b>be thought</b> of as words in an arti\ufb01cial <b>language</b>. Similar. to the Skip-gram model for <b>learning</b> word embeddings ...", "dateLastCrawled": "2022-01-10T18:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>What Are Word Embeddings</b> for Text? - Machine <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "That you <b>can</b> either train <b>a new</b> <b>embedding</b> or use a pre-trained <b>embedding</b> on your natural <b>language</b> processing task. ... Deep Neural Networks with Multitask <b>Learning</b>, 2008. Continuous <b>space</b> <b>language</b> models, 2007. Efficient Estimation of Word Representations in Vector <b>Space</b>, 2013; Distributed Representations of Words and Phrases and their Compositionality, 2013. GloVe: Global Vectors for Word Representation, 2014. Projects. word2vec on Google Code; GloVe: Global Vectors for Word Representation ...", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Where are we in <b>embedding</b> spaces? A Comprehensive Analysis on Network ...", "url": "https://deepai.org/publication/where-are-we-in-embedding-spaces-a-comprehensive-analysis-on-network-embedding-approaches-for-recommender-systems", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/where-are-we-in-<b>embedding</b>-<b>spaces</b>-a-comprehensive...", "snippet": "To solve this problem, <b>a new</b> latent <b>space</b> with a smaller <b>embedding</b> distortion is needed. Recently, hyperbolic <b>space</b> has been explored as <b>a new</b> latent <b>space</b> and has shown impressive performance. It outperforms Euclidean <b>space</b> in many domains including natural <b>language</b> processing (dhingra2018embedding), link prediction and node classification (chami2019hyperbolic), top-n recommendation (vinh2020hyperml), etc. A key property of hyperbolic <b>space</b> is that it expands faster than Euclidean <b>space</b> ...", "dateLastCrawled": "2022-01-17T00:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Graph <b>Embedding</b>: Understanding Graph <b>Embedding</b> Algorithms", "url": "https://www.tigergraph.com/blog/understanding-graph-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.tigergraph.com/blog/<b>understanding-graph-embeddings</b>", "snippet": "The process of creating <b>a new</b> <b>embedding</b> vector is called \u201cencoding\u201d or \u201cencoding a vertex\u201d. The process of regenerating a vertex from the <b>embedding</b> is called \u201cdecoding\u201d or generating a vertex. The process of measuring how well an <b>embedding</b> does and finding similar items is called a \u201closs function\u201d. There may not be \u201csemantics\u201d or meaning associated with each number in an <b>embedding</b>. Embeddings <b>can</b> <b>be thought</b> of as a low-dimensional representation of an item in a vector ...", "dateLastCrawled": "2022-02-03T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Natural Language Processing</b>: From Basics to using RNN and LSTM | by ...", "url": "https://medium.com/analytics-vidhya/natural-language-processing-from-basics-to-using-rnn-and-lstm-ef6779e4ae66", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>natural-language-processing</b>-from-basics-to-using...", "snippet": "Word <b>embedding</b> is the collective name for a set of <b>language</b> modeling and feature <b>learning</b> techniques where words or phrases from the vocabulary are mapped to vectors of real numbers.", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Poincar\u00e9 <b>Embeddings for Learning Hierarchical Representations</b>", "url": "https://proceedings.neurips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/7213-poincare-<b>embeddings</b>-for-<b>learning</b>...", "snippet": "In this work, we introduce <b>a new</b> approach for <b>learning</b> hierarchical representations of symbolic data by <b>embedding</b> them into hyperbolic <b>space</b> \u2013 or more precisely into an n-dimensional Poincar\u00e9 ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious repre-sentations of symbolic data by simultaneously capturing hierarchy and similarity. We present an ef\ufb01cient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that ...", "dateLastCrawled": "2022-01-23T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Paraphrase thought: Sentence embedding module imitating</b> human <b>language</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0020025520305557", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0020025520305557", "snippet": "In this paper, inspired by human <b>language</b> recognition, we propose the following concept of semantic coherence, which should be satisfied for a good sentence <b>embedding</b> method: similar sentences should be located close to each other in the <b>embedding</b> <b>space</b>. Then, we propose the Paraphrase-<b>Thought</b> (P-<b>thought</b>) model to pursue semantic coherence as much as possible. Experimental results on three paraphrase identification datasets (MS COCO, STS benchmark, SICK) show that the P-<b>thought</b> models ...", "dateLastCrawled": "2021-12-31T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Paraphrase <b>thought</b>: Sentence <b>embedding</b> module imitating human <b>language</b> ...", "url": "https://www.sciencedirect.com/science/article/abs/pii/S0020025520305557", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/abs/pii/S0020025520305557", "snippet": "In this paper, inspired by human <b>language</b> recognition, we propose the following concept of semantic coherence, which should be satisfied for a good sentence <b>embedding</b> method: similar sentences should be located close to each other in the <b>embedding</b> <b>space</b>. Then, we propose the Paraphrase-<b>Thought</b> (P-<b>thought</b>) model to pursue semantic coherence as much as possible. Experimental results on three paraphrase identification datasets (MS COCO, STS benchmark, SICK) show that the P-<b>thought</b> models ...", "dateLastCrawled": "2022-01-29T21:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Horsehistory study and the automated discovery of <b>new</b> areas of <b>thought</b> ...", "url": "https://interconnected.org/home/2021/06/16/horsehistory", "isFamilyFriendly": true, "displayUrl": "https://interconnected.org/home/2021/06/16/horsehistory", "snippet": "The creation of this artificial <b>language</b> is an act of resistance and also way to carve out a <b>space</b> for unique feminist <b>thought</b> and being. (It\u2019s a stunning book.) (It\u2019s a stunning book.) In Native Tongue, discovering <b>a new</b> word in this <b>new</b> <b>language</b> is a big deal: <b>a new</b> word, <b>a new</b> concept, <b>a new</b> \u201cEncoding\u201d as Elgin calls it, <b>a new</b> valid <b>embedding</b> in concept phase <b>space</b>, we might say.", "dateLastCrawled": "2022-01-31T07:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Word Embeddings, WordPiece and Language-Agnostic BERT</b> (LaBSE) | by ...", "url": "https://medium.com/mlearning-ai/word-embeddings-wordpiece-and-language-agnostic-bert-labse-98c7626878c7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/<b>word-embeddings-wordpiece-and-language-agnostic-bert</b>...", "snippet": "Word embeddings are the representation of words in a numeric format, which <b>can</b> be understood by a computer. Simplest example would be (Yes, No) represented as (1, 0). But when we are dealing with\u2026", "dateLastCrawled": "2022-02-03T17:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning Multilingual Word Embeddings in Latent</b> Metric <b>Space</b>: A ...", "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00257/43509/Learning-Multilingual-Word-Embeddings-in-Latent", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/.../43509/<b>Learning-Multilingual-Word-Embeddings-in-Latent</b>", "snippet": "This <b>can</b> allow downstream applications to support multiple languages without performance degradation. Even if bilingual embeddings are represented in a single vector <b>space</b> using a pivot <b>language</b>, the <b>embedding</b> quality is inferior <b>compared</b> with GeoMM multi. We discuss more multilingual experiments in Section 7.", "dateLastCrawled": "2022-01-23T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A survey <b>of cross-lingual word embedding models</b>", "url": "https://ruder.io/cross-lingual-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://ruder.io/cross-lingual-<b>embeddings</b>", "snippet": "The ease of constructing a shared <b>embedding</b> <b>space</b> between languages and consequently the success of cross-lingual transfer is intuitively proportional to the similarity of the languages: An <b>embedding</b> <b>space</b> shared between Spanish and Portuguese tends to capture more linguistic nuances of meaning than an <b>embedding</b> <b>space</b> populated with English and Chinese representations. Furthermore, if two languages are too dissimilar, cross-linguistic transfer might not be possible at all -- similar to the ...", "dateLastCrawled": "2022-02-01T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>What Are Word Embeddings</b> for Text? - Machine <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "That there are 3 main algorithms for <b>learning</b> a word <b>embedding</b> from text data. That you <b>can</b> either train <b>a new</b> <b>embedding</b> or use a pre-trained <b>embedding</b> on your natural <b>language</b> processing task. Kick-start your project with my <b>new</b> book Deep <b>Learning</b> for Natural <b>Language</b> Processing, including step-by-step tutorials and the Python source code ...", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Word <b>embedding</b>: Continuous <b>space</b> representation for natural <b>language</b>", "url": "https://www.researchgate.net/publication/289763015_Word_embedding_Continuous_space_representation_for_natural_language", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/289763015_Word_<b>embedding</b>_Continuous_<b>space</b>...", "snippet": "The main advantage of these architec-tures is that they learn an <b>embedding</b> for words (or other symbols) in a continuous <b>space</b> that helps to smooth the <b>language</b> model and pro-vide good ...", "dateLastCrawled": "2021-12-18T08:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Word Embeddings and Pre-training for Large <b>Language</b> Models (BERT, GPT ...", "url": "https://salman-mo.medium.com/word-embeddings-and-pre-training-for-large-language-models-bert-gpt-28727cae1db", "isFamilyFriendly": true, "displayUrl": "https://salman-mo.medium.com/word-<b>embeddings</b>-and-pre-training-for-large-<b>language</b>...", "snippet": "Between 2013 and 2017 Word to Vector (W2V) In 2013, a paper was publish by Mikolov et al., that defined a Word2Vec model; its goal was to define words into a vectorized representation in some vector <b>space</b> based on its pretraining data (there are generally 2 variants CBOW and skip gram for Word2Vec).. This was great fo r Deep <b>learning</b> models because you would take the text data and feed it into Word2Vec model (pre-trained on some large dataset), the result is a vector representation of the ...", "dateLastCrawled": "2022-02-01T04:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Under the hood: Multilingual embeddings</b> - Engineering at Meta", "url": "https://engineering.fb.com/2018/01/24/ml-applications/under-the-hood-multilingual-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://engineering.fb.com/.../24/ml-applications/<b>under-the-hood-multilingual-embeddings</b>", "snippet": "For example, the words futbol in Turkish and soccer in English would appear very close together in the <b>embedding</b> <b>space</b> because they mean the same thing in different languages. In order to make text classification work across languages, then, you use these multilingual word embeddings with this property as the base representations for text classification models. Since the words in the <b>new</b> <b>language</b> will appear close to the words in trained languages in the <b>embedding</b> <b>space</b>, the classifier will ...", "dateLastCrawled": "2022-01-30T17:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "HowTo100M: <b>Learning</b> a Text-Video <b>Embedding</b> by Watching Hundred Million ...", "url": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Miech_HowTo100M_Learning_a_Text-Video_Embedding_by_Watching_Hundred_Million_Narrated_ICCV_2019_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Miech_HowTo100M_<b>Learning</b>_a_Text...", "snippet": "tions and describe them to others using <b>language</b>; while adults <b>can</b> learn <b>new</b> skills by reading books or watching videos. This interplay between video and <b>language</b> ex- \u2217Equal contribution. +Now at DeepMind. 1D\u00b4epartement d\u2019informatique de l\u2019ENS, Ecole normale sup\u00b4erieure, CNRS, PSL Research University, 75005 Paris, France. 3Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague. Figure 1: We learn a joint text-video <b>embedding</b> by watching ...", "dateLastCrawled": "2022-02-01T01:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Introduction to Text Representations for <b>Language</b> Processing \u2014 Part 2 ...", "url": "https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-text-<b>representation</b>s-for-<b>language</b>...", "snippet": "Parameters for training on <b>new</b> languages cannot be shared. If you want to train word2vec in <b>a new</b> <b>language</b>, you have to start from scratch; Requires a comparatively larger corpus for the network to converge (especially if using skip-gram; GloVe. Global Vectors for word <b>representation</b> is another famous <b>embedding</b> technique used quite often in NLP ...", "dateLastCrawled": "2022-01-31T04:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Country prediction using Word Embedding</b> | by Kolamanvitha | MLearning ...", "url": "https://medium.com/mlearning-ai/country-prediction-using-word-embedding-f5c0f930c87b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/<b>country-prediction-using-word-embedding</b>-f5c0f930c87b", "snippet": "Euclidean distance <b>can</b> be misleading at times to understand the similarity between 2 documents when documents of different sizes are <b>compared</b>. So in scenarios where we are working with text data ...", "dateLastCrawled": "2021-12-24T07:50:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Finding the Word <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-word-<b>analogy</b>-from-given-words-using-word2vec...", "snippet": "In the word <b>analogy</b> task, ... What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the word <b>analogy</b>. In this tutorial, we will be using Word2Vec model and a pre-trained model named \u2018GoogleNews-vectors-negative300.bin\u2018 which is trained on over 50 Billion words by Google. Each word inside the pre-trained dataset is embedded in a 300-dimensional <b>space</b> and the words which are similar in context/meaning are placed closer to each other in the <b>space</b>. Methodology to ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "snippet": "A suitable representation is therefore essential for the success of <b>analogy</b>-based <b>learning</b> to rank. Therefore, we propose a method for analogical <b>embedding</b>, i.e., for <b>embedding</b> the data in a target <b>space</b> such that, in this <b>space</b>, the aforementioned <b>analogy</b> assumption is as valid and strongly pronounced as possible. This is accomplished by means of a neural network with a quadruple Siamese structure, which is trained on a suitably designed set of examples in the form of quadruples of objects ...", "dateLastCrawled": "2022-01-17T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://homepages.uni-paderborn.de/ahmadim/IDA%202021.pdf", "isFamilyFriendly": true, "displayUrl": "https://homepages.uni-paderborn.de/ahmadim/IDA 2021.pdf", "snippet": "7 Intelligent Systems and <b>Machine</b> <b>Learning</b> <b>Embedding</b> By ignoring irrelevant or noisy features, the performance can often be improved Common feature selection techniques tailored for the case of <b>analogy</b>-based <b>learning</b> to rank. <b>Analogy</b>-based <b>learning</b> to rank (able2rank) 8 Intelligent Systems and <b>Machine</b> <b>Learning</b> Extension to feature vectors Degree of <b>analogy</b>. Analogical <b>Embedding</b> 9 Intelligent Systems and <b>Machine</b> <b>Learning</b> Positive example: preferences on both sides are coherent Negative ...", "dateLastCrawled": "2022-01-06T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "snippet": "With the emergence of word <b>embedding</b> models, a lot of progress has been made in NLP, essentially assuming that a word <b>analogy</b> like m a n: k i n g:: w o m a n: q u e e n is an instance of a parallelogram within the underlying vector <b>space</b>. In this paper, we depart from this assumption to adopt a <b>machine</b> <b>learning</b> approach, i.e., <b>learning</b> a substitute of the parallelogram model. To achieve our goal, we first review the formal modeling of analogical proportions, highlighting the properties which ...", "dateLastCrawled": "2021-11-13T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-word2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, word <b>embedding</b> is used to map words into vectors of real numbers. There are various word <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce word embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector <b>space</b>, with each unique word in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Graph <b>Embedding</b> for Deep <b>Learning</b> | by Flawnson Tong | Towards Data Science", "url": "https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/overview-of-deep-<b>learning</b>-on-graph-<b>embeddings</b>-4305c10ad4a4", "snippet": "Using an <b>analogy</b> with word2vec, if a document is made of sentences (which is then made of words), then a graph is made of sub-graphs ... Graph <b>embedding</b> techniques take graphs and embed them in a lower dimensional continuous latent <b>space</b> before passing that representation through a <b>machine</b> <b>learning</b> model. Walk <b>embedding</b> methods perform graph traversals with the goal of preserving structure and features and aggregates these traversals which can then be passed through a recurrent neural ...", "dateLastCrawled": "2022-02-01T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting ...", "url": "https://www.researchgate.net/figure/In-the-word-embedding-space-the-analogy-pairs-exhibit-interesting-algebraic_fig1_319370400", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/In-the-word-<b>embedding</b>-<b>space</b>-the-<b>analogy</b>-pairs...", "snippet": "Download scientific diagram | In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting algebraic relationships. from publication: Visual Exploration of Semantic Relationships in Neural ...", "dateLastCrawled": "2021-12-21T11:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "This approach of <b>learning</b> an <b>embedding</b> layer requires a lot of training data and can be slow, but will learn an <b>embedding</b> both targeted to the specific text data and the NLP task. 2. Word2Vec. Word2Vec is a statistical method for efficiently <b>learning</b> a standalone word <b>embedding</b> from a text corpus. It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make the neural-network-based training of the <b>embedding</b> more efficient and since then has become the de facto standard ...", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-<b>embeddings</b>-in-nlp", "snippet": "Word <b>Embedding</b> or Word Vector is a numeric vector input that represents a word in a lower-dimensional <b>space</b>. It allows words with similar meaning to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features. Features: Anything that relates words to one another. Eg: Age, Sports, Fitness, Employed etc. Each word vector has values corresponding to these features. Goal of Word Embeddings. To reduce dimensionality; To use a ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Zero-shot <b>learning</b> via discriminative representation extraction ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "snippet": "The pioneer work in ZSL can be traced to Larochelle et al. , where it verified that when test images belong to some classes that are not available at training stage, a <b>machine</b> <b>learning</b> system can still figure out what a test image is. Due to the importance of zero-shot <b>learning</b>, the number of proposed approaches has increased steadily recently.The number of new zero-shot <b>learning</b> approaches proposed every year was increasing.", "dateLastCrawled": "2021-10-30T07:08:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A self-supervised domain-general <b>learning</b> framework for human ventral ...", "url": "https://www.nature.com/articles/s41467-022-28091-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-022-28091-4", "snippet": "On this view, the <b>embedding space can be thought of as</b> a high-fidelity perceptual interface, with useful visual primitives over which separate conceptual representational systems can operate.", "dateLastCrawled": "2022-01-25T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Spectral Af\ufb01ne-Kernel Embeddings</b> - NSF", "url": "https://par.nsf.gov/servlets/purl/10039348", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10039348", "snippet": "Since <b>machine</b> <b>learn-ing</b> algorithms struggle with high dimensions (an issue known as the curse of dimensionality in this context), one typically needs to map these data points from their high-dimensional space into a lower dimensional space without signi\ufb01cant distortion. Mapping data (living in RD with D\u02db1 but sampling a manifold of low in-trinsic dimensionality d \u02ddD) into a low-dimensional <b>embedding space can be thought of as</b> a preliminary feature extraction step in <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-29T21:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting affinity ties in a surname network", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "snippet": "<b>Machine</b> <b>learning</b>-based approaches for knowledge graph completion To cover the broadest possible range of methods and architectures in the evaluation, we identified representative methods of different model families, taking care that these methods achieve state-of-the-art performances in knowledge graph completion and have open-source implementations that favor the reproducibility of the reported results.", "dateLastCrawled": "2021-09-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(embedding space)  is like +(learning a new language)", "+(embedding space) is similar to +(learning a new language)", "+(embedding space) can be thought of as +(learning a new language)", "+(embedding space) can be compared to +(learning a new language)", "machine learning +(embedding space AND analogy)", "machine learning +(\"embedding space is like\")", "machine learning +(\"embedding space is similar\")", "machine learning +(\"just as embedding space\")", "machine learning +(\"embedding space can be thought of as\")", "machine learning +(\"embedding space can be compared to\")"]}