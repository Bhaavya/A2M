{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Data Science Interview Questions (30 days</b> of Interview Preparation) # ...", "url": "https://inblog.in/Data-Science-Interview-Questions-30-days-of-Interview-Preparation-Day-17-Ozxv3yYzps", "isFamilyFriendly": true, "displayUrl": "https://inblog.in/<b>Data-Science-Interview-Questions-30-days</b>-of-Interview-Preparation...", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>): ... ULMFiT achieves the state-of-the-art result using novel techniques <b>like</b>: 1. Discriminative fine-<b>tuning</b>. 2. Slanted triangular <b>learning</b> rates. 3. Gradual unfreezing. Discriminative Fine-<b>Tuning</b> . Different layers of a neural network capture different types of information so they should be fine tuned to varying extents. Instead of using the same <b>learning</b> rates for all layers of the <b>model</b>, discriminative fine-<b>tuning</b> allows us to tune each layer with ...", "dateLastCrawled": "2021-11-25T12:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Scaling-<b>up Empirical Risk Minimization: Optimization of</b> ...", "url": "https://www.researchgate.net/publication/270824717_Scaling-up_Empirical_Risk_Minimization_Optimization_of_Incomplete_U-statistics", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/270824717_Scaling-up_<b>Empirical</b>_<b>Risk</b>...", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is a fundamental algorithm for statistical <b>learning</b> problems where the data is generated according to some unknown distribution $\\mathsf{P}$ and returns a ...", "dateLastCrawled": "2021-12-23T23:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Empirical risk minimization</b>: probabilistic complexity and stepsize ...", "url": "https://link.springer.com/article/10.1007%2Fs10589-019-00080-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10589-019-00080-2", "snippet": "<b>Empirical risk minimization</b> (<b>ERM</b>) is one of the most powerful tools in applied statistics, and is regarded as the canonical approach to regression analysis. In the context of <b>machine</b> <b>learning</b> and big data analytics, various important problems such as support vector machines, (regularized) linear regression, and logistics regression can be cast as <b>ERM</b> problems, see for e.g. [ 17 ].", "dateLastCrawled": "2022-01-25T18:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>DATA SCIENCE INTERVIEW PREPARATION DAY 17</b>", "url": "https://gamakaai.com/wp-content/uploads/2021/04/GamakaAI_DAY_17_InterviewQuestions.pdf", "isFamilyFriendly": true, "displayUrl": "https://gamakaai.com/wp-content/uploads/2021/04/GamakaAI_DAY_17_InterviewQuestions.pdf", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>): It is a principle in statistical <b>learning</b> theory which defines a family of <b>learning</b> algorithms and is used to give theoretical bounds on their performance. The idea is that we don\u2019t know exactly how well an algorithm will work in practice (the true &quot;<b>risk</b>&quot;) because we don&#39;t know the true distribution of data that the algorithm will work on, but as an alternative we can measure its performance on a known set of training data. We assumed that our samples ...", "dateLastCrawled": "2021-12-01T06:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning as Optimization: Linear Regression</b>", "url": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec4_slides.pdf", "isFamilyFriendly": true, "displayUrl": "https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec4_slides.pdf", "snippet": "<b>Machine</b> <b>Learning</b> (CS771A) <b>Learning as Optimization: Linear Regression</b> 3 . <b>Learning</b> as Optimization To nd the best f, we minimize the <b>empirical</b> <b>risk</b> w.r.t. f.<b>Empirical</b> <b>Risk</b> <b>Minimization</b>(<b>ERM</b>) f^ = arg min f L emp(f) = arg min f XN n=1 \u2018(y n;f(x n)) We also want f to be \\simple&quot;. To do so, we add a \\regularizer&quot; R(f) f^ = arg min f XN n=1 \u2018(y n;f(x n)) + R(f) The regularizer R(f) is a measure of complexity of our <b>model</b> f This is called Regularized (<b>Empirical</b>) <b>Risk</b> <b>Minimization</b> We want both ...", "dateLastCrawled": "2022-02-03T16:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Optimization</b> - mlstory.org", "url": "https://mlstory.org/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>optimization</b>.html", "snippet": "Then we turned to <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) where we instead rely on numerical methods to discover good decision rules when we don\u2019t have such a probability <b>model</b>. In this chapter, we take a closer look at how to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems effectively. We focus on the core <b>optimization</b> methods commonly used to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems and on the mathematical tools used to analyze their running times. Our main subject will be gradient descent ...", "dateLastCrawled": "2022-01-30T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Dr. Jianlin Cheng", "url": "http://calla.rnet.missouri.edu/cheng_courses/supervised_learning/6_model_selection.pdf", "isFamilyFriendly": true, "displayUrl": "calla.rnet.missouri.edu/cheng_courses/supervised_<b>learning</b>/6_<b>model</b>_selection.pdf", "snippet": "Cost of <b>model</b> (log prior) -c(f) cost is small if fis highly probable, cost is large if fis improbable <b>ERM</b> (<b>empirical</b> <b>risk</b> <b>minimization</b>) over a restricted class F uniform prior on f \u20ac F, zero probability for other predictors \u2014 arg min Rn(f) fGFL", "dateLastCrawled": "2021-08-28T20:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "4 \u2013 The Overfitting Iceberg \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie ...", "url": "https://blog.ml.cmu.edu/2020/08/31/4-overfitting/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/4-overfitting", "snippet": "As stated in the original paper, the predictor h n is commonly chosen from some function class H such as logistic regression, using <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>). By changing the <b>model</b> complexity, the capacity of the function class H also changes. It is possible to control the bias-variance tradeoff by selecting our models to balance ...", "dateLastCrawled": "2022-01-25T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear <b>model</b> ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> and Dynamical Systems - Approximation, Optimization ...", "url": "http://www.smartchair.org/f_/JSML2020/8a6e0/F4/Machine%20Learning%20and%20Dynamical%20Systems%20-%20Qianxiao%20Li.pdf", "isFamilyFriendly": true, "displayUrl": "www.smartchair.org/f_/JSML2020/8a6e0/F4/<b>Machine</b> <b>Learning</b> and Dynamical Systems...", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) F^ arg min F2H 1 N XN i=1 (F(xi); yi |{z} F (xi)) ( is aloss function) Population <b>risk</b> <b>minimization</b> (PRM) ~F arg min F2H E (x;y)\u02d8 (F(x);y) ( is theinput-output distribution) We want to solve PRM, but we often can only perform <b>ERM</b>. The gap between ^F and ~F is the problem ofgeneralization. 8 49. <b>Empirical</b> and Population <b>Risk</b> <b>Minimization</b> <b>Learning</b>/approximation involves picking out the \u201cbest\u201d F 2Hso that it makes <b>similar</b> predictions to F <b>Empirical</b> <b>risk</b> ...", "dateLastCrawled": "2022-01-05T19:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>siamcse21</b> - GitHub Pages", "url": "https://amirgholami.github.io/siamcse21/", "isFamilyFriendly": true, "displayUrl": "https://amirgholami.github.io/<b>siamcse21</b>", "snippet": "Training is often formulated as the solution of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) problems, which are optimization programs whose complexity scales with the number of elements in the dataset. In such settings, the implementation cost of second-order methods is prohibitive, and, hence, resort to stochastic methods is inevitable. Stochastic Newton-type methods (<b>similar</b> to their deterministic versions) are superior to first-order methods in a local neighborhood of the optimal solution, but ...", "dateLastCrawled": "2021-12-17T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Optimization</b> - mlstory.org", "url": "https://mlstory.org/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>optimization</b>.html", "snippet": "Then we turned to <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) where we instead rely on numerical methods to discover good decision rules when we don\u2019t have such a probability <b>model</b>. In this chapter, we take a closer look at how to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems effectively. We focus on the core <b>optimization</b> methods commonly used to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems and on the mathematical tools used to analyze their running times. Our main subject will be gradient descent ...", "dateLastCrawled": "2022-01-30T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Private Empirical Risk Minimization Beyond the</b> Worst Case: The Effect ...", "url": "https://www.researchgate.net/publication/268525500_Private_Empirical_Risk_Minimization_Beyond_the_Worst_Case_The_Effect_of_the_Constraint_Set_Geometry", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/268525500_Private_<b>Empirical</b>_<b>Risk</b>_<b>Minimization</b>...", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) is a standard technique in <b>machine</b> <b>learning</b>, where a <b>model</b> is selected by minimizing a loss function over constraint set. When the training dataset consists of ...", "dateLastCrawled": "2021-10-26T05:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Extracurricular <b>Learning</b>: Knowledge Transfer Beyond <b>Empirical</b> Distribution", "url": "https://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular_Learning_Knowledge_Transfer_Beyond_Empirical_Distribution_CVPRW_2021_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular...", "snippet": "major accuracy improvements compared to the <b>empirical</b> <b>risk</b> <b>minimization</b>-based training for various recent neural network architectures: ... <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>): min \u03b8 1 n X i l(f\u03b8(xi),yi) (2) In KD [28], a student <b>model</b> f\u03b8 is encouraged to match the output of a teacher \u03c4 on the training set: min \u03b8 1 n X i l(f\u03b8(xi),\u03c4(xi)) (3) \u03c4 in (3) can be a single more powerful <b>model</b> or an ensemble of several models. In the original KD [28] an average of losses in (2) and (3) is used ...", "dateLastCrawled": "2022-01-17T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Learning</b> to Branch - Proceedings of <b>Machine</b> <b>Learning</b> Research | The ...", "url": "http://proceedings.mlr.press/v80/balcan18a/balcan18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/balcan18a/balcan18a.pdf", "snippet": "training set, and thus performs <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>). We prove that the adaptive nature of our algorithm is necessary: performing <b>ERM</b> over a data-independent dis- cretization of the parameter space can be disastrous. In particular, for any discretization of the parameter space, we provide an in\ufb01nite family of distributions over MILP in-stances such that every point in the discretization results in a B&amp;B tree with exponential size in expectation, but there exist in\ufb01nitely ...", "dateLastCrawled": "2022-01-31T15:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>From predictive to prescriptive analytics: A data-driven</b> multi-item ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167923620300956", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167923620300956", "snippet": "The authors showed that the <b>empirical</b> <b>risk</b> <b>minimization</b> approach is equivalent to a high-dimensional quantile regression and can be solved by convex optimization methods. Oroojlooy, Snyder, and Tak\u00e1\u010d used the deep neural networks to solve the NVM using an integrated method to solve NVM and extended the approach for (r, Q) policy. However, they have not used quantile regression for the same. A <b>similar</b> approach is proposed by Huber et al. , in which the authors used the non-linear <b>machine</b> ...", "dateLastCrawled": "2022-01-11T17:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Chapter 10 Supervised Learning</b> | Introduction to Data Science", "url": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "snippet": "<b>Chapter 10 Supervised Learning</b>. <b>Machine</b> <b>learning</b> is very <b>similar</b> to statistics, but it is certainly not the same. As the name suggests, in <b>machine</b> <b>learning</b> we want machines to learn. This means that we want to replace hard-coded expert algorithm, with data-driven self-learned algorithm. There are many <b>learning</b> setups, that depend on what information is available to the <b>machine</b>. The most common setup, discussed in this chapter, is supervised <b>learning</b>. The name takes from the fact that by ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Lecture Notes for EC500: Optimization for <b>Machine</b> <b>Learning</b>", "url": "https://optmlclass.github.io/notes/optforml_notes.pdf", "isFamilyFriendly": true, "displayUrl": "https://optmlclass.github.io/notes/optforml_notes.pdf", "snippet": "<b>machine</b> <b>learning</b> work\ufb02ow consists of two main choices: 1.Choose some kind of <b>model</b> to explain the data. In supervised <b>learning</b> in which z = (x;y), typically we pick some function fand use the <b>model</b> y \u02c7f(x;w) where w is the parameter of the <b>model</b>. We will let Wbe the set of acceptable values for w. 2.Fit the <b>model</b> to the data. That is, using ...", "dateLastCrawled": "2022-01-28T09:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> Under Fairness Constraints | Request PDF", "url": "https://www.researchgate.net/publication/333044604_Empirical_Risk_Minimization_Under_Fairness_Constraints", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/333044604_<b>Empirical</b>_<b>Risk</b>_<b>Minimization</b>_Under...", "snippet": "Donini et al. (2018) presented a comprehensive approach based on <b>empirical</b> <b>risk</b> <b>minimization</b>, which incorporates a fairness constraint into the <b>learning</b> problem. It encourages the conditional <b>risk</b> ...", "dateLastCrawled": "2021-12-20T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Chapter 10 Supervised Learning</b> | Introduction to Data Science", "url": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "snippet": "We now present the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) approach to supervised <b>learning</b>, a.k.a. M-estimation in the statistical literature. Remark. We do not discuss purely algorithmic approaches such as K-nearest neighbour and kernel smoothing due to space constraints. For a broader review of supervised <b>learning</b>, see the Bibliographic Notes. Example 10.1 (Spam classification) Consider the problem of predicting if a mail is spam or not based on its attributes: length, number of exclamation ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "4 \u2013 The Overfitting Iceberg \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie ...", "url": "https://blog.ml.cmu.edu/2020/08/31/4-overfitting/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/4-overfitting", "snippet": "As stated in the original paper, the predictor h n is commonly chosen from some function class H such as logistic regression, using <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>). By changing the <b>model</b> complexity, the capacity of the function class H also changes. It is possible to control the bias-variance tradeoff by selecting our models to balance ...", "dateLastCrawled": "2022-01-25T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "GIANT: Globally Improved Approximate Newton Method for Distributed ...", "url": "https://deepai.org/publication/giant-globally-improved-approximate-newton-method-for-distributed-optimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/giant-globally-improved-approximate-newton-method-for...", "snippet": "For distributed computing environments, we consider the canonical <b>machine</b> <b>learning</b> problem of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) with quadratic regularization, and we propose a distributed and communication-efficient Newton-type optimization method. At every iteration, each worker locally finds an Approximate NewTon (ANT) direction, and then it sends this direction to the main driver. The driver, then, averages all the ANT directions received from workers to form a Globally Improved ANT ...", "dateLastCrawled": "2021-12-14T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient Starvation</b> - GitHub Pages", "url": "https://mohammadpz.github.io/GradientStarvation.html", "isFamilyFriendly": true, "displayUrl": "https://mohammadpz.github.io/<b>GradientStarvation</b>.html", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b> - vanilla cross-entropy) performs well on the training set (91 %) but very bad on the test set (24 %). It is expected because the color feature is reversed in the test set, and we cannot expect the poor <b>model</b> to ignore the color! On the other hand, Invariant <b>Risk</b> <b>Minimization</b> (IRM) achieves good performance on the test set (67 %). However, SD also performs well (68 %) without requiring access to multiple training environments, unlike IRM. How does SD learn to ...", "dateLastCrawled": "2022-01-29T03:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Lester</b> Mackey: Research - Stanford University", "url": "https://web.stanford.edu/~lmackey/research.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/~lmackey/research.html", "snippet": "Recent work in <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) approximates the expensive refitting with a single Newton step warm-started from the full training set optimizer. While this <b>can</b> greatly reduce runtime, several open questions remain including whether these approximations lead to faithful <b>model</b> selection and whether they are suitable for non-smooth objectives. We address these questions with three main contributions: (i) we provide uniform non-asymptotic, deterministic <b>model</b> assessment ...", "dateLastCrawled": "2022-02-02T17:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Understanding <b>Machine</b> <b>Learning</b>: From Theory To Algorithms 1107057132 ...", "url": "https://ebin.pub/understanding-machine-learning-from-theory-to-algorithms-1107057132-9781107057135.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/understanding-<b>machine</b>-<b>learning</b>-from-theory-to-algorithms-1107057132...", "snippet": "We describe the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), Structural <b>Risk</b> <b>Minimization</b> (SRM), and Minimum Description Length (MDL) <b>learning</b> rules, which show \u201chow <b>a machine</b> <b>can</b> learn.\u201d We quantify the amount of data needed for <b>learning</b> using the <b>ERM</b>, SRM, and MDL rules and show how <b>learning</b> might fail by deriving a \u201cno-free-lunch\u201d theorem. We also discuss how much computation time is required for <b>learning</b>. In the second part of the book we describe various <b>learning</b> algorithms. For some of ...", "dateLastCrawled": "2021-12-31T17:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>A Bayesian perspective of statistical machine learning</b> for big data ...", "url": "https://link.springer.com/article/10.1007/s00180-020-00970-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00180-020-00970-8", "snippet": "The <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) principle discussed in Sect. 4 is often implemented using optimization. The <b>risk</b> function corresponds to the loss function that is appropriate for the problem. Many <b>machine</b> <b>learning</b> problems are convex optimization problems and numerical optimization techniques are used in these problems, see e.g., Nocedal and Wright", "dateLastCrawled": "2021-12-27T17:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Chapter 10 Supervised Learning</b> | R (BGU course)", "url": "http://www.john-ros.com/Rcourse/supervised.html", "isFamilyFriendly": true, "displayUrl": "www.john-ros.com/Rcourse/supervised.html", "snippet": "10.1.3 Unbiased <b>Risk</b> Estimation. The fundamental problem of overfitting, is that the <b>empirical</b> <b>risk</b>, \\(R_n(\\hat f)\\), is downward biased to the population <b>risk</b>, \\(R(\\hat f)\\).We <b>can</b> remove this bias in two ways: (a) purely algorithmic resampling approaches, and (b) theory driven estimators.. Train-Validate-Test: The simplest form of algorithmic validation is to split the data.A train set to train/estimate/learn \\(\\hat f\\).A validation set to compute the out-of-sample expected loss, \\(R(\\hat ...", "dateLastCrawled": "2022-02-03T13:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Remote Sensing | Free Full-Text | Deep Transfer <b>Learning</b> for Few-Shot ...", "url": "https://www.mdpi.com/2072-4292/11/11/1374/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2072-4292/11/11/1374/htm", "snippet": "In the <b>machine</b> <b>learning</b> literature, the challenges of <b>learning</b> in this scenario have been investigated within transfer <b>learning</b> ... the <b>model</b> <b>can</b> <b>be thought</b> of as a better initial point and hence fine-tuned using fewer real labeled data points. Zhang et al. proposed to transfer knowledge from a secondary source SAR task, where labeled data are available. Similarly, a CNN network <b>can</b> be pretrained on the task with labeled data and then fine-tuned on the target task. Lang et al. used the ...", "dateLastCrawled": "2022-01-08T17:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Tilted <b>Empirical Risk</b> <b>Minimization</b> | DeepAI", "url": "https://deepai.org/publication/tilted-empirical-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/tilted-<b>empirical-risk</b>-<b>minimization</b>", "snippet": "<b>Empirical risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, which <b>can</b> result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly.While many methods aim to address these problems individually, in this work, we explore them through a unified framework\u2014tilted <b>empirical risk</b> <b>minimization</b> (TERM).", "dateLastCrawled": "2022-01-11T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Extracurricular <b>Learning</b>: Knowledge Transfer Beyond <b>Empirical</b> Distribution", "url": "https://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular_Learning_Knowledge_Transfer_Beyond_Empirical_Distribution_CVPRW_2021_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content/CVPR2021W/ECV/papers/Pouransari_Extracurricular...", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>): min \u03b8 1 n X i l(f\u03b8(xi),yi) (2) In KD [28], a student <b>model</b> f\u03b8 is encouraged to match the output of a teacher \u03c4 on the training set: min \u03b8 1 n X i l(f\u03b8(xi),\u03c4(xi)) (3) \u03c4 in (3) <b>can</b> be a single more powerful <b>model</b> or an ensemble of several models. In the original KD [28] an average of losses in (2) and (3 ...", "dateLastCrawled": "2022-01-17T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "[2010.14672v1] Why Does MAML <b>Outperform ERM? An Optimization Perspective</b>", "url": "https://arxiv.org/abs/2010.14672v1", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2010.14672v1", "snippet": "<b>Model</b>-Agnostic Meta-<b>Learning</b> (MAML) has demonstrated widespread success in training models that <b>can</b> quickly adapt to new tasks via one or few stochastic gradient descent steps. However, the MAML objective is significantly more difficult to optimize <b>compared</b> to standard <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>), and little is understood about how much MAML improves over <b>ERM</b> in terms of the fast adaptability of their solutions in various scenarios. We analytically address this issue in a linear ...", "dateLastCrawled": "2020-10-29T09:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Tilted <b>Empirical</b> <b>Risk</b> <b>Minimization</b>", "url": "https://www.researchgate.net/publication/342655944_Tilted_Empirical_Risk_Minimization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/342655944_Tilted_<b>Empirical</b>_<b>Risk</b>_<b>Minimization</b>", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is typically designed to perform well on the average loss, whic h. <b>can</b> result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups ...", "dateLastCrawled": "2021-12-27T18:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Optimization</b> - mlstory.org", "url": "https://mlstory.org/optimization.html", "isFamilyFriendly": true, "displayUrl": "https://mlstory.org/<b>optimization</b>.html", "snippet": "Then we turned to <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) where we instead rely on numerical methods to discover good decision rules when we don\u2019t have such a probability <b>model</b>. In this chapter, we take a closer look at how to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems effectively. We focus on the core <b>optimization</b> methods commonly used to solve <b>empirical</b> <b>risk</b> <b>minimization</b> problems and on the mathematical tools used to analyze their running times. Our main subject will be gradient descent ...", "dateLastCrawled": "2022-01-30T03:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fundamental Limits of Ridge-Regularized <b>Empirical</b> <b>Risk</b> <b>Minimization</b> in ...", "url": "https://web.ece.ucsb.edu/Faculty/selected_pubs/Pedarsani-mp/ERM-AISTATS21.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.ece.ucsb.edu/Faculty/selected_pubs/Pedarsani-mp/<b>ERM</b>-AISTATS21.pdf", "snippet": "<b>machine</b> <b>learning</b>. <b>ERM</b> methods are often e\ufb03cient in implementation, but \ufb01rst one needs to make cer-tainchoices: suchas,chooseanappropriatelossfunc-tion and regularization function, and tune the regu-larizationparameter. Classicalstatisticshavecomple-mented the practice of <b>ERM</b> with an elegant theory regardingoptimalsuchchoices,aswellas ...", "dateLastCrawled": "2021-08-30T18:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "4 \u2013 The Overfitting Iceberg \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie ...", "url": "https://blog.ml.cmu.edu/2020/08/31/4-overfitting/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/4-overfitting", "snippet": "As stated in the original paper, the predictor h n is commonly chosen from some function class H such as logistic regression, using <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>). By changing the <b>model</b> complexity, the capacity of the function class H also changes. It is possible to control the bias-variance tradeoff by selecting our models to balance ...", "dateLastCrawled": "2022-01-25T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Learning</b> to Branch - Proceedings of <b>Machine</b> <b>Learning</b> Research | The ...", "url": "http://proceedings.mlr.press/v80/balcan18a/balcan18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v80/bal<b>can</b>18a/bal<b>can</b>18a.pdf", "snippet": "training set, and thus performs <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>). We prove that the adaptive nature of our algorithm is necessary: performing <b>ERM</b> over a data-independent dis- cretization of the parameter space <b>can</b> be disastrous. In particular, for any discretization of the parameter space, we provide an in\ufb01nite family of distributions over MILP in-stances such that every point in the discretization results in a B&amp;B tree with exponential size in expectation, but there exist in\ufb01nitely ...", "dateLastCrawled": "2022-01-31T15:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Chapter 10 Supervised Learning</b> | Introduction to Data Science", "url": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "snippet": "We now present the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) approach to supervised <b>learning</b>, a.k.a. M-estimation in the statistical literature. Remark. We do not discuss purely algorithmic approaches such as K-nearest neighbour and kernel smoothing due to space constraints. For a broader review of supervised <b>learning</b>, see the Bibliographic Notes. Example 10.1 (Spam classification) Consider the problem of predicting if a mail is spam or not based on its attributes: length, number of exclamation ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Workshop on Socially Responsible <b>Machine</b> <b>Learning</b>", "url": "https://icml.cc/virtual/2021/workshop/8347", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2021/workshop/8347", "snippet": "Despite the success of large-scale <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) at achieving high accuracy across a variety of <b>machine</b> <b>learning</b> tasks, fair <b>ERM</b> is hindered by the incompatibility of fairness constraints with stochastic optimization. In this paper, we propose the fair <b>empirical</b> <b>risk</b> <b>minimization</b> via exponential R\u00e9nyi mutual information (FERMI) framework. FERMI is built on a stochastic estimator for exponential R\u00e9nyi mutual information (ERMI), an information divergence measuring the ...", "dateLastCrawled": "2021-12-20T14:01:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> and Stochastic Gradient Descent for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "where F(Sn) is the <b>empirical</b> distribution.2 The <b>ERM</b> dogma is to select the predictor \u03c0\u02c6\u03b8 n given by \u02c6\u03b8 n = argmin\u03b8 R\u02c6(\u03b8,Sn). That is, the objective function that de\ufb01nes <b>learning</b> is the <b>empirical</b> <b>risk</b>. <b>ERM</b> has two useful properties. (1) It provides a prin-cipled framework for de\ufb01ning new <b>machine</b> <b>learning</b> methods. In particular, when ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Statistical <b>Learning</b> Theory and the C-Loss cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the <b>Risk</b> functional as L(.) is called the Loss function, and minimize it w.r.t. w achieving the best possible loss. But we can not do this ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Computational and Statistical <b>Learning</b> Theory", "url": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) / Sample Average Approximation (SAA): Collect sample z1UYU zm ... SGD for <b>Machine</b> <b>Learning</b> Initialize S 4 L r At iteration t: Draw T \u00e7\u00e1U \u00e71\u00de If U \u00e7 S \u00e7 \u00e1\u00f6 T \u00e7 O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00e7\u00f6 T \u00e7 else: S \u00e7 &gt; 5 Z S \u00e7 Return S % \u00cd L 5 \u00cd \u00c3 \u00cd S \u00e7 \u00e7 @ 5 Draw T 5\u00e1U 5 \u00e1\u00e5\u00e1 T \u00e0 \u00e1U \u00e0 1\u00de Initialize S 4 L r At iteration t: Pick E \u00d0 s\u00e5I at random If U \u00dc S \u00e7 \u00e1\u00f6 T \u00dc O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00dc\u00f6 T \u00dc else: S \u00e7 &gt; 5 Z S \u00e7 S \u00e7 &gt; 5 Z ...", "dateLastCrawled": "2022-01-26T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Basics of <b>Machine</b> <b>Learning</b>", "url": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_<b>learning</b>.pdf", "snippet": "This is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) COMPSCI 527 \u2014 Computer Vision Basics of <b>Machine</b> <b>Learning</b> 15/26. Loss and <b>Risk</b> <b>Machine</b> <b>Learning</b> and the Statistical <b>Risk</b> <b>ERM</b>: w^ 2argmin w2R m L T(w) In <b>machine</b> <b>learning</b>, we go much farther: We also want h to do well on previously unseen inputs To relate past and future data, assume that all data comes from the same joint probability distribution p(x;y) p is called the generative data model or just model The goal of <b>machine</b> <b>learning</b> is to ...", "dateLastCrawled": "2021-11-06T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, <b>empirical</b> <b>risk</b>, motivation for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Further Reading, Supplementary: Jan 12: Consistency of <b>ERM</b>, Sufficient condition for <b>ERM</b> as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture 1: Reinforcement <b>Learning</b>: What and Why?", "url": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "snippet": "<b>machine</b> <b>learning</b> and is referred to as <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). 3 Challenges of reinforcement <b>learning</b> Consider the cart pole balancing problem, where a cart carrying an unactuated pole \ufb02oats on a straight horizontal track. The cart is actuated by a torque applied either to the right or the left direction. Seeherefor a real cart ...", "dateLastCrawled": "2021-09-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2006.09461] Robust <b>Compressed Sensing using Generative Models</b> - arXiv", "url": "https://arxiv.org/abs/2006.09461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2006.09461", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-06-27T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_<b>Machine</b>s", "snippet": "The principle used is <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) over a set of possible functions, called hypothesis space. Formally this can be written as minimizing the <b>empirical</b> . error: \u2211 = l. 1 i. x ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ToyotaTechnologicalInstituteatChicago UniversityofTexasatAustin surbhi ...", "url": "https://arxiv.org/pdf/2005.07652", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2005.07652", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, \u02c6h \u2208 RERM U(S) ,argmin h\u2208H 1 m Xm i=1 sup z\u2208U(x) 1 [h(z) 6= y]. In this paper, we provide necessary and su\ufb03cient conditions on perturbation sets U, under which the robust empirical risk minimization (RERM) problem is e\ufb03ciently solvable in the realizable setting. We show that an e\ufb03cient ...", "dateLastCrawled": "2021-10-06T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Efficiently Learning Adversarially Robust Halfspaces with</b> Noise | DeepAI", "url": "https://deepai.org/publication/efficiently-learning-adversarially-robust-halfspaces-with-noise", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>efficiently-learning-adversarially-robust-halfspaces</b>...", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, ^ h \u2208 R E R M U ( S ) \u225c argmin h \u2208 H 1 m m \u2211 i = 1 sup z \u2208 U ( x ) 1 [ h ( z ) \u2260 y ] .", "dateLastCrawled": "2021-12-05T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficiently <b>Learning</b> Adversarially Robust Halfspaces with Noise", "url": "http://proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "snippet": "remains a major challenge in <b>machine</b> <b>learning</b>. A line of work has shown that predictors learned by deep neural networks are not robust to adversarial examples (Szegedy et al.,2014;Biggio et al.,2013;Goodfellow et al.,2015). This has led to a long line of research studying different aspects of robustness to adversarial examples. In this paper, we consider the problem of distribution-independent <b>learning</b> of halfspaces that are robust to ad-versarial examples at test time, also referred to as ...", "dateLastCrawled": "2021-11-21T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(empirical risk minimization (erm))  is like +(tuning a machine learning model)", "+(empirical risk minimization (erm)) is similar to +(tuning a machine learning model)", "+(empirical risk minimization (erm)) can be thought of as +(tuning a machine learning model)", "+(empirical risk minimization (erm)) can be compared to +(tuning a machine learning model)", "machine learning +(empirical risk minimization (erm) AND analogy)", "machine learning +(\"empirical risk minimization (erm) is like\")", "machine learning +(\"empirical risk minimization (erm) is similar\")", "machine learning +(\"just as empirical risk minimization (erm)\")", "machine learning +(\"empirical risk minimization (erm) can be thought of as\")", "machine learning +(\"empirical risk minimization (erm) can be compared to\")"]}