{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Decision Making under Uncertainty: A Quasimetric Approach", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3869775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3869775", "snippet": "Assuming <b>stationary</b> dynamics, a <b>function</b> exists, satisfying . This model enables us to capture uncertainties in the knowledge of the system&#39;s dynamics, and can be used in the Markov Decision Process (MDP) formalism. The aim is to find the optimal policy allowing a goal state to be reached with minimum cumulative cost. The classic method of solving this is to use dynamic programming to build an optimal Value <b>function</b> , minimizing the total expected cumulative cost using <b>Bellman</b> <b>equation</b>: (1 ...", "dateLastCrawled": "2022-01-29T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Kernel <b>Loss for Solving the Bellman Equation</b> | DeepAI", "url": "https://deepai.org/publication/a-kernel-loss-for-solving-the-bellman-equation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-kernel-<b>loss-for-solving-the-bellman-equation</b>", "snippet": "Many popular algorithms <b>like</b> Q-learning do not optimize any objective <b>function</b>, but are fixed-<b>point</b> iterations of some variant of <b>Bellman</b> operator that is not necessarily a contraction. As a result, they may easily lose convergence guarantees, as can be observed in practice. In this paper, we propose a novel loss <b>function</b>, which can be optimized using standard <b>gradient</b>-based methods without risking divergence. The key advantage is that its <b>gradient</b> can be easily approximated using sampled ...", "dateLastCrawled": "2022-01-13T05:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) A Kernel Loss for Solving the <b>Bellman</b> <b>Equation</b>", "url": "https://www.researchgate.net/publication/333418750_A_Kernel_Loss_for_Solving_the_Bellman_Equation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../333418750_A_Kernel_Loss_for_Solving_the_<b>Bellman</b>_<b>Equation</b>", "snippet": "PDF | Value <b>function</b> learning plays a central role in many state-of-the-art reinforcement-learning algorithms. Many popular algorithms <b>like</b> Q-learning... | Find, read and cite all the research you ...", "dateLastCrawled": "2021-10-03T16:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient</b>-bounded dynamic programming for submodular and concave ...", "url": "https://www.sciencedirect.com/science/article/pii/S0005109821004209", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0005109821004209", "snippet": "The general idea is to change the <b>Bellman</b> <b>equation</b> of the DP by an epigraphic reformulation into a linear program, where the <b>Bellman</b> <b>equation</b> appears as a constraint for each state and action pair. Consequently, the number of constraints is often prohibitively large even for problems with finite state and action spaces, but sampling a large enough number of these constraints may be sufficient to generate a good enough solution for the particular problem at hand de Farias and Van Roy (2004 ...", "dateLastCrawled": "2022-01-13T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Kernel Loss for Solving the <b>Bellman</b> <b>Equation</b>", "url": "https://www.arxiv-vanity.com/papers/1905.10506/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1905.10506", "snippet": "Value <b>function</b> learning plays a central role in many state-of-the-art reinforcement-learning algorithms. Many popular algorithms <b>like</b> Q-learning do not optimize any objective <b>function</b>, but are fixed-<b>point</b> iterations of some variants of <b>Bellman</b> operator that are not necessarily a contraction. As a result, they may easily lose convergence guarantees, as can be observed in practice. In this paper, we propose a novel loss <b>function</b>, which can be optimized using standard <b>gradient</b>-based methods ...", "dateLastCrawled": "2021-11-11T05:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A machine learning framework for solving high-dimensional mean ... - <b>PNAS</b>", "url": "https://www.pnas.org/content/117/17/9183", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/17/9183", "snippet": "A key observation is that both problems involve a Hamilton\u2013Jacobi\u2013<b>Bellman</b> (HJB) <b>equation</b> that is coupled with a continuity <b>equation</b>. From the solution of this system of partial differential equations (PDEs), each agent can infer the cost of their optimal action, which is why it is commonly called value <b>function</b>. In addition, the agent can obtain their optimal action from the <b>gradient</b> of the value <b>function</b>, which alleviates the need for individual optimization (see refs.", "dateLastCrawled": "2022-01-14T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Calculus of variations</b> - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Calculus_of_variations", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Calculus_of_variations</b>", "snippet": "The <b>calculus of variations</b> is a field of <b>mathematical</b> analysis that uses variations, which are small changes in functions and functionals, to find maxima and minima of functionals: mappings from a set of functions to the real numbers. [lower-alpha 1] Functionals are often expressed as definite integrals involving functions and their derivatives.Functions that maximize or minimize functionals may be found using the Euler\u2013Lagrange <b>equation</b> of the <b>calculus of variations</b>.. Contents. History ...", "dateLastCrawled": "2021-01-18T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "On <b>the Expected Dynamics of Nonlinear TD Learning</b> | DeepAI", "url": "https://deepai.org/publication/on-the-expected-dynamics-of-nonlinear-td-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-<b>the-expected-dynamics-of-nonlinear-td-learning</b>", "snippet": "In essence, TD learning follows a biased estimate of the <b>gradient</b> of the squared <b>Bellman</b> error, which is minimized by the true value <b>function</b>. The bias is intrinsic to the fact that one cannot obtain more than one independent sample from the environment at any given time. As it turns out, this bias can be seen geometrically as \u201cbending\u201d the <b>gradient</b> flow dynamics and potentially eliminates the convergence guarantees of <b>gradient</b> descent when combined with nonlinear <b>function</b> approximation.", "dateLastCrawled": "2022-01-15T10:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "NEWTON\u2019S METHOD AND FRACTALS", "url": "https://www.whitman.edu/Documents/Academics/Mathematics/burton.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.whitman.edu/Documents/Academics/Mathematics/burton.pdf", "snippet": "Solving the <b>equation</b> f(x) = 0 Given a <b>function</b> f, nding the solutions of the <b>equation</b> f(x) = 0 is one of the oldest <b>mathematical</b> problems. General methods to nd the roots of f(x) = 0 when f(x) is a polynomial of degree one or two have been known since 2000 B.C. [3]. For example, to solve for the roots of a quadratic <b>function</b> ax2 + bx+ c= 0 we may utilize the quadratic formula: x= b p b2 4ac 2a: Methods to solve polynomials of degree three and four were discovered in the 16th century by the ...", "dateLastCrawled": "2022-01-30T08:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Optimization Concepts and Applications in Engineering, Second Edition ...", "url": "https://silo.pub/optimization-concepts-and-applications-in-engineering-second-edition.html", "isFamilyFriendly": true, "displayUrl": "https://<b>silo.pub</b>/optimization-concepts-and-applications-in-engineering-second-edition.html", "snippet": "<b>Equation</b> (3.1) states <b>that the gradient</b> of the <b>function</b> equals <b>zero</b> or that the \u2217 derivatives \u2202 f\u2202(xxi ) = 0, i = 1, 2, . . . , n. A <b>point</b> x\u2217 that satisfies Eq. (3.1) is called 3.2 Necessary and Suf\ufb01cient Conditions for Optimality f(x) 91 f(x) x2 x2 x x1 x1 Minimum <b>Point</b> x Maximum <b>Point</b> x2 Saddle <b>Point</b> f(x) x x1 Figure 3.1. <b>Stationary</b> ...", "dateLastCrawled": "2022-01-30T14:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Kernel <b>Loss for Solving the Bellman Equation</b> | DeepAI", "url": "https://deepai.org/publication/a-kernel-loss-for-solving-the-bellman-equation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-kernel-<b>loss-for-solving-the-bellman-equation</b>", "snippet": "Many popular algorithms like Q-learning do not optimize any objective <b>function</b>, but are fixed-<b>point</b> iterations of some variant of <b>Bellman</b> operator that is not necessarily a contraction. As a result, they may easily lose convergence guarantees, as can be observed in practice. In this paper, we propose a novel loss <b>function</b>, which can be optimized using standard <b>gradient</b>-based methods without risking divergence. The key advantage is that its <b>gradient</b> can be easily approximated using sampled ...", "dateLastCrawled": "2022-01-13T05:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Decision Making under Uncertainty: A Quasimetric Approach", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3869775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3869775", "snippet": "Assuming <b>stationary</b> dynamics, a <b>function</b> exists, satisfying . This model enables us to capture uncertainties in the knowledge of the system&#39;s dynamics, and can be used in the Markov Decision Process (MDP) formalism. The aim is to find the optimal policy allowing a goal state to be reached with minimum cumulative cost. The classic method of solving this is to use dynamic programming to build an optimal Value <b>function</b> , minimizing the total expected cumulative cost using <b>Bellman</b> <b>equation</b>: (1 ...", "dateLastCrawled": "2022-01-29T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "SpringerLink - Research in the <b>Mathematical</b> Sciences", "url": "https://link.springer.com/article/10.1007/s40687-018-0172-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40687-018-0172-y", "snippet": "One can then derive a partial differential <b>equation</b> (Hamilton\u2013Jacobi\u2013<b>Bellman</b> <b>equation</b>, or HJB <b>equation</b>) to be satisfied by such a value <b>function</b>, which characterizes both the optimal loss <b>function</b> value and the optimal control policy of the original control problem. Compared to the classical optimal control case corresponding to empirical risk minimization in learning, here the value <b>function</b>\u2019s state argument is no longer a finite-dimensional vector, but an infinite-dimensional object ...", "dateLastCrawled": "2022-01-28T17:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Partial differential</b> <b>equation</b> models in macroeconomics | Philosophical ...", "url": "https://royalsocietypublishing.org/doi/10.1098/rsta.2013.0397", "isFamilyFriendly": true, "displayUrl": "https://royalsocietypublishing.org/doi/10.1098/rsta.2013.0397", "snippet": "Macroeconomic models with heterogeneous agents share a common <b>mathematical</b> structure which, in continuous time, can be summarized by a system of coupled nonlinear <b>partial differential</b> equations (PDEs): (i) a Hamilton\u2013Jacobi\u2013<b>Bellman</b> (HJB) <b>equation</b> describing the optimal control problem of a single atomistic individual and (ii) an <b>equation</b> describing the evolution of the distribution of a vector of individual state variables in the population (such as a Fokker\u2013Planck <b>equation</b>, Fisher ...", "dateLastCrawled": "2022-01-31T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) A Kernel Loss for Solving the <b>Bellman</b> <b>Equation</b>", "url": "https://www.researchgate.net/publication/333418750_A_Kernel_Loss_for_Solving_the_Bellman_Equation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../333418750_A_Kernel_Loss_for_Solving_the_<b>Bellman</b>_<b>Equation</b>", "snippet": "PDF | Value <b>function</b> learning plays a central role in many state-of-the-art reinforcement-learning algorithms. Many popular algorithms like Q-learning... | Find, read and cite all the research you ...", "dateLastCrawled": "2021-10-03T16:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Mathematical</b> optimization : definition of <b>Mathematical</b> optimization and ...", "url": "http://dictionary.sensagent.com/Mathematical%20optimization/en-en/", "isFamilyFriendly": true, "displayUrl": "dictionary.sensagent.com/<b>Mathematical</b> optimization/en-en", "snippet": "The <b>equation</b> that describes the relationship between these subproblems is called the <b>Bellman</b> <b>equation</b>. <b>Mathematical</b> programming with equilibrium constraints is where the constraints include variational inequalities or complementarities. Multi-objective optimization . Main article: Multi-objective optimization. Adding more than one objective to an optimization problem adds complexity. For example, to optimize a structural design, one would want a design that is both light and rigid. Because ...", "dateLastCrawled": "2022-01-10T08:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Gradient</b>-bounded dynamic programming for submodular and concave ...", "url": "https://www.sciencedirect.com/science/article/pii/S0005109821004209", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0005109821004209", "snippet": "The general idea is to change the <b>Bellman</b> <b>equation</b> of the DP by an epigraphic reformulation into a linear program, where the <b>Bellman</b> <b>equation</b> appears as a constraint for each state and action pair. Consequently, the number of constraints is often prohibitively large even for problems with finite state and action spaces, but sampling a large enough number of these constraints may be sufficient to generate a good enough solution for the particular problem at hand de Farias and Van Roy (2004 ...", "dateLastCrawled": "2022-01-13T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A machine learning framework for solving high-dimensional mean ... - <b>PNAS</b>", "url": "https://www.pnas.org/content/117/17/9183", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/17/9183", "snippet": "A key observation is that both problems involve a Hamilton\u2013Jacobi\u2013<b>Bellman</b> (HJB) <b>equation</b> that is coupled with a continuity <b>equation</b>. From the solution of this system of partial differential equations (PDEs), each agent can infer the cost of their optimal action, which is why it is commonly called value <b>function</b>. In addition, the agent can obtain their optimal action from the <b>gradient</b> of the value <b>function</b>, which alleviates the need for individual optimization (see refs.", "dateLastCrawled": "2022-01-14T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Online model\u2010free reinforcement learning for the automatic control of a ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-cta.2018.6163", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-cta.2018.6163", "snippet": "The adaptive learning scheme is able to solve the underlying model-free <b>Bellman</b> optimality <b>equation</b> (temporal difference <b>equation</b>) of the dynamical system in real-time. The dynamic learning system is realised using a value iteration process with two supporting separate neural network structures, where <b>gradient</b> descent approach is used to tune the neural networks weights. A Riccati development is introduced to understand the duality between the model-based control solution and its equivalent ...", "dateLastCrawled": "2021-11-02T12:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "NEWTON\u2019S METHOD AND FRACTALS", "url": "https://www.whitman.edu/Documents/Academics/Mathematics/burton.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.whitman.edu/Documents/Academics/Mathematics/burton.pdf", "snippet": "Solving the <b>equation</b> f(x) = 0 Given a <b>function</b> f, nding the solutions of the <b>equation</b> f(x) = 0 is one of the oldest <b>mathematical</b> problems. General methods to nd the roots of f(x) = 0 when f(x) is a polynomial of degree one or two have been known since 2000 B.C. [3]. For example, to solve for the roots of a quadratic <b>function</b> ax2 + bx+ c= 0 we may utilize the quadratic formula: x= b p b2 4ac 2a: Methods to solve polynomials of degree three and four were discovered in the 16th century by the ...", "dateLastCrawled": "2022-01-30T08:59:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Optimal control theory and the linear Bellman equation</b>", "url": "https://www.researchgate.net/publication/255662603_Optimal_control_theory_and_the_linear_Bellman_equation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/255662603_<b>Optimal_control_theory_and_the</b>...", "snippet": "Even though the general problem <b>can</b> be formulated as Hamilton-Jacobi-<b>Bellman</b> partial differential <b>equation</b>, it&#39;s solution is often intractable (Bryson and Ho, 1975). ...", "dateLastCrawled": "2021-10-13T16:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Bellman</b> <b>approach for two-domains optimal control problems in</b> $\\R^N$", "url": "https://www.researchgate.net/publication/51965674_A_Bellman_approach_for_two-domains_optimal_control_problems_in_RN", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/51965674_A_<b>Bellman</b>_approach_for_two-domains...", "snippet": "In the ergodic case, they are described by a system coupling a Hamilton- Jacobi-<b>Bellman</b> <b>equation</b> and a Fokker-Planck <b>equation</b>, whose unknowns are the density m of the invariant measure which ...", "dateLastCrawled": "2021-10-25T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A machine learning framework for solving high-dimensional mean ... - <b>PNAS</b>", "url": "https://www.pnas.org/content/117/17/9183", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/17/9183", "snippet": "A key observation is that both problems involve a Hamilton\u2013Jacobi\u2013<b>Bellman</b> (HJB) <b>equation</b> that is coupled with a continuity <b>equation</b>. From the solution of this system of partial differential equations (PDEs), each agent <b>can</b> infer the cost of their optimal action, which is why it is commonly called value <b>function</b>. In addition, the agent <b>can</b> obtain their optimal action from the <b>gradient</b> of the value <b>function</b>, which alleviates the need for individual optimization (see refs.", "dateLastCrawled": "2022-01-14T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Rational thoughts in neural codes | <b>PNAS</b>", "url": "https://www.pnas.org/content/117/47/29311", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/47/29311", "snippet": "In particular, the state-action value <b>function</b> Q (b, a) under a softmax policy \u03c0 (a | b) <b>can</b> be expressed recursively by a <b>Bellman</b> <b>equation</b>, which we solve using value iteration (9, 10). The resultant value <b>function</b> then determines the softmax policy \u03c0 and thereby determines the policy-dependent term in the log-likelihood 1.", "dateLastCrawled": "2021-06-08T06:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Learning near-optimal policies with <b>Bellman</b>-residual minimization ...", "url": "https://www.academia.edu/630060/Learning_near_optimal_policies_with_Bellman_residual_minimization_based_fitted_policy_iteration_and_a_single_sample_path", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/630060/Learning_near_optimal_policies_with_<b>Bellman</b>_residual...", "snippet": "Learning near-optimal policies with <b>Bellman</b>-residual minimization based fitted policy iteration and a single sample path. Machine Learning, 2008. Csaba Szepesvari. Download PDF. Download Full PDF Package. This paper. A short summary of this paper. 37 Full PDFs related to this paper. READ PAPER. Learning near-optimal policies with <b>Bellman</b>-residual minimization based fitted policy iteration and a single sample path . Download. Learning near-optimal policies with <b>Bellman</b>-residual minimization ...", "dateLastCrawled": "2022-01-29T19:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 5, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Hamilton\u2013Jacobi equation</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi_equation", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Hamilton\u2013Jacobi_equation</b>", "snippet": "The <b>Hamilton\u2013Jacobi equation</b> is a single, first-order partial differential <b>equation</b> for the <b>function</b> of the generalized ... Hence, all its derivatives are also <b>zero</b>, and the transformed Hamilton&#39;s equations become trivial \u02d9 = \u02d9 = so the new generalized coordinates and momenta are constants of motion. As they are constants, in this context the new generalized momenta are usually denoted ,, \u2026,, i.e. = and the new generalized coordinates are typically denoted as ,, \u2026,, so =. Setting the ...", "dateLastCrawled": "2022-01-30T18:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 6, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable <b>function</b> is defined and differentiable in a neighborhood of a <b>point</b> , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>OPTIMIZATION FOR ENGINEERING DESIGN</b> | Dineshwar ... - Academia.edu", "url": "https://www.academia.edu/41979322/OPTIMIZATION_FOR_ENGINEERING_DESIGN", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/41979322/<b>OPTIMIZATION_FOR_ENGINEERING_DESIGN</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-29T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Machine learning &amp; <b>artificial intelligence</b> in the quantum domain: a ...", "url": "https://iopscience.iop.org/article/10.1088/1361-6633/aab406", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1361-6633/aab406", "snippet": "Next, a supervised algorithm <b>can</b> perform lazy learning, meaning that the whole labeled dataset is kept in memory in order to label unknown points (which <b>can</b> then be added), or eager learning, in which case, the (total) classifier <b>function</b> is output (and the training set is no longer explicitly required) (Alpaydin 2010).", "dateLastCrawled": "2021-11-23T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>TYBSC CS SEM 5 AI NOTES</b> - SlideShare", "url": "https://www.slideshare.net/SiddheshZele/tybsc-cs-sem-5-ai-notes", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/SiddheshZele/<b>tybsc-cs-sem-5-ai-notes</b>", "snippet": "The <b>function</b> which evaluates the state is the heuristic <b>function</b> and the value calculated by this <b>function</b> is heuristic value of state. The heuristic <b>function</b> is represented as h(n) 8-puzzle problem Admissible heuristics h1(n) = number of misplaced tiles h2(n) = total Manhattan distance (i.e., no. of squares from desired location of each tile) In the example h1(S) = 6 h2(S) = 2 + 0 + 3 + 1 + 0 + 1 + 3 + 4 = 14", "dateLastCrawled": "2022-01-26T09:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Decision Making under Uncertainty: A Quasimetric Approach", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3869775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3869775", "snippet": "Assuming <b>stationary</b> dynamics, a <b>function</b> exists, satisfying . This model enables us to capture uncertainties in the knowledge of the system&#39;s dynamics, and <b>can</b> be used in the Markov Decision Process (MDP) formalism. The aim is to find the optimal policy allowing a goal state to be reached with minimum cumulative cost. The classic method of solving this is to use dynamic programming to build an optimal Value <b>function</b> , minimizing the total expected cumulative cost using <b>Bellman</b> <b>equation</b>: (1 ...", "dateLastCrawled": "2022-01-29T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "SpringerLink - Research in the <b>Mathematical</b> Sciences", "url": "https://link.springer.com/article/10.1007/s40687-018-0172-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40687-018-0172-y", "snippet": "One <b>can</b> then derive a partial differential <b>equation</b> (Hamilton\u2013Jacobi\u2013<b>Bellman</b> <b>equation</b>, or HJB <b>equation</b>) to be satisfied by such a value <b>function</b>, which characterizes both the optimal loss <b>function</b> value and the optimal control policy of the original control problem. <b>Compared</b> to the classical optimal control case corresponding to empirical risk minimization in learning, here the value <b>function</b>\u2019s state argument is no longer a finite-dimensional vector, but an infinite-dimensional object ...", "dateLastCrawled": "2022-01-28T17:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Kernel <b>Loss for Solving the Bellman Equation</b> | DeepAI", "url": "https://deepai.org/publication/a-kernel-loss-for-solving-the-bellman-equation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-kernel-<b>loss-for-solving-the-bellman-equation</b>", "snippet": "Many popular algorithms like Q-learning do not optimize any objective <b>function</b>, but are fixed-<b>point</b> iterations of some variant of <b>Bellman</b> operator that is not necessarily a contraction. As a result, they may easily lose convergence guarantees, as <b>can</b> be observed in practice. In this paper, we propose a novel loss <b>function</b>, which <b>can</b> be optimized using standard <b>gradient</b>-based methods without risking divergence. The key advantage is that its <b>gradient</b> <b>can</b> be easily approximated using sampled ...", "dateLastCrawled": "2022-01-13T05:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Partial differential</b> <b>equation</b> models in macroeconomics | Philosophical ...", "url": "https://royalsocietypublishing.org/doi/10.1098/rsta.2013.0397", "isFamilyFriendly": true, "displayUrl": "https://royalsocietypublishing.org/doi/10.1098/rsta.2013.0397", "snippet": "Macroeconomic models with heterogeneous agents share a common <b>mathematical</b> structure which, in continuous time, <b>can</b> be summarized by a system of coupled nonlinear <b>partial differential</b> equations (PDEs): (i) a Hamilton\u2013Jacobi\u2013<b>Bellman</b> (HJB) <b>equation</b> describing the optimal control problem of a single atomistic individual and (ii) an <b>equation</b> describing the evolution of the distribution of a vector of individual state variables in the population (such as a Fokker\u2013Planck <b>equation</b>, Fisher ...", "dateLastCrawled": "2022-01-31T05:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) A Kernel Loss for Solving the <b>Bellman</b> <b>Equation</b>", "url": "https://www.researchgate.net/publication/339945787_A_Kernel_Loss_for_Solving_the_Bellman_Equation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../339945787_A_Kernel_Loss_for_Solving_the_<b>Bellman</b>_<b>Equation</b>", "snippet": "PDF | Value <b>function</b> learning plays a central role in many state-of-the-art reinforcement-learning algorithms. Many popular algorithms like Q-learning... | Find, read and cite all the research you ...", "dateLastCrawled": "2021-09-08T12:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A machine learning framework for solving high-dimensional mean ... - <b>PNAS</b>", "url": "https://www.pnas.org/content/117/17/9183", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/17/9183", "snippet": "A key observation is that both problems involve a Hamilton\u2013Jacobi\u2013<b>Bellman</b> (HJB) <b>equation</b> that is coupled with a continuity <b>equation</b>. From the solution of this system of partial differential equations (PDEs), each agent <b>can</b> infer the cost of their optimal action, which is why it is commonly called value <b>function</b>. In addition, the agent <b>can</b> obtain their optimal action from the <b>gradient</b> of the value <b>function</b>, which alleviates the need for individual optimization (see refs.", "dateLastCrawled": "2022-01-14T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Gradient</b>-bounded dynamic programming for submodular and concave ...", "url": "https://www.sciencedirect.com/science/article/pii/S0005109821004209", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0005109821004209", "snippet": "The general idea is to change the <b>Bellman</b> <b>equation</b> of the DP by an epigraphic reformulation into a linear program, where the <b>Bellman</b> <b>equation</b> appears as a constraint for each state and action pair. Consequently, the number of constraints is often prohibitively large even for problems with finite state and action spaces, but sampling a large enough number of these constraints may be sufficient to generate a good enough solution for the particular problem at hand de Farias and Van Roy (2004 ...", "dateLastCrawled": "2022-01-13T22:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>Bellman</b> <b>approach for two-domains optimal control problems in</b> $\\R^N$", "url": "https://www.researchgate.net/publication/51965674_A_Bellman_approach_for_two-domains_optimal_control_problems_in_RN", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/51965674_A_<b>Bellman</b>_approach_for_two-domains...", "snippet": "This article is the starting <b>point</b> of a series of works whose aim is the study of deterministic control problems where the dynamic and the running cost <b>can</b> be completely different in two (or more ...", "dateLastCrawled": "2021-10-25T03:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Online model\u2010free reinforcement learning for the automatic control of a ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-cta.2018.6163", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-cta.2018.6163", "snippet": "The adaptive learning scheme is able to solve the underlying model-free <b>Bellman</b> optimality <b>equation</b> (temporal difference <b>equation</b>) of the dynamical system in real-time. The dynamic learning system is realised using a value iteration process with two supporting separate neural network structures, where <b>gradient</b> descent approach is used to tune the neural networks weights. A Riccati development is introduced to understand the duality between the model-based control solution and its equivalent ...", "dateLastCrawled": "2021-11-02T12:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "NEWTON\u2019S METHOD AND FRACTALS", "url": "https://www.whitman.edu/Documents/Academics/Mathematics/burton.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.whitman.edu/Documents/Academics/Mathematics/burton.pdf", "snippet": "Solving the <b>equation</b> f(x) = 0 Given a <b>function</b> f, nding the solutions of the <b>equation</b> f(x) = 0 is one of the oldest <b>mathematical</b> problems. General methods to nd the roots of f(x) = 0 when f(x) is a polynomial of degree one or two have been known since 2000 B.C. [3]. For example, to solve for the roots of a quadratic <b>function</b> ax2 + bx+ c= 0 we may utilize the quadratic formula: x= b p b2 4ac 2a: Methods to solve polynomials of degree three and four were discovered in the 16th century by the ...", "dateLastCrawled": "2022-01-30T08:59:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Automating Analogy: Identifying Meaning Across Domains</b> via AI | by Sean ...", "url": "https://towardsdatascience.com/automating-analogy-using-ai-to-help-researchers-make-discoveries-1ca04e9b620", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/automating-<b>analogy</b>-using-ai-to-help-researchers-make...", "snippet": "That optimization is driven by Hamilton\u2013Jacobi\u2013<b>Bellman</b> <b>equation</b> (HJB), ... This is the power of using automated <b>analogy</b> to make connections between areas we might never think to link together. It\u2019s a nice example of augmenting the way people already work, by using \u201cintelligent\u201d machines that operate in a similar fashion. But, is it really worth exploring the use of the HJB <b>equation</b> matched with Clarke gradients, as used by the authors of an economics journal, to learn the ...", "dateLastCrawled": "2022-01-24T00:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Recent advance in <b>machine</b> <b>learning</b> for partial differential <b>equation</b> ...", "url": "https://www.researchgate.net/publication/354036763_Recent_advance_in_machine_learning_for_partial_differential_equation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/354036763_Recent_advance_in_<b>machine</b>_<b>learning</b>...", "snippet": "Numerical results on examples including the nonlinear Black-Scholes <b>equation</b>, the Hamilton-Jacobi-<b>Bellman</b> <b>equation</b>, and the Allen-Cahn <b>equation</b> suggest that the proposed algorithm is quite ...", "dateLastCrawled": "2021-12-20T16:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Reinforcement Learning</b> as Heuristic Search <b>Analogy</b> - DataDrivenInvestor", "url": "https://medium.datadriveninvestor.com/reinforcement-learning-as-heuristic-search-analogy-31d92b06dadd", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/<b>reinforcement-learning</b>-as-heuristic-search...", "snippet": "Essentially <b>Bellman</b> Optimality <b>Equation</b> says to choose the action that maximizes R(s) + (Some Heuristic). The Heuristic here is the value of your future state upon choosing your action (a), It is also called Value Function, denoted by V. In essence the heuristic changes for every state and action you are in. In this way, the RL algorithm can essentially model most arbitrary heuristic functions present in A* algorithms. So how exactly does it learn this heuristic. Well I will tell you one way ...", "dateLastCrawled": "2022-01-21T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Bayes Meets <b>Bellman</b>: The Gaussian Process Approach to Temporal ...", "url": "https://www.aaai.org/Papers/ICML/2003/ICML03-023.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aaai.org/Papers/ICML/2003/ICML03-023.pdf", "snippet": "Bayes Meets <b>Bellman</b>: The Gaussian Process Approach to Temporal Difference <b>Learning</b> Yaakov ... Reinforcement <b>Learning</b> (RL) is a field of <b>machine</b> <b>learning</b> concerned ~dth problems that can be formu-lated as Markov Decision Processes (MDPs) (Bert-sekas &amp; Tsitsiklis, 1996; Sutton &amp; Barto, 1998). An MDP is a tuple {S,A,R,p} where S and A are the state and action spaces, respectively; R : S x S --+ L~ is the immediate reward which may be a random pro-cess2; p : S x A \u00d7 S --&gt; [0, 1] is the ...", "dateLastCrawled": "2022-01-22T12:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An <b>introduction to Q-Learning: Reinforcement Learning</b>", "url": "https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/an-<b>introduction-to-q-learning-reinforcement-learning</b>", "snippet": "In that description of how we pursue our goals in daily life, we framed for ourselves a representative <b>analogy</b> of reinforcement <b>learning</b>. Let me summarize the above example reformatting the main points of interest. Our reality contains environments in which we perform numerous actions. Sometimes we get good or positive rewards for some of these actions in order to achieve goals. During the entire course of life, our mental and physical states evolve. We strengthen our actions in order to get ...", "dateLastCrawled": "2022-01-31T09:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Physics-informed <b>machine</b> <b>learning</b>", "url": "https://www.researchgate.net/publication/351814752_Physics-informed_machine_learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/351814752_Physics-informed_<b>machine</b>_<b>learning</b>", "snippet": "<b>Machine</b> <b>learning</b> has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained ...", "dateLastCrawled": "2022-01-26T05:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reinforcement <b>learning</b>: <b>Temporal-Difference</b>, SARSA, Q-<b>Learning</b> ...", "url": "https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/reinforcement-<b>learning</b>-<b>temporal-difference</b>-sarsa-q...", "snippet": "<b>Bellman</b> <b>equation</b>; Value, policy functions and iterations; Some Psychology. You may skip this section, it\u2019s optional and not a pre-requisite for the rest of the post. I love studying artificial intelligence concepts while correlating the m to psychology \u2014 Human behaviour and the brain. Reinforcement <b>learning</b> is no exception. Our topic of interest \u2014 <b>Temporal difference</b> was a term coined by Richard S. Sutton. This post is derived from his and Andrew Barto \u2019s book \u2014 An introduction to ...", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Neural Networks and Learning Machines</b> - uniba.sk", "url": "https://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf", "isFamilyFriendly": true, "displayUrl": "https://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf", "snippet": "3.7 The Langevin <b>Equation</b>: Characterization of Brownian Motion 106 3.8 Kushner\u2019s Direct-Averaging Method 107 3.9 Statistical LMS <b>Learning</b> Theory for Small <b>Learning</b>-Rate Parameter 108 3.10 Computer Experiment I: Linear Prediction 110 3.11 Computer Experiment II: Pattern Classification 112 3.12 Virtues and Limitations of the LMS Algorithm 113 3.13 <b>Learning</b>-Rate Annealing Schedules 115 3.14 Summary and Discussion 117 Notes and References 118 Problems 119. Chapter 4 Multilayer Perceptrons 122 ...", "dateLastCrawled": "2022-02-02T00:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Algorithms for Solving High Dimensional PDEs: From Nonlinear ... - DeepAI", "url": "https://deepai.org/publication/algorithms-for-solving-high-dimensional-pdes-from-nonlinear-monte-carlo-to-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/algorithms-for-solving-high-dimensional-pdes-from...", "snippet": "In recent years, tremendous progress has been made on numerical algorithms for solving partial differential equations (PDEs) in a very high dimension, using ideas from either nonlinear (multilevel) Monte Carlo or deep <b>learning</b>.They are potentially free of the curse of dimensionality for many different applications and have been proven to be so in the case of some nonlinear Monte Carlo methods for nonlinear parabolic PDEs. In this paper, we review these numerical and theoretical advances.", "dateLastCrawled": "2022-01-09T23:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "5 most common evaluation metrics used by a <b>machine</b> <b>learning</b> &amp; deep ...", "url": "https://maciejzalwert.medium.com/5-most-common-evaluation-metrics-used-by-a-machine-learning-deep-learning-scientists-that-you-3eaa295f9fdc", "isFamilyFriendly": true, "displayUrl": "https://maciejzalwert.medium.com/5-most-common-evaluation-metrics-used-by-a-<b>machine</b>...", "snippet": "5 the most common evaluation metrics used by a <b>machine</b> <b>learning</b> &amp; deep <b>learning</b> scientists that you should know in depth. Evaluation metrics are the foundations of every ML/AI project. The main goal is to evaluate performance of a particular model. Unfortunately, very often happens that certain metrics are not completely understood \u2014 especially with a client side. In this article I will introduce 5 most common metrics and try to show some potential idiosyncratic* risks they have. Accuracy ...", "dateLastCrawled": "2022-01-26T12:22:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(bellman equation)  is like +(the mathematical statement that the gradient of a function is zero at a stationary point)", "+(bellman equation) is similar to +(the mathematical statement that the gradient of a function is zero at a stationary point)", "+(bellman equation) can be thought of as +(the mathematical statement that the gradient of a function is zero at a stationary point)", "+(bellman equation) can be compared to +(the mathematical statement that the gradient of a function is zero at a stationary point)", "machine learning +(bellman equation AND analogy)", "machine learning +(\"bellman equation is like\")", "machine learning +(\"bellman equation is similar\")", "machine learning +(\"just as bellman equation\")", "machine learning +(\"bellman equation can be thought of as\")", "machine learning +(\"bellman equation can be compared to\")"]}