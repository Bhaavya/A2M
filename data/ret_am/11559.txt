{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>The Consistency between Cross-Entropy and Distance Measures</b> in Fuzzy <b>Sets</b>", "url": "https://www.mdpi.com/2073-8994/11/3/386/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2073-8994/11/3/386/htm", "snippet": "On account of these similar properties <b>between</b> <b>distance</b> measure and <b>cross-entropy</b> (such as non-negativity and symmetry), and when the <b>cross-entropy</b> (<b>distance</b>) <b>between</b> <b>two</b> fuzzy <b>sets</b> is 0 if, and only if the <b>two</b> <b>sets</b> coincide. We also found that the decision principle of <b>cross-entropy</b> is consistent with decision principle of <b>distance</b> measure in decision-making. That decision principle is, among all the choices, we finally chose the one with the smallest <b>cross-entropy</b> (<b>distance</b>) from the ideal ...", "dateLastCrawled": "2021-12-14T01:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. <b>Cross-entropy</b> is commonly used in machine learning as a loss function. <b>Cross-entropy</b> is a measure from the field <b>of information</b> theory, building upon entropy and generally calculating the difference <b>between</b> <b>two</b> probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy <b>between</b> <b>two</b> probability distributions, whereas <b>cross-entropy</b> can be thought to calculate the total entropy <b>between</b> the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "machine learning - What is <b>cross-entropy</b>? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/41990250/what-is-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/41990250", "snippet": "In short, <b>cross-entropy</b> (CE) is the measure of how far is your predicted value from the true label. The cross here refers to calculating the entropy <b>between</b> <b>two</b> or more features / true labels (<b>like</b> 0, 1). And the term entropy itself refers to randomness, so large value of it means your prediction is far off from real labels.", "dateLastCrawled": "2022-01-28T16:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>distance</b> measures and <b>cross-entropy</b> based on complex fuzzy <b>sets</b> and ...", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs191718", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs191718", "snippet": "The <b>cross entropy</b> <b>between</b> CFSs is used to discriminate uncertain <b>information</b> expressed by CFSs, we can define <b>cross entropy</b> measure based on the DIMs with respect to these factors determining complex fuzzy uncertainty. It has been pointed that uncertainty of CFSs contained <b>two</b> dimensional <b>information</b> in a single set. Generally, the entropy measure merely quantifies the fuzziness, without consideration of the distribution of fuzziness. In other words, the entropy is determined by the ...", "dateLastCrawled": "2022-02-01T15:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The <b>distance measures and cross-entropy</b> based on complex fuzzy <b>sets</b> and ...", "url": "https://www.researchgate.net/publication/340906659_The_distance_measures_and_cross-entropy_based_on_complex_fuzzy_sets_and_their_application_in_decision_making", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340906659_The_<b>distance</b>_measures_and_cross...", "snippet": "However, the relation <b>between</b> <b>cross-entropy</b> and <b>distance</b> measures in fuzzy <b>sets</b> or neutrosophic <b>sets</b> has not yet been verified. In this paper, we mainly consider the relation <b>between</b> the ...", "dateLastCrawled": "2022-01-03T07:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>Distance Model of Intuitionistic Fuzzy Cross Entropy</b> to Solve ...", "url": "https://www.hindawi.com/journals/mpe/2016/8324124/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2016/8324124", "snippet": "(1) The <b>distance</b> measure of the intuitionistic fuzzy multiple attribute decision-making method with preference <b>information</b> on alternatives adopts the intuitionistic fuzzy <b>cross entropy</b>, which is not <b>like</b> the geometric <b>distance</b> such as the Hamming or the Euclidean <b>distance</b>, but a <b>distance</b> <b>of information</b> that can better reflect the difference <b>between</b> the intuitionistic fuzzy <b>sets</b> and not just keep the <b>information</b>. Take the calculation process as an example in this section. The subjective ...", "dateLastCrawled": "2022-01-04T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Visual Information Theory</b> -- colah&#39;s blog", "url": "http://colah.github.io/posts/2015-09-Visual-Information/", "isFamilyFriendly": true, "displayUrl": "colah.github.io/posts/2015-09-Visual-<b>Information</b>", "snippet": "The really neat thing about KL divergence is that it\u2019s <b>like</b> a <b>distance</b> <b>between</b> <b>two</b> distributions. It measures how different they are! (If you take that idea seriously, you end up with <b>information</b> geometry.) <b>Cross-Entropy</b> and KL divergence are incredibly useful in machine learning. Often, we want one distribution to be close to another. For example, we might want a predicted distribution to be close to the ground truth. KL divergence gives us a natural way to do this, and so it shows up ...", "dateLastCrawled": "2022-02-01T11:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Cross-entropy</b> and Maximum Likelihood Estimation | by Roan Gylberth ...", "url": "https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/konvergen/<b>cross-entropy</b>-and-maximum-<b>like</b>lihood-estimation-58942b52517a", "snippet": "<b>Cross-entropy</b> and Maximum Likelihood Estimation. So, we are on our way to train our first neural network model for classification. We design our network depth, the activation function, set all the ...", "dateLastCrawled": "2022-01-29T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "In binary classification, is it okay to use <b>cross entropy</b> loss instead ...", "url": "https://machinelearning1.quora.com/In-binary-classification-is-it-okay-to-use-cross-entropy-loss-instead-of-binary-cross-entropy-loss-I-am-using-pre-defi", "isFamilyFriendly": true, "displayUrl": "https://machinelearning1.quora.com/In-binary-classification-is-it-okay-to-use-cross...", "snippet": "<b>Cross Entropy</b> is definitely a good loss function for Classification Problems, because it minimizes the <b>distance</b> <b>between</b> <b>two</b> probability distributions - predicted and actual.. Conceptually, you can understand it <b>like</b> this - Consider a classifier which predicts whether the given animal is dog, cat or horse with a probability associated with each.", "dateLastCrawled": "2022-01-19T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is the <b>cross-entropy</b> cost function better than mean squared error cost ...", "url": "https://www.quora.com/Is-the-cross-entropy-cost-function-better-than-mean-squared-error-cost-function-Why", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-the-<b>cross-entropy</b>-cost-function-better-than-mean-squared...", "snippet": "Answer: TL;DR: I do not think you can put a strict preferance on either without specifying the intended use. Some background: * For a \u201csoft-max\u201d output, the cross ...", "dateLastCrawled": "2022-01-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>The Consistency between Cross-Entropy and Distance Measures</b> in Fuzzy <b>Sets</b>", "url": "https://www.mdpi.com/2073-8994/11/3/386/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2073-8994/11/3/386/htm", "snippet": "On account of these <b>similar</b> properties <b>between</b> <b>distance</b> measure and <b>cross-entropy</b> (such as non-negativity and symmetry), and when the <b>cross-entropy</b> (<b>distance</b>) <b>between</b> <b>two</b> fuzzy <b>sets</b> is 0 if, and only if the <b>two</b> <b>sets</b> coincide. We also found that the decision principle of <b>cross-entropy</b> is consistent with decision principle of <b>distance</b> measure in decision-making. That decision principle is, among all the choices, we finally chose the one with the smallest <b>cross-entropy</b> (<b>distance</b>) from the ideal ...", "dateLastCrawled": "2021-12-14T01:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross entropy</b> of mass function and its application in similarity ...", "url": "https://link.springer.com/article/10.1007/s10489-021-02890-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10489-021-02890-6", "snippet": "Hence, setting the relationship <b>between</b> <b>cross entropy</b> and similarity is an interesting issue, which not only can expand the application of <b>cross entropy</b> but also can better analyze <b>information</b>. Moreover, uncertainty is the foundation <b>of information</b> theory. Hence, the paper proposed new similarity measure of mass functions by considering <b>cross entropy</b> and uncertainty. Uncertainty can be computed by belief entropy proposed by Deng. The proposed similarity measure is a novel viewpoint and an ...", "dateLastCrawled": "2021-12-05T20:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. <b>Cross-entropy</b> is commonly used in machine learning as a loss function. <b>Cross-entropy</b> is a measure from the field <b>of information</b> theory, building upon entropy and generally calculating the difference <b>between</b> <b>two</b> probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy <b>between</b> <b>two</b> probability distributions, whereas <b>cross-entropy</b> can be thought to calculate the total entropy <b>between</b> the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>distance</b> measures and <b>cross-entropy</b> based on complex fuzzy <b>sets</b> and ...", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs191718", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs191718", "snippet": "The <b>cross entropy</b> <b>between</b> CFSs is used to discriminate uncertain <b>information</b> expressed by CFSs, we can define <b>cross entropy</b> measure based on the DIMs with respect to these factors determining complex fuzzy uncertainty. It has been pointed that uncertainty of CFSs contained <b>two</b> dimensional <b>information</b> in a single set. Generally, the entropy measure merely quantifies the fuzziness, without consideration of the distribution of fuzziness. In other words, the entropy is determined by the ...", "dateLastCrawled": "2022-02-01T15:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "machine learning - What is <b>cross-entropy</b>? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/41990250/what-is-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/41990250", "snippet": "Here&#39;s a <b>similar</b> question on the datascience sister site: ... Correct, <b>cross-entropy</b> describes the loss <b>between</b> <b>two</b> probability distributions. It is one of many possible loss functions. Then we can use, for example, gradient descent algorithm to find the minimum. Yes, the <b>cross-entropy</b> loss function can be used as part of gradient descent. Further reading: one of my other answers related to TensorFlow. Share. Follow edited May 16 &#39;21 at 4:28. answered Feb 1 &#39;17 at 22:21 ...", "dateLastCrawled": "2022-01-28T16:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>novel cross-entropy and entropy</b> measures of IFSs and their ...", "url": "https://www.sciencedirect.com/science/article/pii/S0950705113001238", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705113001238", "snippet": "In order to measure the discrimination uncertain <b>information</b>, <b>cross-entropy</b> and symmetric <b>cross-entropy</b> are defined based on intuitionistic factor and fuzzy factor. In particular, the constructive principle of entropy is refined; relationship <b>between</b> <b>cross-entropy</b> and entropy is investigated, so we establish a novel intuitionistic fuzzy entropy formula that simultaneously takes into account intuitionistic entropy and fuzzy entropy. We also propose concepts of marginal rate of substitution ...", "dateLastCrawled": "2021-11-29T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Cross-entropy</b> and Maximum Likelihood Estimation | by Roan Gylberth ...", "url": "https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/konvergen/<b>cross-entropy</b>-and-maximum-likelihood-estimation-58942b52517a", "snippet": "<b>Cross-entropy</b> and Maximum Likelihood Estimation. So, we are on our way to train our first neural network model for classification. We design our network depth, the activation function, set all the ...", "dateLastCrawled": "2022-01-29T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "In binary classification, is it okay to use <b>cross entropy</b> loss instead ...", "url": "https://machinelearning1.quora.com/In-binary-classification-is-it-okay-to-use-cross-entropy-loss-instead-of-binary-cross-entropy-loss-I-am-using-pre-defi", "isFamilyFriendly": true, "displayUrl": "https://machinelearning1.quora.com/In-binary-classification-is-it-okay-to-use-cross...", "snippet": "<b>Cross Entropy</b> is definitely a good loss function for Classification Problems, because it minimizes the <b>distance</b> <b>between</b> <b>two</b> probability distributions - predicted and actual.. Conceptually, you can understand it like this - Consider a classifier which predicts whether the given animal is dog, cat or horse with a probability associated with each.", "dateLastCrawled": "2022-01-19T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is the <b>cross-entropy</b> cost function better than mean squared error cost ...", "url": "https://www.quora.com/Is-the-cross-entropy-cost-function-better-than-mean-squared-error-cost-function-Why", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-the-<b>cross-entropy</b>-cost-function-better-than-mean-squared...", "snippet": "Answer: TL;DR: I do not think you can put a strict preferance on either without specifying the intended use. Some background: * For a \u201csoft-max\u201d output, the cross ...", "dateLastCrawled": "2022-01-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "2.3 Measures of Distributional Similarity", "url": "https://www.cs.cornell.edu/courses/cs6742/2017fa/handouts/lee-ch2.3.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs6742/2017fa/handouts/lee-ch2.3.pdf", "snippet": "being compared aremaximally <b>similar</b> (i.e., identical). The work described in chapters 4 and 5 uses negative exponentials of <b>distance</b> functions when true similarity functions (that is, functions that increase as similarity increases) are required. We certainly do not intend to give an exhaustive listing of all <b>distance</b> functions. (See Anderberg (1973) for an extensive survey.) Our purpose is simply to examine important properties of functions that we use or that are commonly employed by other ...", "dateLastCrawled": "2021-12-03T11:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. <b>Cross-entropy</b> is commonly used in machine learning as a loss function. <b>Cross-entropy</b> is a measure from the field <b>of information</b> theory, building upon entropy and generally calculating the difference <b>between</b> <b>two</b> probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy <b>between</b> <b>two</b> probability distributions, whereas <b>cross-entropy</b> <b>can</b> <b>be thought</b> to calculate the total entropy <b>between</b> the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A data assimilation framework that uses the Kullback-Leibler divergence", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8389478/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8389478", "snippet": "Sometimes referred to as the <b>cross-entropy</b> <b>distance</b> or relative entropy, the Kullback-Leibler divergence <b>can</b> <b>be thought</b> of as measuring the discrepancy <b>between</b> probability distributions, in this case the divergence of P from Q.From a Bayesian perspective it is a measure <b>of information</b> gained when an a priori probability distribution, Q, is updated to the posterior probability distribution, P.The Kullback-Leibler divergence is not strictly a true metric, and is not symmetric, hence in general ...", "dateLastCrawled": "2022-01-06T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Cross-entropy</b> and Maximum Likelihood Estimation | by Roan Gylberth ...", "url": "https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/konvergen/<b>cross-entropy</b>-and-maximum-likelihood-estimation-58942b52517a", "snippet": "Equation 10 shows the relation of <b>cross entropy</b> and maximum likelihood estimation principle, that is if we take p_example(x) as p(x) and p_model(x;\ud835\udf03) as q(x), we <b>can</b> write equation 10 as", "dateLastCrawled": "2022-01-29T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>cross entropy</b> - Quantifying overlap <b>between</b> <b>two</b> categorical ...", "url": "https://stats.stackexchange.com/questions/512621/quantifying-overlap-between-two-categorical-distributions-with-some-non-identica", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/512621/quantifying-overlap-<b>between</b>-<b>two</b>...", "snippet": "My first <b>thought</b> was Bhattacharyya <b>distance</b> (which I&#39;m only vaguely familiar with), but that seems to be related to continuous quantitative distributions. Otherwise, what I <b>can</b> find for qualitative distributions through random Googling seem to all require that the <b>two</b> distributions have the same exact categories (e.g., Y should have the categories ...", "dateLastCrawled": "2022-01-14T07:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Hesitant fuzzy entropy and <b>cross-entropy</b> and their use in ...", "url": "https://www.researchgate.net/publication/262364213_Hesitant_fuzzy_entropy_and_cross-entropy_and_their_use_in_multiattribute_decision-making", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/262364213_Hesitant_fuzzy_entropy_and_cross...", "snippet": "Considering that <b>cross-entropy</b> <b>can</b> be used to measure discrimination <b>between</b> <b>information</b> [51], we use the above new intuitionistic hesitant fuzzy <b>cross-entropy</b> as the <b>distance</b> <b>between</b> x and <b>two</b> ...", "dateLastCrawled": "2022-01-13T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Divergence-based <b>cross entropy</b> and uncertainty measures of Atanassov\u2019s ...", "url": "https://www.researchgate.net/publication/335217472_Divergence-based_cross_entropy_and_uncertainty_measures_of_Atanassov's_intuitionistic_fuzzy_sets_with_their_application_in_decision_making", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335217472_Divergence-based_<b>cross_entropy</b>_and...", "snippet": "<b>Distance</b> or divergence measure is a dominant way to describe the degree of deviation <b>between</b> <b>two</b> <b>sets</b> and also plays a remarkable role in the <b>information</b> process.", "dateLastCrawled": "2021-11-17T16:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Hyperspherical Alternatives to Softmax | by Stephan Tulkens | Towards ...", "url": "https://towardsdatascience.com/hyperspherical-alternatives-to-softmax-6da03388fe3d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/hyperspherical-alternatives-to-softmax-6da03388fe3d", "snippet": "Softmax classifiers are typically trained by minimizing the <b>cross entropy</b> <b>between</b> the predictions of a network and the targets. This <b>can</b> be understood as attempting to maximize the magnitude of the correct output in relation to the incorrect output units. As a softmax classifier has |C| output units, the output layer of a softmax network grows by a factor of H for each class, where H is the number of hidden units in the last layer. Hyperspherical Prototype networks. In contrast ...", "dateLastCrawled": "2022-01-14T23:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "linear algebra - <b>Distance</b> <b>Between</b> Sparse Matrices - Mathematics Stack ...", "url": "https://math.stackexchange.com/questions/4192487/distance-between-sparse-matrices", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/4192487/<b>distance</b>-<b>between</b>-sparse-matrices", "snippet": "My goal is to measure the <b>distance</b> <b>between</b> those <b>two</b> matrices in the most accurate way with the use of a <b>distance</b> measure (probabilistic, or whatever else measure): I was wondering if there exists a <b>distance</b> (I do not mind having all the <b>distance</b> properties) that is suitable for such comparisons, of sparse matrices with elements only $0$ and $1$ .", "dateLastCrawled": "2022-01-26T17:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Loss function <b>autoencoder</b> vs variational-<b>autoencoder</b> or MSE-loss vs ...", "url": "https://stats.stackexchange.com/questions/350211/loss-function-autoencoder-vs-variational-autoencoder-or-mse-loss-vs-binary-cross", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/350211", "snippet": "<b>Cross-entropy</b> loss is assymetrical.. If your true intensity is high, e.g. 0.8, generating a pixel with the intensity of 0.9 is penalized more than generating a pixel with intensity of 0.7.. Conversely if it&#39;s low, e.g. 0.3, predicting an intensity of 0.4 is penalized less than a predicted intensity of 0.2.. You might have guessed by now - <b>cross-entropy</b> loss is biased towards 0.5 whenever the ground truth is not binary. For a ground truth of 0.5, the per-pixel zero-normalized loss is equal to ...", "dateLastCrawled": "2022-02-01T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Loss Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms...", "snippet": "Embedding loss functions: It deals with problems where we have to measure whether <b>two</b> inputs are similar or dissimilar. Some examples are: 1. L1 Hinge Error- Calculates the L1 <b>distance</b> <b>between</b> <b>two</b> ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The <b>cross-entropy</b> and improved <b>distance</b> measures for complex q-rung ...", "url": "https://link.springer.com/article/10.1007/s40747-021-00551-2", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40747-021-00551-2", "snippet": "The <b>cross-entropy</b> measures of Cq-ROHFSs. Entropy measure (EM), as one of the most useful and proficient techniques, is important to examine the relation <b>between</b> <b>two</b> objects. Zadeh presented the EM based on FS. Maassen and Uffink improved the EM to explore the cross EM (CEM) as the starting point in the <b>information</b> theory.", "dateLastCrawled": "2022-01-01T00:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>Distance Model of Intuitionistic Fuzzy Cross Entropy</b> to Solve ...", "url": "https://www.hindawi.com/journals/mpe/2016/8324124/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2016/8324124", "snippet": "As <b>can</b> be seen from property (2), when the <b>two</b> intuitionistic fuzzy <b>sets</b> are exactly equal, the intuitionistic fuzzy <b>cross entropy</b> is the least; therefore, the <b>cross entropy</b> <b>can</b> be used to measure the difference <b>between</b> <b>two</b> intuitionistic fuzzy <b>sets</b>. The intuitionistic fuzzy <b>cross entropy</b> adds the meaning of the <b>information</b> entropy on the basis of the original intuitionistic fuzzy complete <b>information</b>. It <b>can</b> be used to measure the fuzzy degree and the uncertainty degree of the ...", "dateLastCrawled": "2022-01-04T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The <b>distance</b> measures and <b>cross-entropy</b> based on complex fuzzy <b>sets</b> and ...", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs191718", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs191718", "snippet": "The <b>cross entropy</b> <b>between</b> CFSs is used to discriminate uncertain <b>information</b> expressed by CFSs, we <b>can</b> define <b>cross entropy</b> measure based on the DIMs with respect to these factors determining complex fuzzy uncertainty. It has been pointed that uncertainty of CFSs contained <b>two</b> dimensional <b>information</b> in a single set. Generally, the entropy measure merely quantifies the fuzziness, without consideration of the distribution of fuzziness. In other words, the entropy is determined by the ...", "dateLastCrawled": "2022-02-01T15:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "The <b>cross-entropy</b> <b>between</b> <b>two</b> probability distributions, such as Q from P, <b>can</b> be stated formally as: H(P, Q) ... Recall that the KL divergence is the extra bits required to transmit one variable <b>compared</b> to another. It is the <b>cross-entropy</b> without the entropy of the class label, which we know would be zero anyway. As such, minimizing the KL divergence and the <b>cross entropy</b> for a classification task are identical. Minimizing this KL divergence corresponds exactly to minimizing the cross ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Distance</b> Model of Intuitionistic Fuzzy <b>Cross Entropy</b> to Solve ...", "url": "https://www.thefreelibrary.com/A+Distance+Model+of+Intuitionistic+Fuzzy+Cross+Entropy+to+Solve...-a0524388659", "isFamilyFriendly": true, "displayUrl": "https://www.thefreelibrary.com/A+<b>Distance</b>+Model+of+Intuitionistic+Fuzzy+<b>Cross+Entropy</b>...", "snippet": "As <b>can</b> be seen from property (2), when the <b>two</b> intuitionistic fuzzy <b>sets</b> are exactly equal, the intuitionistic fuzzy <b>cross entropy</b> is the least; therefore, the <b>cross entropy</b> <b>can</b> be used to measure the difference <b>between</b> <b>two</b> intuitionistic fuzzy <b>sets</b>. The intuitionistic fuzzy <b>cross entropy</b> adds the meaning of the <b>information</b> entropy on the basis of the original intuitionistic fuzzy complete <b>information</b>. It <b>can</b> be used to measure the fuzzy degree and the uncertainty degree of the ...", "dateLastCrawled": "2021-04-21T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Divergence-based <b>cross entropy and uncertainty measures</b> of Atanassov\u2019s ...", "url": "https://www.sciencedirect.com/science/article/pii/S1568494619304843", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1568494619304843", "snippet": "We hold the perspective that the <b>distance</b> <b>between</b> <b>two</b> AIFSs and the difference of their uncertainty are <b>two</b> different concepts. In other words, <b>two</b> AIFSs with equal uncertainty degree may be quite different from the viewpoint of <b>distance</b> measure. Uncertainty measure of AIFSs should be defined based on the difference <b>between</b> the <b>information</b> conveyed by an AIFS and the <b>information</b> conveyed the most certain one or the most uncertainty one. Thus, we <b>can</b> study uncertainty measures of AIFSs from ...", "dateLastCrawled": "2021-10-26T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Divergence-based <b>cross entropy</b> and uncertainty measures of Atanassov\u2019s ...", "url": "https://www.researchgate.net/publication/335217472_Divergence-based_cross_entropy_and_uncertainty_measures_of_Atanassov's_intuitionistic_fuzzy_sets_with_their_application_in_decision_making", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335217472_Divergence-based_<b>cross_entropy</b>_and...", "snippet": "<b>Distance</b> or divergence measure is a dominant way to describe the degree of deviation <b>between</b> <b>two</b> <b>sets</b> and also plays a remarkable role in the <b>information</b> process.", "dateLastCrawled": "2021-11-17T16:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Relative Complexity Metric for Decision-theoretic Applications in ...", "url": "https://wpmedia.wolfram.com/uploads/sites/13/2018/02/12-3-2.pdf", "isFamilyFriendly": true, "displayUrl": "https://wpmedia.wolfram.com/uploads/sites/13/2018/02/12-3-2.pdf", "snippet": "present approach is directed at measuring the <b>cross-entropy</b> <b>between</b> <b>two</b> systems (or subsystems). This <b>cross-entropy</b> measure depicts the \u201c<b>distance</b>\u201d <b>between</b> or \u201cdivergence\u201d of statistical pro\ufb01les of the systems being <b>compared</b>. Hence, it is shown (using the approach due to [2]) that the <b>cross-entropy</b> measure <b>can</b> serve as a metric of relative com-plexity. Consider a complex system speci\ufb01ed by a domain X as illustrated in Figure 1. Suppose <b>two</b> constituent (interacting) subsystems (i,j ...", "dateLastCrawled": "2021-11-22T13:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Set <b>Cross Entropy: Likelihood-based Permutation Invariant Loss</b> Function ...", "url": "https://deepai.org/publication/set-cross-entropy-likelihood-based-permutation-invariant-loss-function-for-probability-distributions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/set-<b>cross-entropy</b>-likelihood-based-permutation...", "snippet": "SCE measures the <b>cross entropy</b> <b>between</b> <b>two</b> <b>sets</b> that consists of multiple elements, where each element is represented as a multi-dimensional probability distribution in [0, 1] \u2282 R (a closed set of reals <b>between</b> 0,1). SCE is invariant to the object permutation, therefore does not distinguish <b>two</b> vector representations of a set with the different ordering. The SCE is simple enough to fit in one line and <b>can</b> be naturally interpreted as a formulation of the log-likelihood maximization <b>between</b> ...", "dateLastCrawled": "2021-11-28T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "2.3 Measures of Distributional Similarity", "url": "https://www.cs.cornell.edu/courses/cs6742/2017fa/handouts/lee-ch2.3.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs6742/2017fa/handouts/lee-ch2.3.pdf", "snippet": "the \u201csimilarity\u201d <b>between</b> distributions. We refer to these functions as <b>distance</b> functions, rather than similarity functions, since most of them achieve their minimum when the <b>two</b> distributions being <b>compared</b> aremaximally similar (i.e., identical). The work described in chapters 4 and 5 uses", "dateLastCrawled": "2021-12-03T11:21:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - <b>Cross-entropy loss</b> explanation - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/20296", "snippet": "The answer from Neil is correct. However I think its important to point out that while the loss does not depend on the distribution between the incorrect classes (only the distribution between the correct class and the rest), the gradient of this loss function does effect the incorrect classes differently depending on how wrong they are. So when you use cross-ent in <b>machine</b> <b>learning</b> you will change weights differently for [0.1 0.5 0.1 0.1 0.2] and [0.1 0.6 0.1 0.1 0.1].", "dateLastCrawled": "2022-01-27T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross-Entropy</b> Demystified. What is it? Is there any relation to\u2026 | by ...", "url": "https://naokishibuya.medium.com/demystifying-cross-entropy-e80e3ad54a8", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/demystifying-<b>cross-entropy</b>-e80e3ad54a8", "snippet": "However, the <b>machine</b> <b>learning</b> application uses the base e logarithm for implementation convenience. Binary <b>Cross-Entropy</b>. We can use the binary <b>cross-entropy</b> for binary classification where we have yes/no answer. For example, there are only dogs or cats in images. For the binary classifications, the <b>cross-entropy</b> formula contains only two ...", "dateLastCrawled": "2022-01-26T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why and How to use <b>Cross Entropy</b>. The fundamental reasons for ...", "url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-and-how-to-use-<b>cross-entropy</b>-4e983cbdd873", "snippet": "The fundamental reasons for minimizing binary <b>cross entropy</b> (log loss) with probabilistic classification models . Will Arliss. Sep 26, 2020 \u00b7 7 min read. Introduction. This post discusses why logistic regression necessarily uses a different loss function than linear regression. First, the simple yet inefficient way to solve logistic regression will be presented, then the slightly less simple but much more efficient way will be explained and compared. The simple way. Linear regression is the ...", "dateLastCrawled": "2022-01-31T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Shannon <b>entropy</b> in the context of <b>machine</b> <b>learning</b> and AI | by Frank ...", "url": "https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/shannon-<b>entropy</b>-in-the-context-of-<b>machine</b>-<b>learning</b>-and-ai-24...", "snippet": "Closely related to <b>cross entropy</b>, the KL divergence from q to p, written DKL(p||q), is another similarity measure often used in <b>machine</b> <b>learning</b>. In the language of Bayesian Inference, DKL(p||q ...", "dateLastCrawled": "2022-01-30T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "ML Concepts. A.I \u2014 Intelligence showed by machines which is common for humans <b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>.We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Main concepts behind Machine Learning</b> | by Bruno Eidi Nishimoto ...", "url": "https://medium.com/neuronio/main-concepts-behind-machine-learning-22cd81d68a11", "isFamilyFriendly": true, "displayUrl": "https://medium.com/neuronio/<b>main-concepts-behind-machine-learning</b>-22cd81d68a11", "snippet": "<b>Machine</b> <b>Learning</b> is a concept that is currently trending. It is a subarea from Artificial Intelligence and it consists on the fact that the <b>machine</b> can learn by itself without being explicitly ...", "dateLastCrawled": "2022-01-19T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Gentle Introduction to Information Entropy - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-is-information-entropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/what-is-information-entropy", "snippet": "Calculating information and entropy is a useful tool in <b>machine</b> <b>learning</b> and is used as the basis for techniques such as feature selection, building decision trees, and, more generally, fitting classification models. As such, a <b>machine</b> <b>learning</b> practitioner requires a strong understanding and intuition for information and entropy. In this post, you will discover a gentle introduction to information entropy. After reading this post, you will know: Information theory is concerned with data ...", "dateLastCrawled": "2022-02-02T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep Learning and Information Theory</b> \u2013 Deep &amp; Shallow", "url": "https://deep-and-shallow.com/2020/01/09/deep-learning-and-information-theory/", "isFamilyFriendly": true, "displayUrl": "https://deep-and-shallow.com/2020/01/09/<b>deep-learning-and-information-theory</b>", "snippet": "If you have tried to understand the maths behind <b>machine</b> <b>learning</b>, including deep <b>learning</b>, you would have come across topics from Information Theory \u2013 Entropy, <b>Cross Entropy</b>, KL Divergence, etc. The concepts from information theory is ever prevalent in the realm of <b>machine</b> <b>learning</b>, right from the splitting criteria of a Decision Tree to loss functions in Generative Adversarial Networks.", "dateLastCrawled": "2022-02-01T00:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[D] A Short Introduction to Entropy, <b>Cross-Entropy</b> and KL-Divergence ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7vhmp7/d_a_short_introduction_to_entropy_crossentropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7vhmp7/d_a_short_introduction_to...", "snippet": "I am having trouble reconciling the concept with the <b>analogy</b>. At 2:35 even if a rainy day was 25% likely, there&#39;s still only two states, rainy and sunny, and therefor only 1 bit of information is needed to convey that, so only one bit of data needs to be sent, even though the 1 bit of data reduces the uncertainty of a rainy day by a factor of 4. I quite don&#39;t get what he means by this being 2 bits of information. I guess where I am stuck is how the uncertainty reduction factor translates to ...", "dateLastCrawled": "2021-08-20T08:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Lecture 4 Fundamentals of deep <b>learning</b> and neural networks", "url": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "snippet": "Deep <b>learning</b>: <b>Machine</b> <b>learning</b> models based on \u201cdeep\u201d neural networks comprising millions (sometimes billions) of parameters organized into hierarchical layers. Features are multiplied and added together repeatedly, with the outputs from one layer of parameters being fed into the next layer -- before a prediction is made. Contrast with linear regression: Agenda for today - More on the structure of neural network models - <b>Machine</b> <b>learning</b> training loop and concept of loss, in the context ...", "dateLastCrawled": "2022-02-02T09:07:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Beat the Bookmakers With Tree-Based <b>Machine</b> <b>Learning</b> Algorithms | by ...", "url": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-machine-learning-algorithms-1d349335b54", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-<b>machine</b>...", "snippet": "<b>Cross-entropy is similar</b> to Gini Impurity, but it involves using the concept of entropy from information theory. This article won\u2019t go in depth about it, but essentially, as the cross-entropy ...", "dateLastCrawled": "2022-01-26T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>A Traveler\u2019s Diary on the Road to Machine</b> <b>Learning</b> - Chapter 1 | by ...", "url": "https://medium.com/swlh/a-travelers-diary-on-the-road-to-machine-learning-chapter-1-8850ec5b4243", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>a-travelers-diary-on-the-road-to-machine</b>-<b>learning</b>-chapter-1...", "snippet": "Types of <b>Machine</b> <b>Learning</b> algorithms: ... Sparse categorical <b>cross entropy is similar</b> to categorical cross entropy, only difference is it uses only one value as target. It saves memory as well as ...", "dateLastCrawled": "2021-05-21T04:27:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Deep Learning for Computer Architects</b> | Chen Jeff - Academia.edu", "url": "https://www.academia.edu/40860009/Deep_Learning_for_Computer_Architects", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40860009/<b>Deep_Learning_for_Computer_Architects</b>", "snippet": "This text serves as a primer for computer architects in a new and rapidly evolving \ufb01eld. We review how <b>machine</b> <b>learning</b> has evolved since its inception in the 1960s and track the key developments leading up to the emergence of the powerful deep <b>learning</b> techniques that emerged in the last decade.", "dateLastCrawled": "2022-01-28T02:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(cross-entropy)  is like +(distance between two sets of information)", "+(cross-entropy) is similar to +(distance between two sets of information)", "+(cross-entropy) can be thought of as +(distance between two sets of information)", "+(cross-entropy) can be compared to +(distance between two sets of information)", "machine learning +(cross-entropy AND analogy)", "machine learning +(\"cross-entropy is like\")", "machine learning +(\"cross-entropy is similar\")", "machine learning +(\"just as cross-entropy\")", "machine learning +(\"cross-entropy can be thought of as\")", "machine learning +(\"cross-entropy can be compared to\")"]}