{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Algorithmic bias: on the implicit biases of social technology</b> ...", "url": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "snippet": "<b>Computer</b> scientists call this algorithmic <b>bias</b>. This paper explores the relationship between machine <b>bias</b> and human cognitive <b>bias</b>. In it, I argue similarities between algorithmic and cognitive biases indicate a disconcerting sense in which sources of <b>bias</b> emerge out of seemingly innocuous patterns of information processing. The emergent nature of this <b>bias</b> obscures the existence of the <b>bias</b> itself, making it difficult to identify, mitigate, or evaluate using standard resources in ...", "dateLastCrawled": "2022-01-25T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Battling Implicit Bias</b> | Think magazine | CWRU", "url": "https://case.edu/think/spring2021/battling-implicit-bias.html", "isFamilyFriendly": true, "displayUrl": "https://<b>case.edu</b>/think/spring2021/<b>battling-implicit-bias</b>.html", "snippet": "Consider this: Health systems relying on a common <b>computer</b> <b>algorithm</b> to identify patients needing extra care failed to identify many Black members who qualified. The reason, according to a 2019 study: An <b>algorithm</b> that used health costs as a proxy for health needs. But many of the Black patients who were considerably sicker than white patients had unequal access to care, and as a result, they weren\u2019t identified because less money was spent on them.", "dateLastCrawled": "2021-12-14T22:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What Is <b>Bias</b> In <b>Computer</b> - SeniorCare2Share", "url": "https://www.seniorcare2share.com/what-is-bias-in-computer/", "isFamilyFriendly": true, "displayUrl": "https://www.seniorcare2share.com/what-is-<b>bias</b>-in-<b>computer</b>", "snippet": "<b>Bias</b> is just <b>like</b> an intercept added in a linear equation. It is an additional parameter in the Neural Network which is used to adjust the output along with the weighted sum of the inputs to the neuron. Moreover, <b>bias</b> value allows you to shift the activation function to either right or left.", "dateLastCrawled": "2022-01-21T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Algorithms and <b>bias</b>, explained - <b>Vox</b>", "url": "https://www.vox.com/recode/2020/2/18/21121286/algorithms-bias-discrimination-facial-recognition-transparency", "isFamilyFriendly": true, "displayUrl": "https://www.<b>vox</b>.com/recode/2020/2/18/21121286", "snippet": "For instance, in a very simplified example, let\u2019s say you wanted to train your <b>computer</b> system to recognize whether an object is a book, based on a few factors, <b>like</b> its texture, weight, and ...", "dateLastCrawled": "2022-01-30T21:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Algorithm</b> <b>Bias</b>. Abstract: This article discusses the\u2026 | by Jae Makitalo ...", "url": "https://medium.com/qmind-ai/algorithm-bias-3933a0153841", "isFamilyFriendly": true, "displayUrl": "https://medium.com/qmind-ai/<b>algorithm</b>-<b>bias</b>-3933a0153841", "snippet": "Algorithms trained on historical data may stereotype. This phenomenon is called <b>implicit</b> or \u201clatent\u201d <b>bias</b>. Remember when Amazon built a <b>computer</b> program to review job applicants? This ...", "dateLastCrawled": "2021-09-08T01:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mitigating <b>implicit bias in machine learning</b>", "url": "https://faraday.ai/blog/implicit-bias-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://faraday.ai/blog/<b>implicit</b>-<b>bias</b>-machine-learning", "snippet": "Reporting <b>bias</b> (at the source) \u2014 Maybe the folks who filled out your online survey were mostly young, male, and <b>computer</b>-literate. <b>Implicit</b> <b>bias</b> (the call is coming from inside the building) \u2014 Perhaps your image of an ideal job candidate fits into a mental archetype of which you&#39;re only semi-aware, and that image excludes those who don&#39;t match the archetype.", "dateLastCrawled": "2021-12-24T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "<b>Bias</b> in humans can be unconscious (also called <b>implicit</b> <b>bias</b>), which indicates humans can introduce <b>bias</b> without even knowing they are doing so. Let\u2019s explore how we can detect <b>bias</b> in <b>machine learning</b> models and how it can be eliminated. Types of <b>bias</b>. <b>Bias</b> in <b>machine learning</b> data sets and models is such a problem that you\u2019ll find tools from many of the leaders in <b>machine learning</b> development. Detecting <b>bias</b> starts with the data set. A data set might not represent the problem space ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Implicitly Biased <b>Implicit</b> <b>Bias</b> Test", "url": "https://paultaylor.substack.com/p/the-implicitly-biased-implicit-bias", "isFamilyFriendly": true, "displayUrl": "https://paultaylor.substack.com/p/the-<b>implicit</b>ly-<b>bias</b>ed-<b>implicit</b>-<b>bias</b>", "snippet": "<b>Implicit</b> <b>bias</b> has enjoyed blockbuster success because there is a simple test that anyone can take to measure one\u2019s own level of this affliction: the <b>implicit</b> association test, or IAT. If you\u2019ve been in a diversity training anytime in the last few years, it\u2019s likely you\u2019ve come across this tool, which is promoted by Harvard University and a veritable army of well-credentialed social psychologists who study <b>bias</b> and discrimination. You can go to Harvard\u2019s Project <b>Implicit</b> website at ", "dateLastCrawled": "2022-01-29T01:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>How biased algorithms perpetuate inequality</b>", "url": "https://www.newstatesman.com/science-tech/2020/04/how-biased-algorithms-perpetuate-inequality", "isFamilyFriendly": true, "displayUrl": "https://www.newstatesman.com/science-tech/2020/04/how-<b>bias</b>ed-<b>algorithms</b>-perpetuate...", "snippet": "This <b>algorithm</b> and others <b>like</b> it have been used in the care of hundreds of million patients a year in the US. What is going on here? ... Ultimatley, this <b>bias</b> is due to the <b>algorithm</b> selecting and linking features in problematic ways, and the features these algorithms link can be as surprising as having played football in college. There are feature-linking biases where the stakes are even higher. One is the Northpointe recidivism risk software COMPAS, which is used in courtrooms across the ...", "dateLastCrawled": "2022-01-28T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The Dangers of Human-<b>Like</b> <b>Bias</b> in Machine-Learning Algorithms", "url": "https://scholarsmine.mst.edu/cgi/viewcontent.cgi?article=1030&context=peer2peer", "isFamilyFriendly": true, "displayUrl": "https://scholarsmine.mst.edu/cgi/viewcontent.cgi?article=1030&amp;context=peer2peer", "snippet": "optimizing the <b>algorithm</b>&#39;s performance (Mooney 1996); however, learned biases can cause greater harm when the data set involves actual humans. Learned biases formed on human-related data frequently resemble . human-<b>like</b>. biases towards race, sex, religion, and many other common forms of discrimination. This discrimination and the question of the fairness of artificial intelligence have received increasing public attention thanks to the numerous social media-based AIs launched in recent years ...", "dateLastCrawled": "2021-12-22T20:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Algorithmic bias: on the implicit biases of social technology</b> ...", "url": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "snippet": "<b>Computer</b> scientists call this algorithmic <b>bias</b>. This paper explores the relationship between machine <b>bias</b> and human cognitive <b>bias</b>. In it, I argue similarities between algorithmic and cognitive biases indicate a disconcerting sense in which sources of <b>bias</b> emerge out of seemingly innocuous patterns of information processing. The emergent nature of this <b>bias</b> obscures the existence of the <b>bias</b> itself, making it difficult to identify, mitigate, or evaluate using standard resources in ...", "dateLastCrawled": "2022-01-25T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Algorithmic <b>Bias</b> - University of Pittsburgh", "url": "http://philsci-archive.pitt.edu/17169/1/Algorithmic%20Bias.pdf", "isFamilyFriendly": true, "displayUrl": "philsci-archive.pitt.edu/17169/1/<b>Algorithm</b>ic <b>Bias</b>.pdf", "snippet": "<b>Computer</b> scien-tists call this algorithmic <b>bias</b>. This paper explores the relationship between machine <b>bias</b> and human cognitive <b>bias</b>. In it, I argue similarities between algorithmic and cognitive biases indicate a disconcerting sense in which sources of <b>bias</b> emerge out of seemingly innocuous patterns of information processing. The emergent nature of this <b>bias</b> obscures the existence of the <b>bias</b> itself, making it di cult to identify, mitigate, or evaluate using standard resources in ...", "dateLastCrawled": "2022-01-29T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The Implicitly Biased <b>Implicit</b> <b>Bias</b> Test", "url": "https://paultaylor.substack.com/p/the-implicitly-biased-implicit-bias", "isFamilyFriendly": true, "displayUrl": "https://paultaylor.substack.com/p/the-<b>implicit</b>ly-<b>bias</b>ed-<b>implicit</b>-<b>bias</b>", "snippet": "As you peck away at the keyboard, the <b>computer</b> measures your reaction times, which it plugs into an <b>algorithm</b>. That <b>algorithm</b>, in turn, generates your score. If you were quicker to associate good words with white faces than good words with black faces, and/or slower to associate bad words with white faces than bad words with black ones, then the test will report that you have a slight, moderate, or strong \u201cpreference for white faces over black faces,\u201d or some <b>similar</b> language. But there ...", "dateLastCrawled": "2022-01-29T01:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Potential Biases in Machine Learning Algorithms Using Electronic Health ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6347576/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6347576", "snippet": "Practitioners can have <b>bias</b> in their diagnostic or therapeutic decision makingthat might be circumvented if a <b>computer</b> <b>algorithm</b> could objectively synthesize and interpret the data in the medical record and offer clinical decision support toaid or guide diagnosis and treatment. Although all statistical models existalonga continuum offully human-guided vs fully machine-guided data analyses, 1 machine learningalgorithms in general tend to rely less on human specification (ie, defininga set of ...", "dateLastCrawled": "2022-01-27T11:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "<b>Bias</b> in humans can be unconscious (also called <b>implicit</b> <b>bias</b>), which indicates humans can introduce <b>bias</b> without even knowing they are doing so. Let\u2019s explore how we can detect <b>bias</b> in <b>machine learning</b> models and how it can be eliminated. Types of <b>bias</b>. <b>Bias</b> in <b>machine learning</b> data sets and models is such a problem that you\u2019ll find tools from many of the leaders in <b>machine learning</b> development. Detecting <b>bias</b> starts with the data set. A data set might not represent the problem space ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>How biased algorithms perpetuate inequality</b>", "url": "https://www.newstatesman.com/science-tech/2020/04/how-biased-algorithms-perpetuate-inequality", "isFamilyFriendly": true, "displayUrl": "https://www.newstatesman.com/science-tech/2020/04/how-<b>bias</b>ed-<b>algorithms</b>-perpetuate...", "snippet": "The <b>algorithm</b> then filters for applicants that are <b>similar</b> to past employees who satisfy those two criteria. ... Ultimatley, this <b>bias</b> is due to the <b>algorithm</b> selecting and linking features in problematic ways, and the features these algorithms link can be as surprising as having played football in college. There are feature-linking biases where the stakes are even higher. One is the Northpointe recidivism risk software COMPAS, which is used in courtrooms across the US. The software ...", "dateLastCrawled": "2022-01-28T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Algorithmic <b>bias</b> detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/<b>algorithm</b>ic-<b>bias</b>-detection-and-mitigation-best-", "snippet": "When detecting <b>bias</b>, <b>computer</b> programmers normally examine the set of outputs that the <b>algorithm</b> produces to check for anomalous results. Comparing outcomes for different groups can be a useful ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bias</b> In <b>Personalization Algorithms</b> - Thank you, Eve!", "url": "https://www.thankyoueve.com/personalization-algorithm-bias", "isFamilyFriendly": true, "displayUrl": "https://www.thankyoueve.com/personalization-<b>algorithm</b>-<b>bias</b>", "snippet": "With this view one would have to argue that most or all <b>bias</b> resulting from a <b>computer</b> system is technical, as long as there is no complete copy of &quot;real&quot; life into an <b>algorithm</b>, there will always be just technical <b>bias</b>, all <b>bias</b> resulting from the system would be ascribable to some technological shortcoming. The problems in the search <b>algorithm</b> of Search Company Inc. would then be only part of technological <b>bias</b>, the individual biases would only be introduced into this system because of ...", "dateLastCrawled": "2021-12-22T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The Dangers of Human-Like <b>Bias</b> in Machine-Learning Algorithms", "url": "https://scholarsmine.mst.edu/cgi/viewcontent.cgi?article=1030&context=peer2peer", "isFamilyFriendly": true, "displayUrl": "https://scholarsmine.mst.edu/cgi/viewcontent.cgi?article=1030&amp;context=peer2peer", "snippet": "In technical problems, <b>bias</b> may only raise concerns over efficiency and optimizing the <b>algorithm</b>&#39;s performance (Mooney 1996); however, learned biases can cause greater harm when the data set involves actual humans. Learned biases formed on human-related data frequently resemble . human-like. biases towards race, sex, religion, and many other common forms of discrimination. This discrimination and the question of the fairness of artificial intelligence have received increasing public ...", "dateLastCrawled": "2021-12-22T20:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Biased Algorithms Are Easier to Fix</b> Than Biased People - <b>The New York Times</b>", "url": "https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.nytimes.com</b>/2019/12/06/business/<b>algorithm</b>-<b>bias</b>-fix.html", "snippet": "At <b>similar</b> levels of sickness, black patients were deemed to be at lower risk than white patients. The magnitude of the distortion was immense: Eliminating the algorithmic <b>bias</b> would more than ...", "dateLastCrawled": "2022-01-30T14:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Can an Algorithm be Biased</b>? - Catherine Stinson", "url": "http://www.catherinestinson.ca/Files/Papers/Algorithm_bias.pdf", "isFamilyFriendly": true, "displayUrl": "www.catherinestinson.ca/Files/Papers/<b>Algorithm</b>_<b>bias</b>.pdf", "snippet": "\u201c<b>Bias</b> in <b>Computer</b> Systems\u201d distinguishes value neutral uses of \u2018<b>bias</b>\u2019 from \u201c<b>bias</b> of moral import\u201d or unfair discrimination, which they de\ufb01ne as follows: \u201cA system discriminates unfairly if it denies an opportunity or a good or if it assigns an undesirable outcome to an individual or group of individuals on grounds that are unreasonable or inappropriate\u201d 1One exception is the phrase \u2018algorithmic <b>bias</b>\u2019 which refers broadly to <b>bias</b> in computational systems. (Friedman and ...", "dateLastCrawled": "2022-01-27T14:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The Implicitly Biased <b>Implicit</b> <b>Bias</b> Test", "url": "https://paultaylor.substack.com/p/the-implicitly-biased-implicit-bias", "isFamilyFriendly": true, "displayUrl": "https://paultaylor.substack.com/p/the-<b>implicit</b>ly-<b>bias</b>ed-<b>implicit</b>-<b>bias</b>", "snippet": "<b>Implicit</b> <b>bias</b> has enjoyed blockbuster success because there is a simple test that anyone <b>can</b> take to measure one\u2019s own level of this affliction: the <b>implicit</b> association test, or IAT. If you\u2019ve been in a diversity training anytime in the last few years, it\u2019s likely you\u2019ve come across this tool, which is promoted by Harvard University and a veritable army of well-credentialed social psychologists who study <b>bias</b> and discrimination. You <b>can</b> go to Harvard\u2019s Project <b>Implicit</b> website at ", "dateLastCrawled": "2022-01-29T01:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>How biased algorithms perpetuate inequality</b>", "url": "https://www.newstatesman.com/science-tech/2020/04/how-biased-algorithms-perpetuate-inequality", "isFamilyFriendly": true, "displayUrl": "https://www.newstatesman.com/science-tech/2020/04/how-<b>bias</b>ed-<b>algorithms</b>-perpetuate...", "snippet": "Ultimatley, this <b>bias</b> is due to the <b>algorithm</b> selecting and linking features in problematic ways, and the features these algorithms link <b>can</b> be as surprising as having played football in college. There are feature-linking biases where the stakes are even higher. One is the Northpointe recidivism risk software COMPAS, which is used in courtrooms across the US. The software calculates a score predicting the likelihood of someone committing a crime in the future and judges use these recidivism ...", "dateLastCrawled": "2022-01-28T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Algorithmic <b>Bias</b> in Machine Learning - Duke Forge", "url": "https://forge.duke.edu/sites/default/files/atoms/files/Algorithmic%20Bias%20in%20Machine%20Learning_0.pdf", "isFamilyFriendly": true, "displayUrl": "https://forge.duke.edu/sites/default/files/atoms/files/<b>Algorithm</b>ic <b>Bias</b> in Machine...", "snippet": "seem a <b>computer</b> program would not exhibit <b>bias</b>, it is increasingly clear that algorithms often incorporate the conscious and unconscious biases of their creators or the data on which they are trained. This introduces the possibility that by using them, algorithms will cause clinicians to care for subpopulations of patients inequitably. With funding from the Moore Foundation, Duke Forge hosted a conference of experts to discuss algorithmic <b>bias</b> and its implications in healthcare and ...", "dateLastCrawled": "2022-01-19T02:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Addressing Fairness, <b>Bias</b>, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "<b>Bias</b> <b>can</b> also be addressed at the data collection stage by adjusting the sampling process ... We <b>can</b> ask whether a <b>computer</b> <b>algorithm</b> disproportionately helps or harms specific individuals or specific groups of people. Ideally, an <b>algorithm</b> would be customized to an individual, and fairness criteria could be satisfied by ensuring that the <b>algorithm</b> provides similar treatment to individuals that share similar characteristics. Mathematically, if it were possible to describe an individual by a ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Implicit Bias</b> (<b>Stanford Encyclopedia of Philosophy</b>)", "url": "https://plato.stanford.edu/entries/implicit-bias/", "isFamilyFriendly": true, "displayUrl": "https://<b>plato.stanford.edu</b>/entries/<b>implicit-bias</b>", "snippet": "Research on \u201c<b>implicit bias</b>\u201d suggests that people <b>can</b> act on the basis of prejudice and stereotypes without intending to do so. While psychologists in the field of \u201c<b>implicit</b> social cognition\u201d study consumer products, self-esteem, food, alcohol, political values, and more, the most striking and well-known research has focused on <b>implicit</b> biases toward members of socially stigmatized groups, such as African-Americans, women, and the LGBTQ community. [] For example, imagine Frank, who ...", "dateLastCrawled": "2022-01-30T04:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Coded <b>Bias</b>: How algorithmic biases affect all of us \u2013 ScienceBorealis ...", "url": "https://blog.scienceborealis.ca/coded-bias-how-algorithmic-biases-affect-all-of-us/", "isFamilyFriendly": true, "displayUrl": "https://blog.scienceborealis.ca/coded-<b>bias</b>-how-<b>algorithm</b>ic-<b>bias</b>es-affect-all-of-us", "snippet": "In simple terms, an <b>algorithm</b> is a procedure or set of instructions used to perform a computational task or solve a problem. One method of programming a <b>computer</b> is to give it instructions on what to do. Alternatively, you could supply it with large amounts of data and let the machine figure out how to classify the data by processing the available information. This kind of <b>computer</b> \u2018learning\u2019 method works well, but programmers need to proceed carefully. Zeynep Tufeki, a social technology ...", "dateLastCrawled": "2022-02-01T00:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>An Empirical Study on Algorithmic Bias</b>", "url": "https://www.researchgate.net/publication/341411703_An_Empirical_Study_on_Algorithmic_Bias", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/341411703_<b>An_Empirical_Study_on_Algorithmic_Bias</b>", "snippet": "T o demonstrate the <b>implicit</b> <b>bias</b> that exist in any ... <b>algorithm</b> <b>can</b> be biased as programmer intends and by this . process some entity or object <b>can</b> get unfair advantages by. giving incentive to ...", "dateLastCrawled": "2022-01-19T07:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Algorithmic <b>bias</b> detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/<b>algorithm</b>ic-<b>bias</b>-detection-and-mitigation-best-", "snippet": "When detecting <b>bias</b>, <b>computer</b> programmers normally examine the set of outputs that the <b>algorithm</b> produces to check for anomalous results. Comparing outcomes for different groups <b>can</b> be a useful ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Biased Algorithms Are Easier to Fix</b> Than Biased People - <b>The New York Times</b>", "url": "https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.nytimes.com</b>/2019/12/06/business/<b>algorithm</b>-<b>bias</b>-fix.html", "snippet": "For an <b>algorithm</b>, we <b>can</b> create equally controlled just by feeding it the right data and observing its behavior. Algorithms and humans also differ on what <b>can</b> be done about <b>bias</b> once it is found.", "dateLastCrawled": "2022-01-30T14:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Algorithmic bias: on the implicit biases of social technology</b> ...", "url": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "snippet": "These examples demonstrate that machine <b>bias</b> exists and that the patterns of such <b>bias</b> mimic well-known <b>implicit</b> <b>bias</b> patterns in humans. However, we need not from the existence of these biases infer that programmers are writing explicitly racist or sexist code. Instead, it is possible that such biases emerge out of the operation of seemingly innocuous code paired with statistical regularities of the training data. These cases of algorithmic <b>bias</b> <b>can</b> be demonstrated with a toy model using a ...", "dateLastCrawled": "2022-01-25T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "You <b>can</b> also use an online interactive demonstration over three data sets (including the COMPAS recidivism data set) that allows you to explore <b>bias</b> metrics, then apply a <b>bias</b> mitigation <b>algorithm</b> and view the results as <b>compared</b> to the original model. The toolkit is designed to be open to permit researchers to add their own fairness metrics and migration algorithms.", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Potential Biases in Machine Learning Algorithms Using Electronic Health ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6347576/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6347576", "snippet": "Practitioners <b>can</b> have <b>bias</b> in their diagnostic or therapeutic decision makingthat might be circumvented if a <b>computer</b> <b>algorithm</b> could objectively synthesize and interpret the data in the medical record and offer clinical decision support toaid or guide diagnosis and treatment. Although all statistical models existalonga continuum offully human-guided vs fully machine-guided data analyses, 1 machine learningalgorithms in general tend to rely less on human specification (ie, defininga set of ...", "dateLastCrawled": "2022-01-27T11:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bias</b> and Discrimination in AI: A Cross-Disciplinary Perspective - IEEE ...", "url": "https://technologyandsociety.org/bias-and-discrimination-in-ai-a-cross-disciplinary-perspective/", "isFamilyFriendly": true, "displayUrl": "https://technologyandsociety.org/<b>bias</b>-and-discrimination-in-ai-a-cross-disciplinary...", "snippet": "We <b>can</b> distinguish between two general approaches to measure <b>bias</b>: 1) procedural approaches, which focus on identifying biases in the decision-making the process of an <b>algorithm</b> [6] and 2) relational approaches, which focus on identifying (and preventing) biased decisions in the data set or algorithmic output. Although ensuring unbiased outcomes is useful to attest whether a specific <b>algorithm</b> has a discriminatory impact on a population, focusing on the algorithmic process itself <b>can</b> help ...", "dateLastCrawled": "2022-01-30T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Artificial intelligence and algorithmic <b>bias</b>: implications for health ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6875681/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6875681", "snippet": "In AI, the choice of data, <b>algorithm</b>, performance measures and analysis of algorithmic outputs to optimize performance and minimize <b>bias</b> requires considerable judgment. Where possible, data science teams should be as diverse as the populations that the AI algorithms they develop will affect. Specifically, the interests, skills, and life experiences of underrepresented minority populations are relevant in identifying potential sources of <b>bias</b>, as diverse teams will be more intimately familiar ...", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>How Algorithms Reduce Bias in Decision Making</b> - The Apex", "url": "https://www.apexofinnovation.com/how-algorithms-reduce-bias-in-decision-making/", "isFamilyFriendly": true, "displayUrl": "https://www.apexofinnovation.com/<b>how-algorithms-reduce-bias-in-decision-making</b>", "snippet": "Examples of algorithmic <b>bias</b>\u2014when a <b>computer</b> system reflects the <b>implicit</b> values of humans\u2014<b>can</b> pop up at virtually any company carrying out data analytics initiatives. While unintentional and at times inconsequential, the ramifications of algorithmic <b>bias</b> are very real for the victims of it. Rejected loan applications, gender discrimination on the job, and racial inequality are just a few examples.", "dateLastCrawled": "2022-02-01T02:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Algorithmic <b>bias</b> detection and mitigation: Best practices and policies ...", "url": "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.brookings.edu</b>/research/<b>algorithm</b>ic-<b>bias</b>-detection-and-mitigation-best-", "snippet": "When detecting <b>bias</b>, <b>computer</b> programmers normally examine the set of outputs that the <b>algorithm</b> produces to check for anomalous results. Comparing outcomes for different groups <b>can</b> be a useful ...", "dateLastCrawled": "2022-02-03T04:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bias</b> In <b>Personalization Algorithms</b> - Thank you, Eve!", "url": "https://www.thankyoueve.com/personalization-algorithm-bias", "isFamilyFriendly": true, "displayUrl": "https://www.thankyoueve.com/personalization-<b>algorithm</b>-<b>bias</b>", "snippet": "Even if one would argue that the resulting <b>bias</b> was introduced by a skewed dataset, my answer would be that for one, a skewed dataset is not preexisting <b>bias</b> (it is not individual <b>bias</b>, it is system <b>bias</b>) and that a skewed dataset will do nothing of harm in a properly working <b>algorithm</b> 31. Only a skewed dataset that is combined with a lack of transparency and improper (or missing) feedback loops <b>can</b> produce biased results, otherwise the results would be caught beforehand. Only if the ...", "dateLastCrawled": "2021-12-22T19:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The AI of <b>the beholder? Algorithm can now learn</b> which faces you find ...", "url": "https://www.rt.com/news/517496-ai-predict-attraction-brain-waves/", "isFamilyFriendly": true, "displayUrl": "https://www.rt.com/news/517496-ai-predict-attraction-brain-waves", "snippet": "\u201cPotentially, we might gear the device towards identifying stereotypes or <b>implicit</b> <b>bias</b> and better understand individual differences.\u201d Also on rt.com Free will hacked: AI <b>can</b> be trained to manipulate human behavior and decisions, according to research in Australia", "dateLastCrawled": "2022-02-02T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Biased Algorithms Are Easier to Fix</b> Than Biased People - <b>The New York Times</b>", "url": "https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.nytimes.com</b>/2019/12/06/business/<b>algorithm</b>-<b>bias</b>-fix.html", "snippet": "But <b>compared</b> with the intransigence of human <b>bias</b>, it does look a great deal simpler. Discrimination by <b>algorithm</b> <b>can</b> be more readily discovered and more easily fixed.", "dateLastCrawled": "2022-01-30T14:13:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine learning</b> and <b>bias</b> \u2013 IBM Developer", "url": "https://developer.ibm.com/articles/machine-learning-and-bias/", "isFamilyFriendly": true, "displayUrl": "https://developer.ibm.com/articles/<b>machine-learning</b>-and-<b>bias</b>", "snippet": "<b>Bias</b> in humans can be unconscious (also called <b>implicit</b> <b>bias</b>), which indicates humans can introduce <b>bias</b> without even knowing they are doing so. Let\u2019s explore how we can detect <b>bias</b> in <b>machine learning</b> models and how it can be eliminated. Types of <b>bias</b>. <b>Bias</b> in <b>machine learning</b> data sets and models is such a problem that you\u2019ll find tools ...", "dateLastCrawled": "2022-02-02T20:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Implicit/Machine learning gender bias</b> | ReberLab", "url": "https://www.reberlab.psych.northwestern.edu/2018/10/11/implicitmachine-learning-gender-bias/", "isFamilyFriendly": true, "displayUrl": "https://www.reberlab.psych.northwestern.edu/.../10/11/<b>implicitmachine-learning-gender-bias</b>", "snippet": "First, I have found myself describing on a few recent occasions that it is reasonable to think of <b>implicit</b> <b>learning</b> (IL) as the brain\u2019s <b>machine</b> <b>learning</b> (ML) algorithm. ML is a super-hot topic in AI and data science research, so this might be a useful <b>analogy</b> to help people understand what we mean by studying IL. We characterize IL as the statistical extraction of patterns in the environment and the shaping of cognitive processing to maximize efficiency and effectiveness to these patterns ...", "dateLastCrawled": "2020-11-16T16:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Algorithmic bias: on the implicit biases of social technology</b> ...", "url": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11229-020-02696-y", "snippet": "Often <b>machine</b> <b>learning</b> programs inherit social patterns reflected in their training data without any directed effort by programmers to include such biases. Computer scientists call this algorithmic <b>bias</b>. This paper explores the relationship between <b>machine</b> <b>bias</b> and human cognitive <b>bias</b>. In it, I argue similarities between algorithmic and cognitive biases indicate a disconcerting sense in which sources of <b>bias</b> emerge out of seemingly innocuous patterns of information processing. The emergent ...", "dateLastCrawled": "2022-01-25T14:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning</b> | Gendered Innovations", "url": "http://genderedinnovations.stanford.edu/case-studies/machinelearning.html", "isFamilyFriendly": true, "displayUrl": "genderedinnovations.stanford.edu/case-studies/<b>machinelearning</b>.html", "snippet": "As <b>machine learning</b> becomes increasingly ubiquitous in everyday lives, such <b>bias</b>, if uncorrected, can lead to social inequities. Researchers need to understand how gender and ethnicity operate within the context of their algorithm in order to enhance or, at least, not reinforce social equalities. Here we suggest avenues for reducing <b>bias</b> in training data and algorithms in efforts to produce AI that enhances social equalities.", "dateLastCrawled": "2022-01-25T22:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Harnessing data for unbiased <b>Learning</b> of Machines | by Vivek Gupta | Medium", "url": "https://gvivek.medium.com/harnessing-data-for-unbiased-learning-of-machines-dc614497a29c", "isFamilyFriendly": true, "displayUrl": "https://gvivek.medium.com/harnessing-data-for-un<b>bias</b>ed-<b>learning</b>-of-<b>machines</b>-dc614497a29c", "snippet": "In <b>Machine</b> <b>Learning</b>, the input data reflects stereotypes and biases of the broader society, then the output of the <b>learning</b> algorithm also captures these stereotypes. Here, we will discuss the gender stereotypes in word embedding. word-embedding encode semantic information, they also exhibit hidden biases inherent in the dataset they are trained on associations such as: father:doctor :: mother:nurse. man:computer programmer :: woman:homemaker. The prejudices and stereotypes in these ...", "dateLastCrawled": "2022-01-12T17:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Fairness Metrics - Data Analytics - Data Science, <b>Machine</b> <b>Learning</b>, AI", "url": "https://vitalflux.com/fairness-metrics-ml-model-sensitivity-bias-detection/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/<b>fairness-metrics-ml-model-sensitivity</b>-<b>bias</b>-detection", "snippet": "There are many different ways in which <b>machine</b> <b>learning</b> ... Instinctively (based on <b>implicit</b> human <b>bias</b>), it is one of the specific \u201cgender\u201d (male or female) against which the model could get biased as like in the real world in predicting whether those with a specific gender could re-offend or not. Thus, one of the protected attributes becomes gender. For the current example, in the real world, one could get biased in favor of female and not (or fail to) classify them as the ones who ...", "dateLastCrawled": "2022-01-20T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Introduction to Natural Language Processing</b> (NLP) and <b>Bias</b> in AI | by ...", "url": "https://towardsdatascience.com/introduction-to-natural-language-processing-nlp-and-bias-in-ai-877d3f3ee680", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>introduction-to-natural-language-processing</b>-nlp-and...", "snippet": "In this post, I will introduce key concepts of NLP such as word embeddings, and we will see how an algorithm can become biased, and how we can remove that <b>bias</b>. Le t \u2019s get started! For hands-on video tutorials on <b>machine</b> <b>learning</b>, deep <b>learning</b>, and artificial intelligence, checkout my YouTube channel.", "dateLastCrawled": "2022-01-20T19:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning is Requirements Engineering</b> \u2014 On the Role of Bugs ...", "url": "https://medium.com/analytics-vidhya/machine-learning-is-requirements-engineering-8957aee55ef4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>machine-learning-is-requirements-engineering</b>-8957...", "snippet": "A <b>machine</b>-learned model is not an attempt at implementing an <b>implicit</b> specification; a <b>machine</b>-learned model is a specification! It is a learned description of how the system shall behave. The ...", "dateLastCrawled": "2022-01-19T17:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word ...", "url": "https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf", "snippet": "signi\ufb01cant risk and challenge for <b>machine</b> <b>learning</b> and its applications. The analogies generated from these embeddings spell out the <b>bias</b> <b>implicit</b> in the data on which they were trained. Hence, word embeddings may serve as a means to extract <b>implicit</b> gender associations from a large text corpus", "dateLastCrawled": "2022-02-02T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Man is to Computer Programmer as Woman is</b> to Homemaker? | by Sheldon ...", "url": "https://towardsdatascience.com/man-is-to-computer-programmer-as-woman-is-to-homemaker-e57b07cbde96", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>man-is-to-computer-programmer-as-woman-is</b>-to-homemaker...", "snippet": "The paper discusses gender <b>bias</b> in <b>machine</b> <b>learning</b> as a result of using biased training data and proposes a solution to debias the model. This article contains an overview of the paper and discusses key findings in the python implementation of the paper. Overview. The authors of the paper used Word Embe d ding Model to demonstrate gender <b>bias</b> in the training data. The Word Embedding Model was trained on Google News articles and the vector representation of all words were stored in the ...", "dateLastCrawled": "2022-01-30T22:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CSWA: <b>Unconscious Bias</b> | American Astronomical Society", "url": "https://aas.org/comms/cswa/resources/unconsciousbias", "isFamilyFriendly": true, "displayUrl": "https://aas.org/comms/cswa/resources/<b>unconsciousbias</b>", "snippet": "The intervention is based on the premise that <b>implicit bias is like</b> a habit that can be broken through a combination of awareness of implicit bias, concern about the effects of that bias, and the application of strategies to reduce bias. In a 12-week longitudinal study, people who received the intervention showed dramatic reductions in implicit race bias. People who were concerned about discrimination or who reported using the strategies showed the greatest reductions. The intervention also ...", "dateLastCrawled": "2022-02-03T07:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Patrick Forscher</b> \u2013 Research Lead \u2013 Busara Center for Behavioral ...", "url": "https://ch.linkedin.com/in/patrick-forscher-91163854", "isFamilyFriendly": true, "displayUrl": "https://ch.linkedin.com/in/<b>patrick-forscher</b>-91163854", "snippet": "The intervention is based on the premise that <b>implicit bias is like</b> a habit that can be broken through a combination of awareness of implicit bias, concern about the effects of that bias, and the application of strategies to reduce bias. In a 12-week longitudinal study, people who received the intervention showed dramatic reductions in implicit race bias. People who were\u2026 We developed a multi-faceted prejudice habit-breaking intervention to produce long-term reductions in implicit race ...", "dateLastCrawled": "2022-02-03T15:10:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Those designing healthcare algorithms must become actively</b> anti-racist ...", "url": "https://www.nature.com/articles/s41591-020-1020-3", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41591-020-1020-3", "snippet": "<b>Just as \u2018implicit bias</b>\u2019 training for police does little to change racist behavior 10 \u2014in large part because departmental cultures do not fully support the lessons of anti-racism\u2014healthcare ...", "dateLastCrawled": "2021-11-15T00:12:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(implicit bias)  is like +(computer algorithm)", "+(implicit bias) is similar to +(computer algorithm)", "+(implicit bias) can be thought of as +(computer algorithm)", "+(implicit bias) can be compared to +(computer algorithm)", "machine learning +(implicit bias AND analogy)", "machine learning +(\"implicit bias is like\")", "machine learning +(\"implicit bias is similar\")", "machine learning +(\"just as implicit bias\")", "machine learning +(\"implicit bias can be thought of as\")", "machine learning +(\"implicit bias can be compared to\")"]}