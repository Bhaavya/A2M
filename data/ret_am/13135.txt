{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Practical Guide To <b>Hyperparameter Optimization</b>.", "url": "https://nanonets.com/blog/hyperparameter-optimization/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/<b>hyperparameter-optimization</b>", "snippet": "<b>Bayesian</b> <b>Optimization</b>. Unlike the other methods we\u2019ve seen so far, <b>Bayesian</b> <b>optimization</b> uses knowledge of previous iterations of the algorithm. With grid search and random search, each hyperparameter guess is independent. But with <b>Bayesian</b> methods, each time we select and try out different hyperparameters, the inches toward perfection ...", "dateLastCrawled": "2022-01-29T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Guide to Bayesian Optimization Using BoTorch</b>", "url": "https://analyticsindiamag.com/guide-to-bayesian-optimization-using-botorch/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>guide-to-bayesian-optimization-using-botorch</b>", "snippet": "<b>Bayesian</b> <b>Optimization</b> works by building a surrogate function.Surrogate function is a smooth probabilistic model of the original objective function.This surrogate functions must provide predictions for the value of function at parameters we haven\u2019t yet tested. They should also quantify the amount of uncertainty at these parameters. Now these predictions and uncertainties from the surrogate model are used to generate an acquisition function.This acquisition function is used to select the ...", "dateLastCrawled": "2022-01-29T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Conceptual Explanation of <b>Bayesian</b> Hyperparameter <b>Optimization</b> for ...", "url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-conceptual-explanation-of-<b>bayesian</b>-model-based-hyper...", "snippet": "SMBO is a formalization of <b>Bayesian</b> <b>optimization</b> which is more efficient at finding the best hyperparameters for a machine learning model than random or grid search. Sequential model-based <b>optimization</b> methods differ in they build the surrogate, but they all rely on information from previous trials to propose better hyperparameters for the next evaluation. The Tree Parzen Estimator is one algorithm that uses <b>Bayesian</b> reasoning to construct the surrogate model and can select the next ...", "dateLastCrawled": "2022-02-02T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bayesian</b> <b>Optimization</b> is Superior to Random Search for Machine Learning ...", "url": "http://proceedings.mlr.press/v133/turner21a/turner21a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v133/turner21a/turner21a.pdf", "snippet": "<b>optimization</b> (e.g., <b>Bayesian</b> <b>optimization</b>) is relevant for hyperparameter tuning in almost every machine learning project as well as many applications outside of machine learning. The nal leaderboard was determined using the <b>optimization</b> performance on held-out (hid- den) objective functions, where the optimizers ran without human intervention. Baselines were set using the default settings of several open source black-box <b>optimization</b> packages as well as random search. Keywords: Black-box ...", "dateLastCrawled": "2022-01-25T13:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Bayesian</b> Optimisation over Multiple Continuous and Categorical Inputs", "url": "http://proceedings.mlr.press/v119/ru20a/ru20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/ru20a/ru20a.pdf", "snippet": "<b>Bayesian</b> Optimisation over Multiple Continuous and Categorical Inputs Binxin Ru 1Ahsan S. Alvi Vu Nguyen Michael A. Osborne 1Stephen J Roberts Abstract Ef\ufb01cient optimisation of black-box problems that comprise both continuous and categorical in-puts is important, yet poses signi\ufb01cant chal-lenges. Current approaches, <b>like</b> one-hot encod-ing, severely increase the dimension of the search space, while separate modelling of category-speci\ufb01c data is sample-inef\ufb01cient. Both frame-works are ...", "dateLastCrawled": "2022-01-31T17:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bayesian Brain Hypothesis</b>. Continuing on to another fascinating\u2026 | by ...", "url": "https://amit02093.medium.com/bayesian-brain-hypothesis-3554540132d0", "isFamilyFriendly": true, "displayUrl": "https://amit02093.medium.com/<b>bayesian-brain-hypothesis</b>-3554540132d0", "snippet": "It <b>is like</b> blind folding you and giving you a neural shock analogous to the stimulant which generate image of <b>a cat</b> in your brain and then I ask you, what do you think that object is. So while going working on my exploration of the <b>Bayesian Brain Hypothesis</b>, I came across this gem of a statement which I never thought of considering and contemplating before ever.", "dateLastCrawled": "2022-01-12T14:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bayesian</b> Convolutional Neural Network | Chan`s Jupyter", "url": "https://goodboychan.github.io/python/coursera/tensorflow_probability/icl/2021/08/26/01-Bayesian-Convolutional-Neural-Network.html", "isFamilyFriendly": true, "displayUrl": "https://goodboychan.github.io/.../2021/08/26/01-<b>Bayesian</b>-Convolutional-Neural-Network.html", "snippet": "The reason for using such a prior is that it <b>is like</b> a standard unit normal, but makes values far away from 0 more likely, allowing the model to explore a larger weight space. Run the code below to create a &quot;spike and slab&quot; distribution and plot its probability density function, compared with a standard unit normal. def spike_and_slab (event_shape, dtype): distribution = tfd. Mixture (<b>cat</b> = tfd. Categorical (probs = [0.5, 0.5]), components = [tfd. Independent (tfd. Normal (loc = tf. zeros ...", "dateLastCrawled": "2022-01-30T09:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Hyperparameters Optimization methods - ML - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/hyperparameters-optimization-methods-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>hyperparameters-optimization-methods-ml</b>", "snippet": "<b>Bayesian</b> <b>Optimization</b>: Instead of random guess, In <b>bayesian</b> <b>optimization</b> we use our previous knowledge to guess the hyper parameter. They use these results to form a probabilistic model mapping hyperparameters to a probability function of a score on the objective function. These probability function is defined below. This function is also called \u201csurrogate\u201d of objective function. It is much easier to optimize than Objective function. Below are the steps for applying <b>Bayesian</b> <b>Optimization</b> ...", "dateLastCrawled": "2022-01-25T19:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>skopt.BayesSearchCV</b> \u2014 scikit-optimize 0.8.1 documentation", "url": "https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html", "isFamilyFriendly": true, "displayUrl": "https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html", "snippet": "<b>Bayesian</b> <b>optimization</b> over hyper parameters. BayesSearchCV implements a \u201cfit\u201d and a \u201cscore\u201d method. It also implements \u201cpredict\u201d, \u201cpredict_proba\u201d, \u201cdecision_function\u201d, \u201ctransform\u201d and \u201cinverse_transform\u201d if they are implemented in the estimator used. The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings. In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed ...", "dateLastCrawled": "2022-02-02T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>to deal with categorical parameters</b> \u00b7 Issue #191 \u00b7 fmfn ...", "url": "https://github.com/fmfn/BayesianOptimization/issues/191", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/fmfn/<b>BayesianOptimization</b>/issues/191", "snippet": "The idea for categorical variables is to one-hot encode them, meaning if you have a variable with categories {&#39;A&#39;, &#39;B&#39;, &#39;C&#39;}, you create 3 helper variables each with values in the real interval [0,1] and set the highest one to 1 and the others to 0. If you think of it as a vector, the possible states (1,0,0), (0,1,0), (0,0,1) are all ...", "dateLastCrawled": "2021-10-17T16:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian</b> Optimisation over Multiple Continuous and Categorical Inputs", "url": "http://proceedings.mlr.press/v119/ru20a/ru20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/ru20a/ru20a.pdf", "snippet": "<b>Bayesian</b> Optimisation over Multiple Continuous and Categorical Inputs Binxin Ru 1Ahsan S. Alvi Vu Nguyen Michael A. Osborne 1Stephen J Roberts Abstract Ef\ufb01cient optimisation of black-box problems that comprise both continuous and categorical in-puts is important, yet poses signi\ufb01cant chal-lenges. Current approaches, like one-hot encod-ing, severely increase the dimension of the search space, while separate modelling of category-speci\ufb01c data is sample-inef\ufb01cient. Both frame-works are ...", "dateLastCrawled": "2022-01-31T17:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian</b> <b>Optimization</b> is Superior to Random Search for Machine Learning ...", "url": "http://proceedings.mlr.press/v133/turner21a/turner21a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v133/turner21a/turner21a.pdf", "snippet": "<b>optimization</b> (e.g., <b>Bayesian</b> <b>optimization</b>) is relevant for hyperparameter tuning in almost every machine learning project as well as many applications outside of machine learning. The nal leaderboard was determined using the <b>optimization</b> performance on held-out (hid- den) objective functions, where the optimizers ran without human intervention. Baselines were set using the default settings of several open source black-box <b>optimization</b> packages as well as random search. Keywords: Black-box ...", "dateLastCrawled": "2022-01-25T13:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Applying <b>Bayesian</b> <b>Optimization</b> for Machine Learning Models in ...", "url": "https://www.hindawi.com/journals/mpe/2021/6815802/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2021/6815802", "snippet": "This paper deals with the prediction of surface roughness in manufacturing polycarbonate (PC) by applying <b>Bayesian</b> <b>optimization</b> for machine learning models. The input variables of ultraprecision turning\u2014namely, feed rate, depth of cut, spindle speed, and vibration of the X- , Y-, and Z -axis\u2014are the main factors affecting surface quality. In this research, six machine learning- (ML-) based models\u2014artificial neural network (ANN), <b>Cat</b> Boost Regression (<b>CAT</b>), Support Vector Machine (SVR ...", "dateLastCrawled": "2022-01-29T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Comparative study of Hyper-Parameter <b>Optimization</b> Tools | DeepAI", "url": "https://deepai.org/publication/a-comparative-study-of-hyper-parameter-optimization-tools", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publi<b>cat</b>ion/a-comparative-study-of-hyper-parameter-<b>optimization</b>-tools", "snippet": "In this paper, we compare the hyper-parameter <b>optimization</b> techniques based on <b>Bayesian</b> <b>optimization</b> (Optuna [12], HyperOpt [10]) and SMAC [11a], and evolutionary or nature-inspired algorithms such as Optunity [15].As part of the experiment, we have done a CASH [3] benchmarking and the replication of NeurIPS black box <b>optimization</b> challenge of 2020 [ID]In the CASH problem, 12 different classifier models are chosen for solving large-scale machine learning problems, and to get the best ...", "dateLastCrawled": "2022-01-29T01:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An Example of Hyperparameter <b>Optimization</b> on XGBoost, LightGBM and ...", "url": "https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/an-example-of-hyperparameter-<b>optimization</b>-on-xgboost...", "snippet": "<b>Bayesian</b> <b>Optimization</b>. Compared with GridSearch which is a brute-force approach, or RandomSearch which is purely random, the classical <b>Bayesian</b> <b>Optimization</b> combines randomness and posterior probability distribution in searching the optimal parameters by approximating the target function through Gaussian Process (i.e. random samples are drawn iteratively (Sequential Model-Based <b>Optimization</b> (SMBO)) and the function outputs between the samples are approximated by a confidence region). New ...", "dateLastCrawled": "2022-01-31T20:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lipid Composition Design of Lipid Nanoparticles", "url": "https://meddocsonline.org/journal-of-nanomedicine/lipid-composition-design-of-lipid-nanoparticles-by-bayesian-optimization-for-high-efficiency-gene-delivery-to-peripheral-blood-mononuclear-cells.pdf", "isFamilyFriendly": true, "displayUrl": "https://meddocsonline.org/journal-of-nanomedicine/lipid-composition-design-of-lipid-na...", "snippet": "Lipid Nanoparticles (LNPs), which consist mainly of <b>cat</b>-ionic lipids, are considered a promising gene delivery tool for gene therapy with advantages such as ease of manufac-turing and reduced toxicity. Human Peripheral Blood Mono- nuclear Cells (hPBMCs) are an important raw material for gene therapy and there is a need to develop tools for ef-fectively delivering genes to them. LNPs are expected to be a suitable delivery tool for this purpose. LNPs are unique in that their membrane structure ...", "dateLastCrawled": "2022-01-27T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Hyperparameters Optimization methods - ML - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/hyperparameters-optimization-methods-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>hyperparameters-optimization-methods-ml</b>", "snippet": "BOHB (<b>Bayesian</b> <b>Optimization</b> and HyperBand) is a combination of the Hyperband algorithm and <b>Bayesian</b> <b>optimization</b>. First, it uses Hyperband capability to sample many configurations with a small budget to explore quickly and efficiently the hyper-parameter search space and get very soon promising configurations, then it uses the <b>Bayesian</b> optimizer predictive capability to propose set of hyperparameters that are close to optimum.This algorithm can also be run in parallel (as Hyperband) which ...", "dateLastCrawled": "2022-01-25T19:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bayesian</b> Convolutional Neural Network | Chan`s Jupyter", "url": "https://goodboychan.github.io/python/coursera/tensorflow_probability/icl/2021/08/26/01-Bayesian-Convolutional-Neural-Network.html", "isFamilyFriendly": true, "displayUrl": "https://goodboychan.github.io/.../2021/08/26/01-<b>Bayesian</b>-Convolutional-Neural-Network.html", "snippet": "The MNIST and MNIST-C datasets. In this notebook, you will use the MNIST and MNIST-C datasets, which both consist of a training set of 60,000 handwritten digits with corresponding labels, and a test set of 10,000 images. The images have been normalised and centred. The MNIST-C dataset is a corrupted version of the MNIST dataset, to test out-of-distribution robustness of computer vision models.", "dateLastCrawled": "2022-01-30T09:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How <b>to deal with categorical parameters</b> \u00b7 Issue #191 \u00b7 fmfn ...", "url": "https://github.com/fmfn/BayesianOptimization/issues/191", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/fmfn/<b>BayesianOptimization</b>/issues/191", "snippet": "Here, select_a and select_b are a one-hot encoded categorical variable selecting between two different output functions (for more than two values you would have to replace the simple if clause with something more sophisticated). The WrappedMatern constructor gets a tuple containing the type of each parameter (positionally): &#39;discrete&#39;, &#39;continuous&#39; or an identifier that is equal for all parameters that belong to the same categorical variable. In this case, to group select_a and select_b ...", "dateLastCrawled": "2021-10-17T16:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Bayesian Imputation</b> \u2014 NumPyro documentation", "url": "http://num.pyro.ai/en/stable/tutorials/bayesian_imputation.html", "isFamilyFriendly": true, "displayUrl": "num.pyro.ai/en/stable/tutorials/<b>bayesian_imputation</b>.html", "snippet": "&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ----- ----- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null ...", "dateLastCrawled": "2022-01-31T07:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian Brain Hypothesis</b>. Continuing on to another fascinating\u2026 | by ...", "url": "https://amit02093.medium.com/bayesian-brain-hypothesis-3554540132d0", "isFamilyFriendly": true, "displayUrl": "https://amit02093.medium.com/<b>bayesian-brain-hypothesis</b>-3554540132d0", "snippet": "This <b>optimization</b> <b>can</b> be finessed using a (variational free-energy) bound on surprise. In short, the free-energy principle entails the <b>Bayesian brain hypothesis</b> and <b>can</b> be implemented by the many schemes considered in this field. Almost invariably, these involve some form of message passing or belief propagation among brain areas or units.", "dateLastCrawled": "2022-01-12T14:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "python - <b>Bayesian optimization for a Light</b> GBM Model - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/56043436/bayesian-optimization-for-a-light-gbm-model", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/56043436", "snippet": "The RMSE (-1 x \u201ctarget\u201d) generated during <b>Bayesian</b> <b>optimization</b> should be better than that generated by the default values of LightGBM but I cannot achieve a better RMSE (looking for better/higher than -538.728 achieved through the above mentioned \u201cnormal\u201d early stopping process). The maxDepth and num_leaves should be integers; it looks ...", "dateLastCrawled": "2022-01-20T08:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Bayesian</b> Statistics | Statistical Modeling, Causal Inference, and ...", "url": "https://statmodeling.stat.columbia.edu/category/bayesian-statistics/page/3/", "isFamilyFriendly": true, "displayUrl": "https://statmodeling.stat.columbia.edu/<b>cat</b>egory/<b>bayesian</b>-statistics/page/3", "snippet": "A <b>Bayesian</b> procedure that does not include informative priors <b>can</b> <b>be thought</b> of as a frequentist procedure, the outputs of which become misleading when (as seems so common in practice) they are interpreted as posterior probability statements. Such interpretation is licensed only by uniform (\u201cnoninformative\u201d) priors, which at best leave the resulting posterior as an utterly hypothetical object that should be believed only when data information overwhelms all prior information. That ...", "dateLastCrawled": "2022-01-13T05:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Learning search <b>spaces for Bayesian optimization: Another</b> view of ...", "url": "https://proceedings.neurips.cc/paper/2019/file/6ea3f1874b188558fafbab78e8c3a968-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2019/file/6ea3f1874b188558fafbab78e8c3a968-Paper.pdf", "snippet": "<b>Bayesian</b> <b>optimization</b> (BO) is a successful methodology to optimize black-box functions that are expensive to evaluate. While traditional methods optimize each black-box function in isolation, there has been recent interest in speeding up BO by transferring knowledge across multiple related black-box functions. In this work, we introduce a method to automatically design the BO search space by relying on evaluations of previous black-box functions. We depart from the common practice of ...", "dateLastCrawled": "2022-01-26T05:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Practical Guide To <b>Hyperparameter Optimization</b>.", "url": "https://nanonets.com/blog/hyperparameter-optimization/", "isFamilyFriendly": true, "displayUrl": "https://nanonets.com/blog/<b>hyperparameter-optimization</b>", "snippet": "You\u2019ve built a <b>cat</b> and dog classifier. You tried your hand at a half-decent character-level RNN. You\u2019re just one pip ... the momentum constant <b>can</b> <b>be thought</b> of as the mass of a ball that\u2019s rolling down the surface of the loss function. The heavier the ball, the quicker it falls. But if it\u2019s too heavy, it <b>can</b> get stuck or overshoot the target. Source Dropout. If you\u2019re sensing a theme here, I\u2019m now going to direct you to Amar Budhiraja\u2019s article on dropout. Source. But as a ...", "dateLastCrawled": "2022-01-29T23:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mapping between stimuli and neural responses using the <b>Bayesian</b> ...", "url": "https://www.researchgate.net/figure/Mapping-between-stimuli-and-neural-responses-using-the-Bayesian-optimization-approach-a_fig1_291419479", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/Mapping-between-stimuli-and-neural-responses-using...", "snippet": "Similarly to previous work using <b>Bayesian</b> <b>optimization</b> for the navigation of pre-defined experimental spaces [37, 38], the method presented here <b>can</b> help improve the poor reproducibility present ...", "dateLastCrawled": "2022-02-03T10:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "9.1. Lecture 22 \u2014 Learning from data", "url": "https://furnstahl.github.io/Physics-8820/content/Machine_learning/lecture_22.html", "isFamilyFriendly": true, "displayUrl": "https://furnstahl.github.io/Physics-8820/content/Machine_learning/lecture_22.html", "snippet": "ML encompasses a broad array of techniques, many of which do not require a <b>Bayesian</b> approach, or they may even have a philosophy largely counter to the Bayes way of doing things. But there are clearly places where <b>Bayesian</b> methods are useful. We will tough upon two examples: <b>Bayesian</b> <b>Optimization</b>. <b>Bayesian</b> Neural Networks", "dateLastCrawled": "2022-02-03T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bayesian</b> networks for interpretable machine learning and <b>optimization</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221009644", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221009644", "snippet": "Explanation in <b>Bayesian</b> networks. The explanations of BNs <b>can</b> be focused on the model, on the reasoning process, or on the evidence . While explaining the model and the reasoning process aim to aid the user, explaining the evidence studies the use of BNs as a tool for explaining observed phenomena. Explaining the model means, in its most basic sense, displaying it to the user either graphically or verbally . When the network\u2019s graph is too large to fit onto a screen, software tools <b>can</b> ...", "dateLastCrawled": "2021-12-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Bayesian</b> \u2013 Machine Learning (Theory)", "url": "https://hunch.net/?cat=6", "isFamilyFriendly": true, "displayUrl": "https://hunch.net/?<b>cat</b>=6", "snippet": "In general, the religious <b>Bayesian</b> states that no good and only harm <b>can</b> come from randomized experiments. In principle, he is opposed even to random sampling in opinion polling. However, this principle puts him in untenable computational positions, and a pragmatic <b>Bayesian</b> will often ignore what seems useless design information if there are no obvious quirks in a randomly selected sample.", "dateLastCrawled": "2022-01-30T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Particle swarm based algorithms for finding locally</b> and <b>Bayesian</b> D ...", "url": "https://jsdajournal.springeropen.com/articles/10.1186/s40488-019-0092-4", "isFamilyFriendly": true, "displayUrl": "https://jsdajournal.springeropen.com/articles/10.1186/s40488-019-0092-4", "snippet": "When a model-based approach is appropriate, an optimal design <b>can</b> guide how to collect data judiciously for making reliable inference at minimal cost. However, finding optimal designs for a statistical model with several possibly interacting factors <b>can</b> be both theoretically and computationally challenging, and this issue is rarely discussed in the literature. We propose nature-inspired metaheuristic algorithms, like particle swarm <b>optimization</b> (PSO) and its variants, to solve such ...", "dateLastCrawled": "2022-01-07T00:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian</b> <b>optimization</b> approach for rapidly mapping residual network ...", "url": "https://academic.oup.com/brain/article/144/7/2120/6174117", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/brain/article/144/7/2120/6174117", "snippet": "Neuroadaptive <b>Bayesian</b> <b>optimization</b> <b>can</b> efficiently search a large task space ... (<b>CAT</b>) outside of the scanner 27 before the experiment began (Supplementary material). For the real-time functional MRI study, each participant underwent two, independent <b>optimization</b> runs for which the algorithm was reinitialized and thus blind to any data collected in the subject\u2019s previous run or any previous subjects, allowing us to assess the intra-subject reliability of results. Each run was initiated ...", "dateLastCrawled": "2021-12-27T05:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Applying <b>Bayesian</b> <b>Optimization</b> for Machine Learning Models in ...", "url": "https://www.hindawi.com/journals/mpe/2021/6815802/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/mpe/2021/6815802", "snippet": "<b>Bayesian</b> <b>optimization</b> <b>can</b> be considered a probability approach, using probability theories to optimize the hyperparameters . In this work, we have chosen <b>Bayesian</b> <b>optimization</b> because of its performance, which has been demonstrated in previous studies in the literature [63, 64]. For simplicity\u2019s sake, in the rest of the paper, we use XGB_opt ...", "dateLastCrawled": "2022-01-29T14:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "python - <b>Bayesian</b> Optimisation applied in CatBoost - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/52989242/bayesian-optimisation-applied-in-catboost", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/52989242", "snippet": "Any ideas of where I went wrong/How <b>can</b> I improve this? Salut, python python-3.x pandas <b>bayesian</b> catboost. Share. Follow asked Oct 25 &#39;18 at 12:25. prp prp. 834 1 1 gold badge 8 8 silver badges 23 23 bronze badges. 5. Did you happen to get your solution? also if you know, <b>can</b> you tell me where <b>can</b> we define list of categorical variable indices in your code? \u2013 poPYtheSailor. May 5 &#39;19 at 2:31. yeah @poPYtheSailor, lowering the max depth value as stated by LucaMassaron did the trick. \u2013 prp ...", "dateLastCrawled": "2022-01-21T05:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bayesian</b> <b>Optimization</b> with High-Dimensional Outputs", "url": "https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/271642041_401255041777110_5461928400890499388_n.pdf?_nc_cat=108&ccb=1-5&_nc_sid=ad8a9d&_nc_ohc=0AJ339FxyPAAX9QM1By&_nc_oc=AQlRe-6KiclmLrC8nnRM2Q-QykJBFvChhrmWEjCHj_4MSLor3xQbhqA2j3ngjjlBtK4&_nc_ht=scontent-sea1-1.xx&oh=00_AT_rhOBYMn63662WlnyK88zocS0_xaVLZkNHlnZxhiVjUA&oe=61F1D16B", "isFamilyFriendly": true, "displayUrl": "https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/271642041_401255041777110...", "snippet": "<b>Bayesian</b> <b>Optimization</b> is a sample-ef\ufb01cient black-box <b>optimization</b> procedure that is typically applied to problems with a small number of independent objectives. However, in practice we often wish to optimize objectives de\ufb01ned over many correlated outcomes (or \u201ctasks\u201d). For example, network operators may want to optimize the coverage of a cell tower network across a dense grid of locations. Similarly, engineers may seek to balance the performance of a robot across dozens of different ...", "dateLastCrawled": "2022-01-23T11:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Comparative study of Hyper-Parameter <b>Optimization</b> Tools | DeepAI", "url": "https://deepai.org/publication/a-comparative-study-of-hyper-parameter-optimization-tools", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publi<b>cat</b>ion/a-comparative-study-of-hyper-parameter-<b>optimization</b>-tools", "snippet": "<b>Compared</b> to <b>Bayesian</b> <b>optimization</b>, this method does not exploit the knowledge of well-performing search space [5] [6]. Iii-C <b>Bayesian</b> Hyper-parameter <b>Optimization</b>. In this section, we describe briefly the techniques involved in <b>Bayesian</b> hyper-parameter <b>optimization</b>. Iii-C1 Overview. The hyper-parameter <b>optimization</b> problem given in (1) involves a black-box function f whose analytical form is not known. We do not have the knowledge about its derivatives with respect to the hyper-parameters ...", "dateLastCrawled": "2022-01-29T01:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Conceptual Explanation of <b>Bayesian</b> Hyperparameter <b>Optimization</b> for ...", "url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-conceptual-explanation-of-<b>bayesian</b>-model-based-hyper...", "snippet": "At a high-level, <b>Bayesian</b> <b>optimization</b> methods are efficient because they choose the next hyperparameters in an informed manner. The basic idea is: spend a little more time selecting the next hyperparameters in order to make fewer calls to the objective function. In practice, the time spent selecting the next hyperparameters is inconsequential <b>compared</b> to the time spent in the objective function. By evaluating hyperparameters that appear more promising from past results, <b>Bayesian</b> methods <b>can</b> ...", "dateLastCrawled": "2022-02-02T22:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "python - <b>Bayesian optimization for a Light</b> GBM Model - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/56043436/bayesian-optimization-for-a-light-gbm-model", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/56043436", "snippet": "The RMSE (-1 x \u201ctarget\u201d) generated during <b>Bayesian</b> <b>optimization</b> should be better than that generated by the default values of LightGBM but I cannot achieve a better RMSE (looking for better/higher than -538.728 achieved through the above mentioned \u201cnormal\u201d early stopping process). The maxDepth and num_leaves should be integers; it looks ...", "dateLastCrawled": "2022-01-20T08:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) A Global <b>Bayesian</b> <b>Optimization</b> Algorithm and Its Application to ...", "url": "https://www.researchgate.net/publication/322412960_A_Global_Bayesian_Optimization_Algorithm_and_Its_Application_to_Integrated_System_Design", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publi<b>cat</b>ion/322412960_A_Global_<b>Bayesian</b>_<b>Optimization</b>...", "snippet": "Machine learning (ML) provides opportunities for analyzing such systems with multiple control parameters, where techniques based on <b>Bayesian</b> <b>optimization</b> (BO) <b>can</b> be used to meet or exceed design ...", "dateLastCrawled": "2022-01-27T05:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Bayesian</b> Convolutional Neural Network | Chan`s Jupyter", "url": "https://goodboychan.github.io/python/coursera/tensorflow_probability/icl/2021/08/26/01-Bayesian-Convolutional-Neural-Network.html", "isFamilyFriendly": true, "displayUrl": "https://goodboychan.github.io/.../2021/08/26/01-<b>Bayesian</b>-Convolutional-Neural-Network.html", "snippet": "The MNIST and MNIST-C datasets. In this notebook, you will use the MNIST and MNIST-C datasets, which both consist of a training set of 60,000 handwritten digits with corresponding labels, and a test set of 10,000 images. The images have been normalised and centred. The MNIST-C dataset is a corrupted version of the MNIST dataset, to test out-of-distribution robustness of computer vision models.", "dateLastCrawled": "2022-01-30T09:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Chapter 1 Hyperparameter Optimization</b> - AutoML", "url": "https://www.automl.org/wp-content/uploads/2019/05/AutoML_Book_Chapter1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.automl.org/wp-content/uploads/2019/05/AutoML_Book_Chapter1.pdf", "snippet": "since different methods <b>can</b> only <b>be compared</b> fairly if they all receive the same level of tuning for the problem at hand [14, 133]. The problem of HPO has a long history, dating back to the 1990s (e.g., [77, 82, 107, 126]), and it was also established early that different hyperparameter con\ufb01gurations tend to work best for different datasets [82]. In contrast, it is a rather new insight that HPO <b>can</b> be used to adapt general-purpose pipelines to speci\ufb01c application domains [30]. Nowadays ...", "dateLastCrawled": "2022-01-31T18:12:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>machine</b> <b>learning</b> approach to <b>Bayesian</b> parameter estimation | npj ...", "url": "https://www.nature.com/articles/s41534-021-00497-w", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41534-021-00497-w", "snippet": "<b>Bayesian</b> estimation is a powerful theoretical paradigm for the operation of the approach to parameter estimation. However, the <b>Bayesian</b> method for statistical inference generally suffers from ...", "dateLastCrawled": "2022-02-03T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian Optimization</b>: fine-tuning black-box processes", "url": "https://www.innovating-automation.blog/bayesian-optimization/", "isFamilyFriendly": true, "displayUrl": "https://www.innovating-automation.blog/<b>bayesian-optimization</b>", "snippet": "AutoML (automated <b>machine</b> <b>learning</b>) is one of the recent paradigm-breaking developments in <b>machine</b> <b>learning</b>. New libraries such as HyperOpt allow data scientists and <b>machine</b> <b>learning</b> practitioners to save time spent on selecting, tuning and evaluating different algorithms, tasks that are often very time consuming. Some of these libraries \u2013 HyperOpt, for example \u2013 use a technique called <b>Bayesian Optimization</b>. Instead of randomly or exhaustively iterating through combinations of algorithms ...", "dateLastCrawled": "2022-02-03T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Machine</b> <b>Learning</b>: A <b>Bayesian</b> and <b>Optimization</b> Perspective [2&amp;nbsp;ed ...", "url": "https://dokumen.pub/machine-learning-a-bayesian-and-optimization-perspective-2nbsped-0128188030-9780128188033.html", "isFamilyFriendly": true, "displayUrl": "https://dokumen.pub/<b>machine</b>-<b>learning</b>-a-<b>bayesian</b>-and-<b>optimization</b>-perspective-2nbsped...", "snippet": "<b>Machine</b> <b>Learning</b> A <b>Bayesian</b> and <b>Optimization</b> Perspective <b>Machine</b> <b>Learning</b> A <b>Bayesian</b> and <b>Optimization</b> Perspective 2nd Edition Sergios Theodoridis Department of Informatics and Telecommunications National and Kapodistrian University of Athens Athens, Greece Shenzhen Research Institute of Big Data The Chinese University of Hong Kong Shenzhen, China Academic Press is an imprint of Elsevier 125 London Wall, London EC2Y 5AS, United Kingdom 525 B Street, Suite 1650, San Diego, CA 92101, United ...", "dateLastCrawled": "2022-01-28T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The Hitchhiker\u2019s Guide to <b>Optimization</b> in <b>Machine Learning</b> | by Aman ...", "url": "https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-machine-learning-edcf5a104210", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-hitchhikers-guide-to-<b>optimization</b>-in-<b>machine</b>...", "snippet": "Gradient descent is one of the easiest to implement (and arguably one of the worst) <b>optimization</b> algorithms in <b>machine learning</b>. It is a first-order (i.e., gradient-based) <b>optimization</b> algorithm where we iteratively update the parameters of a differentiable cost function until its minimum is attained. Before we understand how gradient descent works, first let us have a look at the generalized formula of GD: Gradient descent (Image by author) The basic idea here is to update the model ...", "dateLastCrawled": "2022-02-02T19:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Three things to help you <b>get started on Bayesian Optimisation</b> | Oxford ...", "url": "https://www.blopig.com/blog/2019/09/three-things-to-help-you-get-started-on-bayesian-optimisation/", "isFamilyFriendly": true, "displayUrl": "https://www.blopig.com/blog/2019/09/three-things-to-help-you-get-started-on-<b>bayesian</b>...", "snippet": "This entry was posted in Code, <b>Machine</b> <b>Learning</b>, <b>Optimization</b>, Python and tagged <b>Bayesian</b> <b>optimization</b> on September 3, 2019 by Susan Leung. Post navigation \u2190 OpenMM \u2013 easy to learn, highly flexible molecular dynamics in Python When OPIGlets leave the office \u2192", "dateLastCrawled": "2022-01-22T10:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>learning</b> for high-throughput experimental exploration of metal ...", "url": "https://www.sciencedirect.com/science/article/pii/S2542435121004451", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2542435121004451", "snippet": "The synthesis process is controlled by <b>Bayesian</b> <b>optimization</b> (BO) workflow that can simultaneously optimize the optoelectronic properties by composition selection and processing parameters for thin film materials. The alternative is the microfluidic systems as e.g., developed by Abolhasani et al. Figure 3B). 52, 53, 54 Here, using a modular microfluidic platform enables continuous manufacturing of inorganic MHP QDs guided by an ensemble neural network (ENN) exploration of the colloidal ...", "dateLastCrawled": "2022-01-23T17:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Comparison of Hyperparameter Tuning algorithms: <b>Grid search</b>, Random ...", "url": "https://medium.com/analytics-vidhya/comparison-of-hyperparameter-tuning-algorithms-grid-search-random-search-bayesian-optimization-5326aaef1bd1", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/comparison-of-hyperparameter-tuning-algorithms...", "snippet": "<b>Bayesian</b> <b>optimization</b> is a sequential model-based <b>optimization</b> ... An interesting <b>analogy</b> is to compare this to Bagging Vs Boosting. If you think about it, the idea is very similar! In bagging, we ...", "dateLastCrawled": "2022-01-26T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Demystifying <b>Hyper-Parameter tuning</b> | by Charles Brecque | Towards Data ...", "url": "https://towardsdatascience.com/demystifying-hyper-parameter-tuning-acb83af0258f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/demystifying-<b>hyper-parameter-tuning</b>-acb83af0258f", "snippet": "<b>Bayesian</b> <b>Optimization</b> addresses the pitfalls of grid and random search by incorporating a \u201cbelief\u201d of what the solution space looks like, and by \u201c<b>learning</b>\u201d from the configurations it evaluates. This belief can be specified by a domain expert but can also be flat at the beginning. If and when you try to run a marathon in stilettos, you won\u2019t try it again with every other pair of stilletos you own because you will have learnt that it\u2019s painful. Moreover, if you had prior knowledge ...", "dateLastCrawled": "2022-01-28T04:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian Optimization</b> Concept Explained in Layman Terms | by Wei Wang ...", "url": "https://towardsdatascience.com/bayesian-optimization-concept-explained-in-layman-terms-1d2bcdeaf12f", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>bayesian-optimization</b>-concept-explained-in-layman-terms...", "snippet": "<b>Bayesian Optimization</b> has been widely used for the hyperparameter tuning purpose in the <b>Machine</b> <b>Learning</b> world. Despite the fact that there are many terms and math formulas involved, the concept behind turns out to be very simple. The goal of this article is to share what I learned about <b>Bayesian Optimization</b> with a straight forward interpretation of textbook terminologies, and hopefully, it will help you understand what <b>Bayesian Optimization</b> is in a short period of time. The Overview of ...", "dateLastCrawled": "2022-01-29T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian optimization</b> or <b>gradient descent</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/161923/bayesian-optimization-or-gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/161923/<b>bayesian-optimization</b>-or-<b>gradient-descent</b>", "snippet": "The most immediate difference is that <b>Bayesian optimization</b> is applicable when you don&#39;t know the gradients. If you can cheaply compute gradients of your function, you&#39;ll want to use a method that can incorporate those, since they can be extremely helpful in understanding the function. If you can&#39;t easily compute gradients and need to resort to ...", "dateLastCrawled": "2022-02-02T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Study Neural Architecture Search", "url": "https://www.cse.cuhk.edu.hk/lyu/_media/students/lyu2002_1st_term_report.pdf?id=students%3Afyp&cache=cache", "isFamilyFriendly": true, "displayUrl": "https://www.cse.cuhk.edu.hk/lyu/_media/students/lyu2002_1st_term_report.pdf?id=students...", "snippet": "searching for the best hyperparameters of a <b>machine</b> <b>learning</b> model to attain the best performance. Common hyperparameters of a model are <b>learning</b> rate, batch size, number of training epoch etc. While it is not the focus of our project, it is worth to mention that hyperparameter optimization overlaps a lot with NAS. We can think of the architecture of a network as one of the hyperparameters of the network. Meta-<b>learning</b> suggests using meta-data to lead the <b>learning</b> of our model. Meta-data is ...", "dateLastCrawled": "2021-12-15T21:44:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What&#39;s <b>trending in machine learning (outside of deep learning</b>)? - Quora", "url": "https://www.quora.com/Whats-trending-in-machine-learning-outside-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Whats-<b>trending-in-machine-learning-outside-of-deep-learning</b>", "snippet": "Answer (1 of 11): I don\u2019t know about trending, but I know of a powerful method (outside of mainstream ML) which is demonstrated to have tremendous flexibility, interpretability, and the advantage of relative ease of implementation in VLSI/FPGA hardware. Volterra Kernels The easiest way to under...", "dateLastCrawled": "2022-01-22T10:52:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(bayesian optimization)  is like +(a cat)", "+(bayesian optimization) is similar to +(a cat)", "+(bayesian optimization) can be thought of as +(a cat)", "+(bayesian optimization) can be compared to +(a cat)", "machine learning +(bayesian optimization AND analogy)", "machine learning +(\"bayesian optimization is like\")", "machine learning +(\"bayesian optimization is similar\")", "machine learning +(\"just as bayesian optimization\")", "machine learning +(\"bayesian optimization can be thought of as\")", "machine learning +(\"bayesian optimization can be compared to\")"]}