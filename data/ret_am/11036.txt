{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "35.1 <b>Gradient</b> descent | Social Data Science with R", "url": "https://www.sds.pub/gradient-descent.html", "isFamilyFriendly": true, "displayUrl": "https://www.sds.pub/<b>gradient</b>-descent.html", "snippet": "35.1 <b>Gradient</b> descent. 35.1. <b>Gradient</b> descent. <b>Gradient</b> descent is a general purpose optimization algorithm that is used (or most frequently, a variant is used) throughout many machine learning applications. When thinking about <b>gradient</b> descent conceptually, the scenario described previously of <b>walking</b> around a meadow blindfolded is again useful.", "dateLastCrawled": "2022-02-03T10:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A high-bias, low-variance introduction to Machine Learning for physicists", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6688775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6688775", "snippet": "To circumvent this <b>problem</b>, ideally our algorithm would keep track of curvature and take large steps in shallow, flat directions and small steps in <b>steep</b>, narrow directions. Second-order methods accomplish this by calculating or approximating the Hessian and normalizing the learning rate by the curvature. However, this is very computationally expensive for models with extremely large number of parameters. Ideally, we would <b>like</b> to be able to adaptively change the step size to match the ...", "dateLastCrawled": "2022-02-02T11:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction to neural network optimizers [part 2] \u2013 adaptive learning ...", "url": "https://www.jansellner.net/blog/Introduction_to_neural_network_optimizers_%5Bpart_2%5D_%E2%80%93_adaptive_learning_rates_%28RMSProp%2C_AdaGrad%29", "isFamilyFriendly": true, "displayUrl": "https://www.jansellner.net/blog/Introduction_to_neural_network_optimizers_[part_2...", "snippet": "Figure 1: Comparison of classical <b>gradient</b> descent (blue) with the adaptive learning scheme (orange).The two weight updates of the numerical example are shown. The weight vectors \\(\\fvec{w}\\), the gradients \\(\\nabla E\\) (used directly in classical <b>gradient</b> descent) in blue and the update vectors \\(\\fvec{v}(t)\\) (\\eqref{eq:Optimizer_AdaptiveUpdateVector}) of the adaptive learning rate scheme in orange are shown. Hover over the points to get more information. The picture here is quite ...", "dateLastCrawled": "2022-02-03T07:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "\u201cStep-free\u201d doesn\u2019t mean <b>DDA-compliant</b> \u2013 Daniel Bowen", "url": "https://www.danielbowen.com/2017/11/06/dda-compliant-stations/", "isFamilyFriendly": true, "displayUrl": "https://www.danielbowen.com/2017/11/06/<b>dda-compliant</b>-stations", "snippet": "The ramps being too <b>steep</b> is not only a <b>problem</b> for hand-operated wheelchairs. <b>Steep</b> gradients also cause problems for the stability of wheelchairs. To quote the Frontier V6 Power Chair Owners Manual \u201cWARNING! Any attempt to climb or descend a slope steeper than 1 in 14 <b>gradient</b> may put your power chair in an unstable position and cause it to tip, resulting in personal injury.\u201d \u201cExtreme care must be taken when ascending any incline over a 1 in 14 <b>gradient</b> to ensure the wheelchair is ...", "dateLastCrawled": "2021-12-31T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "2.3 <b>Position</b> vs. Time Graphs - Physics | OpenStax", "url": "https://openstax.org/books/physics/pages/2-3-position-vs-time-graphs", "isFamilyFriendly": true, "displayUrl": "https://openstax.org/books/physics/pages/2-3-<b>position</b>-vs-time-graphs", "snippet": "If the graph looks <b>like</b> a series of straight lines, then you can calculate the average velocity for each time interval by looking at the slope. If you then want to calculate the average velocity for the entire trip, you can do a weighted average. Let\u2019s look at another example. Figure 2.12 shows a graph of <b>position</b> versus time for a jet-powered car on a very flat dry lake bed in Nevada. Figure 2.12 The diagram shows a graph of <b>position</b> versus time for a jet-powered car on the Bonneville ...", "dateLastCrawled": "2022-02-02T08:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>6.4 Sedimentary Structures and Fossils</b> \u2013 Physical Geology", "url": "https://opentextbc.ca/geology/chapter/6-4-sedimentary-structures-and-fossils/", "isFamilyFriendly": true, "displayUrl": "https://opentextbc.ca/geology/chapter/<b>6-4-sedimentary-structures-and-fossils</b>", "snippet": "Ripples can also help to determine flow direction as they tend to have their steepest surface facing <b>down</b> flow. In a stream environment, boulders, cobbles, and pebbles can become imbricated, meaning that they are generally tilted in the same direction. Clasts in streams tend to tilt with their upper ends pointing downstream because this is the ...", "dateLastCrawled": "2022-01-30T11:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Neural Networks - Milania&#39;s Blog", "url": "https://www.jansellner.net/blog/category/Neural_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.jansellner.net/blog/category/Neural_Networks", "snippet": "Instead of moving <b>down</b> the steepest <b>hill</b> and then <b>walking</b> along the horizontal direction of the valley, the adaptive route points more directly towards the optimum. In this case, this results in a shorter path. This is an effect of the general principle that the adaptive scheme tends to focus more on the direction than on the magnitude of the <b>gradient</b>. In momentum optimization, on the other side, we mainly focused on improving the magnitude. We even accepted small divergences from the ...", "dateLastCrawled": "2021-12-13T05:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>The Fred</b> \u2013 Diary of an Amature Cyclist - WordPress.com", "url": "https://turneycycles.wordpress.com/2018/01/25/the-fred/", "isFamilyFriendly": true, "displayUrl": "https://turneycycles.wordpress.com/2018/01/25/<b>the-fred</b>", "snippet": "<b>Walking</b> up was <b>like</b> trying to push my bike up an escalator, my heart was still in my mouth just pushing the bike. The tank had run dry. Wrynose Pass comes shortly after this and again the <b>gradient</b> picks up to +20% so I was <b>walking</b> again, then <b>down</b> the <b>steep</b> slopes on the other side, but now I knew the end was in sight. All <b>down</b> <b>hill</b> from here on to the finish right?", "dateLastCrawled": "2022-01-25T01:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Dangers in the Pyrenees? | Camino de Santiago Forum", "url": "https://www.caminodesantiago.me/community/threads/dangers-in-the-pyrenees.41897/", "isFamilyFriendly": true, "displayUrl": "https://www.caminodesantiago.me/community/threads/dangers-in-the-pyrenees.41897", "snippet": "People sometimes get injured on the slower direct route <b>down</b> the <b>hill</b>. It is under trees (so it gets dark earlier) and is relatively <b>steep</b> and slippery. I used the longer but recommended route following an old road (with many shortcuts) <b>down</b> to the highway and then into Roncesvalles. When I went it was sideways blowing rain for a couple of hours. Bring rain gear and use it if the weather is wet. Last edited by a moderator: Jul 11, 2016. Reply. Reactions: SeaHorse and David. M. Mike Trebert ...", "dateLastCrawled": "2022-01-08T08:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "35.1 <b>Gradient</b> descent | Social Data Science with R", "url": "https://www.sds.pub/gradient-descent.html", "isFamilyFriendly": true, "displayUrl": "https://www.sds.pub/<b>gradient</b>-descent.html", "snippet": "Vanishing gradients is a <b>similar</b> <b>problem</b>, where the <b>gradient</b> is small enough that the new parameters do not change much and the algorithm gets \u201cstuck\u201d. This is okay (and expected) if the algorithm has reached the global minimum, but is a <b>problem</b> if it\u2019s only at a local minimum (i.e., stuck in a valley on a <b>hill</b>). There are numerous ...", "dateLastCrawled": "2022-02-03T10:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Neural Networks - Milania&#39;s Blog", "url": "https://www.milania.de/blog/category/Neural_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.milania.de/blog/category/Neural_Networks", "snippet": "Figure 1: Comparison of classical <b>gradient</b> descent (blue) with the adaptive learning scheme (orange).The two weight updates of the numerical example are shown. The weight vectors \\(\\fvec{w}\\), the gradients \\(\\nabla E\\) (used directly in classical <b>gradient</b> descent) in blue and the update vectors \\(\\fvec{v}(t)\\) (\\eqref{eq:Optimizer_AdaptiveUpdateVector}) of the adaptive learning rate scheme in orange are shown. Hover over the points to get more information. The picture here is quite ...", "dateLastCrawled": "2022-01-30T20:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A high-bias, low-variance introduction to Machine Learning for physicists", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6688775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6688775", "snippet": "This <b>problem</b> is called \u201coverfitting\u201d and leads to <b>a steep</b> drop-off in predictive performance. We can guard against overfitting in two ways: we can use less expressive models with fewer parameters, or we can collect more data so that the likelihood that the noise appears patterned decreases. Indeed, when we increase the size of the training data set by two orders of magnitude to N train = 10 4 (see Figure 3) the tenth order polynomial clearly gives both the best fits and the most ...", "dateLastCrawled": "2022-02-02T11:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "\u201cStep-free\u201d doesn\u2019t mean <b>DDA-compliant</b> \u2013 Daniel Bowen", "url": "https://www.danielbowen.com/2017/11/06/dda-compliant-stations/", "isFamilyFriendly": true, "displayUrl": "https://www.danielbowen.com/2017/11/06/<b>dda-compliant</b>-stations", "snippet": "The ramps being too <b>steep</b> is not only a <b>problem</b> for hand-operated wheelchairs. <b>Steep</b> gradients also cause problems for the stability of wheelchairs. To quote the Frontier V6 Power Chair Owners Manual \u201cWARNING! Any attempt to climb or descend a slope steeper than 1 in 14 <b>gradient</b> may put your power chair in an unstable position and cause it to tip, resulting in personal injury.\u201d \u201cExtreme care must be taken when ascending any incline over a 1 in 14 <b>gradient</b> to ensure the wheelchair is ...", "dateLastCrawled": "2021-12-31T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>6.4 Sedimentary Structures and Fossils</b> \u2013 Physical Geology", "url": "https://opentextbc.ca/geology/chapter/6-4-sedimentary-structures-and-fossils/", "isFamilyFriendly": true, "displayUrl": "https://opentextbc.ca/geology/chapter/<b>6-4-sedimentary-structures-and-fossils</b>", "snippet": "Ripples can also help to determine flow direction as they tend to have their steepest surface facing <b>down</b> flow. In a stream environment, boulders, cobbles, and pebbles can become imbricated, meaning that they are generally tilted in the same direction. Clasts in streams tend to tilt with their upper ends pointing downstream because this is the ...", "dateLastCrawled": "2022-01-30T11:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Cambridge-international-as-and-a-level-physics-coursebook-second ...", "url": "https://www.academia.edu/19977963/Cambridge_international_as_and_a_level_physics_coursebook_second_edition_part_one_web_1_", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/19977963", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-03T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Suppose that a bike rider pedaled up a <b>hill</b> and never came to a stop ...", "url": "https://www.quora.com/Suppose-that-a-bike-rider-pedaled-up-a-hill-and-never-came-to-a-stop-In-what-system-is-energy-conserved-From-what-form-of-energy-did-the-bike-gain-mechanical-energy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Suppose-that-a-bike-rider-pedaled-up-a-<b>hill</b>-and-never-came-to-a...", "snippet": "Answer (1 of 3): If you already know the answer, as it seems you may, why ask the question. That said, I will discuss the question as I understand it. For reference information, I did ride my bicycle from the west coast of the U.S.A. to the east coast in the year 2000, so I do have some well expe...", "dateLastCrawled": "2022-01-16T03:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>The Fred</b> \u2013 Diary of an Amature Cyclist - WordPress.com", "url": "https://turneycycles.wordpress.com/2018/01/25/the-fred/", "isFamilyFriendly": true, "displayUrl": "https://turneycycles.wordpress.com/2018/01/25/<b>the-fred</b>", "snippet": "<b>Walking</b> up was like trying to push my bike up an escalator, my heart was still in my mouth just pushing the bike. The tank had run dry. Wrynose Pass comes shortly after this and again the <b>gradient</b> picks up to +20% so I was <b>walking</b> again, then <b>down</b> the <b>steep</b> slopes on the other side, but now I knew the end was in sight. All <b>down</b> <b>hill</b> from here ...", "dateLastCrawled": "2022-01-25T01:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Dangers in the Pyrenees? | Camino de Santiago Forum", "url": "https://www.caminodesantiago.me/community/threads/dangers-in-the-pyrenees.41897/", "isFamilyFriendly": true, "displayUrl": "https://www.caminodesantiago.me/community/threads/dangers-in-the-pyrenees.41897", "snippet": "People sometimes get injured on the slower direct route <b>down</b> the <b>hill</b>. It is under trees (so it gets dark earlier) and is relatively <b>steep</b> and slippery. I used the longer but recommended route following an old road (with many shortcuts) <b>down</b> to the highway and then into Roncesvalles. When I went it was sideways blowing rain for a couple of hours. Bring rain gear and use it if the weather is wet. Last edited by a moderator: Jul 11, 2016. Reply. Reactions: SeaHorse and David. M. Mike Trebert ...", "dateLastCrawled": "2022-01-08T08:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A high-bias, low-variance introduction to Machine Learning for physicists", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6688775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6688775", "snippet": "This <b>problem</b> is called \u201coverfitting\u201d and leads to <b>a steep</b> drop-off in predictive performance. We <b>can</b> guard against overfitting in two ways: we <b>can</b> use less expressive models with fewer parameters, or we <b>can</b> collect more data so that the likelihood that the noise appears patterned decreases. Indeed, when we increase the size of the training data set by two orders of magnitude to N train = 10 4 (see Figure 3) the tenth order polynomial clearly gives both the best fits and the most ...", "dateLastCrawled": "2022-02-02T11:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Gradient</b> Energy Penalty Term", "url": "https://groups.google.com/g/kxhwph/c/FcM1o-2gAhA", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/kxhwph/c/FcM1o-2gAhA", "snippet": "All groups and messages ... ...", "dateLastCrawled": "2022-01-16T21:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Neural Networks in Plain English | Bear&#39;s Den", "url": "http://dillingers.com/blog/2015/11/02/neural-networks-in-english/", "isFamilyFriendly": true, "displayUrl": "dillingers.com/blog/2015/11/02/neural-networks-in-english", "snippet": "Steepest <b>gradient</b> descent <b>can</b> wind up going VERY slow if it is proceeding \u2018downhill\u2019 toward a good solution in a \u2018diagonal groove\u2019 that requires two or more weights to change at the same time because if one of them (or not enough of them) change without the others changing, it makes the solution worse. Momentum <b>can</b> straighten out these paths and make the different weights change together, even when individual examples only \u2018pull\u2019 in one direction. This matters a lot less if you ...", "dateLastCrawled": "2022-01-19T00:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Cambridge-international-as-and-a-level-physics-coursebook-second ...", "url": "https://www.academia.edu/19977963/Cambridge_international_as_and_a_level_physics_coursebook_second_edition_part_one_web_1_", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/19977963", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-03T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "ss19.script - Google Groups", "url": "https://groups.google.com/g/alt.clearing.technology/c/A7fssKXfrkY/m/2qTHPL_zAgAJ", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/alt.clearing.technology/c/A7fssKXfrkY/m/2qTHPL_zAgAJ", "snippet": "machine&quot; and therefore <b>thought</b> he had the whole thing. Other endwords may be: 1. postulating machine 2. thinkingness machine 3. somatic machine 4. reality machine 5. confusion machine 6. forgettingness machine ----- CHAPTER 11: The One Command GPM (to seek treasure) There is a missing group of items. Add these as 21. on the platen: a) Abandon, b) Do Not Abandon, c) Have, d) Never Have. As given, the platen only has one endword which is &quot;Treasure&quot;. This is incomplete. The full series of ...", "dateLastCrawled": "2021-12-29T04:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "\u201cStep-free\u201d doesn\u2019t mean <b>DDA-compliant</b> \u2013 Daniel Bowen", "url": "https://www.danielbowen.com/2017/11/06/dda-compliant-stations/", "isFamilyFriendly": true, "displayUrl": "https://www.danielbowen.com/2017/11/06/<b>dda-compliant</b>-stations", "snippet": "And my mum \u2013 my formerly fit, active, healthy mum who used to walk everywhere, for miles \u2013 now uses a <b>walking</b> stick, and cannot walk <b>down</b> <b>steep</b> ramps or manage a step higher than about 12cm. I was confronted with that reality some years ago when I was planning a journey for the two of us: \u201c\u2026 So, Mum, you\u2019ll get off the bus at Essendon. I\u2019ll meet you there, and we\u2019ll catch the train\u2026\u201d, and she paused and then said quietly, \u201cI <b>can</b>\u2019t use Essendon any more. I <b>can</b> change ...", "dateLastCrawled": "2021-12-31T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "the nomadic welshman \u2013 Being respectfully disrespectful allows you to ...", "url": "https://thenomadicwelshman.wordpress.com/", "isFamilyFriendly": true, "displayUrl": "https://thenomadicwelshman.wordpress.com", "snippet": "Whilst <b>walking</b> <b>down</b> the <b>steep</b> <b>gradient</b> towards the beach that would match the slopes in the Swiss Alps, we were hounded by the tracksuit bottom cladded PR guru\u2019s standing outside the \u2018Traditional Full English Breakfast\u2019 joints and received pitches in either a Scouse or Geordie accent, this was my worth nightmare. I answered in my best German voice which resembled that of Arnold Swarzenegger (I know he\u2019s Austrian), highlighting my dislike for the English breakfast. Thinking that I had ...", "dateLastCrawled": "2021-12-21T15:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Opinion: Why Is Everyone Talking About Seat Tube</b> Angles? - <b>Pinkbike</b>", "url": "https://www.pinkbike.com/news/opinion-why-is-everyone-talking-about-seat-tube-angles.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pinkbike.com</b>/news/<b>opinion-why-is-everyone-talking-about-seat-tube</b>-angles.html", "snippet": "So when you <b>can</b> ride a 160 travel 29er, don\u2019t have to change the travel, and barely have to weight the bars to climb <b>a steep</b> pitch and keep the front end <b>down</b>, like an xc bike, that\u2019s what we ...", "dateLastCrawled": "2022-01-29T04:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Introduction to neural network optimizers [part 2] \u2013 adaptive learning ...", "url": "https://www.jansellner.net/blog/Introduction_to_neural_network_optimizers_%5Bpart_2%5D_%E2%80%93_adaptive_learning_rates_%28RMSProp%2C_AdaGrad%29", "isFamilyFriendly": true, "displayUrl": "https://www.jansellner.net/blog/Introduction_to_neural_network_optimizers_[part_2...", "snippet": "Instead of moving <b>down</b> the steepest <b>hill</b> and then <b>walking</b> along the horizontal direction of the valley, the adaptive route points more directly towards the optimum. In this case, this results in a shorter path. This is an effect of the general principle that the adaptive scheme tends to focus more on the direction than on the magnitude of the <b>gradient</b>. In momentum optimization, on the other side, we mainly focused on improving the magnitude. We even accepted small divergences from the ...", "dateLastCrawled": "2022-02-03T07:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A high-bias, low-variance introduction to Machine Learning for physicists", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6688775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6688775", "snippet": "This <b>problem</b> is called \u201coverfitting\u201d and leads to <b>a steep</b> drop-off in predictive performance. We <b>can</b> guard against overfitting in two ways: we <b>can</b> use less expressive models with fewer parameters, or we <b>can</b> collect more data so that the likelihood that the noise appears patterned decreases. Indeed, when we increase the size of the training data set by two orders of magnitude to N train = 10 4 (see Figure 3) the tenth order polynomial clearly gives both the best fits and the most ...", "dateLastCrawled": "2022-02-02T11:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Neural Networks - Milania&#39;s Blog", "url": "https://www.milania.de/blog/category/Neural_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.milania.de/blog/category/Neural_Networks", "snippet": "Instead of moving <b>down</b> the steepest <b>hill</b> and then <b>walking</b> along the horizontal direction of the valley, the adaptive route points more directly towards the optimum. In this case, this results in a shorter path. This is an effect of the general principle that the adaptive scheme tends to focus more on the direction than on the magnitude of the <b>gradient</b>. In momentum optimization, on the other side, we mainly focused on improving the magnitude. We even accepted small divergences from the ...", "dateLastCrawled": "2022-01-30T20:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 3, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Gradient descent</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Gradient_descent", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Gradient_descent</b>", "snippet": "<b>Gradient descent</b> is based on the observation that if the multi-variable function is defined and differentiable in a neighborhood of a point , then () decreases fastest if one goes from in the direction of the negative <b>gradient</b> of at , ().It follows that, if + = for a small enough step size or learning rate +, then (+).In other words, the term () is subtracted from because we want to move against the <b>gradient</b>, toward the local minimum. With this observation in mind, one starts with a guess ...", "dateLastCrawled": "2022-02-03T03:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>6.4 Sedimentary Structures and Fossils</b> \u2013 Physical Geology", "url": "https://opentextbc.ca/geology/chapter/6-4-sedimentary-structures-and-fossils/", "isFamilyFriendly": true, "displayUrl": "https://opentextbc.ca/geology/chapter/<b>6-4-sedimentary-structures-and-fossils</b>", "snippet": "Ripples <b>can</b> also help to determine flow direction as they tend to have their steepest surface facing <b>down</b> flow. In a stream environment, boulders, cobbles, and pebbles <b>can</b> become imbricated, meaning that they are generally tilted in the same direction. Clasts in streams tend to tilt with their upper ends pointing downstream because this is the ...", "dateLastCrawled": "2022-01-30T11:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Adaptive Computation and Machine Learning series- Deep learning</b> ...", "url": "https://www.academia.edu/38223830/Adaptive_Computation_and_Machine_Learning_series_Deep_learning_The_MIT_Press_2016_pdf", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/38223830/<b>Adaptive_Computation_and_Machine_Learning_series</b>...", "snippet": "<b>Adaptive Computation and Machine Learning series- Deep learning</b>-The MIT Press (2016).pdf", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "All about UPSC IAS, IPS, IFS by Lokayan IAS Academy: <b>NCERT BOOK GEOGRAPHY</b>", "url": "http://www.lokayan.com/2021/03/ncert-book-geography.html", "isFamilyFriendly": true, "displayUrl": "www.lokayan.com/2021/03/<b>ncert-book-geography</b>.html", "snippet": "When the river tumbles at <b>steep</b> angle over very hard rocks or <b>down</b> <b>a steep</b> valley side it forms a waterfall As the river enters the plain it twists and turns forming large bends known as meanders. Due to continuous erosion and deposition along the sides of the meander, the ends of the meander loop come closer and closer. In due course of time the meander loop cuts off from the river and forms a cut-off lake, also called an ox-bow lake. At times the river overflows its banks. This leads to ...", "dateLastCrawled": "2022-02-03T04:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Opinion: Why Is Everyone Talking About Seat Tube</b> Angles? - <b>Pinkbike</b>", "url": "https://www.pinkbike.com/news/opinion-why-is-everyone-talking-about-seat-tube-angles.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pinkbike.com</b>/news/<b>opinion-why-is-everyone-talking-about-seat-tube</b>-angles.html", "snippet": "It goes <b>down</b> the <b>hill</b> like an insane badger <b>compared</b> to the previous bike. I&#39;m riding on a whole new level and it was a night/day change from the old one. Granted, a large factor of it is a 170 mm ...", "dateLastCrawled": "2022-01-29T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GEOL 1303 Final Exam</b> Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/40969688/geol-1303-final-exam-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/40969688/<b>geol-1303-final-exam</b>-flash-cards", "snippet": "Rocks <b>can</b> almost be considered alive. We have observed that a sedimentary rock <b>can</b> change to a metamorphic rock, which <b>can</b> then change into an igneous rock. We use the term &quot;rock cycle&quot; to explain these processes. But this cycle needs energy sources to drive the rock changes. The major driving energy forces for all parts of this rock cycle are", "dateLastCrawled": "2019-08-05T23:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Will riding a bike on a reserve have any effect on fuel ... - Quora", "url": "https://www.quora.com/Will-riding-a-bike-on-a-reserve-have-any-effect-on-fuel-consumption", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Will-riding-a-bike-on-a-reserve-have-any-effect-on-fuel-consumption", "snippet": "Answer (1 of 3): There\u2019s no effect on fuel consumption when using reserve fuel. A motorcycle will consume fuel at the same rate. The only difference is that reserve fuel is often at the bottom of the fuel tank and as such might contain contaminants like water. On some older motorcycles, the fuel ...", "dateLastCrawled": "2022-01-28T02:15:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Exploding</b> Gradients and the <b>Problem</b> with Overshooting \u2013 Populus Press", "url": "https://populuspress.blog/2021/12/24/exploding-gradients-and-the-problem-with-overshooting/", "isFamilyFriendly": true, "displayUrl": "https://populuspress.blog/2021/12/24/<b>exploding</b>-<b>gradients</b>-and-the-<b>problem</b>-with-overshooting", "snippet": "Similar to the vanishing <b>gradient</b>, an <b>exploding</b> <b>gradient</b> can occur when individual layer gradients turn out to be large. When the model multiples these individual gradients together during backpropagation, this can result in a huge <b>gradient</b> since multiplying many large numbers together will cause the product to skyrocket. The thing is, we want our model to make smaller adjustments as time passes. If the model is <b>learning</b> and getting closer and closer to making predictions in line with the ...", "dateLastCrawled": "2022-01-24T21:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Lecture 15: <b>Exploding</b> and Vanishing Gradients", "url": "https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15 <b>Exploding</b> and...", "snippet": "1.1 <b>Learning</b> Goals Understand why gradients explode or vanish, both { in terms of the mechanics of computing the gradients { the functional relationship between the hidden units at di erent time steps Be able to analyze simple examples of iterated functions, including identifying xed points and qualitatively determining the long-term behavior from a given initialization. Know about various methods for dealing with the <b>problem</b>, and why they help: { <b>Gradient</b> clipping { Reversing the input ...", "dateLastCrawled": "2022-01-30T11:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Exploding And Vanishing Gradient Problem: Math Behind</b> The Truth | by ...", "url": "https://becominghuman.ai/exploding-and-vanishing-gradient-problem-math-behind-the-truth-2d17f9bf6a57", "isFamilyFriendly": true, "displayUrl": "https://becominghuman.ai/<b>exploding-and-vanishing-gradient-problem-math-behind</b>-the...", "snippet": "But what if the <b>gradient</b> becomes negligible? When the <b>gradient</b> becomes negligible, subtracting it from original matrix doesn\u2019t makes any sense and hence the model stops <b>learning</b>. This <b>problem</b> is called as Vanishing <b>Gradient</b> <b>Problem</b>. We\u2019ll first visualise the <b>problem</b> practically in our mind. We\u2019ll train a Deep <b>Learning</b> Model with MNIST(you ...", "dateLastCrawled": "2022-01-17T22:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Vanishing gradient</b> and <b>exploding</b> <b>gradient</b> in Neural networks | by Arun ...", "url": "https://medium.com/tech-break/vanishing-gradient-and-exploding-gradient-in-neural-networks-15950664447e", "isFamilyFriendly": true, "displayUrl": "https://medium.com/tech-break/<b>vanishing-gradient</b>-and-<b>exploding</b>-<b>gradient</b>-in-neural...", "snippet": "<b>Vanishing gradient</b> <b>problem</b> is a common <b>problem</b> that we face while training deep neural networks.Gradients of neural networks are found during back propagation. Generally, adding more hidden layers\u2026", "dateLastCrawled": "2022-01-25T21:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Bird\u2019s-Eye View <b>Of Artificial Intelligence, Machine Learning, Neural</b> ...", "url": "https://medium.com/geekculture/birds-eye-view-of-artificial-intelligence-machine-learning-neural-networks-language-part-2-a53d93495de1", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/birds-eye-view-of-artificial-intelligence-<b>machine</b>...", "snippet": "The <b>analogy</b> is like someone ... Vanishing and <b>Exploding</b> <b>Gradient</b>. This is a <b>problem</b> some neural networks have during backpropagation due to long-term dependencies between earlier layers and later ...", "dateLastCrawled": "2021-05-17T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>Vanishing Gradient Problem</b>? - Great <b>Learning</b>", "url": "https://www.mygreatlearning.com/blog/the-vanishing-gradient-problem/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/blog/the-<b>vanishing-gradient-problem</b>", "snippet": "In <b>Machine</b> <b>Learning</b>, the <b>Vanishing Gradient Problem</b> is encountered while training Neural Networks with <b>gradient</b>-based methods (example, Back Propagation). This <b>problem</b> makes it hard to learn and tune the parameters of the earlier layers in the network. The vanishing gradients <b>problem</b> is one example of unstable behaviour that you may encounter when training a deep neural network. It describes the situation where a deep multilayer feed-forward network or a recurrent neural network is unable to ...", "dateLastCrawled": "2022-02-02T21:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning: Text Generation, A Summary</b> \u2013 Alan&#39;s Blog", "url": "https://achungweb.wordpress.com/2017/04/14/machine-learning-text-generation-a-summary/", "isFamilyFriendly": true, "displayUrl": "https://achungweb.wordpress.com/2017/04/14/<b>machine-learning-text-generation-a-summary</b>", "snippet": "The Vanishing (and <b>Exploding</b>!) <b>Gradient</b> <b>Problem</b>. Previously, we stated that the output from the (n-1)th unit is multiplied by some hidden weight matrix H before it gets transferred to the next unit. As a program runs, therefore, a previous piece of information will be multiplied by hundreds of thousands of such matrices as it gets transferred along the RNN. As we know, repeated multiplication has the potential to grow staggering large, and our previous data will become so inflated to the ...", "dateLastCrawled": "2022-01-20T17:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Gradient Descent</b>. It is a slippery slope, but promise it\u2026 | by Hamza ...", "url": "https://towardsdatascience.com/gradient-descent-3a7db7520711", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent</b>-3a7db7520711", "snippet": "tl;dr <b>Gradient Descent</b> is an optimization technique that is used to improve deep <b>learning</b> and neural network-based models by minimizing the cost function.. In our previous post, we talked about activation functions (link here) and where it is used in <b>machine</b> <b>learning</b> models.However, we also heavily used the term \u2018<b>Gradient Descent</b>\u2019 which is a key element in deep <b>learning</b> models, which are going to talk about in this post.", "dateLastCrawled": "2022-01-30T15:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The <b>Vanishing Gradient</b> <b>Problem</b>. The <b>Problem</b>, Its Causes, Its\u2026 | by Chi ...", "url": "https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-<b>vanishing-gradient</b>-<b>problem</b>-69bf08b15484", "snippet": "For shallow network with only a few layers that use these activations, this isn\u2019t a big <b>problem</b>. However, when more layers are used, it can cause the <b>gradient</b> to be too small for training to work effectively. Gradients of neural networks are found using backpropagation. Simply put, backpropagation finds the derivatives of the network by ...", "dateLastCrawled": "2022-02-02T06:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "This shortcoming \u2026 referred to in the literature as the vanishing <b>gradient</b> <b>problem</b> \u2026 <b>Long Short-Term Memory</b> (LSTM) is an RNN architecture specifically designed to address the vanishing <b>gradient</b> <b>problem</b>. \u2014 Alex Graves, et al., A Novel Connectionist System for Unconstrained Handwriting Recognition, 2009. The key to the LSTM solution to the technical problems was the specific internal structure of the units used in the model. \u2026 governed by its ability to deal with vanishing and ...", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(exploding gradient problem)  is like +(walking down a steep hill)", "+(exploding gradient problem) is similar to +(walking down a steep hill)", "+(exploding gradient problem) can be thought of as +(walking down a steep hill)", "+(exploding gradient problem) can be compared to +(walking down a steep hill)", "machine learning +(exploding gradient problem AND analogy)", "machine learning +(\"exploding gradient problem is like\")", "machine learning +(\"exploding gradient problem is similar\")", "machine learning +(\"just as exploding gradient problem\")", "machine learning +(\"exploding gradient problem can be thought of as\")", "machine learning +(\"exploding gradient problem can be compared to\")"]}