{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Comparing <b>Different</b> <b>Classification</b> Machine ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/comparing-different-classification-machine-learning-models-for-an-imbalanced-dataset-fdae1af3677f", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/comparing-<b>different</b>-<b>classification</b>-machine-learning...", "snippet": "<b>Data</b> sets are unbalanced when at least one <b>class</b> is represented by only a small number of training examples (called the <b>minority</b> <b>class</b>) while other classes make up the majority. In this scenario, classifiers can have good accuracy on the majority <b>class</b> but very poor accuracy on the <b>minority</b> <b>class</b>(es) due to the influence that the larger majority <b>class</b>. The common example of such <b>dataset</b> is credit card fraud detection, where <b>data</b> <b>points</b> for fraud = 1, are usually very less in comparison to ...", "dateLastCrawled": "2022-02-03T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Multiclass <b>Classification</b> with Imbalanced <b>Dataset</b> - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/machine-learning-multi<b>class</b>-<b>classification</b>-with...", "snippet": "A total of 80 instances are labeled with <b>Class</b>-1 (Oranges), 10 instances with <b>Class</b>-2 (Apples) and the remaining 10 instances are labeled with <b>Class</b>-3 (Pears). This is an imbalanced <b>dataset</b> and the ratio of 8:1:1. Most <b>classification</b> <b>data</b> sets do not have exactly equal number of instances in each <b>class</b>, but a small difference often does not matter. There are problems where a <b>class</b> imbalance is not just common, it is expected. For example, in datasets <b>like</b> those that characterize fraudulent ...", "dateLastCrawled": "2022-02-02T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Classification Algorithms for Imbalanced Datasets</b> - BLOCKGENI", "url": "https://blockgeni.com/classification-algorithms-for-imbalanced-datasets/", "isFamilyFriendly": true, "displayUrl": "https://blockgeni.com/<b>classification-algorithms-for-imbalanced-datasets</b>", "snippet": "Outliers or anomalies are rare examples that do not fit in with the <b>rest</b> <b>of the data</b>. Identifying outliers in <b>data</b> is referred to as outlier or anomaly detection and a subfield of machine learning focused on this problem is referred to as one-<b>class</b> classification. These are unsupervised learning algorithms that attempt to model \u201cnormal\u201d examples in order to classify new examples as either normal or abnormal (e.g. outliers). One-<b>class</b> classification algorithms can be used for binary ...", "dateLastCrawled": "2022-02-02T16:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "SMOTE for Imbalanced Classification with Python", "url": "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/smote-oversampling-for-imbalanc", "snippet": "Next, we can oversample the <b>minority</b> <b>class</b> using SMOTE and plot the transformed <b>dataset</b>. We can use the SMOTE implementation provided by the imbalanced-learn Python library in the SMOTE <b>class</b>.. The SMOTE <b>class</b> acts <b>like</b> a <b>data</b> transform object from scikit-learn in that it must be defined and configured, fit on a <b>dataset</b>, then applied to create a new transformed version of the <b>dataset</b>.", "dateLastCrawled": "2022-02-02T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Imbalanced <b>data</b> learning by <b>minority class augmentation using capsule</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231220312091", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231220312091", "snippet": "The methodology for evaluation is as follows: Firstly, we need to create an imbalanced <b>dataset</b>; thus for each <b>class</b>, we remove some part of that <b>class</b> (dropping a percentage of images for each <b>class</b> from the training set) then we create imbalanced <b>dataset</b> by a percentage of 40%, 20%, 10%, 5% and 2.5%. By using our generator, we generate <b>minority</b> samples from novel multivariate distribution; train the designed discriminator for the balanced datasets and measure the balanced accuracy over the ...", "dateLastCrawled": "2021-12-09T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Calculate Precision, Recall, and F-Measure for Imbalanced ...", "url": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-", "snippet": "Classification accuracy is the total number of correct predictions divided by the total number of predictions made for a <b>dataset</b>. As a performance measure, accuracy is inappropriate for imbalanced classification problems. The main reason is that the overwhelming number of examples from the majority <b>class</b> (or classes) will overwhelm the number of examples in the <b>minority</b> <b>class</b>, meaning that even unskillful models", "dateLastCrawled": "2022-02-03T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Assignment 7 (Sol.)", "url": "https://www.nptel.ac.in/content/storage2/courses/110106072/Week%207%20Assignment%201.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.nptel.ac.in/content/storage2/courses/110106072/Week 7 Assignment 1.pdf", "snippet": "In an imbalanced <b>data set</b>, accuracy should not be used as a measure of performance because 99% (as given) might only be predicting majority <b>class</b> correctly, but our <b>class</b> of interest is also the <b>minority</b> <b>class</b> (1%). Hence, in order to evaluate model performance, we should use", "dateLastCrawled": "2022-01-29T11:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Fraud Detection</b> with Python - GitHub Pages", "url": "https://trenton3983.github.io/files/projects/2019-07-19_fraud_detection_python/2019-07-19_fraud_detection_python.html", "isFamilyFriendly": true, "displayUrl": "https://trenton3983.github.io/files/projects/2019-07-19_<b>fraud_detection</b>_python/2019-07...", "snippet": "It should by now be clear that SMOTE has balanced our <b>data</b> completely, and that the <b>minority</b> <b>class</b> is now equal in size to the majority <b>class</b>. Visualizing the <b>data</b> shows the effect on the <b>data</b> very clearly. The next exercise will demonstrate multiple ways to implement SMOTE and that each method will have a slightly <b>different</b> effect. <b>Fraud detection</b> algorithms in action\u00b6 Rules Based Systems\u00b6 Might block transactions from risky zip codes; Block transactions from cards used too frequently (e ...", "dateLastCrawled": "2022-02-02T18:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "classification - <b>Data Science Stack Exchange</b>", "url": "https://datascience.stackexchange.com/questions/57902/oversampling-only-balances-the-training-set-what-about-the-testing-set", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/57902/oversampling-only-balances-the...", "snippet": "Also, I have tried SMOTE, SMOTE-NC, and <b>Class</b>_weight to oversample my training set. To increase the chance of having more <b>data</b> from the <b>minority</b> <b>class</b>, I changed the 10-fold to 5-fold cross-validation (when developing the models), no improvement! PS: I have &gt;100K <b>data</b> <b>points</b> in my <b>dataset</b>.", "dateLastCrawled": "2022-01-26T04:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - Calculating <b>F-Score</b>, which is the &quot;positive&quot; <b>class</b> ...", "url": "https://stats.stackexchange.com/questions/191645/calculating-f-score-which-is-the-positive-class-the-majority-or-minority-cla", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/191645", "snippet": "I&#39;m just making these numbers up for the sake of understanding what the correct convention is with regard to computing the <b>F-Score</b>. Fundamentally it&#39;s just a skewed <b>dataset</b>. I can compute the <b>F-Score</b> two ways, considering the true-postiive as a measure of the healthy, majority <b>class</b>, or as the unhealthy, <b>minority</b> <b>class</b>. $\\endgroup$ \u2013", "dateLastCrawled": "2022-01-24T21:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Comparing <b>Different</b> <b>Classification</b> Machine ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/comparing-different-classification-machine-learning-models-for-an-imbalanced-dataset-fdae1af3677f", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/comparing-<b>different</b>-<b>classification</b>-machine-learning...", "snippet": "<b>Data</b> sets are unbalanced when at least one <b>class</b> is represented by only a small number of training examples (called the <b>minority</b> <b>class</b>) while other classes make up the majority. In this scenario, classifiers can have good accuracy on the majority <b>class</b> but very poor accuracy on the <b>minority</b> <b>class</b>(es) due to the influence that the larger majority <b>class</b>. The common example of such <b>dataset</b> is credit card fraud detection, where <b>data</b> <b>points</b> for fraud = 1, are usually very less in comparison to ...", "dateLastCrawled": "2022-02-03T05:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Multiclass <b>Classification</b> with Imbalanced <b>Dataset</b> - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/machine-learning-multi<b>class</b>-<b>classification</b>-with...", "snippet": "A total of 80 instances are labeled with <b>Class</b>-1 (Oranges), 10 instances with <b>Class</b>-2 (Apples) and the remaining 10 instances are labeled with <b>Class</b>-3 (Pears). This is an imbalanced <b>dataset</b> and the ratio of 8:1:1. Most <b>classification</b> <b>data</b> sets do not have exactly equal number of instances in each <b>class</b>, but a small difference often does not matter. There are problems where a <b>class</b> imbalance is not just common, it is expected. For example, in datasets like those that characterize fraudulent ...", "dateLastCrawled": "2022-02-02T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "SMOTE for Imbalanced Classification with Python", "url": "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/smote-oversampling-for-imbalanc", "snippet": "This can be achieved by simply duplicating examples from the <b>minority</b> <b>class</b> in the training <b>dataset</b> prior to fitting a model. This can balance the <b>class</b> distribution but does not provide any additional information to the model. An improvement on duplicating examples from the <b>minority</b> <b>class</b> is to synthesize new examples from the <b>minority</b> <b>class</b>. This is a type <b>of data</b> augmentation for tabular <b>data</b> and can be very effective. Perhaps the most widely used approach to synthesizing new examples is ...", "dateLastCrawled": "2022-02-02T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ANALYSIS OF SAMPLING TECHNIQUES FOR IMBALANCED <b>DATA</b>: AN N=648 ADNI STUDY", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3946903/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3946903", "snippet": "A <b>dataset</b> is said to be imbalanced if there are significantly more <b>data</b> <b>points</b> of one <b>class</b> and fewer occurrences of the other <b>class</b>. For example, the number of control cases in the ADNI <b>dataset</b> is half of the number of AD cases for proteomics measurement, whereas for MRI modality, there are 40% more control cases than AD cases. <b>Data</b> imbalance is also ubiquitous in worldwide ADNI type initiatives from Europe, Japan and Australia, etc. Weiner et al., 2012). In addition, lots of medical ...", "dateLastCrawled": "2022-01-18T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Imbalanced <b>data</b> learning by <b>minority class augmentation using capsule</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231220312091", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231220312091", "snippet": "The methodology for evaluation is as follows: Firstly, we need to create an imbalanced <b>dataset</b>; thus for each <b>class</b>, we remove some part of that <b>class</b> (dropping a percentage of images for each <b>class</b> from the training set) then we create imbalanced <b>dataset</b> by a percentage of 40%, 20%, 10%, 5% and 2.5%. By using our generator, we generate <b>minority</b> samples from novel multivariate distribution; train the designed discriminator for the balanced datasets and measure the balanced accuracy over the ...", "dateLastCrawled": "2021-12-09T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Imbalanced K-Means: An algorithm to cluster imbalanced-distributed <b>data</b>", "url": "https://www.erpublication.org/published_paper/IJETR021251.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.erpublication.org/published_paper/IJETR021251.pdf", "snippet": "create a bias toward the <b>minority</b> <b>class</b> or introduce costs in the learning process to compensate the <b>minority</b> <b>class</b>. \u2022 External approaches acting on the <b>data</b>. These algorithms act on the <b>data</b> instead of the learning method. They have the advantage of being independent from the classifier used.", "dateLastCrawled": "2022-02-03T02:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Imbalanced <b>Data</b> Learning by <b>Minority</b> <b>Class</b> Augmentation using Capsule ...", "url": "https://deepai.org/publication/imbalanced-data-learning-by-minority-class-augmentation-using-capsule-adversarial-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/imbalanced-<b>data</b>-learning-by-<b>minority</b>-<b>class</b>-augmentation...", "snippet": "The training <b>dataset</b> is divided into majority and <b>minority</b> <b>class</b> sample components. Our central idea is to learn these components jointly in order to approximate the <b>data</b> distribution for the <b>minority</b> samples. We consider the density <b>of data</b> <b>points</b> of random <b>class</b> c, as p c (f) with a threshold \u03b4 c, and f c be the subset <b>of data</b> on the condition, f c = f: p c (k) &gt; \u03b4 c; \u03b4 c is positive (&gt; 0), such that the f c disjoints with the boundary margin if, (\u03b4 c) c c = 1. In practice, it is ...", "dateLastCrawled": "2022-01-04T08:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification of Imbalanced <b>Data</b> Using Deep Learning with Adding Noise", "url": "https://www.hindawi.com/journals/js/2021/1735386/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/js/2021/1735386", "snippet": "This paper proposes a method to treat the classification of imbalanced <b>data</b> by adding noise to the feature space of convolutional neural network (CNN) without changing a <b>data set</b> (ratio of majority and <b>minority</b> <b>data</b>). Besides, a hybrid loss function of crossentropy and KL divergence is proposed. The proposed approach can improve the accuracy of <b>minority</b> <b>class</b> in the testing <b>data</b>. In addition, a simple design method for selecting structure of CNN is first introduced and then, we add noise in ...", "dateLastCrawled": "2022-01-30T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Survey on <b>deep learning</b> with <b>class</b> <b>imbalance</b> | Journal of Big <b>Data</b> ...", "url": "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0192-5", "isFamilyFriendly": true, "displayUrl": "https://journalofbig<b>data</b>.springeropen.com/articles/10.1186/s40537-019-0192-5", "snippet": "In addition to varying the <b>class</b> size, the <b>different</b> distributions also varied the number of <b>minority</b> classes, where a <b>minority</b> <b>class</b> is any <b>class</b> smaller than the largest <b>class</b>. For example, a major 50\u201350 split (Dist. 3) reduced five of the classes to 6% <b>of the data set</b> size and increased five of the classes to 14%. As another example, a major singular over-representation (Dist. 5) increased the size of the airplane <b>class</b> to 14.5%, reducing the other nine classes slightly to 9.5%.", "dateLastCrawled": "2022-01-31T15:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "8 <b>Tactics to Combat Imbalanced Classes</b> in Your Machine Learning <b>Dataset</b>", "url": "https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/tactics", "snippet": "You will have more and <b>different</b> <b>data</b>, but the non-linear relationships between the attributes may not be preserved. There are systematic algorithms that you can use to generate synthetic samples. The most popular of such algorithms is called SMOTE or the Synthetic <b>Minority</b> Over-sampling Technique. As its name suggests, SMOTE is an oversampling method. It works by creating synthetic samples from the minor <b>class</b> instead of creating copies. The algorithm selects two or more <b>similar</b> instances ...", "dateLastCrawled": "2022-02-02T06:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Random Oversampling and Undersampling for Imbalanced Classification", "url": "https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/random-oversampling-and-undersampling-for...", "snippet": "This means that examples from the <b>minority</b> <b>class</b> <b>can</b> be chosen and added to the new ... For example, if we set sampling_strategy to 0.5 in an imbalanced <b>data</b> <b>dataset</b> with 1,000 examples in the majority <b>class</b> and 100 examples in the <b>minority</b> <b>class</b>, then there would be 200 examples for the majority <b>class</b> in the transformed <b>dataset</b> (or 100/200 = 0.5). 1. 2. 3... # define undersample strategy. undersample = RandomUnderSampler (sampling_strategy = 0.5) This might be preferred to ensure that the ...", "dateLastCrawled": "2022-02-03T00:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Why Balancing Classes is Over-Hyped | by Gabe Verzino | Towards <b>Data</b> ...", "url": "https://towardsdatascience.com/why-balancing-classes-is-over-hyped-e382a8a410f7", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/why-balancing-<b>class</b>es-is-over-hyped-e382a8a410f7", "snippet": "Conversely, over-sampling the <b>minority</b> <b>class</b> <b>can</b> unintentionally expand hidden <b>data</b> anomalies and create over-fitting. SMOTE is another resampling technique that <b>can</b> improve decision boundaries [2], but predictive enhancement is minor if new <b>data</b> contains similar properties as labeled samples in the <b>minority</b> <b>class</b> [3]. SMOTE also tends to suffer in highly-dimensional datasets because it does not take into account all neighboring classes and <b>can</b> result in an overlap of important <b>class</b> ...", "dateLastCrawled": "2022-01-29T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "SMOTE for Imbalanced Classification with Python", "url": "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/smote-oversampling-for-imbalanc", "snippet": "This <b>can</b> be achieved by simply duplicating examples from the <b>minority</b> <b>class</b> in the training <b>dataset</b> prior to fitting a model. This <b>can</b> balance the <b>class</b> distribution but does not provide any additional information to the model. An improvement on duplicating examples from the <b>minority</b> <b>class</b> is to synthesize new examples from the <b>minority</b> <b>class</b>. This is a type <b>of data</b> augmentation for tabular <b>data</b> and <b>can</b> be very effective. Perhaps the most widely used approach to synthesizing new examples is ...", "dateLastCrawled": "2022-02-02T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Imbalanced <b>data</b> learning by <b>minority class augmentation using capsule</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231220312091", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231220312091", "snippet": "The methodology for evaluation is as follows: Firstly, we need to create an imbalanced <b>dataset</b>; thus for each <b>class</b>, we remove some part of that <b>class</b> (dropping a percentage of images for each <b>class</b> from the training set) then we create imbalanced <b>dataset</b> by a percentage of 40%, 20%, 10%, 5% and 2.5%. By using our generator, we generate <b>minority</b> samples from novel multivariate distribution; train the designed discriminator for the balanced datasets and measure the balanced accuracy over the ...", "dateLastCrawled": "2021-12-09T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Imbalanced <b>Data</b> Learning by <b>Minority</b> <b>Class</b> Augmentation using ...", "url": "https://www.researchgate.net/publication/340475101_Imbalanced_Data_Learning_by_Minority_Class_Augmentation_using_Capsule_Adversarial_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340475101_Imbalanced_<b>Data</b>_Learning_by...", "snippet": "ation is as follows: Firstly, we need to create an imbalanced. <b>dataset</b>; thus for each <b>class</b>, we remov e some part of that. <b>class</b> (dropping a percentage of images for each <b>class</b> from. the training ...", "dateLastCrawled": "2021-12-12T19:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Imbalanced <b>Data</b> Learning by <b>Minority</b> <b>Class</b> Augmentation using Capsule ...", "url": "https://deepai.org/publication/imbalanced-data-learning-by-minority-class-augmentation-using-capsule-adversarial-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/imbalanced-<b>data</b>-learning-by-<b>minority</b>-<b>class</b>-augmentation...", "snippet": "The training <b>dataset</b> is divided into majority and <b>minority</b> <b>class</b> sample components. Our central idea is to learn these components jointly in order to approximate the <b>data</b> distribution for the <b>minority</b> samples. We consider the density <b>of data</b> <b>points</b> of random <b>class</b> c, as p c (f) with a threshold \u03b4 c, and f c be the subset <b>of data</b> on the condition, f c = f: p c (k) &gt; \u03b4 c; \u03b4 c is positive (&gt; 0), such that the f c disjoints with the boundary margin if, (\u03b4 c) c c = 1. In practice, it is ...", "dateLastCrawled": "2022-01-04T08:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Race-Ethnicity and Culture in the Family and Youth Outcomes: Test of a ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3941842/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3941842", "snippet": "Given that the <b>data</b> provided only two time <b>points</b>, it seemed reasonable to not model too many mediating pathways. In addition, we left out other factors that are less salient to this specific sample but are part of racial-ethnic socialization. For example, neighborhoods and regions of residence, varying in racial-ethnic compositions, determine experience of race-ethnicity. The sample we used is mainly from the Midwest and predominantly living in suburbs, so we did not include a neighborhood ...", "dateLastCrawled": "2022-02-02T20:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "machine learning - Calculating <b>F-Score</b>, which is the &quot;positive&quot; <b>class</b> ...", "url": "https://stats.stackexchange.com/questions/191645/calculating-f-score-which-is-the-positive-class-the-majority-or-minority-cla", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/191645", "snippet": "I&#39;m just making these numbers up for the sake of understanding what the correct convention is with regard to computing the <b>F-Score</b>. Fundamentally it&#39;s just a skewed <b>dataset</b>. I <b>can</b> compute the <b>F-Score</b> two ways, considering the true-postiive as a measure of the healthy, majority <b>class</b>, or as the unhealthy, <b>minority</b> <b>class</b>. $\\endgroup$ \u2013", "dateLastCrawled": "2022-01-24T21:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "classification - <b>Data Science Stack Exchange</b>", "url": "https://datascience.stackexchange.com/questions/57902/oversampling-only-balances-the-training-set-what-about-the-testing-set", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/57902/oversampling-only-balances-the...", "snippet": "Also, I have tried SMOTE, SMOTE-NC, and <b>Class</b>_weight to oversample my training set. To increase the chance of having more <b>data</b> from the <b>minority</b> <b>class</b>, I changed the 10-fold to 5-fold cross-validation (when developing the models), no improvement! PS: I have &gt;100K <b>data</b> <b>points</b> in my <b>dataset</b>.", "dateLastCrawled": "2022-01-26T04:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: <b>Data</b> ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "This is an imbalanced <b>dataset</b> and the ratio of <b>Class</b>-1 to <b>Class</b>-2 instances is 80:20 or more concisely 4:1. You <b>can</b> have a <b>class</b> imbalance problem on two-<b>class</b> classification problems as well as multi-<b>class</b> classification problems. Most techniques <b>can</b> be used on either. The remaining discussions will assume a two-<b>class</b> classification problem because it is easier to think about and describe. <b>Can</b> You Collect More <b>Data</b>? A larger <b>dataset</b> might expose a <b>different</b> and perhaps more balanced ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Multiclass <b>Classification</b> with Imbalanced <b>Dataset</b> - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/machine-learning-multi<b>class</b>-<b>classification</b>-with...", "snippet": "Imbalanced <b>Dataset</b>: Imbalanced <b>data</b> typically refers to a problem with <b>classification</b> problems where the classes are not represented equally. For example, you may have a 3-<b>class</b> <b>classification</b> problem of set of fruits to classify as oranges, apples or pears with total 100 instances . A total of 80 instances are labeled with <b>Class</b>-1 (Oranges), 10 instances with <b>Class</b>-2 (Apples) and the remaining 10 instances are labeled with <b>Class</b>-3 (Pears). This is an imbalanced <b>dataset</b> and the ratio of 8:1 ...", "dateLastCrawled": "2022-02-02T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Survey on <b>deep learning</b> with <b>class</b> <b>imbalance</b> | Journal of Big <b>Data</b> ...", "url": "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0192-5", "isFamilyFriendly": true, "displayUrl": "https://journalofbig<b>data</b>.springeropen.com/articles/10.1186/s40537-019-0192-5", "snippet": "In addition to varying the <b>class</b> size, the <b>different</b> distributions also varied the number of <b>minority</b> classes, where a <b>minority</b> <b>class</b> is any <b>class</b> smaller than the largest <b>class</b>. For example, a major 50\u201350 split (Dist. 3) reduced five of the classes to 6% <b>of the data set</b> size and increased five of the classes to 14%. As another example, a major singular over-representation (Dist. 5) increased the size of the airplane <b>class</b> to 14.5%, reducing the other nine classes slightly to 9.5%.", "dateLastCrawled": "2022-01-31T15:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Boosting methods for multi-<b>class</b> <b>imbalanced</b> <b>data</b> classification: an ...", "url": "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00349-y", "isFamilyFriendly": true, "displayUrl": "https://journalofbig<b>data</b>.springeropen.com/articles/10.1186/s40537-020-00349-y", "snippet": "A multi-<b>class</b> problem <b>can</b> have <b>different</b> styles, ... a <b>class</b> <b>can</b> be considered as a majority one <b>compared</b> to other classes, but it is considered as a <b>minority</b> or well-balanced one for the <b>rest</b> of the classes . Furthermore, the difficulty increases in the case of multi-<b>class</b> <b>imbalanced</b> datasets. For handling this unwanted circumstance, researchers have proposed some methods that <b>can</b> be classified into two major groups: <b>data</b>-level methods and algorithm-level methods. The details of these ...", "dateLastCrawled": "2022-02-02T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Random Oversampling and Undersampling for Imbalanced Classification", "url": "https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/random-oversampling-and-undersampling-for...", "snippet": "Resampling involves creating a new transformed version of the training <b>dataset</b> in which the selected examples have a <b>different</b> <b>class</b> distribution. This is a simple and effective strategy for imbalanced classification problems. Applying re-sampling strategies to obtain a more balanced <b>data</b> distribution is an effective solution to the imbalance problem \u2014 A Survey of Predictive Modelling under Imbalanced Distributions, 2015. The simplest strategy is to choose examples for the transformed ...", "dateLastCrawled": "2022-02-03T00:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Imbalanced <b>data</b> learning by <b>minority class augmentation using capsule</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0925231220312091", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231220312091", "snippet": "The methodology for evaluation is as follows: Firstly, we need to create an imbalanced <b>dataset</b>; thus for each <b>class</b>, we remove some part of that <b>class</b> (dropping a percentage of images for each <b>class</b> from the training set) then we create imbalanced <b>dataset</b> by a percentage of 40%, 20%, 10%, 5% and 2.5%. By using our generator, we generate <b>minority</b> samples from novel multivariate distribution; train the designed discriminator for the balanced datasets and measure the balanced accuracy over the ...", "dateLastCrawled": "2021-12-09T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "SMOTE for Imbalanced Classification with Python", "url": "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/smote-oversampling-for-imbalanc", "snippet": "This <b>can</b> be achieved by simply duplicating examples from the <b>minority</b> <b>class</b> in the training <b>dataset</b> prior to fitting a model. This <b>can</b> balance the <b>class</b> distribution but does not provide any additional information to the model. An improvement on duplicating examples from the <b>minority</b> <b>class</b> is to synthesize new examples from the <b>minority</b> <b>class</b>. This is a type <b>of data</b> augmentation for tabular <b>data</b> and <b>can</b> be very effective. Perhaps the most widely used approach to synthesizing new examples is ...", "dateLastCrawled": "2022-02-02T13:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Imbalanced <b>Data</b> Learning by <b>Minority</b> <b>Class</b> Augmentation using Capsule ...", "url": "https://deepai.org/publication/imbalanced-data-learning-by-minority-class-augmentation-using-capsule-adversarial-networks", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/imbalanced-<b>data</b>-learning-by-<b>minority</b>-<b>class</b>-augmentation...", "snippet": "We consider the density <b>of data</b> <b>points</b> of random <b>class</b> c, as p c (f) with a threshold \u03b4 c, and f c be the subset <b>of data</b> on the condition, f c = f: p c (k) &gt; \u03b4 c; \u03b4 c is positive (&gt; 0), such that the f c disjoints with the boundary margin if, (\u03b4 c) c c = 1. In practice, it is difficult to access the <b>minority</b> sample distribution, because there are fewer samples in the training set. According to [36] using mixture distribution for the <b>minority</b> <b>class</b> samples will be in the form of", "dateLastCrawled": "2022-01-04T08:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "ANALYSIS OF SAMPLING TECHNIQUES FOR IMBALANCED <b>DATA</b>: AN N=648 ADNI STUDY", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3946903/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3946903", "snippet": "A <b>dataset</b> is said to be imbalanced if there are significantly more <b>data</b> <b>points</b> of one <b>class</b> and fewer occurrences of the other <b>class</b>. For example, the number of control cases in the ADNI <b>dataset</b> is half of the number of AD cases for proteomics measurement, whereas for MRI modality, there are 40% more control cases than AD cases. <b>Data</b> imbalance is also ubiquitous in worldwide ADNI type initiatives from Europe, Japan and Australia, etc. Weiner et al., 2012). In addition, lots of medical ...", "dateLastCrawled": "2022-01-18T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Oversampling the <b>Minority</b> <b>Class</b> in the Feature Space", "url": "https://www.researchgate.net/publication/281307947_Oversampling_the_Minority_Class_in_the_Feature_Space", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/281307947_Oversampling_the_<b>Minority</b>_<b>Class</b>_in...", "snippet": "Note that when \u03b2 &lt; 0, <b>points</b> deep within the <b>minority</b> <b>class</b> (in the feature space) are more likely to be picked; when \u03b2 &gt; 0 , <b>points</b> closer to the <b>class</b> boundary or lying inside the", "dateLastCrawled": "2022-01-27T05:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "classification - <b>Data Science Stack Exchange</b>", "url": "https://datascience.stackexchange.com/questions/57902/oversampling-only-balances-the-training-set-what-about-the-testing-set", "isFamilyFriendly": true, "displayUrl": "https://<b>datascience.stackexchange</b>.com/questions/57902/oversampling-only-balances-the...", "snippet": "The testing set is still highly skewed and has only 1% of my positive <b>class</b>. I am using XGBoost, Random Forest, Logistic Regression, and KNN for the classification task. Also, I have tried SMOTE, SMOTE-NC, and <b>Class</b>_weight to oversample my training set. To increase the chance of having more <b>data</b> from the <b>minority</b> <b>class</b>, I changed the 10-fold to 5-fold cross-validation (when developing the models), no improvement! PS: I have &gt;100K <b>data</b> <b>points</b> in my <b>dataset</b>. classification cross-validation ...", "dateLastCrawled": "2022-01-26T04:48:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> \u2014 Multiclass <b>Classification</b> with Imbalanced Dataset ...", "url": "https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine</b>-<b>learning</b>-multi<b>class</b>-<b>classification</b>-with...", "snippet": "The skewed distribution makes many conventional <b>machine</b> <b>learning</b> algorithms less effective, especially in predicting <b>minority</b> <b>class</b> examples. In order to do so, let us first understand the problem at hand and then discuss the ways to overcome those. Multiclass <b>Classification</b>: A <b>classification</b> task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multi-<b>class</b> <b>classification</b> makes the assumption that each sample is assigned to one and ...", "dateLastCrawled": "2022-02-02T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Reliable and explainable machine-learning</b> methods for accelerated ...", "url": "https://www.nature.com/articles/s41524-019-0248-2", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41524-019-0248-2", "snippet": "However, in practice, correctly classifying and <b>learning</b> from the <b>minority</b> <b>class</b> of interest may be more important than possibly misclassifying the majority classes. Fig. 1", "dateLastCrawled": "2022-02-02T00:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Techniques to handle <b>class</b> imbalance using python", "url": "https://www.letthedataconfess.com/blog/2020/06/10/techniques-to-handle-class-imbalance/", "isFamilyFriendly": true, "displayUrl": "https://www.letthedataconfess.com/blog/2020/06/10/techniques-to-handle-<b>class</b>-imbalance", "snippet": "Cost Sensitive <b>Learning</b>. Another approach to deal with <b>class</b> imbalance is cost function is modified in such a way that penalty for misclassification of <b>minority</b> instances will be more. In the sklearn library, there is one argument \u201c<b>class</b> weight\u201d. Using this argument, we can penalize the <b>minority</b> <b>class</b> according to how much less proportion ...", "dateLastCrawled": "2022-01-27T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Evaluation of Supervised and Unsupervised <b>Machine</b> <b>Learning</b> Classifiers ...", "url": "https://link.springer.com/chapter/10.1007/978-3-030-74753-4_11", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-74753-4_11", "snippet": "We used Synthetic <b>Minority</b> Over-sampling Technique (SMOTE) for the upsampling of <b>minority</b> <b>class</b> and train the classifiers with a balanced dataset. The experiment results show that the balanced dataset reduces bias towards the majority <b>class</b> and increases the <b>machine</b> <b>learning</b> classifiers\u2019 accuracy. Using this approach, we successfully achieved higher accuracy for five <b>machine</b> <b>learning</b> algorithms with a low false-positive rate.", "dateLastCrawled": "2022-01-09T21:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A method for solving the <b>class</b> imbalance Problem in Classification ...", "url": "http://www.ijsrd.com/articles/IJSRDV2I4101.pdf", "isFamilyFriendly": true, "displayUrl": "www.ijsrd.com/articles/IJSRDV2I4101.pdf", "snippet": "attention in areas such as <b>Machine</b> <b>Learning</b> and Pattern Recognition. A two-<b>class</b> dataset is said to be imbalanced when one of the classes (the <b>minority</b> one) is heavily under- represented in comparison to the other <b>class</b> (the majority one) .The resulting model (classier) will Enable us to predict the outcome for new unseen examples. We describe the basic classification techniques. Several major kinds of classification method including Decision tree induction, Bayesian networks, K-nearest ...", "dateLastCrawled": "2022-01-11T06:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>machine</b> <b>learning</b> - <b>Logistic regression on biased data</b> - Data Science ...", "url": "https://datascience.stackexchange.com/questions/12234/logistic-regression-on-biased-data", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/12234", "snippet": "<b>machine</b>-<b>learning</b> r logistic-regression. Share. Improve this question. Follow asked Jun 16 &#39;16 at 12:54. Anonymint Anonymint. 155 2 2 ... (<b>analogy</b>, decision trees, bagging, Bayesian). Finally... Unbalanced Classes. There are two typical methods for dealing with unbalanced classes. These include oversampling the <b>minority</b> <b>class</b>, and fixing the model by altering the hyperplane (SVM) or changing priors (Bayes). There are lots of summaries of this problem and solution if you search for &quot;unbalanced ...", "dateLastCrawled": "2022-02-01T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Values and inductive risk in <b>machine</b> <b>learning</b> modelling: the case of ...", "url": "https://link.springer.com/article/10.1007/s13194-021-00405-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s13194-021-00405-1", "snippet": "For instance, in medical applications, patients having cancer constitute the <b>minority</b> <b>class</b> in a given population, while those not having cancer constitutes the majority <b>class</b>. The cost of a false negative, i.e., misclassifying a cancer case as non-cancer, has a much higher cost than a false positive, i.e., misclassifying a non-cancer case as cancer. This is because the former case might result in the delay of the treatment of the case, which is a life-threatening situation, while the former ...", "dateLastCrawled": "2022-01-04T15:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "From <b>imbalanced</b> datasets to boosting algorithms | by Linda Chen ...", "url": "https://towardsdatascience.com/from-imbalanced-dataset-to-boosting-algorithms-1-2-798cd6384ecc", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/from-<b>imbalanced</b>-dataset-to-boosting-algorithms-1-2-798...", "snippet": "Tools Overview. Downsampling: randomly select some points from the majority <b>class</b> and delete them. Upsampling: randomly select a point from the <b>minority</b> <b>class</b>, copy and paste it to make a new point. Repeat the process until you have the same amount of samples as the majority <b>class</b>. SMOTE: it creates more samples in the <b>minority</b> <b>class</b>. However, not by replicating the existing data points but by creating new points within the range of possibility.", "dateLastCrawled": "2022-01-28T07:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - When should I balance classes in a training data set ...", "url": "https://stats.stackexchange.com/questions/227088/when-should-i-balance-classes-in-a-training-data-set", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/227088", "snippet": "The <b>class</b> imbalance problem is caused by there not being enough patterns belonging to the <b>minority</b> <b>class</b>, not by the ratio of positive and negative patterns itself per se. Generally if you have enough data, the &quot;<b>class</b> imbalance problem&quot; doesn&#39;t arise. As a conclusion, artificial balancing is rarely useful if training set is large enough.", "dateLastCrawled": "2022-01-28T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "In <b>machine learning, what\u2019s the purpose of splitting data up</b> into test ...", "url": "https://www.quora.com/In-machine-learning-what-s-the-purpose-of-splitting-data-up-into-test-sets-and-training-sets", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-<b>machine-learning-what-s-the-purpose-of-splitting-data-up</b>-into...", "snippet": "Answer (1 of 13): Naturally, the concept of train, validation, and test influences the way you should process your data as you are getting ready for training and deployment of your computer vision model. Preprocessing steps are image transformations that are used to standardize your dataset acro...", "dateLastCrawled": "2022-01-26T16:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Prediction of Web Service Anti-patterns Using Aggregate Software ...", "url": "https://www.researchgate.net/publication/340138873_Prediction_of_Web_Service_Anti-patterns_Using_Aggregate_Software_Metrics_and_Machine_Learning_Techniques", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340138873_Prediction_of_Web_Service_Anti...", "snippet": "Prediction of Web Service Anti-pa erns Using Aggregate So ware Metrics and <b>Machine</b> <b>Learning</b> T echniques ISEC 2020, February 27\u201329, 2020, Jabalpur, India the metrics respectively. Figure 3, 6 and ...", "dateLastCrawled": "2021-11-27T20:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs</b> ...", "url": "https://deepai.org/publication/evolvegcn-evolving-graph-convolutional-networks-for-dynamic-graphs", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>evolvegcn-evolving-graph-convolutional-networks-for</b>...", "snippet": "Code Repositories EvolveGCN. Code for <b>EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs</b>. view repo AMLSim. The AMLSim project is intended to provide a multi-agent based simulator that generates synthetic banking transaction data together with a set of known money laundering patterns - mainly for the purpose of testing <b>machine</b> <b>learning</b> models and graph algorithms.", "dateLastCrawled": "2022-01-31T23:44:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(minority class)  is like +(group of data points that are different from the rest of the data points in a dataset)", "+(minority class) is similar to +(group of data points that are different from the rest of the data points in a dataset)", "+(minority class) can be thought of as +(group of data points that are different from the rest of the data points in a dataset)", "+(minority class) can be compared to +(group of data points that are different from the rest of the data points in a dataset)", "machine learning +(minority class AND analogy)", "machine learning +(\"minority class is like\")", "machine learning +(\"minority class is similar\")", "machine learning +(\"just as minority class\")", "machine learning +(\"minority class can be thought of as\")", "machine learning +(\"minority class can be compared to\")"]}