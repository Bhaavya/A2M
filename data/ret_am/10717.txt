{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "SparseX: A <b>Library</b> for High-Performance <b>Sparse</b> Matrix-<b>Vector</b> ...", "url": "https://dl.acm.org/doi/abs/10.1145/3134442", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/abs/10.1145/3134442", "snippet": "The <b>library</b>&#39;s kernels are based on the application of CSX for <b>sparse</b> matrices and are used to prepare a high-performance <b>sparse</b> matrix-<b>vector</b> multiplication code (written in the C/C++ language), which can be used in different high-level <b>sparse</b> solvers for systems of linear algebraic equations via iterative methods. The authors of the paper are trying to a) provide simple and clear semantics; b) serve users with different levels of expertise; c) facilitate the integration of their kernels in ...", "dateLastCrawled": "2022-01-04T00:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Data Representation in NLP. What is Vectorization ? | by Shivangi ...", "url": "https://shiivangii.medium.com/data-representation-in-nlp-7bb6a771599a", "isFamilyFriendly": true, "displayUrl": "https://shiivangii.medium.com/data-representation-in-nlp-7bb6a771599a", "snippet": "In <b>Sparse</b> <b>Vector</b> representations (Latent Semantic Analysis) ... Suppose If I write the sentence \u201cHe loved <b>books</b>. Education is best found in <b>books</b>\u201d. It would create two vectors one for \u201cHe loved <b>books</b>\u201d and other for \u201cEducation is best found in <b>books</b>.\u201d It would treat both of them orthogonal which makes them independent, but in reality, they are related to each other ; To overcome these limitations, Word Embeddings is developed. Word2Vec is an approach to implement such. Word2Vec ...", "dateLastCrawled": "2022-02-03T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparse Matrix-Vector Multiplication</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/sparse-matrix-vector-multiplication", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>sparse-matrix-vector-multiplication</b>", "snippet": "<b>Sparse matrix-vector multiplication</b> (SpMV) is a fundamental computational kernel used in scientific and engineering applications. The nonzero elements of <b>sparse</b> matrices are represented in different formats, and a single <b>sparse</b> matrix representation is not suitable for all <b>sparse</b> matrices with different sparsity patterns. Extensive studies have been done on improving the performance of <b>sparse</b> matrices processing on different platforms. Graphics processing units (GPUs) are very well suited ...", "dateLastCrawled": "2022-01-28T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sparse, Dense, and Attentional Representations for</b> Text ... - <b>Books</b> Gateway", "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00369/100684/Sparse-Dense-and-Attentional-Representations-for", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00369/100684", "snippet": "Next, we offer a multi-<b>vector</b> encoding model, which is computationally feasible for retrieval <b>like</b> the dual-encoder architecture and achieves significantly better quality. A simple hybrid that interpolates models based on dense and <b>sparse</b> representations leads to further improvements. We compare the performance of dual encoders, multi-<b>vector</b> encoders, and their <b>sparse</b>-dense hybrids with classical <b>sparse</b> retrieval models and attentional neural networks, as well as state-of-the-art published ...", "dateLastCrawled": "2022-02-01T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Review on <b>Sparse</b> Matrix Storage Formats With Space Complexity Analysis ...", "url": "https://www.igi-global.com/chapter/review-on-sparse-matrix-storage-formats-with-space-complexity-analysis/265582", "isFamilyFriendly": true, "displayUrl": "https://www.igi-global.com/chapter/review-on-<b>sparse</b>-matrix-storage-formats-with-space...", "snippet": "<b>Sparse</b> matrix-<b>vector</b> multiplication (SpMV) is a challenging computational kernel in linear algebra applications, <b>like</b> data mining, image processing, and machine learning. The performance of this kernel is greatly dependent on the size of the input matrix and the underlying hardware features. Various <b>sparse</b> matrix storage formats referred to commonly as <b>sparse</b> formats have been proposed in the literature to reduce the size of the matrix. In modern multi-core and many-core architectures, the ...", "dateLastCrawled": "2022-01-01T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Plotting <b>Library</b> Catalog Subjects | Data Science Portfolio", "url": "https://sourestdeeds.github.io/blog/library-catalog-subject/", "isFamilyFriendly": true, "displayUrl": "https://sourestdeeds.github.io/blog/<b>library</b>-catalog-subject", "snippet": "For word vectors, one problem is that words <b>like</b> \u201cthe\u201d occur with all subjects equally (or for <b>library</b> catalog subject vectors, maybe \u201cLarge print <b>books</b>\u201d co-occur with many other subjects). These subjects don\u2019t provide more information about the interesting subjects. So instead of using the counts directly, I\u2019ll use PPMI, which is higher if the words co-occur more often than chance. It is a function of the co-occurrence counts and global counts.", "dateLastCrawled": "2022-01-29T17:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "4. Text Vectorization and Transformation Pipelines - Applied Text ...", "url": "https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/<b>library</b>/view/applied-text-analysis/9781491963036/ch04.html", "snippet": "The dictionary object can be loaded or saved to disk, and implements a doc2bow <b>library</b> that accepts a pretokenized document and returns a <b>sparse</b> matrix of (id, count ) tuples where the id is the token\u2019s id in the dictionary. Because the doc2bow method only takes a single document instance, we use the list comprehension to restore the entire corpus, loading the tokenized documents into memory so we don\u2019t exhaust our generator: import gensim corpus = [tokenize(doc) for doc in corpus ...", "dateLastCrawled": "2022-02-01T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Introduction to Sparse Matrices in</b> R - Python and R Tips", "url": "https://cmdlinetips.com/2019/05/introduction-to-sparse-matrices-in-r/", "isFamilyFriendly": true, "displayUrl": "https://cmdlinetips.com/2019/05/<b>introduction-to-sparse-matrices-in</b>-r", "snippet": "Let us create a matrix with <b>sparse</b> data from scratch. We will first create data, a <b>vector</b> with million random numbers from normal distribution with zero mean and unit variance. data &lt;- rnorm(1e6) The above data <b>vector</b> is not <b>sparse</b> and contains data in all elements. Let us randomly select the indices and make them to contain zeroes. data &lt;- rnorm(1e6) zero_index &lt;- sample(1e6)[1:9e5] data[zero_index] &lt;- 0 Now we have created a <b>vector</b> of million elements, but 90% of the elements are zeros ...", "dateLastCrawled": "2022-01-28T12:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is there a good PHP <b>library</b> for vectors and matrices? - Stack Overflow", "url": "https://stackoverflow.com/questions/4828237/is-there-a-good-php-library-for-vectors-and-matrices", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/4828237", "snippet": "The <b>library</b> I&#39;m looking for should include functions <b>like</b> matrix multiplication, <b>vector</b> cosine similarity, convolution etc. I also expect such a <b>library</b> to have a proper compact internal representation of <b>sparse</b> vectors and matrices. I&#39;m using PHP 5.2. php <b>vector</b> matrix. Share. Improve this question. Follow edited Jan 29 &#39;11 at 21:15. snakile. asked Jan 28 &#39;11 at 12:32. snakile snakile. 48.9k 60 60 gold badges 163 163 silver badges 233 233 bronze badges. 3. possible duplicate of Is there a ...", "dateLastCrawled": "2021-12-05T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - <b>numpy/scipy equivalent of MATLAB&#39;s sparse</b> function - Stack ...", "url": "https://stackoverflow.com/questions/40890960/numpy-scipy-equivalent-of-matlabs-sparse-function", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/40890960", "snippet": "The scipy.<b>sparse</b> equivalent is csr_matrix ( (SV, (I, J))) -- yes, a single argument which is a 2-tuple containing a <b>vector</b> and a 2-tuple of vectors. You also have to correct the index vectors because Python consistently uses 0-based indexing. Note that scipy, unlike Matlab, does not automatically discard explicit zeroes, and will use integer ...", "dateLastCrawled": "2022-01-26T02:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Data Representation in NLP. What is Vectorization ? | by Shivangi ...", "url": "https://shiivangii.medium.com/data-representation-in-nlp-7bb6a771599a", "isFamilyFriendly": true, "displayUrl": "https://shiivangii.medium.com/data-representation-in-nlp-7bb6a771599a", "snippet": "In <b>Sparse</b> <b>Vector</b> representations (Latent Semantic Analysis) ... Suppose If I write the sentence \u201cHe loved <b>books</b>. Education is best found in <b>books</b>\u201d. It would create two vectors one for \u201cHe loved <b>books</b>\u201d and other for \u201cEducation is best found in <b>books</b>.\u201d It would treat both of them orthogonal which makes them independent, but in reality, they are related to each other ; To overcome these limitations, Word Embeddings is developed. Word2Vec is an approach to implement such. Word2Vec ...", "dateLastCrawled": "2022-02-03T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Sparse Matrix-Vector Multiplication</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/sparse-matrix-vector-multiplication", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>sparse-matrix-vector-multiplication</b>", "snippet": "<b>Sparse matrix-vector multiplication</b> (SpMV) is a fundamental computational kernel used in scientific and engineering applications. The nonzero elements of <b>sparse</b> matrices are represented in different formats, and a single <b>sparse</b> matrix representation is not suitable for all <b>sparse</b> matrices with different sparsity patterns. Extensive studies have been done on improving the performance of <b>sparse</b> matrices processing on different platforms. Graphics processing units (GPUs) are very well suited ...", "dateLastCrawled": "2022-01-28T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Computing the <b>sparse</b> matrix <b>vector</b> product using block-based kernels ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7924463/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7924463", "snippet": "The <b>sparse</b> matrix-<b>vector</b> product (SpMV) is an important operation in many applications, which often needs to be performed multiple times in the course of the algorithm. It is often the case that no matter how sophisticated the particular algorithm is, most of the CPU time is spent in matrix-<b>vector</b> product evaluations. The prominent examples are iterative solvers based on Krylov subspaces, such as the popular CG method. Here the solution <b>vector</b> is found after multiple matrix-<b>vector</b> ...", "dateLastCrawled": "2021-12-20T03:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sparse, Dense, and Attentional Representations for</b> Text ... - <b>Books</b> Gateway", "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00369/100684/Sparse-Dense-and-Attentional-Representations-for", "isFamilyFriendly": true, "displayUrl": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00369/100684", "snippet": "Retrieving relevant documents is a core task for language technology, and is a component of applications such as information extraction and question answering (e.g., Narasimhan et al., 2016; Kwok et al., 2001; Voorhees, 2001).While classical information retrieval has focused on heuristic weights for <b>sparse</b> bag-of-words representations (Sp\u00e4rck Jones, 1972), more recent work has adopted a two-stage retrieval and ranking pipeline, where a large number of documents are retrieved using <b>sparse</b> ...", "dateLastCrawled": "2022-02-01T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Plotting <b>Library</b> Catalog Subjects | Data Science Portfolio", "url": "https://sourestdeeds.github.io/blog/library-catalog-subject/", "isFamilyFriendly": true, "displayUrl": "https://sourestdeeds.github.io/blog/<b>library</b>-catalog-subject", "snippet": "One way to turn this into word vectors is to make <b>sparse</b> vectors of metrics based on co-occurring words, and then reduce the number of dimensions to get a dense <b>vector</b>. Plan Using this assumption with <b>library</b> catalog subjects, I can describe \u201cMusicians Fiction\u201d with the subjects \u201cBullfighters Fiction\u201d, \u201cBest friends Fiction\u201d, and so on.", "dateLastCrawled": "2022-01-29T17:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "6 most <b>commonly used Java Machine learning libraries</b> | Packt Hub", "url": "https://hub.packtpub.com/most-commonly-used-java-machine-learning-libraries/", "isFamilyFriendly": true, "displayUrl": "https://hub.packtpub.com/most-<b>commonly-used-java-machine-learning-libraries</b>", "snippet": "Dense vectors are presented as an array of double-typed values, for example, (2.0, 0.0, 1.0, 0.0); while <b>sparse</b> <b>vector</b> is presented by the size of the <b>vector</b>, an array of indices, and an array of values, for example, [4, (0, 2), (2.0, 1.0)]. Labeled point is used for supervised learning algorithms and consists of a local <b>vector</b> labeled with a double-typed class values. Label can be class index, binary outcome, or a list of multiple class indices (multiclass classification). For example, a ...", "dateLastCrawled": "2022-02-03T08:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Sparse</b> Representations And Compressive Sensing For Imaging And Vision ...", "url": "http://mail.nanocenter.org/sparse_representations_and_compressive_sensing_for_imaging_and_vision_springerbriefs_in_electrical_and_computer_engineering.pdf", "isFamilyFriendly": true, "displayUrl": "mail.nanocenter.org/<b>sparse</b>_representations_and_compressive_sensing_for_imaging_and...", "snippet": "Getting the <b>books</b> <b>sparse</b> representations and compressive sensing for imaging and vision springerbriefs in electrical and computer engineering now is not type of challenging means. You could not unaided going <b>similar</b> to ebook store or <b>library</b> or borrowing from your contacts to right to use them. This is an entirely simple means to specifically acquire lead by on-line. This online notice <b>sparse</b> representations and compressive sensing for imaging and vision springerbriefs in electrical and ...", "dateLastCrawled": "2022-02-02T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "4. Text Vectorization and Transformation Pipelines - Applied Text ...", "url": "https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/<b>library</b>/view/applied-text-analysis/9781491963036/ch04.html", "snippet": "The dictionary object can be loaded or saved to disk, and implements a doc2bow <b>library</b> that accepts a pretokenized document and returns a <b>sparse</b> matrix of (id, count) tuples where the id is the token \u2019s id in the dictionary. Because the doc2bow method only takes a single document instance, we use the list comprehension to restore the entire corpus, loading the tokenized documents into memory so we don\u2019t exhaust our generator: import gensim corpus = [tokenize(doc) for doc in corpus ...", "dateLastCrawled": "2022-02-01T20:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "S. Williams, L. Oliker, R. Vuduc, J. Shalf, K. Yelick &amp; J. Demmel ...", "url": "https://philpapers.org/rec/WILOOS-5", "isFamilyFriendly": true, "displayUrl": "https://philpapers.org/rec/WILOOS-5", "snippet": "Through your <b>library</b>. Only published works are available at libraries. References found in this work BETA. No references found. Add more references Citations of this work BETA. No citations found. Add more citations <b>Similar</b> <b>books</b> and articles. <b>Sparse</b> Matrix-<b>Vector</b> Multiplication on Multicore and Accelerators. S. W. Williams, N. Bell, J. Choi, M. Garland, L. Oliker &amp; R. Vuduc - unknown. Optimizing and Tuning the Fast Multipole Method for State-of-the-Art Multicore Architectures. A ...", "dateLastCrawled": "2022-01-01T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "1. Vectors, Matrices, and Arrays - Machine Learning with Python ...", "url": "https://www.oreilly.com/library/view/machine-learning-with/9781491989371/ch01.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/<b>library</b>/view/machine-learning-with/9781491989371/ch01.html", "snippet": "# Load <b>library</b> import numpy as np # Create a <b>vector</b> as a row <b>vector</b>_row = np.array([1, 2, 3]) # Create a <b>vector</b> as a column <b>vector</b>_column = np.array([[1], [2], [3]]) Discussion . NumPy\u2019s main data structure is the multidimensional array. To create a <b>vector</b>, we simply create a one-dimensional array. Just like vectors, these arrays can be represented horizontally (i.e., rows) or vertically (i.e., columns). See Also. Vectors, Math Is Fun. Euclidean <b>vector</b>, Wikipedia. 1.2 Creating a Matrix ...", "dateLastCrawled": "2022-02-03T02:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse</b> nonnegative solution of underdetermined linear equations by ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1172251/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC1172251", "snippet": "Consider an underdetermined system of linear equations y = Ax with known y and d \u00d7 n matrix A.We seek the nonnegative x with the fewest nonzeros satisfying y = Ax.In general, this problem is NP-hard. However, for many matrices A there is a threshold phenomenon: if the sparsest solution is sufficiently <b>sparse</b>, it <b>can</b> be found by linear programming. We explain this by the theory of convex polytopes.", "dateLastCrawled": "2022-01-21T06:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Sparse</b> Matrices for Machine Learning", "url": "https://machinelearningmastery.com/sparse-matrices-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>sparse</b>-matrices-for-machine-learning", "snippet": "<b>Books</b>. Introduction to Linear Algebra, Fifth Edition, 2016. Section 2.7 <b>Sparse</b> Linear Systems, Numerical Recipes: The Art of Scientific Computing, Third Edition, 2007. Artificial Intelligence: A Modern Approach, Third Edition, 2009. Direct Methods for <b>Sparse</b> Matrices, Second Edition, 2017. API. <b>Sparse</b> matrices (scipy.<b>sparse</b>) API; scipy.<b>sparse</b>.csr_matrix() API; numpy.count_nonzero() API; numpy.ndarray.size API; Articles. <b>Sparse</b> matrix on Wikipedia; Summary. In this tutorial, you discovered ...", "dateLastCrawled": "2022-02-02T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "linear algebra - How to define <b>sparseness</b> of a <b>vector</b>? - Mathematics ...", "url": "https://math.stackexchange.com/questions/117860/how-to-define-sparseness-of-a-vector", "isFamilyFriendly": true, "displayUrl": "https://math.stackexchange.com/questions/117860", "snippet": "Here, <b>Sparseness</b>(X) = 0 whenever the <b>vector</b> is dense (all components are equal and non-zero) and <b>Sparseness</b>(X) = 1 whenever the <b>vector</b> is <b>sparse</b> (only one component is non zero). This post only explains the when 0 and 1 achieved by the above mentioned measure. Is there any other function defining the <b>sparseness</b> of the <b>vector</b>.", "dateLastCrawled": "2022-01-25T19:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "PELLR: A Permutated ELLPACK-R Format for SpMV on GPUs", "url": "https://www.scirp.org/journal/paperinformation.aspx?paperid=99538", "isFamilyFriendly": true, "displayUrl": "https://www.scirp.org/journal/paperinformation.aspx?paperid=99538", "snippet": "The <b>sparse</b> matrix <b>vector</b> multiplication (SpMV) is inevitable in almost all kinds of scientific computation, such as iterative methods for solving linear systems and eigenvalue problems. With the emergence and development of Graphics Processing Units (GPUs), high efficient formats for SpMV should be constructed. The performance of SpMV is mainly determinted by the storage format for <b>sparse</b> matrix. Based on the idea of JAD format, this paper improved the ELLPACK-R format, reduced the waiting ...", "dateLastCrawled": "2022-02-01T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Sparse Matrix Computation</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/sparse-matrix-computation", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>sparse-matrix-computation</b>", "snippet": "For <b>sparse</b> matrices we <b>can</b> investigate certain properties depending only on the sparsity structure, that is, the information about what elements are different from zero regardless of their numerical value. This analysis is particularly relevant for irregular <b>sparse</b> matrices where zero elements <b>can</b> be present on the main diagonal. In order to analyze these properties, it is convenient to associate an incidence matrix to the matrix under consideration. The incidence matrix", "dateLastCrawled": "2022-01-24T19:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sparse autoencoder</b> - SlideShare", "url": "https://www.slideshare.net/DevashishPatel/sparse-autoencoder-95661509", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/DevashishPatel/<b>sparse-autoencoder</b>-95661509", "snippet": "Here, y <b>can</b> be <b>vector</b> valued. In the case of an autoencoder, y = x. (x(i) , y(i) ) The i-th training example hW,b(x) Output of our hypothesis on input x, using parameters W, b. This should be a <b>vector</b> of the same dimension as the target value y. W (l) ij The parameter associated with the connection between unit j in layer l, and unit i in layer l + 1. b (l) i The bias term associated with unit i in layer l + 1. <b>Can</b> also <b>be thought</b> of as the parameter associated with the connection between ...", "dateLastCrawled": "2022-01-08T07:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Efficient sparse matrix\u2013vector multiplication using cache oblivious</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167739X15000606", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167739X15000606", "snippet": "In this paper, we elaborate on improving the <b>sparse</b> matrix storage format to optimize the data locality of <b>sparse</b> matrix\u2013<b>vector</b> multiplication (S p M V M) algorithm, and its parallel performance.First of all, we propose a cache oblivious extension quadtree storage structure (C O E Q T), in which the <b>sparse</b> matrix is recursively divided into sub-regions that <b>can</b> well fit into cache to improve the data locality.Later on, we present a C O E Q T based S p M V M algorithm and optimize its ...", "dateLastCrawled": "2021-11-11T20:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "From the Cover: Modularity and community structure in networks", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1482622/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC1482622", "snippet": "As <b>can</b> be seen, one of these communities consists almost entirely of liberal <b>books</b> and one almost entirely of conservative <b>books</b>. Most of the centrist <b>books</b> fall in the two remaining communities. Thus the <b>books</b> appear to form communities of copurchasing that align closely with political views, a finding that encourages us to believe that the algorithm is capable of extracting meaningful results from raw network data. It is particularly interesting to note that the centrist <b>books</b> belong to ...", "dateLastCrawled": "2022-01-17T09:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Mahout</b> - SlideShare", "url": "https://www.slideshare.net/EdurekaIN/mahout-36625358", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/EdurekaIN/<b>mahout</b>-36625358", "snippet": "Vectors implementation in <b>Mahout</b> Dense <b>Vector</b> Sequential Access <b>Sparse</b> <b>Vector</b> Random Access <b>Sparse</b> <b>Vector</b> Vectors implementation in <b>Mahout</b> It <b>can</b> <b>be thought</b> of as an array of doubles, whose size is the number of features in the data. Because all the entries in the array are preallocated regardless of whether the value is 0 or not, we call it dense. It is implemented as a HashMap between an integer and a double, where only nonzero valued features are allocated. Hence, they\u2019re called as ...", "dateLastCrawled": "2022-02-03T03:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The 4 Recommendation Engines That <b>Can</b> Predict Your <b>Movie</b> Tastes | by ...", "url": "https://towardsdatascience.com/the-4-recommendation-engines-that-can-predict-your-movie-tastes-109dc4e10c52", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-4-recommendation-engines-that-<b>can</b>-predict-your...", "snippet": "I use the scikit-learn <b>library</b> to split the dataset into testing and training. ... It <b>can</b>\u2019t deal with <b>sparse</b> data, meaning it\u2019s hard to find users that have rated the same items. It suffers when new users or items that don\u2019t have any ratings enter the system. It tends to recommend popular items. Note: The complete code for content-based and memory-based collaborative filtering <b>can</b> be found in this Jupyter Notebook. Collaborative Filtering vs Content-Based Filtering 3 \u2014 Matrix ...", "dateLastCrawled": "2022-02-02T17:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Computing the <b>sparse</b> matrix <b>vector</b> product using block-based kernels ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7924463/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7924463", "snippet": "While the gain of using a <b>sparse</b> matrix instead of a dense one <b>can</b> be huge in terms of memory occupancy and speed, the effective Flop rate of a <b>sparse</b> kernel generally remains low <b>compared</b> to its dense counterpart. In fact, in a <b>sparse</b> matrix storage, we provide a way to know the respective column and row of each non-zero value (NNZ). Therefore, the general SpMV is a bandwidth/memory bound operation because it pays the price of this extra storage and leads to a low ratio of", "dateLastCrawled": "2021-12-20T03:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) A <b>library</b> for parallel <b>sparse</b> matrix-<b>vector</b> multiplies", "url": "https://www.researchgate.net/publication/228544585_A_library_for_parallel_sparse_matrix-vector_multiplies", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228544585_A_<b>library</b>_for_parallel_<b>sparse</b>...", "snippet": "Aztec is an iterative <b>library</b> that greatly simplifies the parallelization process when solving the linear systems of equations Ax = b where A is a user supplied n x n <b>sparse</b> matrix, b is a user ...", "dateLastCrawled": "2022-01-18T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparse</b> Solutions of Underdetermined Linear Systems ... - <b>books</b>.google.com", "url": "https://books.google.com/books/about/Sparse_Solutions_of_Underdetermined_Line.html?id=ulY1EAAAQBAJ", "isFamilyFriendly": true, "displayUrl": "https://<b>books</b>.google.com/<b>books</b>/about/<b>Sparse</b>_Solutions_of_Underdetermined_Line.html?id=...", "snippet": "This textbook presents a special solution to underdetermined linear systems where the number of nonzero entries in the solution is very small <b>compared</b> to the total number of entries. This is called a <b>sparse</b> solution. Since underdetermined linear systems <b>can</b> be very different, the authors explain how to compute a <b>sparse</b> solution using many approaches. <b>Sparse</b> Solutions of Underdetermined Linear Systems and Their Applications contains 64 algorithms for finding <b>sparse</b> solutions of ...", "dateLastCrawled": "2022-01-14T08:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Review on <b>Sparse</b> Matrix Storage Formats With Space Complexity Analysis ...", "url": "https://www.igi-global.com/chapter/review-on-sparse-matrix-storage-formats-with-space-complexity-analysis/265582", "isFamilyFriendly": true, "displayUrl": "https://www.igi-global.com/chapter/review-on-<b>sparse</b>-matrix-storage-formats-with-space...", "snippet": "<b>Sparse</b> matrix-<b>vector</b> multiplication (SpMV) is a challenging computational kernel in linear algebra applications, like data mining, image processing, and machine learning. The performance of this kernel is greatly dependent on the size of the input matrix and the underlying hardware features. Various <b>sparse</b> matrix storage formats referred to commonly as <b>sparse</b> formats have been proposed in the literature to reduce the size of the matrix. In modern multi-core and many-core architectures, the ...", "dateLastCrawled": "2022-01-01T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Performance evaluation of <b>sparse</b> matrix-<b>vector</b> product (SpMV ...", "url": "https://ieeexplore.ieee.org/abstract/document/7060964", "isFamilyFriendly": true, "displayUrl": "https://ieeexplore.ieee.org/abstract/document/7060964", "snippet": "Abstract: <b>Sparse</b> matrices are entailed in many linear algebra problems such as linear systems resolution, matrix eigen-values/vectors computation and partial differential equations, wherefore <b>sparse</b> matrix <b>vector</b> product (SpMV) constitutes a basic kernel for solving many scientific and engineering applications problems. With the appearance of Graphics Processing Units (GPUs) as platforms that provides important acceleration factors, the optimization of SpMV on GPUs and its implementation has ...", "dateLastCrawled": "2020-01-29T10:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sparse Composite Document Vector (Emnlp 2017</b>)", "url": "https://www.slideshare.net/vivekgupta3150/sparse-composite-document-vector-emnlp-2017", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/vivekgupta3150/<b>sparse-composite-document-vector-emnlp-2017</b>", "snippet": "We present a feature <b>vector</b> formation technique for documents - <b>Sparse Composite Document Vector</b> (SCDV) - which overcomes several shortcomings of the current distributional paragraph <b>vector</b> representations that are widely used for text representation. In SCDV, word embeddings are clustered to capture multiple semantic contexts in which words occur. They are then chained together to form document topic-vectors that <b>can</b> express complex, multi-topic documents. Through extensive experiments on ...", "dateLastCrawled": "2022-01-24T07:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Sparse</b> Solutions of Underdetermined Linear Systems and Their ...", "url": "https://epubs.siam.org/doi/book/10.1137/1.9781611976519", "isFamilyFriendly": true, "displayUrl": "https://epubs.siam.org/doi/book/10.1137/1.9781611976519", "snippet": "A linear system of equations consists of a known matrix A and a known <b>vector</b> b such that Ax = b for an unknown <b>vector</b> x, where A is of size m \u00d7 n.When m = n and m &gt; n, methods of solving for x are well known and are discussed in standard numerical analysis textbooks.However, when m &lt; n, Ax = b is called an underdetermined linear system.Such a linear system has become a subject of research in the last 15 years as part of an extremely active study in the community of compressive sensing.", "dateLastCrawled": "2022-02-01T15:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Sparse Matrix</b> \u2013 LearnDataSci", "url": "https://www.learndatasci.com/glossary/sparse-matrix/", "isFamilyFriendly": true, "displayUrl": "https://www.learndatasci.com/glossary/<b>sparse-matrix</b>", "snippet": "A <b>sparse matrix</b> is a special case of a matrix in which the number of zero elements is much higher than the number of non-zero elements. As a rule of thumb, if 2/3 of the total elements in a matrix are zeros, it <b>can</b> be called a <b>sparse matrix</b>. Using a <b>sparse matrix</b> representation \u2014 where only the non-zero values are stored \u2014 the space used ...", "dateLastCrawled": "2022-02-01T18:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python - scipy <b>sparse</b>.linalg.eigen slower using Intel MKL then OpenBLAS ...", "url": "https://stackoverflow.com/questions/70578974/scipy-sparse-linalg-eigen-slower-using-intel-mkl-then-openblas", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/70578974/scipy-<b>sparse</b>-linalg-eigen-slower-using...", "snippet": "For solving I use scipy.<b>sparse</b>.linalg.eigen.eigs, which seems to be the best suite for this case. Because this takes also a long time (often more than 60 seconds for each solving) and I need to solve a couple of thousand matrices, I&#39;m looking for a way to speed up this process. For this, I now have switched from the &quot;normal&quot; scipy installation ...", "dateLastCrawled": "2022-01-26T08:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>SPARSE STORAGE RECOMMENDATION SYSTEM FOR SPARSE MATRIX</b> <b>VECTOR</b> MULTIPL\u2026", "url": "https://www.slideshare.net/iaeme/sparse-storage-recommendation-system-for-sparse-matrix-vector-multiplication-on-gpu", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/iaeme/<b>sparse-storage-recommendation-system-for-sparse</b>...", "snippet": "<b>Sparse</b> Matrix <b>Vector</b> Multiplication (SpMV) Ax=b is a well-known kernel in science, engineering, and web world. Harnessing large computing capabilities of GPU device, many <b>sparse</b> storage formats have been proposed to optimize performance of SpMV on GPU. Compressed <b>Sparse</b> Row (CSR), ELLPACK (ELL), Hybrid (HYB), and Aligned COO <b>sparse</b> storage ...", "dateLastCrawled": "2021-12-29T01:53:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to Vectors for <b>Machine</b> <b>Learning</b>", "url": "https://machinelearningmastery.com/gentle-introduction-vectors-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>vectors</b>-<b>machine</b>-<b>learning</b>", "snippet": "It is common to introduce vectors using a geometric <b>analogy</b>, where a <b>vector</b> represents a point or coordinate in an n-dimensional space, where n is the number of dimensions, such as 2. The <b>vector</b> can also be thought of as a line from the origin of the <b>vector</b> space with a direction and a magnitude. These analogies are good as a starting point, but should not be held too tightly as we often consider very high dimensional vectors in <b>machine</b> <b>learning</b>. I find the <b>vector</b>-as-coordinate the most ...", "dateLastCrawled": "2022-02-01T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Word Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond king - man ...", "url": "https://aclanthology.org/C16-1332.pdf", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/C16-1332.pdf", "snippet": "Word Embeddings, Analogies, and <b>Machine</b> <b>Learning</b>: Beyond King - Man + Woman = Queen Aleksandr Drozd y, Anna Gladkova z, Satoshi Matsuoka y yTokyo Institute of Technology, Meguro-ku, Tokyo 152-8550, Japan alex@smg.is.titech.ac.jp, matsu@is.titech.ac.jp z The University of Tokyo, Meguro-ku, Tokyo 153-8902 Japan gladkova@phiz.c.u-tokyo.ac.jp Abstract Solving word analogies became one of the most popular benchmarks for word embeddings on the assumption that linear relations between word pairs ...", "dateLastCrawled": "2022-01-20T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Introduction to Matrices and Matrix Arithmetic for <b>Machine Learning</b>", "url": "https://machinelearningmastery.com/introduction-matrices-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/introduction-matrices-<b>machine-learning</b>", "snippet": "The geometric <b>analogy</b> used to help understand vectors and some of their operations does not hold with matrices. Further, a <b>vector</b> itself may be considered a matrix with one column and multiple rows. Often the dimensions of the matrix are denoted as m and n for the number of rows and the number of columns. Now that we know what a matrix is, let\u2019s look at defining one in Python. Defining a Matrix. We can represent a matrix in Python using a two-dimensional NumPy array. A NumPy array can be ...", "dateLastCrawled": "2022-02-02T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning :: Cosine Similarity for Vector</b> Space Models (Part III ...", "url": "https://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/", "isFamilyFriendly": true, "displayUrl": "https://blog.christianperone.com/2013/09/<b>machine</b>-<b>learning</b>-", "snippet": "<b>Machine Learning :: Cosine Similarity for Vector</b> Space Models (Part III) 12/09/2013 19/01/2020 Christian S. Perone <b>Machine</b> <b>Learning</b> , Programming , Python * It has been a long time since I wrote the TF-IDF tutorial ( Part I and Part II ) and as I promissed, here is the continuation of the tutorial.", "dateLastCrawled": "2022-01-29T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Word Embedding: Syntactics or Semantics</b> \u00b7 Shengbin&#39;s Studio", "url": "https://wushbin.github.io/2017/10/09/Word-Embedding-Syntactics-or-Semantics/", "isFamilyFriendly": true, "displayUrl": "https://wushbin.github.io/2017/10/09/<b>Word-Embedding-Syntactics-or-Semantics</b>", "snippet": "From all the result of the two method, we know that the dense <b>vector</b> method get a better result than the <b>sparse</b> PPMI method in <b>analogy</b> analysis and similar word search. In addition, the computational efficiency of the dense <b>vector</b> is also better than the PPMI. Short vectors may be easier to use as features in <b>machine</b> <b>learning</b>. Dense vectors may generalize better than storing explicit counts. In addition, dense vectors may perform better in capturing synonymy than <b>sparse</b> vectors.", "dateLastCrawled": "2022-01-09T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-embeddings-in-nlp", "snippet": "Word Embeddings are a method of extracting features out of text so that we can input those features into a <b>machine</b> <b>learning</b> model to work with text data. They try to preserve syntactical and semantic information. The methods such as Bag of Words(BOW), CountVectorizer and TFIDF rely on the word count in a sentence but do not save any syntactical or semantic information. In these algorithms, the size of the <b>vector</b> is the number of elements in the vocabulary. We can get a <b>sparse</b> matrix if most ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is the difference between a <b>Vector</b> and a Tensor in <b>Machine</b> <b>Learning</b>?", "url": "https://www.quora.com/What-is-the-difference-between-a-Vector-and-a-Tensor-in-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-difference-between-a-<b>Vector</b>-and-a-Tensor-in-<b>Machine</b>...", "snippet": "Answer (1 of 2): A <b>vector</b> is a tensor of rank 1, a matrix is a tensor of rank 2. For a tensor with more than 2 dimensions, we refer to it as a tensor. Note that, rank of a matrix [1] from linear algebra is not the same as tensor rank [2] 1. Rank (linear algebra) - Wikipedia 2. Tensor - Wikipedia", "dateLastCrawled": "2022-01-13T06:46:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sparse vector)  is like +(library of books)", "+(sparse vector) is similar to +(library of books)", "+(sparse vector) can be thought of as +(library of books)", "+(sparse vector) can be compared to +(library of books)", "machine learning +(sparse vector AND analogy)", "machine learning +(\"sparse vector is like\")", "machine learning +(\"sparse vector is similar\")", "machine learning +(\"just as sparse vector\")", "machine learning +(\"sparse vector can be thought of as\")", "machine learning +(\"sparse vector can be compared to\")"]}