{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Python <b>LSTM</b> (<b>Long Short-Term Memory</b> Network) for Stock Predictions ...", "url": "https://www.datacamp.com/community/tutorials/lstm-python-stock-market", "isFamilyFriendly": true, "displayUrl": "https://www.datacamp.com/community/tutorials/<b>lstm</b>-python-stock-market", "snippet": "<b>Long Short-Term Memory</b> models are extremely powerful time-series models. They can predict an arbitrary number of steps into the future. An <b>LSTM</b> module (or cell) has 5 essential components which allows it to model both <b>long</b>-term and <b>short-term</b> data.", "dateLastCrawled": "2022-02-02T22:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Deep <b>learning-based classification of earthquake-impacted buildings</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S221242091830983X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S221242091830983X", "snippet": "To this end, the <b>long short-term memory</b> (<b>LSTM</b>) deep learning method is applied to classify building damage based on textual descriptions of damage. The damaged state of an individual building is classified using the ATC-20 tags (red, yellow and green). The application of the <b>LSTM</b> approach is demonstrated using building damage descriptions recorded following the 2014 South Napa, California earthquake. The dataset, which consists of 3423 buildings (1552 green tagged, 1674 yellow tagged, 197 ...", "dateLastCrawled": "2021-11-08T21:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep Bi-Directional <b>LSTM</b> Networks for Device Workload Forecasting", "url": "https://annals-csis.org/Volume_21/drp/pdf/213.pdf", "isFamilyFriendly": true, "displayUrl": "https://annals-csis.org/Volume_21/drp/pdf/213.pdf", "snippet": "bi-directional <b>Long-Short-Term-Memory</b> (<b>LSTM</b>) regression net-work that reported the most accurate hourly predictions of the weekly workload time series from the thousands of different network devices with diverse shape and seasonality pro\ufb01les. We will also show how intuitive human-led post-processing of the raw <b>LSTM</b> predictions could easily destroy the generalization abilities of such prediction model. Index Terms\u2014Workload prediction, time series, <b>Long Short-Term Memory</b> (<b>LSTM</b>), ensemble ...", "dateLastCrawled": "2022-01-09T08:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Grid Long Short-Term Memory</b> - researchgate.net", "url": "https://www.researchgate.net/publication/279864537_Grid_Long_Short-Term_Memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/279864537_<b>Grid_Long_Short-Term_Memory</b>", "snippet": "A <b>long short-term memory</b> (<b>LSTM</b>) model was proposed in [37] to increase the <b>long</b>-term dependency property and overcome the problem of gradient descent. <b>LSTM</b> is a variant of RNN that can capture and ...", "dateLastCrawled": "2022-01-27T16:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Associative Long Short-Term Memory</b> - ResearchGate", "url": "https://www.researchgate.net/publication/301848436_Associative_Long_Short-Term_Memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/301848436_<b>Associative_Long_Short-Term_Memory</b>", "snippet": "Those methods for sequences are based on recurrent models <b>like</b> recurrent neural network (RNN), <b>Long Short-Term Memory</b> (<b>LSTM</b>) [9], and Gated Recurrent Units (GRU) [10], which inherently suffer from ...", "dateLastCrawled": "2021-09-20T10:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Keras <b>LSTM</b> tutorial \u2013 How to easily build a powerful deep learning ...", "url": "https://adventuresinmachinelearning.com/keras-lstm-tutorial/", "isFamilyFriendly": true, "displayUrl": "https://adventuresinmachinelearning.com/keras-<b>lstm</b>-tutorial", "snippet": "In previous posts, I introduced Keras for building convolutional neural networks and performing word embedding.The next natural step is to talk about implementing recurrent neural networks in Keras. In a previous tutorial of mine, I gave a very comprehensive introduction to recurrent neural networks and <b>long short term memory</b> (<b>LSTM</b>) networks, implemented in TensorFlow.In this tutorial, I\u2019ll concentrate on creating <b>LSTM</b> networks in Keras, briefly giving a recap or overview of how LSTMs work.", "dateLastCrawled": "2022-02-03T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Applications of Artificial Intelligence in Fire &amp; Safety</b> | by Vedant ...", "url": "https://towardsdatascience.com/applications-of-artificial-intelligence-in-fire-safety-20f66f19bdf9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>applications-of-artificial-intelligence-in-fire-safety</b>...", "snippet": "Again, classification models <b>like</b> the random forest can be used to detect forest fires. An advanced application of the same concept would be to predict the occurrence of the next forest fires. This is a time series problem and can be solved using deep learning models <b>like</b> <b>Long Short-Term Memory</b> (<b>LSTM</b>) or recurrent neural network (RNN).", "dateLastCrawled": "2022-01-28T19:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Large-Batch Training for <b>LSTM</b> and Beyond", "url": "https://sc19.supercomputing.org/proceedings/tech_paper/tech_paper_files/pap106s5.pdf", "isFamilyFriendly": true, "displayUrl": "https://sc19.supercomputing.org/proceedings/tech_paper/tech_paper_files/pap106s5.pdf", "snippet": "How about RNN applications <b>like</b> <b>LSTM</b> (<b>Long Short-Term Memory</b>)? If we x the dataset (e.g. ImageNet) Can we scale on di erent models? CNN: Convolutional Neural Network RNN: Recurrent Neural Network Yang You (advised by James Demmel) UC Berkeley Computer Science Fast Deep Learning 10 / 44 . Outline Problems in Distributed Deep Learning Our Approach Experimental Results Yang You (advised by James Demmel) UC Berkeley Computer Science Fast Deep Learning 11 / 44. Previous e ective techniques ...", "dateLastCrawled": "2021-09-14T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "De novo generation of hit-<b>like</b> molecules from gene expression ...", "url": "https://www.nature.com/articles/s41467-019-13807-w", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-019-13807-w", "snippet": "Hochreiter, S. &amp; Schmidhuber, J. <b>Long short-term memory</b>. Neural Comput. 9 , 1735\u20131780 (1997). CAS PubMed Article PubMed Central Google Scholar", "dateLastCrawled": "2022-02-03T17:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Staircase</b> Attention for Recurrent Processing of Sequences \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2106.04279/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.04279", "snippet": "<b>Staircase</b> attention, <b>like</b> self-attention, processes tokens in parallel for speed, but unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step (processing block) in the <b>staircase</b> comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence). Thus, on each time step the block moves forward in time, retaining a <b>memory</b> comprised of multiple vectors ...", "dateLastCrawled": "2022-01-09T18:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Python <b>LSTM</b> (<b>Long Short-Term Memory</b> Network) for Stock Predictions ...", "url": "https://www.datacamp.com/community/tutorials/lstm-python-stock-market", "isFamilyFriendly": true, "displayUrl": "https://www.datacamp.com/community/tutorials/<b>lstm</b>-python-stock-market", "snippet": "<b>Long Short-Term Memory</b> models are extremely powerful time-series models. They can predict an arbitrary number of steps into the future. An <b>LSTM</b> module (or cell) has 5 essential components which allows it to model both <b>long</b>-term and <b>short-term</b> data. Cell state (c t) - This represents the internal <b>memory</b> of the cell which stores both <b>short term</b> <b>memory</b> and <b>long</b>-term memories; Hidden state (h t) - This is output state information calculated w.r.t. current input, previous hidden state and current ...", "dateLastCrawled": "2022-02-02T22:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Deep <b>learning-based classification of earthquake-impacted buildings</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S221242091830983X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S221242091830983X", "snippet": "To this end, the <b>long short-term memory</b> (<b>LSTM</b>) deep learning method is applied to classify building damage based on textual descriptions of damage. The damaged state of an individual building is classified using the ATC-20 tags (red, yellow and green). The application of the <b>LSTM</b> approach is demonstrated using building damage descriptions recorded following the 2014 South Napa, California earthquake. The dataset, which consists of 3423 buildings (1552 green tagged, 1674 yellow tagged, 197 ...", "dateLastCrawled": "2021-11-08T21:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Grid Long Short-Term Memory</b> - researchgate.net", "url": "https://www.researchgate.net/publication/279864537_Grid_Long_Short-Term_Memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/279864537_<b>Grid_Long_Short-Term_Memory</b>", "snippet": "A <b>long short-term memory</b> (<b>LSTM</b>) model was proposed in [37] to increase the <b>long</b>-term dependency property and overcome the problem of gradient descent. <b>LSTM</b> is a variant of RNN that can capture and ...", "dateLastCrawled": "2022-01-27T16:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep <b>stair walking detection using wearable inertial sensor</b> via <b>long</b> ...", "url": "http://eprints.utm.my/id/eprint/90862/1/MuhammadAmirAs%60Ari2020_DeepStairWalkingDetectionUsingWearableInertialSensor.pdf", "isFamilyFriendly": true, "displayUrl": "eprints.utm.my/id/eprint/90862/1/MuhammadAmirAs`Ari2020...", "snippet": "This paper proposes a stair walking detection via <b>Long Short-Term Memory</b> (<b>LSTM</b>) network to prevent stair fall event happen by alerting caregiver for assistance as soon as possible. The tri-axial accelerometer and gyroscope data of five activities of daily living (ADLs) including stair walking right heel, chest, left wrist and right wrist. Several parameters which are window size, sensor deployment, number of hidden cell unit and <b>LSTM</b> architecture were varied in finding an optimized <b>LSTM</b> ...", "dateLastCrawled": "2021-08-29T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Sensors | Free Full-Text | Understanding <b>LSTM</b> Network Behaviour of IMU ...", "url": "https://www.mdpi.com/1424-8220/21/4/1264/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/21/4/1264/htm", "snippet": "<b>Long Short-Term Memory</b> (<b>LSTM</b>) machine-learning networks can perform LMR with high accuracy levels. However, the internal behavior during classification is unknown, and they struggle to generalize when presented with novel users. The target problem addressed in this paper is understanding the <b>LSTM</b> classification behavior for LMR. A dataset of six locomotive activities (walking, stopped, stairs and ramps) from 22 non-amputee subjects is collected, capturing both steady-state and transitions ...", "dateLastCrawled": "2022-01-18T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Frontiers | Improvement of Prediction Performance With Conjoint ...", "url": "https://www.frontiersin.org/articles/10.3389/fphar.2020.606668/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fphar.2020.606668", "snippet": "<b>Long short-term memory</b> network (<b>LSTM</b>) is improved based on the recurrent neural network (RNN). The advantage of <b>LSTM</b> is its ability to process sequence information with <b>long</b>-term dependency information. <b>LSTM</b> may be benefited from conjoint fingerprints, where two types of fingerprints are kept. The general architecture of <b>LSTM</b> unit is composed of an input gate, a forget gate, an output gate and a <b>memory</b> block. The forget gate is used to decide what information will be forgot from previous ...", "dateLastCrawled": "2022-01-31T14:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Keras <b>LSTM</b> tutorial \u2013 How to easily build a powerful deep learning ...", "url": "https://adventuresinmachinelearning.com/keras-lstm-tutorial/", "isFamilyFriendly": true, "displayUrl": "https://adventuresinmachinelearning.com/keras-<b>lstm</b>-tutorial", "snippet": "In previous posts, I introduced Keras for building convolutional neural networks and performing word embedding.The next natural step is to talk about implementing recurrent neural networks in Keras. In a previous tutorial of mine, I gave a very comprehensive introduction to recurrent neural networks and <b>long short term memory</b> (<b>LSTM</b>) networks, implemented in TensorFlow.In this tutorial, I\u2019ll concentrate on creating <b>LSTM</b> networks in Keras, briefly giving a recap or overview of how LSTMs work.", "dateLastCrawled": "2022-02-03T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A Deep Learning-Based Framework for Human Activity Recognition in Smart ...", "url": "https://www.hindawi.com/journals/misy/2021/6961343/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/misy/2021/6961343", "snippet": "Human behavior modeling in smart environments is a growing research area treating several challenges related to ubiquitous computing, pattern recognition, and ambient assisted living. Thanks to recent progress in sensing devices, it is now possible to design computational models able of accurate detection of residents\u2019 activities and daily routines. For this goal, we introduce in this paper a deep learning-based framework for activity recognition in smart homes. This framework proposes a ...", "dateLastCrawled": "2022-02-02T23:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Applications of Artificial Intelligence in Fire &amp; Safety</b> | by Vedant ...", "url": "https://towardsdatascience.com/applications-of-artificial-intelligence-in-fire-safety-20f66f19bdf9", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>applications-of-artificial-intelligence-in-fire-safety</b>...", "snippet": "The task is very <b>similar</b> to that of an object detection model. Once the model has learned the features of the flames from the data and its annotations, it can be used for the detection purpose. The most commonly used model is MobileNet because it offers good accuracy with less computational complexity owing to the use of depth-wise convolution. This can be further extended to detect the severity of the incident by using the k-nearest neighbor algorithm. This approach <b>is similar</b> to what the ...", "dateLastCrawled": "2022-01-28T19:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Staircase</b> Attention for Recurrent Processing of Sequences \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2106.04279/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.04279", "snippet": "<b>Staircase</b> models exploit parallelism <b>similar</b> to Transformers while maintaining several chunks of recurrent (per token) features to more expressively track state than conventional RNNs. <b>Memory</b> Networks. MemNets as implemented in (Sukhbaatar et al., 2015) employ recurrence in the stacked layers of attention and computation for the current token, but only compute input embeddings h 0 t = f in (x t) for previous tokens, and can thus be seen as a kind of simplified Ladder model, or equivalently a ...", "dateLastCrawled": "2022-01-09T18:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Sensors | Free Full-Text | Understanding <b>LSTM</b> Network Behaviour of IMU ...", "url": "https://www.mdpi.com/1424-8220/21/4/1264/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/21/4/1264/htm", "snippet": "<b>Long Short-Term Memory</b> (<b>LSTM</b>) machine-learning networks <b>can</b> perform LMR with high accuracy levels. However, the internal behavior during classification is unknown, and they struggle to generalize when presented with novel users. The target problem addressed in this paper is understanding the <b>LSTM</b> classification behavior for LMR. A dataset of six locomotive activities (walking, stopped, stairs and ramps) from 22 non-amputee subjects is collected, capturing both steady-state and transitions ...", "dateLastCrawled": "2022-01-18T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Grid Long Short-Term Memory</b> - researchgate.net", "url": "https://www.researchgate.net/publication/279864537_Grid_Long_Short-Term_Memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/279864537_<b>Grid_Long_Short-Term_Memory</b>", "snippet": "A <b>long short-term memory</b> (<b>LSTM</b>) model was proposed in [37] to increase the <b>long</b>-term dependency property and overcome the problem of gradient descent. <b>LSTM</b> is a variant of RNN that <b>can</b> capture and ...", "dateLastCrawled": "2022-01-27T16:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Associative Long Short-Term Memory</b> - ResearchGate", "url": "https://www.researchgate.net/publication/301848436_Associative_Long_Short-Term_Memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/301848436_<b>Associative_Long_Short-Term_Memory</b>", "snippet": "Those methods for sequences are based on recurrent models like recurrent neural network (RNN), <b>Long Short-Term Memory</b> (<b>LSTM</b>) [9], and Gated Recurrent Units (GRU) [10], which inherently suffer from ...", "dateLastCrawled": "2021-09-20T10:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Journal of Computational Methods in Sciences and Engineering - Volume ...", "url": "https://content.iospress.com/journals/journal-of-computational-methods-in-sciences-and-engineering/21/5?rows=50", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/journals/journal-of-computational-methods-in-sciences-and...", "snippet": "<b>Long Short-Term memory</b> (<b>LSTM</b>) <b>can</b> perform better in longer sequences than ordinary RNN. This paper is applied to the analysis of big data of grain storage and the early warning of grain storage temperature. In this paper, the selected <b>LSTM</b> is optimized and the early warning model of grain situation is established, and the analysis steps of the early warning model are", "dateLastCrawled": "2021-12-22T12:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Harry Potter and the Sorcerer\u2019s</b> AI | Cognitive Times", "url": "https://www.cognitivetimes.com/2020/04/harry-potter-and-the-sorcerers-ai/", "isFamilyFriendly": true, "displayUrl": "https://www.cognitivetimes.com/2020/04/harry-<b>potter-and-the-sorcerers</b>-ai", "snippet": "To further improve this capacity for continuity, the most commonly employed type of RNN is a <b>long short-term memory</b>, or <b>LSTM</b>, network. LSTMs <b>can</b> keep track of dependencies over greater distances, allowing continuity in generated text to extend beyond just a few sentences. Writing Through Statistics. How exactly do LSTMs create written works? In the simplest terms, through statistics. A model working at the word level, when fed a body of text, analyzes the probabilities of any given word or ...", "dateLastCrawled": "2021-12-23T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Frontiers | Application of Machine Learning Models for Tracking ...", "url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2020.01532/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fpsyg.2020.01532", "snippet": "A key need in cognitive training interventions is to personalize task difficulty to each user and to adapt this difficulty to continually apply appropriate challenges as users improve their skill to perform the tasks. Here we examine how Bayesian filtering approaches, such as hidden Markov models and Kalman filters, and deep-learning approaches, such as the <b>long short-term memory</b> (<b>LSTM</b>) model, may be useful methods to estimate user skill level and predict appropriate task challenges. A ...", "dateLastCrawled": "2022-01-28T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Automated tracking of level of consciousness and delirium in critical ...", "url": "https://www.nature.com/articles/s41746-019-0167-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41746-019-0167-0", "snippet": "The overall deep learning model consisted of convolutional neural network (CNN) followed by <b>long-short term memory</b> (<b>LSTM</b>), as shown in Supplementary Fig. 9. CNN extracts useful information from ...", "dateLastCrawled": "2021-09-20T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reading is an effective way to improve personal quality --- 2019 summer ...", "url": "https://www.programmerall.com/article/5395861485/", "isFamilyFriendly": true, "displayUrl": "https://www.programmerall.com/article/5395861485", "snippet": "__Call__, the object <b>can</b> be called, if the __call__ method is implemented in the class, the instance object will also become a callable object, and the class-based decorator <b>can</b> be implemented, and the state is recorded in the class. 3.2 Using the Subproduction Subproduction Methods Skilled the usage of common functions. <b>LSTM</b>, <b>Long Short Term Memory</b> Networks, is a variant of RNN. It is used to solve more problems and have achieved very good results. People who understand deep learning are ...", "dateLastCrawled": "2022-01-01T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "IOS Press Ebooks - Information Technology and Intelligent ...", "url": "https://ebooks.iospress.nl/volume/information-technology-and-intelligent-transportation-systems-proceedings-of-the-3rd-international-conference-on-information-technology-and-intelligent-transpor", "isFamilyFriendly": true, "displayUrl": "https://ebooks.iospress.nl/volume/information-technology-and-intelligent...", "snippet": "This paper uses the <b>long</b>-term and <b>short-term</b> <b>memory</b> (<b>LSTM</b>) neural network, combined with actual passenger traffic data, through experiments and comparison with other representative forecast models validates that the proposed <b>LSTM</b> network <b>can</b> achieve a better performance, simultaneously analyzed the effects of various input settings on the <b>LSTM</b> forecast performance. In the passenger environment with large quantity and periodic regularity, the calculation time is saved and the prediction ...", "dateLastCrawled": "2021-12-28T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Recent advances in physical reservoir computing: A review</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0893608019300784", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0893608019300784", "snippet": "The CA-based RC system <b>can</b> perform 5-bit and 20-bit temporal <b>memory</b> tasks, which require <b>long short-term memory</b> capability, with less computation compared to ESNs (Yilmaz, 2015a). The binary operations and simple update rules of CA reservoirs are advantageous for implementation with parallel hardware, such as field-programmable gate arrays (FPGAs) and graphics processing units (GPUs). In a recent study", "dateLastCrawled": "2022-01-29T23:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Sensors | Free Full-Text | Understanding <b>LSTM</b> Network Behaviour of IMU ...", "url": "https://www.mdpi.com/1424-8220/21/4/1264/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1424-8220/21/4/1264/htm", "snippet": "<b>Long Short-Term Memory</b> (<b>LSTM</b>) machine-learning networks <b>can</b> perform LMR with high accuracy levels. However, the internal behavior during classification is unknown, and they struggle to generalize when presented with novel users. The target problem addressed in this paper is understanding the <b>LSTM</b> classification behavior for LMR. A dataset of six locomotive activities (walking, stopped, stairs and ramps) from 22 non-amputee subjects is collected, capturing both steady-state and transitions ...", "dateLastCrawled": "2022-01-18T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Spatial Structure-Related Sensory Landmarks Recognition Based on <b>Long</b> ...", "url": "https://www.mdpi.com/2072-666X/12/7/781/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/2072-666X/12/7/781/htm", "snippet": "To this end, we improved a <b>Long Short-Term Memory</b> (<b>LSTM</b>) neural network to recognize different kinds of spatial structure-related sensory landmarks. Labels of structural sensory landmarks were proposed, and data processing methods (including interpolation, filter, and window length) were used and <b>compared</b> to achieve the highest recognition accuracy of 99.6%.", "dateLastCrawled": "2021-12-11T22:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Deep Bi-Directional <b>LSTM</b> Networks for Device Workload Forecasting", "url": "https://annals-csis.org/Volume_21/drp/pdf/213.pdf", "isFamilyFriendly": true, "displayUrl": "https://annals-csis.org/Volume_21/drp/pdf/213.pdf", "snippet": "An approach based on the <b>Long Short-Term Memory</b> (<b>LSTM</b>) encoder-decoder network with attention mechanism was proposed in [11]. In [12], a GRU-based encoder-decoder network containing 2 gated recurrent neural networks was implemented for prediction of multi-step-ahead host workload in cloud computing. Accurate workload prediction is a challenging problem. Different time series from various devices typically have varied patterns that not only lack of well pronounced stationarity but also are ...", "dateLastCrawled": "2022-01-09T08:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Grid Long Short-Term Memory</b> - researchgate.net", "url": "https://www.researchgate.net/publication/279864537_Grid_Long_Short-Term_Memory", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/279864537_<b>Grid_Long_Short-Term_Memory</b>", "snippet": "A <b>long short-term memory</b> (<b>LSTM</b>) model was proposed in [37] to increase the <b>long</b>-term dependency property and overcome the problem of gradient descent. <b>LSTM</b> is a variant of RNN that <b>can</b> capture and ...", "dateLastCrawled": "2022-01-27T16:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Frontiers | Improvement of Prediction Performance With Conjoint ...", "url": "https://www.frontiersin.org/articles/10.3389/fphar.2020.606668/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/fphar.2020.606668", "snippet": "<b>Long short-term memory</b> network (<b>LSTM</b>) is improved based on the recurrent neural network (RNN). The advantage of <b>LSTM</b> is its ability to process sequence information with <b>long</b>-term dependency information. <b>LSTM</b> may be benefited from conjoint fingerprints, where two types of fingerprints are kept. The general architecture of <b>LSTM</b> unit is composed of an input gate, a forget gate, an output gate and a <b>memory</b> block. The forget gate is used to decide what information will be forgot from previous ...", "dateLastCrawled": "2022-01-31T14:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Daily <b>activity recognition based on recurrent</b> neural network using ...", "url": "https://www.cambridge.org/core/journals/apsipa-transactions-on-signal-and-information-processing/article/daily-activity-recognition-based-on-recurrent-neural-network-using-multimodal-signals/F57A48C0411E510EE4535DBF917BAB5E", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/apsipa-transactions-on-signal-and-information...", "snippet": "RNN with <b>long short-term memory</b>. The <b>LSTM</b>-RNN has the special architecture, <b>LSTM</b> <b>memory</b> blocks. The hidden units in SRNN are replaced with it. The <b>LSTM</b> <b>memory</b> block contains <b>memory</b> cell which stores past information of the state, and gates which control the duration of storing. The <b>LSTM</b>-RNN <b>can</b> capture <b>long</b>-term context by representing information from past inputs as hidden vector and propagating it to future direction. The following sections describe several variants of <b>LSTM</b>-RNN with much ...", "dateLastCrawled": "2021-12-20T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Keras <b>LSTM</b> tutorial \u2013 How to easily build a powerful deep learning ...", "url": "https://adventuresinmachinelearning.com/keras-lstm-tutorial/", "isFamilyFriendly": true, "displayUrl": "https://adventuresinmachinelearning.com/keras-<b>lstm</b>-tutorial", "snippet": "In a previous tutorial of mine, I gave a very comprehensive introduction to recurrent neural networks and <b>long short term memory</b> (<b>LSTM</b>) networks, implemented in TensorFlow. In this tutorial, I\u2019ll concentrate on creating <b>LSTM</b> networks in Keras, briefly giving a recap or overview of how LSTMs work. In this Keras <b>LSTM</b> tutorial, we\u2019ll implement a sequence-to-sequence text prediction model by utilizing a large text data set called the PTB corpus. All the code in this tutorial <b>can</b> be found on this", "dateLastCrawled": "2022-02-03T06:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Feasibility of a virtual reality-based exercise intervention and low ...", "url": "https://jneuroengrehab.biomedcentral.com/articles/10.1186/s12984-021-00978-1", "isFamilyFriendly": true, "displayUrl": "https://jneuroengrehab.biomedcentral.com/articles/10.1186/s12984-021-00978-1", "snippet": "The action recognition neural network consisted of an input layer of 3129 (= 60 \u00d7 52) dimensions, three hidden <b>long short-term memory</b> (<b>LSTM</b>) layers of 20 nodes each, and a softmax output layer of 10 dimensions. The softmax function applies a normalized exponential transformation to the outputs\u2019 linear combinations, converting them to a discrete probability assignment over the 10 possible labels. It was trained from a small dataset collected and augmented by flipping the data on the Y axis ...", "dateLastCrawled": "2022-01-29T17:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Reinforcement Learning-Based Complete Area Coverage Path Planning for a ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7913922/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7913922", "snippet": "The proposed method generates a better cost-weight path (59.52 Nm as <b>compared</b> with ACO\u2019s 61.35 Nm) and quicker (0.314 s <b>compared</b> to ACO\u2019s 1.22 s). The running time signifies the time taken for the path to be generated by the algorithm. In our case, the time taken for the path generation is 0.314 s. However, as the current implementation is based on neural networks, there will be an overhead load time which <b>can</b> be taken care once every time the robot boots up. Thus, the RL based method ...", "dateLastCrawled": "2022-01-25T22:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Staircase</b> Attention for Recurrent Processing of Sequences \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2106.04279/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2106.04279", "snippet": "RNNs Elman that store recurrent state in a single vector and ingest tokens one at a time <b>can</b> <b>be compared</b> to a <b>Staircase</b> model with a single backward token and a single forward token, i.e. a chunk size of C = 1 and N = 2. <b>Staircase</b> models exploit parallelism similar to Transformers while maintaining several chunks of recurrent (per token) features to more expressively track state than conventional RNNs. <b>Memory</b> Networks. MemNets as implemented in (Sukhbaatar et al., 2015) employ recurrence in ...", "dateLastCrawled": "2022-01-09T18:20:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep <b>Learning</b> : Intro to <b>LSTM</b> (<b>Long Short Term Memory</b>) | by HIMANSHU ...", "url": "https://medium.com/@himanshunpatel01/deep-learning-intro-to-lstm-long-short-term-memory-ce504dc6e585", "isFamilyFriendly": true, "displayUrl": "https://medium.com/.../deep-<b>learning</b>-intro-to-<b>lstm</b>-<b>long-short-term-memory</b>-ce504dc6e585", "snippet": "A simple <b>machine</b> <b>learning</b> model or an Artificial Neural Network may learn to predict the stock prices based on a number of features: the volume of the stock, the opening value etc. While the price ...", "dateLastCrawled": "2022-01-27T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Long Short-Term Memory</b> Networks by the Experts", "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/gentle-introduction-<b>long-short-term-memory</b>-networks...", "snippet": "<b>Long Short-Term Memory</b> (<b>LSTM</b>) networks are a type of recurrent neural network capable of <b>learning</b> order dependence in sequence prediction problems. This is a behavior required in complex problem domains like <b>machine</b> translation, speech recognition, and more. LSTMs are a complex area of deep <b>learning</b>. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional", "dateLastCrawled": "2022-01-31T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Long Short Term Memory</b>(<b>LSTM</b>) and <b>Gated Recurrent</b> Units(GRU) | by ...", "url": "https://prvnk10.medium.com/long-short-term-memory-lstm-and-gated-recurrent-units-gru-240d8a62db9", "isFamilyFriendly": true, "displayUrl": "https://prvnk10.medium.com/<b>long-short-term-memory</b>-<b>lstm</b>-and-<b>gated-recurrent</b>-units-gru...", "snippet": "<b>Long Short Term Memory</b> (<b>LSTM</b>) and <b>Gated Recurrent</b> Units (GRU) This article covers the content discussed in the LSTMs and GRU module of the Deep <b>Learning</b> course offered on the website: https://padhai.onefourthlabs.in. The problem with the RNN is that we want the output at every time step to b e dependent on the previous input and the way we do ...", "dateLastCrawled": "2022-01-30T07:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "9.2. <b>Long Short-Term Memory</b> (<b>LSTM</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/lstm.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/<b>lstm</b>.html", "snippet": "The challenge to address <b>long</b>-term information preservation and <b>short-term</b> input skipping in latent variable models has existed for a <b>long</b> time. One of the earliest approaches to address this was the <b>long short-term memory</b> (<b>LSTM</b>) [Hochreiter &amp; Schmidhuber, 1997]. It shares many of the properties of the GRU. Interestingly, LSTMs have a slightly more complex design than GRUs but predates GRUs by almost two decades. 9.2.1. Gated <b>Memory</b> Cell\u00b6 Arguably <b>LSTM</b>\u2019s design is inspired by logic gates ...", "dateLastCrawled": "2022-02-02T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>CPSC 540: Machine Learning</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Courses/540-W20/L32.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Courses/540-W20/L32.pdf", "snippet": "<b>CPSC 540: Machine Learning</b> <b>Long Short Term Memory</b> Winter 2020. Previously: Sequence-to-Sequence \u2022Sequence-to-sequence: \u2013Recurrent neural network for sequences of different lengths. \u2022 ^Encoding phase that takes an input at each time. \u2022 ^Decoding phase that makes an output at each time. \u2013Encoding ends with BOS, decoding ends with EOS. x 1 z 1 x 2 z 2 x 3 z 0 z 3 z 4 z 5 y 1 y 2. Variations on Recurrent Neural Networks \u2022Bi-directional RNNs: feedforward from past and future ...", "dateLastCrawled": "2021-11-08T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Model Reduction with Memory and</b> <b>the Machine Learning of Dynamical</b> ...", "url": "https://deepai.org/publication/model-reduction-with-memory-and-the-machine-learning-of-dynamical-systems", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>model-reduction-with-memory-and</b>-the-<b>machine</b>-<b>learning</b>-of...", "snippet": "2.2 <b>Long short-term memory</b> networks. Theoretically, RNNs is capable of <b>learning</b> <b>long</b>-term <b>memory</b> effects in the time series. However, in practice it is hard for RNN to catch such dependencies, because of the exploding or shrinking gradient effects , . The <b>Long Short-Term Memory</b> (<b>LSTM</b>) network is designed to solve this problem. Proposed by Hochreiter et al. , the <b>LSTM</b> introduces a new group of hidden units called states, and uses gates to control the information flow through the states. Since ...", "dateLastCrawled": "2022-01-17T23:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Long Short Term Memory and Gated Recurrent Unit</b>\u2019s Explained \u2014 ELI5 Way ...", "url": "https://towardsdatascience.com/long-short-term-memory-and-gated-recurrent-units-explained-eli5-way-eff3d44f50dd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>long-short-term-memory-and-gated-recurrent</b>-units...", "snippet": "Hi All, welcome to my blog \u201c<b>Long Short Term Memory and Gated Recurrent Unit</b>\u2019s Explained \u2014 ELI5 Way\u201d this is my last blog of the year 2019.My name is Niranjan Kumar and I\u2019m a Senior Consultant Data Science at Allstate India.. Recurrent Neural Networks(RNN) are a type of Neural Network where the output from the previous step is fed as input to the current step.", "dateLastCrawled": "2022-01-24T06:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "NPTEL :: Computer Science and Engineering - NOC:Deep <b>Learning</b>- Part 1", "url": "https://www.nptel.ac.in/courses/106/106/106106184/", "isFamilyFriendly": true, "displayUrl": "https://www.nptel.ac.in/courses/106/106/106106184", "snippet": "Selective Read, Selective Write, Selective Forget - The Whiteboard <b>Analogy</b>: Download: 109: <b>Long Short Term Memory</b>(<b>LSTM</b>) and Gated Recurrent Units(GRUs) Download: 110: How LSTMs avoid the problem of vanishing gradients: Download: 111: How LSTMs avoid the problem of vanishing gradients (Contd.) Download: 112: Introduction to Encoder Decoder ...", "dateLastCrawled": "2022-01-25T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Multistep Time Series Forecasting with</b> LSTMs in Python", "url": "https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>multi-step-time-series-forecasting</b>-<b>long</b>-<b>short-term</b>...", "snippet": "The <b>Long Short-Term Memory</b> network or <b>LSTM</b> is a recurrent neural network that can learn and forecast <b>long</b> sequences. A benefit of LSTMs in addition to <b>learning</b> <b>long</b> sequences is that they can learn to make a one-shot multi-step forecast which may be useful for <b>time series forecasting</b>. A difficulty with LSTMs is that they can be tricky to configure and it", "dateLastCrawled": "2022-02-02T18:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Introduction to Transformers in Machine Learning</b>", "url": "https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/.../12/28/<b>introduction-to-transformers-in-machine-learning</b>", "snippet": "Fortunately, in the 2010s, <b>Long Short-Term Memory</b> networks (LSTMs, top right) and Gated Recurrent Units (GRUs, bottom) were researched and applied to resolve many of the three issues above. LSTMs in particular, through the cell like structure where <b>memory</b> is retained, are robust to the vanishing gradients problem. What\u2019s more, because <b>memory</b> is now maintained separately from the previous cell output (the \\(c_{t}\\) flow in the", "dateLastCrawled": "2022-02-03T03:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep <b>learning</b> hybrid model with Boruta-Random forest optimiser ...", "url": "https://www.sciencedirect.com/science/article/pii/S0022169421003978", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0022169421003978", "snippet": "The <b>long short-term memory (LSTM) is like</b> the recurrent neural network (RNN), popularly used in the deep <b>learning</b> field. Likewise, the RNN architecture, LSTM, has a feedback connection with the layers, which can establish the complete sequences of the inputs. The description of LSTM networks can be found different from researches Britz, 2015, Chollet, 2016, Ghimire et al., 2019c, Graves, 2012, Olah, 2015). The LSTM networks are introduced to solve the problems associated with conventional ...", "dateLastCrawled": "2022-01-26T05:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Deep <b>Learning</b> Approach for Aggressive Driving Behaviour Detection", "url": "https://arxiv.org/pdf/2111.04794v1", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2111.04794v1", "snippet": "ML = <b>Machine</b> <b>Learning</b> DL = Deep <b>Learning</b> RNN = Recurrent Neural Network GRU = Gated Recurrent Unit LSTM = Long Short-Term Memory Introduction With the number of automobile accidents, fuel economy, and determining the level of driving talent, the DBA (Driving Behaviour Analysis) becomes a critical subject to be calculated. Depending on the types of car sensors, the inputs . and outputs can then be examined to establish if the DBC (Driving Behaviour Classification) is normal or deviant ...", "dateLastCrawled": "2021-12-09T07:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep <b>Learning</b> Methods Cancer Diagnosis", "url": "https://www.linkedin.com/pulse/deep-learning-methods-cancer-diagnosis-jims-vasant-kunj-ii", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/deep-<b>learning</b>-methods-cancer-diagnosis-jims-vasant-kunj-ii", "snippet": "Classifiers in <b>Machine</b> <b>Learning</b> and its Application: ... <b>Long Short-Term Memory (LSTM) is similar</b> to RNN. It is used for <b>learning</b> order dependence in sequential prediction problems. Conclusion ...", "dateLastCrawled": "2022-01-13T06:31:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(long short-term memory (lstm))  is like +(staircase)", "+(long short-term memory (lstm)) is similar to +(staircase)", "+(long short-term memory (lstm)) can be thought of as +(staircase)", "+(long short-term memory (lstm)) can be compared to +(staircase)", "machine learning +(long short-term memory (lstm) AND analogy)", "machine learning +(\"long short-term memory (lstm) is like\")", "machine learning +(\"long short-term memory (lstm) is similar\")", "machine learning +(\"just as long short-term memory (lstm)\")", "machine learning +(\"long short-term memory (lstm) can be thought of as\")", "machine learning +(\"long short-term memory (lstm) can be compared to\")"]}