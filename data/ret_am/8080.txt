{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Process</b> - GitHub Pages", "url": "https://jmlb.github.io/ml/2016/09/03/MarkovDecisionProcess/", "isFamilyFriendly": true, "displayUrl": "https://jmlb.github.io/ml/2016/09/03/<b>MarkovDecisionProcess</b>", "snippet": "Hence, exploring the world could increase our knowledge and eventually help us make <b>better</b> <b>decisions</b>. RL is balancing the exploration-exploitation trade-off in sequential <b>decision</b> <b>making</b> problems. Behavioral Psychology: The simplified goal of behavioral psychology is to explain why, when, and how humans make <b>decisions</b>. We consider humans as rational agents, and hence psychology is also to some extent trying to explain rational behavior. One can study the biological principles of how opinions ...", "dateLastCrawled": "2022-01-27T22:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov</b> <b>Decision</b> Processes - Insufficient <b>Information</b>", "url": "https://insufficientinformation.wordpress.com/2019/04/20/an-introduction-to-reinforcement-learning-i-markov-decision-processes/", "isFamilyFriendly": true, "displayUrl": "https://insufficient<b>information</b>.wordpress.com/2019/04/20/an-introduction-to...", "snippet": "It\u2019s about time for us to introduce actions or <b>decisions</b> <b>into</b> our <b>Markov</b> Reward Processes, ... Fairly intuitively, a <b>Markov Decision Process</b> is a <b>Markov</b> Reward <b>Process</b> with <b>decisions</b>. An <b>MDP</b> is an environment in which all states are <b>Markov</b>. MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. The learner and <b>decision</b> maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the ...", "dateLastCrawled": "2022-02-01T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "CS 294-5: Statistical Natural Language Processing", "url": "https://inst.eecs.berkeley.edu/~cs188/sp21/assets/slides/lec23.pptx", "isFamilyFriendly": true, "displayUrl": "https://inst.eecs.berkeley.edu/~cs188/sp21/assets/slides/lec23.pptx", "snippet": "<b>Markov Decision Process</b> (<b>MDP</b>) Environment history: [s 0, a 0, s 1, a 1, \u2026, s t]\u201c<b>Markov</b>\u201d generally means that given the present state, the future and the <b>past</b> are independent. For <b>Markov</b> <b>decision</b> processes, \u201c<b>Markov</b>\u201d means action outcomes depend only on the current state", "dateLastCrawled": "2022-02-01T12:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Markov</b> <b>Decision</b> Processes and Bellman Equations | by Steve Roberts ...", "url": "https://towardsdatascience.com/markov-decision-processes-and-bellman-equations-45234cce9d25", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>markov</b>-<b>decision</b>-<b>process</b>es-and-bellman-equations-45234...", "snippet": "<b>Markov</b> Reward Processes. The <b>Markov</b> <b>process</b> defines a state space and the transition probabilities of moving between those states. It doesn\u2019t specify which states are good states to be in, nor if it\u2019s good to move from one state to another. For this we need to add rewards to the system and move from a <b>Markov</b> <b>process</b> to a <b>Markov</b> Reward <b>process</b>.", "dateLastCrawled": "2022-02-03T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What <b>functions is the &#39;Markov decision process&#39; used for</b> in machine ...", "url": "https://www.quora.com/What-functions-is-the-Markov-decision-process-used-for-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>functions-is-the-Markov-decision-process-used-for</b>-in...", "snippet": "Answer: A mathematical representation of a complex <b>decision</b> <b>making</b> <b>process</b> is \u201c<b>Markov</b> <b>Decision</b> Processes\u201d (<b>MDP</b>). <b>MDP</b> is defined by: * A state S, which represents every state that one could be in, within a defined world. * A model or transition function T; which is a function of the current st...", "dateLastCrawled": "2022-01-17T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "An introduction to Reinforcement Learning | by Sara Ghibaudo | Betacom ...", "url": "https://medium.com/betacom/an-introduction-to-reinforcement-learning-b749abd1f281", "isFamilyFriendly": true, "displayUrl": "https://medium.com/betacom/an-introduction-to-reinforcement-learning-b749abd1f281", "snippet": "At this point, it is important to introduce the <b>Markov Decision Process</b> (<b>MDP</b>) that is defined by a set of states, actions, rewards, and a state transition probability. It models <b>decision</b>-<b>making</b> ...", "dateLastCrawled": "2021-12-16T18:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Mdp</b> - SlideShare", "url": "https://www.slideshare.net/RonaldTeo1/mdp-88266189", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/RonaldTeo1/<b>mdp</b>-88266189", "snippet": "Lecture 2: <b>Markov</b> <b>Decision</b> Processes <b>Markov</b> <b>Decision</b> Processes <b>MDP</b> <b>Markov Decision Process</b> A <b>Markov decision process</b> (<b>MDP</b>) is a <b>Markov</b> reward <b>process</b> with <b>decisions</b>. It is an environment in which all states are <b>Markov</b>. De\ufb01nition A <b>Markov Decision Process</b> is a tuple S, A, P, R, \u03b3 S is a \ufb01nite set of states A is a \ufb01nite set of actions P is a state transition probability matrix, Pa ss = P [St+1 = s | St = s, At = a] R is a reward function, Ra s = E [Rt+1 | St = s, At = a] \u03b3 is a ...", "dateLastCrawled": "2022-01-06T11:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The <b>Algorithm Behind the Curtain: Reinforcement Learning Concepts</b> (2 of ...", "url": "https://randomant.net/reinforcement-learning-concepts/", "isFamilyFriendly": true, "displayUrl": "https://randomant.net/reinforcement-learning-concepts", "snippet": "A simple <b>Markov Decision Process</b> (<b>MDP</b>) that represents a typical workday . A key characteristic of an <b>MDP</b> is that each state must contain all the <b>information</b> the agent needs in order to make an informed <b>decision</b>, a requirement called the \u201c<b>Markov</b> property.\u201d The <b>Markov</b> property basically says that the agent can\u2019t be expected to have any historical memory of its own, outside the state itself. For example, the current state of the Chess board tells me everything I need to know about which ...", "dateLastCrawled": "2022-01-31T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Reinforcement Learning for Formula 1</b> Race Strategy | by Ashref Maiza ...", "url": "https://towardsdatascience.com/reinforcement-learning-for-formula-1-race-strategy-7f29c966472a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning-for-formula-1</b>-race-strategy-7f29...", "snippet": "A Formula 1 race can then be formulated as a <b>Markov Decision Process</b> (<b>MDP</b>) where the probability of transitioning from a state to another relies only on the last observed state. At lap 11, all we need to decide for the next lap is the situation we observe at lap 11 (lap 1 to 10 become much less important). \u201cThe future is independent of the <b>past</b> given the present.\u201d Planning a strategy for the next laps in the race may require a perfect model of the environment where we know exactly what ...", "dateLastCrawled": "2022-02-02T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are <b>some alternative learning paradigms besides Markov</b> <b>decision</b> ...", "url": "https://www.quora.com/What-are-some-alternative-learning-paradigms-besides-Markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>some-alternative-learning-paradigms-besides-Markov</b>...", "snippet": "Answer (1 of 2): Great question! First, we need some history to put things <b>into</b> perspective. <b>Markov</b> <b>decision</b> processes (MDPs) are a probabilistic model of sequential <b>decision</b> <b>making</b>, which originated in operations research (OR) from a remarkable PhD dissertation at MIT by Howard in the late 1950s...", "dateLastCrawled": "2022-01-23T04:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Process</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/markov-decision-process", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>markov-decision-process</b>", "snippet": "A discrete-time <b>Markov Decision Process</b> has been proposed to evaluate the long-term economic benefits of different assets technologies and redundant unit allocation in a chemical <b>process</b>. The discrete <b>decisions</b> and the inspection periods are translated to MINLP model to optimise the long-term discounted cost. The method is tested on a real case study from Sinopec petrochemical plant. This approach is regardless to the initial state of the subsystem, which means that the optimal policies ...", "dateLastCrawled": "2022-02-03T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov</b> <b>Decision</b> Processes - Insufficient <b>Information</b>", "url": "https://insufficientinformation.wordpress.com/2019/04/20/an-introduction-to-reinforcement-learning-i-markov-decision-processes/", "isFamilyFriendly": true, "displayUrl": "https://insufficient<b>information</b>.wordpress.com/2019/04/20/an-introduction-to...", "snippet": "It\u2019s about time for us to introduce actions or <b>decisions</b> <b>into</b> our <b>Markov</b> Reward Processes, ... Fairly intuitively, a <b>Markov Decision Process</b> is a <b>Markov</b> Reward <b>Process</b> with <b>decisions</b>. An <b>MDP</b> is an environment in which all states are <b>Markov</b>. MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. The learner and <b>decision</b> maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the ...", "dateLastCrawled": "2022-02-01T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reinforcement Learning for Clinical <b>Decision</b> Support in Critical Care ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7400046/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7400046", "snippet": "Mathematically, this sequential <b>decision</b>-<b>making</b> <b>process</b> is called the <b>Markov decision process</b> (<b>MDP</b>) . An <b>MDP</b> is defined by 4 major components: (1) a state that represents the environment at each time; (2) an action the agent takes at each time that influences the next state; (3) a transition probability that provides an estimate for reaching different subsequent states, which reflects the environment for an agent to interact with; (4) a reward function is the observed feedback given a state ...", "dateLastCrawled": "2022-01-07T08:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What are <b>some alternative learning paradigms besides Markov</b> <b>decision</b> ...", "url": "https://www.quora.com/What-are-some-alternative-learning-paradigms-besides-Markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>some-alternative-learning-paradigms-besides-Markov</b>...", "snippet": "Answer (1 of 2): Great question! First, we need some history to put things <b>into</b> perspective. <b>Markov</b> <b>decision</b> processes (MDPs) are a probabilistic model of sequential <b>decision</b> <b>making</b>, which originated in operations research (OR) from a remarkable PhD dissertation at MIT by Howard in the late 1950s...", "dateLastCrawled": "2022-01-23T04:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "CS 294-5: Statistical Natural Language Processing", "url": "https://inst.eecs.berkeley.edu/~cs188/sp21/assets/slides/lec23.pptx", "isFamilyFriendly": true, "displayUrl": "https://inst.eecs.berkeley.edu/~cs188/sp21/assets/slides/lec23.pptx", "snippet": "<b>Markov Decision Process</b> (<b>MDP</b>) Environment history: [s 0, a 0, s 1, a 1, \u2026, s t]\u201c<b>Markov</b>\u201d generally means that given the present state, the future and the <b>past</b> are independent. For <b>Markov</b> <b>decision</b> processes, \u201c<b>Markov</b>\u201d means action outcomes depend only on the current state", "dateLastCrawled": "2022-02-01T12:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Decision</b> <b>Making</b> under Uncertainty: A Quasimetric Approach", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3869775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3869775", "snippet": "This problem that we propose to deal with in this paper can be viewed as sequential <b>decision</b> <b>making</b>, usually expressed as a Markovian <b>Decision</b> <b>Process</b> (<b>MDP</b>) \u2013 and its extension to Partially Observable cases (POMDP) , . Knowing the transition probability of switching from one state to another by performing a particular action as well as the associated instantaneous cost, the aim is to define an optimal policy, either deterministic or probabilistic, which maps the state space to the action ...", "dateLastCrawled": "2022-01-29T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What <b>functions is the &#39;Markov decision process&#39; used for</b> in machine ...", "url": "https://www.quora.com/What-functions-is-the-Markov-decision-process-used-for-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>functions-is-the-Markov-decision-process-used-for</b>-in...", "snippet": "Answer: A mathematical representation of a complex <b>decision</b> <b>making</b> <b>process</b> is \u201c<b>Markov</b> <b>Decision</b> Processes\u201d (<b>MDP</b>). <b>MDP</b> is defined by: * A state S, which represents every state that one could be in, within a defined world. * A model or transition function T; which is a function of the current st...", "dateLastCrawled": "2022-01-17T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Reinforcement Learning and Markov Decision Processes</b>", "url": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_<b>Markov</b>...", "snippet": "The <b>Markov Decision Process</b>. Putting all elements together results in the de\ufb01nition of a <b>Markov decision process</b> , which will be the base model for the large majority of methods described in this", "dateLastCrawled": "2022-01-24T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Reinforcement Learning for Formula 1</b> Race Strategy | by Ashref Maiza ...", "url": "https://towardsdatascience.com/reinforcement-learning-for-formula-1-race-strategy-7f29c966472a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning-for-formula-1</b>-race-strategy-7f29...", "snippet": "A Formula 1 race can then be formulated as a <b>Markov Decision Process</b> (<b>MDP</b>) where the probability of transitioning from a state to another relies only on the last observed state. At lap 11, all we need to decide for the next lap is the situation we observe at lap 11 (lap 1 to 10 become much less important). \u201cThe future is independent of the <b>past</b> given the present.\u201d Planning a strategy for the next laps in the race may require a perfect model of the environment where we know exactly what ...", "dateLastCrawled": "2022-02-02T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The <b>Algorithm Behind the Curtain: Reinforcement Learning Concepts</b> (2 of ...", "url": "https://randomant.net/reinforcement-learning-concepts/", "isFamilyFriendly": true, "displayUrl": "https://randomant.net/reinforcement-learning-concepts", "snippet": "A simple <b>Markov Decision Process</b> (<b>MDP</b>) that represents a typical workday . A key characteristic of an <b>MDP</b> is that each state must contain all the <b>information</b> the agent needs in order to make an informed <b>decision</b>, a requirement called the \u201c<b>Markov</b> property.\u201d The <b>Markov</b> property basically says that the agent can\u2019t be expected to have any historical memory of its own, outside the state itself. For example, the current state of the Chess board tells me everything I need to know about which ...", "dateLastCrawled": "2022-01-31T04:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov</b> <b>Decision</b> Processes - Insufficient <b>Information</b>", "url": "https://insufficientinformation.wordpress.com/2019/04/20/an-introduction-to-reinforcement-learning-i-markov-decision-processes/", "isFamilyFriendly": true, "displayUrl": "https://insufficient<b>information</b>.wordpress.com/2019/04/20/an-introduction-to...", "snippet": "Then, once our agent <b>can</b> start <b>making</b> <b>decisions</b> we have ourselves a ... while a discount factor of 1 means that the return takes <b>into</b> <b>account</b> all future rewards (\u2018far-sightedness\u2019). So the closer the gamma factor is to 1, the more future rewards we want to take <b>into</b> <b>account</b>. How much you set the discount factor at depends on the particular situation, for instance, in finance applications, immediate rewards may have more benefits than long-term rewards. It is possible to use un-discounted ...", "dateLastCrawled": "2022-02-01T06:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Learning Cost-Effective Treatment Regimes using Markov</b> <b>Decision</b> ...", "url": "https://deepai.org/publication/learning-cost-effective-treatment-regimes-using-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>learning-cost-effective-treatment-regimes-using-markov</b>...", "snippet": "An element in L <b>can</b> <b>be thought</b> of as a rule in a <b>decision</b> list and an element in C (L) <b>can</b> <b>be thought</b> of a list of rules in a <b>decision</b> list (without the default rule). We then search over all elements in the set C ( L ) \u00d7 A to find a regime which maximizes the expected outcome (Eqn. 5 ) while minimizing the expected assessment (Eqn. 6 ), and treatment costs (Eqn. 7 ) all of which are computed over D .", "dateLastCrawled": "2021-12-30T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reinforcement Learning for Clinical <b>Decision</b> Support in Critical Care ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7400046/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7400046", "snippet": "Mathematically, this sequential <b>decision</b>-<b>making</b> <b>process</b> is called the <b>Markov decision process</b> (<b>MDP</b>) . An <b>MDP</b> is defined by 4 major components: (1) a state that represents the environment at each time; (2) an action the agent takes at each time that influences the next state; (3) a transition probability that provides an estimate for reaching different subsequent states, which reflects the environment for an agent to interact with; (4) a reward function is the observed feedback given a state ...", "dateLastCrawled": "2022-01-07T08:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reinforcement Learning for Formula 1</b> Race Strategy | by Ashref Maiza ...", "url": "https://towardsdatascience.com/reinforcement-learning-for-formula-1-race-strategy-7f29c966472a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning-for-formula-1</b>-race-strategy-7f29...", "snippet": "A Formula 1 race <b>can</b> then be formulated as a <b>Markov Decision Process</b> (<b>MDP</b>) where the probability of transitioning from a state to another relies only on the last observed state. At lap 11, all we need to decide for the next lap is the situation we observe at lap 11 (lap 1 to 10 become much less important). \u201cThe future is independent of the <b>past</b> given the present.\u201d Planning a strategy for the next laps in the race may require a perfect model of the environment where we know exactly what ...", "dateLastCrawled": "2022-02-02T11:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "R&amp;D Connections - Simulations of <b>Thought</b>: The Role of Computational ...", "url": "https://www.ets.org/Media/Research/pdf/RD_Connections_26.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ets.org</b>/Media/Research/pdf/RD_Connections_26.pdf", "snippet": "The <b>Markov decision process</b> (<b>MDP</b>) is a cognitive model that predicts sequential <b>decision</b> <b>making</b> in open-ended environments, such as navigating various steps to solve a problem. <b>MDP</b> models are built to simulate human <b>decision</b> <b>making</b> as a function of a person\u2019s goals and beliefs about the world (Baker, Saxe, &amp; Tenenbaum, 2011). The models are designed to <b>account</b> for the current state of the environment as understood by the <b>decision</b> maker, as well as that person\u2019s predictions of how various ...", "dateLastCrawled": "2022-01-07T18:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Multi-timescale, multi-period <b>decision</b>-<b>making</b> model development by ...", "url": "https://www.sciencedirect.com/science/article/pii/S0098135418308913", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0098135418308913", "snippet": "In this study, we develop a multi-timescale <b>decision</b>-<b>making</b> model that combines <b>Markov decision process</b> (<b>MDP</b>) and mathematical programming (MP) in a complementary way and introduce a computationally tractable solution algorithm based on reinforcement learning (RL) to solve the optimization-embedded <b>MDP</b> problem. Specifically, the proposed modeling approach <b>can</b> address the following challenges: 1) Efficient integration of multi-timescale <b>decision</b> hierarchy by complementary use of MP and RL, 2 ...", "dateLastCrawled": "2021-10-14T01:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>PlanningWithDynamicsAndUncertainty</b>", "url": "http://motion.cs.illinois.edu/RoboticSystems/PlanningWithDynamicsAndUncertainty.html", "isFamilyFriendly": true, "displayUrl": "motion.cs.illinois.edu/RoboticSystems/<b>PlanningWithDynamicsAndUncertainty</b>.html", "snippet": "A <b>Markov Decision Process</b> (<b>MDP</b>) is a principled method for representing <b>decision</b>-<b>making</b> problems under probabilistic movement uncertainty. Using MDPs, we <b>can</b> calculate safer navigation functions that give suitable trade-offs between optimality and collision risk. MDPs are also general-purpose, so they have been useful in modeling uncertainty in obstacle motion, wind gusts for UAVs, traffic delays, human behavior, as well as many other applications outside of robotics.", "dateLastCrawled": "2022-01-30T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A simulation study: comparing the <b>Markov decision process</b> approach to ...", "url": "https://www.deepdyve.com/lp/inderscience-publishers/a-simulation-study-comparing-the-markov-decision-process-approach-to-PDvLvUA4Tv", "isFamilyFriendly": true, "displayUrl": "https://www.deepdyve.com/lp/inderscience-publishers/a-simulation-study-comparing-the...", "snippet": "In this article, we compare <b>Markov Decision Process</b> (<b>MDP</b>) to expected marginal seat revenue b (EMSRb) under realistic demand behaviour by implementing both in a stochastic simulation model. First come first served as well as an upper bound, serves for reference. We give a short explanation of the investigated methods, describe the simulation setup, in particular the demand model, and carry out sensitivity analyses regarding settings of the demand generator and the optimisation methods ...", "dateLastCrawled": "2020-06-09T16:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Artificial intelligence framework for simulating clinical <b>decision</b> ...", "url": "https://www.researchgate.net/publication/234050168_Artificial_intelligence_framework_for_simulating_clinical_decision-making_A_Markov_decision_process_approach", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/234050168_Artificial_intelligence_framework...", "snippet": "The traditional RL approach [13], which is formulated based on <b>Markov decision process</b> (<b>MDP</b>) [8,12,14, 15], enables a single agent (or a <b>decision</b> maker) to interact with its operating environment ...", "dateLastCrawled": "2021-12-12T07:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Information</b> Structures for Causally Explainable <b>Decisions</b>", "url": "https://www.mdpi.com/1099-4300/23/5/601/htm", "isFamilyFriendly": true, "displayUrl": "https://www.<b>mdp</b>i.com/1099-4300/23/5/601/htm", "snippet": "For example, a <b>Markov decision process</b> (<b>MDP</b>) is a multistate transition reward <b>process</b>, for which a policy specifies which feasible action to take in each state. Actions <b>can</b> affect rewards from transitions as well as next-state transition probabilities. MDPs with finite numbers of states and actions <b>can</b> be solved for optimal policies that maximize expected discounted rewards or average rewards per unit time by using either linear programming, dynamic programming, or well-known special ...", "dateLastCrawled": "2022-01-26T08:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Process</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/markov-decision-process", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>markov-decision-process</b>", "snippet": "The classical formalism of <b>Markov Decision Process</b> (<b>MDP</b>) was implemented to aid the learning feature in the agent representing the CO 2 distribution centre. More specifically, a temporal difference learning approach called Q-learning was used to maximise the expected cumulative value of an action (a) taken under a given state (s) of the agent during the simulation period of one year. More formally, the network objectives, namely order fulfilment time and utilisation rate, are modelled as ...", "dateLastCrawled": "2022-02-03T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A simulation study: comparing the <b>Markov decision process</b> approach to ...", "url": "https://www.deepdyve.com/lp/inderscience-publishers/a-simulation-study-comparing-the-markov-decision-process-approach-to-PDvLvUA4Tv", "isFamilyFriendly": true, "displayUrl": "https://www.deepdyve.com/lp/inderscience-publishers/a-simulation-study-comparing-the...", "snippet": "In this article, we compare <b>Markov Decision Process</b> (<b>MDP</b>) to expected marginal seat revenue b (EMSRb) under realistic demand behaviour by implementing both in a stochastic simulation model. First come first served as well as an upper bound, serves for reference. We give a short explanation of the investigated methods, describe the simulation setup, in particular the demand model, and carry out sensitivity analyses regarding settings of the demand generator and the optimisation methods ...", "dateLastCrawled": "2020-06-09T16:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Markov</b> <b>Decision</b> Processes - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/mathematics/markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/mathematics/<b>markov</b>-<b>decision</b>-<b>process</b>es", "snippet": "<b>Markov</b> <b>Decision</b> Processes. A finite <b>MDP</b> <b>can</b> be defined using the four-tuple (S,A,P,R), where S is a finite set of states of the system or environment, A is a finite set of possible actions when in the states sk\u2208S, P represents a state transition probability matrix, Pak(sk,sk+1) is the probability that the state sk\u2208S at k transits to the state sk+1 at k + 1 with an action ak\u2208A at k, and R represents a reward that assesses the advantage of an action ak\u2208A for all sk\u2208S.", "dateLastCrawled": "2022-01-06T10:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Markov Decision Processes and Its Applications in Healthcare</b>", "url": "https://www.researchgate.net/publication/281272258_Markov_Decision_Processes_and_Its_Applications_in_Healthcare", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/281272258_<b>Markov_Decision_Processes_and_Its</b>...", "snippet": "This need not be the case when the redeployment problem is formulated as a <b>Markov Decision Process</b> (<b>MDP</b>) [21], as other <b>information</b> related to the system state <b>can</b> be captured in the <b>decision</b> ...", "dateLastCrawled": "2022-01-21T22:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Reinforcement Learning for Clinical <b>Decision</b> Support in Critical Care ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7400046/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7400046", "snippet": "Mathematically, this sequential <b>decision</b>-<b>making</b> <b>process</b> is called the <b>Markov decision process</b> (<b>MDP</b>) . An <b>MDP</b> is defined by 4 major components: (1) a state that represents the environment at each time; (2) an action the agent takes at each time that influences the next state; (3) a transition probability that provides an estimate for reaching different subsequent states, which reflects the environment for an agent to interact with; (4) a reward function is the observed feedback given a state ...", "dateLastCrawled": "2022-01-07T08:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Reinforcement Learning and Markov Decision Processes</b>", "url": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_<b>Markov</b>...", "snippet": "The <b>Markov Decision Process</b>. Putting all elements together results in the de\ufb01nition of a <b>Markov decision process</b> , which will be the base model for the large majority of methods described in this", "dateLastCrawled": "2022-01-24T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Decision</b> <b>Making</b> under Uncertainty: A Quasimetric Approach", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3869775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3869775", "snippet": "This problem that we propose to deal with in this paper <b>can</b> be viewed as sequential <b>decision</b> <b>making</b>, usually expressed as a Markovian <b>Decision</b> <b>Process</b> (<b>MDP</b>) \u2013 and its extension to Partially Observable cases (POMDP) , . Knowing the transition probability of switching from one state to another by performing a particular action as well as the associated instantaneous cost, the aim is to define an optimal policy, either deterministic or probabilistic, which maps the state space to the action ...", "dateLastCrawled": "2022-01-29T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Markov decision process</b>\u2010based computation offloading algorithm and ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-com.2020.0062", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-com.2020.0062", "snippet": "Generally speaking, <b>MDP</b> is the stochastic dynamic optimal <b>decision</b> <b>process</b> that has <b>Markov</b> property that the future system state is dependent only on the current state, not the <b>past</b> states . We model an offloading <b>decision</b> <b>process</b> as known stochastic model-<b>MDP</b> in terms of a finite state space , a finite action space , transition probability and immediate reward [ 37 ].", "dateLastCrawled": "2021-12-04T09:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What are <b>the differences between hidden Markov models</b> and partially ...", "url": "https://www.quora.com/What-are-the-differences-between-hidden-Markov-models-and-partially-observed-Markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>the-differences-between-hidden-Markov-models</b>-and...", "snippet": "Answer: They are similar in that both assume the true state of the system is unknown, but we <b>can</b> hope to make inferences about it based on observations. Also, both make some sort of <b>Markov</b> assumption about the state not depending on the entire history of the system. The key difference, at a high...", "dateLastCrawled": "2022-01-13T08:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Reinforcement Learning for Formula 1</b> Race Strategy | by Ashref Maiza ...", "url": "https://towardsdatascience.com/reinforcement-learning-for-formula-1-race-strategy-7f29c966472a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning-for-formula-1</b>-race-strategy-7f29...", "snippet": "A Formula 1 race <b>can</b> then be formulated as a <b>Markov Decision Process</b> (<b>MDP</b>) where the probability of transitioning from a state to another relies only on the last observed state. At lap 11, all we need to decide for the next lap is the situation we observe at lap 11 (lap 1 to 10 become much less important). \u201cThe future is independent of the <b>past</b> given the present.\u201d Planning a strategy for the next laps in the race may require a perfect model of the environment where we know exactly what ...", "dateLastCrawled": "2022-02-02T11:37:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does <b>Markov Decision Process</b> matter in Reinforcement <b>Learning</b>? | by ...", "url": "https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-does-malkov-<b>decision</b>-<b>process</b>-matter-in...", "snippet": "It is named by <b>analogy</b> to \u201cone-armed bandit\u201d(= a slot <b>machine</b>) although the framework has k levers instead of one. ... we introduce <b>Markov Decision Process</b>(<b>MDP</b>) to solve such a problem. An <b>MDP</b> consists of two elements; the agent and the environment. The agent is a learner or <b>decision</b>-maker. In the above example, the agent is the rabbit. The environment is everything surrounding the agent. In the example, the environment includes everything in the field where the rabbit is with food and ...", "dateLastCrawled": "2022-01-31T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b>: How Does Value Iteration Work? | Baeldung on ...", "url": "https://www.baeldung.com/cs/mdp-value-iteration", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>mdp</b>-value-iteration", "snippet": "From this point, we can make an <b>analogy</b> with the <b>Markov</b> model since the solution for this problem is a sequence of actions. A <b>Markov Decision Process</b> is used to model the agent, considering that the agent itself generates a series of actions. In the real world, we can have observable, hidden, or partially observed states, depending on the ...", "dateLastCrawled": "2022-01-30T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Markov decision process</b>: value iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-value-iteration-2d161d50a6ff", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-value-iteration-2d161d50a6ff", "snippet": "<b>Markov decision process</b>, <b>MDP</b>, value iteration, policy iteration, policy evaluation, policy improvement, sweep, iterative policy evaluation, policy, optimal policy ...", "dateLastCrawled": "2022-01-08T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "18.1. <b>Markov Decision Process</b> (<b>MDP</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.0 ...", "url": "http://preview.d2l.ai.s3-website-us-west-2.amazonaws.com/d2l-en/master/chapter_reinforcement_learning/mdp.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai.s3-website-us-west-2.amazonaws.com/...reinforcement_<b>learning</b>/<b>mdp</b>.html", "snippet": "In this section, we will discuss how to formulate reinforcement <b>learning</b> problems using <b>Markov</b> <b>decision</b> processes (MDPs) and describe in detail various components of MDPs. Definition of an <b>MDP</b> \u00b6 A <b>Markov decision process</b> (<b>MDP</b>) is a model for how the state of a system evolves as different actions are applied to it.", "dateLastCrawled": "2022-01-27T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Markov decision process</b>: policy iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-policy-iteration-42d35ee87c82?source=post_internal_links---------0-------------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-policy-iteration-42d35ee87c82?source=...", "snippet": "<b>Markov decision process</b>: policy iteration with code implementation . Nan. Dec 19, 2021 \u00b7 16 min read. In today\u2019s story we focus on policy iteration of <b>MDP</b>. We are still using the grid world ...", "dateLastCrawled": "2022-01-22T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>", "url": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "snippet": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>. <b>Machine</b> <b>Learning</b> [cs.LG]. Uni-versit\u00e9 Paris Saclay (COmUE), 2019. English. \uffffNNT: 2019SACLS588\uffff. \ufffftel-02422144v2\uffff I would like to dedicate this thesis to my loving parents doctorat CLS588 Meta-<b>Learning</b> as a <b>Markov Decision Process</b> Th\u00e8se de doctorat de l\u2019Universit\u00e9 Paris-Saclay pr\u00e9par\u00e9e \u00e0 l\u2019Universit\u00e9 Paris-Sud Ecole doctorale n 580 Sciences et Technologies de l\u2019Information et de la Communication (STIC) Sp\u00e9cialit\u00e9 de doctorat ...", "dateLastCrawled": "2022-01-13T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Why is the <b>optimal policy</b> in <b>Markov Decision Process</b> ...", "url": "https://stats.stackexchange.com/questions/132890/why-is-the-optimal-policy-in-markov-decision-process-mdp-independent-of-the-i", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132890", "snippet": "The intuition behind the argument saying that the <b>optimal policy</b> is independent of initial state is the following: The <b>optimal policy</b> is defined by a function that selects an action for every possible state and actions in different states are independent.. Formally speaking, for an unknown initial distribution, the value function to maximize would be the following (not conditioned on initial state)", "dateLastCrawled": "2022-01-25T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Real-life <b>examples</b> of <b>Markov</b> <b>Decision</b> Processes - Cross Validated", "url": "https://stats.stackexchange.com/questions/145122/real-life-examples-of-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/145122", "snippet": "A Markovian <b>Decision</b> <b>Process</b> indeed has to do with going from one state to another and is mainly used for planning and <b>decision</b> making. The theory. Just repeating the theory quickly, an <b>MDP</b> is: $$\\text{<b>MDP</b>} = \\langle S,A,T,R,\\gamma \\rangle$$", "dateLastCrawled": "2022-01-24T10:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Overview: Representation Techniques", "url": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66fffb5d0bd4c20697922f5ffbf9a602b66bec3f74ac83fb77c/DecisionMaking.pdf", "isFamilyFriendly": true, "displayUrl": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66...", "snippet": "<b>Markov Decision Process MDP is like</b> a Markov process, except every round we make a decision Transition probabilities depend on actions taken P(St+1 = S&#39; | St = s, At = a) = P(S, a, S&#39;) Rewards for every state, action pair u(St = s, At = a) Discount factor \u03b4 Example. A <b>machine</b> can be in one of three states: good, deteriorating, broken Can take ...", "dateLastCrawled": "2022-01-21T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(markov decision process (mdp))  is like +(making better decisions by taking into account information about the past)", "+(markov decision process (mdp)) is similar to +(making better decisions by taking into account information about the past)", "+(markov decision process (mdp)) can be thought of as +(making better decisions by taking into account information about the past)", "+(markov decision process (mdp)) can be compared to +(making better decisions by taking into account information about the past)", "machine learning +(markov decision process (mdp) AND analogy)", "machine learning +(\"markov decision process (mdp) is like\")", "machine learning +(\"markov decision process (mdp) is similar\")", "machine learning +(\"just as markov decision process (mdp)\")", "machine learning +(\"markov decision process (mdp) can be thought of as\")", "machine learning +(\"markov decision process (mdp) can be compared to\")"]}