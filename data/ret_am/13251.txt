{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Chapter 20: <b>Function</b> and Region Shapes, the Karush-Kuhn- Tucker (KKT ...", "url": "https://www.researchgate.net/profile/Mohamed-Mourad-Lafifi/post/Is_there_any_method_that_can_obtain_the_analytical_solution_of_the_integer_programming/attachment/59d644d979197b80779a00af/AS%3A450385493139466%401484391677571/download/Chapter20.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/profile/Mohamed-Mourad-Lafifi/post/Is_there_any_method...", "snippet": "composed of <b>convex</b> or concave parts (e.g. the sum of <b>convex</b> functions is itself a <b>convex</b> <b>function</b>), so you can often reason out the shape of the complete <b>function</b>. This is the approach", "dateLastCrawled": "2022-01-03T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "LESSON 6: GEOMORPHOLOGY II SECTION A: SLOPES", "url": "https://learn.mindset.africa/sites/default/files/resourcelib/emshare-show-note-asset/774_fdoc.pdf", "isFamilyFriendly": true, "displayUrl": "https://learn.mindset.africa/sites/default/files/resourcelib/emshare-show-note-asset/...", "snippet": "<b>Gentle</b> <b>Slope</b> A <b>slope</b> with contour lines spread far apart from each other. This even spacing is maintained in both up and down <b>slope</b>. Steep <b>Slope</b> A <b>slope</b> represented with contour lines close to each other on a topographical map. <b>Convex</b> <b>Slope</b> A <b>slope</b> which becomes progressively steeper downhill. It can refer to an entire <b>slope</b> or part of one. On a map the contour lines will be spaced closer together with a decline in height above sea-level. Concave <b>Slope</b> A <b>slope</b> which becomes progressively ...", "dateLastCrawled": "2022-02-02T05:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Map Contours - Scouting Resources", "url": "https://www.scoutingresources.org.uk/downloads/mapping_contours_001.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.scoutingresources.org.uk/downloads/mapping_contours_001.pdf", "snippet": "<b>convex</b> When the contour lines of a <b>hill</b> are close together at first, then get farther apart. The <b>hill</b> will be hard to climb at first, but then it becomes easy. valley A feature shown on a map by the contour lines becoming V-shaped. The V points uphill. <b>Gentle</b> When contour lines are far apart, the slope of the <b>hill</b> is <b>gentle</b>. steep", "dateLastCrawled": "2022-01-30T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What Is Concave And <b>Convex</b> Slope? \u2013 chetumenu.com", "url": "https://chetumenu.com/what-is-concave-and-convex-slope/", "isFamilyFriendly": true, "displayUrl": "https://chetumenu.com/what-is-concave-and-<b>convex</b>-slope", "snippet": "A terrain feature that is rounded inward <b>like</b> the inside of a bowl, i.e. goes from more steep to less steep. Slope Shape: Slope shape makes more difference on smaller slopes than on larger ones. Moreover, How are <b>convex</b> slopes formed? <b>Convex</b> slope elements are usually <b>gentle</b>, generally at the top of a slope, and formed by soil creep and rainsplash. Slopes with shallow debris and steep slopes undercut by rivers or waves both tend to have straight segments; talus slopes are often straight ...", "dateLastCrawled": "2022-01-22T03:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Univariate Function Optimization in Python</b>", "url": "https://machinelearningmastery.com/univariate-function-optimization-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>univariate-function-optimization-in-python</b>", "snippet": "Non-<b>Convex</b> Univariate <b>Function</b> Optimization. A <b>convex</b> <b>function</b> is one that does not resemble a basin, meaning that it may have more than one <b>hill</b> or valley. This can make it more challenging to locate the global optima as the multiple hills and valleys can cause the search to get stuck and report a false or local optima instead.", "dateLastCrawled": "2022-02-01T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>What is a Cervical Curvature and Why is</b> it Important?", "url": "https://clear-institute.org/blog/cervical-curvature/", "isFamilyFriendly": true, "displayUrl": "https://clear-institute.org/blog/cervical-curvature", "snippet": "the sacral and coccyx region feature a <b>gentle</b> <b>convex</b> curve at the base of the spinal cord. <b>Function</b> of the Curves. Together, the five curves of the spine maintain its strength and flexibility. The curves <b>function</b> <b>like</b> a coiled spring to absorb force, maintain the body\u2019s center of gravity and balance, and allow the body to bend and twist. Engineers tell us that the amount of resistance boils down to a formula: the number of curves squared plus one. Five curves multiplied by five, plus one ...", "dateLastCrawled": "2022-02-03T13:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "INTRODUCTION TO INFORMATION THEORY", "url": "https://web.stanford.edu/~montanar/RESEARCH/BOOK/partA.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/~montanar/RESEARCH/BOOK/partA.pdf", "snippet": "in a set X \u2286 R and f a <b>convex</b> <b>function</b> (i.e. a <b>function</b> such that \u2200x,yand \u2200\u03b1\u2208 [0,1]: f(\u03b1x+(1\u2212\u03b1y)) \u2264 \u03b1f(x)+(1\u2212\u03b1)f(y)). Then {eq:Jensen} Ef(X) \u2265 f(EX) . (1.6) Supposing for simplicity that X is a \ufb01nite set with |X| = n, prove this equality by recursion on n. 1.2 Entropy {se:entropy} The entropy HX of a discrete random variable Xwith probability distribution p(x) is de\ufb01ned as HX\u2261 \u2212 X x\u2208X p(x)log 2p(x) = Elog 1 p(X) {S_def}, (1.7) \u2018\u2018Info Phys Comp\u2019\u2019 Draft ...", "dateLastCrawled": "2022-02-03T06:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Deep Learning Optimizers. In Deep Learning the optimizers play an\u2026 | by ...", "url": "https://medium.com/mlearning-ai/deep-learning-optimizers-4c13d0799b4d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/mlearning-ai/deep-learning-optimizers-4c13d0799b4d", "snippet": "Gradient descent is a <b>convex</b> <b>function</b>. We know we want to find the values of w and b that correspond to the minimum of the cost <b>function</b> (marked with the red arrow).", "dateLastCrawled": "2022-01-30T10:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Gradient Descent</b> Explained. A comprehensive guide to Gradient\u2026 | by ...", "url": "https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent</b>-explained-9b953fc0d2c", "snippet": "This is feasible if the objective <b>function</b> is <b>convex</b>, i.e. any local minimum is a global minimum. Find the lowest possible value of the objective <b>function</b> within its neighborhood. That\u2019s usually the case if the objective <b>function</b> is not <b>convex</b> as the case in most deep learning problems. <b>Gradient Descent</b>. <b>Gradient Descent</b> is an optimizing algorithm used in Machine/ Deep Learning algorithms. The goal of <b>Gradient Descent</b> is to minimize the objective <b>convex</b> <b>function</b> f(x) using iteration ...", "dateLastCrawled": "2022-02-02T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Logistic Regression</b>. By Neeta Ganamukhi | by Neeta Ganamukhi | The ...", "url": "https://medium.com/swlh/logistic-regression-7791655bc480", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>logistic-regression</b>-7791655bc480", "snippet": "That\u2019s why we still need a neat <b>convex</b> <b>function</b> just <b>like</b> in linear regression, a bowl-shaped <b>function</b> that eases the gradient descent <b>function</b>\u2019s work to converge to the optimal minimum point ...", "dateLastCrawled": "2022-02-01T21:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "LESSON 6: GEOMORPHOLOGY II SECTION A: SLOPES", "url": "https://learn.mindset.africa/sites/default/files/resourcelib/emshare-show-note-asset/774_fdoc.pdf", "isFamilyFriendly": true, "displayUrl": "https://learn.mindset.africa/sites/default/files/resourcelib/emshare-show-note-asset/...", "snippet": "<b>Gentle</b> <b>Slope</b> A <b>slope</b> with contour lines spread far apart from each other. This even spacing is maintained in both up and down <b>slope</b>. Steep <b>Slope</b> A <b>slope</b> represented with contour lines close to each other on a topographical map. <b>Convex</b> <b>Slope</b> A <b>slope</b> which becomes progressively steeper downhill. It can refer to an entire <b>slope</b> or part of one. On a map the contour lines will be spaced closer together with a decline in height above sea-level. Concave <b>Slope</b> A <b>slope</b> which becomes progressively ...", "dateLastCrawled": "2022-02-02T05:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Hillslope evolution</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Hillslope_evolution", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Hillslope_evolution</b>", "snippet": "<b>Convex</b> forms can thus indirectly reflect accelerated crustal uplift and its associated river incision. [9] [10] [A] As shown by equation 2 the angle of steep slopes changes very little even at very high increases of erosion rates, meaning that it is not possible to infer erosion rates from topography in steep slopes other than hinting they are much higher than for lower angle slopes.", "dateLastCrawled": "2022-01-28T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Gradient Descent</b> Explained. A comprehensive guide to Gradient\u2026 | by ...", "url": "https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent</b>-explained-9b953fc0d2c", "snippet": "This is feasible if the objective <b>function</b> is <b>convex</b>, i.e. any local minimum is a global minimum. Find the lowest possible value of the objective <b>function</b> within its neighborhood. That\u2019s usually the case if the objective <b>function</b> is not <b>convex</b> as the case in most deep learning problems. <b>Gradient Descent</b>. <b>Gradient Descent</b> is an optimizing algorithm used in Machine/ Deep Learning algorithms. The goal of <b>Gradient Descent</b> is to minimize the objective <b>convex</b> <b>function</b> f(x) using iteration ...", "dateLastCrawled": "2022-02-02T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Chapter 4: Non-Linear Conservation Laws, the Scalar Case</b>", "url": "https://www3.nd.edu/~dbalsara/Numerical-PDE-Course/ch4/Chp4_NonLinScalr.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www3.nd.edu</b>/~dbalsara/Numerical-PDE-Course/ch4/Chp4_NonLinScalr.pdf", "snippet": "disjoint from <b>similar</b> structures from another characteristic family. This enables one to prove that certain solution techniques for a hyperbolic system with a <b>convex</b> flux will produce results that will always converge to the physical solution (Lax 1972, Harten 1983a). We, therefore, devote much of our attention in this chapter to scalar hyperbolic equations with <b>convex</b> fluxes. We do, however, point out that many physical systems can be . non-<b>convex</b>, prominent examples being the MHD and ...", "dateLastCrawled": "2022-01-26T04:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Convex</b> Optimization Theory Chapter 2 Exercises And", "url": "http://backend.steexp.com/upload/pub.php?article=convex_optimization_theory_chapter_2_exercises_and_pdf&encrypt=a900aee85e0bb36507e47da53eb30d20", "isFamilyFriendly": true, "displayUrl": "backend.steexp.com/upload/pub.php?article=<b>convex</b>_optimization_theory_chapter_2...", "snippet": "<b>convex</b>, nonsmooth, and variational \u2026It <b>is similar</b> in style to the author&#39;s 2009 <b>Convex</b> Optimization Theory book, but can be read independently. The latter book focuses on convexity theory and optimization duality, while the 2015 <b>Convex</b> Optimization Algorithms book focuses on algorithmic issues. The two books share notation, and together cover ...", "dateLastCrawled": "2021-10-23T11:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Tillage and its Implements", "url": "http://www.hillagric.ac.in/edu/coa/agengg/lecture/243/lecture%2011%20primary%20tillage.pdf", "isFamilyFriendly": true, "displayUrl": "www.<b>hill</b>agric.ac.in/edu/coa/agengg/lecture/243/lecture 11 primary tillage.pdf", "snippet": "<b>Function</b>: 1) cutting the furrow slice 2) lifting the soil 3) turning the furrow slice and 4) pulverising the soil. Components M.B. plough consists of (Fig.2 ) Share, Mould board, Land side, Frog and Tail piece. Share - It penetrates into the soil and makes a horizontal cut below the soil surface. It is a sharp, well polished and pointed component. The shares are made of chilled cast iron or steel. The steel mainly contains about 0.70 to 0.80% carbon and about 0.50 to 0.80% manganese besides ...", "dateLastCrawled": "2022-02-03T04:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Question Bank \u00b7 AIMA Exercises", "url": "https://aimacode.github.io/aima-exercises/question_bank/", "isFamilyFriendly": true, "displayUrl": "https://aimacode.github.io/aima-exercises/question_bank", "snippet": "Implement and test a <b>hill</b>-climbing method to solve TSPs. Compare the results with optimal solutions obtained from the A* algorithm with the MST heuristic (Exercise tsp-mst-exercise) 2. Repeat part (a) using a genetic algorithm instead of <b>hill</b> climbing. You may want to consult @Larranaga+al:1999 for some suggestions for representations. Exercise 4 (<b>hill</b>-climbing-exercise) Generate a large number of 8-puzzle and 8-queens instances and solve them (where possible) by <b>hill</b> climbing (steepest ...", "dateLastCrawled": "2022-02-02T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A <b>Gentle</b> <b>Introduction to the BFGS Optimization Algorithm</b>", "url": "https://machinelearningmastery.com/bfgs-optimization-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/bfgs-optimization-in-python", "snippet": "A <b>Gentle</b> <b>Introduction to the BFGS Optimization Algorithm</b>. The Broyden, Fletcher, Goldfarb, and Shanno, or BFGS Algorithm, is a local search optimization algorithm. It is a type of second-order optimization algorithm, meaning that it makes use of the second-order derivative of an objective <b>function</b> and belongs to a class of algorithms referred ...", "dateLastCrawled": "2022-01-29T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Logistic Regression</b>. By Neeta Ganamukhi | by Neeta Ganamukhi | The ...", "url": "https://medium.com/swlh/logistic-regression-7791655bc480", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>logistic-regression</b>-7791655bc480", "snippet": "Fig1.Sigmoid <b>Function</b>.. The following equation is used to presents Sigmoid <b>function</b>: 1/1+e^-z.The classes can be divided into positive or negative.The output is the probability of positive class ...", "dateLastCrawled": "2022-02-01T21:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A <b>Gentle</b> Introduction to Particle Swarm Optimization", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-particle-swarm-optimization/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/a-<b>gentle</b>-introduction-to-particle-swarm-optimization", "snippet": "<b>Similar</b> to the flock of birds looking for food, we start with a number of random points on the plane (call them particles) and let them look for the minimum point in random directions. At each step, every particle should search around the minimum point it ever found as well as around the minimum point found by the entire swarm of particles. After certain iterations, we consider the minimum point of the <b>function</b> as the minimum point ever explored by this swarm of particles.", "dateLastCrawled": "2022-02-02T23:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A (<b>Gentle) Introduction to Behavioral Economics</b> : American Journal of ...", "url": "https://www.ajronline.org/doi/full/10.2214/AJR.13.11352", "isFamilyFriendly": true, "displayUrl": "https://www.ajronline.org/doi/full/10.2214/AJR.13.11352", "snippet": "Because of the shape of the prospect theory utility <b>function</b>, which is concave in the gains domain and <b>convex</b> in the losses domain, it is more painful to experience two $5 losses than it is to experience one $10 loss. If we shift toward health insurance with relatively lower premiums and relatively higher copays, we may induce greater disutility even if the aggregate amount paid is unchanged. Prospect theory suggests that disaggregating the cost of care into multiple small payments should ...", "dateLastCrawled": "2021-12-25T18:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Hillslope evolution</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Hillslope_evolution", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Hillslope_evolution</b>", "snippet": "Following this <b>thought</b> erosion by the sea and lateral stream migration are of prime importance as these ... At low erosion rates increased stream or river incision may make <b>gentle</b> slopes evolve into <b>convex</b> forms. <b>Convex</b> forms <b>can</b> thus indirectly reflect accelerated crustal uplift and its associated river incision. As shown by equation 2 the angle of steep slopes changes very little even at very high increases of erosion rates, meaning that it is not possible to infer erosion rates from ...", "dateLastCrawled": "2022-01-28T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 2, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Convex hull</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Convex_hull", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Convex_hull</b>", "snippet": "It is the unique maximal <b>convex</b> <b>function</b> majorized by . The definition <b>can</b> be ... The orthogonal <b>convex hull</b> is a special case of a much more general construction, the hyperconvex hull, which <b>can</b> <b>be thought</b> of as the smallest injective metric space containing the points of a given metric space. The holomorphically <b>convex hull</b> is a generalization of similar concepts to complex analytic manifolds, obtained as an intersection of sublevel sets of holomorphic functions containing a given set. The ...", "dateLastCrawled": "2022-02-02T13:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Functional Analysis Lecture Notes, Spring 2020", "url": "https://ocw.mit.edu/courses/mathematics/18-102-introduction-to-functional-analysis-spring-2021/lecture-notes-and-readings/MIT18_102s20_lec_FA.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/mathematics/18-102-introduction-to-<b>function</b>al-analysis...", "snippet": "<b>Hill</b>\u2019s equation 161 2. Mehler\u2019s formula and completeness 162 3. Friedrichs\u2019 extension 166 4. Dirichlet problem revisited 170 5. Isotropic space 171 Appendix. Bibliography 175 . PREFACE 5 Preface These are notes for the course \u2018Introduction to Functional Analysis\u2019 { or in the MIT style, 18.102, from various years culminating in Spring 2020. There are many people who I should like to thank for comments on and corrections to the notes over the years, but for the moment I would simply ...", "dateLastCrawled": "2022-01-31T06:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Gentle</b> <b>Introduction to the BFGS Optimization Algorithm</b>", "url": "https://machinelearningmastery.com/bfgs-optimization-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/bfgs-optimization-in-python", "snippet": "A <b>Gentle</b> <b>Introduction to the BFGS Optimization Algorithm</b>. The Broyden, Fletcher, Goldfarb, and Shanno, or BFGS Algorithm, is a local search optimization algorithm. It is a type of second-order optimization algorithm, meaning that it makes use of the second-order derivative of an objective <b>function</b> and belongs to a class of algorithms referred ...", "dateLastCrawled": "2022-01-29T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Question Bank \u00b7 AIMA Exercises", "url": "https://aimacode.github.io/aima-exercises/question_bank/", "isFamilyFriendly": true, "displayUrl": "https://aimacode.github.io/aima-exercises/question_bank", "snippet": "Consider the problem of finding the shortest path between two points on a plane that has <b>convex</b> polygonal obstacles as shown in . This is an idealization of the problem that a robot has to solve to navigate in a crowded environment. 1. Suppose the state space consists of all positions $(x,y)$ in the plane. How many states are there? How many paths are there to the goal? 2. Explain briefly why the shortest path from one polygon vertex to any other in the scene must consist of straight-line ...", "dateLastCrawled": "2022-02-02T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Utility</b> Maximization Problem Questions and Answers | Study.com", "url": "https://study.com/learn/utility-maximization-problem-questions-and-answers.html", "isFamilyFriendly": true, "displayUrl": "https://study.com/learn/<b>utility</b>-maximization-problem-questions-and-answers.html", "snippet": "Suppose there is a consumer whose <b>utility</b> <b>can</b> be expressed by the <b>utility</b> <b>function</b> u(x, y) = 10x^{0.4}y^{ 0.6} where x is the quantity of good X consumed and y is the quantity of good Y consumed. G...", "dateLastCrawled": "2022-02-03T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Python Machine Learning: Machine Learning and Deep Learning with Python ...", "url": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with-python-scikit-learn-and-tensorflow-2-3rd-edition-3nbsped-9781789955750-1789955750.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/python-machine-learning-machine-learning-and-deep-learning-with...", "snippet": "Another nice property of this cost <b>function</b> is that it is <b>convex</b>; thus, we <b>can</b> use a very simple yet powerful optimization algorithm called gradient descent to find the weights that minimize our cost <b>function</b> to classify the examples in the Iris dataset. As illustrated in the following figure, we <b>can</b> describe the main idea behind gradient descent as climbing down a <b>hill</b> until a local or global cost minimum is reached. In each iteration, we take a step in the opposite direction of the ...", "dateLastCrawled": "2022-01-31T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Hillslope</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/earth-and-planetary-sciences/hillslope", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/earth-and-planetary-sciences/<b>hillslope</b>", "snippet": "The road <b>can</b> operate as a slope-outlet decoupling system, creating dis-connectivity between the <b>hillslope</b> and the catchment outlet (blue colors on the road carriageway in Fig. 5 C), or in some locations, as a slope-outlet coupling system (red colors on the road carriageway and downstream the road in Fig. 5 C). These features could <b>function</b> as a switch that <b>can</b> turn on/off processes of sediment delivery, determining the amount of sediment potentially reaching the watershed outlet.", "dateLastCrawled": "2022-02-02T05:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "45 Irrevocably Enchanting Quotes About the Beauty of <b>Clouds</b> - Quotabulary", "url": "https://quotabulary.com/quotes-about-beauty-of-clouds", "isFamilyFriendly": true, "displayUrl": "https://quotabulary.com/quotes-about-beauty-of-<b>clouds</b>", "snippet": "\u201cNo doubt the phenomena of cloud formation are designed primarily to water the earth; to gather together the moisture from the salt sea, and form dark, unwholesome fens; to purify them by the mysterious alchemy of the sky; to carry them onward by sweeping storm or by <b>gentle</b> zephyr, and let them descend gently in the mist, or steadily in the rain, which will waken sleeping seeds, and revive drooping vegetation.\u201d \u2015 Alfred Rowland", "dateLastCrawled": "2022-02-02T08:34:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>Gentle</b> <b>Introduction to Function Optimization</b>", "url": "https://machinelearningmastery.com/introduction-to-function-optimization/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>introduction-to-function-optimization</b>", "snippet": "<b>Function</b> optimization is a foundational area of study and the techniques are used in almost every quantitative field. Importantly, <b>function</b> optimization is central to almost all machine learning algorithms, and predictive modeling projects. As such, it is critical to understand what <b>function</b> optimization is, the terminology used in the field, and the elements that constitute a <b>function</b> optimization problem. In this tutorial,", "dateLastCrawled": "2022-02-03T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Gradient Descent</b> Explained. A comprehensive guide to Gradient\u2026 | by ...", "url": "https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>gradient-descent</b>-explained-9b953fc0d2c", "snippet": "This is feasible if the objective <b>function</b> is <b>convex</b>, i.e. any local minimum is a global minimum. Find the lowest possible value of the objective <b>function</b> within its neighborhood. That\u2019s usually the case if the objective <b>function</b> is not <b>convex</b> as the case in most deep learning problems. <b>Gradient Descent</b>. <b>Gradient Descent</b> is an optimizing algorithm used in Machine/ Deep Learning algorithms. The goal of <b>Gradient Descent</b> is to minimize the objective <b>convex</b> <b>function</b> f(x) using iteration ...", "dateLastCrawled": "2022-02-02T16:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Chapter 4: Non-Linear Conservation Laws, the Scalar Case</b>", "url": "https://www3.nd.edu/~dbalsara/Numerical-PDE-Course/ch4/Chp4_NonLinScalr.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www3.nd.edu</b>/~dbalsara/Numerical-PDE-Course/ch4/Chp4_NonLinScalr.pdf", "snippet": "4 from the top of the <b>hill</b> is given by . 22( ) v x v gx= + 0 2 sin\u03b8 . At a distance \u201cx\u201d let the number density of skiers be nx( ).Flux conservation then gives us . n v nx vx 00 = ( ) ( ). We <b>can</b> therefore obtain the number density of skiers at any distance \u201c\u201d as x ( ) 2 n x n v v gx= +00 0 2 sin\u03b8 .The inset plot shows the variation of nx( ) and vx( ) as a <b>function</b> of \u201cx\u201d.We see that the number density of skiers decreases as the skiers pick up", "dateLastCrawled": "2022-01-26T04:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Local Optimization Versus Global Optimization</b>", "url": "https://machinelearningmastery.com/local-optimization-versus-global-optimization/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>local-optimization-versus-global-optimization</b>", "snippet": "<b>Hill</b>-Climbing Algorithm; Now that we are familiar with local optimization, let\u2019s take a look at global optimization. Want to Get Started With Optimization Algorithms? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Start your FREE Mini-Course now! Global Optimization. A global optimum is the extrema (minimum or maximum) of the objective <b>function</b> for the entire input search space. Global optimization, where ...", "dateLastCrawled": "2022-02-03T01:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The topographical factor", "url": "https://www.fao.org/3/t1765e/t1765e0g.htm", "isFamilyFriendly": true, "displayUrl": "https://www.fao.org/3/t1765e/t1765e0g.htm", "snippet": "On a steep slope at the top of the <b>hill</b> (concave slope), rain infiltrates directly as far as the impermeable level, and then drains quickly down to the foot of the <b>hill</b> (<b>gentle</b> slope), where it re-emerges (Roose 1971). And this is where the gullies start that then climb back up to attack hills in regressive (headward) erosion. As Heusch (1971) has rightly pointed out, the steeper the topography, the steeper the hydraulic gradient. This means that water circulates quickly inside the soil ...", "dateLastCrawled": "2022-01-28T16:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "SLOPE ILLUSION (MAGNETIC <b>HILL</b>) IN RADAN", "url": "http://www.psy.ritsumei.ac.jp/~akitaoka/Serbia-Radan-slopeillusion-ZBORNIK-jul%202015-WEB751-760-Kitaoka.pdf", "isFamilyFriendly": true, "displayUrl": "www.psy.ritsumei.ac.jp/~akitaoka/Serbia-Radan-slopeillusion-ZBORNIK-jul 2015-WEB751-760...", "snippet": "A schematic cross section of this site. It consists of a <b>gentle</b> slope sandwiched by steep slopes of the same orientation. The point where the <b>gentle</b> slope and one of the steeper slopes meet is called \u201csag\u201d or \u201ccrest\u201d according to whether it is concave or <b>convex</b>, respectively. (b) A schematic perspectives of this site seen from a lower ...", "dateLastCrawled": "2021-12-03T10:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Scattering and Focusing of SH Waves by a Lower Semielliptic <b>Convex</b> ...", "url": "https://www.researchgate.net/publication/252633985_Scattering_and_Focusing_of_SH_Waves_by_a_Lower_Semielliptic_Convex_Topography", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/252633985_Scattering_and_Focusing_of_SH_Waves...", "snippet": "It illustrates that the semi-analytical IBIEM <b>can</b> accurately and steadily solve the wave scattering around a steep or a <b>gentle</b> <b>hill</b> both for low and high frequency waves. Numerical results ...", "dateLastCrawled": "2022-01-15T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "OPTIMUM SPATIAL RESOLUTION OF DIGITAL ELEVATION MODEL FOR TOPOGRAPHICAL ...", "url": "http://www.infra.kochi-tech.ac.jp/takagi/Papers/ISPRS02_taka.pdf", "isFamilyFriendly": true, "displayUrl": "www.infra.kochi-tech.ac.jp/takagi/Papers/ISPRS02_taka.pdf", "snippet": "An auto-correlation <b>function</b> of generated DEM was calculated. The results showed 8m spatial resolution was required to keep 0.9 of auto-correlation coefficient in mountainous area, 16m spatial resolution was required in case of <b>hill</b> area. In the present situation, meaning of auto-correlation coefficient in DEM is not understood clearly. However the auto-correlation coefficient would make one kind of index for complexity of landform. 1. BACKGROUND Digital Elevation Model (DEM) is ...", "dateLastCrawled": "2021-12-06T15:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Logistic Regression</b>. By Neeta Ganamukhi | by Neeta Ganamukhi | The ...", "url": "https://medium.com/swlh/logistic-regression-7791655bc480", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>logistic-regression</b>-7791655bc480", "snippet": "Fig1.Sigmoid <b>Function</b>.. The following equation is used to presents Sigmoid <b>function</b>: 1/1+e^-z.The classes <b>can</b> be divided into positive or negative.The output is the probability of positive class ...", "dateLastCrawled": "2022-02-01T21:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A reduced Yld2004 <b>function</b> for modeling of anisotropic plastic ...", "url": "https://www.sciencedirect.com/science/article/pii/S0020740319303145", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0020740319303145", "snippet": "The numerical simulation with the reduced Yld2004 <b>function</b> and the other three yield functions proves that the reduced Yld2004 yield <b>function</b> provides reasonable prediction of the strength around plane strain tension <b>compared</b> with the Yld2004-18p equation and the anisotropic Drucker equation with non-AFR. However, there is no obvious difference in computation cost for numerical simulation between the reduced Yld2004 <b>function</b> and Yld2004-18p equation, both of which are higher than simulation ...", "dateLastCrawled": "2022-01-15T22:17:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "11.2. <b>Convexity</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 documentation", "url": "https://d2l.ai/chapter_optimization/convexity.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_optimization/<b>convexity</b>.html", "snippet": "A twice-differentiable <b>function</b> is <b>convex</b> if and only if its Hessian (a matrix of second derivatives) is positive semidefinite. <b>Convex</b> constraints can be added via the Lagrangian. In practice we may simply add them with a penalty to the objective <b>function</b>. Projections map to points in the <b>convex</b> set closest to the original points.", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Gradient</b> Descent: A Quick, Simple Introduction | Built In", "url": "https://builtin.com/data-science/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://builtin.com/data-science/<b>gradient</b>-descent", "snippet": "<b>Gradient</b> descent is an optimization algorithm that&#39;s used when training a <b>machine</b> <b>learning</b> model. It&#39;s based on a <b>convex</b> <b>function</b> and tweaks its parameters iteratively to minimize a given <b>function</b> to its local minimum.", "dateLastCrawled": "2022-02-02T07:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Machine</b> <b>learning</b> terminology for model building and validation There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear regression with two independent variables is trying to fit the best plane with\u2026", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "The loss <b>function</b> or cost <b>function</b> in <b>machine</b> <b>learning</b> is a <b>function</b> that maps the values of variables onto a real number intuitively representing some cost associated with the variable values. Optimization methods are applied to minimize the loss <b>function</b> by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Understanding Gradient Descent Fundamentals</b> \u2014 <b>Machine</b> <b>Learning</b> \u2014 DATA ...", "url": "https://datascience.eu/machine-learning/gradient-descent/", "isFamilyFriendly": true, "displayUrl": "https://datascience.eu/<b>machine</b>-<b>learning</b>/gradient-descent", "snippet": "Programmers utilize gradient descent as an optimization algorithm when training <b>machine</b> <b>learning</b> models. Based on <b>convex</b> functions, the gradient descent iteratively tweaks some of its parameters to minimize a particular <b>function</b> to its minimum. Data scientists use gradient descent to find a <b>function</b>\u2019s parameter values that reduce cost functions as much as possible. They start by establishing the beginning parameter\u2019s values. The gradient descent utilizes calculus for iteratively ...", "dateLastCrawled": "2022-01-16T23:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Weak <b>learning</b> <b>convex</b> sets under normal distributions", "url": "http://proceedings.mlr.press/v134/de21a/de21a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v134/de21a/de21a.pdf", "snippet": "Keywords: weak <b>learning</b>, <b>convex</b> geometry, Gaussian space 1. Introduction Background and motivation. Several results in Boolean <b>function</b> analysis and computational <b>learning</b> theory suggest an <b>analogy</b> between <b>convex</b> sets in Gaussian space and monotone Boolean functions1 with respect to the uniform distribution over the hypercube. As an example ...", "dateLastCrawled": "2022-01-21T10:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Solving Word <b>Analogies: A Machine Learning Perspective</b> | Request PDF", "url": "https://www.researchgate.net/publication/335597029_Solving_Word_Analogies_A_Machine_Learning_Perspective", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335597029_Solving_Word_Analogies_A_<b>Machine</b>...", "snippet": "We introduce a supervised corpus-based <b>machine</b> <b>learning</b> algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT <b>analogy</b> questions, TOEFL synonym questions ...", "dateLastCrawled": "2021-10-16T21:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - How does Gradient Descent work? - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/102509/how-does-gradient-descent-work", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/102509/how-does-gradient-descent-work", "snippet": "If the <b>function</b> we minimize was <b>convex</b>, it would not matter what we choose for initial values, as gradient descent would get us to the minimum no matter what. But as the dimensions of the model increase, it is extremely unlikely that we have a <b>convex</b> loss <b>function</b>. And in this case, initialization of the weight depends on the activation functions used in the model. As discussed in", "dateLastCrawled": "2022-01-16T12:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Cost <b>function</b> of neural network is non-<b>convex</b> ...", "url": "https://stats.stackexchange.com/questions/106334/cost-function-of-neural-network-is-non-convex", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/106334", "snippet": "$\\begingroup$ I mean, this is how it should be interpreted, not just an <b>analogy</b>. $\\endgroup$ \u2013 avocado. May 23 &#39;16 at 12:27 . 5 $\\begingroup$ @loganecolss You are correct that this is not the only reason why cost functions are non-<b>convex</b>, but one of the most obvious reasons. Depdending on the network and the training set, there might be other reasons why there are multiple minima. But the bottom line is: The permuation alone creates non-convexity, regardless of other effects. $\\endgroup ...", "dateLastCrawled": "2022-02-03T01:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective <b>function</b> to perform maximum or minimum evaluation. In reality, optimization is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(convex function)  is like +(gentle hill)", "+(convex function) is similar to +(gentle hill)", "+(convex function) can be thought of as +(gentle hill)", "+(convex function) can be compared to +(gentle hill)", "machine learning +(convex function AND analogy)", "machine learning +(\"convex function is like\")", "machine learning +(\"convex function is similar\")", "machine learning +(\"just as convex function\")", "machine learning +(\"convex function can be thought of as\")", "machine learning +(\"convex function can be compared to\")"]}