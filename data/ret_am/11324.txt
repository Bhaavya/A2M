{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The \u2018<b>Percentage Correct\u2019 and other Performance Prediction Methods</b> ...", "url": "https://www.joe0.com/2017/02/19/the-percentage-correct-and-other-performance-prediction-methods/", "isFamilyFriendly": true, "displayUrl": "https://www.joe0.com/2017/02/19/the-<b>percentage-correct-and-other-performance</b>...", "snippet": "For example, \u201cwhen predicting time series <b>like</b> currency exchange <b>rate</b> variation over time or risk of being specific cyber criminals\u2019 target, <b>percentage</b> <b>correct</b> prediction is not applicable\u201d. (Herron, 1999). The reason it is not perfectly applicable to all types of data sets it due to its most praised quality of summarizing performance into a single <b>percentage</b> number, which isn\u2019t always the best method in situations where we encounter and want to report on several different factors ...", "dateLastCrawled": "2022-02-03T03:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is <b>true positive and true negative \u2013 confusion matrix</b> | Vikas D More", "url": "https://moredvikas.wordpress.com/2017/09/12/what-is-true-positive-and-true-negative-confusion-matrix/", "isFamilyFriendly": true, "displayUrl": "https://moredvikas.wordpress.com/2017/09/12/what-is-<b>true-positive-and-true-negative</b>...", "snippet": "In other words Recall or Sensitivity or <b>True</b> <b>Positive</b> <b>Rate</b> corresponds to the proportion of <b>positive</b> data points that are correctly considered as <b>positive</b>, with respect to all <b>positive</b> data points. Sensitivity (SN) is calculated as the number <b>of correct</b> <b>positive</b> <b>predictions</b> divided by the total number of positives. It is also called recall (REC) or <b>true</b> <b>positive</b> <b>rate</b> (TPR). The best sensitivity is 1.0, whereas the worst is 0.0.", "dateLastCrawled": "2022-02-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Model</b> Evaluation in <b>Machine Learning</b> | by ANUSHKA BAJPAI | Medium", "url": "https://medium.com/@anushkhabajpai/model-evaluation-in-machine-learning-7516dd465f94", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@anushkhabajpai/<b>model</b>-evaluation-in-<b>machine-learning</b>-7516dd465f94", "snippet": "Measuring the area under the ROC curve is also a very useful method for evaluating a <b>model</b>. By plotting the <b>true</b> <b>positive</b> <b>rate</b> (sensitivity) versus the false-<b>positive</b> <b>rate</b> (1 \u2014 specificity), we ...", "dateLastCrawled": "2022-02-02T18:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Evaluation Metrics For <b>Machine Learning</b> Models", "url": "https://apoorvtyagi.tech/metrics-to-evaluate-your-machine-learning-algorithm", "isFamilyFriendly": true, "displayUrl": "https://apoorvtyagi.tech/metrics-to-evaluate-your-<b>machine-learning</b>-algorithm", "snippet": "In simple words, it is a summary of the prediction result <b>made</b> by the classification <b>model</b>. Terms Associated with Confusion Matrix. <b>True</b> <b>Positive</b> (TP): <b>True</b> <b>positive</b> means their actual class value is 1 and the predicted value is also 1. For example, The case where the woman is actually pregnant and the <b>model</b> also classifies that she is pregnant.", "dateLastCrawled": "2022-01-28T19:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How to Calculate Precision, Recall, and F-Measure for Imbalanced ...", "url": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/precision-recall-and-f-measure-for-", "snippet": "You can see that precision is simply the ratio <b>of correct</b> <b>positive</b> <b>predictions</b> out of all <b>positive</b> <b>predictions</b> <b>made</b>, or the accuracy of minority class <b>predictions</b>. Consider the same dataset, where a <b>model</b> predicts 50 examples belonging to the minority class, 45 of which are <b>true</b> positives and five of which are false positives.", "dateLastCrawled": "2022-02-03T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "CBSE | DEPARTMENT OF SKILL EDUCATION", "url": "http://cbseacademic.nic.in/web_material/Curriculum21/SQP_MS_X/417_AI_MS.pdf", "isFamilyFriendly": true, "displayUrl": "cbseacademic.nic.in/web_material/Curriculum21/SQP_MS_X/417_AI_MS.pdf", "snippet": "d) <b>True</b> <b>Positive</b>, False <b>Positive</b> Ans: d) <b>True</b> <b>Positive</b>, False <b>Positive</b> 1 v Recall-Evaluation method is a) defined as the fraction of <b>positive</b> cases that are correctly identified. b) defined as the <b>percentage</b> of <b>true</b> <b>positive</b> cases versus all the cases where the prediction is <b>true</b>. c) defined as the <b>percentage</b> <b>of correct</b> <b>predictions</b> out of all ...", "dateLastCrawled": "2022-02-02T22:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Confusion Matrix and Performance Measures</b> in ML - CodeSpeedy", "url": "https://www.codespeedy.com/confusion-matrix-and-performance-measures-in-ml/", "isFamilyFriendly": true, "displayUrl": "https://www.codespeedy.com/<b>confusion-matrix-and-performance-measures</b>-in-ml", "snippet": "Accuracy \u2013 Accuracy is the <b>percentage</b> <b>of correct</b> <b>predictions</b> that our classifier has <b>made</b> on the testing dataset. In confusion matrix, <b>Correct</b> <b>predictions</b> are <b>True</b> <b>positive</b> and <b>True</b> Negatives (T.P + T.N) while the total will be the sum of all <b>predictions</b> including False-<b>positive</b> and False-Negative(T.P + T.N + F.P + F.N). therefore accuracy ...", "dateLastCrawled": "2022-01-25T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Evaluation of Classification Model Accuracy</b>: Essentials - Articles - STHDA", "url": "http://www.sthda.com/english/articles/36-classification-methods-essentials/143-evaluation-of-classification-model-accuracy-essentials/", "isFamilyFriendly": true, "displayUrl": "www.sthda.com/.../143-<b>evaluation-of-classification-model-accuracy</b>-essentials", "snippet": "The diagonal elements of the confusion matrix indicate <b>correct</b> <b>predictions</b>, while the off-diagonals represent incorrect <b>predictions</b>. So, the <b>correct</b> classification <b>rate</b> is the sum of the number on the diagonal divided by the sample size in the test data. In our example, that is (48 + 15)/78 = 81%. Each cell of the table has an important meaning: <b>True</b> positives (d): these are cases in which we predicted the individuals would be diabetes-<b>positive</b> and they were. <b>True</b> negatives (a): We predicted ...", "dateLastCrawled": "2022-02-03T05:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Confusion Matrix</b> in <b>Machine Learning</b> with EXAMPLE", "url": "https://www.guru99.com/confusion-matrix-machine-learning-example.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/<b>confusion-matrix</b>-<b>machine-learning</b>-example.html", "snippet": "F Score: F1 score is a weighted average score of the <b>true</b> <b>positive</b> (recall) and precision. Roc Curve: Roc curve shows the <b>true</b> <b>positive</b> rates against the false <b>positive</b> <b>rate</b> at various cut points. It also demonstrates a trade-off between sensitivity (recall and specificity or the <b>true</b> negative <b>rate</b>).", "dateLastCrawled": "2022-02-02T20:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Accuracy, Precision, Recall &amp; F1-Score - Python</b> Examples - <b>Data Analytics</b>", "url": "https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/<b>accuracy-precision-recall-f1-score-python</b>-example", "snippet": "<b>Model</b> accuracy is a <b>machine learning</b> <b>model</b> performance metric that is defined as the ratio of <b>true</b> positives and <b>true</b> negatives to all <b>positive</b> and negative observations. In other words, accuracy tells us how often we can expect our <b>machine learning</b> <b>model</b> will correctly predict an outcome out of the total number of times it <b>made</b> <b>predictions</b>. For example: Let\u2019s assume that you were testing your <b>machine learning</b> <b>model</b> with a dataset of 100 records and that your <b>machine learning</b> <b>model</b> ...", "dateLastCrawled": "2022-02-02T22:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>True Positive Rate</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/true-positive-rate", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>true-positive-rate</b>", "snippet": "Of the 266 images that contained NLs, 83 were classified as complete <b>true</b> positives and 27 were classified as partial <b>true</b> positives, which gives a total <b>true positive rate</b> of 42% and a false negative <b>rate</b> of 58%. All test images with no NLs were classified as <b>true</b> negatives. The remainder of our analysis was done via precision, recall, and specificity, and accuracy. Precision is the <b>percentage</b> of complete <b>true</b> <b>positive</b> matches out of all <b>true</b> <b>positive</b> matches. Recall is the <b>percentage</b> of ...", "dateLastCrawled": "2022-02-02T19:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The \u2018<b>Percentage Correct\u2019 and other Performance Prediction Methods</b> ...", "url": "https://www.joe0.com/2017/02/19/the-percentage-correct-and-other-performance-prediction-methods/", "isFamilyFriendly": true, "displayUrl": "https://www.joe0.com/2017/02/19/the-<b>percentage-correct-and-other-performance</b>...", "snippet": "The following post discusses the method of \u2018<b>percentage</b> <b>correct</b>\u2019 <b>predictions</b> and explains why it may not be the most precise method to measure performance. I also examine the topic of analytic measurement techniques in general and recommend the <b>correct</b> substitute prediction method for the situation when \u2018<b>percentage</b> <b>correct</b>\u2019 is not a suitable performance measurement approach. The \u2018<b>Percentage</b> <b>Correct</b>\u2019 Prediction Method. Before we dive into the topic of performance measurement and ...", "dateLastCrawled": "2022-02-03T03:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Confusion Matrix</b> for Your Multi-Class <b>Machine Learning</b> <b>Model</b> | by ...", "url": "https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>confusion-matrix</b>-for-your-multi-class-<b>machine-learning</b>...", "snippet": "It is also known as <b>True</b> <b>Positive</b> <b>Rate</b> (TPR), Sensitivity, Probability of Detection. To calculate ... a precision of 1 and a recall of 1. That means a F1-score of 1, i.e. a 100% accuracy which is often not the case for a <b>machine learning</b> <b>model</b>. So what we should try, is to get a higher precision with a higher recall value. Okay, now that we know about the performance measures for <b>confusion matrix</b>, Let\u2019s see how we can use that in a multi-class <b>machine learning</b> <b>model</b>. <b>Confusion Matrix</b> for ...", "dateLastCrawled": "2022-01-30T03:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding and using sensitivity, specificity and predictive values", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2636062/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2636062", "snippet": "It is the <b>percentage</b> of patients with a <b>positive</b> test who actually have the disease. In a 2 x 2 table ... (<b>true</b> <b>positive</b>) / a+b (<b>true</b> <b>positive</b> + false <b>positive</b>) = Probability (patient having disease when test is <b>positive</b>) Example: We will use sensitivity and specificity provided in Table 3 to calculate <b>positive</b> predictive value. PPV = a (<b>true</b> <b>positive</b>) / a+b (<b>true</b> <b>positive</b> + false <b>positive</b>) = 75 / 75 + 15 = 75 / 90 = 83.3%. Negative Predictive Value (NPV) It is the <b>percentage</b> of patients ...", "dateLastCrawled": "2022-01-30T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evaluating Deep <b>Learning Models: The Confusion Matrix</b>, Accuracy ...", "url": "https://www.kdnuggets.com/2021/02/evaluating-deep-learning-models-confusion-matrix-accuracy-precision-recall.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2021/02/evaluating-deep-learning-<b>models</b>-confusion-matrix...", "snippet": "If the <b>model</b> <b>made</b> a total of 530/550 <b>correct</b> <b>predictions</b> for the <b>Positive</b> class, compared to just 5/50 for the ... The <b>model</b> correctly classified two <b>Positive</b> samples, but incorrectly classified one Negative sample as <b>Positive</b>. Thus, the <b>True</b> <b>Positive</b> <b>rate</b> is 2 and the False <b>Positive</b> <b>rate</b> is 1, and the precision is 2/(2+1)=0.667. In other words, the trustiness <b>percentage</b> of the <b>model</b> when it says that a sample is <b>Positive</b> is 66.7%. The goal of the precision is to classify all the <b>Positive</b> ...", "dateLastCrawled": "2022-01-31T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Accuracy, Precision, Recall &amp; F1-Score - Python</b> Examples - <b>Data Analytics</b>", "url": "https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/<b>accuracy-precision-recall-f1-score-python</b>-example", "snippet": "<b>Model</b> accuracy is a <b>machine learning</b> <b>model</b> performance metric that is defined as the ratio of <b>true</b> positives and <b>true</b> negatives to all <b>positive</b> and negative observations. In other words, accuracy tells us how often we can expect our <b>machine learning</b> <b>model</b> will correctly predict an outcome out of the total number of times it <b>made</b> <b>predictions</b>. For example: Let\u2019s assume that you were testing your <b>machine learning</b> <b>model</b> with a dataset of 100 records and that your <b>machine learning</b> <b>model</b> ...", "dateLastCrawled": "2022-02-02T22:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to Calculate Precision, Recall, and F-Measure for Imbalanced ...", "url": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/precision-recall-and-f-measure-for-", "snippet": "Recall is a metric that quantifies the number <b>of correct</b> <b>positive</b> <b>predictions</b> <b>made</b> out of all <b>positive</b> <b>predictions</b> that could have been <b>made</b>. Unlike precision that only comments on the <b>correct</b> <b>positive</b> <b>predictions</b> out of all <b>positive</b> <b>predictions</b>, recall provides an indication of missed <b>positive</b> <b>predictions</b>. In this way, recall provides some notion of the coverage of the <b>positive</b> class. For imbalanced learning, recall is typically used to measure the coverage of the minority class. \u2014 Page ...", "dateLastCrawled": "2022-02-03T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Confusion Matrix</b>, Accuracy, Precision, Recall, F1 Score | by ...", "url": "https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1-score-ade299cf63cd", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>confusion-matrix</b>-accuracy-precision-recall-f1...", "snippet": "Accuracy represents the number of correctly classified data instances over the total number of data instances. In this example, Accuracy = (55 + 30)/(55 + 5 + 30 + 10 ) = 0.85 and in <b>percentage</b> ...", "dateLastCrawled": "2022-02-02T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Evaluating a <b>Classification Model</b> | <b>Machine Learning</b>, Deep Learning ...", "url": "https://www.ritchieng.com/machine-learning-evaluate-classification-model/", "isFamilyFriendly": true, "displayUrl": "https://www.ritchieng.com/<b>machine-learning</b>-evaluate-<b>classification-model</b>", "snippet": "Classification accuracy: <b>percentage</b> <b>of correct</b> <b>predictions</b>. In [7]: # calculate accuracy from sklearn import metrics print (metrics. accuracy_score (y_test, y_pred_class)) 0.692708333333 Classification accuracy is 69% . Null accuracy: accuracy that could be achieved by always predicting the most frequent class. We must always compare with this; In [8]: # examine the class distribution of the testing set (using a Pandas Series method) y_test. value_counts Out[8]: 0 130 1 62 Name: label, dtype ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - Scikit-learn: How to obtain <b>True Positive</b>, <b>True</b> Negative ...", "url": "https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31324218", "snippet": "They are not <b>correct</b>, because in the first answer, False <b>Positive</b> should be where actual is 0, but the predicted is 1, not the opposite. It is also same for False Negative. And, if we use the second answer, the results are computed as follows: FP: 3 FN: 1 TP: 4 TN: 3. <b>True Positive</b> and <b>True</b> Negative numbers are not <b>correct</b>, they should be opposite.", "dateLastCrawled": "2022-01-26T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>GitHub</b> - <b>iamtodor/data-science-interview-questions-and-answers</b>: Data ...", "url": "https://github.com/iamtodor/data-science-interview-questions-and-answers", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>iamtodor/data-science-interview-questions-and-answers</b>", "snippet": "What percent of <b>positive</b> <b>predictions</b> were <b>correct</b>? You answer: ... It is also called Sensitivity or the <b>True</b> <b>Positive</b> <b>Rate</b>. Recall <b>can</b> <b>be thought</b> of as a measure of a classifiers completeness. A low recall indicates many False Negatives. F1 Score (or F-score): A weighted average of precision and recall. I would also advise you to take a look at the following: Kappa (or Cohen\u2019s kappa): Classification accuracy normalized by the imbalance of the classes in the data. ROC Curves: Like precision ...", "dateLastCrawled": "2022-02-03T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Top 70+ <b>Data Science Interview Questions and Answers</b> for 2022", "url": "https://intellipaat.com/blog/interview-question/data-science-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://intellipaat.com/blog/interview-question/data-science-interview-", "snippet": "<b>True</b> <b>positive</b> <b>rate</b>: In <b>Machine Learning</b>, <b>true</b>-<b>positive</b> rates, which are also referred to as sensitivity or recall, are used to measure the <b>percentage</b> of actual positives which are correctly identified. Formula: <b>True</b> <b>Positive</b> <b>Rate</b> = <b>True</b> Positives/Positives False <b>positive</b> <b>rate</b>: False <b>positive</b> <b>rate</b> is basically the probability of falsely rejecting the null hypothesis for a particular test. The false-<b>positive</b> <b>rate</b> is calculated as the ratio between the number of negative events wrongly ...", "dateLastCrawled": "2022-02-03T02:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Classification Accuracy is Not Enough: More Performance Measures You ...", "url": "https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/classification-accuracy-is-not-enough-", "snippet": "Put another way it is the number of <b>positive</b> <b>predictions</b> divided by the number of <b>positive</b> class values in the test data. It is also called Sensitivity or the <b>True</b> <b>Positive</b> <b>Rate</b>. Recall <b>can</b> <b>be thought</b> of as a measure of a classifiers completeness. A low recall indicates many False Negatives. The recall of the All No Recurrence <b>model</b> is 0/(0+85 ...", "dateLastCrawled": "2022-02-03T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is <b>Sensitivity, Specificity, False positive, False negative</b>?", "url": "https://microbenotes.com/sensitivity-specificity-false-positive-false-negative/", "isFamilyFriendly": true, "displayUrl": "https://microbenotes.com/<b>sensitivity-specificity-false-positive-false-negative</b>", "snippet": "It is also known as the <b>True</b> Negative <b>Rate</b> (TNR), i.e the <b>percentage</b> of healthy people who are correctly identified as not having the condition. A test that <b>can</b> identify all sample tests from healthy individuals to be negative is very specific. Therefore, a test with 100% specificity correctly identifies all patients without the disease, while a test with 80% specificity correctly reports 80% of patients without the disease as test negative (<b>true</b> negatives) but 20% patients without the ...", "dateLastCrawled": "2022-02-02T20:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Classification: <b>Precision</b> and Recall | <b>Machine Learning</b> Crash Course ...", "url": "https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/crash-course/classification/<b>precision</b>...", "snippet": "<b>Precision</b> = T P T P + F P = 8 8 + 2 = 0.8. Recall measures the <b>percentage</b> of actual spam emails that were correctly classified\u2014that is, the <b>percentage</b> of green dots that are to the right of the threshold line in Figure 1: Recall = T P T P + F N = 8 8 + 3 = 0.73. Figure 2 illustrates the effect of increasing the classification threshold. Figure 2.", "dateLastCrawled": "2022-01-30T23:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning Model Performance</b> and Error Analysis", "url": "https://www.linkedin.com/pulse/machine-learning-model-performance-error-analysis-payam-mokhtarian", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/<b>machine-learning-model-performance</b>-error-analysis-payam...", "snippet": "Precision (<b>positive</b> predictive value) is the ratio of <b>true</b> positives to the total amount of <b>positive</b> <b>predictions</b> <b>made</b> (i.e., <b>true</b> or false). Said another way, precision measures the proportion of ...", "dateLastCrawled": "2022-01-22T01:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Classification Accuracy</b> is Not Enough | by Johar M. Ashfaque | AI In ...", "url": "https://medium.com/ai-in-plain-english/classification-accuracy-is-not-enough-d8e37f225b95", "isFamilyFriendly": true, "displayUrl": "https://medium.com/ai-in-plain-english/<b>classification-accuracy</b>-is-not-enough-d8e37f225b95", "snippet": "<b>Classification accuracy</b> is our starting point. It is the number <b>of correct</b> <b>predictions</b> <b>made</b> divided by the total number of <b>predictions</b> <b>made</b>, multiplied by 100 to turn it into a <b>percentage</b>. No ...", "dateLastCrawled": "2021-01-01T14:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Entry 23: Scoring Classification Models - Theory</b> - Data Science Diaries", "url": "https://julielinx.github.io/blog/23_class_score_theory/", "isFamilyFriendly": true, "displayUrl": "https://julielinx.github.io/blog/23_class_score_theory", "snippet": "LinkedIn. <b>Entry 23: Scoring Classification Models - Theory</b>. 14 minute read. Classification models present a different challenge than regression models. Because a numeric value isn\u2019t returned, another way of measuring goodness of fit has to be used. The Problem Permalink. Regression models return a numeric prediction.", "dateLastCrawled": "2022-01-29T03:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python - Scikit-learn: How to obtain <b>True Positive</b>, <b>True</b> Negative ...", "url": "https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31324218", "snippet": "They are not <b>correct</b>, because in the first answer, False <b>Positive</b> should be where actual is 0, but the predicted is 1, not the opposite. It is also same for False Negative. And, if we use the second answer, the results are computed as follows: FP: 3 FN: 1 TP: 4 TN: 3. <b>True Positive</b> and <b>True</b> Negative numbers are not <b>correct</b>, they should be opposite.", "dateLastCrawled": "2022-01-26T12:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "logistic regression - Roc curve and cut off point. <b>Python</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/28719067/roc-curve-and-cut-off-point-python", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/28719067", "snippet": "I ran a logistic regression <b>model</b> and <b>made</b> <b>predictions</b> of the logit values. I used this to get the points on the ROC curve: from sklearn import metrics fpr, tpr, thresholds = metrics.roc_curve(Y_test,p) I know metrics.roc_auc_score gives the area under the ROC curve. <b>Can</b> anyone tell me what command will find the optimal cut-off point (threshold value)? <b>python</b> logistic-regression roc. Share. Improve this question. Follow edited Apr 4 &#39;19 at 10:08. gmds. 17.4k 4 4 gold badges 25 25 silver ...", "dateLastCrawled": "2022-01-28T10:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>True Positive Rate</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/true-positive-rate", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>true-positive-rate</b>", "snippet": "Of the 266 images that contained NLs, 83 were classified as complete <b>true</b> positives and 27 were classified as partial <b>true</b> positives, which gives a total <b>true positive rate</b> of 42% and a false negative <b>rate</b> of 58%. All test images with no NLs were classified as <b>true</b> negatives. The remainder of our analysis was done via precision, recall, and specificity, and accuracy. Precision is the <b>percentage</b> of complete <b>true</b> <b>positive</b> matches out of all <b>true</b> <b>positive</b> matches. Recall is the <b>percentage</b> of ...", "dateLastCrawled": "2022-02-02T19:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Model</b> Evaluation in <b>Machine Learning</b> | by ANUSHKA BAJPAI | Medium", "url": "https://medium.com/@anushkhabajpai/model-evaluation-in-machine-learning-7516dd465f94", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@anushkhabajpai/<b>model</b>-evaluation-in-<b>machine-learning</b>-7516dd465f94", "snippet": "Measuring the area under the ROC curve is also a very useful method for evaluating a <b>model</b>. By plotting the <b>true</b> <b>positive</b> <b>rate</b> (sensitivity) versus the false-<b>positive</b> <b>rate</b> (1 \u2014 specificity), we ...", "dateLastCrawled": "2022-02-02T18:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "The \u2018<b>Percentage Correct\u2019 and other Performance Prediction Methods</b> ...", "url": "https://www.joe0.com/2017/02/19/the-percentage-correct-and-other-performance-prediction-methods/", "isFamilyFriendly": true, "displayUrl": "https://www.joe0.com/2017/02/19/the-<b>percentage-correct-and-other-performance</b>...", "snippet": "The following post discusses the method of \u2018<b>percentage</b> <b>correct</b>\u2019 <b>predictions</b> and explains why it may not be the most precise method to measure performance. I also examine the topic of analytic measurement techniques in general and recommend the <b>correct</b> substitute prediction method for the situation when \u2018<b>percentage</b> <b>correct</b>\u2019 is not a suitable performance measurement approach. The \u2018<b>Percentage</b> <b>Correct</b>\u2019 Prediction Method. Before we dive into the topic of performance measurement and ...", "dateLastCrawled": "2022-02-03T03:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding and using sensitivity, specificity and predictive values", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2636062/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2636062", "snippet": "It is the <b>percentage</b> of patients with a <b>positive</b> test who actually have the disease. In a 2 x 2 table ... (<b>true</b> <b>positive</b>) / a+b (<b>true</b> <b>positive</b> + false <b>positive</b>) = Probability (patient having disease when test is <b>positive</b>) Example: We will use sensitivity and specificity provided in Table 3 to calculate <b>positive</b> predictive value. PPV = a (<b>true</b> <b>positive</b>) / a+b (<b>true</b> <b>positive</b> + false <b>positive</b>) = 75 / 75 + 15 = 75 / 90 = 83.3%. Negative Predictive Value (NPV) It is the <b>percentage</b> of patients ...", "dateLastCrawled": "2022-01-30T18:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Evaluating Deep <b>Learning Models: The Confusion Matrix</b>, Accuracy ...", "url": "https://www.kdnuggets.com/2021/02/evaluating-deep-learning-models-confusion-matrix-accuracy-precision-recall.html", "isFamilyFriendly": true, "displayUrl": "https://www.kdnuggets.com/2021/02/evaluating-deep-learning-<b>models</b>-confusion-matrix...", "snippet": "If the <b>model</b> <b>made</b> a total of 530/550 <b>correct</b> <b>predictions</b> for the <b>Positive</b> class, <b>compared</b> to just 5/50 for the ... The <b>model</b> correctly classified two <b>Positive</b> samples, but incorrectly classified one Negative sample as <b>Positive</b>. Thus, the <b>True</b> <b>Positive</b> <b>rate</b> is 2 and the False <b>Positive</b> <b>rate</b> is 1, and the precision is 2/(2+1)=0.667. In other words, the trustiness <b>percentage</b> of the <b>model</b> when it says that a sample is <b>Positive</b> is 66.7%. The goal of the precision is to classify all the <b>Positive</b> ...", "dateLastCrawled": "2022-01-31T15:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Classification: <b>True</b> vs. False and <b>Positive</b> vs. Negative | Machine ...", "url": "https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/crash-course/classification/<b>true</b>-false...", "snippet": "A <b>true</b> <b>positive</b> is an outcome where the <b>model</b> correctly predicts the <b>positive</b> class. Similarly, a <b>true</b> negative is an outcome where the <b>model</b> correctly predicts the negative class.. A false <b>positive</b> is an outcome where the <b>model</b> incorrectly predicts the <b>positive</b> class. And a false negative is an outcome where the <b>model</b> incorrectly predicts the negative class.. In the following sections, we&#39;ll look at how to evaluate classification models using metrics derived from these four outcomes.", "dateLastCrawled": "2022-02-02T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Top <b>10 Evaluation Metrics for Classification</b> Models", "url": "https://www.explorium.ai/blog/top-10-evaluation-metrics-for-classification-models/", "isFamilyFriendly": true, "displayUrl": "https://www.explorium.ai/blog/top-<b>10-evaluation-metrics-for-classification</b>-<b>models</b>", "snippet": "3. Detection <b>rate</b>. This metric basically shows the number <b>of correct</b> <b>positive</b> class <b>predictions</b> <b>made</b> as a proportion of all of the <b>predictions</b> <b>made</b>. Detection <b>Rate</b> = TP / TP + FP + FN + TN. 4. Logarithmic loss. Also known as log loss, logarithmic loss basically functions by penalizing all false/incorrect classifications. The classifier must ...", "dateLastCrawled": "2022-01-25T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Calculate Precision, Recall, and F-Measure for Imbalanced ...", "url": "https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/precision-recall-and-f-measure-for-", "snippet": "Recall is a metric that quantifies the number <b>of correct</b> <b>positive</b> <b>predictions</b> <b>made</b> out of all <b>positive</b> <b>predictions</b> that could have been <b>made</b>. Unlike precision that only comments on the <b>correct</b> <b>positive</b> <b>predictions</b> out of all <b>positive</b> <b>predictions</b>, recall provides an indication of missed <b>positive</b> <b>predictions</b>. In this way, recall provides some notion of the coverage of the <b>positive</b> class. For imbalanced learning, recall is typically used to measure the coverage of the minority class. \u2014 Page ...", "dateLastCrawled": "2022-02-03T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Accuracy, Precision, and Recall</b> in Deep Learning | Paperspace Blog", "url": "https://blog.paperspace.com/deep-learning-metrics-precision-recall-accuracy/", "isFamilyFriendly": true, "displayUrl": "https://blog.paperspace.com/deep-learning-metrics-precision-recall-accuracy", "snippet": "If the <b>model</b> <b>made</b> a total of 530/550 <b>correct</b> <b>predictions</b> for the <b>Positive</b> class, <b>compared</b> to just 5/50 for the Negative class, then the total accuracy is (530 + 5) / 600 = 0.8917. This means the <b>model</b> is 89.17% accurate. With that in mind, you might think that for any sample (regardless of its class) the <b>model</b> is likely to make a <b>correct</b> ...", "dateLastCrawled": "2022-02-02T19:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "roc - Given <b>true</b> <b>positive</b>, false negative rates, <b>can</b> you calculate ...", "url": "https://stats.stackexchange.com/questions/61829/given-true-positive-false-negative-rates-can-you-calculate-false-positive-tru", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/61829", "snippet": "If your <b>true</b> <b>positive</b> <b>rate</b> is 0.25 it means that every time you call a <b>positive</b>, you have a probability of 0.75 of being wrong. This is your false <b>positive</b> <b>rate</b>. Similarly, every time you call a negative, you have a probability of 0.25 of being right, which is your <b>true</b> negative <b>rate</b>.", "dateLastCrawled": "2022-02-02T16:42:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning Accuracy</b>: <b>True</b> vs. False <b>Positive</b>/Negative", "url": "https://research.aimultiple.com/machine-learning-accuracy/", "isFamilyFriendly": true, "displayUrl": "https://research.aimultiple.com/<b>machine-learning-accuracy</b>", "snippet": "There are various theoretical approaches to measuring accuracy* of competing <b>machine</b> <b>learning</b> models however, in most commercial applications, you simply need to assign a business value to 4 types of results: <b>true</b> positives, <b>true</b> negatives, false positives and false negatives.By multiplying number of results in each bucket with the associated business values, you will ensure that you use the best model available.", "dateLastCrawled": "2022-02-03T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine Learning</b> <b>Evaluation Metrics</b> - GitHub Pages", "url": "https://kevalnagda.github.io/evaluation-metrics", "isFamilyFriendly": true, "displayUrl": "https://kevalnagda.github.io/<b>evaluation-metrics</b>", "snippet": "It essentially shows the <b>True</b> <b>Positive</b> <b>Rate</b> (TPR) against the False <b>Positive</b> <b>Rate</b> (FPR) for various threshold values. AUC The Area Under the Curve (AUC), is an aggregated measure of performance of a binary classifier on all possible threshold values (and therefore it is threshold invariant). AUC calculates the area under the ROC curve, and therefore it is between 0 and 1. One way of interpreting AUC is the probability that the model ranks a random <b>positive</b> example more highly than a random ...", "dateLastCrawled": "2021-10-13T19:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning from positive</b> and unlabeled data: a survey - <b>Machine</b> <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-020-05877-5", "snippet": "The SCAR assumption was introduced in <b>analogy</b> with the Missing Completely A Random assumption (MCAR) that is common when ... the <b>true</b> <b>positive</b> <b>rate</b>, the false <b>positive</b> <b>rate</b>, and precision. Hence, it is possible in this circumstance to report estimates of these metrics. PU <b>learning</b> methods. This section provides an overview of the methods that address PU <b>learning</b>. Most methods can be divided into the following three categories: Two-step techniques, biased <b>learning</b> and class prior ...", "dateLastCrawled": "2022-02-02T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) A Comparison of Various <b>Machine</b> <b>Learning</b> Algorithms in a ...", "url": "https://www.academia.edu/68902781/A_Comparison_of_Various_Machine_Learning_Algorithms_in_a_Distributed_Denial_of_Service_Intrusion", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/68902781/A_Comparison_of_Various_<b>Machine</b>_<b>Learning</b>_Algorithms...", "snippet": "2) <b>True</b> <b>Positive</b> <b>Rate</b> (TPR) 4) Decision Tree (DT) This metric calculates how often the model is able to predict a This algorithm uses a tree structure <b>analogy</b> to represent a <b>positive</b> result correctly. Similar to Accuracy, but difference is series of rules that lead to a class or value [16]. It starts with a it only takes <b>positive</b> observation. root node, which is the best predictor. Then, it progresses TPR:: \ud835\udc47\ud835\udc43 through branch nodes to other predictors. Ultimately it reaches \ud835\udc47\ud835\udc43 ...", "dateLastCrawled": "2022-02-05T20:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Receiver Operating Curve (ROC) and The Scale \u2014 Understanding ROC ...", "url": "https://medium.com/data-science-everyday/the-receiver-operating-curve-roc-and-the-scale-understanding-roc-through-an-analogy-8b8f9d954f84", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-everyday/the-receiver-operating-curve-roc-and-the...", "snippet": "The <b>rate</b> of <b>True</b> Positives is also called Sensitivity. Similarly, the <b>rate</b> of False Positives means counting the False Positives as part of the actual Negatives. In other words, represents the ...", "dateLastCrawled": "2021-02-13T00:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Evaluation metric for Supervised <b>Learning</b>: | by Anuganti Suresh | Medium", "url": "https://anugantisuresh.medium.com/evaluation-metric-for-supervised-learning-ba063f1bb1af", "isFamilyFriendly": true, "displayUrl": "https://anugantisuresh.medium.com/evaluation-metric-for-supervised-<b>learning</b>-ba063f1bb1af", "snippet": "In <b>machine</b> <b>learning</b>, ... By <b>analogy</b>, Higher the AUC, better the model is at distinguishing between patients with disease and no disease. a. AUC : Area Under Curve. One of the widely used metrics for binary classification is the Area Under Curve(AUC) AUC represents the probability that the classifier will rank a randomly chosen <b>positive</b> example higher than a randomly chosen negative example. The AUC is based on a plot of the false <b>positive</b> <b>rate</b> vs the <b>true</b> <b>positive</b> <b>rate</b> which are defined as ...", "dateLastCrawled": "2022-01-06T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is the best example for <b>false negative, false positive, true</b> ...", "url": "https://www.quora.com/What-is-the-best-example-for-false-negative-false-positive-true-negative-and-true-positive-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-the-best-example-for-<b>false-negative-false-positive-true</b>...", "snippet": "Answer (1 of 6): There was a funny picture I\u2019d come across a while ago [1]: Extending this example, a man whose test results say \u201cNot pregnant\u201d is <b>True</b> Negative, and a pregnant woman whose test results say \u201cPregnant\u201d is <b>True</b> <b>Positive</b>. A good way to understand it is this: * First, define what...", "dateLastCrawled": "2022-01-22T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Evaluation Metric Special ROC-AUC Candra\u2019s blog", "url": "https://saltfarmer.github.io/blog/machine%20learning/Evaluation-Metrics-Special-ROCAUC/", "isFamilyFriendly": true, "displayUrl": "https://saltfarmer.github.io/blog/<b>machine</b> <b>learning</b>/Evaluation-Metrics-Special-ROCAUC", "snippet": "The ROC curve is created by plotting the <b>true</b> <b>positive</b> <b>rate</b> (TPR) against the false <b>positive</b> <b>rate</b> (FPR) at various threshold settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s ...", "dateLastCrawled": "2022-02-03T01:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "ROC (Receiver Operating Characteristic curve): A curve of <b>true</b> <b>positive</b> <b>rate</b> vs. false <b>positive</b> <b>rate</b> at different classification thresholds. The x-axis is False <b>Positive</b> <b>rate</b>, and the y-axis is <b>True</b> <b>Positive</b> <b>rate</b>. [3] . Close to the up left point (TPR=1.0, FPR=0.0) indicates the model is better.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Chi-squared tests to <b>compare two machine learning models and determine</b> ...", "url": "https://towardsdatascience.com/chi-squared-tests-to-compare-two-machine-learning-models-and-determine-whether-they-are-random-2a405fc55181", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/chi-squared-tests-to-compare-two-<b>machine</b>-<b>learning</b>...", "snippet": "Take T as the random variable describing the number of <b>true</b> <b>positive</b> instances (Heads by the coin <b>analogy</b>). Doesn\u2019t T follow a binomial distribution with p=0.65 (probability of <b>positive</b>) and n=69 (total data instances)? Sure it does. I assume you know that binomial distribution can be approximated from a normal distribution, provided both np and n(1-p) exceed 10. Accordingly, we get a normal variable z~N(0,1)", "dateLastCrawled": "2022-01-29T04:24:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "On <b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> - Online courses and text books ...", "url": "https://www.linkedin.com/pulse/machine-learning-deep-online-courses-text-books-point-ajay-taneja-1f", "isFamilyFriendly": true, "displayUrl": "https://www.linkedin.com/pulse/<b>machine</b>-<b>learning</b>-deep-online-courses-text-books-point...", "snippet": "On <b>Machine</b> <b>Learning</b> and Deep <b>Learning</b> - Online courses and text books - A point of view \u2013 Evaluation Metrics (Part 2) Published on October 4, 2020 October 4, 2020 \u2022 8 Likes \u2022 0 Comments", "dateLastCrawled": "2021-08-15T14:28:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(true positive rate)  is like +(percentage of correct predictions made by a machine-learning model)", "+(true positive rate) is similar to +(percentage of correct predictions made by a machine-learning model)", "+(true positive rate) can be thought of as +(percentage of correct predictions made by a machine-learning model)", "+(true positive rate) can be compared to +(percentage of correct predictions made by a machine-learning model)", "machine learning +(true positive rate AND analogy)", "machine learning +(\"true positive rate is like\")", "machine learning +(\"true positive rate is similar\")", "machine learning +(\"just as true positive rate\")", "machine learning +(\"true positive rate can be thought of as\")", "machine learning +(\"true positive rate can be compared to\")"]}