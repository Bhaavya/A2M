{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Cross Entropy</b> Explained | What is <b>Cross Entropy</b> for ... - Great <b>Learning</b>", "url": "https://www.mygreatlearning.com/blog/cross-entropy-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/blog/<b>cross-entropy</b>-explained", "snippet": "<b>Cross entropy</b> is the average number of bits required to send the message from distribution A to Distribution B. <b>Cross entropy</b> as a concept is applied in the field of machine <b>learning</b> when algorithms are built to predict from the model build. Model building is based on a comparison of actual results with the predicted results. This will be explained further by working on Logistic regression where <b>cross-entropy</b> is referred to as Log Loss.", "dateLastCrawled": "2022-02-02T07:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. <b>Cross-entropy</b> is commonly used in machine <b>learning</b> as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas <b>cross-entropy</b> can be thought to calculate the total entropy between the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Basic Introduction To <b>Cross Entropy</b> For Machine <b>Learning</b> (2021)", "url": "https://www.jigsawacademy.com/blogs/ai-ml/cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.<b>jigsawacademy</b>.com/blogs/ai-ml/<b>cross-entropy</b>", "snippet": "<b>Cross entropy</b> is widely utilized as a <b>cross-entropy</b> loss Function while advancing characterization models, for example, algorithms or logistics regression utilized for classification undertakings. <b>Cross-entropy</b> loss quantifies the accomplishment of a classification model that gives yield as far as likelihood having values somewhere in the range of ZERO and ONE.", "dateLastCrawled": "2022-01-21T15:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Cross Entropy of Neural Language Models at Infinity\u2014A New</b> Bound ...", "url": "https://www.researchgate.net/publication/328739594_Cross_Entropy_of_Neural_Language_Models_at_Infinity-A_New_Bound_of_the_Entropy_Rate", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328739594_<b>Cross_Entropy</b>_of_Neural_<b>Language</b>...", "snippet": "<b>Cross entropy</b> of (a) RHN on OB dataset, (b) AWD-LSTM-MoS on CNA dataset, and (c) n-gram <b>language</b> models on OB dataset with different training data sizes. The fitting function f 1 was applied to ...", "dateLastCrawled": "2021-10-19T19:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Cross-Entropy</b> Cost Functions used in Classification - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/cross-entropy-cost-functions-used-in-classification/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>cross-entropy</b>-cost-functions-used-in-classification", "snippet": "This is the categorical <b>cross-entropy</b>. Categorical <b>cross-entropy</b> is used when the actual-value labels are one-hot encoded. This means that only one \u2018bit\u2019 of data is true at a time, <b>like</b> [1,0,0], [0,1,0] or [0,0,1]. The categorical <b>cross-entropy</b> can be mathematically represented as: Categorical <b>Cross-Entropy</b> = (Sum of <b>Cross-Entropy</b> for N data)/N", "dateLastCrawled": "2022-01-28T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep <b>Learning</b> Fundamentals - <b>Cross Entropy</b>", "url": "https://www.cross-entropy.net/ML530/Deep_Learning_1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.<b>cross-entropy</b>.net/ML530/Deep_<b>Learning</b>_1.pdf", "snippet": "<b>A New</b> Wave of Investment e. The Democratization of Deep <b>Learning</b> f. Will it Last? \u2022This chapter covers \u2022High-level definitions of fundamental concepts \u2022Timeline of the development of machine <b>learning</b> \u2022Key factors behind deep <b>learning</b>\u2019s rising popularity and future potential. Artificial Intelligence \u2022Concise definition: the effort to automate intellectual tasks normally performed by humans \u2022Initial take: expert rules \u2022Fine for chess \u2022Difficult to develop rules for image ...", "dateLastCrawled": "2022-01-27T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Should <b>cross-entropy</b> be used <b>in classification tasks</b>? | AI-SCHOLAR | AI ...", "url": "https://ai-scholar.tech/en/articles/deep-learning/closs-square", "isFamilyFriendly": true, "displayUrl": "https://ai-scholar.tech/en/articles/deep-<b>learning</b>/closs-square", "snippet": "When training with <b>cross-entropy</b> loss, <b>learning</b> was stopped when validation performance did not improve for five consecutive epochs. When training with squared loss, the following two protocols were used. Similar to the <b>cross-entropy</b> loss, <b>learning</b> is stopped when validation performance does not improve for 5 consecutive epochs.", "dateLastCrawled": "2021-12-01T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[D] any <b>principled reason for cross entropy</b> instead of L2 in <b>language</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/dqoh2u/d_any_principled_reason_for_cross_entropy_instead/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/dqoh2u/d_any_principled_reason_for...", "snippet": "Is there any principled reason for doing softmax and <b>cross entropy</b> for the loss in for example transformers, rather than doing L2 over the target embeddings and the output from the model? When the output from your model by necessity is a dot product such as in shallow models I understand why you need to do <b>cross entropy</b> loss. But for models ...", "dateLastCrawled": "2021-09-16T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "machine <b>learning</b> - Why can we use entropy to measure the quality of a ...", "url": "https://stats.stackexchange.com/questions/93117/why-can-we-use-entropy-to-measure-the-quality-of-a-language-model", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/93117/why-can-we-use-entropy-to-measure-the...", "snippet": "To evaluate a <b>language</b> model, we should measure how much surprise it gives us for real sequences in that <b>language</b>. For each real word encountered, the <b>language</b> model will give a probability p. And we use -log(p) to quantify the surprise. And we average the total surprise over a long enough sequence. So, in case of a 1000-letter sequence with 500 A and 500 B, the surprise given by the 1/3-2/3 model will be:", "dateLastCrawled": "2022-01-12T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Perplexity</b> in <b>Language</b> Models. Evaluating <b>language</b> models using the ...", "url": "https://towardsdatascience.com/perplexity-in-language-models-87a196019a94?source=post_internal_links---------4----------------------------", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>perplexity</b>-in-<b>language</b>-models-87a196019a94?source=post...", "snippet": "This <b>is like</b> saying that under these <b>new</b> conditions, at each roll our model is as uncertain of the outcome as if it had to pick between 4 different options, as opposed to 6 when all sides had equal probability. To clarify this further, let\u2019s push it to the extreme. Let\u2019s say we now have an unfair die that gives a 6 with 99% probability, and the other numbers with a probability of 1/500 each. We again train the model on this die and then create a test set with 100 rolls where we get a 6 ...", "dateLastCrawled": "2022-01-26T00:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Cross-Entropy</b> Cost Functions used in Classification - GeeksforGeeks", "url": "https://www.geeksforgeeks.org/cross-entropy-cost-functions-used-in-classification/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>cross-entropy</b>-cost-functions-used-in-classification", "snippet": "In machine <b>learning</b> lingo, a ... <b>Cross-entropy</b>(d) = \u2013 y*log(p) when y = 1; <b>Cross-entropy</b>(d) = \u2013 (1-y)*log(1-p) when y = 0. Problem implementation for this method is the same as those of multi-class cost functions. The difference is that only binary classes can be accepted. Sparse Categorical <b>Cross-Entropy</b>. In sparse categorical <b>cross-entropy</b>, truth labels are labelled with integral values. For example, if a 3-class problem is taken into consideration, the labels would be encoded as [1 ...", "dateLastCrawled": "2022-01-28T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. <b>Cross-entropy</b> is commonly used in machine <b>learning</b> as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas <b>cross-entropy</b> can be thought to calculate the total entropy between the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Basic Introduction To <b>Cross Entropy</b> For Machine <b>Learning</b> (2021)", "url": "https://www.jigsawacademy.com/blogs/ai-ml/cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.<b>jigsawacademy</b>.com/blogs/ai-ml/<b>cross-entropy</b>", "snippet": "In information theory, the <b>cross entropy</b> between 2 probability distributions x and y over the <b>similar</b> basic arrangement of occasions measures the normal number of bits expected to recognize an occasion drawn from the set if a coding plan utilized for the set is improved for an expected probability distribution y, as opposed to the genuine distribution x.", "dateLastCrawled": "2022-01-21T15:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep <b>Learning</b> Fundamentals - <b>Cross Entropy</b>", "url": "https://www.cross-entropy.net/ML530/Deep_Learning_1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.<b>cross-entropy</b>.net/ML530/Deep_<b>Learning</b>_1.pdf", "snippet": "<b>A New</b> Wave of Investment e. The Democratization of Deep <b>Learning</b> f. Will it Last? \u2022This chapter covers \u2022High-level definitions of fundamental concepts \u2022Timeline of the development of machine <b>learning</b> \u2022Key factors behind deep <b>learning</b>\u2019s rising popularity and future potential. Artificial Intelligence \u2022Concise definition: the effort to automate intellectual tasks normally performed by humans \u2022Initial take: expert rules \u2022Fine for chess \u2022Difficult to develop rules for image ...", "dateLastCrawled": "2022-01-27T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Cross-Entropy</b> vs. Squared Error Training: a Theoretical and ...", "url": "https://www.researchgate.net/publication/266030536_Cross-Entropy_vs_Squared_Error_Training_a_Theoretical_and_Experimental_Comparison", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/266030536_<b>Cross-Entropy</b>_vs_Squared_Error...", "snippet": "<b>cross-entropy</b> functions for training neural network classi\ufb01ers,\u201d Neural Computing and Applications, vol. 14, no. 4, pp. 310\u2013318, Dec. 2005. [5] H. Ney, \u201cOn the relationship between ...", "dateLastCrawled": "2021-12-29T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Should <b>cross-entropy</b> be used <b>in classification tasks</b>? | AI-SCHOLAR | AI ...", "url": "https://ai-scholar.tech/en/articles/deep-learning/closs-square", "isFamilyFriendly": true, "displayUrl": "https://ai-scholar.tech/en/articles/deep-<b>learning</b>/closs-square", "snippet": "\ufe0f Validated on a variety of tasks, including natural <b>language</b> processing, speech recognition, ... <b>Similar</b> to the <b>cross-entropy</b> loss, <b>learning</b> is stopped when validation performance does not improve for 5 consecutive epochs. Training with the same number of epochs as the number of epochs when training <b>cross-entropy</b> loss ; The latter is designed so that the computational resources are the same for <b>cross-entropy</b> loss and squared loss, which is a favorable setting for <b>cross-entropy</b> loss. In ...", "dateLastCrawled": "2021-12-01T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[D] any <b>principled reason for cross entropy</b> instead of L2 in <b>language</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/dqoh2u/d_any_principled_reason_for_cross_entropy_instead/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/dqoh2u/d_any_principled_reason_for...", "snippet": "The softmax function is then just a proxy for the neural net to learn a final weight matrix in such a way that by making one column vector more <b>similar</b> to the previous layer&#39;s activations, it also has to make all the other columns dissimilar to it. Essentially what you&#39;re asking is if you can manually do all of this process that the softmax does for you implicitely.", "dateLastCrawled": "2021-09-16T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bigram Language Model and Cross-entropy</b> in Python : LanguageTechnology", "url": "https://www.reddit.com/r/LanguageTechnology/comments/5wxf5h/bigram_language_model_and_crossentropy_in_python/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../5wxf5h/<b>bigram_language_model_and_crossentropy</b>_in_python", "snippet": "<b>Bigram Language Model and Cross-entropy</b> in Python. Hello everybody, I want to replicate the <b>language</b> analysis of No Country for Old Members: User lifecycle and linguistic change in online communities and use it on <b>reddit</b> data. They build a <b>language</b> model for each month and compare it to posts made by users in that month. I am working with Python.", "dateLastCrawled": "2021-01-30T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Natural <b>Language</b> Processing: Are there any tools that can measure the ...", "url": "https://www.quora.com/Natural-Language-Processing-Are-there-any-tools-that-can-measure-the-cross-entropy-of-2-corpora-of-short-text-snippets", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Natural-<b>Language</b>-Processing-Are-there-any-tools-that-can-measure...", "snippet": "Answer (1 of 2): Reliably measuring semantic similarity between two pieces of text is an open problem in natural <b>language</b> processing. As an illustration consider that as recently as this year, there was a shared task as part of the SemEval (Semantic Evaluation) workshop designed to elicit ideas o...", "dateLastCrawled": "2022-01-17T18:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Contrastive Representation <b>Learning</b> - Lil&#39;Log", "url": "https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html", "isFamilyFriendly": true, "displayUrl": "https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-<b>learning</b>.html", "snippet": "The main idea of <b>contrastive learning</b> is to learn representations such that <b>similar</b> samples stay close to each other, while dissimilar ones are far apart. <b>Contrastive learning</b> can be applied to both supervised and unsupervised data and has been shown to achieve good performance on a variety of vision and <b>language</b> tasks. Lil&#39;Log \uf984 Contact FAQ \u231b Archive. Contrastive Representation <b>Learning</b>. May 31, 2021 by Lilian Weng representation-<b>learning</b> long-read <b>language</b>-model unsupervised-<b>learning</b> ...", "dateLastCrawled": "2022-02-02T19:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "<b>Cross-entropy</b> is commonly used in machine <b>learning</b> as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas <b>cross-entropy</b> <b>can</b> <b>be thought</b> to calculate the", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Basic Introduction To <b>Cross Entropy</b> For Machine <b>Learning</b> (2021)", "url": "https://www.jigsawacademy.com/blogs/ai-ml/cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.<b>jigsawacademy</b>.com/blogs/ai-ml/<b>cross-entropy</b>", "snippet": "<b>Cross-entropy</b> <b>can</b> be utilized as a loss function while enhancing grouping models like artificial neural networks and logistic regression. There are no right or wrong ways of <b>learning</b> AI and ML technologies \u2013 the more, the better! These valuable resources <b>can</b> be the starting point for your journey on how to learn Artificial Intelligence and Machine <b>Learning</b>. Do pursuing AI and ML interest you? If you want to step into the world of emerging tech, you <b>can</b> accelerate your career with this ...", "dateLastCrawled": "2022-01-21T15:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introducing <b>Cross-Entropy for Machine Learning</b> - BLOCKGENI", "url": "https://blockgeni.com/introducing-cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://blockgeni.com/introducing-<b>cross-entropy-for-machine-learning</b>", "snippet": "<b>Cross-entropy</b> is commonly used in machine <b>learning</b> as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas <b>cross-entropy</b> <b>can</b> <b>be thought</b> to calculate the total entropy between the distributions. <b>Cross-entropy</b> is also ...", "dateLastCrawled": "2022-01-27T13:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Cross-Entropy Loss</b> and Its Applications in Deep <b>Learning</b> - neptune.ai", "url": "https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/<b>cross-entropy-loss</b>-and-its-applications-in-deep-<b>learning</b>", "snippet": "<b>Cross-entropy loss</b> is the sum of the negative logarithm of predicted probabilities of each student. Model A\u2019s <b>cross-entropy loss</b> is 2.073; model B\u2019s is 0.505. <b>Cross-Entropy</b> gives a good measure of how effective each model is. Binary <b>cross-entropy</b> (BCE) formula. In our four student prediction \u2013 model B:", "dateLastCrawled": "2022-02-02T02:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Gentle Introduction to Cross-Entropy for Machine Learning</b> \u2013 AiProBlog.Com", "url": "https://www.aiproblog.com/index.php/2019/10/20/a-gentle-introduction-to-cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.aiproblog.com/.../a-<b>gentle-introduction-to-cross-entropy-for-machine-learning</b>", "snippet": "<b>Cross-entropy</b> is commonly used in machine <b>learning</b> as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between ...", "dateLastCrawled": "2021-12-28T17:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Deep <b>Learning</b> Fundamentals - <b>Cross Entropy</b>", "url": "https://www.cross-entropy.net/ML530/Deep_Learning_1.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.<b>cross-entropy</b>.net/ML530/Deep_<b>Learning</b>_1.pdf", "snippet": "<b>A New</b> Wave of Investment e. The Democratization of Deep <b>Learning</b> f. Will it Last? \u2022This chapter covers \u2022High-level definitions of fundamental concepts \u2022Timeline of the development of machine <b>learning</b> \u2022Key factors behind deep <b>learning</b>\u2019s rising popularity and future potential. Artificial Intelligence \u2022Concise definition: the effort to automate intellectual tasks normally performed by humans \u2022Initial take: expert rules \u2022Fine for chess \u2022Difficult to develop rules for image ...", "dateLastCrawled": "2022-01-27T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "machine <b>learning</b> - Why would a <b>cross-entropy approach negative infinity</b> ...", "url": "https://stats.stackexchange.com/questions/524297/why-would-a-cross-entropy-approach-negative-infinity", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/524297/why-would-a-<b>cross-entropy</b>-approach...", "snippet": "I&#39;m studying Deep <b>Learning</b> by Ian Goodfellow. In section 6.2.1.1 it says. For real-valued output variables, if the model <b>can</b> control the density of the output distribution (for example, by <b>learning</b> the variance parameter of a Gaussian output distribution) then it becomes possible to assign extremely high density to the correct training set outputs, resulting in <b>cross-entropy</b> approaching negative infinity. But the <b>cross-entropy</b> is defined as", "dateLastCrawled": "2022-01-19T00:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "[D] any <b>principled reason for cross entropy</b> instead of L2 in <b>language</b> ...", "url": "https://www.reddit.com/r/MachineLearning/comments/dqoh2u/d_any_principled_reason_for_cross_entropy_instead/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/Machine<b>Learning</b>/comments/dqoh2u/d_any_principled_reason_for...", "snippet": "These column vectors <b>can</b> <b>be thought</b> of as word embeddings and so <b>can</b> the activation values of the previous layer. The softmax function is then just a proxy for the neural net to learn a final weight matrix in such a way that by making one column vector more similar to the previous layer&#39;s activations, it also has to make all the other columns dissimilar to it.", "dateLastCrawled": "2021-09-16T14:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Deep <b>Learning</b> Structure for Cross-Domain <b>Sentiment Classification</b> Based ...", "url": "https://www.hindawi.com/journals/sp/2020/3810261/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/sp/2020/3810261", "snippet": "Within the <b>sentiment classification</b> field, the convolutional neural network (CNN) and long short-term memory (LSTM) are praised for their classification and prediction performance, but their accuracy, loss rate, and time are not ideal. To this purpose, a deep <b>learning</b> structure combining the improved <b>cross entropy</b> and weight for word is proposed for solving cross-domain <b>sentiment classification</b>, which focuses on achieving better text <b>sentiment classification</b> by optimizing and improving ...", "dateLastCrawled": "2021-12-23T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Contrastive Representation <b>Learning</b> - Lil&#39;Log", "url": "https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html", "isFamilyFriendly": true, "displayUrl": "https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-<b>learning</b>.html", "snippet": "<b>Contrastive learning</b> <b>can</b> be applied to both supervised and unsupervised data and has been shown to achieve good performance on a variety of vision and <b>language</b> tasks. Lil&#39;Log \uf984 Contact FAQ \u231b Archive. Contrastive Representation <b>Learning</b> . May 31, 2021 by Lilian Weng representation-<b>learning</b> long-read <b>language</b>-model unsupervised-<b>learning</b> . The main idea of <b>contrastive learning</b> is to learn representations such that similar samples stay close to each other, while dissimilar ones are far apart ...", "dateLastCrawled": "2022-02-02T19:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Gentle Introduction to <b>Cross-Entropy for Machine Learning</b>", "url": "https://machinelearningmastery.com/cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machine<b>learning</b>mastery.com/<b>cross-entropy-for-machine-learning</b>", "snippet": "Last Updated on December 22, 2020. <b>Cross-entropy</b> is commonly used in machine <b>learning</b> as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas <b>cross-entropy</b> <b>can</b> be thought to calculate the total entropy between the ...", "dateLastCrawled": "2022-02-03T03:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cross Entropy</b> Explained | What is <b>Cross Entropy</b> for ... - Great <b>Learning</b>", "url": "https://www.mygreatlearning.com/blog/cross-entropy-explained/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/blog/<b>cross-entropy</b>-explained", "snippet": "<b>Cross entropy</b> is the average number of bits required to send the message from distribution A to Distribution B. <b>Cross entropy</b> as a concept is applied in the field of machine <b>learning</b> when algorithms are built to predict from the model build. Model building is based on a comparison of actual results with the predicted results. This will be explained further by working on Logistic regression where <b>cross-entropy</b> is referred to as Log Loss.", "dateLastCrawled": "2022-02-02T07:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introducing <b>Cross-Entropy for Machine Learning</b> - BLOCKGENI", "url": "https://blockgeni.com/introducing-cross-entropy-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://blockgeni.com/introducing-<b>cross-entropy-for-machine-learning</b>", "snippet": "<b>Cross-entropy</b> is commonly used in machine <b>learning</b> as a loss function. <b>Cross-entropy</b> is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas <b>cross-entropy</b> <b>can</b> be thought to calculate the total entropy between the distributions. <b>Cross-entropy</b> is also ...", "dateLastCrawled": "2022-01-27T13:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Cross-Entropy Loss in ML</b>. What is Entropy in ML? | by Inara Koppert ...", "url": "https://medium.com/unpackai/cross-entropy-loss-in-ml-d9f22fc11fe0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/unpackai/<b>cross-entropy-loss-in-ml</b>-d9f22fc11fe0", "snippet": "<b>Cross - entropy</b> loss is used when adjusting model weights during training. The aim is to minimize the loss, i.e, the smaller the loss the better the model. A perfect model has a <b>cross-entropy</b> loss ...", "dateLastCrawled": "2022-01-31T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Should <b>cross-entropy</b> be used <b>in classification tasks</b>? | AI-SCHOLAR | AI ...", "url": "https://ai-scholar.tech/en/articles/deep-learning/closs-square", "isFamilyFriendly": true, "displayUrl": "https://ai-scholar.tech/en/articles/deep-<b>learning</b>/closs-square", "snippet": "When training with <b>cross-entropy</b> loss, <b>learning</b> was stopped when validation performance did not improve for five consecutive epochs. When training with squared loss, the following two protocols were used. Similar to the <b>cross-entropy</b> loss, <b>learning</b> is stopped when validation performance does not improve for 5 consecutive epochs.", "dateLastCrawled": "2021-12-01T11:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Post: Device Placement with <b>Cross-Entropy</b> Minimization and Proximal ...", "url": "https://fid3024.github.io/papers/2018%20-%20Post:%20Device%20Placement%20with%20Cross-Entropy%20Minimization%20and%20Proximal%20Policy%20Optimization.pdf", "isFamilyFriendly": true, "displayUrl": "https://fid3024.github.io/papers/2018 - Post: Device Placement with <b>Cross-Entropy</b>...", "snippet": "design and implementation of <b>a new</b> <b>learning</b> algorithm, called Post, that integrates <b>cross-entropy</b> minimization and proximal policy optimization [10, 11], a state-of-the-art reinforcement <b>learning</b> algorithm. With Post, <b>cross-entropy</b> minimization is applied to each batch of trials, to reduce the sample space of placement for a higher sample ef\ufb01ciency. Within a batch, proximal policy optimization is applied to each placement trial to incrementally improve the placement distribution, which ...", "dateLastCrawled": "2021-08-11T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Cross-Entropy</b>: <b>A New</b> <b>Metric for Software Defect Prediction</b> | CPS-VO", "url": "https://cps-vo.org/node/63685", "isFamilyFriendly": true, "displayUrl": "https://cps-vo.org/node/63685", "snippet": "To improve prediction performance, this paper introduces <b>cross-entropy</b>, one common measure for natural <b>language</b>, as <b>a new</b> code metric into defect prediction tasks and proposes a framework called DefectLearner for this process. We first build a recurrent neural network <b>language</b> model to learn regularities in source code from software repository. Based on the trained model, the <b>cross-entropy</b> of each component <b>can</b> be calculated. To evaluate the discrimination for defect-proneness, <b>cross-entropy</b> ...", "dateLastCrawled": "2022-01-24T03:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Cross Entropy</b> of Neural <b>Language</b> Models at Infinity\u2014<b>A New</b> Bound of the ...", "url": "https://www.mdpi.com/1099-4300/20/11/839/htm", "isFamilyFriendly": true, "displayUrl": "https://www.mdpi.com/1099-4300/20/11/839/htm", "snippet": "Neural <b>language</b> models have drawn a lot of attention for their strong ability to predict natural <b>language</b> text. In this paper, we estimate the entropy rate of natural <b>language</b> with state-of-the-art neural <b>language</b> models. To obtain the estimate, we consider the <b>cross entropy</b>, a measure of the prediction accuracy of neural <b>language</b> models, under the theoretically ideal conditions that they are trained with an infinitely large dataset and receive an infinitely long context for prediction. We ...", "dateLastCrawled": "2022-01-19T19:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Cross Entropy of Neural Language Models at Infinity\u2014A New</b> Bound ...", "url": "https://www.researchgate.net/publication/328739594_Cross_Entropy_of_Neural_Language_Models_at_Infinity-A_New_Bound_of_the_Entropy_Rate", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328739594_<b>Cross_Entropy</b>_of_Neural_<b>Language</b>...", "snippet": "<b>Cross entropy</b> of (a) RHN on OB dataset, (b) AWD-LSTM-MoS on CNA dataset, and (c) n-gram <b>language</b> models on OB dataset with different training data sizes. The fitting function f 1 was applied to ...", "dateLastCrawled": "2021-10-19T19:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Natural <b>Language</b> Processing: Are there any tools that <b>can</b> measure the ...", "url": "https://www.quora.com/Natural-Language-Processing-Are-there-any-tools-that-can-measure-the-cross-entropy-of-2-corpora-of-short-text-snippets", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Natural-<b>Language</b>-Processing-Are-there-any-tools-that-<b>can</b>-measure...", "snippet": "Answer (1 of 2): Reliably measuring semantic similarity between two pieces of text is an open problem in natural <b>language</b> processing. As an illustration consider that as recently as this year, there was a shared task as part of the SemEval (Semantic Evaluation) workshop designed to elicit ideas o...", "dateLastCrawled": "2022-01-17T18:36:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Cross-Entropy</b> Demystified. What is it? Is there any relation to\u2026 | by ...", "url": "https://naokishibuya.medium.com/demystifying-cross-entropy-e80e3ad54a8", "isFamilyFriendly": true, "displayUrl": "https://naokishibuya.medium.com/demystifying-<b>cross-entropy</b>-e80e3ad54a8", "snippet": "However, the <b>machine</b> <b>learning</b> application uses the base e logarithm for implementation convenience. Binary <b>Cross-Entropy</b>. We can use the binary <b>cross-entropy</b> for binary classification where we have yes/no answer. For example, there are only dogs or cats in images. For the binary classifications, the <b>cross-entropy</b> formula contains only two ...", "dateLastCrawled": "2022-01-26T13:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Why and How to use <b>Cross Entropy</b>. The fundamental reasons for ...", "url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-and-how-to-use-<b>cross-entropy</b>-4e983cbdd873", "snippet": "The fundamental reasons for minimizing binary <b>cross entropy</b> (log loss) with probabilistic classification models . Will Arliss. Sep 26, 2020 \u00b7 7 min read. Introduction. This post discusses why logistic regression necessarily uses a different loss function than linear regression. First, the simple yet inefficient way to solve logistic regression will be presented, then the slightly less simple but much more efficient way will be explained and compared. The simple way. Linear regression is the ...", "dateLastCrawled": "2022-01-31T14:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Gentle Introduction to Information Entropy - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-is-information-entropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/what-is-information-entropy", "snippet": "Calculating information and entropy is a useful tool in <b>machine</b> <b>learning</b> and is used as the basis for techniques such as feature selection, building decision trees, and, more generally, fitting classification models. As such, a <b>machine</b> <b>learning</b> practitioner requires a strong understanding and intuition for information and entropy. In this post, you will discover a gentle introduction to information entropy. After reading this post, you will know: Information theory is concerned with data ...", "dateLastCrawled": "2022-02-02T13:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "<b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed. Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>. We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms. We have to use different techniques like neural networks.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>machine</b> <b>learning</b> - <b>Cross-entropy loss</b> explanation - Data Science Stack ...", "url": "https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/20296", "snippet": "The answer from Neil is correct. However I think its important to point out that while the loss does not depend on the distribution between the incorrect classes (only the distribution between the correct class and the rest), the gradient of this loss function does effect the incorrect classes differently depending on how wrong they are. So when you use cross-ent in <b>machine</b> <b>learning</b> you will change weights differently for [0.1 0.5 0.1 0.1 0.2] and [0.1 0.6 0.1 0.1 0.1].", "dateLastCrawled": "2022-01-27T03:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Shannon <b>entropy</b> in the context of <b>machine</b> <b>learning</b> and AI | by Frank ...", "url": "https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/shannon-<b>entropy</b>-in-the-context-of-<b>machine</b>-<b>learning</b>-and-ai-24...", "snippet": "Closely related to <b>cross entropy</b>, the KL divergence from q to p, written DKL(p||q), is another similarity measure often used in <b>machine</b> <b>learning</b>. In the language of Bayesian Inference, DKL(p||q ...", "dateLastCrawled": "2022-01-30T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Main concepts behind Machine Learning</b> | by Bruno Eidi Nishimoto ...", "url": "https://medium.com/neuronio/main-concepts-behind-machine-learning-22cd81d68a11", "isFamilyFriendly": true, "displayUrl": "https://medium.com/neuronio/<b>main-concepts-behind-machine-learning</b>-22cd81d68a11", "snippet": "<b>Machine</b> <b>Learning</b> is a concept that is currently trending. It is a subarea from Artificial Intelligence and it consists on the fact that the <b>machine</b> can learn by itself without being explicitly ...", "dateLastCrawled": "2022-01-19T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Lecture 4 Fundamentals of deep <b>learning</b> and neural networks", "url": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/biods388/downloads/BIODS388_Lecture_4.pdf", "snippet": "Deep <b>learning</b>: <b>Machine</b> <b>learning</b> models based on \u201cdeep\u201d neural networks comprising millions (sometimes billions) of parameters organized into hierarchical layers. Features are multiplied and added together repeatedly, with the outputs from one layer of parameters being fed into the next layer -- before a prediction is made. Contrast with linear regression: Agenda for today - More on the structure of neural network models - <b>Machine</b> <b>learning</b> training loop and concept of loss, in the context ...", "dateLastCrawled": "2022-02-02T09:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Learning and Information Theory</b> \u2013 Deep &amp; Shallow", "url": "https://deep-and-shallow.com/2020/01/09/deep-learning-and-information-theory/", "isFamilyFriendly": true, "displayUrl": "https://deep-and-shallow.com/2020/01/09/<b>deep-learning-and-information-theory</b>", "snippet": "If you have tried to understand the maths behind <b>machine</b> <b>learning</b>, including deep <b>learning</b>, you would have come across topics from Information Theory \u2013 Entropy, <b>Cross Entropy</b>, KL Divergence, etc. The concepts from information theory is ever prevalent in the realm of <b>machine</b> <b>learning</b>, right from the splitting criteria of a Decision Tree to loss functions in Generative Adversarial Networks.", "dateLastCrawled": "2022-02-01T00:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "[D] A Short Introduction to Entropy, <b>Cross-Entropy</b> and KL-Divergence ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7vhmp7/d_a_short_introduction_to_entropy_crossentropy/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7vhmp7/d_a_short_introduction_to...", "snippet": "I am having trouble reconciling the concept with the <b>analogy</b>. At 2:35 even if a rainy day was 25% likely, there&#39;s still only two states, rainy and sunny, and therefor only 1 bit of information is needed to convey that, so only one bit of data needs to be sent, even though the 1 bit of data reduces the uncertainty of a rainy day by a factor of 4. I quite don&#39;t get what he means by this being 2 bits of information. I guess where I am stuck is how the uncertainty reduction factor translates to ...", "dateLastCrawled": "2021-08-20T08:03:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Beat the Bookmakers With Tree-Based <b>Machine</b> <b>Learning</b> Algorithms | by ...", "url": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-machine-learning-algorithms-1d349335b54", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/beat-the-bookmakers-with-tree-based-<b>machine</b>...", "snippet": "<b>Cross-entropy is similar</b> to Gini Impurity, but it involves using the concept of entropy from information theory. This article won\u2019t go in depth about it, but essentially, as the cross-entropy ...", "dateLastCrawled": "2022-01-26T04:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>A Traveler\u2019s Diary on the Road to Machine</b> <b>Learning</b> - Chapter 1 | by ...", "url": "https://medium.com/swlh/a-travelers-diary-on-the-road-to-machine-learning-chapter-1-8850ec5b4243", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>a-travelers-diary-on-the-road-to-machine</b>-<b>learning</b>-chapter-1...", "snippet": "Types of <b>Machine</b> <b>Learning</b> algorithms: ... Sparse categorical <b>cross entropy is similar</b> to categorical cross entropy, only difference is it uses only one value as target. It saves memory as well as ...", "dateLastCrawled": "2021-05-21T04:27:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Deep Learning for Computer Architects</b> | Chen Jeff - Academia.edu", "url": "https://www.academia.edu/40860009/Deep_Learning_for_Computer_Architects", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40860009/<b>Deep_Learning_for_Computer_Architects</b>", "snippet": "This text serves as a primer for computer architects in a new and rapidly evolving \ufb01eld. We review how <b>machine</b> <b>learning</b> has evolved since its inception in the 1960s and track the key developments leading up to the emergence of the powerful deep <b>learning</b> techniques that emerged in the last decade.", "dateLastCrawled": "2022-01-28T02:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(cross-entropy)  is like +(learning a new language)", "+(cross-entropy) is similar to +(learning a new language)", "+(cross-entropy) can be thought of as +(learning a new language)", "+(cross-entropy) can be compared to +(learning a new language)", "machine learning +(cross-entropy AND analogy)", "machine learning +(\"cross-entropy is like\")", "machine learning +(\"cross-entropy is similar\")", "machine learning +(\"just as cross-entropy\")", "machine learning +(\"cross-entropy can be thought of as\")", "machine learning +(\"cross-entropy can be compared to\")"]}