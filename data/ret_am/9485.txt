{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>quick intro to Bayesian neural networks</b> - matthewmcateer.me", "url": "https://matthewmcateer.me/blog/a-quick-intro-to-bayesian-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://matthewmcateer.me/blog/a-<b>quick-intro-to-bayesian-neural-networks</b>", "snippet": "Our model is a <b>neural</b> <b>network</b> with two DenseVariational hidden layers, each having 20 units, and one DenseVariational output layer with one unit. Instead of modeling a full probability distribution p (y \u2223 x, w) p(y \\lvert \\mathbf{x},\\mathbf{w}) p (y \u2223 x, w) as output the <b>network</b> simply outputs the mean of the corresponding Gaussian distribution. In other words, we do not model aleatoric uncertainty here and assume it is known.", "dateLastCrawled": "2021-05-30T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian methods for neural networks</b> - FAQ. - Inference <b>Group</b>: Home", "url": "http://www.inference.org.uk/mackay/Bayes_FAQ.html", "isFamilyFriendly": true, "displayUrl": "www.inference.org.uk/mackay/Bayes_FAQ.html", "snippet": "I have read your introductory papers on the <b>Bayesian</b> framework for training multilayer perceptrons (&quot;<b>Bayesian</b> Interpolation&quot;, &quot;A Practical <b>Bayesian</b> Framework for Backpropagation Networks&quot;, <b>Neural</b> Computation 4 (1992)) with great interest, but have a question concerning the practical implementation. The scale parameters alpha and beta can be calculated via the <b>number</b> of free parameters, gamma= k-alpha*Trace(A^-1), according to 2*alpha*Ew(w_mp)=gamma, 2*beta*Ed(w_mp)=N-gamma (equations (4.8 ...", "dateLastCrawled": "2021-12-14T17:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>An overview of artificial neural network</b>", "url": "http://reports.ias.ac.in/report/21405/an-overview-of-artificial-neural-network", "isFamilyFriendly": true, "displayUrl": "reports.ias.ac.in/report/21405/<b>an-overview-of-artificial-neural-network</b>", "snippet": "Different types of Artificial <b>Neural</b> <b>Network</b> <b>like</b> Feed Forward, Convolutional, Radial Basis Function (RBFN) and <b>Bayesian</b> <b>Neural</b> <b>Network</b> were introduced. I successfully implemented FNN and CNN, and had an introductory lessons on RBFN and <b>Bayesian</b> <b>Neural</b> <b>Network</b> without implementation. I also tried to learn how to enhance the capability of <b>neural</b> <b>network</b> but I didn&#39;t arrived at any conclusion. In future, A detailed study on RBFN and <b>Bayesian</b> <b>neural</b> <b>network</b> with their implementation is intended ...", "dateLastCrawled": "2021-12-30T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Why Neural Networks Generalise, and Why They</b> Are (Kind of) <b>Bayesian</b> ...", "url": "https://www.greaterwrong.com/posts/YSFJosoHYFyXjoYWa/why-neural-networks-generalise-and-why-they-are-kind-of", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/YSFJosoHYFyXjoYWa/why-<b>neural</b>-<b>networks</b>-generalise...", "snippet": "Currently, we do not have a good theoretical understanding of how or why <b>neural</b> networks actually work. For example, we know that large <b>neural</b> networks are sufficiently expressive to compute almost any kind of function. Moreover, most functions that fit a given set of training data will not generalise well to new data. And yet, if we train a <b>neural</b> <b>network</b> we will usually obtain a function that gives good generalisation. What is the mechanism behind this phenomenon? There has been some ...", "dateLastCrawled": "2021-12-28T12:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Why hasn&#39;t the <b>Bayesian network been as successful</b> as the Deep <b>Neural</b> ...", "url": "https://www.quora.com/Why-hasnt-the-Bayesian-network-been-as-successful-as-the-Deep-Neural-Network", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-hasnt-the-<b>Bayesian-network-been-as-successful</b>-as-the-Deep...", "snippet": "Answer (1 of 5): Well, the definition of \u201csuccessful\u201d is important, here. <b>Bayesian</b> networks are arguably more successful than deep <b>neural</b> networks to date. They are widely applied across industries and can readily be used for inference, modelling, and prediction. Presumably your question is real...", "dateLastCrawled": "2022-01-15T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Exploring Faster Screening with Fewer</b> Tests via <b>Bayesian</b> <b>Group</b> Testing", "url": "https://ai.googleblog.com/2020/07/exploring-faster-screening-with-fewer.html", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com/2020/07/<b>exploring-faster-screening-with-fewer</b>.html", "snippet": "In \u201cNoisy Adaptive <b>Group</b> Testing using <b>Bayesian</b> Sequential Experimental Design\u201d, we present an approach to <b>group</b> testing that can operate in a noisy setting (i.e., where tests can be mistaken) to decide adaptively by looking at past results which groups to test next, with the goal to converge on a reliable detection as quickly, and with as few tests, as possible. Large scale simulations suggest that this approach may result in significant improvements over both adaptive and non-adaptive ...", "dateLastCrawled": "2022-01-26T00:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "deep learning - What are some well-known problems where <b>neural</b> networks ...", "url": "https://ai.stackexchange.com/questions/18576/what-are-some-well-known-problems-where-neural-networks-dont-do-very-well", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/18576/what-are-some-well-known-problems-where...", "snippet": "To conclude, <b>neural</b> networks are just function approximators, i.e. they approximate a specific function (or set of functions, in the case of <b>Bayesian</b> <b>neural</b> networks), given a specific configuration of the parameters. They can&#39;t do more than that. They cannot magically do something that they have not been trained to do, and it is usually the case that you don&#39;t really know the specific function the <b>neural</b> <b>network</b> is representing (hence the expression", "dateLastCrawled": "2022-01-17T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bayesian</b> Optimization - Can you do better than randomly <b>guessing</b> ...", "url": "https://av.tib.eu/media/38177", "isFamilyFriendly": true, "displayUrl": "https://av.tib.eu/media/38177", "snippet": "Choosing the right hyper-parameters for a deep <b>neural</b> <b>network</b>, configuring a fluid dynamics simulation or finding the recipe of the next prize winning beer have three things in common: each trial is expensive, you don&#39;t have an analytic function you can minimise with scipy.minimize and you only get noisy observations from each trial. <b>Bayesian</b> optimisation (BO) to the rescue! BO is a clever piece of math designed to solve exactly these kinds of problems. This talk is for <b>people</b> who have to ...", "dateLastCrawled": "2022-01-17T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "To Resolve <b>Neural</b> <b>Network</b> Problems, Leave It to the \u201cBoard of Experts ...", "url": "https://cpedder.medium.com/to-resolve-neural-network-problems-leave-it-to-the-board-of-experts-fe540bb7b628", "isFamilyFriendly": true, "displayUrl": "https://cpedder.medium.com/to-resolve-<b>neural</b>-<b>network</b>-problems-leave-it-to-the-board-of...", "snippet": "Much <b>like</b> the fairgoers who guess 10 pounds versus 10 tons in Galton\u2019s ox-<b>guessing</b> game, each member of the board in our multiple <b>neural</b> net training example overfits differently, and they tend to cancel each other out. Second, this approach allows the <b>neural</b> <b>network</b> to express lack of certainty. If the board is split between options, it makes the prediction less certain. Finally, we get to see a little more behind the curtain with this method \u2014 with several classifiers, we get to see ...", "dateLastCrawled": "2022-01-15T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Explaining Explainable AI. Explainable AI \u2014 \u201cAn approximation of\u2026 | by ...", "url": "https://medium.com/swlh/explaining-explainable-ai-b3ca0f8b357b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/explaining-explainable-ai-b3ca0f8b357b", "snippet": "A very oversimplified definition of a <b>neural</b> <b>network</b> is a function which, based on the inputs can automatically adjust its parameters to approximate the output.", "dateLastCrawled": "2022-01-06T03:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>quick intro to Bayesian neural networks</b> - matthewmcateer.me", "url": "https://matthewmcateer.me/blog/a-quick-intro-to-bayesian-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://matthewmcateer.me/blog/a-<b>quick-intro-to-bayesian-neural-networks</b>", "snippet": "Their approach to variational inference <b>is similar</b> to the approach described here but differs in some details. For example, they compute the complexity cost analytically instead of estimating it from Monte Carlo samples, among other differences. Part 4: <b>Bayesian</b> LeNet5 in Tensorflow Probability. Thanks to Tensorflow Probability, we can extend our <b>bayesian</b> example to an image classification task with relative ease. Much of our process for building the model <b>is similar</b>. For example, we import ...", "dateLastCrawled": "2021-05-30T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian methods for neural networks</b> - FAQ. - Inference <b>Group</b>: Home", "url": "http://www.inference.org.uk/mackay/Bayes_FAQ.html", "isFamilyFriendly": true, "displayUrl": "www.inference.org.uk/mackay/Bayes_FAQ.html", "snippet": "I have read your introductory papers on the <b>Bayesian</b> framework for training multilayer perceptrons (&quot;<b>Bayesian</b> Interpolation&quot;, &quot;A Practical <b>Bayesian</b> Framework for Backpropagation Networks&quot;, <b>Neural</b> Computation 4 (1992)) with great interest, but have a question concerning the practical implementation. The scale parameters alpha and beta can be calculated via the <b>number</b> of free parameters, gamma= k-alpha*Trace(A^-1), according to 2*alpha*Ew(w_mp)=gamma, 2*beta*Ed(w_mp)=N-gamma (equations (4.8 ...", "dateLastCrawled": "2021-12-14T17:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Bayesian approach for neural networks--review and case studies</b> ...", "url": "https://www.academia.edu/2842466/Bayesian_approach_for_neural_networks_review_and_case_studies", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2842466/<b>Bayesian_approach_for_neural_networks_review</b>_and_case...", "snippet": "The <b>Bayesian</b> approach permits propagation of uncertainty in quantities which are unknown to other assumptions in the model, which may be more generally valid or easier to guess in the problem. The case problem studied in this paper include a regression, a classi\u00aecation, and an inverse problem.", "dateLastCrawled": "2022-01-16T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) A <b>Bayesian</b> Deep <b>Neural</b> <b>Network</b> Approach to Seven-Point Thermal ...", "url": "https://www.researchgate.net/publication/357637299_A_Bayesian_Deep_Neural_Network_Approach_to_Seven-Point_Thermal_Sensation_Perception", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357637299_A_<b>Bayesian</b>_Deep_<b>Neural</b>_<b>Network</b>...", "snippet": "<b>neural</b> <b>network</b>,\u201d Building and En vironment, vol. 188, p. 107485, 2021. [16] J. Ahn and S. Cho, \u201cDevelopment of an intelligent building controller to mitigate indoor thermal dissatisfaction and ...", "dateLastCrawled": "2022-01-31T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>An overview of artificial neural network</b>", "url": "http://reports.ias.ac.in/report/21405/an-overview-of-artificial-neural-network", "isFamilyFriendly": true, "displayUrl": "reports.ias.ac.in/report/21405/<b>an-overview-of-artificial-neural-network</b>", "snippet": "In future, A detailed study on RBFN and <b>Bayesian</b> <b>neural</b> <b>network</b> with their implementation is intended. I will also try to learn the mathematical aspects of enhancing the <b>neural</b> <b>network</b> capability. In all, this internship proved fruitful to me in achieving good technical and coding skills and having an insight to the world of Artificial Intelligence. INTRODUCTION. Artificial Intelligence has been a popular topic in science fiction and news articles for at least a few decades, and is the ...", "dateLastCrawled": "2021-12-30T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Why hasn&#39;t the <b>Bayesian network been as successful</b> as the Deep <b>Neural</b> ...", "url": "https://www.quora.com/Why-hasnt-the-Bayesian-network-been-as-successful-as-the-Deep-Neural-Network", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-hasnt-the-<b>Bayesian-network-been-as-successful</b>-as-the-Deep...", "snippet": "Answer (1 of 5): Well, the definition of \u201csuccessful\u201d is important, here. <b>Bayesian</b> networks are arguably more successful than deep <b>neural</b> networks to date. They are widely applied across industries and can readily be used for inference, modelling, and prediction. Presumably your question is real...", "dateLastCrawled": "2022-01-15T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "deep learning - What are some well-known problems where <b>neural</b> networks ...", "url": "https://ai.stackexchange.com/questions/18576/what-are-some-well-known-problems-where-neural-networks-dont-do-very-well", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/18576/what-are-some-well-known-problems-where...", "snippet": "To conclude, <b>neural</b> networks are just function approximators, i.e. they approximate a specific function (or set of functions, in the case of <b>Bayesian</b> <b>neural</b> networks), given a specific configuration of the parameters. They can&#39;t do more than that. They cannot magically do something that they have not been trained to do, and it is usually the case that you don&#39;t really know the specific function the <b>neural</b> <b>network</b> is representing (hence the expression", "dateLastCrawled": "2022-01-17T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bayesian</b> Optimization - Can you do better than randomly <b>guessing</b> ...", "url": "https://av.tib.eu/media/38177", "isFamilyFriendly": true, "displayUrl": "https://av.tib.eu/media/38177", "snippet": "Choosing the right hyper-parameters for a deep <b>neural</b> <b>network</b>, configuring a fluid dynamics simulation or finding the recipe of the next prize winning beer have three things in common: each trial is expensive, you don&#39;t have an analytic function you can minimise with scipy.minimize and you only get noisy observations from each trial. <b>Bayesian</b> optimisation (BO) to the rescue! BO is a clever piece of math designed to solve exactly these kinds of problems. This talk is for <b>people</b> who have to ...", "dateLastCrawled": "2022-01-17T08:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 8, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>GPT-3</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/GPT-3", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>GPT-3</b>", "snippet": "GPT-n models are based on this Transformer-based deep learning <b>neural</b> <b>network</b> architecture. There are a <b>number</b> of NLP systems capable of processing, mining, organizing, connecting, contrasting, understanding and generating answers to questions. On June 11, 2018, OpenAI researchers and engineers posted their original paper on generative models\u2014language models\u2014artificial intelligence systems\u2014that could be pre-trained with an enormous and diverse corpus of text via datasets, in a process ...", "dateLastCrawled": "2022-02-02T22:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Explaining Explainable AI. Explainable AI \u2014 \u201cAn approximation of\u2026 | by ...", "url": "https://medium.com/swlh/explaining-explainable-ai-b3ca0f8b357b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/explaining-explainable-ai-b3ca0f8b357b", "snippet": "A very oversimplified definition of a <b>neural</b> <b>network</b> is a function which, based on the inputs can automatically adjust its parameters to approximate the output.", "dateLastCrawled": "2022-01-06T03:13:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bayesian methods for neural networks</b> - FAQ. - Inference <b>Group</b>: Home", "url": "http://www.inference.org.uk/mackay/Bayes_FAQ.html", "isFamilyFriendly": true, "displayUrl": "www.inference.org.uk/mackay/Bayes_FAQ.html", "snippet": "I have read your introductory papers on the <b>Bayesian</b> framework for training multilayer perceptrons (&quot;<b>Bayesian</b> Interpolation&quot;, &quot;A Practical <b>Bayesian</b> Framework for Backpropagation Networks&quot;, <b>Neural</b> Computation 4 (1992)) with great interest, but have a question concerning the practical implementation. The scale parameters alpha and beta <b>can</b> be calculated via the <b>number</b> of free parameters, gamma= k-alpha*Trace(A^-1), according to 2*alpha*Ew(w_mp)=gamma, 2*beta*Ed(w_mp)=N-gamma (equations (4.8 ...", "dateLastCrawled": "2021-12-14T17:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Why Neural Networks Generalise, and Why They</b> Are (Kind of) <b>Bayesian</b> ...", "url": "https://www.alignmentforum.org/posts/YSFJosoHYFyXjoYWa/why-neural-networks-generalise-and-why-they-are-kind-of", "isFamilyFriendly": true, "displayUrl": "https://www.alignmentforum.org/posts/YSFJosoHYFyXjoYWa/why-<b>neural</b>-<b>networks</b>-generalise...", "snippet": "If we take &quot;the space of functions&quot; to be all the functions that a given <b>neural</b> <b>network</b> <b>can</b> express on the entire vector space in which it is defined, then there would be an uncountably infinite <b>number</b> of such functions, and any given function would never show up more than once in any kind of experiment we could do. We therefore need a way to lump the functions together into sensible buckets, and we decided to do that by looking at what output the function gives on a set of images not used ...", "dateLastCrawled": "2021-12-24T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why hasn&#39;t the <b>Bayesian network been as successful</b> as the Deep <b>Neural</b> ...", "url": "https://www.quora.com/Why-hasnt-the-Bayesian-network-been-as-successful-as-the-Deep-Neural-Network", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-hasnt-the-<b>Bayesian-network-been-as-successful</b>-as-the-Deep...", "snippet": "Answer (1 of 5): Well, the definition of \u201csuccessful\u201d is important, here. <b>Bayesian</b> networks are arguably more successful than deep <b>neural</b> networks to date. They are widely applied across industries and <b>can</b> readily be used for inference, modelling, and prediction. Presumably your question is real...", "dateLastCrawled": "2022-01-15T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "To Resolve <b>Neural</b> <b>Network</b> Problems, Leave It to the \u201cBoard of Experts ...", "url": "https://cpedder.medium.com/to-resolve-neural-network-problems-leave-it-to-the-board-of-experts-fe540bb7b628", "isFamilyFriendly": true, "displayUrl": "https://cpedder.medium.com/to-resolve-<b>neural</b>-<b>network</b>-problems-leave-it-to-the-board-of...", "snippet": "Interestingly, the principle at play in this ox-weight <b>guessing</b> game <b>can</b> also help us in training <b>neural</b> networks. To explain, I\u2019ll start with some background about how <b>neural</b> networks operate. The Paradox of Arbitrary Choice. <b>Neural</b> networks are weird things \u2014 they confuse and befuddle <b>people</b> used to statistical thinking. For example, it\u2019s a well-known fact that if you collect N data points, and you want to do a regression on M variables, you\u2019d better make sure that N is greater ...", "dateLastCrawled": "2022-01-15T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Why Neural Networks Generalise, and Why They</b> Are (Kind of) <b>Bayesian</b> ...", "url": "https://www.greaterwrong.com/posts/YSFJosoHYFyXjoYWa/why-neural-networks-generalise-and-why-they-are-kind-of", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/YSFJosoHYFyXjoYWa/why-<b>neural</b>-<b>networks</b>-generalise...", "snippet": "<b>Why Neural Networks Generalise, and Why They</b> Are (Kind of) <b>Bayesian</b>. Joar Skalse 29 Dec 2020 13:33 UTC. LW: 60 AF: 47. 57 comments LW link. AI Post permalink Link without comments Link without top nav bars Link without comments or top nav bars. Currently, we do not have a good theoretical understanding of how or why <b>neural</b> networks actually work. For example, we know that large <b>neural</b> networks are sufficiently expressive to compute almost any kind of function. Moreover, most functions that ...", "dateLastCrawled": "2021-12-28T12:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "\u201cMachine <b>Learning Under a Modern Optimization Lens\u201d Under a Bayesian</b> ...", "url": "https://statmodeling.stat.columbia.edu/2019/11/26/machine-learning-under-a-modern-optimization-lens-under-a-bayesian-lens/", "isFamilyFriendly": true, "displayUrl": "https://statmodeling.stat.columbia.edu/2019/11/26/machine-learning-under-a-modern...", "snippet": "The authors prove that a deep enough optimal classification tree <b>can</b> achieve the same prediction ability as a deep <b>neural</b> <b>network</b> \u2014 when the tree makes splits exactly according to the same <b>network</b> \u2014 whereas tree has a much better interpretability (evidently we could prove a multilevel model at the same setting will achieve a prediction ability no worse than that deep net). The first glance might suggest a computationally prohibitive expense of solving a high dimensional discrete ...", "dateLastCrawled": "2022-01-31T08:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bayesian collective learning emerges from heuristic social learning</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0010027720302882", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0010027720302882", "snippet": "When the <b>number</b> of options is large or when \u03b7 \u2217 = 0.5 + \u03f5 for small \u03f5 &gt; 0, this hide-and-seek setting <b>can</b> <b>be thought</b> of as a pessimistic assumption about the identifiability of the best option in the environment. In other words, this setting <b>can</b> be interpreted as one in which good options are rare or difficult to identify. Similar results <b>can</b> also be derived in more general contexts", "dateLastCrawled": "2021-10-28T21:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "243 questions with answers in <b>BAYESIAN</b> | Science topic", "url": "https://www.researchgate.net/topic/Bayesian", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/topic/<b>Bayesian</b>", "snippet": "I am modeling annual densities of birds to estimate population trends in a <b>Bayesian</b> framework, using a log-link regression in rjags. I am modeling long term (2002 - 2020) and short term (2011-2020 ...", "dateLastCrawled": "2022-02-03T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to best impute missing values of county-level time series data using R?", "url": "https://stats.stackexchange.com/questions/530033/how-to-best-impute-missing-values-of-county-level-time-series-data-using-r", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/530033/how-to-best-impute-missing-values-of...", "snippet": "Another <b>thought</b> is to use a <b>Bayesian</b> <b>neural</b> <b>network</b> as a very flexible modeling approach, I&#39;m just note sure what exactly would fit your problem best. E.g. you could have an embedding layer for each county and each record within the county (this would deal decently well with counties/records for which you have at least some data, but less well with ones that you don&#39;t have any data for).", "dateLastCrawled": "2022-01-18T21:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Experiences with bayesian hyperparameter optimization</b>? : MachineLearning", "url": "https://www.reddit.com/r/MachineLearning/comments/2m1cad/experiences_with_bayesian_hyperparameter/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/MachineLearning/comments/2m1cad/experiences_with_<b>bayesian</b>...", "snippet": "One of the key complaints I have heard from others in the past for something like finding <b>neural</b> <b>network</b> hyperparameters is that these <b>Bayesian</b> optimization algs tend to try and explore the &quot;edges&quot; of the space, even when intialized with hundereds of other experiments. This <b>can</b> be quite wasteful when networks take days or weeks to evaluate. This coupled with the sequential nature of <b>Bayesian</b> optimization means a &quot;random search&quot;, which is truly parallel, <b>can</b> be easier, faster in certain cases ...", "dateLastCrawled": "2021-01-10T13:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>quick intro to Bayesian neural networks</b> - matthewmcateer.me", "url": "https://matthewmcateer.me/blog/a-quick-intro-to-bayesian-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://matthewmcateer.me/blog/a-<b>quick-intro-to-bayesian-neural-networks</b>", "snippet": "Pretty impressive! This illustrates one of the other add-ons we <b>can</b> easily make for <b>bayesian</b> <b>neural</b> networks: a probability cutoff. In this case, if none of our probabilities exceed 0.2, we <b>can</b> get our <b>network</b> to refuse to classify the images. Of course, it wasn\u2019t always like this. It took our <b>network</b> a while before it was correctly able to ...", "dateLastCrawled": "2021-05-30T11:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bayesian methods for neural networks</b> - FAQ. - Inference <b>Group</b>: Home", "url": "http://www.inference.org.uk/mackay/Bayes_FAQ.html", "isFamilyFriendly": true, "displayUrl": "www.inference.org.uk/mackay/Bayes_FAQ.html", "snippet": "I have read your introductory papers on the <b>Bayesian</b> framework for training multilayer perceptrons (&quot;<b>Bayesian</b> Interpolation&quot;, &quot;A Practical <b>Bayesian</b> Framework for Backpropagation Networks&quot;, <b>Neural</b> Computation 4 (1992)) with great interest, but have a question concerning the practical implementation. The scale parameters alpha and beta <b>can</b> be calculated via the <b>number</b> of free parameters, gamma= k-alpha*Trace(A^-1), according to 2*alpha*Ew(w_mp)=gamma, 2*beta*Ed(w_mp)=N-gamma (equations (4.8 ...", "dateLastCrawled": "2021-12-14T17:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) A <b>Bayesian</b> Deep <b>Neural</b> <b>Network</b> Approach to Seven-Point Thermal ...", "url": "https://www.researchgate.net/publication/357637299_A_Bayesian_Deep_Neural_Network_Approach_to_Seven-Point_Thermal_Sensation_Perception", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/357637299_A_<b>Bayesian</b>_Deep_<b>Neural</b>_<b>Network</b>...", "snippet": "The hyperparameter-tuning process of the proposed model is optimized using the <b>Bayesian</b> strategy and predicts the thermal sensation of occupants with 78% accuracy, which is much higher than the ...", "dateLastCrawled": "2022-01-31T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Exploring Faster Screening with Fewer</b> Tests via <b>Bayesian</b> <b>Group</b> Testing", "url": "https://ai.googleblog.com/2020/07/exploring-faster-screening-with-fewer.html", "isFamilyFriendly": true, "displayUrl": "https://ai.googleblog.com/2020/07/<b>exploring-faster-screening-with-fewer</b>.html", "snippet": "A <b>group</b> testing strategy is an algorithm that is tasked with <b>guessing</b> who, among a list of n <b>people</b>, carries a particular pathogen. To do so, the strategy provides instructions for pooling individuals into groups. Assuming a laboratory <b>can</b> execute k tests at a time, the strategy will form a k \u2a09 n pooling matrix that defines these groups. Once ...", "dateLastCrawled": "2022-01-26T00:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Bayesian approach for neural networks--review and case studies</b> ...", "url": "https://www.academia.edu/2842466/Bayesian_approach_for_neural_networks_review_and_case_studies", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2842466/<b>Bayesian_approach_for_neural_networks_review</b>_and_case...", "snippet": "The <b>Bayesian</b> approach permits propagation of uncertainty in quantities which are unknown to other assumptions in the model, which may be more generally valid or easier to guess in the problem. The case problem studied in this paper include a regression, a classi\u00aecation, and an inverse problem.", "dateLastCrawled": "2022-01-16T22:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Why hasn&#39;t the <b>Bayesian network been as successful</b> as the Deep <b>Neural</b> ...", "url": "https://www.quora.com/Why-hasnt-the-Bayesian-network-been-as-successful-as-the-Deep-Neural-Network", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-hasnt-the-<b>Bayesian-network-been-as-successful</b>-as-the-Deep...", "snippet": "Answer (1 of 5): Well, the definition of \u201csuccessful\u201d is important, here. <b>Bayesian</b> networks are arguably more successful than deep <b>neural</b> networks to date. They are widely applied across industries and <b>can</b> readily be used for inference, modelling, and prediction. Presumably your question is real...", "dateLastCrawled": "2022-01-15T17:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>An overview of artificial neural network</b>", "url": "http://reports.ias.ac.in/report/21405/an-overview-of-artificial-neural-network", "isFamilyFriendly": true, "displayUrl": "reports.ias.ac.in/report/21405/<b>an-overview-of-artificial-neural-network</b>", "snippet": "In future, A detailed study on RBFN and <b>Bayesian</b> <b>neural</b> <b>network</b> with their implementation is intended. I will also try to learn the mathematical aspects of enhancing the <b>neural</b> <b>network</b> capability. In all, this internship proved fruitful to me in achieving good technical and coding skills and having an insight to the world of Artificial Intelligence. INTRODUCTION. Artificial Intelligence has been a popular topic in science fiction and news articles for at least a few decades, and is the ...", "dateLastCrawled": "2021-12-30T17:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How does one interpret the distribution over parameters in <b>bayesian</b> ...", "url": "https://stats.stackexchange.com/questions/115009/how-does-one-interpret-the-distribution-over-parameters-in-bayesian-estimation", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/115009/how-does-one-interpret-the...", "snippet": "Show activity on this post. I am new to <b>Bayesian</b> estimation. The assumption that the parameters are random variables seems a little unsettling to me. For example when considering a model for data, what physical interpretation <b>can</b> I provide to the equation. P ( D a t a) = \u2211 \u03b8 P ( D a t a, \u03b8) = \u2211 \u03b8 P ( D a t a | \u03b8) \u2217 P ( \u03b8)", "dateLastCrawled": "2022-01-27T20:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "deep learning - What are some well-known problems where <b>neural</b> networks ...", "url": "https://ai.stackexchange.com/questions/18576/what-are-some-well-known-problems-where-neural-networks-dont-do-very-well", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/18576/what-are-some-well-known-problems-where...", "snippet": "To conclude, <b>neural</b> networks are just function approximators, i.e. they approximate a specific function (or set of functions, in the case of <b>Bayesian</b> <b>neural</b> networks), given a specific configuration of the parameters. They <b>can</b>&#39;t do more than that. They cannot magically do something that they have not been trained to do, and it is usually the case that you don&#39;t really know the specific function the <b>neural</b> <b>network</b> is representing (hence the expression", "dateLastCrawled": "2022-01-17T07:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Yarin Gal - <b>OATML</b>", "url": "https://oatml.cs.ox.ac.uk/members/yarin/", "isFamilyFriendly": true, "displayUrl": "https://<b>oatml</b>.cs.ox.ac.uk/members/yarin", "snippet": "The most common approach to inference in <b>Bayesian</b> <b>neural</b> networks is to approximate the posterior distribution over the <b>network</b> parameters. However, explicit inference over the <b>network</b> parameters <b>can</b> make it difficult to incorporate meaningful prior information about the prediction task into the inference process. In this paper, we consider an alternative approach. Taking advantage of the fact that <b>Bayesian</b> <b>neural</b> networks define distributions over functions induced by distributions over ...", "dateLastCrawled": "2022-02-03T09:10:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>machine</b> <b>learning</b> approach to <b>Bayesian</b> parameter estimation | npj ...", "url": "https://www.nature.com/articles/s41534-021-00497-w", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41534-021-00497-w", "snippet": "The parameter estimation discussed in this manuscript is divided in two parts: i) a <b>neural</b> <b>network</b> is trained and ii) <b>Bayesian</b> estimation performed on a test set, which we detail below.", "dateLastCrawled": "2022-02-03T06:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "9.7. <b>Bayesian</b> <b>neural</b> networks \u2014 <b>Learning</b> from data", "url": "https://furnstahl.github.io/Physics-8820/notebooks/Machine_learning/Bayesian_neural_networks_tif285.html", "isFamilyFriendly": true, "displayUrl": "https://furnstahl.github.io/.../<b>Machine</b>_<b>learning</b>/<b>Bayesian</b>_<b>neural</b>_<b>networks</b>_tif285.html", "snippet": "<b>Bayesian</b> <b>neural</b> networks differ from plain <b>neural</b> networks in that their weights are assigned a probability distribution instead of a single value or point estimate. These probability distributions describe the uncertainty in weights and can be used to estimate uncertainty in predictions. Training a <b>Bayesian</b> <b>neural</b> <b>network</b> via variational inference learns the parameters of these distributions instead of the weights directly.", "dateLastCrawled": "2021-12-21T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Researchers Explore <b>Bayesian</b> <b>Neural</b> Networks -- Pure AI", "url": "https://pureai.com/articles/2021/09/07/bayesian-neural-networks.aspx", "isFamilyFriendly": true, "displayUrl": "https://pureai.com/articles/2021/09/07/<b>bayesian</b>-<b>neural</b>-<b>networks</b>.aspx", "snippet": "<b>Bayesian</b> <b>neural</b> networks are best explained using an <b>analogy</b> example. Suppose that instead of a <b>neural</b> <b>network</b>, you have a prediction equation y = (8.5 * x1) + (9.5 * x2) + 2.5 where y is the predicted income of an employee, x1 is normalized age, and x2 is years of job tenure. The predicted income of a 30-year old who has been on the job for 4 years would be y = (8.5 * 3.0) + (9.5 * 4.0) + 2.5 = 64.5 = $64,500. If you feed the same (age, tenure) input of (3.0, 4.0) to the prediction equation ...", "dateLastCrawled": "2022-01-30T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "34. <b>Bayesian</b> <b>neural</b> networks \u2014 <b>Learning</b> from data", "url": "https://physics-chalmers.github.io/tif285/doc/LectureNotes/_build/html/content/MachineLearning/BNN/bnn.html", "isFamilyFriendly": true, "displayUrl": "https://physics-chalmers.github.io/.../_build/html/content/<b>MachineLearning</b>/BNN/bnn.html", "snippet": "34. <b>Bayesian</b> <b>neural</b> networks\u00b6. The introduction part of this lecture is inspired by the chapter \u201c<b>Learning</b> as Inference\u201d in the excellent book Information Theory, Inference, and <b>Learning</b> Algorithms by David MacKay.. Some python libraries that are relevant for <b>Bayesian</b> <b>Neural</b> Networks (and part of the general trend towards Probabilistic Programming in <b>Machine</b> <b>Learning</b>) are:", "dateLastCrawled": "2022-01-13T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep <b>neural</b> <b>network</b> models, and it has been used for conducting ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Bayesian Belief Network in Artificial Intelligence</b> - Javatpoint", "url": "https://www.javatpoint.com/bayesian-belief-network-in-artificial-intelligence", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/<b>bayesian-belief-network-in-artificial-intelligence</b>", "snippet": "<b>Bayesian Belief Network in artificial intelligence</b>. <b>Bayesian</b> belief <b>network</b> is key computer technology for dealing with probabilistic events and to solve a problem which has uncertainty. We can define a <b>Bayesian</b> <b>network</b> as: &quot;A <b>Bayesian</b> <b>network</b> is a probabilistic graphical model which represents a set of variables and their conditional ...", "dateLastCrawled": "2022-02-02T21:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Dropout</b> as a <b>Bayesian</b> Approximation: Representing Model <b>Uncertainty</b> in ...", "url": "https://ahmdtaha.medium.com/dropout-as-a-bayesian-approximation-representing-model-uncertainty-in-deep-learning-7a2e49e64a15", "isFamilyFriendly": true, "displayUrl": "https://ahmdtaha.medium.com/<b>dropout</b>-as-a-<b>bayesian</b>-approximation-representing-model...", "snippet": "A regression <b>neural</b> <b>network</b>, with <b>dropout</b> enabled during testing, generates a different output every forward pass for the same input. In the figure below, the same input is passed six times and the <b>network</b> regresses to [5, 4.9, 4.8, 5.3, 5.4, 5]. The paper mathematically shows that these multiple passes are equivalent to Monte-Carlo sampling. Thus, the first and second moment (mean and variance) provides the <b>network</b>\u2019s output and <b>uncertainty</b> respectively. In this example, the <b>network</b> output ...", "dateLastCrawled": "2022-01-31T10:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Neural Networks and Learning Machines</b> - uniba.sk", "url": "https://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf", "isFamilyFriendly": true, "displayUrl": "https://dai.fmph.uniba.sk/courses/NN/haykin.<b>neural</b>-<b>networks</b>.3ed.2009.pdf", "snippet": "<b>Neural Networks and Learning Machines</b> Third Edition Simon Haykin McMaster University Hamilton, Ontario, Canada New York Boston San Francisco London Toronto Sydney Tokyo Singapore Madrid Mexico City Munich Paris Cape Town Hong Kong Montreal. Library of Congress Cataloging-in-Publication Data Haykin, Simon <b>Neural networks and learning machines</b> / Simon Haykin.\u20143rd ed. p. cm. Rev. ed of: <b>Neural</b> networks. 2nd ed., 1999. Includes bibliographical references and index. ISBN-13: 978-0-13-147139-9 ...", "dateLastCrawled": "2022-02-02T00:48:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bayesian neural network)  is like +(group of people guessing number)", "+(bayesian neural network) is similar to +(group of people guessing number)", "+(bayesian neural network) can be thought of as +(group of people guessing number)", "+(bayesian neural network) can be compared to +(group of people guessing number)", "machine learning +(bayesian neural network AND analogy)", "machine learning +(\"bayesian neural network is like\")", "machine learning +(\"bayesian neural network is similar\")", "machine learning +(\"just as bayesian neural network\")", "machine learning +(\"bayesian neural network can be thought of as\")", "machine learning +(\"bayesian neural network can be compared to\")"]}