{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Migration and <b>accumulation</b> of bacteria with chemotaxis and chemokinesis ...", "url": "https://link.springer.com/article/10.1140/epje/s10189-021-00009-w", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1140/epje/s10189-021-00009-w", "snippet": "The <b>Hill</b> parameter n allows us to introduce an inflection point and change the <b>gradient</b> \\(\\partial v/\\partial c\\). As shown in Fig. 2 , the speed increases monotonically with increasing attractant concentrations c for all n and \\(v_c&gt;0\\) , where the half-maximum speed is reached at the attractant concentration \\(k_c\\) .", "dateLastCrawled": "2021-10-10T04:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An <b>overview of gradient descent optimization algorithms</b>", "url": "https://www.slideshare.net/ssuser77b8c6/an-overview-of-gradient-descent-optimization-algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ssuser77b8c6/an-<b>overview-of-gradient-descent-optimization</b>...", "snippet": "Nesterov accelerated <b>gradient</b> \u2022 However, a ball that rolls down a <b>hill</b>, blindly following the slope, is highly unsatisfactory. \u2022 We would <b>like</b> to have a smarter ball that has a notion of where it is going so that it knows to slow down before the <b>hill</b> slopes up again. \u2022 Nesterov accelerated <b>gradient</b> gives us a way of it. 33.", "dateLastCrawled": "2022-01-27T22:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "An <b>overview of gradient descent optimization algorithms</b> \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1609.04747/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1609.04747", "snippet": "An <b>overview of gradient descent optimization algorithms</b> ... However, a ball that rolls down a <b>hill</b>, blindly following the slope, is highly unsatisfactory. We would <b>like</b> to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the <b>hill</b> slopes up again. Nesterov accelerated <b>gradient</b> (NAG) Nesterov is a way to give our momentum term this kind of prescience. We know that we will use our momentum term \u03b3 v t \u2212 1 to move the parameters \u03b8 ...", "dateLastCrawled": "2022-01-26T04:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An overview of <b>gradient</b> descent optimization algorithms", "url": "https://ruder.io/optimizing-gradient-descent/", "isFamilyFriendly": true, "displayUrl": "https://ruder.io/optimizing-<b>gradient</b>-descent", "snippet": "<b>Gradient</b> descent is a way to minimize an objective function \\(J(\\theta)\\) parameterized by a model&#39;s parameters \\(\\theta \\in \\mathbb{R}^d \\) by updating the parameters in the opposite direction of the <b>gradient</b> of the objective function \\(\\nabla_\\theta J(\\theta)\\) w.r.t. to the parameters. The learning rate \\(\\eta\\) determines the size of the steps we take to reach a (local) minimum. In other words, we follow the direction of the slope of the surface created by the objective function downhill ...", "dateLastCrawled": "2022-02-02T21:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Gradient Descent Optimization Techniques</b>. | by Ayush Pradhan ...", "url": "https://medium.com/analytics-vidhya/gradient-descent-optimization-techniques-4316419c5b74", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>gradient-descent-optimization-techniques</b>-4316419c5b74", "snippet": "<b>Gradient</b> descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning\u2026", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Water potential gradient between sapwood and heartwood</b> as a driving ...", "url": "https://link.springer.com/article/10.1007/s00226-019-01081-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00226-019-01081-4", "snippet": "Wetwood, water <b>accumulation</b> in the heartwood of tree trunks, is a defect of forest trees and needs to be improved for wood utilization. To understand the mechanism of wetwood formation, differences in water potential between sapwood and heartwood and their seasonal changes were investigated. The water potential of specimens sampled from tree trunks using an increment corer was measured by psychrometry at four-week intervals for 2 years in two coniferous species. A water potential <b>gradient</b> ...", "dateLastCrawled": "2022-01-06T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Optimizers</b> \u00b7 MingH&#39;s Studio", "url": "http://mhugh.top/2019/04/12/Optimizer/", "isFamilyFriendly": true, "displayUrl": "mhugh.top/2019/04/12/Optimizer", "snippet": "Nesterov accelerated <b>gradient</b>. We\u2019d <b>like</b> to have a smarter $\\theta$ which has a notion where it is going so it will decrease when deviation from the target. $$ v_{d\\theta} = \\beta v_{d(\\theta-1)} + (1-\\beta) \\frac{dJ(\\theta - \\beta v_{d(\\theta-1)})}{\\theta} $$$$ \\theta = \\theta - \\alpha v_{d\\theta} $$ It looks <b>like</b> the $\\theta$ can predict the <b>gradient</b> of next time and tune according to the goal. Here is an image can explain the the process of NAG: While Momentum first computes the current ...", "dateLastCrawled": "2022-01-24T13:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "In AI, <b>how would 1D gradient descent look like? - Quora</b>", "url": "https://www.quora.com/In-AI-how-would-1D-gradient-descent-look-like", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-AI-<b>how-would-1D-gradient-descent-look</b>-<b>like</b>", "snippet": "Answer: In 1D, the <b>gradient</b> descent, or steepest descent (GD-SD), is a tool for finding a minimum (or maximum) point of f(x)=0. The technique applies to an AI problem, or to any other problem, in any scientific area where a 1D minimization (or maximization) is to be done. So if in AI you need a ...", "dateLastCrawled": "2022-01-22T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Photosynthesis in Higher Plants Class</b> 11 Notes Biology ... - CBSE Tuts", "url": "https://www.cbsetuts.com/photosynthesis-higher-plants-cbse-notes-class-11-biology/", "isFamilyFriendly": true, "displayUrl": "https://www.cbsetuts.com/photosynthesis-higher-plants-cbse-notes-class-11-biology", "snippet": "Contents1 <b>Photosynthesis in Higher Plants Class</b> 11 Notes Biology Chapter 131.1 Topic 1 Introduction to Photosynthesis1.2 Chloroplasts: The Site of Photosynthesis1.3 Topic 2 Mechanism of Photosynthesis1.4 Photophosphorylation1.5 ATPase Enzyme <b>Photosynthesis in Higher Plants Class</b> 11 Biology Notes Chapter 1 Pdf free download was designed by expert teachers from the latest edition of NCERT books to get [\u2026]", "dateLastCrawled": "2022-02-02T22:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What <b>gradient</b> allows an alluvial fan to form? - Quora", "url": "https://www.quora.com/What-gradient-allows-an-alluvial-fan-to-form", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>gradient</b>-allows-an-alluvial-fan-to-form", "snippet": "Answer (1 of 2): It is not <b>gradient</b> based as are other fluvial features <b>like</b> meandering. Fans form when a stream carrying significant amounts of solids leaves a confined channel and enters a relatively flat area. They tend to form more under intermittent flows aka flash flooding. Their brothers, ...", "dateLastCrawled": "2022-01-23T17:31:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>What is another word for gradient</b>? | <b>Gradient</b> Synonyms - WordHippo ...", "url": "https://www.wordhippo.com/what-is/another-word-for/gradient.html", "isFamilyFriendly": true, "displayUrl": "https://www.wordhippo.com/<b>what-is/another-word-for/gradient</b>.html", "snippet": "Synonyms for <b>gradient</b> include incline, slope, grade, <b>hill</b>, rise, bank, acclivity, declivity, ramp and cant. Find more <b>similar</b> words at wordhippo.com!", "dateLastCrawled": "2022-02-02T13:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An overview of <b>gradient</b> descent optimization algorithms", "url": "https://ruder.io/optimizing-gradient-descent/", "isFamilyFriendly": true, "displayUrl": "https://ruder.io/optimizing-<b>gradient</b>-descent", "snippet": "<b>Gradient</b> descent is a way to minimize an objective function \\(J(\\theta)\\) parameterized by a model&#39;s parameters \\(\\theta \\in \\mathbb{R}^d \\) by updating the parameters in the opposite direction of the <b>gradient</b> of the objective function \\(\\nabla_\\theta J(\\theta)\\) w.r.t. to the parameters. The learning rate \\(\\eta\\) determines the size of the steps we take to reach a (local) minimum. In other words, we follow the direction of the slope of the surface created by the objective function downhill ...", "dateLastCrawled": "2022-02-02T21:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Estimation of slope length <b>gradient</b> (LS) factor for the sub ...", "url": "https://www.researchgate.net/publication/350372911_Estimation_of_slope_length_gradient_LS_factor_for_the_sub-watershed_areas_of_Juri_River_in_Tripura", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350372911_Estimation_of_slope_length_<b>gradient</b>...", "snippet": "ogy employed can be widely used in <b>similar</b> mountainous <b>to hill</b> y watersheds around the world f or calculation of soil loss. Keywords Length factor \u00b7 Slope factor \u00b7 DEM \u00b7 Slope \u00b7 Flo w <b>accumulation</b>", "dateLastCrawled": "2022-01-14T19:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "An <b>overview of gradient descent optimization algorithms</b>", "url": "https://www.slideshare.net/ssuser77b8c6/an-overview-of-gradient-descent-optimization-algorithms", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ssuser77b8c6/an-<b>overview-of-gradient-descent-optimization</b>...", "snippet": "In the course of this overview, we look at different variants of <b>gradient</b> descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing <b>gradient</b> descent. Saddle : (\u99ac\u306a\u3069\u306e)\u978d(\u304f\u3089),(\u81ea\u8ee2\u8eca\u30fb\u30d0\u30a4\u30af\u306a\u3069\u306e)\u30b5\u30c9\u30eb,\u978d\u4e0b\u8089,(\u5c71\u306e)\u978d\u90e8(\u3042\u3093\u3076) Plateau :\u9ad8\u539f,\u53f0\u5730,(\u30b0\u30e9\u30d5\u306e)\u5e73\u5766\u57df,(\u666f\u6c17\u30fb\u5b66\u7fd2\u306a\u3069\u306e)\u9ad8\u539f\u72b6\u614b,\u30d7\u30e9\u30c8\u30fc", "dateLastCrawled": "2022-01-27T22:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>GitHub</b> - saeidsoheily/<b>gradient</b>-descent-variants-optimization-algorithms ...", "url": "https://github.com/saeidsoheily/gradient-descent-variants-optimization-algorithms", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/saeidsoheily/<b>gradient</b>-descent-variants-optimization-algorithms", "snippet": "The momentum parameter \u03b3 is usually set to 0.9 or a <b>similar</b> value. As a result, we gain faster convergence and reduced oscillation. Nesterov accelerated <b>gradient</b>: Using the momentum method, we push a ball down a <b>hill</b>. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way. However, a ball rolls down a <b>hill</b> ...", "dateLastCrawled": "2021-09-15T22:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Gradient Descent Optimization Techniques</b>. | by Ayush Pradhan ...", "url": "https://medium.com/analytics-vidhya/gradient-descent-optimization-techniques-4316419c5b74", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>gradient-descent-optimization-techniques</b>-4316419c5b74", "snippet": "<b>Gradient</b> descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning\u2026", "dateLastCrawled": "2022-02-03T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Mercury <b>accumulation</b> along a contamination <b>gradient</b> and nondestructive ...", "url": "https://setac.onlinelibrary.wiley.com/doi/full/10.1002/etc.121", "isFamilyFriendly": true, "displayUrl": "https://setac.onlinelibrary.wiley.com/doi/full/10.1002/etc.121", "snippet": "The second objective was to determine whether spatial patterns of Hg <b>accumulation</b> in these amphibian species were <b>similar</b> to other taxa along the South River. The final objective was to develop nondestructive methods for assessing Hg bioaccumulation in amphibians, which will be critical for future studies of amphibians on the South River, as well as in other localities where Hg contamination is a concern. To meet this objective, Hg concentrations in tissues sampled nonlethally were ...", "dateLastCrawled": "2022-01-09T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Water potential gradient between sapwood and heartwood</b> as a driving ...", "url": "https://link.springer.com/article/10.1007/s00226-019-01081-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00226-019-01081-4", "snippet": "Wetwood, water <b>accumulation</b> in the heartwood of tree trunks, is a defect of forest trees and needs to be improved for wood utilization. To understand the mechanism of wetwood formation, differences in water potential between sapwood and heartwood and their seasonal changes were investigated. The water potential of specimens sampled from tree trunks using an increment corer was measured by psychrometry at four-week intervals for 2 years in two coniferous species. A water potential <b>gradient</b> ...", "dateLastCrawled": "2022-01-06T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What <b>gradient</b> allows an alluvial fan to form? - Quora", "url": "https://www.quora.com/What-gradient-allows-an-alluvial-fan-to-form", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-<b>gradient</b>-allows-an-alluvial-fan-to-form", "snippet": "Answer (1 of 2): It is not <b>gradient</b> based as are other fluvial features like meandering. Fans form when a stream carrying significant amounts of solids leaves a confined channel and enters a relatively flat area. They tend to form more under intermittent flows aka flash flooding. Their brothers, ...", "dateLastCrawled": "2022-01-23T17:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>the difference between gradient descent and</b> coordinate ... - Quora", "url": "https://www.quora.com/What-is-the-difference-between-gradient-descent-and-coordinate-descent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-difference-between-gradient-descent-and</b>-coordinate...", "snippet": "Answer (1 of 2): EDIT: I realized I totally read the question wrong, thinking descent vs. ascent, anyways, an updated answer follows. <b>Gradient</b> descent vs coordinate descent are two simple algorithms for local optimization. In particular, <b>gradient</b> descent uses the <b>gradient</b> and coordinate descent ...", "dateLastCrawled": "2022-01-24T19:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Gradient Descent</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/engineering/gradient-descent", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/engineering/<b>gradient-descent</b>", "snippet": "Its effect <b>can</b> <b>be thought</b> of as a ball rolling down a <b>hill</b> in the weights space and picking up pace to roll up the opposite slope and potentially escape a local minimum. Other popular <b>gradient descent</b> methods used in deep learning include the adaptive <b>gradient</b> (Adagrad) [7] , adaptive moment estimation (Adam) [8] , and adaptive learning rate (Adadelta) [9] .", "dateLastCrawled": "2022-01-24T03:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Cryospheric Processes</b> - cchs geography revision", "url": "https://geoteach2017.weebly.com/cryospheric-processes.html", "isFamilyFriendly": true, "displayUrl": "https://geoteach2017.weebly.com/<b>cryospheric-processes</b>.html", "snippet": "They <b>can</b> <b>be thought</b> of as remnants from the last Ice Age which ended about 10,000 years ago, when ice covered nearly 32% of the land and 30% of the oceans. Inputs The most obvious input into the glacial system is snow. The fresh snow that gradually starts to become more compact is called ne\u0301ve\u0301. As the snow become more compact after a year or more, it is called firn. This compaction continues for hundreds or even thousands of years, with more layers being added at the surface, increasing ...", "dateLastCrawled": "2022-01-27T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Localized accumulation and a</b> shelf\u2010basin <b>gradient of particles in the</b> ...", "url": "https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2015JC010794", "isFamilyFriendly": true, "displayUrl": "https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2015JC010794", "snippet": "The <b>accumulation</b> of particles at the density interfaces has been reported in other marine environments [Wurl and Holmes, 2008; Mari et al., 2012; Malpezzi et al., 2013] and one study has reported that the particle concentration was high around the halocline in the Chukchi and Beaufort seas [Bates et al., 2005]. However, our data, collected in a broad area of shelf and slope-basin regions of the Chukchi Sea, first revealed that the <b>accumulation</b> of particles on the pycnocline is a general ...", "dateLastCrawled": "2022-02-03T02:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Visualisation and Quantification of Morphogen <b>Gradient</b> Formation in the ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2675906/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2675906", "snippet": "Members of the nodal family are <b>thought</b> to form a morphogen <b>gradient</b> in the developing zebrafish embryo and to be essential for pattern formation. Mesoderm and endoderm are believed to develop due to high levels of nodal signalling, while cells experiencing the lowest concentrations of nodal signalling become ectoderm. Although this idea is widely accepted, the formation of a nodal morphogen <b>gradient</b> has never been observed directly, and we have therefore used two different approaches to ...", "dateLastCrawled": "2021-11-14T21:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Water potential gradient between sapwood and heartwood</b> as a driving ...", "url": "https://link.springer.com/article/10.1007/s00226-019-01081-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00226-019-01081-4", "snippet": "Wetwood, water <b>accumulation</b> in the heartwood of tree trunks, is a defect of forest trees and needs to be improved for wood utilization. To understand the mechanism of wetwood formation, differences in water potential between sapwood and heartwood and their seasonal changes were investigated. The water potential of specimens sampled from tree trunks using an increment corer was measured by psychrometry at four-week intervals for 2 years in two coniferous species. A water potential <b>gradient</b> ...", "dateLastCrawled": "2022-01-06T16:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "LECTURE 4: Marangoni \ufb02ows", "url": "http://web.mit.edu/1.63/www/Lec-notes/Surfacetension/Lecture4.pdf", "isFamilyFriendly": true, "displayUrl": "<b>web.mit.edu</b>/1.63/www/Lec-notes/Surfacetension/Lecture4.pdf", "snippet": "<b>can</b> sustain normal stress jumps across a \ufb02uid interface, they do not contribute to the tangential stress jump. Consequently, tangential surface stresses <b>can</b> only be balanced by viscous stresses associated with \ufb02uid motion. 4.1 Tears of wine The \ufb01rst Marangoni \ufb02ow considered was the tears of wine phenomenon (Thomson 1885), which actually predates Marangoni\u2019s \ufb01rst published work on the subject by a decade. The tears of wine phenomenon is readily observed in a glass of any but the ...", "dateLastCrawled": "2022-01-28T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Why is the <b>gradient</b> in a neural network layer the multiplication of ...", "url": "https://www.quora.com/Why-is-the-gradient-in-a-neural-network-layer-the-multiplication-of-gradients-in-prior-layers-What-is-a-vanishing-gradient-in-a-simple-explanation", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-the-<b>gradient</b>-in-a-neural-network-layer-the-multiplication...", "snippet": "Answer (1 of 3): A neural network <b>can</b> <b>be thought</b> of a set of nested functions, with each layer consisting of a matrix multiplication followed by some nonlinear function (often the logistic function, hyperbolic tangent, ReLU or softmax). When you add layers to a neural network, you embed these fun...", "dateLastCrawled": "2022-01-22T17:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Hydrodynamic accumulation of small molecules</b> and ions into cell-sized ...", "url": "https://www.nature.com/articles/s42004-020-0277-2", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s42004-020-0277-2", "snippet": "Interpreting all the results together, currently we <b>can</b> postulate that the mechanism of the hydrodynamic <b>accumulation</b> of substances across the liposomal membrane against the concentration <b>gradient</b> ...", "dateLastCrawled": "2021-10-13T09:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Changes in Submarine Channel Morphology and Slope Sedimentation ...", "url": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118311172.ch3", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118311172.ch3", "snippet": "The retrogradational stacking is <b>thought</b> to be a response to a feedback process whereby each wedge, deposited at the critical <b>gradient</b> for ignition, reduces the <b>gradient</b> for the next flow. The non-channelled portion of the delta slope adjacent to the distributary channel is characterized by sediment waves which repeat surveys show to be migrating up-slope. It is suggested that this is evidence for unconfined turbidity currents.", "dateLastCrawled": "2022-01-07T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Temperate grassland songbird species accumulate incrementally along a ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0186809", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0186809", "snippet": "Detritus <b>accumulation</b> <b>can</b> be promoted by reducing the frequency of disturbances such as prescribed fire and livestock grazing. This would be especially true in areas with low to moderate water availability or areas where climate change in increasing the frequency or severity of drought. Tall grass prairie studies have found that the <b>accumulation</b> of songbirds peaks at 3 to 4 years following disturbances such as fire or other disturbance", "dateLastCrawled": "2021-12-06T23:23:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>updated overview of recent gradient descent algorithms</b> \u2014 Anastasios ...", "url": "https://akyrillidis.github.io/2020/03/05/algo.html", "isFamilyFriendly": true, "displayUrl": "https://akyrillidis.github.io/2020/03/05/algo.html", "snippet": "These are the two common explanations for the need for a <b>gradient</b> <b>accumulation</b> or averaging mechanism, which leads us to momentum. Momentum is an exponential moving average of past gradients parameterized by \\(\\beta\\) (commonly equal to 0.9), and is given by the below algorithm: \\[\\theta_{t+1} = \\theta_t + \\eta v_{t}, \\quad v_{t} = \\beta v_{t-1} - g_t.\\] This remains one of the two most popular optimizers for training deep neural networks 1, and there is a long list of state-of-the-art ...", "dateLastCrawled": "2022-02-03T05:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An overview of <b>gradient</b> descent optimization algorithms", "url": "https://ruder.io/optimizing-gradient-descent/", "isFamilyFriendly": true, "displayUrl": "https://ruder.io/optimizing-<b>gradient</b>-descent", "snippet": "If you are unfamiliar with <b>gradient</b> descent, you <b>can</b> find a good introduction on optimizing neural networks here. <b>Gradient</b> descent variants. There are three variants of <b>gradient</b> descent, which differ in how much data we use to compute the <b>gradient</b> of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update. Batch <b>gradient</b> descent. Vanilla <b>gradient</b> descent, aka batch <b>gradient</b> descent ...", "dateLastCrawled": "2022-02-02T21:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Optimizers | Darel\u2019s Site", "url": "https://darel13712.github.io/ml/optimizers.html", "isFamilyFriendly": true, "displayUrl": "https://darel13712.github.io/ml/optimizers.html", "snippet": "More stable convergence <b>compared</b> to SGD; Visualization Convergence. Saddle point. Momentums Momentum. What if we immitate a ball rolling down the <b>hill</b>? is usually 0.9. Reduces oscillation and converges faster; Nesterov Accelerated <b>Gradient</b> (NAG) <b>Can</b> we get a smarter ball that knows when it\u2019s time to slow down? Let\u2019s make a virtual step in the direction of the momentum first and calculate the value of the <b>gradient</b> from that point instead of . Adaptive Optimizers Adagrad. Shouldn\u2019t ...", "dateLastCrawled": "2022-01-17T01:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Fluid Management in Patients with Chronic Heart Failure", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5490880/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5490880", "snippet": "Chronic fluid <b>accumulation</b> is responsible for a substantial number of hospital admissions, and identifies patients with a worse prognosis than those admitted due to a sudden increase in LV filling pressures. Peripheral congestion in patients with heart failure usually develops over weeks or even months, and patients may present \u2018acutely\u2019 having gained over 20 litres of excess fluid, and hence over 20 kg of excess weight. The aim of management is to remove the excess fluid, so that the ...", "dateLastCrawled": "2022-02-03T01:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Mercury <b>accumulation</b> along a contamination <b>gradient</b> and nondestructive ...", "url": "https://setac.onlinelibrary.wiley.com/doi/full/10.1002/etc.121", "isFamilyFriendly": true, "displayUrl": "https://setac.onlinelibrary.wiley.com/doi/full/10.1002/etc.121", "snippet": "Mercury <b>accumulation</b> in amphibians followed the same spatial pattern as observed in birds, fish, and turtles along the South River, which generally increased for several miles downstream from the point source before peaking between miles 10 and 20 and decreasing or remaining high until the confluence with the South Fork of the Shenandoah River (20, 22; G.W. Murphy, 2004, Master&#39;s thesis). Although it is not currently known why Hg concentrations peak downstream from the source, it is ...", "dateLastCrawled": "2022-01-09T21:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Localized accumulation and a</b> shelf\u2010basin <b>gradient of particles in the</b> ...", "url": "https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2015JC010794", "isFamilyFriendly": true, "displayUrl": "https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2015JC010794", "snippet": "The particle-size distribution analysis indicated that relatively small particles were dominant in the shelf region <b>compared</b> to the slope-basin region. These results suggest that particles containing large amounts of TEP are produced in the shelf region and are potentially delivered to the slope-basin region along the pycnocline, which might support productivity and material cycles in the nutrient-depleted basin region of the western Arctic Ocean. 1 Introduction. Particles in seawater have ...", "dateLastCrawled": "2022-02-03T02:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Neural Networks - Milania&#39;s Blog", "url": "https://www.milania.de/blog/category/Neural_Networks", "isFamilyFriendly": true, "displayUrl": "https://www.milania.de/blog/category/Neural_Networks", "snippet": "Instead of moving down the steepest <b>hill</b> and then walking along the horizontal direction of the valley, the adaptive route points more directly towards the optimum. In this case, this results in a shorter path. This is an effect of the general principle that the adaptive scheme tends to focus more on the direction than on the magnitude of the <b>gradient</b>. In momentum optimization, on the other side, we mainly focused on improving the magnitude. We even accepted small divergences from the ...", "dateLastCrawled": "2022-01-30T20:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What is <b>the difference between gradient descent and</b> coordinate ... - Quora", "url": "https://www.quora.com/What-is-the-difference-between-gradient-descent-and-coordinate-descent", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-difference-between-gradient-descent-and</b>-coordinate...", "snippet": "Answer (1 of 2): EDIT: I realized I totally read the question wrong, thinking descent vs. ascent, anyways, an updated answer follows. <b>Gradient</b> descent vs coordinate descent are two simple algorithms for local optimization. In particular, <b>gradient</b> descent uses the <b>gradient</b> and coordinate descent ...", "dateLastCrawled": "2022-01-24T19:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "In AI, <b>how would 1D gradient descent look like? - Quora</b>", "url": "https://www.quora.com/In-AI-how-would-1D-gradient-descent-look-like", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-AI-<b>how-would-1D-gradient-descent-look</b>-like", "snippet": "Answer: In 1D, the <b>gradient</b> descent, or steepest descent (GD-SD), is a tool for finding a minimum (or maximum) point of f(x)=0. The technique applies to an AI problem, or to any other problem, in any scientific area where a 1D minimization (or maximization) is to be done. So if in AI you need a ...", "dateLastCrawled": "2022-01-22T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>Can</b> a <b>Slope</b> Affect the Soil in an Area? | Home Guides | <b>SF Gate</b>", "url": "https://homeguides.sfgate.com/can-slope-affect-soil-area-39386.html", "isFamilyFriendly": true, "displayUrl": "https://<b>homeguides.sfgate.com</b>/<b>can</b>-<b>slope</b>-affect-soil-area-39386.html", "snippet": "Angle. In general, a <b>slope</b> with a steep angle will have a greater impact on the soil composition <b>compared</b> to a gentle slant. As rain falls, the topsoil&#39;s nutrient-rich organic matter will move ...", "dateLastCrawled": "2022-01-26T05:37:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The Hitchhiker\u2019s Guide to Optimization in <b>Machine Learning</b> | by Aman ...", "url": "https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-machine-learning-edcf5a104210", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-<b>machine</b>...", "snippet": "<b>Gradient</b> descent is one of the easiest to implement (and arguably one of the worst) optimization algorithms in <b>machine learning</b>. It is a first-order (i.e., <b>gradient</b>-based) optimization algorithm where we iteratively update the parameters of a differentiable cost function until its minimum is attained. Before we understand how <b>gradient</b> descent ...", "dateLastCrawled": "2022-02-02T19:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Deeper Look into <b>Gradient</b> Based <b>Learning</b> for Neural Networks | by ...", "url": "https://towardsdatascience.com/a-deeper-look-into-gradient-based-learning-for-neural-networks-ad7a35b17b93", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-deeper-look-into-<b>gradient</b>-based-<b>learning</b>-for-neural...", "snippet": "In practice, larger \u03b7 also causes overshooting in <b>machine</b> <b>learning</b> and is termed as the <b>learning</b> rate and is a hyper parameter. Limitations. One of the limitations of Vanilla <b>Gradient</b> Descent is that in the region of gentle slopes, computed gradients are very small resulting in a very slow convergence. One may simply increase the value of \u03b7 ...", "dateLastCrawled": "2022-02-01T13:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep Learning</b> <b>Optimizers-Hard?Not.[2</b>] | Analytics Vidhya", "url": "https://medium.com/analytics-vidhya/neural-network-optimizers-hard-not-2-7ecc677892cc", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/neural-network-<b>optimizers-hard-not-2</b>-7ecc677892cc", "snippet": "<b>Machine</b> <b>Learning</b>; Hackathon ; Contribute; Free Courses ... the negative <b>gradient</b> is the force <b>analogy</b>. It pushes the parameters through the parameter space with the <b>accumulation</b> accelerating ...", "dateLastCrawled": "2021-01-11T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>ML Optimization - Advanced Optimizers from scratch with</b> Python", "url": "https://rubikscode.net/2020/11/02/ml-optimization-advanced-optimizers-from-scratch-with-python/", "isFamilyFriendly": true, "displayUrl": "https://rubikscode.net/2020/11/02/<b>ml-optimization-advanced-optimizers-from-scratch</b>...", "snippet": "We also talked about how to quantify <b>machine</b> <b>learning</b> model performance and how to improve it with ... this algorithm restricts the <b>accumulation</b> of gradients by using a decay hyperparameter. So instead of adding complete square <b>gradient</b> to the s vector every iteration, it does it like this: where betta is the decay hyperparameter. Hinton proposed a value of 0.9 for \u03b2 and 0.001 for the <b>learning</b> rate. The parameter update is done in the same way as for AdaGrad: Python Implementation. As you ...", "dateLastCrawled": "2022-01-31T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Dynamical <b>machine</b> <b>learning</b> volumetric reconstruction of objects ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8027224/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8027224", "snippet": "Later, a <b>machine</b> <b>learning</b> approach with a Convolutional Neural Network (CNN) replacing the iterative <b>gradient</b> descent algorithm exhibited even better robustness to strong scattering for layered objects, which match well with the BPM assumptions 45. Despite great progress reported by these prior works, the problem of reconstruction through multiple scattering remains difficult due to the extreme ill-posedness and uncertainty in the forward operator; residual distortion and artifacts are not ...", "dateLastCrawled": "2022-01-08T19:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Dynamical machine learning volumetric reconstruction of</b> objects ...", "url": "https://www.nature.com/articles/s41377-021-00512-x", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41377-021-00512-x", "snippet": "The <b>analogy</b> of the BPM computational structure with a neural network was exploited, in conjunction with <b>gradient</b> descent optimization, to obtain the 3D refractive index as the \u201cweights\u201d of the ...", "dateLastCrawled": "2022-02-02T23:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective function to perform maximum or minimum evaluation. In reality, optimization is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Solving Word <b>Analogies: A Machine Learning Perspective</b> | Request PDF", "url": "https://www.researchgate.net/publication/335597029_Solving_Word_Analogies_A_Machine_Learning_Perspective", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335597029_Solving_Word_Analogies_A_<b>Machine</b>...", "snippet": "From a <b>machine</b> <b>learning</b> perspective, this provides guidelines to build training sets of positive and negative examples. We then suggest improved methods to classify word-analogies and also to ...", "dateLastCrawled": "2021-10-16T21:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Spectrofluorometric analysis combined with <b>machine</b> <b>learning</b> for ...", "url": "https://www.sciencedirect.com/science/article/pii/S0308814621011559", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0308814621011559", "snippet": "Data from the confusion matrix presented in Table 2 reiterates the 100% accuracy, with the maximum result achieved for all parameters for each varietal class with multi-block analysis. In comparison, analysis using only EEM data with XGBDA afforded somewhat lower accuracy (96.1% correct classification) and inferior model parameters, especially when sample numbers were low (i.e., Merlot and Shiraz/Cabernet Sauvignon, Table S5 of the Supplementary material).Fluorescence spectroscopy has been ...", "dateLastCrawled": "2021-12-26T01:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Automatic Differentiation Using</b> <b>Gradient</b> Tapes - Gowri Shankar", "url": "https://gowrishankar.info/blog/automatic-differentiation-using-gradient-tapes/", "isFamilyFriendly": true, "displayUrl": "https://gowrishankar.info/blog/<b>automatic-differentiation-using</b>-<b>gradient</b>-tapes", "snippet": "<b>Automatic Differentiation Using</b> <b>Gradient</b> Tapes. Posted December 14, 2020 by Gowri Shankar &amp;dash; 9 min read As a Data Scientist or Deep <b>Learning</b> Researcher, one must have a deeper knowledge in various differentiation techniques due to the fact that <b>gradient</b> based optimization techniques like Backpropagation algorithms are critical for model efficiency and convergence.", "dateLastCrawled": "2022-01-28T07:43:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Memory-Efficient Pipeline-Parallel DNN Training</b>", "url": "https://www.researchgate.net/publication/342258674_Memory-Efficient_Pipeline-Parallel_DNN_Training", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/342258674_Memory-Efficient_Pipeline-Parallel...", "snippet": "<b>gradient accumulation is similar</b>, with the minibatch size mul- tiplied by an appropriate scale factor (number of replicas, or degree of gradient accumulation), similar to data parallelism.", "dateLastCrawled": "2022-02-02T12:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Memory-Efficient Pipeline-Parallel DNN Training \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2006.09503/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2006.09503", "snippet": "Many state-of-the-art results in domains such as NLP and computer vision have been obtained by scaling up the number of parameters in existing models. However, the weight parameters and intermediate outputs of these large models often do not fit in the main memory of a single accelerator device; this means that it is necessary to use multiple accelerators to train large models, which is challenging to do in a time-efficient way. In this work, we propose PipeDream-2BW, a system that performs ...", "dateLastCrawled": "2021-09-14T05:28:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(gradient accumulation)  is like +(hill)", "+(gradient accumulation) is similar to +(hill)", "+(gradient accumulation) can be thought of as +(hill)", "+(gradient accumulation) can be compared to +(hill)", "machine learning +(gradient accumulation AND analogy)", "machine learning +(\"gradient accumulation is like\")", "machine learning +(\"gradient accumulation is similar\")", "machine learning +(\"just as gradient accumulation\")", "machine learning +(\"gradient accumulation can be thought of as\")", "machine learning +(\"gradient accumulation can be compared to\")"]}