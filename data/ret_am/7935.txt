{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretability vs Explainability: The Black</b> Box of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-<b>interpretability</b>-vs-explainability", "snippet": "It is an extra step in the building process\u2014<b>like</b> wearing a seat belt while driving a <b>car</b>. It is unnecessary for the <b>car</b> to perform, but offers insurance when things crash. The benefit a deep neural net offers to engineers is it creates a black box of parameters, <b>like</b> fake additional data points, that allow a model to base its decisions against. These fake data points go unknown to the engineer. The black box, or hidden layers, allow a model to make associations among the given data points ...", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interpretability</b> vs Explainability: The Black Box of Machine Learning", "url": "https://blogs.bmc.com/machine-learning-interpretability-vs-explainability/?print=pdf", "isFamilyFriendly": true, "displayUrl": "https://blogs.bmc.com/machine-learning-<b>interpretability</b>-vs-explainability/?print=pdf", "snippet": "process\u2014<b>like</b> wearing a seat belt while driving a <b>car</b>. It is unnecessary for the <b>car</b> to perform, but offers insurance when things crash. The benefit a deep neural net offers to engineers is it creates a black box of parameters, <b>like</b> fake additional data points, that allow a model to base its decisions against. These fake data points go unknown to the engineer. The black box, or hidden layers, allow a model to make associations among the given data points to predict better results. For ...", "dateLastCrawled": "2022-01-19T06:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Can you explain this? The story of machine learning <b>interpretability</b> ...", "url": "https://medium.com/codex/can-you-explain-this-221130c6e264", "isFamilyFriendly": true, "displayUrl": "https://medium.com/codex/can-you-explain-this-221130c6e264", "snippet": "Why did the detector think that seawater looks <b>like</b> a <b>car</b>? The visualizations offer an explanation. Below is the output from visualization on the features for false <b>car</b> detection. This ...", "dateLastCrawled": "2021-06-21T21:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Explainability and Auditability in ML: Definitions, Techniques, and ...", "url": "https://neptune.ai/blog/explainability-auditability-ml-definitions-techniques-tools", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/explainability-auditability-ml-definitions-techniques-tools", "snippet": "A <b>car</b> needs fuel to move, i.e it is the fuel that causes the engines to move \u2013 <b>interpretability</b>. <b>Understanding</b> how and why the engine consumes and uses the fuel \u2013 explainability. Most tools and techniques mentioned in this article can be used for both Explainability and <b>Interpretability</b> because <b>like</b> I mentioned earlier both concepts give a perspective on <b>understanding</b> what the model is about. Explainable AI is about <b>understanding</b> ML models better. How they make decisions, and why. The ...", "dateLastCrawled": "2022-02-03T19:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interpretability</b> and explainability | by Christian K\u00e4stner | Medium", "url": "https://ckaestne.medium.com/interpretability-and-explainability-a80131467856", "isFamilyFriendly": true, "displayUrl": "https://ckaestne.medium.com/<b>interpretability</b>-and-explainability-a80131467856", "snippet": "Without <b>understanding</b> how a model <b>works</b> and why a model makes specific predictions, it can be difficult to trust a model, to audit it, ... (e.g., drive a different <b>car</b>, install an alarm system), increase the chance for a loan when using an automated credit scoring model (e.g., have a longer credit history, pay down a larger percentage), or improve grades from an automated grading system (e.g., avoid certain kinds of mistakes). One can also use insights from machine-learned model to aim to ...", "dateLastCrawled": "2022-01-31T21:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Interpretable Machine Learning</b> - Data Revenue", "url": "https://www.datarevenue.com/en-blog/interpretable-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.datarevenue.com/en-blog/<b>interpretable-machine-learning</b>", "snippet": "global <b>interpretability</b> is <b>understanding</b> how the complete model <b>works</b>; local <b>interpretability</b> is <b>understanding</b> how a single decision was reached. A model is globally interpretable if it\u2019s small and simple enough for a human to understand it entirely. A model is locally interpretable if a human can trace back a single decision and understand how the model reached that decision. A model is globally interpretable if we understand each and every rule it factors in. For example, a simple model ...", "dateLastCrawled": "2022-02-03T15:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Interpretability</b> in Machine Learning | by Conor O&#39;Sullivan | Towards ...", "url": "https://towardsdatascience.com/interpretability-in-machine-learning-ab0cf2e66e1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpretability</b>-in-machine-learning-ab0cf2e66e1", "snippet": "We would need an <b>understanding</b> of the sigmoid function and how coefficients are related to odds/probability. This complexity may also lead to errors in our interpretations. In general, the more interpretable a model; the easier it is to understand and the more certain we can be that our <b>understanding</b> is correct. <b>Interpretability</b> is important because of the many benefits that flow from this. Easier to explain. Our first benefit is that interpretable models are easier to explain to other ...", "dateLastCrawled": "2022-01-28T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Interpretability</b> of machine learning models - explaining the black-box", "url": "https://ayc-data.com/data_science/2021/09/09/machine-learning-interpretability-on-top.html", "isFamilyFriendly": true, "displayUrl": "https://ayc-data.com/data_science/2021/09/09/machine-learning-<b>interpretability</b>-on-top.html", "snippet": "<b>Interpretability</b> of machine learning models - explaining the black-box The &#39;why&#39; is just as important as the &#39;what&#39; Posted by Albert Cheng on 09 September 2021. Last Updated: 12 January 2022 . The world we are in is very automated. In the world we are in, automation is becoming increasingly common. It is impossible to go about your day-to-day life without some sort of automated decision-making occurring. Some are minor (e.g. what YouTube video is recommended), but others may be life-changing ...", "dateLastCrawled": "2022-01-13T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Improving Interpretability of Deep Neural Networks</b> With Semantic ...", "url": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dong_Improving_Interpretability_of_CVPR_2017_paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dong_Improving_<b>Interpretability</b>...", "snippet": "<b>Improving Interpretability of Deep Neural Networks with Semantic Information</b> ... related words <b>like</b> road, street, and drive can interpret the learned features of the blue neuron). With the aids of these interpretable features, human users can easily visualize and interact with the system, which allows a human-in-the-loop learning procedure. age analysis [15], \ufb01nancial investment [1], etc. The high-performance of DNNs highly lies on the fact that they often stack tens of or even hundreds of ...", "dateLastCrawled": "2022-01-30T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Deep Learning <b>and the Demand for Interpretability</b> \u2013 Syed Ashar Javed ...", "url": "http://stillbreeze.github.io/Deep-Learning-and-the-Demand-For-Interpretability/", "isFamilyFriendly": true, "displayUrl": "stillbreeze.github.io/Deep-Learning-<b>and-the-Demand-For-Interpretability</b>", "snippet": "Transparency, he defines as \u201copposite of blackbox-ness\u201d and \u201csome sense of <b>understanding</b> the mechanism by which the model <b>works</b>\u201d, which seem <b>like</b> a very broad definition and highlights the difficulty in defining it. Post-hoc <b>interpretability</b>, on the other hand, is simply the extraction and analysis of information from models after they have been learned. Clearly, the first one is the more interesting characteristic here, but also the one more difficult to achieve. He also argues in ...", "dateLastCrawled": "2022-01-04T01:43:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Explainability and Auditability in ML: Definitions, Techniques, and ...", "url": "https://neptune.ai/blog/explainability-auditability-ml-definitions-techniques-tools", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/explainability-auditability-ml-definitions-techniques-tools", "snippet": "A <b>car</b> needs fuel to move, i.e it is the fuel that causes the engines to move \u2013 <b>interpretability</b>. <b>Understanding</b> how and why the engine consumes and uses the fuel \u2013 explainability. Most tools and techniques mentioned in this article can be used for both Explainability and <b>Interpretability</b> because like I mentioned earlier both concepts give a perspective on <b>understanding</b> what the model is about. Explainable AI is about <b>understanding</b> ML models better. How they make decisions, and why. The ...", "dateLastCrawled": "2022-02-03T19:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interpretable Machine Learning</b> - Data Revenue", "url": "https://www.datarevenue.com/en-blog/interpretable-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.datarevenue.com/en-blog/<b>interpretable-machine-learning</b>", "snippet": "global <b>interpretability</b> is <b>understanding</b> how the complete model <b>works</b>; local <b>interpretability</b> is <b>understanding</b> how a single decision was reached. A model is globally interpretable if it\u2019s small and simple enough for a human to understand it entirely. A model is locally interpretable if a human can trace back a single decision and understand how the model reached that decision. A model is globally interpretable if we understand each and every rule it factors in. For example, a simple model ...", "dateLastCrawled": "2022-02-03T15:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interpretability vs Explainability: The Black</b> Box of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-<b>interpretability</b>-vs-explainability", "snippet": "With this <b>understanding</b>, we can define explainability as: ... of a dog, then the output should be \u201cdog\u201d. How this happens can be completely unknown, and, as long as the model <b>works</b> (high <b>interpretability</b>), there is often no question as to how. In image detection algorithms, usually Convolutional Neural Networks, their first layers will contain references to shading and edge detection. The human never had to explicitly define an edge or a shadow, but because both are common among every ...", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Interpretability</b> and explainability | by Christian K\u00e4stner | Medium", "url": "https://ckaestne.medium.com/interpretability-and-explainability-a80131467856", "isFamilyFriendly": true, "displayUrl": "https://ckaestne.medium.com/<b>interpretability</b>-and-explainability-a80131467856", "snippet": "Without <b>understanding</b> how a model <b>works</b> and why a model makes specific predictions, it can be difficult to trust a model, to audit it, ... <b>Interpretability</b>: We consider a model intrinsically interpretable, if a human can understand the internal workings of the model, either the entire model at once or at least the parts of the model relevant for a given prediction. This may include <b>understanding</b> decision rules and cutoffs and the ability to manually derive the outputs of the model. For ...", "dateLastCrawled": "2022-01-31T21:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interpretability</b> in Machine Learning | by Conor O&#39;Sullivan | Towards ...", "url": "https://towardsdatascience.com/interpretability-in-machine-learning-ab0cf2e66e1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpretability</b>-in-machine-learning-ab0cf2e66e1", "snippet": "A model that provided <b>similar</b> explanations would be more useful than one that just provided predictions. Source: ... We would need an <b>understanding</b> of the sigmoid function and how coefficients are related to odds/probability. This complexity may also lead to errors in our interpretations. In general, the more interpretable a model; the easier it is to understand and the more certain we can be that our <b>understanding</b> is correct. <b>Interpretability</b> is important because of the many benefits that ...", "dateLastCrawled": "2022-01-28T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Towards Interpretability of Segmentation Networks by Analyzing DeepDreams</b>", "url": "https://perso.telecom-paristech.fr/bloch/papers/proceedings/MICCAI-iMIMIC2019-Vincent.pdf", "isFamilyFriendly": true, "displayUrl": "https://perso.telecom-paristech.fr/bloch/papers/proceedings/MICCAI-iMIMIC2019-Vincent.pdf", "snippet": "deep learning algorithms perform critical tasks such as driving a <b>car</b> or assist-ing a physician in establishing a diagnosis. In this work we are interested in interpreting segmentation networks by appraising their sensitivity to high-level features. Indeed, segmenting anatomical structures in medical images is one of the tasks that hugely bene\ufb01ted from Convolutional Neural Networks (CNNs), to the point that this framework is now state-of-the-art in most segmentation tasks [5,6,8]. Research ...", "dateLastCrawled": "2021-10-29T04:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep Learning <b>and the Demand for Interpretability</b> \u2013 Syed Ashar Javed ...", "url": "http://stillbreeze.github.io/Deep-Learning-and-the-Demand-For-Interpretability/", "isFamilyFriendly": true, "displayUrl": "stillbreeze.github.io/Deep-Learning-<b>and-the-Demand-For-Interpretability</b>", "snippet": "Transparency, he defines as \u201copposite of blackbox-ness\u201d and \u201csome sense of <b>understanding</b> the mechanism by which the model <b>works</b>\u201d, which seem like a very broad definition and highlights the difficulty in defining it. Post-hoc <b>interpretability</b>, on the other hand, is simply the extraction and analysis of information from models after they have been learned. Clearly, the first one is the more interesting characteristic here, but also the one more difficult to achieve. He also argues in ...", "dateLastCrawled": "2022-01-04T01:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "The Building Blocks of <b>Interpretability</b> - Distill", "url": "https://distill.pub/2018/building-blocks/", "isFamilyFriendly": true, "displayUrl": "https://distill.pub/2018/building-blocks", "snippet": "Moreover, this grammar allows us to systematically explore the space of <b>interpretability</b> interfaces, enabling us to evaluate whether they meet particular goals. We will present interfaces that show what the network detects and explain how it develops its <b>understanding</b>, while keeping the amount of information human-scale. For example, we will ...", "dateLastCrawled": "2022-02-02T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Interpretability</b> 2020", "url": "https://ff06-2020.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://ff06-2020.fastforwardlabs.com", "snippet": "The table, as a sort of global view of local <b>interpretability</b>, highlights how <b>interpretability</b> depends on collaboration. The intuitive feel a user builds up from scrolling through the highlighted features depends on our ability to recognize patterns and develop models in our heads that explain those patterns, a process that mirrors some of the work the computer model is doing. In a loose sense, you can imagine that the highlighted features give you a glimpse of how the model sees the data ...", "dateLastCrawled": "2022-01-16T21:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Confusion Matrices</b> &amp; <b>Interpretable ML</b> | by andrea b | high stakes ...", "url": "https://medium.com/high-stakes-design/interpretability-techniques-explained-in-simple-terms-f5e1573674f3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/high-stakes-design/<b>interpretability</b>-techniques-explained-in-simple...", "snippet": "<b>Interpretability</b> requires some sort of manipulation of the actual model to test if your explanation is valid or not. But with explainability you don\u2019t [have to] do that. For example, a lot of ...", "dateLastCrawled": "2021-03-22T05:36:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretability vs Explainability: The Black</b> Box of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-<b>interpretability</b>-vs-explainability", "snippet": "<b>Interpretability</b> has to do with how accurate a machine learning model <b>can</b> associate a cause to an effect. Explainability has to do with the ability of the parameters, often hidden in Deep Nets, to justify the results. This is a long article. Hang in there and, by the end, you will understand: How <b>interpretability</b> is different from explainability.", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interpretability</b> in Machine Learning | by Conor O&#39;Sullivan | Towards ...", "url": "https://towardsdatascience.com/interpretability-in-machine-learning-ab0cf2e66e1", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpretability</b>-in-machine-learning-ab0cf2e66e1", "snippet": "In general, the more interpretable a model; the easier it is to understand and the more certain we <b>can</b> be that our <b>understanding</b> is correct. <b>Interpretability</b> is important because of the many benefits that flow from this. Easier to explain. Our first benefit is that interpretable models are easier to explain to other people. For any topic, the better we understand it the easier it is to explain. We should also be able to explain it in simple terms (i.e. without mentioning the technical ...", "dateLastCrawled": "2022-01-28T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interpretable Machine Learning</b> Models | HCL Blogs", "url": "https://www.hcltech.com/blogs/interpretable-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.hcltech.com/blogs/<b>interpretable-machine-learning</b>", "snippet": "Here is a <b>thought</b> on <b>interpretability</b> and decision making using machine learning. Click to read @hclfs #machinelearning #machineinterpretability. So, what is <b>interpretability</b>? Simply stated, it is having the ability to confidently tell how/why a machine learning model is making the decisions it was programmed to make. Why it is important? We <b>can</b> predict that some customers may no longer want to be associated with a brand. We would therefore want to know why such a prediction was made, as ...", "dateLastCrawled": "2022-01-12T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The Building Blocks of <b>Interpretability</b> - Distill", "url": "https://distill.pub/2018/building-blocks/", "isFamilyFriendly": true, "displayUrl": "https://distill.pub/2018/building-blocks", "snippet": "The Space of <b>Interpretability</b> Interfaces. The interface ideas presented in this article combine building blocks such as feature visualization and attribution. Composing these pieces is not an arbitrary process, but rather follows a structure based on the goals of the interface.", "dateLastCrawled": "2022-02-02T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>interpretability</b> - Chandan Singh | chandan singh", "url": "https://csinva.io/notes/research_ovws/ovw_interp.html", "isFamilyFriendly": true, "displayUrl": "https://csinva.io/notes/research_ovws/ovw_interp.html", "snippet": "Evaluating <b>interpretability</b> <b>can</b> be very difficult (largely because it rarely makes sense to talk about <b>interpretability</b> outside of a specific context). The best possible evaluation of <b>interpretability</b> requires benchmarking it with respect to the relevant audience in a context. For example, if an interpretation claims to help understand radiology models, it should be tested based on how well it helps radiologists when actually making diagnoses. The papers here try to find more generic ...", "dateLastCrawled": "2021-11-06T01:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Confusion Matrices</b> &amp; <b>Interpretable ML</b> | by andrea b | high stakes ...", "url": "https://medium.com/high-stakes-design/interpretability-techniques-explained-in-simple-terms-f5e1573674f3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/high-stakes-design/<b>interpretability</b>-techniques-explained-in-simple...", "snippet": "<b>Confusion Matrices</b> &amp; <b>Interpretable ML</b>. Andrea speaks with Nina Lopatina, a data scientist at IQT Labs. Andrea and Nina discuss different approaches to interpreting machine learning systems ...", "dateLastCrawled": "2021-03-22T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Explainability and Auditability in ML: Definitions, Techniques, and ...", "url": "https://neptune.ai/blog/explainability-auditability-ml-definitions-techniques-tools", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/explainability-auditability-ml-definitions-techniques-tools", "snippet": "A <b>car</b> needs fuel to move, i.e it is the fuel that causes the engines to move \u2013 <b>interpretability</b>. <b>Understanding</b> how and why the engine consumes and uses the fuel \u2013 explainability. Most tools and techniques mentioned in this article <b>can</b> be used for both Explainability and <b>Interpretability</b> because like I mentioned earlier both concepts give a perspective on <b>understanding</b> what the model is about. Explainable AI is about <b>understanding</b> ML models better. How they make decisions, and why. The ...", "dateLastCrawled": "2022-02-03T19:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Deep Learning <b>and the Demand for Interpretability</b> \u2013 Syed Ashar Javed ...", "url": "http://stillbreeze.github.io/Deep-Learning-and-the-Demand-For-Interpretability/", "isFamilyFriendly": true, "displayUrl": "stillbreeze.github.io/Deep-Learning-<b>and-the-Demand-For-Interpretability</b>", "snippet": "Transparency, he defines as \u201copposite of blackbox-ness\u201d and \u201csome sense of <b>understanding</b> the mechanism by which the model <b>works</b>\u201d, which seem like a very broad definition and highlights the difficulty in defining it. Post-hoc <b>interpretability</b>, on the other hand, is simply the extraction and analysis of information from models after they have been learned. Clearly, the first one is the more interesting characteristic here, but also the one more difficult to achieve. He also argues in ...", "dateLastCrawled": "2022-01-04T01:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Model Interpretability with SHAP</b> \u2013 F1 predictor", "url": "https://www.f1-predictor.com/model-interpretability-with-shap/", "isFamilyFriendly": true, "displayUrl": "https://www.f1-predictor.com/<b>model-interpretability-with-shap</b>", "snippet": "<b>Understanding</b> the most important features of a model gives us insights into its inner workings and gives directions for improving its performance and removing bias. Therefore we <b>can</b> debug and audit the model. The most important reason, however, for providing explanations along with the predictions is that explainable ML models are necessary to gain end user trust (think of medical applications as an example).", "dateLastCrawled": "2022-02-03T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Interpretability</b> 2020", "url": "https://ff06-2020.fastforwardlabs.com/", "isFamilyFriendly": true, "displayUrl": "https://ff06-2020.fastforwardlabs.com", "snippet": "<b>Interpretability</b> opens up opportunities for collaboration with algorithms. During their development, it promises better processes for feature engineering and model debugging. After completion, it <b>can</b> enhance users\u2019 <b>understanding</b> of the system being modeled and advise on actions to take.", "dateLastCrawled": "2022-01-16T21:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Balance: <b>Accuracy vs. Interpretability in Regulated Environments</b> ...", "url": "https://www.elderresearch.com/blog/balance-accuracy-vs-interpretability-in-regulated-environments/", "isFamilyFriendly": true, "displayUrl": "https://www.elderresearch.com/blog/balance-<b>accuracy-vs-interpretability-in-regulated</b>...", "snippet": "A <b>car</b> <b>can</b> be made faster by giving it a higher horsepower engine. It likewise may be made faster by making it lighter. These two things are typically at odds with one another. More powerful engines typically are bigger and heavier, affecting performance around turns. Reducing weight improves performance everywhere around the track, but is often viewed negatively because it is harder to achieve the necessary compromise (i.e., what must be left out?).", "dateLastCrawled": "2022-02-02T15:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Interpretability vs Explainability: The Black</b> Box of Machine Learning ...", "url": "https://www.bmc.com/blogs/machine-learning-interpretability-vs-explainability/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/machine-learning-<b>interpretability</b>-vs-explainability", "snippet": "<b>Interpretability</b> has to do with how accurate a machine learning model <b>can</b> associate a cause to an effect. Explainability has to do with the ability of the parameters, often hidden in Deep Nets, to justify the results. This is a long article. Hang in there and, by the end, you will understand: How <b>interpretability</b> is different from explainability.", "dateLastCrawled": "2022-01-30T19:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Explainability and Auditability in ML: Definitions, Techniques, and ...", "url": "https://neptune.ai/blog/explainability-auditability-ml-definitions-techniques-tools", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/explainability-auditability-ml-definitions-techniques-tools", "snippet": "A <b>car</b> needs fuel to move, i.e it is the fuel that causes the engines to move \u2013 <b>interpretability</b>. <b>Understanding</b> how and why the engine consumes and uses the fuel \u2013 explainability. Most tools and techniques mentioned in this article <b>can</b> be used for both Explainability and <b>Interpretability</b> because like I mentioned earlier both concepts give a perspective on <b>understanding</b> what the model is about. Explainable AI is about <b>understanding</b> ML models better. How they make decisions, and why. The ...", "dateLastCrawled": "2022-02-03T19:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Interpretability</b> and explainability | by Christian K\u00e4stner | Medium", "url": "https://ckaestne.medium.com/interpretability-and-explainability-a80131467856", "isFamilyFriendly": true, "displayUrl": "https://ckaestne.medium.com/<b>interpretability</b>-and-explainability-a80131467856", "snippet": "<b>Interpretability</b>: We consider a model intrinsically interpretable, if a human <b>can</b> understand the internal workings of the model, either the entire model at once or at least the parts of the model relevant for a given prediction. This may include <b>understanding</b> decision rules and cutoffs and the ability to manually derive the outputs of the model. For example, the scorecard for the recidivism model <b>can</b> be considered interpretable, as it is compact and simple enough to be fully understood ...", "dateLastCrawled": "2022-01-31T21:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Explainable AI (XAI) and Interpretable Machine Learning (IML) models ...", "url": "https://ambiata.com/blog/2021-04-12-xai-part-1/", "isFamilyFriendly": true, "displayUrl": "https://ambiata.com/blog/2021-04-12-xai-part-1", "snippet": "<b>Interpretability</b> of modelling methods <b>compared</b>. Imagine we have a data-science task involving a very large dataset with hundreds of predictors and potentially many non-linear relationships with multi-way interactions. A simple linear model would not work well, and it would be infeasible to plot and inspect all the possible relationships in the data in an attempt to create features that capture the non-linear behaviours. The more powerful machine learning methods such as decision trees and ...", "dateLastCrawled": "2022-01-28T21:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Interpretability</b> in Machine Learning: An Overview", "url": "https://hacker-news.news/post/25250455", "isFamilyFriendly": true, "displayUrl": "https://hacker-news.news/post/25250455", "snippet": "Another work, Interpreting <b>Interpretability</b>: <b>Understanding</b> Data Scientists&#39; Use of <b>Interpretability</b> Tools for Machine Learning, found that even data scientists may not understand what interpretable visualizations tell them. This <b>can</b> inspire unwarranted confidence in the underlying model, even leading to an ad-hoc rationalization of suspicious results.", "dateLastCrawled": "2022-01-25T22:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Explainable AI and Interpretation of Models | AltexSoft", "url": "https://www.altexsoft.com/blog/interpretability-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.altexsoft.com/blog/<b>interpretability</b>-<b>machine-learning</b>", "snippet": "So, you should have clear <b>understanding</b> which data points act as powerful predictors to catch bias inherited from humans. Consider this recommendation if your data models have social impacts and are based on human-generated data. Social acceptance and insights. <b>Interpretability</b> shouldn\u2019t be considered a restrictive matter only. It\u2019s also an ...", "dateLastCrawled": "2022-01-30T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Model Interpretability with SHAP</b> \u2013 F1 predictor", "url": "https://www.f1-predictor.com/model-interpretability-with-shap/", "isFamilyFriendly": true, "displayUrl": "https://www.f1-predictor.com/<b>model-interpretability-with-shap</b>", "snippet": "<b>Understanding</b> the most important features of a model gives us insights into its inner workings and gives directions for improving its performance and removing bias. Therefore we <b>can</b> debug and audit the model. The most important reason, however, for providing explanations along with the predictions is that explainable ML models are necessary to gain end user trust (think of medical applications as an example).", "dateLastCrawled": "2022-02-03T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Confusion Matrices</b> &amp; <b>Interpretable ML</b> | by andrea b | high stakes ...", "url": "https://medium.com/high-stakes-design/interpretability-techniques-explained-in-simple-terms-f5e1573674f3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/high-stakes-design/<b>interpretability</b>-techniques-explained-in-simple...", "snippet": "Andrea speaks with Nina Lopatina, a data scientist at IQT Labs. Andrea and Nina discuss different approaches to interpreting machine learning systems, including the use of \u201c<b>confusion matrices</b>.\u201d", "dateLastCrawled": "2021-03-22T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Interpretability</b> in Machine Learning: An Overview | Hacker News", "url": "https://news.ycombinator.com/item?id=25250455", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=25250455", "snippet": "For example, when people refer to concepts, they often rely on some established culture, a common <b>understanding</b>, rather than things you <b>can</b> measure. Let&#39;s take the simple example of classifying spoiled fruit at the grocery store. You <b>can</b> train a ConvNet and it will probably learn to recognize some visual traits of &quot;spoiledness&quot;, but how objective <b>can</b> it really be, given that humans don&#39;t always agree what &quot;spoiled&quot; really means? In other words, fruit is spoiled only when a large enough ...", "dateLastCrawled": "2021-04-10T13:17:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretability</b> in <b>Machine</b> <b>Learning</b>: An Overview", "url": "https://thegradient.pub/interpretability-in-ml-a-broad-overview/", "isFamilyFriendly": true, "displayUrl": "https://thegradient.pub/<b>interpretability</b>-in-ml-a-broad-overview", "snippet": "First, <b>interpretability</b> in <b>machine</b> <b>learning</b> is useful because it can aid in trust. As humans, we may be reluctant to rely on <b>machine</b> <b>learning</b> models for certain critical tasks, e.g., medical diagnosis, unless we know &quot;how they work.&quot; There&#39;s often a fear of the unknown when trusting in something opaque, which we see when people confront new technology, and this can slow down adoption. Approaches to <b>interpretability</b> that focus on transparency could help mitigate some of these fears.", "dateLastCrawled": "2022-02-01T15:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "6 \u2013 <b>Interpretability</b> \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-<b>interpretability</b>", "snippet": "Figure 1: <b>Interpretability</b> for <b>machine</b> <b>learning</b> models bridges the concrete objectives models optimize for and the real-world (and less easy to define) desiderata that ML applications aim to achieve. Introduction . The objectives <b>machine</b> <b>learning</b> models optimize for do not always reflect the actual desiderata of the task at hand. <b>Interpretability</b> in models allows us to evaluate their decisions and obtain information that the objective alone cannot confer. <b>Interpretability</b> takes many forms ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interpretability</b> in <b>Machine</b> <b>Learning</b>", "url": "https://www.cl.cam.ac.uk/teaching/1819/P230/IWML-Lecture-4.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cl.cam.ac.uk/teaching/1819/P230/IWML-Lecture-4.pdf", "snippet": "This provides a novel <b>analogy</b> between data compression and regularization. Qualitative and quantitative state-of-the-art results on three datasets. 20 / 33. Interpretable Lens Variable Model (ILVM) 21 / 33. Interactive <b>Interpretability</b> via Active <b>Learning</b> Interactive \u2018human-in-the-loop\u2019 <b>interpretability</b> Choose the point with index j that maximizes : ^j = argmax jI(s ; ) = H(s ) E q \u02da(z js)[H(s jz j)] = Z p(s j)log p(s j)ds + E q \u02da(z js) Z p (s jjz)log p (s jjz)ds : (5) Choose the point ...", "dateLastCrawled": "2022-01-19T17:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interpretable Machine Learning: Advantages and Disadvantages</b> | by ...", "url": "https://towardsdatascience.com/interpretable-machine-learning-advantages-and-disadvantages-901769f48c43", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpretable-machine-learning-advantages-and</b>...", "snippet": "There is increasing emphasis on interpretable <b>machine</b> <b>learning</b> in the world of data. Models have been growing ever more complex with the use of neural networks becoming more mainstream, along with the sheer size of data being analysed today. In many cases, such complex models may not be fit for human interpretation in their own right. Therefore, there has been a push to make the model interpretable, whereby both the results and the process in achieving those results are understood by humans ...", "dateLastCrawled": "2022-01-19T03:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Confusion Matrices</b> &amp; <b>Interpretable ML</b> | by andrea b | high stakes ...", "url": "https://medium.com/high-stakes-design/interpretability-techniques-explained-in-simple-terms-f5e1573674f3", "isFamilyFriendly": true, "displayUrl": "https://medium.com/high-stakes-design/<b>interpretability</b>-techniques-explained-in-simple...", "snippet": "The best [<b>analogy</b>] I can think of is an indicator light in your car \u2014 [and the] <b>machine</b> that you plug in to tell you more about the readout. ANDREA: Do you see <b>interpretability</b>, primarily, as ...", "dateLastCrawled": "2021-03-22T05:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Economic Methodology Meets Interpretable Machine Learning</b> - Part I ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_blackboxes", "snippet": "In this series of posts, we will develop an <b>analogy</b> between the realistic assumptions debate in economic methodology and the current discussion over <b>interpretability</b> when using <b>machine</b> <b>learning</b> models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for <b>machine</b> <b>learning</b> theorists and practitioners to avoid analysis paralysis and make progress on the <b>interpretability</b> issue - one way or the other. Intro - Part ...", "dateLastCrawled": "2022-01-22T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Towards <b>Analogy</b>-Based Explanations in <b>Machine</b> <b>Learning</b>", "url": "https://arxiv.org/abs/2005.12800", "isFamilyFriendly": true, "displayUrl": "https://<b>arxiv</b>.org/abs/2005.12800", "snippet": "Principles of analogical reasoning have recently been applied in the context of <b>machine</b> <b>learning</b>, for example to develop new methods for classification and preference <b>learning</b>. In this paper, we argue that, while analogical reasoning is certainly useful for constructing new <b>learning</b> algorithms with high predictive accuracy, is is arguably not less interesting from an <b>interpretability</b> and explainability point of view. More specifically, we take the view that an <b>analogy</b>-based approach is a ...", "dateLastCrawled": "2021-10-24T20:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Economic Methodology Meets Interpretable <b>Machine</b> <b>Learning</b> ...", "url": "https://bcmullins.github.io/economic_methodology_interpretable_ml_intro/", "isFamilyFriendly": true, "displayUrl": "https://bcmullins.github.io/economic_methodology_interpretable_ml_intro", "snippet": "In this series of posts, we will develop an <b>analogy</b> between the realistic assumptions debate in economic methodology and the current discussion over <b>interpretability</b> when using <b>machine</b> <b>learning</b> models in the wild. While this connection may seem fuzzy at first, the past seventy years or so of economic methodology offers many lessons for <b>machine</b> <b>learning</b> theorists and practitioners to avoid analysis paralysis and make progress on the <b>interpretability</b> issue - one way or the other. But first ...", "dateLastCrawled": "2022-01-05T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Analogies between Biology and Deep <b>Learning</b> [rough note] -- colah&#39;s blog", "url": "http://colah.github.io/notes/bio-analogies/", "isFamilyFriendly": true, "displayUrl": "colah.github.io/notes/bio-analogies", "snippet": "Neuroscience \u2194 <b>Interpretability</b>. <b>Analogy</b>: model=brain. Artificial neural networks are historically inspired by neuroscience, but I used to be pretty skeptical that the connection was anything more than superficial. I&#39;ve since come around: I now think this is a very deep connection. The thing that personally persuaded me was that, in my own investigations of what goes on inside neural networks, we kept finding things that were previously discovered by neuroscientists. The most recent ...", "dateLastCrawled": "2022-01-30T16:04:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Chris Olah on what the hell is going on inside neural networks - 80,000 ...", "url": "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/", "isFamilyFriendly": true, "displayUrl": "https://80000hours.org/podcast/episodes/chris-olah-interpretability-research", "snippet": "Chris is a <b>machine</b> <b>learning</b> researcher currently focused on neural network interpretability. Until last December he led OpenAI\u2019s interpretability team but along with some colleagues he recently moved on to help start a new AI lab focussed on large models and safety called Anthropic. Rob Wiblin: Before OpenAI he spent 4 years at Google Brain developing tools to visualize what\u2019s going on in neural networks. Chris was hugely impactful at Google Brain. He was second author on the launch of ...", "dateLastCrawled": "2022-02-01T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Marketing AI: Interpretability and Explainability</b> - Christopher S. Penn ...", "url": "https://www.christopherspenn.com/2021/03/marketing-ai-interpretability-and-explainability/", "isFamilyFriendly": true, "displayUrl": "https://www.christopherspenn.com/2021/03/<b>marketing-ai-interpretability-and-explainability</b>", "snippet": "<b>Interpretability is like</b> inspecting the baker\u2019s recipe for the cake. We look at the list of ingredients and the steps taken to bake the cake, and we verify that the recipe makes sense and the ingredients were good. This is a much more rigorous way of validating our results, but it\u2019s the most complete \u2013 and if we\u2019re in a high-stakes situation where we need to remove all doubt, this is the approach we take. Interpretability in AI is like that \u2013 we step through the code itself that ...", "dateLastCrawled": "2022-01-29T12:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Causal <b>Learning</b> From Predictive Modeling for Observational Data", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7931928/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7931928", "snippet": "Given the recent success of <b>machine</b> <b>learning</b>, specifically deep <b>learning</b>, in several applications (Goodfellow et al., ... This statistical <b>interpretability is similar</b> in spirit to traditional interpretability. This allows to answer questions, such as \u201cdoes BMI influence susceptibility to Covid?\u201d Moreover, it has been argued that developing an effective CBN for practical applications requires expert knowledge when data collection is cumbersome (Fenton and Neil, 2012). This applies to ...", "dateLastCrawled": "2021-12-09T23:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Optimal <b>Predictive Clustering</b> - Dimitris Bertsimas", "url": "https://dbertsim.mit.edu/pdfs/papers/2020-sobiesk-optimal-predictive-clustering.pdf", "isFamilyFriendly": true, "displayUrl": "https://dbertsim.mit.edu/pdfs/papers/2020-sobiesk-optimal-<b>predictive-clustering</b>.pdf", "snippet": "Table 1 Comparison of major <b>machine</b> <b>learning</b> methods relative to each other across the metrics of performance (out-of-sample R2), scalability and interpretability. 1 is the best, while 5 is the worst. Optimal <b>Predictive Clustering</b> 3 From Table 1, we observe all existing methods have weakness in at least one category. We therefore seek to design a method that has strong performance in all three categories at the same time. Optimal <b>Predictive Clustering</b> (OPC) is an algorithm that uses mixed ...", "dateLastCrawled": "2021-11-24T20:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reviewing Challenges of Predicting Protein Melting Temperature Change ...", "url": "https://link.springer.com/article/10.1007/s12033-021-00349-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12033-021-00349-0", "snippet": "Predicting the effects of mutations on protein stability is a key problem in fundamental and applied biology, still unsolved even for the relatively simple case of small, soluble, globular, monomeric, two-state-folder proteins. Many articles discuss the limitations of prediction methods and of the datasets used to train them, which result in low reliability for actual applications despite globally capturing trends. Here, we review these and other issues by analyzing one of the most detailed ...", "dateLastCrawled": "2022-02-03T02:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Interpretable Machine Learning: Advantages and Disadvantages</b> | by ...", "url": "https://towardsdatascience.com/interpretable-machine-learning-advantages-and-disadvantages-901769f48c43", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>interpretable-machine-learning-advantages-and</b>...", "snippet": "In my view, a shor t coming of interpretable <b>machine</b> <b>learning</b> is that it assumes to a degree that the data being fed into the model is always going to be suitable for human interpretation. This is not necessarily the case. For instance, let\u2019s say that a company is trying to implement interpretable <b>machine</b> <b>learning</b> to devise a credit scoring model, whereby prospective credit card applications are classified as approved or rejected based on numerous features. It is often the case that such ...", "dateLastCrawled": "2022-01-19T03:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Breaking the interpretability barrier - a</b> method for interpreting deep ...", "url": "http://www.di.uniba.it/~loglisci/NFMCP2019/NFMCP/nfMCP2019_paper_17.pdf", "isFamilyFriendly": true, "displayUrl": "www.di.uniba.it/~loglisci/NFMCP2019/NFMCP/nfMCP2019_paper_17.pdf", "snippet": "Last, but not least, <b>interpretability can be thought of as</b> a useful tool for understanding and correcting model errors. In general, we are faced with a trade-o between performance and inter-pretability. Graph classi cation is normally a domain which requires the ap-plication of complex <b>learning</b> models, such as deep neural networks, which are not interpretable by nature. Several relevant attempts have been made to in-terpret complex models post-hoc (brie y reviewed in section2). However, most ...", "dateLastCrawled": "2021-09-22T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Discovering Discriminative Nodes for Classi\ufb01cation with Deep Graph ...", "url": "http://muresanlab.tins.ro/publications/preprints/Palcu_et_al_LNAI_2020.pdf", "isFamilyFriendly": true, "displayUrl": "muresanlab.tins.ro/publications/preprints/Palcu_et_al_LNAI_2020.pdf", "snippet": "Last, but not least, <b>interpretability can be thought of as</b> a useful tool for understanding and correcting model errors. In general, we are faced with a trade-o\ufb00 between performance and inter-pretability. Graph classi\ufb01cation is normally a domain which requires the appli-cation of complex <b>learning</b> models, such as deep neural networks, which are not interpretable by nature. Several relevant attempts have been made to interpret complex models post-hoc (brie\ufb02y reviewed in Sect.2). However ...", "dateLastCrawled": "2021-09-02T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Confronting Abusive Language Online: A Survey from the Ethical and ...", "url": "https://www.arxiv-vanity.com/papers/2012.12305/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2012.12305", "snippet": "The pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. Significant effort in Natural Language Processing (NLP) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. Although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this ...", "dateLastCrawled": "2021-10-13T19:21:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(interpretability)  is like +(understanding how a car works)", "+(interpretability) is similar to +(understanding how a car works)", "+(interpretability) can be thought of as +(understanding how a car works)", "+(interpretability) can be compared to +(understanding how a car works)", "machine learning +(interpretability AND analogy)", "machine learning +(\"interpretability is like\")", "machine learning +(\"interpretability is similar\")", "machine learning +(\"just as interpretability\")", "machine learning +(\"interpretability can be thought of as\")", "machine learning +(\"interpretability can be compared to\")"]}