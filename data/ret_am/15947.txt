{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Log</b> <b>Loss</b> Function Explained by Experts | <b>Dasha.AI</b> | <b>Dasha.AI</b>", "url": "https://dasha.ai/en-us/blog/log-loss-function", "isFamilyFriendly": true, "displayUrl": "https://<b>dasha.ai</b>/en-us/b<b>log</b>/<b>log</b>-<b>loss</b>-function", "snippet": "Logarithmic <b>loss</b> indicates how close a prediction probability comes to the <b>actual</b>/corresponding true value. Here is the <b>log</b> <b>loss</b> formula: Binary Cross-Entropy , <b>Log</b> <b>Loss</b>. Let&#39;s think of how the linear regression problem is solved. We want to get a linear <b>log</b> <b>loss</b> function (i.e. weights w) that approximates the target value up to <b>error</b>: linear ...", "dateLastCrawled": "2022-01-26T20:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding binary <b>cross-entropy</b> / <b>log</b> <b>loss</b>: a visual explanation ...", "url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-binary-<b>cross-entropy</b>-<b>log</b>-<b>loss</b>-a-visual...", "snippet": "For a binary classification <b>like</b> our example, the typical <b>loss</b> function is the binary <b>cross-entropy</b> / <b>log</b> <b>loss</b>. <b>Loss</b> Function: Binary <b>Cross-Entropy</b> / <b>Log</b> <b>Loss</b>. If you look this <b>loss</b> function up, this is what you\u2019ll find: Binary <b>Cross-Entropy</b> / <b>Log</b> <b>Loss</b>. where y is the label (1 for green points and 0 for red points) and p(y) is the predicted probability of the point being green for all N points. Reading this formula, it tells you that, for each green point (y=1), it adds <b>log</b>(p(y)) to the ...", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Log</b> transformations: How to handle <b>negative</b> data values? - The DO Loop", "url": "https://blogs.sas.com/content/iml/2011/04/27/log-transformations-how-to-handle-negative-data-values.html", "isFamilyFriendly": true, "displayUrl": "https://b<b>log</b>s.sas.com/.../27/<b>log</b>-transformations-how-to-handle-<b>negative</b>-data-values.html", "snippet": "I am having <b>negative</b> data for variables <b>like</b> population growth <b>rate</b> and consumer price index. Should i use the method you suggested above - The transformation is therefore <b>log</b>(Y+a) where a is the constant. Some people <b>like</b> to choose a so that min(Y+a) is a very small positive number (<b>like</b> 0.001). Others choose a so that min(Y+a) = 1. For the latter choice, you can show that a = b \u2013 min(Y), where b is either a small number or is 1?", "dateLastCrawled": "2022-02-03T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "How <b>To Build Custom Loss Functions In Keras</b> For Any Use Case | cnvrg.io", "url": "https://cnvrg.io/keras-custom-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://cnvrg.io/keras-custom-<b>loss</b>-functions", "snippet": "This <b>loss</b> function is also known as the <b>Log</b> <b>Loss</b> function, because of the <b>logarithm</b> of the <b>loss</b>. Standalone Implementation: You can create an object for Binary Cross Entropy from Keras.losses. Then you have to pass in our true and predicted labels.", "dateLastCrawled": "2022-01-31T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "machine learning - What&#39;s considered a good <b>log loss</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/276067/whats-considered-a-good-log-loss", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/276067", "snippet": "This answer is not useful. Show activity on this post. The <b>logloss</b> is simply L ( p i) = \u2212 <b>log</b>. \u2061. ( p i) where p is simply the probability attributed to the real class. So L ( p) = 0 is good, we attributed the probability 1 to the right class, while L ( p) = + \u221e is bad, because we attributed the probability 0 to the <b>actual</b> class.", "dateLastCrawled": "2022-01-30T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Logarithmic <b>Transformation</b> in Linear Regression Models: Why &amp; When ...", "url": "https://dev.to/rokaandy/logarithmic-transformation-in-linear-regression-models-why-when-3a7c", "isFamilyFriendly": true, "displayUrl": "https://dev.to/rokaandy/<b>logarithm</b>ic-<b>transformation</b>-in-linear-regression-models-why...", "snippet": "<b>Taking</b> the <b>log</b> of one or both variables will effectively change the case from a unit change to a percent change. This is especially important when using medium to large datasets. Another way to think about it is when <b>taking</b> a <b>log</b> of a dataset is transforming your model(s) to take advantage of statistical tools such as linear regression that improve on features that are normally distributed. A <b>logarithm</b> is the base of a positive number. For example, the base10 <b>log</b> of 100 is 2, because 10 2 ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Which <b>loss</b> function is correct for <b>logistic regression</b> ... - Cross ...", "url": "https://stats.stackexchange.com/questions/250937/which-loss-function-is-correct-for-logistic-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/250937", "snippet": "Instead of Mean Squared <b>Error</b>, we use a cost function called Cross-Entropy, also known as <b>Log</b> <b>Loss</b>. Cross-entropy <b>loss</b> can be divided into two separate cost functions: one for y=1 and one for y=0. Cross-entropy <b>loss</b> can be divided into two separate cost functions: one for y=1 and one for y=0.", "dateLastCrawled": "2022-02-01T19:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Log</b> and natural Logarithmic <b>value of a column in Pandas</b> - Python ...", "url": "https://www.geeksforgeeks.org/log-and-natural-logarithmic-value-of-a-column-in-pandas-python/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/<b>log</b>-and-natural-<b>logarithm</b>ic-<b>value-of-a-column-in-pandas</b>...", "snippet": "<b>Like</b> Article. <b>Log</b> and natural Logarithmic <b>value of a column in Pandas</b> \u2013 Python. Last Updated : 28 Jul, 2020. <b>Log</b> and natural logarithmic <b>value of a column in pandas</b> can be calculated using the <b>log</b>(), log2(), and log10() numpy functions respectively. Before applying the functions, we need to create a dataframe. Code: Python3 # Import required libraries. import pandas as pd. import numpy as np # Dictionary. data = { &#39;Name&#39;: [&#39;Geek1&#39;, &#39;Geek2&#39;, &#39;Geek3&#39;, &#39;Geek4&#39;], &#39;Salary&#39;: [18000, 20000, 15000 ...", "dateLastCrawled": "2022-01-30T11:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[Blog Post #3] Metrics for Regression - Prevision.io", "url": "https://prevision.io/blog/blog-post-3-metrics-for-regression/", "isFamilyFriendly": true, "displayUrl": "https://prevision.io/b<b>log</b>/b<b>log</b>-post-3-metrics-for-regression", "snippet": "RMSLE adds 1 to both <b>actual</b> and predicted values before <b>taking</b> the natural <b>logarithm</b> to avoid <b>taking</b> the natural <b>log</b> of possible 0 (zero) values. In case of RMSLE, you take the <b>log</b> of the predictions and <b>actual</b> values. So basically, what changes is the variance that you are measuring. I believe RMSLE is usually used when you don\u2019t want to penalize huge differences in the predicted and the <b>actual</b> values when both predicted and true values are huge numbers. where: yi are the real values, \u0177i ...", "dateLastCrawled": "2022-01-25T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - The <b>cross-entropy</b> <b>error</b> function in neural networks ...", "url": "https://datascience.stackexchange.com/questions/9302/the-cross-entropy-error-function-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/9302", "snippet": "Question 2. I&#39;ve learned that <b>cross-entropy</b> is defined as H y \u2032 ( y) := \u2212 \u2211 i ( y i \u2032 <b>log</b>. \u2061. ( y i) + ( 1 \u2212 y i \u2032) <b>log</b>. \u2061. ( 1 \u2212 y i)) This formulation is often used for a network with one output predicting two classes (usually positive class membership for 1 and <b>negative</b> for 0 output). In that case i may only have one value ...", "dateLastCrawled": "2022-01-30T02:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Log</b> <b>Loss</b> Function Explained by Experts | <b>Dasha.AI</b> | <b>Dasha.AI</b>", "url": "https://dasha.ai/en-us/blog/log-loss-function", "isFamilyFriendly": true, "displayUrl": "https://<b>dasha.ai</b>/en-us/b<b>log</b>/<b>log</b>-<b>loss</b>-function", "snippet": "Logarithmic <b>loss</b> indicates how close a prediction probability comes to the <b>actual</b>/corresponding true value. Here is the <b>log</b> <b>loss</b> formula: Binary Cross-Entropy , <b>Log</b> <b>Loss</b>. Let&#39;s think of how the linear regression problem is solved. We want to get a linear <b>log</b> <b>loss</b> function (i.e. weights w) that approximates the target value up to <b>error</b>: linear ...", "dateLastCrawled": "2022-01-26T20:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Log</b> transformations: How to handle <b>negative</b> data values? - The DO Loop", "url": "https://blogs.sas.com/content/iml/2011/04/27/log-transformations-how-to-handle-negative-data-values.html", "isFamilyFriendly": true, "displayUrl": "https://b<b>log</b>s.sas.com/.../27/<b>log</b>-transformations-how-to-handle-<b>negative</b>-data-values.html", "snippet": "To answer your question, if you take logs first and then difference them, you are forming the <b>log</b> of a ratio, since <b>log</b>(y_{i+1}) - <b>log</b>(y_i) = <b>log</b>(y_{i+1} /y_i). This would mean that you would be examining the &quot;proportion of change&quot; from one year to the next. Assuming that none of your data are zero, this is a reasonable thing to do. It centers the &quot;no change&quot; situation at 1 instead of 0, and it also eliminates <b>negative</b> numbers (assuming your data are positive).", "dateLastCrawled": "2022-02-03T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Logarithmic <b>Transformation</b> in Linear Regression Models: Why &amp; When ...", "url": "https://dev.to/rokaandy/logarithmic-transformation-in-linear-regression-models-why-when-3a7c", "isFamilyFriendly": true, "displayUrl": "https://dev.to/rokaandy/<b>logarithm</b>ic-<b>transformation</b>-in-linear-regression-models-why...", "snippet": "<b>Taking</b> the <b>log</b> of one or both variables will effectively change the case from a unit change to a percent change. This is especially important when using medium to large datasets. Another way to think about it is when <b>taking</b> a <b>log</b> of a dataset is transforming your model(s) to take advantage of statistical tools such as linear regression that improve on features that are normally distributed. A <b>logarithm</b> is the base of a positive number. For example, the base10 <b>log</b> of 100 is 2, because 10 2 ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Gentle Introduction to <b>Probability Metrics for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>probability-metrics-for-imbalanced-classification</b>", "snippet": "The <b>log</b> <b>loss</b> function calculates <b>the negative</b> <b>log</b> likelihood for probability predictions made by the binary classification model. Most notably, ... the <b>log</b> <b>loss</b> can be calculated using the expected probabilities for each class and the natural <b>logarithm</b> of the predicted probabilities for each class; for example: LogLoss = -(P(class=0) * <b>log</b>(P(class=0)) + (P(class=1)) * <b>log</b>(P(class=1))) The best possible <b>log</b> <b>loss</b> is 0.0, and values are positive to infinite for progressively worse scores. If ...", "dateLastCrawled": "2022-02-02T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Estimators, <b>Loss</b> Functions, Optimizers \u2014Core of ML Algorithms | by ...", "url": "https://towardsdatascience.com/estimators-loss-functions-optimizers-core-of-ml-algorithms-d603f6b0161a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/estimators-<b>loss</b>-functions-<b>optimizer</b>s-core-of-ml...", "snippet": "In case of a binary classification each predicted probability is compared to the <b>actual</b> class output value (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected value. Visualization. The graph below shows the range of possible <b>log</b> <b>loss</b> values given a true observation (y= 1). As the predicted probability approaches 1, <b>log</b> <b>loss</b> slowly decreases. As the predicted probability decreases, however, the <b>log</b> <b>loss</b> increases rapidly. <b>Log</b> <b>loss</b> ...", "dateLastCrawled": "2022-02-02T04:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "When (&amp; why) to <b>use log transformation in regression</b>?", "url": "https://gdcoder.com/when-why-to-use-log-transformation-in-regression/", "isFamilyFriendly": true, "displayUrl": "https://gdcoder.com/when-why-to-<b>use-log-transformation-in-regression</b>", "snippet": "Let&#39;s look at the <b>actual</b> effect on some target values. For example a DT trained on the raw target variable, would predict a terrible average of df.SalePrice.mean()=$180921. Whereas an DT trained on the <b>log</b> of the target variable predicts an average of np.expm1(df[&#39;<b>log</b>_SalePrice&#39;].mean()=$166716.", "dateLastCrawled": "2022-02-02T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "In linear <b>regression</b>, when is it appropriate to use the <b>log</b> of an ...", "url": "https://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/298", "snippet": "If the variable has <b>negative</b> skew you could firstly invert the variable before <b>taking</b> the <b>logarithm</b>. I&#39;m thinking here particularly of Likert scales that are inputed as continuous variables. While this usually applies to the dependent variable you occasionally have problems with the residuals (e.g. heteroscedasticity) caused by an independent variable which can be sometimes corrected by <b>taking</b> the <b>logarithm</b> of that variable. For example when running a model that explained lecturer ...", "dateLastCrawled": "2022-01-27T11:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Using <b>Logarithms in the Real World</b> \u2013 BetterExplained", "url": "https://betterexplained.com/articles/using-logs-in-the-real-world/", "isFamilyFriendly": true, "displayUrl": "https://betterexplained.com/articles/using-<b>log</b>s-in-the-real-world", "snippet": "With logarithms a &quot;.5&quot; means halfway in terms of multiplication, i.e the square root ( 9 .5 means the square root of 9 -- 3 is halfway in terms of multiplication because it&#39;s 1 to 3 and 3 to 9). <b>Taking</b> <b>log</b> (500,000) we get 5.7, add 1 for the extra digit, and we can say &quot;500,000 is a 6.7 figure number&quot;. Try it out here:", "dateLastCrawled": "2022-02-02T18:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "model.compile(<b>loss</b>=&#39;...&#39;, optimizer=opt) # fit model. history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0) Now that we have the basis of a problem and model, we can take a look evaluating three common <b>loss</b> functions that are appropriate for a regression predictive modeling problem.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Is it normal if a have a <b>negative</b> value for <b>log</b> likelyhood with an ...", "url": "https://www.quora.com/Is-it-normal-if-a-have-a-negative-value-for-log-likelyhood-with-an-ARIMA-model", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-it-normal-if-a-have-a-<b>negative</b>-value-for-<b>log</b>-likelyhood-with...", "snippet": "Answer: Yes. Think of the likelihood of a sample from a discrete random variable. The probability of each observation is less than 1. The likelihood is the product of such probabilities and is then also less than 1. The <b>log</b> of this probability is then less than one. I can construct an artificial...", "dateLastCrawled": "2022-01-17T03:30:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Log</b> transformations: How to handle <b>negative</b> data values? - The DO Loop", "url": "https://blogs.sas.com/content/iml/2011/04/27/log-transformations-how-to-handle-negative-data-values.html", "isFamilyFriendly": true, "displayUrl": "https://b<b>log</b>s.sas.com/.../27/<b>log</b>-transformations-how-to-handle-<b>negative</b>-data-values.html", "snippet": "The following example uses b=1 and calls the LOG10 function, but you <b>can</b> call <b>LOG</b>, the natural <b>logarithm</b> function, if you prefer. proc iml; Y = {-3,1,2,.,5,10,100}; /* <b>negative</b> datum */ LY = log10(Y + 1 - min(Y)); /* translate, then transform */ Solution 2: Use Missing Values. A criticism of the previous method is that some practicing statisticians don&#39;t like to add an arbitrary constant to the data. They argue that a better way to handle <b>negative</b> values is to use missing values for the ...", "dateLastCrawled": "2022-02-03T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding binary <b>cross-entropy</b> / <b>log</b> <b>loss</b>: a visual explanation ...", "url": "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-binary-<b>cross-entropy</b>-<b>log</b>-<b>loss</b>-a-visual...", "snippet": "Binary <b>Cross-Entropy</b> / <b>Log</b> <b>Loss</b>. where y is the label (1 for green points and 0 for red points) and p(y) is the predicted probability of the point being green for all N points.. Reading this formula, it tells you that, for each green point (y=1), it adds <b>log</b>(p(y)) to the <b>loss</b>, that is, the <b>log</b> probability of it being green.Conversely, it adds <b>log</b>(1-p(y)), that is, the <b>log</b> probability of it being red, for each red point (y=0).Not necessarily difficult, sure, but no so intuitive too\u2026", "dateLastCrawled": "2022-02-03T02:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How <b>To Build Custom Loss Functions In Keras</b> For Any Use Case | cnvrg.io", "url": "https://cnvrg.io/keras-custom-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://cnvrg.io/keras-custom-<b>loss</b>-functions", "snippet": "This <b>loss</b> function is also known as the <b>Log</b> <b>Loss</b> function, because of the <b>logarithm</b> of the <b>loss</b>. Standalone Implementation: You <b>can</b> create an object for Binary Cross Entropy from Keras.losses. Then you have to pass in our true and predicted labels.", "dateLastCrawled": "2022-01-31T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Logarithmic <b>Transformation</b> in Linear Regression Models: Why &amp; When ...", "url": "https://dev.to/rokaandy/logarithmic-transformation-in-linear-regression-models-why-when-3a7c", "isFamilyFriendly": true, "displayUrl": "https://dev.to/rokaandy/<b>logarithm</b>ic-<b>transformation</b>-in-linear-regression-models-why...", "snippet": "<b>Taking</b> the <b>log</b> of one or both variables will effectively change the case from a unit change to a percent change. This is especially important when using medium to large datasets. Another way to think about it is when <b>taking</b> a <b>log</b> of a dataset is transforming your model(s) to take advantage of statistical tools such as linear regression that improve on features that are normally distributed. A <b>logarithm</b> is the base of a positive number. For example, the base10 <b>log</b> of 100 is 2, because 10 2 ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Building a neural network from scratch in</b> R | R-bloggers", "url": "https://www.r-bloggers.com/2018/01/building-a-neural-network-from-scratch-in-r/", "isFamilyFriendly": true, "displayUrl": "https://www.r-b<b>log</b>gers.com/2018/01/<b>building-a-neural-network-from-scratch-in</b>-r", "snippet": "accuracy and compute_<b>loss</b> return the proportion of correct class assignments and <b>the negative</b> <b>log</b> likelihood (\u2018<b>log</b> <b>loss</b>\u2019), respectively; sigmoid, dsigmoid and softmax are utility functions. Let\u2019s see our fancy neural network in action on the famous iris data set. Firstly, initialise the network by specifying the input and output variables ...", "dateLastCrawled": "2022-01-12T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>The Indefinite Logarithm, Logarithmic Units, and</b> the Nature of ...", "url": "https://www.researchgate.net/publication/2173230_The_Indefinite_Logarithm_Logarithmic_Units_and_the_Nature_of_Entropy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2173230_The_Indefinite_<b>Logarithm</b>_<b>Logarithm</b>ic...", "snippet": "We define the indefinite <b>logarithm</b> [<b>log</b> x] of a real number x&gt;0 to be a mathematical object representing the abstract concept of the <b>logarithm</b> of x with an indeterminate base (i.e., not ...", "dateLastCrawled": "2021-10-25T23:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Fathoming the Deep in <b>Deep Learning \u2013 A Practical Approach</b> \u2013 thoughtful ...", "url": "https://avantlive.wordpress.com/2019/04/29/fathoming-the-deep-in-deep-learning-a-practical-approach/", "isFamilyFriendly": true, "displayUrl": "https://avantlive.wordpress.com/2019/04/29/fathoming-the-deep-in-deep-learning-a...", "snippet": "Or it <b>can</b> be computed easily \u2013 given a set of data points, <b>Log</b> <b>Loss</b> measures the divergence of probability distribution of <b>actual</b> versus predicted. We <b>can</b> also assume that the <b>actual</b> and predicted data points represent a probability distribution. From a binary classification perspective, y being the either/or label for an event A or B (1 if A happens, 0 if doesn\u2019t or B happens) and p(y) is its probability. Left hand side represents, label times probability of label being A whereas right ...", "dateLastCrawled": "2022-01-21T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep Learning</b> with Theano - Part 1: Logistic Regression | QuantStart", "url": "https://www.quantstart.com/articles/Deep-Learning-with-Theano-Part-1-Logistic-Regression/", "isFamilyFriendly": true, "displayUrl": "https://www.quantstart.com/articles/<b>Deep-Learning</b>-with-Theano-Part-1-<b>Log</b>istic-Regression", "snippet": "The installation instructions for Theano are best followed from the <b>actual</b> site here as there are many variations of platforms and Python ... Hence it is often easier to work with the logarithmic likelihood (&quot;<b>log</b>-likelihood&quot;). <b>Taking</b> a natural <b>logarithm</b> of a product reduces it to a sum of logarithms making differentiation far simpler. This works because the <b>logarithm</b> itself is a monotonic function meaning that the maximum of the <b>log</b>-likelihood is also the maximum of the likelihood. Hence we ...", "dateLastCrawled": "2022-01-31T19:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "In linear regression, when is it appropriate to use the <b>log</b> of an ...", "url": "https://www.quora.com/In-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-variable-instead-of-the-actual-values", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-linear-regression-when-is-it-appropriate-to-use-the-<b>log</b>-of-an...", "snippet": "Answer (1 of 3): Firstly we apply transformations such as sqrt(x), x^2, <b>log</b>(x) or ln(x) to an independent variable (x) , when we intuitively assume that the original raw format of the independent variable(x) is not the best representation of the effect it has on the dependant variable(y) which is...", "dateLastCrawled": "2022-01-27T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Using Graphs to Determine <b>Rate</b> Laws, <b>Rate</b> Constants, and Reaction Orders", "url": "https://2012books.lardbucket.org/books/principles-of-general-chemistry-v1.0/s18-04-using-graphs-to-determine-rate.html", "isFamilyFriendly": true, "displayUrl": "https://2012books.lardbucket.org/.../s18-04-using-graphs-to-determine-<b>rate</b>.html", "snippet": "Experimental data for this reaction at 330\u00b0C are listed in Table 14.5 &quot;Concentration of NO&quot;; they are provided as [NO 2], ln[NO 2], and 1/[NO 2] versus time to correspond to the integrated <b>rate</b> laws for zeroth-, first-, and second-order reactions, respectively.The <b>actual</b> concentrations of NO 2 are plotted versus time in part (a) in Figure 14.15 &quot;The Decomposition of NO&quot;.Because the plot of [NO 2] versus t is not a straight line, we know the reaction is not zeroth order in NO 2.A plot of ln ...", "dateLastCrawled": "2022-02-03T00:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Estimators, <b>Loss</b> Functions, Optimizers \u2014Core of ML Algorithms | by ...", "url": "https://towardsdatascience.com/estimators-loss-functions-optimizers-core-of-ml-algorithms-d603f6b0161a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/estimators-<b>loss</b>-functions-<b>optimizer</b>s-core-of-ml...", "snippet": "<b>Log</b> <b>Loss</b> uses <b>negative</b> <b>log</b> to provide an easy metric for comparison. It takes this approach because the positive <b>log</b> of numbers &lt; 1 returns <b>negative</b> values, which is confusing to work with when comparing the performance of two models. See this post for detailed discussion on cross-entropy <b>loss</b>. ML problems and corresponding <b>Loss</b> functions", "dateLastCrawled": "2022-02-02T04:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Probability Metrics for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>probability-metrics-for-imbalanced-classification</b>", "snippet": "The <b>log</b> <b>loss</b> function calculates <b>the negative</b> <b>log</b> likelihood for probability predictions made by the binary classification model. Most ... such as cross-entropy. Generally, the <b>log</b> <b>loss</b> <b>can</b> be calculated using the expected probabilities for each class and the natural <b>logarithm</b> of the predicted probabilities for each class; for example: LogLoss = -(P(class=0) * <b>log</b>(P(class=0)) + (P(class=1)) * <b>log</b>(P(class=1))) The best possible <b>log</b> <b>loss</b> is 0.0, and values are positive to infinite for ...", "dateLastCrawled": "2022-02-02T23:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "machine learning - What&#39;s considered a good <b>log loss</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/276067/whats-considered-a-good-log-loss", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/276067", "snippet": "A perfect predictor would have a <b>loss</b>, for probability P: <b>Loss</b> = P ln P + (1-P) ln (1-P) If you are trying to predict something where, at its worse, some events will be predicted with an outcome of 50/50, then by integrating and <b>taking</b> the average the average <b>loss</b> would be: L=0.5", "dateLastCrawled": "2022-01-30T18:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Machine Learning 102: Logistic Regression | by Y. Natsume | Jan, 2022 ...", "url": "https://towardsdatascience.com/machine-learning-102-logistic-regression-9e6dc2807772", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/machine-learning-102-<b>log</b>istic-regression-9e6dc2807772", "snippet": "We still need to take into account <b>negative</b> values. To do so, we take the <b>logarithm</b> of the odds O to get the <b>log</b>-odds l: l = <b>log</b>(O), where l is continuous and has the range (-\u221e, \u221e), and without any <b>loss</b> in generality <b>log</b> is the <b>logarithm</b> in base e. This is exactly the solution we need in order to use the linear regression equation above to model boolean dependent variables using odds and probabilities! All we need to do is to treat d as the <b>log</b>-odds l: l = <b>log</b>(O) = <b>log</b>(P / (1 - P ...", "dateLastCrawled": "2022-02-03T02:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "How <b>To Build Custom Loss Functions In Keras</b> For Any Use Case | cnvrg.io", "url": "https://cnvrg.io/keras-custom-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://cnvrg.io/keras-custom-<b>loss</b>-functions", "snippet": "This <b>loss</b> function is also known as the <b>Log</b> <b>Loss</b> function, because of the <b>logarithm</b> of the <b>loss</b>. Standalone Implementation: You <b>can</b> create an object for Binary Cross Entropy from Keras.losses. Then you have to pass in our true and predicted labels.", "dateLastCrawled": "2022-01-31T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "In linear <b>regression</b>, when is it appropriate to use the <b>log</b> of an ...", "url": "https://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/298", "snippet": "If the variable has <b>negative</b> skew you could firstly invert the variable before <b>taking</b> the <b>logarithm</b>. I&#39;m thinking here particularly of Likert scales that are inputed as continuous variables. While this usually applies to the dependent variable you occasionally have problems with the residuals (e.g. heteroscedasticity) caused by an independent variable which <b>can</b> be sometimes corrected by <b>taking</b> the <b>logarithm</b> of that variable. For example when running a model that explained lecturer ...", "dateLastCrawled": "2022-01-27T11:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[Blog Post #3] Metrics for Regression - Prevision.io", "url": "https://prevision.io/blog/blog-post-3-metrics-for-regression/", "isFamilyFriendly": true, "displayUrl": "https://prevision.io/b<b>log</b>/b<b>log</b>-post-3-metrics-for-regression", "snippet": "RMSLE adds 1 to both <b>actual</b> and predicted values before <b>taking</b> the natural <b>logarithm</b> to avoid <b>taking</b> the natural <b>log</b> of possible 0 (zero) values. In case of RMSLE, you take the <b>log</b> of the predictions and <b>actual</b> values. So basically, what changes is the variance that you are measuring. I believe RMSLE is usually used when you don\u2019t want to penalize huge differences in the predicted and the <b>actual</b> values when both predicted and true values are huge numbers. where: yi are the real values, \u0177i ...", "dateLastCrawled": "2022-01-25T00:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Log</b> and Percent Reductions in Microbiology and Antimicrobial Testing ...", "url": "https://microchemlab.com/information/log-and-percent-reductions-microbiology-and-antimicrobial-testing/", "isFamilyFriendly": true, "displayUrl": "https://microchemlab.com/information/<b>log</b>-and-percent-<b>reduction</b>s-microbio<b>log</b>y-and...", "snippet": "Simply speaking, <b>taking</b> the <b>log</b> value of a large number, such as the number of cells killed in a disinfectant test, transforms it into a smaller one that is easier to work with. Understandably, this \u201cscientist shorthand\u201d often prompts questions concerning how to translate <b>log</b> reductions to percent reductions and vice versa. A series of true mathematical statements showing a pattern is presented below. If you <b>can</b> identify the pattern, then you are well on your way to understanding the ...", "dateLastCrawled": "2022-01-31T09:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "model.compile(<b>loss</b>=&#39;...&#39;, optimizer=opt) # fit model. history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0) Now that we have the basis of a problem and model, we <b>can</b> take a look evaluating three common <b>loss</b> functions that are appropriate for a regression predictive modeling problem.", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Introduction to Optical Fibers, dB, Attenuation and Measurements</b> - <b>Cisco</b>", "url": "https://www.cisco.com/c/en/us/support/docs/optical/synchronous-digital-hierarchy-sdh/29000-db-29000.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.cisco.com</b>/c/en/us/support/docs/optical/synchronous-digital-hierarchy-sdh/...", "snippet": "A decibel is expressed as the base 10 <b>logarithm</b> of the ratio of the power of two signals, as shown here: dB = 10 x <b>Log</b> 10 (P1/P2) where <b>Log</b> 10 is the base 10 <b>logarithm</b>, and P1 and P2 are the powers to <b>be compared</b>. Note: <b>Log</b> 10 is different from the Neparian <b>Logarithm</b> (Ln or LN) base e <b>logarithm</b>. You <b>can</b> also express signal amplitude in dB ...", "dateLastCrawled": "2022-02-03T05:37:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, ... Cross-Entropy (aka <b>log</b> <b>loss</b>): calculates the differences between the predicted class probabilities and those from ground truth across a logarithmic scale. Useful for object detection. Weighted Cross-Entropy: improves on Cross-Entropy accuracy by adding weights to certain aspects (e.g., certain object classes) which are under-represented in the data (e.g., objects occurring in fewer data samples\u00b3 ...", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machined Learnings: ML and OR: An <b>analogy</b> with cost-sensitive ...", "url": "http://www.machinedlearnings.com/2010/07/ml-and-or.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2010/07/ml-and-or.html", "snippet": "Nonetheless I&#39;ve been amusing myself by thinking about it, in particular trying to think about it from a <b>machine</b> <b>learning</b> reduction standpoint. The simplest well-understood reduction that I can think of which is analogous to supplying estimates to a linear program is the reduction of cost-sensitive multiclass classification (CSMC) to regression.", "dateLastCrawled": "2021-12-25T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "ML Concepts. A.I \u2014 Intelligence showed by machines which is common for humans <b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>.We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - What is the relation between a <b>loss</b> function and an ...", "url": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-loss-function-and-an-energy-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-<b>loss</b>...", "snippet": "A <b>loss</b> function is a function that measures the distance between the expected value and the actual value of a model (an example of a <b>loss</b> function is the cross entropy).. An energy function can be defined as a function that we want to minimise or maximise and it is a function of the variables of the system. It is referred to as &quot;energy function&quot; because it is often related or compared to the concept of &quot;energy&quot; in physics. These two expression seem to refer to the same concept.", "dateLastCrawled": "2022-01-17T15:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Shannon <b>entropy</b> in the context of <b>machine</b> <b>learning</b> and AI | by Frank ...", "url": "https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/shannon-<b>entropy</b>-in-the-context-of-<b>machine</b>-<b>learning</b>-and-ai-24...", "snippet": "In <b>machine</b> <b>learning</b>, it is a very useful measure for the similarity of probability distributions and serves as a <b>loss</b> function (more details below). Uses in <b>machine</b> <b>learning</b>", "dateLastCrawled": "2022-01-30T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Analogy</b> between Neural network and naive bayes - Cross Validated", "url": "https://stats.stackexchange.com/questions/219687/analogy-between-neural-network-and-naive-bayes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/219687", "snippet": "w 0 = ln. \u2061. 1 \u2212 \u03c0 \u03c0 + \u2211 i \u03bc i 1 2 \u2212 \u03bc i 0 2 2 \u03c3 i 2. This is exactly the form of logistic regression, where w i are the weights and w 0 is the bias. However, logistic regression (and therefore single-layer neural nets) don&#39;t necessarily impose these forms on the weights/bias, and are therefore more general than naive Bayes.", "dateLastCrawled": "2022-01-26T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>All Unit MCQ questions of ML</b> \u2013 TheCodingShef", "url": "https://thecodingshef.com/all-unit-mcq-questions-of-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thecodingshef.com/<b>all-unit-mcq-questions-of</b>-<b>machine</b>-<b>learning</b>", "snippet": "Correct option is C. Choose the correct option regarding <b>machine</b> <b>learning</b> (ML) and artificial intelligence (AI) ML is a set of techniques that turns a dataset into a software. AI is a software that can emulate the human mind. ML is an alternate way of programming intelligent machines. All of the above. Correct option is D.", "dateLastCrawled": "2022-01-30T22:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Predicting the 2019 All-<b>NBA teams with machine learning</b> - <b>Dribble Analytics</b>", "url": "https://dribbleanalytics.blog/2019/03/ml-all-nba-predict/", "isFamilyFriendly": true, "displayUrl": "https://<b>dribbleanalytics</b>.blog/2019/03/ml-all-nba-predict", "snippet": "<b>Log loss is like</b> accuracy, but instead of analyzing the labeled predictions, it analyzes the prediction probabilities. This is particularly important given that we\u2019re more interested in the probabilities than we are in the actual labels. A \u201cperfect\u201d model will have a log loss of 0. The table below shows each model\u2019s log loss. Model Log loss; SVC: 0.416: RF: 0.416: KNN: 0.403: DNN: 0.43: The SVC and RF have the same log loss, while the KNN has the lowest. Next, let\u2019s look at the ...", "dateLastCrawled": "2022-01-04T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "[OC] Predicting the 2019 All-<b>NBA teams with machine learning</b> : nba", "url": "https://www.reddit.com/r/nba/comments/aw51j6/oc_predicting_the_2019_allnba_teams_with_machine/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../aw51j6/oc_predicting_the_2019_allnba_teams_with_<b>machine</b>", "snippet": "[OC] Predicting the 2019 All-<b>NBA teams with machine learning</b>. Original Content. This post has a lot of graphs. If you don&#39;t want to click on each one individually, they&#39;re all in an imgur album here. There is a tl;dr and summary infographic at the very end. Introduction . Last year, media members unanimously selected LeBron James to the All-NBA first team, giving him a record 12 All-NBA first team selections. However, given the Lakers recent struggles and LeBron&#39;s absence earlier in the ...", "dateLastCrawled": "2021-10-14T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting the 2019 All-NBA teams with <b>machine</b> <b>learning</b>", "url": "https://dribbleanalytics.blogspot.com/2019/03/ml-all-nba-predict.html", "isFamilyFriendly": true, "displayUrl": "https://dribbleanalytics.blogspot.com/2019/03/ml-all-nba-predict.html", "snippet": "Predicting the 2019 All-NBA teams with <b>machine</b> <b>learning</b> Get link; Facebook; Twitter; Pinterest; Email; Other Apps; March 01, 2019 There is a summary at the bottom if you want to skip to the results. Introduction Last year, media members unanimously selected LeBron James to the All-NBA first team, giving him a record 12 All-NBA first team selections. However, given the Lakers recent struggles and LeBron&#39;s absence earlier in the season, LeBron might miss not only the first team but also the ...", "dateLastCrawled": "2021-12-11T07:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What\u2019s considered a good Log <b>Loss</b> in <b>Machine</b> <b>Learning</b> ? | by Federico ...", "url": "https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@fzammito/whats-considered-a-good-log-<b>loss</b>-in-<b>machine</b>-<b>learning</b>-a529...", "snippet": "<b>Log Loss is similar</b> to the Accuracy, but it will favor models that distinguish more strongly the classes. Log <b>Loss</b> it useful to compare models not only on their output but on their probabilistic ...", "dateLastCrawled": "2022-01-30T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>What is an intuitive explanation for the log</b> loss function? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-for-the-log-loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-an-intuitive-explanation-for-the-log</b>-loss-function", "snippet": "Answer (1 of 8): To me an intuitive explanation is that minimizing the log loss equals minimizing the Kullback-Leibler divergence (Kullback\u2013Leibler divergence - Wikipedia) between the function you want to optimize (for example a neural network) and the true function that generates the data (from ...", "dateLastCrawled": "2022-01-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Key techniques for Evaluating <b>Machine</b> <b>Learning</b> models - Data Analytics", "url": "https://vitalflux.com/key-techniques-evaluating-machine-learning-models-performance/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/key-techniques-evaluating-<b>machine</b>-<b>learning</b>-models-performance", "snippet": "Log loss is used to evaluate the performance of classification <b>machine</b> <b>learning</b> models that are built using classification algorithms such as logistic regression, support vector <b>machine</b> (SVM), random forest, and gradient boosting. The idea behind the use of <b>Log loss is similar</b> to taking a base-e exponential or natural logarithm in order to compare model scores from high-value functions which may indicate poor <b>machine</b> <b>learning</b> model performance. The logarithmic loss value is defined as ...", "dateLastCrawled": "2022-01-31T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss In Machine Learning</b> - 02/2021 - Course f", "url": "https://www.coursef.com/loss-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.coursef.com/<b>loss-in-machine-learning</b>", "snippet": "<b>Log Loss is similar</b> to the Accuracy, but it will favor models that ... Two of the most popular loss functions in <b>machine</b> <b>learning</b> are the 0-1 loss function and the quadratic loss function. The 0-1 loss function is an indicator function that returns 1 when the target and output are not equal and zero otherwise: 0-1 Loss: The quadratic loss is a commonly used symmetric loss \u2026 161 People Used View all course \u203a\u203a Visit Site \u2039 1; 2 \u203a FAQs. Do online classes have tests? Not all online ...", "dateLastCrawled": "2021-02-08T01:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Diagnosing malaria from some symptoms: a <b>machine</b> <b>learning</b> approach and ...", "url": "https://link.springer.com/article/10.1007/s12553-020-00488-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12553-020-00488-5", "snippet": "<b>Machine</b> <b>learning</b> tools have become available in the diagnosis and prediction of diseases, thereby saving costs and improving the likelihood of survivorship, especially in some terminal diseases. In the case of infectious diseases, early diagnosis is highly needed in isolating the subjects to reduce the spread of the disease. Researchers continue to propose new data mining tools that help in the early diagnosis of diseases, reducing the mortality rate, and improving the quality of life of ...", "dateLastCrawled": "2021-12-03T05:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(log loss)  is like +(taking the negative logarithm of the actual error rate)", "+(log loss) is similar to +(taking the negative logarithm of the actual error rate)", "+(log loss) can be thought of as +(taking the negative logarithm of the actual error rate)", "+(log loss) can be compared to +(taking the negative logarithm of the actual error rate)", "machine learning +(log loss AND analogy)", "machine learning +(\"log loss is like\")", "machine learning +(\"log loss is similar\")", "machine learning +(\"just as log loss\")", "machine learning +(\"log loss can be thought of as\")", "machine learning +(\"log loss can be compared to\")"]}