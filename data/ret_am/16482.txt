{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>DECISION</b> <b>TREE</b> ALGORITHM FOR MDP", "url": "https://openreview.net/pdf?id=Yr_1QZaRqmv", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=Yr_1QZaRqmv", "snippet": "Linear Programming (LP), rather than iterative methods to compute the <b>state-action</b> <b>value</b> <b>function</b>; Second, the use of <b>decision</b> trees to partition the state space and codify the optimal policy rather than using deep neural networks. In Figure 1 we show the subset of problems we aim to tackle in the gray area. That is, problems with large enough ...", "dateLastCrawled": "2022-01-01T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Decision</b> <b>Tree</b> <b>Function</b> Approximation in Reinforcement Learning", "url": "https://web.cecs.pdx.edu/~mperkows/CLASS_ROBOTICS/FEBR26-2004/ROBOT-DECISION-TREE/pyeatt98decision-tree-robotics.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.cecs.pdx.edu/~mperkows/CLASS_ROBOTICS/FEBR26-2004/ROBOT-<b>DECISION</b>-<b>TREE</b>/...", "snippet": "<b>Decision</b> <b>Tree</b> <b>Function</b> Approximation in Reinforcement Learning Larry D. Pyeatt Texas Tech University Lubbock, Texas 79409 larry.pyeatt@ttu.edu Adele E. Howe Colorado State University Fort Collins, CO 80523 howe@cs.colostate.edu Abstract The goal in reinforcement learning is to learn the <b>value</b> of taking each action from each possible state in order to maximize the total reward. In scaling reinforcement learning to problems with large numbers of states and/or actions, the representation of the ...", "dateLastCrawled": "2022-01-03T22:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Value</b> <b>Function</b> Approximation - Tulane University", "url": "http://www.cs.tulane.edu/~zzheng3/teaching/cmps6660/fall20/vapprox.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.tulane.edu/~zzheng3/teaching/cmps6660/fall20/vapprox.pdf", "snippet": "\u2022Or every <b>state-action</b> pair #,(has an entry )(#,() ... \u2022<b>Decision</b> <b>tree</b> \u2022Nearest neighbor \u2022Fourier / wavelet bases \u2022... 7. Nonlinear <b>Value</b> <b>Function</b> Approximation \u2022Use artificial neural networks 8 S v\u02c6(S, w) state <b>Value</b> w parameter. Which <b>Function</b> Approximator? \u2022We consider differentiablefunction approximators, e.g. \u2022Linear combinations of features \u2022Neural network \u2022<b>Decision</b> <b>tree</b> \u2022Nearest neighbor \u2022Fourier / wavelet bases \u2022... \u2022Furthermore, we require a training ...", "dateLastCrawled": "2022-01-21T03:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Reinforcement Learning</b> - Carnegie Mellon University", "url": "https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture26-ri.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture26-ri.pdf", "snippet": "Markov <b>Decision</b> Processes: State-<b>Value</b> <b>function</b>, Action-<b>Value</b> <b>Function</b> Bellman Equation Policy Evaluation, Policy Improvement, Optimal Policy Dynamical programming: Policy Iteration <b>Value</b> Iteration Modell Free methods: MC <b>Tree</b> search TD Learning . RL Books . 4 Introduction to <b>Reinforcement Learning</b> . 5 <b>Reinforcement Learning</b> Applications Finance Portfolio optimization Trading Inventory optimization Control Elevator, Air conditioning, power grid, \u2026 Robotics Games Go, Chess, Backgammon ...", "dateLastCrawled": "2022-02-03T08:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Reinforcement Learning Basics With Examples (Markov Chain and <b>Tree</b> ...", "url": "https://neptune.ai/blog/reinforcement-learning-basics-markov-chain-tree-search", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/reinforcement-learning-basics-markov-chain-<b>tree</b>-search", "snippet": "<b>Value</b> \u2013 determines how good a <b>state-action</b> pair is, i.e. this <b>function</b> attempts to find a policy that can help maximize the returns. Q-<b>value</b> \u2013 maps <b>state-action</b> pairs to rewards. This refers to the long-term impact of an action taken under a policy in a certain state.", "dateLastCrawled": "2022-01-31T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Decision</b> <b>Tree Function Approximation in Reinforcement Learning</b>", "url": "https://www.researchgate.net/publication/2406466_Decision_Tree_Function_Approximation_in_Reinforcement_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2406466_<b>Decision</b>_<b>Tree</b>_<b>Function</b>_Approximation...", "snippet": "In scaling reinforcement learning to problems with large numbers of states and/or actions, the representation of the <b>value</b> <b>function</b> becomes critical. We present a <b>decision</b> <b>tree</b> based approach to ...", "dateLastCrawled": "2021-12-03T20:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Lecture 6: <b>Value</b> <b>Function</b> Approximation - David Silver", "url": "https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf", "snippet": "So far we have represented <b>value</b> <b>function</b> by a lookup table Every state s has an entry V(s) Or every <b>state-action</b> pair s;a has an entry Q(s;a) Problem with large MDPs: There are too many states and/or actions to store in memory It is too slow to learn the <b>value</b> of each state individually Solution for large MDPs: Estimate <b>value</b> <b>function</b> with <b>function</b> approximation v^(s;w) \u02c7v \u02c7(s) or ^q(s;a;w) \u02c7q \u02c7(s;a) Generalise from seen states to unseen states Update parameter w using MC or TD learning ...", "dateLastCrawled": "2022-01-30T11:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Tree</b> Based Discretization for Continuous State Space Reinforcement Learning", "url": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-109.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-109.pdf", "snippet": "<b>value</b> <b>function</b>, V (s). The Q <b>function</b> is a <b>function</b> from <b>state/action</b> pairs to an expected sum of discounted reward (Watkins &amp; Dayan 1992). Continuous U <b>Tree</b> U <b>Tree</b> (McCallum 1995) includes a method to apply propositional <b>decision</b> <b>tree</b> techniques to reinforcement learning. We introduce an extension to U <b>Tree</b> that is capa-ble of directly handling both propositional and continuous-valued domains. Our resulting algorithm, \u201cContinuous U <b>Tree</b>\u201d, uses <b>decision</b> <b>tree</b> learning techniques (Breiman ...", "dateLastCrawled": "2022-01-12T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Conservative Q-Improvement: Reinforcement Learning</b> for an ...", "url": "https://www.researchgate.net/publication/334192771_Conservative_Q-Improvement_Reinforcement_Learning_for_an_Interpretable_Decision-Tree_Policy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/334192771_Conservative_Q-Improvement...", "snippet": "Existing methods for creating <b>decision</b> <b>tree</b> policies via reinforcement learning focus on accurately representing an action-<b>value</b> <b>function</b> during training, but this leads to much larger trees than ...", "dateLastCrawled": "2021-08-05T10:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>SARSA Reinforcement Learning - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/sarsa-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.geeksforgeeks.org/sarsa-reinforcement-learning", "snippet": "<b>Like</b> Article. SARSA Reinforcement Learning. Last Updated : 24 Jun, 2021. Prerequisites: Q-Learning technique SARSA algorithm is a slight variation of the popular Q-Learning algorithm. For a learning agent in any Reinforcement Learning algorithm it\u2019s policy can be of two types:- On Policy: In this, the learning agent learns the <b>value</b> <b>function</b> according to the current action derived from the policy currently being used. Off Policy: In this, the learning agent learns the <b>value</b> <b>function</b> ...", "dateLastCrawled": "2022-01-30T20:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Reinforcement Learning</b> - Carnegie Mellon University", "url": "https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture26-ri.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture26-ri.pdf", "snippet": "Markov <b>Decision</b> Processes: State-<b>Value</b> <b>function</b>, Action-<b>Value</b> <b>Function</b> Bellman Equation Policy Evaluation, Policy Improvement, Optimal Policy Dynamical programming: Policy Iteration <b>Value</b> Iteration Modell Free methods: MC <b>Tree</b> search TD Learning . RL Books . 4 Introduction to <b>Reinforcement Learning</b> . 5 <b>Reinforcement Learning</b> Applications Finance Portfolio optimization Trading Inventory optimization Control Elevator, Air conditioning, power grid, \u2026 Robotics Games Go, Chess, Backgammon ...", "dateLastCrawled": "2022-02-03T08:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A <b>DECISION</b> <b>TREE</b> ALGORITHM FOR MDP", "url": "https://openreview.net/pdf?id=Yr_1QZaRqmv", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=Yr_1QZaRqmv", "snippet": "<b>state-action</b> <b>value</b> <b>function</b> can be updated locally without having to handle the whole <b>state-action</b> space at once as in exact methods; Second, the non-linear parameterization of the <b>state-action</b> <b>value</b> <b>function</b> and the policy <b>function</b> with deep neural networks, where in a sense, the state space model is \u201clearned\u201d and compressed without having to update and explore the whole state space. While successful methods, they are not devoid of drawbacks. Their strength in not taking into account ...", "dateLastCrawled": "2022-01-01T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reinforcement Learning Basics With Examples (Markov Chain and <b>Tree</b> ...", "url": "https://neptune.ai/blog/reinforcement-learning-basics-markov-chain-tree-search", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/reinforcement-learning-basics-markov-chain-<b>tree</b>-search", "snippet": "<b>Value</b> \u2013 determines how good a <b>state-action</b> pair is, i.e. this <b>function</b> attempts to find a policy that can help maximize the returns. Q-<b>value</b> \u2013 maps <b>state-action</b> pairs to rewards. This refers to the long-term impact of an action taken under a policy in a certain state.", "dateLastCrawled": "2022-01-31T02:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Supervised Reinforcement Learning via <b>Value</b> <b>Function</b>", "url": "https://res.mdpi.com/symmetry/symmetry-11-00590/article_deploy/symmetry-11-00590.pdf", "isFamilyFriendly": true, "displayUrl": "https://res.mdpi.com/symmetry/symmetry-11-00590/article_deploy/symmetry-11-00590.pdf", "snippet": "The State <b>value</b> <b>function</b> vs and the <b>State-action</b> <b>value</b> <b>function</b> Qsa are collectively called <b>value</b> <b>function</b>. In recent years, DQN [2] is a breakthrough in RL via <b>value</b> <b>function</b>, which extends state space from finite discrete space to infinite continuous space by combining deep neural network with Q-learning [17] algorithm. The principle of DQN ...", "dateLastCrawled": "2022-02-01T01:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Planning and Learning in Model-Based Reinforcement Learning Methods | Zero", "url": "https://xlnwel.github.io/blog/reinforcement%20learning/planning/", "isFamilyFriendly": true, "displayUrl": "https://xlnwel.github.io/blog/reinforcement learning/planning", "snippet": "Background planning simulates experiences to update the <b>value</b> <b>function</b> and policy, while <b>decision</b>-time planning simulates experiences to select an action for a specific state. The former is preferable if low latency action selection is the priority, and the latter is most useful in applications in which fast responses are not required, such as chess playing programs. Table of Contents. Background Planning. Dyna-Q; Dyna-Q+; Prioritized Sweeping; Trajectory Sampling; Real-Time Dynamic ...", "dateLastCrawled": "2022-01-04T00:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Tree</b> Based Discretization for Continuous State Space Reinforcement Learning", "url": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-109.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.aaai.org/Papers/AAAI/1998/AAAI98-109.pdf", "snippet": "<b>value</b> <b>function</b>, V (s). The Q <b>function</b> is a <b>function</b> from <b>state/action</b> pairs to an expected sum of discounted reward (Watkins &amp; Dayan 1992). Continuous U <b>Tree</b> U <b>Tree</b> (McCallum 1995) includes a method to apply propositional <b>decision</b> <b>tree</b> techniques to reinforcement learning. We introduce an extension to U <b>Tree</b> that is capa-ble of directly handling both propositional and continuous-valued domains. Our resulting algorithm, \u201cContinuous U <b>Tree</b>\u201d, uses <b>decision</b> <b>tree</b> learning techniques (Breiman ...", "dateLastCrawled": "2022-01-12T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Decision</b> <b>Tree</b> <b>Function</b> Approximation in Reinforcement Learning", "url": "https://web.cecs.pdx.edu/~mperkows/CLASS_ROBOTICS/FEBR26-2004/ROBOT-DECISION-TREE/pyeatt98decision-tree-robotics.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.cecs.pdx.edu/~mperkows/CLASS_ROBOTICS/FEBR26-2004/ROBOT-<b>DECISION</b>-<b>TREE</b>/...", "snippet": "<b>Decision</b> <b>Tree</b> <b>Function</b> Approximation in Reinforcement Learning Larry D. Pyeatt Texas Tech University Lubbock, Texas 79409 larry.pyeatt@ttu.edu Adele E. Howe Colorado State University Fort Collins, CO 80523 howe@cs.colostate.edu Abstract The goal in reinforcement learning is to learn the <b>value</b> of taking each action from each possible state in order to maximize the total reward. In scaling reinforcement learning to problems with large numbers of states and/or actions, the representation of the ...", "dateLastCrawled": "2022-01-03T22:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Rule <b>Abstraction and Transfer in Reinforcement Learning by Decision Tree</b>", "url": "http://www.robot.t.u-tokyo.ac.jp/~yamashita/paper/B/B085Final.pdf", "isFamilyFriendly": true, "displayUrl": "www.robot.t.u-tokyo.ac.jp/~yamashita/paper/B/B085Final.pdf", "snippet": "with corresponding <b>value</b> <b>function</b> v we use the form that If state is A and action is B then its a Good Choice It will be shown later that this form is convenient when <b>decision</b> <b>tree</b> is used to generate the rule base autonomously. We generate rules from agents\u2019 experienced <b>state-action</b> pairs. To archive autonomous rule generation, <b>decision</b> <b>tree</b>", "dateLastCrawled": "2021-08-30T04:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Function Approximation</b> in Reinforcement Learning | by Ziad SALLOUM ...", "url": "https://towardsdatascience.com/function-approximation-in-reinforcement-learning-85a4864d566", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>function-approximation</b>-in-reinforcement-learning-85a...", "snippet": "The aim is to use these set of features to generalise the estimation of the <b>value</b> at states that have <b>similar</b> features. We used the word estimation to indicate that this approach will never find the true <b>value</b> of a state, but an <b>approximation</b> of it. Despite this seemingly inconvenient result, however this will achieve faster computation and much more generalisations. The methods that compute these approximations are called <b>Function</b> Approximators. There are many <b>function</b> approximators: Linear ...", "dateLastCrawled": "2022-02-03T11:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "reinforcement learning - Is Monte Carlo <b>Tree</b> Search appropriate for ...", "url": "https://ai.stackexchange.com/questions/9909/is-monte-carlo-tree-search-appropriate-for-problems-with-large-state-and-action", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/9909/is-monte-carlo-<b>tree</b>-search-appropriate-for...", "snippet": "$\\begingroup$ Do <b>similar</b> actions yield <b>similar</b> results? I.E. if I choose between two actions that appear <b>similar</b>, will they have very <b>similar</b> <b>value</b> in the long run? If so, it may be appropriate to embed the actions in some continuous space in the agent&#39;s policy, then perform an MCTS on the k-nearest neighbors (which are valid actions, of course) to the desired continuous proto-action.", "dateLastCrawled": "2022-01-15T12:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A <b>Q-learning agent</b> for <b>automated trading</b> in equity stock markets ...", "url": "https://www.sciencedirect.com/science/article/pii/S0957417420305856", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0957417420305856", "snippet": "The optimal <b>state-action</b>-<b>value</b> <b>function</b> Q ... and RL solves MDP. Stock market trading <b>can</b> <b>be thought</b> of as a sequential <b>decision</b>-making problem as at every duration of the stock trading session a trader has to make a trading <b>decision</b>. Our target market is the Equity stock market. We have performed experimentation on the Indian and the American Equity stock markets. The proposed models follow the rules of trading in the Equity stock market. We <b>can</b> take a short position (first sell and buy ...", "dateLastCrawled": "2022-01-10T21:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Data Science Interview Questions for IT Industry</b> Part-5: Reinforcement ...", "url": "https://thinkingneuron.com/data-science-interview-questions-reinforcement-ml/", "isFamilyFriendly": true, "displayUrl": "https://thinkingneuron.com/data-science-interview-questions-reinforcement-ml", "snippet": "The estimation of state-<b>value</b> <b>function</b>(V) or action-<b>value</b> <b>function</b>(q) is known a prediction problem in reinforcement learning. As discussed in the previous section, Monte Carlo finds out <b>value</b> for a given state by averaging the returns of that state in multiple episodes. Since a state <b>can</b> be visited more than twice in a single episode.", "dateLastCrawled": "2022-02-01T02:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "3 Pillers of Reinforcement Learning", "url": "https://www.asjadk.io/untitled-4/", "isFamilyFriendly": true, "displayUrl": "https://www.asjadk.io/untitled-4", "snippet": "A <b>value</b> <b>function</b> is an estimate of expected cumulative future reward, usually as a <b>function</b> of state or <b>state-action</b> pair. The reward may be discounted, with lesser weight being given to delayed reward, or it may be cumulative only within individual episodes of interaction with the environment. Finally, in the average-reward case, the values are all relative to the mean reward received when following the current policy.", "dateLastCrawled": "2022-01-27T18:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Introduction to <b>Reinforcement Learning</b> (DDPG and TD3) for News ...", "url": "https://towardsdatascience.com/reinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning</b>-ddpg-and-td3-for-news...", "snippet": "So the <b>Value</b> <b>Function</b> does not imply a <b>function</b> that estimates the Reward. <b>Value</b> <b>can</b> only mean how good the action is for the current state. Although, as you will see, it does not necessarily mean \u2018give me the reward\u2019 for that action and state. It is a more abstract measure of \u2018goodness\u2019 and <b>can</b> be expressed as a continuous real-valued ...", "dateLastCrawled": "2022-02-02T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Learning attack mechanisms in <b>Wireless Sensor Networks</b> using Markov ...", "url": "https://www.sciencedirect.com/science/article/pii/S0957417419300235", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0957417419300235", "snippet": "The <b>state-action</b> <b>value</b> <b>function</b> Q ... Thus, RNNs are able to store information about the past inputs on this hidden state, which <b>can</b> <b>be thought</b> of as a feedback loop. The main problem with RNNs is that these networks are hard to train. There has been a significant effort addressed to alleviate this (Sutskever, 2013). Nowadays, the main architecture used to implement a RNN is the LSTM (long-short term memory), owed to Hochreiter and Schmidhuber (1997). Other structures, such as GRU, have been ...", "dateLastCrawled": "2021-10-17T15:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Decision</b> Trees for <b>Decision</b> Making - <b>hbr.org</b>", "url": "https://hbr.org/1964/07/decision-trees-for-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>hbr.org</b>/1964/07/<b>decision</b>-<b>trees</b>-for-<b>decision</b>-making", "snippet": "The <b>decision</b> <b>tree</b> <b>can</b> clarify for management, as <b>can</b> no other analytical tool that I know of, the choices, risks, objectives, monetary gains, and information needs involved in an investment ...", "dateLastCrawled": "2022-02-02T21:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Notes On Reinforcement Learning - GitHub Pages", "url": "https://wuciawe.github.io/machine%20learning/math/2018/12/19/notes-on-reinforcement-learning.html", "isFamilyFriendly": true, "displayUrl": "https://wuciawe.github.io/machine learning/math/2018/12/19/notes-on-reinforcement...", "snippet": "The <b>value</b> <b>function</b> <b>can</b> be decomposed into two parts: immediate reward \\(R_{t+1}\\) and discounted <b>value</b> of successor state \\(\\gamma v(S_{t+1})\\): The Bellman equation <b>can</b> be expressed concisely using matrices, where \\(v\\) is a column vector with one entry per state. The Bellman equation is a linear equation, it <b>can</b> be solved directly, \\(v = (I - \\gamma P)^{-1}R\\). For \\(n\\) states, the computational complexity is \\(O(n^3)\\). Direct solution only possible for small MRPs, for large MRPs, there ...", "dateLastCrawled": "2021-12-20T22:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "comparison - What&#39;s the difference between <b>model</b>-free and <b>model-based</b> ...", "url": "https://ai.stackexchange.com/questions/4456/whats-the-difference-between-model-free-and-model-based-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/4456", "snippet": "In practice, a <b>model</b>-free algorithm either estimates a &quot;<b>value</b> <b>function</b>&quot; or the &quot;policy&quot; directly from experience (that is, the interaction between the agent and environment), without using neither the transition <b>function</b> nor the reward <b>function</b>. A <b>value</b> <b>function</b> <b>can</b> <b>be thought</b> of as a <b>function</b> which evaluates a state (or an action taken in a state), for all states. From this <b>value</b> <b>function</b>, a policy <b>can</b> then be derived.", "dateLastCrawled": "2022-01-27T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Convergent reinforcement learning with <b>value</b> <b>function</b> ...", "url": "https://www.academia.edu/1333663/Convergent_reinforcement_learning_with_value_function_interpolation", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/1333663/Convergent_reinforcement_learning_with_<b>value</b>_<b>function</b>...", "snippet": "Therefore algorithm 7 <b>can</b> <b>be thought</b> of as performing a &quot;diagonal&quot; approximation to limt^T^Qo- Although, asynchronicity does not follow directly from the form of 7, using Q-learning as an example, one <b>can</b> see that the form of the algorithm does allow the dif- ferent components of the <b>value</b> <b>function</b> to be updated at different rates - hence the algorithm allows asyn- chronous updates. In (Szepesvari &amp; Littman, 1999) a general convergence result is presented for this algorithm. In the same ar ...", "dateLastCrawled": "2022-01-29T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Markov Decision Processes and Reinforcement Learning</b>", "url": "https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://danieltakeshi.github.io/2015-08-02-<b>markov-decision-processes-and-reinforcement</b>...", "snippet": "A key observation that will help: the <b>value</b> <b>function</b> \\(V(b)\\) over belief states is piecewise linear and convex, because it is the maximum of a collection of hyperplanes. What a modified <b>value</b> iteration algorithm needs to do is to accumulate a set of possible plans, and for a given belief state, compute which of those has the optimal hyperplane. But this is too slow so in practice, people use online algorithms based on one-step lookahead, inspired by Dynamic Bayesian Networks.", "dateLastCrawled": "2022-01-30T21:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement Learning with Non-Conventional <b>Value</b> <b>Function</b> Approximation", "url": "https://project-archive.inf.ed.ac.uk/msc/20214719/msc_proj.pdf", "isFamilyFriendly": true, "displayUrl": "https://project-archive.inf.ed.ac.uk/msc/20214719/msc_proj.pdf", "snippet": "The estimation of the <b>value</b> <b>function</b> <b>can</b> either be done explicitly for each possible state or <b>state/action</b> pair, or implicitly through <b>function</b> approximation. The focus of this project is on the latter. <b>Function</b> Approximation (FA) in RL aims in establishing a generalisable relationship between the agent\u2019s simulated experience and the <b>value</b> of ...", "dateLastCrawled": "2022-01-26T03:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Decision</b> <b>Tree Function Approximation in Reinforcement Learning</b>", "url": "https://www.researchgate.net/publication/2406466_Decision_Tree_Function_Approximation_in_Reinforcement_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2406466_<b>Decision</b>_<b>Tree</b>_<b>Function</b>_Approximation...", "snippet": "We find that the <b>decision</b> <b>tree</b> <b>can</b> provide better learning performance than the neural network <b>function</b> approximation and <b>can</b> solve large problems that are infeasible using table lookup. 1 ...", "dateLastCrawled": "2021-12-03T20:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Computer Science Technical Report <b>Decision</b> <b>Tree</b> <b>Function</b> Approximation ...", "url": "https://www.cs.colostate.edu/pubserv/pubs/Pyeatt-TechReports-Reports-1998-tr98-112.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.colostate.edu/pubserv/pubs/Pyeatt-TechReports-Reports-1998-tr98-112.pdf", "snippet": "the <b>decision</b> <b>tree</b> <b>can</b> provide better learning performance than the neural network <b>function</b> approximation and <b>can</b> solve large problems that are infeasible using table lookup. 1 Motivation A popular approach for estimating the <b>value</b> <b>function</b> in reinforcement learning is the table lookup method. This approach is guaranteed to converge, subject to some restric-tions on the learning parameters [2]. However, table lookup does not scale well with the number of inputs, although some variations of ...", "dateLastCrawled": "2021-11-29T02:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is <b>value</b> of a <b>function</b>?", "url": "https://psichologyanswers.com/library/lecture/read/57841-what-is-value-of-a-function", "isFamilyFriendly": true, "displayUrl": "https://psichologyanswers.com/library/lecture/read/57841-what-is-<b>value</b>-of-a-<b>function</b>", "snippet": "The optimal <b>Value</b> <b>function</b> is one which yields maximum <b>value</b> <b>compared</b> to all other <b>value</b> <b>function</b>. When we say we are solving an MDP it actually means we are finding the Optimal <b>Value</b> <b>Function</b>. So, mathematically Optimal State-<b>Value</b> <b>Function</b> <b>can</b> be expressed as : Optimal State-<b>Value</b> <b>Function</b>. What is state <b>value</b> <b>function</b>? State-<b>Value</b>-<b>Function</b>. Following a policy p the state-<b>value</b>-<b>function</b> returns the <b>value</b>, i.e. the expected return for selecting a certain state s. Return means the overall ...", "dateLastCrawled": "2022-01-15T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Adaptive Reinforcement Learning Method for Sequential <b>Decision</b> Task: A ...", "url": "https://www.ijsr.net/archive/v4i5/SUB154685.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijsr.net/archive/v4i5/SUB154685.pdf", "snippet": "Keywords: Reinforcement Learning, <b>Decision</b> policy, <b>state-action</b> <b>function</b>, Q-Learning, Temporal Difference Learning. 1. Introduction . Computer scientists are interested in developing devices and programs based on the life science by studding its engineering. These studies are based on \u201csynthetic learning\u201d which has produced various methods and mathematical theories for pattern classification, prediction and adaptive control of dynamic systems. The problem still arises in synthetic ...", "dateLastCrawled": "2022-01-14T07:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A <b>Q-learning agent</b> for <b>automated trading</b> in equity stock markets ...", "url": "https://www.sciencedirect.com/science/article/pii/S0957417420305856", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0957417420305856", "snippet": "The optimal <b>state-action</b>-<b>value</b> <b>function</b> Q ... 58.38%, and 24.22% higher <b>compared</b> to the <b>Decision</b> <b>Tree</b> model, Buy-and-Hold model, and the proposed model 1 on the test dataset of index stocks NASDAQ respectively. It also concludes that the percentage accumulated return of the proposed model 2 is 133.49%, and 9.62% higher <b>compared</b> to the <b>Decision</b> <b>Tree</b> model, and Buy-and-Hold model on the test dataset of index stocks DJIA respectively. Similarly, 18.45% and 6.34% for index stocks NIFTY. Equally ...", "dateLastCrawled": "2022-01-10T21:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Lane\u2010changing <b>decision</b> method based Nash Q\u2010learning with considering ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-its.2020.0427", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-its.2020.0427", "snippet": "Rule-based <b>decision</b> models <b>can</b> only be limited to specific conditions, such as following, lane changing and emergency braking ... <b>State-action</b> <b>value</b> <b>function</b>: In the process of driving, when an action is taken, such as changing lanes, it needs a certain time to execute. In this paper, the planning horizon N P = 2.5 s corresponds to the action time. In the lane change <b>decision</b>-making process, as shown in Fig. 7d, the action and state sequence of the ego vehicle is as follows: A t = a t, a t ...", "dateLastCrawled": "2022-01-29T18:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Markov <b>Decision</b> Processes", "url": "https://www.ccs.neu.edu/home/rplatt/cs5335_fall2017/slides/mdps.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ccs.neu.edu/home/rplatt/cs5335_fall2017/slides/mdps.pdf", "snippet": "Output: <b>value</b> <b>function</b>, V 1. let 2. for i=1 to infinity 3. for all 4. 5. if V converged, then break Notice this OR: <b>can</b> solve for <b>value</b> <b>function</b> as the sol&#39;n to a system of linear equations \u2013 <b>can</b>&#39;t do this for <b>value</b> iteration because of the maxes", "dateLastCrawled": "2022-02-02T04:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "reinforcement learning - Is Monte Carlo <b>Tree</b> Search appropriate for ...", "url": "https://ai.stackexchange.com/questions/9909/is-monte-carlo-tree-search-appropriate-for-problems-with-large-state-and-action", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/9909/is-monte-carlo-<b>tree</b>-search-appropriate-for...", "snippet": "The most obvious way would be if you <b>can</b> define meaningful features for actions (or <b>state-action</b> pairs). Standard Reinforcement Learning approaches (with <b>function</b> approximation, maybe linear or maybe Deep Neural Networks) <b>can</b> then relatively &quot;easily&quot; generalize in a meaningful way across lots of actions. They <b>can</b> also be combined with MCTS in various ways (see for example AlphaGo Zero / Alpha Zero).", "dateLastCrawled": "2022-01-15T12:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "POMCPOW: An online <b>algorithm for POMDPs with continuous state, action</b> ...", "url": "https://deepai.org/publication/pomcpow-an-online-algorithm-for-pomdps-with-continuous-state-action-and-observation-spaces", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/pomcpow-an-online-<b>algorithm-for-pomdps-with-continuous</b>...", "snippet": "Adaptive belief <b>tree</b> (ABT) was designed specifically to accommodate changes in the environment without having to replan from scratch ... The Markov <b>decision</b> process (MDP) and partially observable Markov <b>decision</b> process (POMDP) <b>can</b> represent a wide range of sequential <b>decision</b> making problems. In a Markov <b>decision</b> process, an agent takes actions that affect the state of the system and seeks to maximize the expected <b>value</b> of the rewards it collects [Kochenderfer2015]. Formally, an MDP is ...", "dateLastCrawled": "2022-01-26T14:52:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Relationship between state (V) and action(Q) <b>value</b> <b>function</b> in ...", "url": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and-action-q-value-function-in-reinforcement-learning-bb9a988c0127", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and...", "snippet": "<b>Value</b> <b>function</b> can be defined as the expected <b>value</b> of an agent in a certain state. There are two types of <b>value</b> functions in RL: State-<b>value</b> and action-<b>value</b>. It is important to understand the\u2026", "dateLastCrawled": "2022-02-03T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Value</b>. State-<b>value</b> <b>function</b> v_\u03c0: gives us the <b>value</b> of a state under \u03c0; Action-<b>value</b> <b>function</b> q_\u03c0: gives us the <b>value</b> of an action under \u03c0. q_\u03c0 is referred to as the Q-<b>function</b>, and the output from the <b>function</b> for any given <b>state-action</b> pair is called a Q-<b>value</b>.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Teaching machines to behave: Reinforcement <b>Learning</b> | by Diego Gomez ...", "url": "https://towardsdatascience.com/rl-4-all-1-3edac941fe37", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/rl-4-all-1-3edac941fe37", "snippet": "Notice that, as in all <b>machine</b> <b>learning</b> sub-fields, RL programs are not coded explicitly to perform optimally in some given task. ... Similarly, a* is the optimal <b>state-action</b>-<b>value</b> <b>function</b>, obtained if followed an optimal policy \u03c0*. Optimal <b>value</b> functions: Obtained when following a policy \u03c0 that maximizes v(s) and a(s,a) Assuming that the optimal <b>value</b> functions v* and a* are known, but the optimal policy is not, it is possible to build an optimal policy in the following ways: When v*(s ...", "dateLastCrawled": "2022-01-08T13:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Value</b>-<b>Function</b>-<b>Based Transfer for Reinforcement Learning</b> Using ...", "url": "https://www.researchgate.net/publication/221604435_Value-Function-Based_Transfer_for_Reinforcement_Learning_Using_Structure_Mapping", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221604435_<b>Value</b>-<b>Function</b>-Based_Transfer_for...", "snippet": "chological and computational theory about <b>analogy</b> making, ... the form of a <b>state-action</b> <b>value</b> <b>function</b>, or a q-<b>functio n</b>. A. q-<b>function</b> q: S \u00d7 A 7\u2192 R maps from <b>state-action</b> pairs to. real ...", "dateLastCrawled": "2021-10-16T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine learning for biochemical engineering: A</b> review - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1369703X21001303", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1369703X21001303", "snippet": "<b>Value</b>-based algorithms, typically represented by Q-<b>learning</b>, explicitly learn and optimise the <b>state-action</b> <b>value</b> <b>function</b> and generate the optimal policy by acting greedily with respect to it i.e. choosing the control corresponding to the maximum Q \u03c0 x, u <b>value</b> (<b>state-action</b> <b>value</b>). There are also hybrid algorithms, such as actor-critic methods, which combine policy optimisation methods and <b>value</b>-based methods. Although RL has shown success in game-based control benchmarks, such as AlphaGo", "dateLastCrawled": "2022-01-26T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "AI and Reinforcement <b>Learning</b> \u2014 Machines that Learn through Experience ...", "url": "https://www.cantorsparadise.com/ai-and-reinforcement-learning-machines-that-learn-through-experience-e7eea7bb6765", "isFamilyFriendly": true, "displayUrl": "https://www.cantorsparadise.com/ai-and-reinforcement-<b>learning</b>-<b>machines</b>-that-learn...", "snippet": "To align the policy with the updated <b>value</b> <b>function</b>, the algorithm modifies the policy so it would greedily follow the <b>value</b> <b>function</b> (meaning, choosing to perform actions that has the highest <b>value</b>). The algorithm continues by generating a new episode, now under the improved policy, which, in turn, derives a more accurate <b>value</b> estimation and so on. In this process, both the policy and the <b>value</b> <b>function</b> converge to their optimal values, until sufficient accuracy is reached, or when no more ...", "dateLastCrawled": "2022-01-25T17:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning Analogy for Meditation (illustrated</b>) - LessWrong 2.0 ...", "url": "https://www.greaterwrong.com/posts/chHhuCvmZqYLM32gz/machine-learning-analogy-for-meditation-illustrated", "isFamilyFriendly": true, "displayUrl": "https://www.greaterwrong.com/posts/chHhuCvmZqYLM32gz/<b>machine</b>-<b>learning</b>-<b>analogy</b>-for...", "snippet": "<b>Machine Learning Analogy for Meditation (illustrated</b>) ... and the algorithm we use includes a <b>value</b> table: [picture: table, actions on x-axis, states on y-axis, cells of table are estimated values of taking actions in states] A <b>value</b> isn\u2019t just the learned estimate of the immediate reward which you get by taking an action in a state, but rather, the estimate of the eventual rewards, in total, from that action. This makes the values difficult to estimate. An estimate is improved by <b>value</b> it", "dateLastCrawled": "2022-01-17T23:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Reinforcement Learning: Prediction, Control and</b> <b>Value</b> <b>Function</b> ...", "url": "https://deepai.org/publication/reinforcement-learning-prediction-control-and-value-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>reinforcement-learning-prediction-control-and</b>-<b>value</b>...", "snippet": "<b>Reinforcement Learning: Prediction, Control and Value Function Approximation</b>. With the increasing power of computers and the rapid development of self-<b>learning</b> methodologies such as <b>machine</b> <b>learning</b> and artificial intelligence, the problem of constructing an automatic Financial Trading Systems (FTFs) becomes an increasingly attractive research ...", "dateLastCrawled": "2022-01-16T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>value</b> of a <b>function</b>?", "url": "https://psichologyanswers.com/library/lecture/read/57841-what-is-value-of-a-function", "isFamilyFriendly": true, "displayUrl": "https://psichologyanswers.com/library/lecture/read/57841-what-is-<b>value</b>-of-a-<b>function</b>", "snippet": "What is a <b>value</b> <b>function</b> reinforcement <b>learning</b>? <b>Value</b> <b>function</b> Many reinforcement <b>learning</b> introduce the notion of `<b>value</b>-<b>function</b>` which often denoted as V(s) . The <b>value</b> <b>function</b> represent how good is a state for an agent to be in. It is equal to expected total reward for an agent starting from state s . What is optimal <b>value</b> <b>function</b>? The optimal <b>Value</b> <b>function</b> is one which yields maximum <b>value</b> compared to all other <b>value</b> <b>function</b>. When we say we are solving an MDP it actually means we ...", "dateLastCrawled": "2022-01-15T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>SARSA</b> vs Q - <b>learning</b>", "url": "https://tcnguyen.github.io/reinforcement_learning/sarsa_vs_q_learning.html", "isFamilyFriendly": true, "displayUrl": "https://tcnguyen.github.io/reinforcement_<b>learning</b>/<b>sarsa</b>_vs_q_<b>learning</b>.html", "snippet": "<b>SARSA</b> will learn the optimal $\\epsilon$-greedy policy, i.e, the Q-<b>value</b> <b>function</b> will converge to a optimal Q-<b>value</b> <b>function</b> but in the space of $\\epsilon$-greedy policy only (as long as each <b>state action</b> pair will be visited infinitely). We expect that in the limit of $\\epsilon$ decaying to $0$, <b>SARSA</b> will converge to the overall optimal policy. I quote here a paragraph from", "dateLastCrawled": "2022-01-30T04:17:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(state-action value function)  is like +(decision tree)", "+(state-action value function) is similar to +(decision tree)", "+(state-action value function) can be thought of as +(decision tree)", "+(state-action value function) can be compared to +(decision tree)", "machine learning +(state-action value function AND analogy)", "machine learning +(\"state-action value function is like\")", "machine learning +(\"state-action value function is similar\")", "machine learning +(\"just as state-action value function\")", "machine learning +(\"state-action value function can be thought of as\")", "machine learning +(\"state-action value function can be compared to\")"]}