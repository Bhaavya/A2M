{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Attention</b>, please! A <b>survey of Neural Attention Models in Deep Learning</b> ...", "url": "https://deepai.org/publication/attention-please-a-survey-of-neural-attention-models-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>attention</b>-please-a-<b>survey-of-neural-attention-models</b>-in...", "snippet": "This mechanism allows the inputs to interact with each other &quot;self&quot; and determine what they should pay more <b>attention</b> to. The <b>self-attention</b> <b>layer</b>\u2019s main advantages compared to soft and hard mechanisms are parallel computing ability for a long input. This mechanism <b>layer</b> checks the <b>attention</b> with all the same input elements using simple and easily parallelizable matrix calculations. Figure 9 shows an intuitive example of a <b>self-attention</b> mechanism. Figure 9: <b>Self-Attention</b> examples. a ...", "dateLastCrawled": "2022-01-21T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Vision Transformers are Robust Learners \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2105.07581/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2105.07581", "snippet": "In summary, a single <b>attention</b> <b>layer</b> tries to find out how to best align the keys to the queries and quantifies this finding in the form of <b>attention</b> scores. These scores are then multiplied with the values to obtain the final output. To enable feature-rich hierarchical learning, h <b>self-attention</b> layers (or so-<b>called</b> &quot;heads&quot;) are stacked together producing an output of N \u00d7 d h. This output is then fed through a linear transformation <b>layer</b> that produces the final output of N \u00d7 d from MHSA ...", "dateLastCrawled": "2022-01-05T17:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>ATTENTION</b>, PLEASE! A <b>SURVEY OF NEURAL ATTENTION MODELS IN DEEP</b> ...", "url": "https://www.researchgate.net/publication/350539262_ATTENTION_PLEASE_A_SURVEY_OF_NEURAL_ATTENTION_MODELS_IN_DEEP_LEARNING_A_PREPRINT", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350539262_<b>ATTENTION</b>_PLEASE_A_SURVEY_OF_NEURAL...", "snippet": "These architectures and all that use <b>self-attention</b> belong to a new category of neural networks, <b>called</b> Self-Attentiv e Neural Networks. They aim to explore <b>self-attention</b> in", "dateLastCrawled": "2022-01-26T02:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Chinese named entity recognition: The state of the art - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221016581", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221016581", "snippet": "Fig. 9 gives a concept of <b>self-attention</b> mechanism in CNER, the <b>attention</b> layers input the token representations from embedding <b>layer</b> or encoder, then output the <b>attention</b> weighted token representations to encoder or decoder, respectively. The <b>self-attention</b> in CNER is a mechanism that redistributes weights for each token through calculations based on all tokens and assigns more weights to more essential tokens.", "dateLastCrawled": "2022-01-09T13:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Pre-Trained Models: Past, Present and Future - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "snippet": "As compared to RNNs, Transformer is an encoder-decoder structure that applies a <b>self-attention</b> mechanism, which can model correlations between all words of the input sequence in parallel. Hence, owing to the parallel computation of the <b>self-attention</b> mechanism, Transformer could fully take advantage of advanced computing devices to train large-scale models. In both the encoding and decoding phases of Transformer, the <b>self-attention</b> mechanism of Transformer computes representations for all ...", "dateLastCrawled": "2022-01-27T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "New submissions for Mon, 24 May 21 \u00b7 Issue #114 \u00b7 dajinstory/daily ...", "url": "https://github.com/dajinstory/daily-arxiv-noti/issues/114", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dajinstory/daily-arxiv-noti/issues/114", "snippet": "These models are based on multi-head <b>self-attention</b> mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility in attending image-wide context conditioned on a given patch can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing ...", "dateLastCrawled": "2022-01-25T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Track: Poster Session 1", "url": "https://icml.cc/virtual/2021/session/12513", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2021/session/12513", "snippet": "This work proposes a new way to understand <b>self-attention</b> networks: we show that their output can be decomposed into a sum of smaller terms---or paths---each involving the operation of a sequence of <b>attention</b> heads across layers. Using this path decomposition, we prove that <b>self-attention</b> possesses a strong inductive bias towards &quot;token uniformity&quot;. Specifically, without skip connections or multi-<b>layer</b> perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the ...", "dateLastCrawled": "2022-01-22T11:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Data Science Interview Questions (30 days of Interview Preparation) # ...", "url": "https://inblog.in/Data-Science-Interview-Questions-30-days-of-Interview-Preparation-Day-25-WnhyEuW8v7", "isFamilyFriendly": true, "displayUrl": "https://inblog.in/Data-Science-Interview-Questions-30-days-of-Interview-Preparation...", "snippet": "The Transformer encoder adopts the fully-connected <b>self-attention</b> structure to model the long-range context, which is the weakness of RNNs. Moreover, the Transformer has better parallelism ability than RNNs. However, in the NER task, Transformer encoder has been reported to perform poorly Guo et al. (2019), our experiments <b>also</b> confirm this result. Therefore, it is intriguing to explore the reason why the Transformer does not work well in the NER task.", "dateLastCrawled": "2022-01-20T14:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "New submissions for Tue, 10 Aug 21 \u00b7 Issue #63 \u00b7 DongZhouGu/arxiv-daily ...", "url": "https://github.com/DongZhouGu/arxiv-daily/issues/63", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/DongZhouGu/arxiv-daily/issues/63", "snippet": "<b>Self-attention</b> is applied across these query clips to simulate a continuous scale space. We <b>also</b> utilize another <b>self-attention</b> module on the target video to learn the contextual within the sequence. Finally a mutual-<b>attention</b> is used to match the temporal scales to localize the query within the target sequence. Extensive experiments demonstrate that the proposed approach can not only reliably identify isolated signs in continuous videos, regardless of the signers&#39; appearance, but can <b>also</b> ...", "dateLastCrawled": "2021-09-19T16:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Hai Zhao | Papers With Code", "url": "https://paperswithcode.com/author/hai-zhao", "isFamilyFriendly": true, "displayUrl": "https://paperswithcode.com/author/hai-zhao", "snippet": "Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond. 1 code implementation \u2022 13 May 2020 \u2022 Zhuosheng Zhang, Hai Zhao, Rui Wang. In this survey, we provide a comprehensive and comparative review on MRC covering overall research topics about 1) the origin and development of MRC and CLM, with a particular focus on the role of CLMs; 2) the impact of MRC and CLM to the NLP community; 3) the definition, datasets, and evaluation of MRC; 4) general MRC ...", "dateLastCrawled": "2022-01-24T18:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Attention</b>, please! A <b>survey of Neural Attention Models in Deep Learning</b> ...", "url": "https://deepai.org/publication/attention-please-a-survey-of-neural-attention-models-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>attention</b>-please-a-<b>survey-of-neural-attention-models</b>-in...", "snippet": "This mechanism allows the inputs to interact with each other &quot;self&quot; and determine what they should pay more <b>attention</b> to. The <b>self-attention</b> <b>layer</b>\u2019s main advantages compared to soft and hard mechanisms are parallel computing ability for a long input. This mechanism <b>layer</b> checks the <b>attention</b> with all the same input elements using simple and easily parallelizable matrix calculations. Figure 9 shows an intuitive example of a <b>self-attention</b> mechanism. Figure 9: <b>Self-Attention</b> examples. a ...", "dateLastCrawled": "2022-01-21T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Self-Attentional Acoustic Models | Request PDF", "url": "https://www.researchgate.net/publication/324056042_Self-Attentional_Acoustic_Models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324056042_<b>Self-Attention</b>al_Acoustic_Models", "snippet": "The majority of the research direction to avoid the cost is to compute the <b>self-attention</b> at each time step only on a fixed <b>span</b> of the subset from the whole input sequence [16][17][18][19], e.g ...", "dateLastCrawled": "2022-01-08T10:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>ATTENTION</b>, PLEASE! A <b>SURVEY OF NEURAL ATTENTION MODELS IN DEEP</b> ...", "url": "https://www.researchgate.net/publication/350539262_ATTENTION_PLEASE_A_SURVEY_OF_NEURAL_ATTENTION_MODELS_IN_DEEP_LEARNING_A_PREPRINT", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350539262_<b>ATTENTION</b>_PLEASE_A_SURVEY_OF_NEURAL...", "snippet": "These architectures and all that use <b>self-attention</b> belong to a new category of neural networks, <b>called</b> Self-Attentiv e Neural Networks. They aim to explore <b>self-attention</b> in", "dateLastCrawled": "2022-01-26T02:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Vision Transformers are Robust Learners \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/2105.07581/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2105.07581", "snippet": "To enable feature-rich hierarchical learning, h <b>self-attention</b> layers (or so-<b>called</b> &quot;heads&quot;) are stacked together producing an output of N \u00d7 d h. This output is then fed through a linear transformation <b>layer</b> that produces the final output of N \u00d7 d from MHSA. MHSA then forms the core Transformer block. Transformer block. A single transformer block is composed of MHSA, <b>Layer</b> Normalization (LN) ba2016layer , feed-forward network (FFN), and skip connections 7780459 . It is implemented using (2 ...", "dateLastCrawled": "2022-01-05T17:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "New submissions for Mon, 24 May 21 \u00b7 Issue #114 \u00b7 dajinstory/daily ...", "url": "https://github.com/dajinstory/daily-arxiv-noti/issues/114", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dajinstory/daily-arxiv-noti/issues/114", "snippet": "These models are based on multi-head <b>self-attention</b> mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility in attending image-wide context conditioned on a given patch can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing ...", "dateLastCrawled": "2022-01-25T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Data Science Interview Questions (30 days of Interview Preparation) # ...", "url": "https://inblog.in/Data-Science-Interview-Questions-30-days-of-Interview-Preparation-Day-25-WnhyEuW8v7", "isFamilyFriendly": true, "displayUrl": "https://inblog.in/Data-Science-Interview-Questions-30-days-of-Interview-Preparation...", "snippet": "The Transformer encoder adopts the fully-connected <b>self-attention</b> structure to model the long-range context, which is the weakness of RNNs. Moreover, the Transformer has better parallelism ability than RNNs. However, in the NER task, Transformer encoder has been reported to perform poorly Guo et al. (2019), our experiments <b>also</b> confirm this result. Therefore, it is intriguing to explore the reason why the Transformer does not work well in the NER task.", "dateLastCrawled": "2022-01-20T14:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "New submissions for Tue, 10 Aug 21 \u00b7 Issue #63 \u00b7 DongZhouGu/arxiv-daily ...", "url": "https://github.com/DongZhouGu/arxiv-daily/issues/63", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/DongZhouGu/arxiv-daily/issues/63", "snippet": "<b>Self-attention</b> is applied across these query clips to simulate a continuous scale space. We <b>also</b> utilize another <b>self-attention</b> module on the target video to learn the contextual within the sequence. Finally a mutual-<b>attention</b> is used to match the temporal scales to localize the query within the target sequence. Extensive experiments demonstrate that the proposed approach can not only reliably identify isolated signs in continuous videos, regardless of the signers&#39; appearance, but can <b>also</b> ...", "dateLastCrawled": "2021-09-19T16:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "A comparative review of graph convolutional networks for human skeleton ...", "url": "https://link.springer.com/article/10.1007/s10462-021-10107-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10462-021-10107-y", "snippet": "To make the graph structure more adaptive, <b>self-attention</b> mechanism and a free learned mask were proposed to make the graph unique for different layers and samples. In other words, this design made the topological structure of different actions unique, so that the actions were easier to be identified. Meantime, first-order information (coordinates of joints) was ensemble with second-order features (lengths and directions of human bones) to form two-stream framework. The dual-stream network ...", "dateLastCrawled": "2022-01-29T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Pre-Trained Models: Past, Present and Future - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "snippet": "<b>Attention</b> mechanism, the core module in Transformer architecture, is inspired by the micro and atom operation of the <b>human\u2019s</b> cognitive system and only responsible for the perceptive function. However, human-level intelligence is far more complex than the mere understanding of the association between different things. In pursuit for human-level intelligence, understanding the macro architecture of our cognitive functions including decision making, logical reasoning, counterfactual reasoning ...", "dateLastCrawled": "2022-01-27T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "arXiv:1803.05655v1 [cs.CL] 15 Mar 2018", "url": "https://hfl-rc.com/res/pdf/semeval2018_task11_hfl-rc.pdf", "isFamilyFriendly": true, "displayUrl": "https://hfl-rc.com/res/pdf/semeval2018_task11_hfl-rc.pdf", "snippet": "a neural network <b>called</b> Hybrid Multi-Aspects (HMA) model, which mimic the <b>human\u2019s</b> in-tuitions on dealing with the multiple-choice reading comprehension. In this model, we aim to produce the predictions in multiple aspects by calculating <b>attention</b> among the text, ques-tion and choices, and combine these results for \ufb01nal predictions. Experimental results show that our HMA model could give substantial improvements over the baseline system and got the \ufb01rst place on the \ufb01nal test set ...", "dateLastCrawled": "2021-09-15T15:21:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Track: Poster Session 1", "url": "https://icml.cc/virtual/2021/session/12513", "isFamilyFriendly": true, "displayUrl": "https://icml.cc/virtual/2021/session/12513", "snippet": "This work proposes a new way to understand <b>self-attention</b> networks: we show that their output <b>can</b> be decomposed into a sum of smaller terms---or paths---each involving the operation of a sequence of <b>attention</b> heads across layers. Using this path decomposition, we prove that <b>self-attention</b> possesses a strong inductive bias towards &quot;token uniformity&quot;. Specifically, without skip connections or multi-<b>layer</b> perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the ...", "dateLastCrawled": "2022-01-22T11:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Automated Emotion Recognition: Current Trends and Future Perspectives ...", "url": "https://www.sciencedirect.com/science/article/pii/S0169260722000311", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0169260722000311", "snippet": "A system named PCNSE consists of parallel convolutional layers (PCN) integrated with a squeeze-and-excitation network (SEnet) combined with <b>self-attention</b> DRN (Dilated Residual Network) (Z. Zhao, Li, et al., 2021). The experiments conducted on IEMOCAP showed the weighted accuracy (WA) of 73.1% and an unweighted accuracy (UA) of 66.3% and the FAU-AEC database depicted UA of 41.1%. A study utilized combination of CNN and LSTM. This study proposed a model with four local feature learning blocks ...", "dateLastCrawled": "2022-01-23T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "SQuAD: 100,000+ <b>Questions for Machine Comprehension of</b> Text | Request PDF", "url": "https://www.researchgate.net/publication/312250707_SQuAD_100000_Questions_for_Machine_Comprehension_of_Text", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/312250707_SQuAD_100000_Questions_for_Machine...", "snippet": "To verify that the plug-and-play property of topk <b>attention</b> <b>also</b> holds at <b>self-attention</b> layers, we downloaded a BERT-large-uncased-whole-wordmasking checkpoint (Devlin et al., 2019) already fine ...", "dateLastCrawled": "2022-01-13T08:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Conference on Empirical Methods in Natural Language Processing (and ...", "url": "https://aclanthology.org/events/emnlp-2021/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/events/emnlp-2021", "snippet": "Recent efforts to improve the efficiency of <b>self-attention</b> have led to a proliferation of long-range Transformer language models, which <b>can</b> process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM ...", "dateLastCrawled": "2022-01-29T06:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "NIPS2017abs.md \u00b7 GitHub", "url": "https://gist.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "isFamilyFriendly": true, "displayUrl": "https://<b>gist</b>.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "snippet": "We <b>also</b> introduce a stronger encoder for visual dialog, and employ a <b>self-attention</b> mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10).", "dateLastCrawled": "2022-01-31T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Is understanding the human brain the</b> key to true AI? - Machine learning ...", "url": "https://machinelearningandai.quora.com/Is-understanding-the-human-brain-the-key-to-true-AI", "isFamilyFriendly": true, "displayUrl": "https://machinelearningandai.quora.com/<b>Is-understanding-the-human-brain-the</b>-key-to-true-AI", "snippet": "Some functionality <b>can</b> be emulated like the many variations of activation functions in artificial neurons <b>can</b> mimic some narrow properties of biological neurons. Another example is <b>self attention</b> in transformers. <b>Attention</b> is a function that the brain uses a lot to make it more efficiently focus on what&#39;s important while ignoring the less ...", "dateLastCrawled": "2022-01-23T16:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "\u3010NLP\u3011 2014-2020 Recommendation System \u63a8\u8350\u7cfb\u7edf\u76f8\u5173\u8bba\u6587\u6574\u7406 | PROCJX&#39;s BLOGS", "url": "https://procjx.github.io/2020/12/09/%E3%80%90NLP%E3%80%912014-2020%20Recommendation%20System%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86/", "isFamilyFriendly": true, "displayUrl": "https://procjx.github.io/2020/12/09/\u3010NLP\u30112014-2020 Recommendation System \u63a8\u8350...", "snippet": "The original <b>self-attention</b> of Transformer is a deterministic measure without relation-awareness. Therefore, we introduce a latent space to the <b>self-attention</b>, and the latent space models the recommendation context from relation as a multivariate skew-normal distribution with a kernelized covariance matrix from co-occurrences, item characteristics, and user information. This work merges the <b>self-attention</b> of the Transformer and the sequential recommendation by adding a probabilistic model of ...", "dateLastCrawled": "2022-02-02T13:22:00.0000000Z", "language": "zh_chs", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Health Assessment Test 4</b> Flashcards | Quizlet", "url": "https://quizlet.com/7729540/health-assessment-test-4-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/7729540/<b>health-assessment-test-4</b>-flash-cards", "snippet": "not fully alert, drifts off to sleep when not stimulated, <b>can</b> be aroused to name when <b>called</b> in normal voice but looks drowsy, responds appropriately to questions or commands but thinking seems slow and fuzzy, inattentive, loses train of <b>thought</b>, spontaneous movements are decreased . obtunded. transitional state between lethargy and stupor. some sources omit this level. sleeps most of the time, difficult to arouse, needs a loud shout or vigorous shake, acts confused when is aroused ...", "dateLastCrawled": "2018-10-26T09:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "IJPHRD Feb - 2018 VT Special Issue As On 05 - 03 - 2018 PDF | PDF ...", "url": "https://www.scribd.com/document/467178978/IJPHRD-Feb-2018-VT-Special-issue-as-on-05-03-2018-1-pdf", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/467178978/IJPHRD-Feb-2018-VT-Special-issue-as-on-05-03...", "snippet": "Volume 8 Number 4 October-December 2017. SCOPUS IJPHRD CITATION SCORE Indian Journal of Public Health Research and Development Scopus coverage years: from 2010 to 2016 Publisher: R.K. Sharma, Institute of Medico-Legal Publications ISSN:0976-0245E-ISSN: 0976-5506 Subject area: Medicine: Public Health, Environmental and Occupational Health CiteScore 2015- 0.02 SJR 2015- 0.105 SNIP 2015- 0.034. d b y Sc re e. op Cov. us. EMBASE I I", "dateLastCrawled": "2022-01-28T13:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "NIPS2017 \u30bf\u30a4\u30c8\u30eb\uff06\u6982\u8981\u8a33 \u4e00\u89a7 - Google Drive", "url": "https://docs.google.com/spreadsheets/d/1ZQMXFAVapEOm1y53ijEJ1Ds6Mls-z6ZtoJKpJmHogzo/htmlview", "isFamilyFriendly": true, "displayUrl": "https://<b>docs.google.com</b>/spreadsheets/d/1ZQMXFAVapEOm1y53ijEJ1Ds6Mls-z6ZtoJKpJmHogzo/...", "snippet": "We <b>also</b> introduce a stronger encoder for visual dialog, and employ a <b>self-attention</b> mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10 ...", "dateLastCrawled": "2022-01-24T15:32:00.0000000Z", "language": "ja", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Attention</b>, please! A <b>survey of Neural Attention Models in Deep Learning</b> ...", "url": "https://deepai.org/publication/attention-please-a-survey-of-neural-attention-models-in-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>attention</b>-please-a-<b>survey-of-neural-attention-models</b>-in...", "snippet": "This mechanism allows the inputs to interact with each other &quot;self&quot; and determine what they should pay more <b>attention</b> to. The <b>self-attention</b> <b>layer</b>\u2019s main advantages <b>compared</b> to soft and hard mechanisms are parallel computing ability for a long input. This mechanism <b>layer</b> checks the <b>attention</b> with all the same input elements using simple and easily parallelizable matrix calculations. Figure 9 shows an intuitive example of a <b>self-attention</b> mechanism. Figure 9: <b>Self-Attention</b> examples. a ...", "dateLastCrawled": "2022-01-21T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Self-Attentional Acoustic Models | Request PDF", "url": "https://www.researchgate.net/publication/324056042_Self-Attentional_Acoustic_Models", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324056042_<b>Self-Attention</b>al_Acoustic_Models", "snippet": "The majority of the research direction to avoid the cost is to compute the <b>self-attention</b> at each time step only on a fixed <b>span</b> of the subset from the whole input sequence [16][17][18][19], e.g ...", "dateLastCrawled": "2022-01-08T10:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Pyramid Constrained Self-Attention Network for</b> Fast Video Salient ...", "url": "https://www.researchgate.net/publication/342542615_Pyramid_Constrained_Self-Attention_Network_for_Fast_Video_Salient_Object_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/342542615_Pyramid_Constrained_<b>Self-Attention</b>...", "snippet": "Some methods <b>also</b> model the human <b>attention</b> mechanism to select interesting regions in different frames, e.g. <b>self-attention</b> [12], spatial <b>attention</b> supervised by human eye fixation data [10,37 ...", "dateLastCrawled": "2021-12-15T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Pre-Trained Models: Past, Present and Future - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S2666651021000231", "snippet": "As <b>compared</b> to RNNs, Transformer is an encoder-decoder structure that applies a <b>self-attention</b> mechanism, which <b>can</b> model correlations between all words of the input sequence in parallel. Hence, owing to the parallel computation of the <b>self-attention</b> mechanism, Transformer could fully take advantage of advanced computing devices to train large-scale models. In both the encoding and decoding phases of Transformer, the <b>self-attention</b> mechanism of Transformer computes representations for all ...", "dateLastCrawled": "2022-01-27T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "New submissions for Mon, 24 May 21 \u00b7 Issue #114 \u00b7 dajinstory/daily ...", "url": "https://github.com/dajinstory/daily-arxiv-noti/issues/114", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/dajinstory/daily-arxiv-noti/issues/114", "snippet": "These models are based on multi-head <b>self-attention</b> mechanisms that <b>can</b> flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility in attending image-wide context conditioned on a given patch <b>can</b> facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing ...", "dateLastCrawled": "2022-01-25T19:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A comparative review of graph convolutional networks for human skeleton ...", "url": "https://link.springer.com/article/10.1007/s10462-021-10107-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10462-021-10107-y", "snippet": "To make the graph structure more adaptive, <b>self-attention</b> mechanism and a free learned mask were proposed to make the graph unique for different layers and samples. In other words, this design made the topological structure of different actions unique, so that the actions were easier to be identified. Meantime, first-order information (coordinates of joints) was ensemble with second-order features (lengths and directions of human bones) to form two-stream framework. The dual-stream network ...", "dateLastCrawled": "2022-01-29T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Chinese named entity recognition: The state of the art - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0925231221016581", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0925231221016581", "snippet": "The <b>attention</b> mechanism <b>can</b> directly establish long-distance dependence, so it <b>can</b> <b>also</b> effectively encode contextual information. Due to the simple structure, the calculation is simple and easy to parallel. Like the encoder and decoder functions in the NER method, the encoder is responsible for encoding its contextual information for each input token. The decoder obtains an output according to the information. The difference is that the decoder in NER outputs the type while the Transformer ...", "dateLastCrawled": "2022-01-09T13:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "New submissions for Tue, 10 Aug 21 \u00b7 Issue #63 \u00b7 DongZhouGu/arxiv-daily ...", "url": "https://github.com/DongZhouGu/arxiv-daily/issues/63", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/DongZhouGu/arxiv-daily/issues/63", "snippet": "We <b>also</b> utilize another <b>self-attention</b> module on the target video to learn the contextual within the sequence. Finally a mutual-<b>attention</b> is used to match the temporal scales to localize the query within the target sequence. Extensive experiments demonstrate that the proposed approach <b>can</b> not only reliably identify isolated signs in continuous videos, regardless of the signers&#39; appearance, but <b>can</b> <b>also</b> generalize to different sign languages. By taking advantage of the <b>attention</b> mechanism and ...", "dateLastCrawled": "2021-09-19T16:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "NIPS2017abs.md \u00b7 GitHub", "url": "https://gist.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "isFamilyFriendly": true, "displayUrl": "https://<b>gist</b>.github.com/cwhy/ce9f21cbb649d11f646e273a04e0ed4c", "snippet": "We <b>also</b> introduce a stronger encoder for visual dialog, and employ a <b>self-attention</b> mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10).", "dateLastCrawled": "2022-01-31T00:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Is understanding the human brain the</b> key to true AI? - Machine learning ...", "url": "https://machinelearningandai.quora.com/Is-understanding-the-human-brain-the-key-to-true-AI", "isFamilyFriendly": true, "displayUrl": "https://machinelearningandai.quora.com/<b>Is-understanding-the-human-brain-the</b>-key-to-true-AI", "snippet": "Another example is <b>self attention</b> in transformers. <b>Attention</b> is a function that the brain uses a lot to make it more efficiently focus on what&#39;s important while ignoring the less important stuff. There are many narrow properties of the brain that <b>can</b> be emulated or copied, so to speak, in machines but the bigger picture of what really makes the brain intelligent will probably remain elusive for a very very long time. Hope this helps. Footnotes [1] Active properties of neuronal dendrites ...", "dateLastCrawled": "2022-01-23T16:42:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>learning</b> glossary - DataTime", "url": "https://www.dtalg.com/article-1/", "isFamilyFriendly": true, "displayUrl": "https://www.dtalg.com/article-1", "snippet": "<b>self-attention</b> (<b>also</b> <b>called</b> <b>self-attention</b> <b>layer</b>) #language. A neural network <b>layer</b> that transforms a sequence of embeddings (for instance, token embeddings) into another sequence of embeddings. Each embedding in the output sequence is constructed by integrating information from the elements of the input sequence through an attention mechanism.", "dateLastCrawled": "2022-01-25T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Attention in Psychology, Neuroscience, and <b>Machine</b> <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7177153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7177153", "snippet": "These weightings scale the word encodings themselves to create the next <b>layer</b> in the model, a process known as \u201c<b>self-attention</b>.\u201d This process repeats, and eventually interacts with the autoregressive decoder which <b>also</b> has attention mechanisms that allow it to flexibly focus on the encoded input (as in the standard form of attention) and on the previously generated output. The Transformer\u2014the name given to this new attention architecture\u2014outperformed many previous models and quickly ...", "dateLastCrawled": "2021-12-27T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Lecture 7: Transformers</b> - Deep <b>Learning</b>", "url": "https://chinmayhegde.github.io/dl-notes/notes/lecture07/", "isFamilyFriendly": true, "displayUrl": "https://chinmayhegde.github.io/dl-notes/notes/lecture07", "snippet": "<b>Self-Attention</b>. This is the point where papers-blogs-tweets-slides etc start talking about keys/values and attention mechanisms and everything goes a bit haywire. Let\u2019s just ignore all that for now, and instead talk about something <b>called</b> <b>self-attention</b>. The use of the \u201cself-\u201c prefix will become clear later on. Here is how it is defined.", "dateLastCrawled": "2022-02-03T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training ...", "url": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_Self_attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_<b>Self_attention</b>_and_Statef...", "snippet": "<b>Self-attention</b>, <b>also</b> known as intra-attention, is an attention mechanism relat- ing di\ufb00erent positions of a sequence in order to model dependencies between dif- ferent parts of the sequence. This di\ufb00ers from general attention in that instead of seeking to discover the \u201cimportant\u201d parts of the sequence relating to the net- work output, <b>self-attention</b> seeks to \ufb01nd the \u201cimportant\u201d portions of the sequence that relate to each other. This is done in order to leverage those intra ...", "dateLastCrawled": "2022-02-03T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "<b>Self attention</b>, sometimes <b>called</b> intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. In simpler terms, <b>self attention</b> helps us create similar connections but within the same sentence. Look at the following example: \u201cI poured water from the bottle into the cup until it was full.\u201d it =&gt; cup \u201cI poured water from the bottle into the cup until it was empty.\u201d it=&gt; bottle. By changing one word ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine Learning Papers: Molecules</b> - <b>Machine Learning</b> Applied", "url": "https://machinelearningapplied.com/machine-learning-papers-molecules/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>applied.com/<b>machine-learning-papers-molecules</b>", "snippet": "A <b>self-attention</b> based message passing neural network for predicting molecular lipophilicity and aqueous solubility - Tang et al 2020 . Efficient and accurate prediction of molecular properties, such as lipophilicity and solubility, is highly desirable for rational compound design in chemical and pharmaceutical industries. To this end, we build and apply a graph-neural-network framework <b>called</b> <b>self-attention</b>-based message-passing neural network (SAMPN) to study the relationship between ...", "dateLastCrawled": "2021-12-22T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The encoder is composed of a stack of N = 6 identical layers. Each <b>layer</b> has two sub-layers. The first is a multi-head <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by <b>layer</b> normalization. That is, the output ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training", "url": "https://www.researchgate.net/publication/353690477_Enhancing_LSTM_Models_with_Self-attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353690477_Enhancing_LSTM_Models_with_Self...", "snippet": "<b>Self-attention</b>, <b>also</b> known as in tra-attention, is an attention mec hanism re- lating di\ufb00erent positions of a sequence in order to model dependencies b etween di\ufb00erent parts of the sequence.", "dateLastCrawled": "2022-01-13T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to perform Text Summarization with Python, HuggingFace Transformers ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2020/12/21/easy-text-summarization-with-hugging...", "snippet": "Summary &amp; Example: Text Summarization with Transformers. Transformers are taking the world of language processing by storm. These models, which learn to interweave the importance of tokens by means of a mechanism <b>called</b> <b>self-attention</b> and without recurrent segments, have allowed us to train larger models without all the problems of recurrent neural networks.", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Journal of Physics: Conference Series PAPER OPEN ACCESS You may <b>also</b> ...", "url": "https://iopscience.iop.org/article/10.1088/1742-6596/1651/1/012019/pdf", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1742-6596/1651/1/012019/pdf", "snippet": "Different <b>machine</b> <b>learning</b> techniques have been used in this field for many years. But recently, deep <b>learning</b> has caused more and more attention in the field of education. Deep <b>learning</b> is a <b>machine</b> <b>learning</b> method based on neural network structure of multi-<b>layer</b> processing units, and it has been successfully applied to a series of problems in the field of image recognition and natural language processing[2]. With the diversified cultivation of traditional universities and the development ...", "dateLastCrawled": "2021-12-29T04:07:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(self-attention (also called self-attention layer))  is like +(human's attention span)", "+(self-attention (also called self-attention layer)) is similar to +(human's attention span)", "+(self-attention (also called self-attention layer)) can be thought of as +(human's attention span)", "+(self-attention (also called self-attention layer)) can be compared to +(human's attention span)", "machine learning +(self-attention (also called self-attention layer) AND analogy)", "machine learning +(\"self-attention (also called self-attention layer) is like\")", "machine learning +(\"self-attention (also called self-attention layer) is similar\")", "machine learning +(\"just as self-attention (also called self-attention layer)\")", "machine learning +(\"self-attention (also called self-attention layer) can be thought of as\")", "machine learning +(\"self-attention (also called self-attention layer) can be compared to\")"]}