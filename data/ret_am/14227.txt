{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Modeling of Rakugo Speech and Its Various Speaking Styles: Toward ...", "url": "https://www.arxiv-vanity.com/papers/1911.00137/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1911.00137", "snippet": "We modeled rakugo speech using the state-of-the-art Tacotron 2 and an enhanced version of it with <b>self-attention</b> to better consider long-term dependency. We <b>also</b> used global style tokens and manually labeled context features to enrich speaking styles. Through a listening test, we found that the speech synthesis models could not yet reach the professional level, but interestingly, some of the synthetic speech entertained the listeners as well as analysis-by-synthesis speech. Although there is ...", "dateLastCrawled": "2021-08-28T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Appendix to 2020 CS379C Class Discussion Notes", "url": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs379c/archive/2020/class_messages_listing/appendix.html", "snippet": "The most recent \u2013 and <b>also</b> final ... consisting of a masked multi-head <b>self-attention</b> <b>layer</b> and a point-wise, fully connected <b>layer</b> \u2013 so that, for example, if the source is the third character of the first pattern and the sink is the register of the second operand of the comparator, the model should expect that the output register contains a 1 (or some Boolean equivalent) if the current input character matches the third character of the first pattern and zero otherwise. If the match ...", "dateLastCrawled": "2022-01-31T03:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Modeling of Rakugo Speech and Its Various Speaking Styles: Toward ...", "url": "https://www.researchgate.net/publication/337005784_Modeling_of_Rakugo_Speech_and_Its_Various_Speaking_Styles_Toward_Speech_Synthesis_That_Entertains_Audiences", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/337005784_Modeling_of_Rakugo_Speech_and_Its...", "snippet": "We modeled rakugo speech using the state-of-the-art Tacotron 2 and an enhanced version of it with <b>self-attention</b> to better consider long-term dependency. We <b>also</b> used global style tokens and ...", "dateLastCrawled": "2022-01-30T22:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep Learning Based TTS-STT Model With Transliteration For Indic ...", "url": "https://www.scribd.com/document/555296246/Deep-Learning-Based-TTS-STT-Model-With-Transliteration-for-Indic-Languages", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/555296246/Deep-Learning-Based-TTS-STT-Model-With...", "snippet": "B. <b>Text</b>-to-Speech <b>Text</b>-to-speech (TTS) is a form of assistive technology that reads digital <b>text</b> <b>aloud</b>. It is <b>also</b> <b>called</b> &quot;<b>reading</b> <b>aloud</b>&quot; technology.[2]With the click of a button or the touch of a finger, TTS can take words from a computer or other digital device and convert them into sound. TTS is very useful for children with learning disabilities. But it can <b>also</b> help user with writing and editing, even focusing. How <b>text</b>-to-speech works TTS works with almost all personal digital devices ...", "dateLastCrawled": "2022-01-29T04:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Modeling of Rakugo Speech and Its Limitations: Toward Speech ...", "url": "https://www.researchgate.net/publication/343247986_Modeling_of_Rakugo_Speech_and_Its_Limitations_Toward_Speech_Synthesis_That_Entertains_Audiences", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/343247986_Modeling_of_Rakugo_Speech_and_Its...", "snippet": "and <b>self-attention</b> outperforms T acotron 1 in Japanese [17]. W e <b>also</b> combined global style tokens (GSTs) [4] and/or manually labeled context features with T acotron 2 and SA-", "dateLastCrawled": "2021-12-02T07:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Preface \u2014 Dive into <b>Deep Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_preface/index.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_preface/index.html", "snippet": "This comes up especially in the basic tutorials, where we want you to understand everything that happens in a given <b>layer</b> or optimizer. In these cases, we will often present two versions of the example: one where we implement everything from scratch, relying only on NumPy-<b>like</b> functionality and automatic differentiation, and another, more practical example, where we write succinct code using the high-level APIs of <b>deep learning</b> frameworks. Once we have taught you how some component works, we ...", "dateLastCrawled": "2022-01-29T10:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Survey <b>of Natural Language Generation Techniques with</b> a Focus on ...", "url": "https://deepai.org/publication/a-survey-of-natural-language-generation-techniques-with-a-focus-on-dialogue-systems-past-present-and-future-directions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-<b>of-natural-language-generation-techniques-with</b>...", "snippet": "The transformer is a made of the encoder-decoder architecture but each encoder is a stack of six encoders with each encoder containing a <b>self-attention</b> and point-wise fully connected feed forward neural networks. The decoder is <b>also</b> a stack of six decoders with each decocder containing the same components as the encoder, but with an additional attention <b>layer</b> that helps the decoder focus on relevant parts of the input sentence. Work using transformer models is still in its infancy. Radford", "dateLastCrawled": "2021-12-09T20:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) SCORES\u00b01 touch\u00e9 | Sandra Noeth - Academia.edu", "url": "https://www.academia.edu/37469200/SCORES_1_touch%C3%A9", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/37469200/SCORES_1_touch\u00e9", "snippet": "co-edited by Walter Heun, Krassimira Kruschkova, Lejla Mehanovic and Sandra Noeth, Tanzquartier Wien touch\u00e9: touching and being touched \u2013 affected, challenged, made to respond, questioned. In the context of the (stage) body, the concept of touching", "dateLastCrawled": "2021-07-30T00:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Meditation: a Conversational Model</b> \u2013 Matthew Remski", "url": "https://matthewremski.com/wordpress/meditation-a-conversational-model/", "isFamilyFriendly": true, "displayUrl": "https://matthewremski.com/wordpress/<b>meditation-a-conversational-model</b>", "snippet": "Next came vipassana training. I\u2019ve <b>also</b> done a lot of <b>reading</b> in zen, which <b>like</b> many traditions might be cool if a person gets lucky with a non-creepy teacher. But by the time I picked up Suzuki and Dogen I wasn\u2019t a joiner anymore. So under the auspices of several religious traditions, I\u2019ve cycled through the four meditation categories that researchers in clinical psychology and neurophysiology have broken down for distinct study: \u201cfocused attention\u201d, \u201copen monitoring ...", "dateLastCrawled": "2021-12-01T17:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "what are the words formed in the above illustration", "url": "https://besthealth24h.com/tmbjdqxi/what-are-the-words-formed-in-the-above-illustration.html", "isFamilyFriendly": true, "displayUrl": "https://besthealth24h.com/tmbjdqxi/what-are-the-words-formed-in-the-above-illustration...", "snippet": "shape book read <b>aloud</b>; good governance definition; slayer hawaiian shirt chm; dauntless transmog 2021; offshore diving companies; spaghetti and beans recipe; Menu. what are the words formed in the above illustration. venice beach holidays; android52 future groove product; 2008 toyota rav4 engine replacement; characterization marches on; manchester united 98-99 jersey; zinc combustion reaction; kyle kuzma wizards jersey ; anthropomorphous in a sentence; what are the words formed in the above ...", "dateLastCrawled": "2022-01-27T17:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Reinforcement Learning with <b>Self-Attention</b> Networks for ...", "url": "https://www.researchgate.net/publication/353842430_Reinforcement_Learning_with_Self-Attention_Networks_for_Cryptocurrency_Trading", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353842430_Reinforcement_Learning_with_Self...", "snippet": "<b>Self-attention</b> networks are suitable for dealing with the problem because the attention mechanism can process long sequences of data and focus on the most relevant parts of the inputs. Transaction ...", "dateLastCrawled": "2022-01-20T14:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Proceedings of the 2017 Conference on Empirical Methods in Natural ...", "url": "https://aclanthology.org/volumes/D17-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/D17-1", "snippet": "The predicted <b>reading</b> time is then used to build a cognition based attention (CBA) <b>layer</b> for neural sentiment analysis. As a comprehensive model, We can capture attentions of words in sentences as well as sentences in documents. Different attention mechanisms can <b>also</b> be incorporated to capture other aspects of attentions. Evaluations show the CBA based method outperforms the state-of-the-art local context based attention methods significantly. This brings insight to how cognition grounded ...", "dateLastCrawled": "2022-02-01T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Towards Pervasive and Trustworthy Artificial Intelligence How ...", "url": "https://www.academia.edu/69399004/Towards_Pervasive_and_Trustworthy_Artificial_Intelligence_How_standards_can_put_a_great_technology_at_the_service_of_humankind_By", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69399004/Towards_Pervasive_and_Trustworthy_Artificial...", "snippet": "With the printing industry sparing no efforts publishing books on Artificial Intelligence (AI), why should there be another that, in its title and subtitle, combines the overused words AI and trustworthy, with the alien words standards and pervasive?", "dateLastCrawled": "2022-02-02T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "RecSys &#39;19: Proceedings of the 13th ACM Conference on Recommender Systems", "url": "http://st.sigchi.org/publications/toc/recsys-2019.html", "isFamilyFriendly": true, "displayUrl": "st.sigchi.org/publications/toc/recsys-2019.html", "snippet": "Specifically, the Transformer applies a <b>self-attention</b> mechanism that directly models the global relationships between any pair of items in the whole list. We confirm that the performance can be further improved by introducing pre-trained embedding to learn personalized encoding functions for different users. Experimental results on both offline benchmarks and real-world online e-commerce systems demonstrate the significant improvements of the proposed re-ranking model.", "dateLastCrawled": "2022-02-02T17:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "LAK21: LAK21: 11th International Learning Analytics and Knowledge ...", "url": "https://www.solaresearch.org/lak_toc/lak21/", "isFamilyFriendly": true, "displayUrl": "https://www.solaresearch.org/lak_toc/lak21", "snippet": "This study proposed to use a deep learning-based model composed of sparse attention layers, convolutional neural layers, and a fully connected <b>layer</b>, <b>called</b> Sparse Attention Convolutional Neural Networks (SACNN), to predict undergraduate grades. Concretely, sparse attention layers response to the fact that courses have different contributions to the grade prediction of the target course; convolutional neural layers aim to capture the one-dimensional temporal feature on these courses ...", "dateLastCrawled": "2022-01-31T05:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Text Understanding with the Attention Sum</b> Reader Network | Request PDF", "url": "https://www.researchgate.net/publication/306093209_Text_Understanding_with_the_Attention_Sum_Reader_Network", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/306093209_<b>Text</b>_Understanding_with_the...", "snippet": "We <b>also</b> note that many neural models for stage (iv) have been proposed, often <b>called</b> QA or Machine <b>Reading</b> Comprehension (MRC) models (Kadlec et al., 2016; Cui et al., 2017;Zhang et al., 2020 ...", "dateLastCrawled": "2022-01-06T22:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Positional Artefacts Propagate Through Masked Language Model Embeddings", "url": "https://www.readkong.com/page/positional-artefacts-propagate-through-masked-language-5099136", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/positional-artefacts-propagate-through-masked-language...", "snippet": "<b>Similar</b> to \u00a72, we display culprit behind the degree of observed anisotropy. the averaged subword vectors for each <b>layer</b> of our To verify our hypothesis, we clip BERT and models in Appendix C.2, which <b>also</b> corroborate RoBERTa\u2019s outliers by setting each neuron\u2019s value our results. to zero. The left plot in Figure 6 shows that, after clipping the outliers, their vector spaces become 4 Clipping the outliers close to isotropic. In \u00a73, we demonstrated that outlier neurons are re- Self ...", "dateLastCrawled": "2022-01-17T01:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Vladimir Nabokov</b> Nabokov, Vladimir - Essay - eNotes.com", "url": "https://www.enotes.com/topics/vladimir-nabokov/critical-essays/nabokov-vladimir", "isFamilyFriendly": true, "displayUrl": "https://www.enotes.com/topics/<b>vladimir-nabokov</b>/critical-essays/nabokov-vladimir", "snippet": "<b>Vladimir Nabokov</b> 1899\u2013-1977 (Full name Vladimir Vladimirovich Nabokov; <b>also</b> wrote under the pseudonym V. Sirin) Russian-born American novelist, poet, short story writer, essayist, playwright ...", "dateLastCrawled": "2022-02-03T02:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "GPT-2 Neural Network Poetry \u00b7 Gwern.net", "url": "https://www.gwern.net/GPT-2", "isFamilyFriendly": true, "displayUrl": "https://www.gwern.net/GPT", "snippet": "But quality is <b>also</b> reduced by GPT-2-117M being trained on all kinds of <b>text</b>, not just poetry, which means sampling may quickly diverge into prose (as seems to happen particularly easily if given only a single opening line, which presumably makes it hard for it to infer that it\u2019s supposed to generate poetry rather than much more common prose), and it may not have learned poetry as well as it could have, as poetry presumably made up a minute fraction of its corpus (Redditors not being ...", "dateLastCrawled": "2022-01-29T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "what are the words formed in the above illustration", "url": "https://besthealth24h.com/tmbjdqxi/what-are-the-words-formed-in-the-above-illustration.html", "isFamilyFriendly": true, "displayUrl": "https://besthealth24h.com/tmbjdqxi/what-are-the-words-formed-in-the-above-illustration...", "snippet": "shape book read <b>aloud</b>; good governance definition; slayer hawaiian shirt chm; dauntless transmog 2021; offshore diving companies; spaghetti and beans recipe; Menu. what are the words formed in the above illustration. venice beach holidays; android52 future groove product; 2008 toyota rav4 engine replacement; characterization marches on; manchester united 98-99 jersey; zinc combustion reaction; kyle kuzma wizards jersey ; anthropomorphous in a sentence; what are the words formed in the above ...", "dateLastCrawled": "2022-01-27T17:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Aligntts: Efficient Feed-Forward <b>Text</b>-to-Speech System Without Explicit ...", "url": "https://www.researchgate.net/publication/341084072_Aligntts_Efficient_Feed-Forward_Text-to-Speech_System_Without_Explicit_Alignment", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/341084072_Aligntts_Efficient_Feed-Forward...", "snippet": "For the <b>text</b> encoder, we constrain each <b>self-attention</b> <b>layer</b> so the encoder focuses on a <b>text</b> sequence from the local to the global scope. Conversely, the audio decoder constrains its self ...", "dateLastCrawled": "2021-10-22T11:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "ESPnet2 pretrained model, kamo-naoyuki/librispeech_asr_train_asr ...", "url": "https://zenodo.org/record/4604066/", "isFamilyFriendly": true, "displayUrl": "https://zenodo.org/record/4604066", "snippet": "This model was trained by kamo-naoyuki using librispeech recipe in espnet. Python APISee https://github.com/espnet/espnet_model_zoo Evaluate in the recipegit clone ...", "dateLastCrawled": "2021-12-17T00:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Preface \u2014 Dive into <b>Deep Learning</b> 0.17.2 documentation", "url": "http://d2l.ai/chapter_preface/index.html", "isFamilyFriendly": true, "displayUrl": "d2l.ai/chapter_preface/index.html", "snippet": "With these advances in hand, we <b>can</b> now build cars that drive themselves with more autonomy than ever before (and less autonomy than some companies might have you believe), smart reply systems that automatically draft the most mundane emails, helping people dig out from oppressively large inboxes, and software agents that dominate the world\u2019s best humans at board games like Go, a feat once <b>thought</b> to be decades away. Already, these tools exert ever-wider impacts on industry and society ...", "dateLastCrawled": "2022-01-29T10:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Towards Pervasive and Trustworthy Artificial Intelligence How ...", "url": "https://www.academia.edu/69399004/Towards_Pervasive_and_Trustworthy_Artificial_Intelligence_How_standards_can_put_a_great_technology_at_the_service_of_humankind_By", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69399004/Towards_Pervasive_and_Trustworthy_Artificial...", "snippet": "Towards Pervasive and Trustworthy Artificial Intelligence How standards <b>can</b> put a great technology at the service of humankind By. MPAI Community, 2021. Valeria Lazzaroli. Guido Perboli. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 36 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package. Translate PDF ...", "dateLastCrawled": "2022-02-02T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) SCORES\u00b01 touch\u00e9 | Sandra Noeth - Academia.edu", "url": "https://www.academia.edu/37469200/SCORES_1_touch%C3%A9", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/37469200/SCORES_1_touch\u00e9", "snippet": "co-edited by Walter Heun, Krassimira Kruschkova, Lejla Mehanovic and Sandra Noeth, Tanzquartier Wien touch\u00e9: touching and being touched \u2013 affected, challenged, made to respond, questioned. In the context of the (stage) body, the concept of touching", "dateLastCrawled": "2021-07-30T00:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Meditation: a Conversational Model</b> \u2013 Matthew Remski", "url": "https://matthewremski.com/wordpress/meditation-a-conversational-model/", "isFamilyFriendly": true, "displayUrl": "https://matthewremski.com/wordpress/<b>meditation-a-conversational-model</b>", "snippet": "Next came vipassana training. I\u2019ve <b>also</b> done a lot of <b>reading</b> in zen, which like many traditions might be cool if a person gets lucky with a non-creepy teacher. But by the time I picked up Suzuki and Dogen I wasn\u2019t a joiner anymore. So under the auspices of several religious traditions, I\u2019ve cycled through the four meditation categories that researchers in clinical psychology and neurophysiology have broken down for distinct study: \u201cfocused attention\u201d, \u201copen monitoring ...", "dateLastCrawled": "2021-12-01T17:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Augmenting Scientific Papers with Just-in-Time, Position-Sensitive ...", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445648", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445648", "snippet": "<b>Reading</b> interfaces <b>can</b> <b>also</b> assist researchers by helping them navigate to information of interest within a paper. For several years, interfaces for <b>reading</b> PDFs have provided standard affordances for jumping within a paper using hyperlinks. Typesetting software like LaTeX <b>can</b> automatically embed clickable links from references to figures, equations, and sections to the content they refer to, and from citations to bibliographies. Prototype tools have been built to further assist readers in ...", "dateLastCrawled": "2021-12-24T14:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "ICMI 2020 :: <b>Proceedings</b>", "url": "https://icmi.acm.org/2020/index.php?id=proceedings", "isFamilyFriendly": true, "displayUrl": "https://icmi.acm.org/2020/index.php?id=<b>proceedings</b>", "snippet": "For complex <b>reading</b> tasks involving information seeking and context switching, researchers still rely on verbal reports via think-<b>aloud</b>. We present StrategicReading, an intelligent <b>reading</b> system running on unmodified smartphones, to understand high-level strategic <b>reading</b> behaviors on mobile devices. StrategicReading leverages multimodal behavior sensing and takes advantage of signals from camera-based gaze sensing, kinematic scrolling patterns, and cross-page behavior changes. Through a 40 ...", "dateLastCrawled": "2021-12-17T02:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "I Am The Mantra of Bliss, Awareness and Presence - I AM BOUNTIFUL ...", "url": "https://jw.journal-write.net.ru/78", "isFamilyFriendly": true, "displayUrl": "https://jw.journal-write.net.ru/78", "snippet": "This is the mantra of wisdom, both intellectual wisdom and the wisdom beyond all words and <b>thought</b> \u2013 spacious, clear, open mind imbued with bliss and contentment. Om \u2013 not only om <b>can</b> be considered to be the essence of the five wisdoms, but it <b>can</b> <b>also</b> be reflective of an awareness of the surrounding universe. \u201ci love the set up of the curriculum with the multiple ways to work on the subjects that has something4 the spirit, mind and the body. I love the sense of ahhh my body experience ...", "dateLastCrawled": "2022-01-25T08:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "what are the words formed in the above illustration", "url": "https://besthealth24h.com/tmbjdqxi/what-are-the-words-formed-in-the-above-illustration.html", "isFamilyFriendly": true, "displayUrl": "https://besthealth24h.com/tmbjdqxi/what-are-the-words-formed-in-the-above-illustration...", "snippet": "shape book read <b>aloud</b>; good governance definition; slayer hawaiian shirt chm; dauntless transmog 2021; offshore diving companies; spaghetti and beans recipe; Menu. what are the words formed in the above illustration. venice beach holidays; android52 future groove product; 2008 toyota rav4 engine replacement; characterization marches on; manchester united 98-99 jersey; zinc combustion reaction; kyle kuzma wizards jersey ; anthropomorphous in a sentence; what are the words formed in the above ...", "dateLastCrawled": "2022-01-27T17:52:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Reinforcement Learning with <b>Self-Attention</b> Networks for ...", "url": "https://www.researchgate.net/publication/353842430_Reinforcement_Learning_with_Self-Attention_Networks_for_Cryptocurrency_Trading", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353842430_Reinforcement_Learning_with_Self...", "snippet": "<b>Self-attention</b> networks are suitable for dealing with the problem because the attention mechanism <b>can</b> process long sequences of data and focus on the most relevant parts of the inputs. Transaction ...", "dateLastCrawled": "2022-01-20T14:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Modeling of Rakugo Speech and Its Various Speaking Styles: Toward ...", "url": "https://www.arxiv-vanity.com/papers/1911.00137/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1911.00137", "snippet": "We modeled rakugo speech using the state-of-the-art Tacotron 2 and an enhanced version of it with <b>self-attention</b> to better consider long-term dependency. We <b>also</b> used global style tokens and manually labeled context features to enrich speaking styles. Through a listening test, we found that the speech synthesis models could not yet reach the professional level, but interestingly, some of the synthetic speech entertained the listeners as well as analysis-by-synthesis speech. Although there is ...", "dateLastCrawled": "2021-08-28T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "RecSys &#39;19: Proceedings of the 13th ACM Conference on Recommender Systems", "url": "http://st.sigchi.org/publications/toc/recsys-2019.html", "isFamilyFriendly": true, "displayUrl": "st.sigchi.org/publications/toc/recsys-2019.html", "snippet": "Specifically, the Transformer applies a <b>self-attention</b> mechanism that directly models the global relationships between any pair of items in the whole list. We confirm that the performance <b>can</b> be further improved by introducing pre-trained embedding to learn personalized encoding functions for different users. Experimental results on both offline benchmarks and real-world online e-commerce systems demonstrate the significant improvements of the proposed re-ranking model.", "dateLastCrawled": "2022-02-02T17:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Towards Pervasive and Trustworthy Artificial Intelligence How ...", "url": "https://www.academia.edu/69399004/Towards_Pervasive_and_Trustworthy_Artificial_Intelligence_How_standards_can_put_a_great_technology_at_the_service_of_humankind_By", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69399004/Towards_Pervasive_and_Trustworthy_Artificial...", "snippet": "Towards Pervasive and Trustworthy Artificial Intelligence How standards <b>can</b> put a great technology at the service of humankind By. MPAI Community, 2021. Valeria Lazzaroli. Guido Perboli. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 36 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package. Translate PDF ...", "dateLastCrawled": "2022-02-02T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Proceedings of the 2017 Conference on Empirical Methods in Natural ...", "url": "https://aclanthology.org/volumes/D17-1/", "isFamilyFriendly": true, "displayUrl": "https://aclanthology.org/volumes/D17-1", "snippet": "The predicted <b>reading</b> time is then used to build a cognition based attention (CBA) <b>layer</b> for neural sentiment analysis. As a comprehensive model, We <b>can</b> capture attentions of words in sentences as well as sentences in documents. Different attention mechanisms <b>can</b> <b>also</b> be incorporated to capture other aspects of attentions. Evaluations show the CBA based method outperforms the state-of-the-art local context based attention methods significantly. This brings insight to how cognition grounded ...", "dateLastCrawled": "2022-02-01T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "End-to-end acoustic modelling for phone recognition of young readers ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167639321000959", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167639321000959", "snippet": "It aims at enabling extra practice in <b>reading</b> <b>aloud</b>, which is often not allowed a lot of time in class because of practical issues, while providing feedback and support in areas of difficulty for the student. Speech recognition for children learning to read is an arduous task: non-proficient readers\u2019 speech contains many disfluencies and <b>reading</b> mistakes that <b>can</b> be laborious to detect automatically. The usage of <b>reading</b> assistants in classroom environments adds up difficulties due to the ...", "dateLastCrawled": "2021-12-22T06:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "LAK21: LAK21: 11th International Learning Analytics and Knowledge ...", "url": "https://www.solaresearch.org/lak_toc/lak21/", "isFamilyFriendly": true, "displayUrl": "https://www.solaresearch.org/lak_toc/lak21", "snippet": "We <b>also</b> <b>compared</b> the models based on the level of feedback granularity: utterance-level (e.g., whether an utterance is a question or a statement), class session-level proportions by averaging across utterances (e.g., question incidence score of 48%), and session-level ordinal feedback based on pre-determined thresholds (e.g., question asking score is medium [vs. low or high]) and found that BERT generally provided more accurate feedback at all levels of granularity. Thus, BERT appears to be ...", "dateLastCrawled": "2022-01-31T05:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Capsule Networks for <b>Chinese Opinion Questions Machine Reading</b> ...", "url": "https://www.researchgate.net/publication/336542103_Capsule_Networks_for_Chinese_Opinion_Questions_Machine_Reading_Comprehension", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336542103_Capsule_Networks_for_Chinese...", "snippet": "Furthermore, it was <b>also</b> found that experimental students\u2019 lower-level skills of <b>reading</b> comprehension, such as information location and <b>text</b> comprehension, were significantly improved, rather ...", "dateLastCrawled": "2021-12-22T21:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Positional Artefacts Propagate Through Masked Language Model Embeddings", "url": "https://www.readkong.com/page/positional-artefacts-propagate-through-masked-language-5099136", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/positional-artefacts-propagate-through-masked-language...", "snippet": "We <b>layer</b>. <b>also</b> find that, from the 4th position to the final po- It is reasonable to conclude that, after the vector sition, the maximum element of 99.8% positional normalization performed by LN, the outliers embeddings is the 588th element. observed in the raw embeddings are lost. We Digging deeper, we observe similar patterns in hypothesize that these particular neurons are the <b>Layer</b> Normalization (LN, Ba et al. (2016)) pa- somehow important to the network, such that they rameters of both ...", "dateLastCrawled": "2022-01-17T01:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Augmenting Scientific Papers with Just-in-Time, Position-Sensitive ...", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445648", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445648", "snippet": "<b>Reading</b> interfaces <b>can</b> <b>also</b> assist researchers by helping them navigate to information of interest within a paper. For several years, interfaces for <b>reading</b> PDFs have provided standard affordances for jumping within a paper using hyperlinks. Typesetting software like LaTeX <b>can</b> automatically embed clickable links from references to figures, equations, and sections to the content they refer to, and from citations to bibliographies.", "dateLastCrawled": "2021-12-24T14:00:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Attention in Psychology, Neuroscience, and <b>Machine</b> <b>Learning</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7177153/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7177153", "snippet": "These weightings scale the word encodings themselves to create the next <b>layer</b> in the model, a process known as \u201c<b>self-attention</b>.\u201d This process repeats, and eventually interacts with the autoregressive decoder which <b>also</b> has attention mechanisms that allow it to flexibly focus on the encoded input (as in the standard form of attention) and on the previously generated output. The Transformer\u2014the name given to this new attention architecture\u2014outperformed many previous models and quickly ...", "dateLastCrawled": "2021-12-27T10:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>learning</b> glossary - DataTime", "url": "https://www.dtalg.com/article-1/", "isFamilyFriendly": true, "displayUrl": "https://www.dtalg.com/article-1", "snippet": "<b>self-attention</b> (<b>also</b> <b>called</b> <b>self-attention</b> <b>layer</b>) #language. A neural network <b>layer</b> that transforms a sequence of embeddings (for instance, token embeddings) into another sequence of embeddings. Each embedding in the output sequence is constructed by integrating information from the elements of the input sequence through an attention mechanism.", "dateLastCrawled": "2022-01-25T17:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Lecture 7: Transformers</b> - Deep <b>Learning</b>", "url": "https://chinmayhegde.github.io/dl-notes/notes/lecture07/", "isFamilyFriendly": true, "displayUrl": "https://chinmayhegde.github.io/dl-notes/notes/lecture07", "snippet": "<b>Self-Attention</b>. This is the point where papers-blogs-tweets-slides etc start talking about keys/values and attention mechanisms and everything goes a bit haywire. Let\u2019s just ignore all that for now, and instead talk about something <b>called</b> <b>self-attention</b>. The use of the \u201cself-\u201c prefix will become clear later on. Here is how it is defined.", "dateLastCrawled": "2022-02-03T05:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training ...", "url": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_Self_attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/69040947/Enhancing_LSTM_Models_with_<b>Self_attention</b>_and_Statef...", "snippet": "<b>Self-attention</b>, <b>also</b> known as intra-attention, is an attention mechanism relat- ing di\ufb00erent positions of a sequence in order to model dependencies between dif- ferent parts of the sequence. This di\ufb00ers from general attention in that instead of seeking to discover the \u201cimportant\u201d parts of the sequence relating to the net- work output, <b>self-attention</b> seeks to \ufb01nd the \u201cimportant\u201d portions of the sequence that relate to each other. This is done in order to leverage those intra ...", "dateLastCrawled": "2022-02-03T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning Papers: Molecules</b> - <b>Machine Learning</b> Applied", "url": "https://machinelearningapplied.com/machine-learning-papers-molecules/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>applied.com/<b>machine-learning-papers-molecules</b>", "snippet": "A <b>self-attention</b> based message passing neural network for predicting molecular lipophilicity and aqueous solubility - Tang et al 2020 . Efficient and accurate prediction of molecular properties, such as lipophilicity and solubility, is highly desirable for rational compound design in chemical and pharmaceutical industries. To this end, we build and apply a graph-neural-network framework <b>called</b> <b>self-attention</b>-based message-passing neural network (SAMPN) to study the relationship between ...", "dateLastCrawled": "2021-12-22T02:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Enhancing LSTM Models with <b>Self-attention</b> and Stateful Training", "url": "https://www.researchgate.net/publication/353690477_Enhancing_LSTM_Models_with_Self-attention_and_Stateful_Training", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353690477_Enhancing_LSTM_Models_with_Self...", "snippet": "<b>Self-attention</b>, <b>also</b> known as in tra-attention, is an attention mec hanism re- lating di\ufb00erent positions of a sequence in order to model dependencies b etween di\ufb00erent parts of the sequence.", "dateLastCrawled": "2022-01-13T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Attending to Attention. A summary of a revolutionary paper\u2026 | by Akash ...", "url": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/attending-to-attention-eba798f0e940", "snippet": "The encoder is composed of a stack of N = 6 identical layers. Each <b>layer</b> has two sub-layers. The first is a multi-head <b>self-attention</b> mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by <b>layer</b> normalization. That is, the output ...", "dateLastCrawled": "2022-01-25T10:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Transformers in NLP: A beginner friendly explanation | Towards Data Science", "url": "https://towardsdatascience.com/transformers-89034557de14", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>transformer</b>s-89034557de14", "snippet": "<b>Self attention</b>, sometimes <b>called</b> intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. In simpler terms, <b>self attention</b> helps us create similar connections but within the same sentence. Look at the following example: \u201cI poured water from the bottle into the cup until it was full.\u201d it =&gt; cup \u201cI poured water from the bottle into the cup until it was empty.\u201d it=&gt; bottle. By changing one word ...", "dateLastCrawled": "2022-02-02T13:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How to perform Text Summarization with Python, HuggingFace Transformers ...", "url": "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2020/12/21/easy-text-summarization-with-hugging...", "snippet": "Summary &amp; Example: Text Summarization with Transformers. Transformers are taking the world of language processing by storm. These models, which learn to interweave the importance of tokens by means of a mechanism <b>called</b> <b>self-attention</b> and without recurrent segments, have allowed us to train larger models without all the problems of recurrent neural networks.", "dateLastCrawled": "2022-02-02T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Journal of Physics: Conference Series PAPER OPEN ACCESS You may <b>also</b> ...", "url": "https://iopscience.iop.org/article/10.1088/1742-6596/1651/1/012019/pdf", "isFamilyFriendly": true, "displayUrl": "https://iopscience.iop.org/article/10.1088/1742-6596/1651/1/012019/pdf", "snippet": "Different <b>machine</b> <b>learning</b> techniques have been used in this field for many years. But recently, deep <b>learning</b> has caused more and more attention in the field of education. Deep <b>learning</b> is a <b>machine</b> <b>learning</b> method based on neural network structure of multi-<b>layer</b> processing units, and it has been successfully applied to a series of problems in the field of image recognition and natural language processing[2]. With the diversified cultivation of traditional universities and the development ...", "dateLastCrawled": "2021-12-29T04:07:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(self-attention (also called self-attention layer))  is like +(reading text aloud)", "+(self-attention (also called self-attention layer)) is similar to +(reading text aloud)", "+(self-attention (also called self-attention layer)) can be thought of as +(reading text aloud)", "+(self-attention (also called self-attention layer)) can be compared to +(reading text aloud)", "machine learning +(self-attention (also called self-attention layer) AND analogy)", "machine learning +(\"self-attention (also called self-attention layer) is like\")", "machine learning +(\"self-attention (also called self-attention layer) is similar\")", "machine learning +(\"just as self-attention (also called self-attention layer)\")", "machine learning +(\"self-attention (also called self-attention layer) can be thought of as\")", "machine learning +(\"self-attention (also called self-attention layer) can be compared to\")"]}