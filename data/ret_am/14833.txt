{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding the log <b>loss function</b> | by Susmith Reddy | Analytics ...", "url": "https://medium.com/analytics-vidhya/understanding-the-loss-function-of-logistic-regression-ac1eec2838ce", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/understanding-the-<b>loss-function</b>-of-logistic...", "snippet": "The reason MSE squares the <b>distance</b> <b>between</b> the <b>actual</b> and <b>the predicted</b> output values is to ... i.e., when the <b>actual</b> output <b>value</b> is 1 &amp; 0. 1) True output <b>value</b> = 1: Consider the model output ...", "dateLastCrawled": "2022-01-30T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> Functions in <b>Machine Learning</b> - 360DigiTMG", "url": "https://360digitmg.com/loss-functions-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://360digitmg.com/<b>loss</b>-functions-in-<b>machine-learning</b>", "snippet": "<b>Squared</b> Hinge <b>Loss</b>: This is an extension of the hinge <b>loss</b> and it is quite simply the square of the hinge <b>loss</b> function. Since this is a square of the original <b>loss</b>, it has some mathematical properties that make it easier to calculate the gradients.", "dateLastCrawled": "2022-01-30T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "Graph for -log(x) This is pretty simple, the more your input increases, the more output goes lower. If you have a small input(x=0.5) so the output is going to be high(y=0.305).", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Overfitting Regression Models: Problems, Detection, and Avoidance ...", "url": "https://statisticsbyjim.com/regression/overfitting-regression-models/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/regression/overfitting-regression-models", "snippet": "I guess these the resulting model minimizes the sum of squares (the sum of the <b>squared</b> \u201c<b>distance</b>\u201d <b>between</b> <b>the predicted</b> model <b>value</b> <b>and the actual</b> <b>value</b>. My question is this \u2013 Will the residuals for a model obtained by Least squares always sum to zero. I thought that the answer would be they would sum to zero but I\u2019m finding that they do for low order models n=0, n=1, n=2 but not for order n=3 for example (so the n=3 order has the form:", "dateLastCrawled": "2022-02-03T03:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Each <b>predicted</b> probability is compared to the <b>actual</b> class output <b>value</b> (0 or 1) and a score is calculated that penalizes the probability based on the <b>distance</b> from the expected <b>value</b>. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large difference (0.9 or 1.0).", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "8 Important <b>Regression Evaluation Metrics</b> | Python - AI ASPIRANT", "url": "https://aiaspirant.com/regression-evaluation-metrics/", "isFamilyFriendly": true, "displayUrl": "https://aiaspirant.com/<b>regression-evaluation-metrics</b>", "snippet": "Where \u0177 i is <b>the predicted</b> <b>value</b> of the i th sample, and y i is the corresponding <b>actual</b> <b>value</b>, and N is the number of samples.", "dateLastCrawled": "2022-02-02T21:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Mean <b>Square Error &amp; R2 Score Clearly Explained</b> \u2013 BMC Software | Blogs", "url": "https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/mean-<b>squared</b>-error-r2-and-variance-in-regression-analysis", "snippet": "The goal is to have a <b>value</b> that is low. What low means is quantified by the r2 score (explained below). In the code below, this is np.var(err), where err is an array of the differences <b>between</b> observed and <b>predicted</b> values and np.var() is the numpy array variance function. What is r2 score? The r2 score varies <b>between</b> 0 and 100%", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Regression</b> - Ai Quiz Questions", "url": "https://www.aionlinecourse.com/ai-quiz-questions/machine-learning/regression", "isFamilyFriendly": true, "displayUrl": "https://www.aionlinecourse.com/ai-quiz-questions/machine-learning/<b>regression</b>", "snippet": "B. <b>predicted</b> y\u2010coordinate <b>value</b> - <b>actual</b> y coordinate <b>value</b>. C. <b>actual</b> y\u2010coordinate <b>value</b> / <b>predicted</b> y\u2010coordinate <b>value</b>. D. None. view answer: A. <b>actual</b> y\u2010coordinate <b>value</b> - <b>predicted</b> y\u2010coordinate <b>value</b>. 4. How to see the <b>value</b> of residuals geometrically. A. The perpendicular <b>distance</b> <b>between</b> a data point and the <b>regression</b> line. B. The euclidian <b>distance</b> <b>between</b> a data point and the <b>regression</b> line. C. The horizontal <b>distance</b> <b>between</b> a data point and the <b>regression</b> line. D. The ...", "dateLastCrawled": "2022-01-31T03:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "How <b>Lasso Regression Works in Machine Learning</b>", "url": "https://dataaspirant.com/lasso-regression/", "isFamilyFriendly": true, "displayUrl": "https://dataaspirant.com/lasso-regression", "snippet": "Y is <b>the predicted</b> <b>value</b>, X is feature <b>value</b>, m is coefficients or weights, c is ... represents the <b>distance</b> <b>between</b> the <b>actual</b> data points and the model line in the above graph. Least-squares is the sum of squares of the <b>distance</b> <b>between</b> the points from the plotted curve. In linear regression, the best model is chosen in a way to minimize the least-squares. While performing lasso regression, we add a penalizing factor to the least-squares. That is, the model is chosen in a way to reduce the ...", "dateLastCrawled": "2022-02-03T11:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to plot <b>predicted</b> values vs the true <b>value</b>? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/58410187/how-to-plot-predicted-values-vs-the-true-value", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/58410187", "snippet": "I will <b>like</b> to make a plot of my machine learning model&#39;s <b>predicted</b> <b>value</b> vs the <b>actual</b> <b>value</b>. I made a prediction using random forest algorithm and will <b>like</b> to visualize the plot of true values and <b>predicted</b> values. I used the below code, but the plot isn&#39;t showing clearly the relationship <b>between</b> <b>the predicted</b> and <b>actual</b> values.", "dateLastCrawled": "2022-01-27T19:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "Graph for -log(x) This is pretty simple, the more your input increases, the more output goes lower. If you have a small input(x=0.5) so the output is going to be high(y=0.305).", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Each <b>predicted</b> probability is compared to the <b>actual</b> class output <b>value</b> (0 or 1) and a score is calculated that penalizes the probability based on the <b>distance</b> from the expected <b>value</b>. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large difference (0.9 or 1.0).", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "If x &gt; 0 <b>loss</b> will be x itself (higher <b>value</b>), if 0&lt;x&lt;1 <b>loss</b> will be 1 \u2014 x (smaller <b>value</b>) and if x &lt; 0 <b>loss</b> will be 0 (minimum <b>value</b>). For y =1, the <b>loss</b> is as high as the <b>value</b> of x .", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Machine Learning | G. Wu", "url": "https://guangyuwu.wordpress.com/2021/02/02/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://guangyuwu.wordpress.com/2021/02/02/<b>loss</b>-functions-in-machine-learning", "snippet": "TensorFlow: tf.keras.losses.binary_crossentropy Note, we add a very small <b>value</b>, epsilon \u03b5 (in this case 1E-7) to <b>the predicted</b> probabilities to avoid ever calculating the log of 0.0. This means that in practice, the best possible <b>loss</b> will be a <b>value</b> very close to zero, but not exactly zero.", "dateLastCrawled": "2022-01-12T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning With ML.NET - Evaluation Metrics</b>", "url": "https://rubikscode.net/2021/04/12/machine-learning-with-ml-net-evaluation-metrics/", "isFamilyFriendly": true, "displayUrl": "https://rubikscode.net/2021/04/12/<b>machine-learning-with-ml-net-evaluation-metrics</b>", "snippet": "It is quite simple, it finds the average <b>squared</b> <b>distance</b> (error) <b>between</b> <b>the predicted</b> and <b>actual</b> values. The formula that we use to calculate it is: Where N represents the number of samples in the dataset, yi is the <b>actual</b> <b>value</b> for the i-th sample, and yi\u2019 is <b>the predicted</b> <b>value</b> for the ith sample. The result is a non-negative <b>value</b> and the goal is to get this <b>value</b> as close to zero as possible. This function is often used as a <b>loss</b> function of a machine learning model. In the code, we ...", "dateLastCrawled": "2022-01-26T07:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Chapter 7: <b>Correlation and Simple Linear Regression</b> \u2013 Natural Resources ...", "url": "https://milnepublishing.geneseo.edu/natural-resources-biometrics/chapter/chapter-7-correlation-and-simple-linear-regression/", "isFamilyFriendly": true, "displayUrl": "https://milnepublishing.geneseo.edu/natural-resources-biometrics/chapter/chapter-7...", "snippet": "An ordinary least squares regression line minimizes the sum of the <b>squared</b> errors <b>between</b> the observed and <b>predicted</b> values to create a best fitting line. The differences <b>between</b> the observed and <b>predicted</b> values are <b>squared</b> to deal with the positive and negative differences. Coefficient of Determination. After we fit our regression line (compute b 0 and b 1), we usually wish to know how well the model fits our data. To determine this, we need to think back to the idea of analysis of ...", "dateLastCrawled": "2022-02-02T23:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>ML</b>.NET metrics - <b>ML</b>.NET | Microsoft Docs", "url": "https://docs.microsoft.com/en-us/dotnet/machine-learning/resources/metrics", "isFamilyFriendly": true, "displayUrl": "https://docs.microsoft.com/en-us/dotnet/machine-learning/resources/metrics", "snippet": "Logarithmic <b>loss</b> measures the performance of a classification model where the prediction input is a probability <b>value</b> <b>between</b> 0.00 and 1.00. Log-<b>loss</b> increases as <b>the predicted</b> probability diverges from the <b>actual</b> label. The closer to 0.00, the better. A perfect model would have a log-<b>loss</b> of 0.00. The goal of our machine learning models is to minimize this <b>value</b>. Log-<b>Loss</b> Reduction: Logarithmic <b>loss</b> reduction can be interpreted as the advantage of the classifier over a random prediction ...", "dateLastCrawled": "2022-02-02T10:15:00.0000000Z", "searchTags": [{"name": "search.mshattr.devlang", "content": "&quot;csharp&quot;; csharp"}], "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Mean <b>Square Error &amp; R2 Score Clearly Explained</b> \u2013 BMC Software | Blogs", "url": "https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/mean-<b>squared</b>-error-r2-and-variance-in-regression-analysis", "snippet": "The goal is to have a <b>value</b> that is low. What low means is quantified by the r2 score (explained below). In the code below, this is np.var(err), where err is an array of the differences <b>between</b> observed and <b>predicted</b> values and np.var() is the numpy array variance function. What is r2 score? The r2 score varies <b>between</b> 0 and 100%", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Model Evaluation Techniques in Machine Learning</b> - Nakatech", "url": "https://nakatech.com/model-evaluation-techniques-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://nakatech.com/<b>model-evaluation-techniques-in-machine-learning</b>", "snippet": "root_mean_<b>squared</b>_error= sqrt (mean_<b>squared</b>_error) R-<b>Squared</b>; R-<b>Squared</b> can be defined as the statistical measure that is used to represent the goodness fit of a regression model. The ideal <b>value</b> of r-square is known to be 1. Closer the <b>value</b> of r-square to 1, better is the fitted model. R-square is basically the comparison of the residual sum ...", "dateLastCrawled": "2022-01-29T04:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How <b>do Regression Trees Work? | DataDrivenInvestor</b>", "url": "https://www.datadriveninvestor.com/2020/04/13/how-do-regression-trees-work/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>datadriveninvestor</b>.com/2020/04/13/how-do-regression-trees-work", "snippet": "For instance, a drug dosage of 23 mg has a <b>predicted</b> <b>value</b> of 63% efficiency. Unfortunately, data doesn\u2019t always seem to present itself so well. More realistically, we might end up with data points being much noisier. This is seen in the plot below. Plot B. Applying linear regression to the data points (Plot B) above, we notice that there is a large difference <b>between</b> <b>the predicted</b> <b>value</b> <b>and the actual</b> <b>value</b> for a drug dosage of 23 mg. It seems evident that linear regression might not be ...", "dateLastCrawled": "2022-02-03T01:29:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Cutting Your Losses: <b>Loss</b> Functions &amp; the Sum of <b>Squared</b> Errors <b>Loss</b> ...", "url": "https://medium.com/@dustinstansbury/cutting-your-losses-loss-functions-the-sum-of-squared-errors-loss-4c467d52a511", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@dustinstansbury/cutting-your-<b>loss</b>es-<b>loss</b>-functions-the-sum-of...", "snippet": "The difference <b>between</b> <b>the predicted</b> and <b>actual</b> <b>value</b> is often referred to as the model \u201cerror\u201d or \u201cresidual\u201d for the datapoint. The semantics here being that small errors correspond to ...", "dateLastCrawled": "2022-02-01T13:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> Functions | <b>Loss</b> Functions in <b>Machine Learning</b> - 360DigiTMG", "url": "https://360digitmg.com/loss-functions-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://360digitmg.com/<b>loss</b>-functions-in-<b>machine-learning</b>", "snippet": "<b>Squared</b> Hinge <b>Loss</b>: This is an extension of the hinge <b>loss</b> and it is quite simply the square of the hinge <b>loss</b> function. Since this is a square of the original <b>loss</b>, it has some mathematical properties that make it easier to calculate the gradients.", "dateLastCrawled": "2022-01-30T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "Graph for -log(x) This is pretty simple, the more your input increases, the more output goes lower. If you have a small input(x=0.5) so the output is going to be high(y=0.305).", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What does RMSE really mean?. Root <b>Mean Square</b> Error (RMSE) is a\u2026 | by ...", "url": "https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e", "snippet": "This tells us heuristically that RMSE <b>can</b> <b>be thought</b> of as some kind of (normalized) <b>distance</b> <b>between</b> the vector of <b>predicted</b> values and the vector of observed values. B u t why are we dividing by n under the square root here? If we keep n (the number of observations) fixed, all it does is rescale the Euclidean <b>distance</b> by a factor of \u221a(1/n). It\u2019s a bit tricky to see why this is the right thing to do, so let\u2019s delve in a bit deeper. Imagine that our observed values are determined by ...", "dateLastCrawled": "2022-02-02T23:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Overfitting Regression Models: Problems, Detection, and Avoidance ...", "url": "https://statisticsbyjim.com/regression/overfitting-regression-models/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/regression/overfitting-regression-models", "snippet": "I guess these the resulting model minimizes the sum of squares (the sum of the <b>squared</b> \u201c<b>distance</b>\u201d <b>between</b> <b>the predicted</b> model <b>value</b> <b>and the actual</b> <b>value</b>. My question is this \u2013 Will the residuals for a model obtained by Least squares always sum to zero. I <b>thought</b> that the answer would be they would sum to zero but I\u2019m finding that they do for low order models n=0, n=1, n=2 but not for order n=3 for example (so the n=3 order has the form:", "dateLastCrawled": "2022-02-03T03:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mean <b>Square Error &amp; R2 Score Clearly Explained</b> \u2013 BMC Software | Blogs", "url": "https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.bmc.com</b>/blogs/mean-<b>squared</b>-error-r2-and-variance-in-regression-analysis", "snippet": "The goal is to have a <b>value</b> that is low. What low means is quantified by the r2 score (explained below). In the code below, this is np.var(err), where err is an array of the differences <b>between</b> observed and <b>predicted</b> values and np.var() is the numpy array variance function. What is r2 score? The r2 score varies <b>between</b> 0 and 100%", "dateLastCrawled": "2022-02-03T07:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "neural network - <b>Loss Function for Probability Regression</b> - Data ...", "url": "https://datascience.stackexchange.com/questions/45285/loss-function-for-probability-regression", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/45285", "snippet": "In the case when the domain of the output variable is discrete or continuous, and not binary, you could also consider the KL-divergence as the <b>loss</b> to compute the <b>distance</b> <b>between</b> <b>the predicted</b> probability distribution and the target one, for example. This is important also because in some implementations, for efficiency reasons, the cross entropy only get indexes as target variables and not distributions.", "dateLastCrawled": "2022-01-24T09:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Cost Function and <b>Loss</b> Function in Machine Learning - Shishir Kant Singh", "url": "http://shishirkant.com/cost-function-and-loss-function-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "shishirkant.com/cost-function-and-<b>loss</b>-function-in-machine-learning", "snippet": "Huber <b>Loss</b>. A comparison <b>between</b> L1 and L2 <b>loss</b> yields the following results: L1 <b>loss</b> is more robust than its counterpart. On taking a closer look at the formulas, one <b>can</b> observe that if the difference <b>between</b> <b>the predicted</b> <b>and the actual</b> <b>value</b> is high, L2 <b>loss</b> magnifies the effect when compared to L1. Since L2 succumbs to outliers, L1 <b>loss</b> ...", "dateLastCrawled": "2022-01-29T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "The main difference <b>between</b> the hinge <b>loss</b> and the cross entropy <b>loss</b> is that the former arises from trying to maximize the margin <b>between</b> our decision boundary and data points - thus attempting to ensure that each point is correctly and confidently classified*, while the latter comes from a maximum likelihood estimate of our model\u2019s parameters. The softmax function, whose scores are used by the cross entropy <b>loss</b>, allows us to interpret our model\u2019s scores as relative probabilities ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Loss</b> function <b>autoencoder</b> vs variational-<b>autoencoder</b> or MSE-<b>loss</b> vs ...", "url": "https://stats.stackexchange.com/questions/350211/loss-function-autoencoder-vs-variational-autoencoder-or-mse-loss-vs-binary-cross", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/350211", "snippet": "Cross-entropy <b>loss</b> is assymetrical.. If your true intensity is high, e.g. 0.8, generating a pixel with the intensity of 0.9 is penalized more than generating a pixel with intensity of 0.7.. Conversely if it&#39;s low, e.g. 0.3, predicting an intensity of 0.4 is penalized less than a <b>predicted</b> intensity of 0.2.. You might have guessed by now - cross-entropy <b>loss</b> is biased towards 0.5 whenever the ground truth is not binary. For a ground truth of 0.5, the per-pixel zero-normalized <b>loss</b> is equal to ...", "dateLastCrawled": "2022-02-01T22:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Cutting Your Losses: <b>Loss</b> Functions &amp; the Sum of <b>Squared</b> Errors <b>Loss</b> ...", "url": "https://medium.com/@dustinstansbury/cutting-your-losses-loss-functions-the-sum-of-squared-errors-loss-4c467d52a511", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@dustinstansbury/cutting-your-<b>loss</b>es-<b>loss</b>-functions-the-sum-of...", "snippet": "The difference <b>between</b> <b>the predicted</b> and <b>actual</b> <b>value</b> is often referred to as the model \u201cerror\u201d or \u201cresidual\u201d for the datapoint. The semantics here being that small errors correspond to ...", "dateLastCrawled": "2022-02-01T13:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding the log <b>loss function</b> | by Susmith Reddy | Analytics ...", "url": "https://medium.com/analytics-vidhya/understanding-the-loss-function-of-logistic-regression-ac1eec2838ce", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/understanding-the-<b>loss-function</b>-of-logistic...", "snippet": "The reason MSE squares the <b>distance</b> <b>between</b> the <b>actual</b> and <b>the predicted</b> output values is to penalize the samples whose <b>predicted</b> <b>value</b> is very far from the <b>actual</b> <b>value</b> heavily than when <b>compared</b> ...", "dateLastCrawled": "2022-01-30T14:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Squared Distance</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/squared-distance", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/computer-science/<b>squared-distance</b>", "snippet": "The <b>squared distance</b> <b>between</b> a pair of points, ... resulting in \u2018variances\u2019, and adding them, gives a overall measure of <b>loss</b> which <b>can</b> <b>be compared</b> with the <b>squared</b> distances before the generalised Procrustes analysis. It is convenient to express these variances relative to the total variance before the generalised Procrustes analysis (see also Dijksterhuis &amp; Punter, 1990). The thick lines remaining in Figure 7 cannot be made shorter, and these lines represent the <b>loss</b>, i.e. that what ...", "dateLastCrawled": "2022-01-29T05:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Cost Function &amp; <b>Loss</b> Function. In this article, I wanted to put\u2026 | by ...", "url": "https://nadeemm.medium.com/cost-function-loss-function-c3cab1ddffa4", "isFamilyFriendly": true, "displayUrl": "https://nadeemm.medium.com/cost-function-<b>loss</b>-function-c3cab1ddffa4", "snippet": "\u2018<b>Loss</b>\u2019 in Machine learning helps us understand the difference <b>between</b> <b>the predicted</b> <b>value</b> &amp; the <b>actual</b> <b>value</b>. The Function used to quantify this <b>loss</b> during the training phase in the form of a single real number is known as the \u201c<b>Loss</b> Function\u201d. These are used in t h ose supervised learning algorithms that use optimization techniques ...", "dateLastCrawled": "2022-02-02T15:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "The main difference <b>between</b> the hinge <b>loss</b> and the cross entropy <b>loss</b> is that the former arises from trying to maximize the margin <b>between</b> our decision boundary and data points - thus attempting to ensure that each point is correctly and confidently classified*, while the latter comes from a maximum likelihood estimate of our model\u2019s parameters. The softmax function, whose scores are used by the cross entropy <b>loss</b>, allows us to interpret our model\u2019s scores as relative probabilities ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Each <b>predicted</b> probability is <b>compared</b> to the <b>actual</b> class output <b>value</b> (0 or 1) and a score is calculated that penalizes the probability based on the <b>distance</b> from the expected <b>value</b>. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large difference (0.9 or 1.0).", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Types of Loss Function</b> - OpenGenus IQ: Computing Expertise", "url": "https://iq.opengenus.org/types-of-loss-function/", "isFamilyFriendly": true, "displayUrl": "https://iq.opengenus.org/<b>types-of-loss-function</b>", "snippet": "<b>Loss</b> function is an important part in artificial neural networks, which is used to measure the inconsistency <b>between</b> <b>predicted</b> <b>value</b> (^y) and <b>actual</b> label (y). It is a non-negative <b>value</b>, where the robustness of model increases along with the decrease of the <b>value</b> of <b>loss</b> function. At its core, a <b>loss</b> function is incredibly simple: it\u2019s a ...", "dateLastCrawled": "2022-01-28T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "Cross-entropy <b>loss</b> increases as <b>the predicted</b> probability <b>value</b> deviate from the <b>actual</b> label. Hinge <b>loss</b>. Hinge <b>loss</b> <b>can</b> be used as an alternative to cross-entropy, which was initially developed to use with a support vector machine algorithm. Hinge <b>loss</b> works best with the classification problem because target values are in the set of {-1,1 ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Loss</b> Functions in Machine Learning | G. Wu", "url": "https://guangyuwu.wordpress.com/2021/02/02/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://guangyuwu.wordpress.com/2021/02/02/<b>loss</b>-functions-in-machine-learning", "snippet": "Each <b>predicted</b> probability is <b>compared</b> to the <b>actual</b> class output <b>value</b> (0 or 1) and a score is calculated that penalizes the probability based on the <b>distance</b> from the expected <b>value</b>. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large difference (0.9 or 1.0). Cross-entropy <b>loss</b> is minimized, where smaller values represent a better model than larger values. A model that predicts perfect probabilities has a cross entropy or log ...", "dateLastCrawled": "2022-01-12T02:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Ways to Evaluate Regression Models | by ... - Towards Data Science", "url": "https://towardsdatascience.com/ways-to-evaluate-regression-models-77a3ff45ba70", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/ways-to-evaluate-regression-models-77a3ff45ba70", "snippet": "It <b>can</b> <b>be compared</b> <b>between</b> models whose errors are measured in the different units. Mathematically, ... MSE of 1 point is a difference of 1 point of <b>actual</b> <b>between</b> <b>predicted</b> and <b>actual</b>). In RAE and Relative RSE, you divide those differences by the variation of <b>actual</b>, so they have a scale from 0 to 1, and if you multiply this <b>value</b> by 100, you get similarity in 0\u2013100 scale (i.e. percentage). The values of \u2211(MeanofActual \u2014 <b>actual</b>)\u00b2 or \u2211|MeanofActual \u2014 <b>actual</b>| tell you how much ...", "dateLastCrawled": "2022-02-02T07:04:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine</b> <b>Learning</b> fundamentals I: An <b>analogy</b> | Finn Rietz.dev", "url": "http://www.finnrietz.dev/machine%20learning/part-1-analogy/", "isFamilyFriendly": true, "displayUrl": "www.finnrietz.dev/<b>machine</b> <b>learning</b>/part-1-<b>analogy</b>", "snippet": "And this is what the <b>loss</b> function does, so the <b>loss</b> function for a <b>Machine</b> <b>learning</b> algorithm is like the teacher for the real-world dermatologist in-training. In mathematical terms, the <b>loss</b> function could look something like this: \\(L = (y_i - \\hat{y_i})^2\\), where \\(y_i\\) is the actual output value (the one that the teacher has written down) and \\(\\hat{y_i}\\) is the one our <b>learning</b> algorithm produced.", "dateLastCrawled": "2022-01-16T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Bias-Variance Decomposition</b> - mlxtend", "url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/", "isFamilyFriendly": true, "displayUrl": "rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp", "snippet": "We can decompose a <b>loss</b> function such as the <b>squared</b> <b>loss</b> into three terms, a variance, bias, and a noise term (and the same is true for the decomposition of the 0-1 <b>loss</b> later). However, for simplicity, we will ignore the noise term. Before we introduce the <b>bias-variance decomposition</b> of the 0-1 <b>loss</b> for classification, let us start with the decomposition of the <b>squared</b> <b>loss</b> as an easy warm-up exercise to get familiar with the overall concept. The previous section already listed the common ...", "dateLastCrawled": "2022-01-31T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "CPSC 340: Data Mining <b>Machine</b> <b>Learning</b>", "url": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/L35.pdf", "snippet": "\u2022Exam <b>analogy</b> for types of supervised/semi-supervised <b>learning</b>: \u2013Regular supervised <b>learning</b>: ... \u2013Gaussian likelihood =&gt; <b>squared</b> <b>loss</b>. \u2013Laplace likelihood =&gt; absolute <b>loss</b>. \u2013Sigmoid likelihood =&gt; logistic regression. MAP Estimation \u2022We discussed MAP estimation: \u2013Prior can take into account that complex models can overfit. \u2022Makes connection between probabilities and regularization: Softmax <b>Loss</b> for Multi-Class Classification \u2022Sometimes it [s easier to define a likelihood ...", "dateLastCrawled": "2021-11-22T01:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machined Learnings: ML and OR: An <b>analogy</b> with cost-sensitive ...", "url": "http://www.machinedlearnings.com/2010/07/ml-and-or.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2010/07/ml-and-or.html", "snippet": "Nonetheless I&#39;ve been amusing myself by thinking about it, in particular trying to think about it from a <b>machine</b> <b>learning</b> reduction standpoint. The simplest well-understood reduction that I can think of which is analogous to supplying estimates to a linear program is the reduction of cost-sensitive multiclass classification (CSMC) to regression.", "dateLastCrawled": "2021-12-25T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "Optimization methods are applied to minimize the <b>loss</b> function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one <b>loss</b> is L0-1 = 1 (m &lt;= 0); in zero-one <b>loss</b>, value of <b>loss</b> is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this <b>loss</b> is it is not differentiable, non-convex, and also NP-hard. Hence, in order to make optimization feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Solutions to the exercises for <b>Machine</b> <b>Learning</b>", "url": "http://www.perfmath.com/ml/ml_liu_text_solutions.pdf", "isFamilyFriendly": true, "displayUrl": "www.perfmath.com/ml/ml_liu_text_solutions.pdf", "snippet": "<b>Machine</b> <b>Learning</b> A Quantitative Approach Henry H. Liu P PerfMath. ... Bayesian, (4) <b>Analogy</b>, and (5) Unsupervised <b>learning</b>. Pedro Domingos proposed these five ML paradigms, and \u00a71.3 explains briefly what each of these five ML paradigms is about. <b>MACHINE</b> <b>LEARNING</b>: A QUANTITATIVE APPROACH 5 2 <b>Machine</b> <b>Learning</b> Fundamentals Illustrated with Regression 2.1 Try to find a publicly available <b>machine</b> <b>learning</b> dataset and apply an end-to-end procedure similar to the one we used with the fuel economy ...", "dateLastCrawled": "2022-01-18T07:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Calculation of Bias &amp; Variance in</b> python | by Nallaperumal | Analytics ...", "url": "https://medium.com/analytics-vidhya/calculation-of-bias-variance-in-python-8f96463c8942", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>calculation-of-bias-variance-in</b>-python-8f96463c8942", "snippet": "For any <b>machine</b> <b>learning</b> the performance of a model can be determined and characterized in terms of Bias and Variance. In supervised <b>machine</b> <b>learning</b> an algorithm learns a model from training data ...", "dateLastCrawled": "2022-01-29T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Basic Concepts in Machine Learning</b>", "url": "https://machinelearningmastery.com/basic-concepts-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>basic-concepts-in-machine-learning</b>", "snippet": "What are the <b>basic concepts in machine learning</b>? I found that the best way to discover and get a handle on the <b>basic concepts in machine learning</b> is to review the introduction chapters to <b>machine learning</b> textbooks and to watch the videos from the first model in online courses. Pedro Domingos is a lecturer and professor on <b>machine learning</b> at the University of Washing and", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A novel <b>semi-supervised support vector machine with asymmetric</b> squared ...", "url": "https://link.springer.com/article/10.1007/s11634-020-00390-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11634-020-00390-y", "snippet": "In the field of <b>machine</b> <b>learning</b>, loss function is usually one of the key issues in designing <b>learning</b> algorithms since most problems require it to describe the cost of the discrepancy between the prediction and the observation. In fact, the use of the loss function can be traced back to a long time ago. For example, the least-square loss function for regression was already employed by Legendre, Gauss, and Adrain in the early 19th century (Steinwart and Christmann 2008). At present, various ...", "dateLastCrawled": "2021-11-13T10:19:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(squared loss)  is like +(distance between the predicted value and the actual value)", "+(squared loss) is similar to +(distance between the predicted value and the actual value)", "+(squared loss) can be thought of as +(distance between the predicted value and the actual value)", "+(squared loss) can be compared to +(distance between the predicted value and the actual value)", "machine learning +(squared loss AND analogy)", "machine learning +(\"squared loss is like\")", "machine learning +(\"squared loss is similar\")", "machine learning +(\"just as squared loss\")", "machine learning +(\"squared loss can be thought of as\")", "machine learning +(\"squared loss can be compared to\")"]}