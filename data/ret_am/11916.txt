{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Reinforcement Learning \u2014 <b>MDP</b>. Introduction | by Walter Laurito ...", "url": "https://medium.datadriveninvestor.com/reinforcement-learning-mdp-639aecec6da4", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/reinforcement-learning-<b>mdp</b>-639aecec6da4", "snippet": "This means that the <b>choices</b> the agent takes change the environment. Depending on which <b>choices</b> were taken some high or low rewards are emitted and the agent will need to take new actions depending on the updated state. <b>MDP</b> \u2014 <b>Markov Decision Process</b>. A <b>Markov Decision Process</b> is a mathematical framework for modelling <b>decision</b> <b>making</b>.", "dateLastCrawled": "2022-01-12T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b> Measurement Model | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11336-017-9570-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11336-017-9570-0", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) is a model for <b>decision</b> <b>making</b> in the presence of uncertainty based on a longitudinal cost\u2013benefit analysis (Puterman, 1994).MDPs have been used extensively in artificial intelligence and robotics to choose optimal actions in stochastic, dynamic situations (Mnih et al. 2015; Russell &amp; Norvig, 2009) and in economics to model individual choice strategies (Rust, 1994). Formally, an <b>MDP</b> is defined by \\(\\{S,A,T,R,\\gamma \\}\\) where S is the set of possible states ...", "dateLastCrawled": "2021-12-30T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Applying <b>Markov decision process</b> to understand driving decisions using ...", "url": "https://eprints.whiterose.ac.uk/162137/1/Applying%20Markov%20decision%20process%20to%20understand%20driving%20decisions%20using%20basic%20safety%20messages%20data.pdf", "isFamilyFriendly": true, "displayUrl": "https://eprints.whiterose.ac.uk/162137/1/Applying <b>Markov</b> <b>decision</b> <b>process</b> to understand...", "snippet": "<b>Decision</b> <b>Process</b> (<b>MDP</b>) (Bellman, 1954). A specific structure needs to be imposed on the <b>MDP</b> framework that models driver behaviors in terms of different maneuvers, which is explained below. 2.1. <b>Markov</b> <b>Decision</b> Processes The <b>Markov Decision Process</b>, according to (Bellman, 1954) is defined by a set of states ( ), a", "dateLastCrawled": "2021-11-19T08:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Finite Markov Decision Processes</b> - Obviously Awesome", "url": "https://bjpcjp.github.io/pdfs/math/markov-finite-RL.pdf", "isFamilyFriendly": true, "displayUrl": "https://bjpcjp.github.io/pdfs/math/<b>markov</b>-finite-RL.pdf", "snippet": "<b>Finite Markov Decision Processes</b> In this chapter we introduce the formal problem of \ufb01nite <b>Markov</b> <b>decision</b> processes, or \ufb01nite MDPs, which we try to solve in the rest of the book. This problem involves evaluative feedback, as in bandits, but also an associative aspect\u2014choosing di\u21b5erent actions in di\u21b5erent situations. MDPs are a classical formalization of sequential <b>decision</b> <b>making</b>, where actions in\ufb02uence not just immediate rewards, but also subsequent situations, or states, and ...", "dateLastCrawled": "2021-09-16T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Interactive visualization for testing Markov Decision</b> Processes: <b>MDP</b> VIS", "url": "https://www.sciencedirect.com/science/article/pii/S1045926X16301951", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1045926X16301951", "snippet": "To address a broader class of optimization problems, we target the common optimization formulation of a <b>Markov Decision Process</b> (<b>MDP</b>). In an <b>MDP</b>, the state of the world evolves stochastically from one state to another depending on the action chosen at each time step. A scalar reward is received at each time step depending on the system state and the chosen action. An <b>MDP</b> is solved by learning a <b>decision</b> <b>making</b> rule (policy) that maximizes the long-term sum of rewards.", "dateLastCrawled": "2022-02-02T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Algebraic <b>Markov</b> <b>Decision</b> Processes.", "url": "https://www.researchgate.net/publication/220812668_Algebraic_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220812668_Algebraic_<b>Markov</b>_<b>Decision</b>_<b>Process</b>es", "snippet": "Paul Weng. <b>Markov</b> <b>decision</b> processes (<b>MDP</b>) have become one of the standard models for <b>decision</b>-theoretic planning problems under uncertainty. In its standard form, rewards are assumed to be ...", "dateLastCrawled": "2022-01-07T21:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Modeling other minds: Bayesian inference explains human <b>choices</b> in ...", "url": "https://www.science.org/doi/10.1126/sciadv.aax8783", "isFamilyFriendly": true, "displayUrl": "https://www.science.org/doi/10.1126/sciadv.aax8783", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) is a tuple (S, A, T, R), where S represents the set of states of the environment, A is the set of actions, T is the transition function S \u00d7 S \u00d7 A \u2192 [0,1] that determines the probability of the next state given the current state and action, i.e., T(s \u2032, s, a) = P(s \u2032 \u2223 s, a), and R is the reward function S \u00d7 A \u2192 R representing the reward associated with each state and action .", "dateLastCrawled": "2022-02-03T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Is life just a <b>Markov decision process</b>? - Quora", "url": "https://www.quora.com/Is-life-just-a-Markov-decision-process", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-life-just-a-<b>Markov-decision-process</b>", "snippet": "Answer (1 of 6): Yes it is, you can predict the state of the universe at time t from knowing everything about time t-1. All the information you need is contained at t-1, all the information of times t-i that would impact t is also in t-1. But, not unlike John Snow, we know nothing. And hence, it...", "dateLastCrawled": "2022-01-21T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Reinforcement Learning \u2014 <b>MDP</b>. Introduction | by Walter Laurito | Data ...", "url": "https://medium.com/datadriveninvestor/reinforcement-learning-mdp-639aecec6da4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/datadriveninvestor/reinforcement-learning-<b>mdp</b>-639aecec6da4", "snippet": "Reinforcement Learning \u2014 <b>MDP</b>. Walter Laurito. Follow. Mar 27 \u00b7 11 min read. Introduction. Reinforcement learning is based on the reward hypothesis. All good can be described by the maximisation ...", "dateLastCrawled": "2020-12-29T13:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "E-commerce Search Re-<b>Ranking as a Reinforcement Learning Problem</b> | by ...", "url": "https://towardsdatascience.com/e-commerce-search-re-ranking-as-a-reinforcement-learning-problem-a9d1561edbd0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/e-commerce-search-re-ranking-as-a-reinforcement...", "snippet": "<b>Markov Decision Process</b>: refers to a discrete-time stochastic control <b>process</b>. It provides a mathematical framework for the <b>decision</b>-<b>making</b> <b>process</b> where outcomes are partly random and partly controlled by the <b>decision</b>-maker. It contains the following: A <b>Markov Decision Process</b>(<b>MDP</b>) is a tuple M = (State Space, Action Space, Reward, State Transfer Function, Discount Rate). The objective of the <b>MDP</b> is to find a policy the maximizes the expected accumulative reward starting from any state s ...", "dateLastCrawled": "2022-01-21T19:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Process</b> Measurement Model | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11336-017-9570-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11336-017-9570-0", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) is a model for <b>decision</b> <b>making</b> in the presence of uncertainty based on a longitudinal cost\u2013benefit analysis (Puterman, 1994).MDPs have been used extensively in artificial intelligence and robotics to choose optimal actions in stochastic, dynamic situations (Mnih et al. 2015; Russell &amp; Norvig, 2009) and in economics to model individual choice strategies (Rust, 1994). Formally, an <b>MDP</b> is defined by \\(\\{S,A,T,R,\\gamma \\}\\) where S is the set of possible states ...", "dateLastCrawled": "2021-12-30T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov</b> <b>Decision</b> Processes", "url": "https://www.researchgate.net/publication/304193498_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/304193498_<b>Markov</b>_<b>Decision</b>_<b>Process</b>es", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) is a tuple M = (S, Act, P, s init , rew ) where S is a finite set of states, Act a finite set of actions, s init \u2208 S the initial state, P : S \u00d7 Act \u00d7 S \u2192 [0 ...", "dateLastCrawled": "2021-11-15T10:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Interactive visualization for testing Markov Decision</b> Processes: <b>MDP</b> VIS", "url": "https://www.sciencedirect.com/science/article/pii/S1045926X16301951", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1045926X16301951", "snippet": "To address a broader class of optimization problems, we target the common optimization formulation of a <b>Markov Decision Process</b> (<b>MDP</b>). In an <b>MDP</b>, the state of the world evolves stochastically from one state to another depending on the action chosen at each time step. A scalar reward is received at each time step depending on the system state and the chosen action. An <b>MDP</b> is solved by learning a <b>decision</b> <b>making</b> rule (policy) that maximizes the long-term sum of rewards.", "dateLastCrawled": "2022-02-02T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Applying <b>Markov decision process</b> to understand driving decisions using ...", "url": "https://www.sciencedirect.com/science/article/pii/S0968090X20305490", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0968090X20305490", "snippet": "<b>Markov</b> <b>decision</b> processes. The <b>Markov Decision Process</b>, according to (Bellman, 1954) is defined by a set of states (s \u220a S), a set of all possible actions (a \u220a A), a transition function (T (s, a, s &#39;)), a reward function (R (s)), and a discount factor (\u03b3). To make the model mathematically tractable, the discount factor is restricted to 0 ...", "dateLastCrawled": "2022-01-15T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Dynamic treatment selection and modification for personalised blood ...", "url": "https://dash.harvard.edu/bitstream/handle/1/34493144/5695480.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://dash.harvard.edu/bitstream/handle/1/34493144/5695480.pdf?sequence=1", "snippet": "<b>Markov decision process</b> (<b>MDP</b>) model\u2014a model in which outcomes are partly under the control of a <b>decision</b>-maker and partly based on probabilistic calculations from high-quality meta-analytic data. Prior work suggests that <b>person</b>-alising optimal treatment policies using an <b>MDP</b> framework could improve patient health outcomes compared with JNC7", "dateLastCrawled": "2022-01-02T07:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Modeling other minds: Bayesian inference explains human <b>choices</b> in ...", "url": "https://www.science.org/doi/10.1126/sciadv.aax8783", "isFamilyFriendly": true, "displayUrl": "https://www.science.org/doi/10.1126/sciadv.aax8783", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) is a tuple (S, A, T, R), where S represents the set of states of the environment, A is the set of actions, T is the transition function S \u00d7 S \u00d7 A \u2192 [0,1] that determines the probability of the next state given the current state and action, i.e., T(s \u2032, s, a) = P(s \u2032 \u2223 s, a), and R is the reward function S \u00d7 A \u2192 R representing the reward associated with each state and action .", "dateLastCrawled": "2022-02-03T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The egg drop <b>puzzle: brute force, Dynamic Programming</b>, and <b>Markov</b> ...", "url": "https://www.declanoller.com/2018/09/03/the-egg-drop-puzzle-brute-force-dynamic-programming-and-markov-decision-processes/", "isFamilyFriendly": true, "displayUrl": "https://www.declanoller.com/2018/09/03/the-egg-drop-puzzle-brute-force-dynamic...", "snippet": "Last, I do a <b>similar</b> thing, but with a <b>Markov Decision Process</b>. Brute force. In this section, I\u2019m going to try the brute force method. I.e., for a given drop strategy I test, I\u2019m going to build an \u201censemble\u201d where I have it use that strategy for every possible \u201cbreak floor\u201d (the floor the eggs happen to break on). This will give me the definitive average and worst case numbers for each strategy, though it will be only \u201cempirical\u201d; i.e., I\u2019ll only get the best strategy for ...", "dateLastCrawled": "2022-02-01T17:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Unit 4 | PDF | Utility | Applied Mathematics", "url": "https://www.scribd.com/presentation/554787054/Unit-4", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/presentation/554787054/Unit-4", "snippet": "\u2022 A <b>Markov Decision Process</b> (<b>MDP</b>) model contains: ... \u2022 A mathematical representation of a complex <b>decision</b> <b>making</b> <b>process</b> is \u201c<b>Markov</b> <b>Decision</b> Processes \u201d (<b>MDP</b>). \u2022 We do that by attaching rewards and punishments to different outcomes, which ultimately drive the machine to find the \u201cright\u201d priorities. \u2022 An example of it is self-driving cars. There the system \u201cprefers\u201d one optimality versus another, taking into consideration rewards and punishments along the way ...", "dateLastCrawled": "2022-02-03T07:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Is life just a <b>Markov decision process</b>? - Quora", "url": "https://www.quora.com/Is-life-just-a-Markov-decision-process", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-life-just-a-<b>Markov-decision-process</b>", "snippet": "Answer (1 of 6): Yes it is, you can predict the state of the universe at time t from knowing everything about time t-1. All the information you need is contained at t-1, all the information of times t-i that would impact t is also in t-1. But, not unlike John Snow, we know nothing. And hence, it...", "dateLastCrawled": "2022-01-21T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are <b>the differences between hidden Markov models</b> and partially ...", "url": "https://www.quora.com/What-are-the-differences-between-hidden-Markov-models-and-partially-observed-Markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>the-differences-between-hidden-Markov-models</b>-and...", "snippet": "Answer: They are <b>similar</b> in that both assume the true state of the system is unknown, but we can hope to make inferences about it based on observations. Also, both make some sort of <b>Markov</b> assumption about the state not depending on the entire history of the system. The key difference, at a high...", "dateLastCrawled": "2022-01-13T08:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) Game-<b>based Abstraction for Markov Decision Processes</b>.", "url": "https://www.researchgate.net/publication/221406673_Game-based_Abstraction_for_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221406673_Game-based_Abstraction_for_<b>Markov</b>...", "snippet": "A <b>Markov decision process</b> <b>can</b> <b>be thought</b> of as a turn-based stochastic game in which there are no player 2 v ertices and where there is a strict alternation between play er 1 and probabilistic ...", "dateLastCrawled": "2022-01-02T06:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Reinforcement Learning \u2014 <b>MDP</b>. Introduction | by Walter Laurito ...", "url": "https://medium.datadriveninvestor.com/reinforcement-learning-mdp-639aecec6da4", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/reinforcement-learning-<b>mdp</b>-639aecec6da4", "snippet": "<b>MDP</b> \u2014 <b>Markov Decision Process</b>. A <b>Markov Decision Process</b> is a mathematical framework for modelling <b>decision</b> <b>making</b>. As described above the environment\u2019s state is the same as the agent\u2019s state. This is always the case for a so-called fully observable <b>MDP</b>. In our <b>MDP</b> we have the following components: A: a finite set of actions an agent <b>can</b> take", "dateLastCrawled": "2022-01-12T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reinforcement Learning \u2014 <b>MDP</b>. Introduction | by Walter Laurito | Data ...", "url": "https://medium.com/datadriveninvestor/reinforcement-learning-mdp-639aecec6da4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/datadriveninvestor/reinforcement-learning-<b>mdp</b>-639aecec6da4", "snippet": "Reinforcement Learning \u2014 <b>MDP</b>. Walter Laurito. Follow. Mar 27 \u00b7 11 min read. Introduction. Reinforcement learning is based on the reward hypothesis. All good <b>can</b> be described by the maximisation ...", "dateLastCrawled": "2020-12-29T13:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "R&amp;D Connections - Simulations of <b>Thought</b>: The Role of Computational ...", "url": "https://www.ets.org/Media/Research/pdf/RD_Connections_26.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ets.org</b>/Media/Research/pdf/RD_Connections_26.pdf", "snippet": "The <b>Markov decision process</b> (<b>MDP</b>) is a cognitive model that predicts sequential <b>decision</b> <b>making</b> in open-ended environments, such as navigating various steps to solve a problem. <b>MDP</b> models are built to simulate human <b>decision</b> <b>making</b> as a function of a <b>person</b>\u2019s goals and beliefs about the world (Baker, Saxe, &amp; Tenenbaum, 2011). The models are designed to account for the current state of the environment as understood by the <b>decision</b> maker, as well as that <b>person</b>\u2019s predictions of how various ...", "dateLastCrawled": "2022-01-07T18:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Is life just a <b>Markov decision process</b>? - Quora", "url": "https://www.quora.com/Is-life-just-a-Markov-decision-process", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-life-just-a-<b>Markov-decision-process</b>", "snippet": "Answer (1 of 6): Yes it is, you <b>can</b> predict the state of the universe at time t from knowing everything about time t-1. All the information you need is contained at t-1, all the information of times t-i that would impact t is also in t-1. But, not unlike John Snow, we know nothing. And hence, it...", "dateLastCrawled": "2022-01-21T05:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Markov Decision Process</b> | AI Strategy &amp; Policy Blog", "url": "https://aistrategyblog.com/tag/markov-decision-process/", "isFamilyFriendly": true, "displayUrl": "https://aistrategyblog.com/tag/<b>markov-decision-process</b>", "snippet": "The survey, \u201cCorporate data-driven <b>decision</b> <b>making</b> and the role of Artificial Intelligence in the <b>decision</b> <b>making</b> <b>process</b>\u201d, reveals the general perception of the corporate data-driven environment available to corporate <b>decision</b> maker, e.g., the structure and perceived quality of available data. Furthermore, the survey explores the <b>decision</b> makers\u2019 opinions about bias in available data and applied tooling, as well as their own and their peers biases and possible impact on their ...", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) A planning system based on <b>Markov</b> <b>decision</b> processes to guide ...", "url": "https://www.academia.edu/463502/A_planning_system_based_on_Markov_decision_processes_to_guide_people_with_dementia_through_activities_of_daily_living", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/463502/A_planning_system_based_on_<b>Markov</b>_<b>decision</b>_<b>process</b>es...", "snippet": "A planning system based on <b>Markov</b> <b>decision</b> processes to guide people with dementia through activities of daily living. Information \u2026, 2006 . Craig Boutilier. Jesse Hoey. Geoff Fernie. Pascal Poupart. Jennifer Boger. Alex Mihailidis. Jen T. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 37 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package. Translate PDF. Related Papers. A <b>Decision</b> ...", "dateLastCrawled": "2022-01-06T04:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>MDP</b> | AI Strategy &amp; Policy Blog", "url": "https://aistrategyblog.com/category/mdp/", "isFamilyFriendly": true, "displayUrl": "https://aistrategyblog.com/category/<b>mdp</b>", "snippet": "Furthermore, the data-driven <b>decision</b>-<b>making</b> <b>process</b>, as described above, have a substantially higher amount of bias-entry points than a <b>decision</b>-<b>making</b> <b>process</b> starting with an idea or hypothesis followed by a well <b>thought</b> through experimental design (e.g., as in the case of our \u201cideal data-driven <b>decision</b> <b>process</b>\u201d). As a consequence, a business may incur a substantial risk of reputational damage. On top of the consequences of <b>making</b> a poor data-driven business <b>decision</b>.", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Modeling other minds: Bayesian inference explains human <b>choices</b> in ...", "url": "https://www.science.org/doi/10.1126/sciadv.aax8783", "isFamilyFriendly": true, "displayUrl": "https://www.science.org/doi/10.1126/sciadv.aax8783", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) is a tuple (S, A, T, R), where S represents the set of states of the environment, A is the set of actions, T is the transition function S \u00d7 S \u00d7 A \u2192 [0,1] that determines the probability of the next state given the current state and action, i.e., T(s \u2032, s, a) = P(s \u2032 \u2223 s, a), and R is the reward function S \u00d7 A \u2192 R representing the reward associated with each state and action .", "dateLastCrawled": "2022-02-03T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Fundamentals of Reinforcement Learning (Coursera course notes -Week 2 ...", "url": "https://abhilashasaroj-genuine.medium.com/fundamentals-of-reinforcement-learning-621df1f1740e", "isFamilyFriendly": true, "displayUrl": "https://abhilashasaroj-genuine.medium.com/fundamentals-of-reinforcement-learning-621df...", "snippet": "The time steps in <b>MDP</b> problem <b>can</b> be fixed time intervals or successive stages of <b>decision</b> <b>making</b> and action <b>choices</b>. The agent and environment interact at sequence of discrete time steps. The agent receives information about situation of the environment as state (S_{t}) among set of possible states S and chooses an action (A_{t}) among set of possible actions in the state, A(s). In the next time step, in response to last action, the environment gives a reward R_{t+1} that belongs to set of ...", "dateLastCrawled": "2022-02-01T10:41:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Process</b> Measurement Model | SpringerLink", "url": "https://link.springer.com/article/10.1007/s11336-017-9570-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11336-017-9570-0", "snippet": "A <b>Markov decision process</b> (<b>MDP</b>) is a model for <b>decision</b> <b>making</b> in the presence of uncertainty based on a longitudinal cost\u2013benefit analysis (Puterman, 1994).MDPs have been used extensively in artificial intelligence and robotics to choose optimal actions in stochastic, dynamic situations (Mnih et al. 2015; Russell &amp; Norvig, 2009) and in economics to model individual choice strategies (Rust, 1994). Formally, an <b>MDP</b> is defined by \\(\\{S,A,T,R,\\gamma \\}\\) where S is the set of possible states ...", "dateLastCrawled": "2021-12-30T09:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Dynamic treatment selection and modification for personalised blood ...", "url": "https://dash.harvard.edu/bitstream/handle/1/34493144/5695480.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://dash.harvard.edu/bitstream/handle/1/34493144/5695480.pdf?sequence=1", "snippet": "<b>Markov decision process</b> (<b>MDP</b>) model\u2014a model in which outcomes are partly under the control of a <b>decision</b>-maker and partly based on probabilistic calculations from high-quality meta-analytic data. Prior work suggests that <b>person</b>-alising optimal treatment policies using an <b>MDP</b> framework could improve patient health outcomes <b>compared</b> with JNC7", "dateLastCrawled": "2022-01-02T07:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Dynamic treatment selection and modification for ... - BMJ Open", "url": "https://bmjopen.bmj.com/content/7/11/e018374", "isFamilyFriendly": true, "displayUrl": "https://bmjopen.bmj.com/content/7/11/e018374", "snippet": "Design, setting and participants We developed a <b>Markov decision process</b> (<b>MDP</b>) model to incorporate meta-analytic data and estimate the optimal treatment for maximising discounted lifetime quality-adjusted life-years (QALYs) based on individual patient characteristics, incorporating medication adjustment <b>choices</b> when a patient incurs side effects. We <b>compared</b> the <b>MDP</b> to current US blood pressure treatment guidelines (the Eighth Joint National Committee, JNC8) and a variant of current ...", "dateLastCrawled": "2021-09-09T14:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Interactive visualization for testing Markov Decision</b> Processes: <b>MDP</b> VIS", "url": "https://www.sciencedirect.com/science/article/pii/S1045926X16301951", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1045926X16301951", "snippet": "To address a broader class of optimization problems, we target the common optimization formulation of a <b>Markov Decision Process</b> (<b>MDP</b>). In an <b>MDP</b>, the state of the world evolves stochastically from one state to another depending on the action chosen at each time step. A scalar reward is received at each time step depending on the system state and the chosen action. An <b>MDP</b> is solved by learning a <b>decision</b> <b>making</b> rule (policy) that maximizes the long-term sum of rewards.", "dateLastCrawled": "2022-02-02T20:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Dynamic treatment selection and modification for personalised blood ...", "url": "https://europepmc.org/article/PMC/PMC5695480", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC5695480", "snippet": "DESIGN, SETTING AND PARTICIPANTS:We developed a <b>Markov decision process</b> (<b>MDP</b>) model to incorporate meta-analytic data and estimate the optimal treatment for maximising discounted lifetime quality-adjusted life-years (QALYs) based on individual patient characteristics, incorporating medication adjustment <b>choices</b> when a patient incurs side effects. We <b>compared</b> the <b>MDP</b> to current US blood pressure treatment guidelines (the Eighth Joint National Committee, JNC8) and a variant of current ...", "dateLastCrawled": "2021-09-26T11:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mobile Edge Offloading Using <b>Markov</b> <b>Decision</b> Processes | Request PDF", "url": "https://www.researchgate.net/publication/325801695_Mobile_Edge_Offloading_Using_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/325801695_Mobile_Edge_Offloading_Using_<b>Markov</b>...", "snippet": "To address this issue, this study proposes a <b>Markov Decision Process</b> (<b>MDP</b>) based methodology to intelligently make such <b>choices</b> while optimizing multiple objectives. Results demonstrate an 17.47% ...", "dateLastCrawled": "2022-01-25T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Markov Decision Process</b> | AI Strategy &amp; Policy Blog", "url": "https://aistrategyblog.com/tag/markov-decision-process/", "isFamilyFriendly": true, "displayUrl": "https://aistrategyblog.com/tag/<b>markov-decision-process</b>", "snippet": "The survey, \u201cCorporate data-driven <b>decision</b> <b>making</b> and the role of Artificial Intelligence in the <b>decision</b> <b>making</b> <b>process</b>\u201d, reveals the general perception of the corporate data-driven environment available to corporate <b>decision</b> maker, e.g., the structure and perceived quality of available data. Furthermore, the survey explores the <b>decision</b> makers\u2019 opinions about bias in available data and applied tooling, as well as their own and their peers biases and possible impact on their ...", "dateLastCrawled": "2022-02-02T08:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Online Markov Decision Processes</b> - ResearchGate", "url": "https://www.researchgate.net/publication/220442752_Online_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220442752_<b>Online_Markov_Decision_Processes</b>", "snippet": "We consider the adversarial <b>Markov Decision Process</b> (<b>MDP</b>) problem, where the rewards for the <b>MDP</b> <b>can</b> be adversarially chosen, and the transition function <b>can</b> be either known or unknown. In both ...", "dateLastCrawled": "2022-02-01T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Applying <b>Markov decision process</b> to understand driving decisions using ...", "url": "https://www.sciencedirect.com/science/article/pii/S0968090X20305490", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0968090X20305490", "snippet": "Microscopic driving decisions are analyzed using Connected Vehicle Data and <b>Markov Decision Process</b>. \u2022 Speed <b>choices</b> were extracted from <b>Basic Safety Messages</b> to understand drivers\u2019 behavior. \u2022 States were defined based on crowdedness surrounding the host vehicle and distance to the front object. \u2022 Perceived individual rewards from state transitions were estimated from revealed <b>choices</b>. \u2022 A method for learning personalized driving preferences was developed, which <b>can</b> be used to ...", "dateLastCrawled": "2022-01-15T03:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What are <b>the differences between hidden Markov models</b> and partially ...", "url": "https://www.quora.com/What-are-the-differences-between-hidden-Markov-models-and-partially-observed-Markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>the-differences-between-hidden-Markov-models</b>-and...", "snippet": "Answer: They are similar in that both assume the true state of the system is unknown, but we <b>can</b> hope to make inferences about it based on observations. Also, both make some sort of <b>Markov</b> assumption about the state not depending on the entire history of the system. The key difference, at a high...", "dateLastCrawled": "2022-01-13T08:18:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does <b>Markov Decision Process</b> matter in Reinforcement <b>Learning</b>? | by ...", "url": "https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-does-malkov-<b>decision</b>-<b>process</b>-matter-in...", "snippet": "It is named by <b>analogy</b> to \u201cone-armed bandit\u201d(= a slot <b>machine</b>) although the framework has k levers instead of one. ... we introduce <b>Markov Decision Process</b>(<b>MDP</b>) to solve such a problem. An <b>MDP</b> consists of two elements; the agent and the environment. The agent is a learner or <b>decision</b>-maker. In the above example, the agent is the rabbit. The environment is everything surrounding the agent. In the example, the environment includes everything in the field where the rabbit is with food and ...", "dateLastCrawled": "2022-01-31T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b>: How Does Value Iteration Work? | Baeldung on ...", "url": "https://www.baeldung.com/cs/mdp-value-iteration", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>mdp</b>-value-iteration", "snippet": "From this point, we can make an <b>analogy</b> with the <b>Markov</b> model since the solution for this problem is a sequence of actions. A <b>Markov Decision Process</b> is used to model the agent, considering that the agent itself generates a series of actions. In the real world, we can have observable, hidden, or partially observed states, depending on the ...", "dateLastCrawled": "2022-01-30T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement <b>Learning</b> | Ioannis Anifantakis | Analytics Vidhya", "url": "https://medium.com/analytics-vidhya/reinforcement-learning-basic-understanding-4fcb91ba4e4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/reinforcement-<b>learning</b>-basic-understanding-4fcb91ba4e4", "snippet": "<b>Markov Decision Process</b> (<b>MDP</b>) The States, Actions, Rewards, their mechanics (known as One-Step Dynamics ), together with the discount rate (\u03b3) define a <b>Markov Decision Process</b> (<b>MDP</b>) .", "dateLastCrawled": "2021-08-02T19:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "18.1. <b>Markov Decision Process</b> (<b>MDP</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.0 ...", "url": "http://preview.d2l.ai.s3-website-us-west-2.amazonaws.com/d2l-en/master/chapter_reinforcement_learning/mdp.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai.s3-website-us-west-2.amazonaws.com/...reinforcement_<b>learning</b>/<b>mdp</b>.html", "snippet": "In this section, we will discuss how to formulate reinforcement <b>learning</b> problems using <b>Markov</b> <b>decision</b> processes (MDPs) and describe in detail various components of MDPs. Definition of an <b>MDP</b> \u00b6 A <b>Markov decision process</b> (<b>MDP</b>) is a model for how the state of a system evolves as different actions are applied to it.", "dateLastCrawled": "2022-01-27T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Markov decision process</b>: policy iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-policy-iteration-42d35ee87c82?source=post_internal_links---------0-------------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-policy-iteration-42d35ee87c82?source=...", "snippet": "<b>Markov decision process</b>: policy iteration with code implementation . Nan. Dec 19, 2021 \u00b7 16 min read. In today\u2019s story we focus on policy iteration of <b>MDP</b>. We are still using the grid world ...", "dateLastCrawled": "2022-01-22T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>", "url": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "snippet": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>. <b>Machine</b> <b>Learning</b> [cs.LG]. Uni-versit\u00e9 Paris Saclay (COmUE), 2019. English. \uffffNNT: 2019SACLS588\uffff. \ufffftel-02422144v2\uffff I would like to dedicate this thesis to my loving parents doctorat CLS588 Meta-<b>Learning</b> as a <b>Markov Decision Process</b> Th\u00e8se de doctorat de l\u2019Universit\u00e9 Paris-Saclay pr\u00e9par\u00e9e \u00e0 l\u2019Universit\u00e9 Paris-Sud Ecole doctorale n 580 Sciences et Technologies de l\u2019Information et de la Communication (STIC) Sp\u00e9cialit\u00e9 de doctorat ...", "dateLastCrawled": "2022-01-13T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "An <b>introduction to Q-Learning: Reinforcement Learning</b>", "url": "https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/an-<b>introduction-to-q-learning-reinforcement-learning</b>", "snippet": "Here is the definition of <b>Markov</b> <b>Decision</b> Processes (collected from Wikipedia): A <b>Markov decision process</b> (<b>MDP</b>) is a discrete time stochastic control <b>process</b>. It provides a mathematical framework for modeling <b>decision</b> making in situations where outcomes are partly random and partly under the control of a <b>decision</b> maker.", "dateLastCrawled": "2022-01-31T09:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Why is the <b>optimal policy</b> in <b>Markov Decision Process</b> ...", "url": "https://stats.stackexchange.com/questions/132890/why-is-the-optimal-policy-in-markov-decision-process-mdp-independent-of-the-i", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132890", "snippet": "The intuition behind the argument saying that the <b>optimal policy</b> is independent of initial state is the following: The <b>optimal policy</b> is defined by a function that selects an action for every possible state and actions in different states are independent.. Formally speaking, for an unknown initial distribution, the value function to maximize would be the following (not conditioned on initial state)", "dateLastCrawled": "2022-01-25T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Real-life <b>examples</b> of <b>Markov</b> <b>Decision</b> Processes - Cross Validated", "url": "https://stats.stackexchange.com/questions/145122/real-life-examples-of-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/145122", "snippet": "A Markovian <b>Decision</b> <b>Process</b> indeed has to do with going from one state to another and is mainly used for planning and <b>decision</b> making. The theory. Just repeating the theory quickly, an <b>MDP</b> is: $$\\text{<b>MDP</b>} = \\langle S,A,T,R,\\gamma \\rangle$$", "dateLastCrawled": "2022-01-24T10:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Overview: Representation Techniques", "url": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66fffb5d0bd4c20697922f5ffbf9a602b66bec3f74ac83fb77c/DecisionMaking.pdf", "isFamilyFriendly": true, "displayUrl": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66...", "snippet": "<b>Markov Decision Process MDP is like</b> a Markov process, except every round we make a decision Transition probabilities depend on actions taken P(St+1 = S&#39; | St = s, At = a) = P(S, a, S&#39;) Rewards for every state, action pair u(St = s, At = a) Discount factor \u03b4 Example. A <b>machine</b> can be in one of three states: good, deteriorating, broken Can take ...", "dateLastCrawled": "2022-01-21T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(markov decision process (mdp))  is like +(person making choices)", "+(markov decision process (mdp)) is similar to +(person making choices)", "+(markov decision process (mdp)) can be thought of as +(person making choices)", "+(markov decision process (mdp)) can be compared to +(person making choices)", "machine learning +(markov decision process (mdp) AND analogy)", "machine learning +(\"markov decision process (mdp) is like\")", "machine learning +(\"markov decision process (mdp) is similar\")", "machine learning +(\"just as markov decision process (mdp)\")", "machine learning +(\"markov decision process (mdp) can be thought of as\")", "machine learning +(\"markov decision process (mdp) can be compared to\")"]}