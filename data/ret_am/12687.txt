{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Black</b>-<b>box</b> adversarial attacks on XSS attack detection model - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0167404821003783", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167404821003783", "snippet": "After adding the influence of entropy to <b>Q-function</b>, Soft <b>Q-function</b> is defined by Eqs. (3) and . (3) ... Due to unknown benign samples trained by XSS attack detection models under the <b>Black</b>-<b>box</b> environment, the FUZZ dataset is constructed from the following three aspects to obtain benign examples: (1) We use web crawler technology to crawl the Alexa (Cooper, 2020) top 1000 website in depth first with a maximum depth of 4 layers. The parameters in the crawled URL are segmented, obtaining ...", "dateLastCrawled": "2022-01-24T15:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "12.9.1 <b>SARSA with Linear Function Approximation</b>\u2023 12.9 Reinforcement ...", "url": "https://artint.info/2e/html/ArtInt2e.Ch12.S9.SS1.html", "isFamilyFriendly": true, "displayUrl": "https://artint.info/2e/html/ArtInt2e.Ch12.S9.SS1.html", "snippet": "These features will be used to represent the linear <b>Q-function</b>: Q w \u00af \u2062 (s, a) = w 0 + w 1 \u2062 F 1 \u2062 (s, a) + \u2026 + w n \u2062 F n \u2062 (s, a) for some tuple of weights, w \u00af = w 0, w 1, \u2026, w n that have to be learned. Assume that there is an extra feature F 0 \u2062 (s, a) whose value is always 1, so that w 0 is not a special case. Example 12.6. Consider the grid game of Example 12.2. From understanding the domain, and not just treating it as a <b>black</b> <b>box</b>, some possible features that can be ...", "dateLastCrawled": "2022-01-10T09:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Graying the <b>black</b> <b>box</b>: Understanding DQNs", "url": "http://proceedings.mlr.press/v48/zahavy16.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v48/zahavy16.pdf", "snippet": "Graying the <b>black</b> <b>box</b>: Understanding DQNs y t= 8 &lt;: r t s t+1 is terminal r t+ max a\u2019 Q target s t+1;a 0 otherwise Notice that this is an off-line algorithm, meaning that the tuples fs t;a t;r t;s t+1; gare collected from the agents ex-perience, stored in the ER and later used for training. The reward r t is clipped to the range of [ 1;1] to ...", "dateLastCrawled": "2021-12-19T11:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement Learning with Tensorflow and Keras - O\u2019Reilly Live Events", "url": "https://www.oreilly.com/live-events/-/0636920203124/0636920203117/", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/live-events/-/0636920203124/0636920203117", "snippet": "<b>Black</b>-<b>box</b> algorithms: random search, genetic algorithms, and other heuristics; The Cross-Entropy method; Natural Evolution strategies; Temporal Difference Methods. Q-Learning and Sarsa. Eligibility traces (40 min) In this session, we will go through the different methods for estimating value functions, which are used later for estimating the optimal behavior. <b>Q-function</b>: definition and examples; Estimating Q-functions via Q-Learning; Sarsa: Off-policy methods; Beyond Q-Learning: double Q ...", "dateLastCrawled": "2022-01-22T07:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Learning User-Interpretable Descriptions of <b>Black</b>-<b>Box</b> AI System ...", "url": "https://deepai.org/publication/learning-user-interpretable-descriptions-of-black-box-ai-system-capabilities", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/learning-user-interpretable-descriptions-of-<b>black</b>-<b>box</b>...", "snippet": "This paper presents an approach for addressing these problems by learning user-interpretable symbolic descriptions of the limits and capabilities of a <b>black</b>-<b>box</b> AI system using low-level simulators. It uses a hierarchical active querying paradigm to generate questions and to learn a user-interpretable model of the AI system based on its responses. In contrast to prior work, we consider settings where imprecision of the user&#39;s conceptual vocabulary precludes a direct expression of the agent&#39;s ...", "dateLastCrawled": "2021-12-18T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Beginner&#39;s Guide to Deep <b>Reinforcement Learning</b> | Pathmind", "url": "https://wiki.pathmind.com/deep-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/deep-<b>reinforcement-learning</b>", "snippet": "It is a <b>black</b> <b>box</b> where we only see the inputs and outputs. It\u2019s <b>like</b> most people\u2019s relationship with technology: we know what it does, but we don\u2019t know how it works. <b>Reinforcement learning</b> represents an agent\u2019s attempt to approximate the environment\u2019s function, such that we can send actions into the <b>black</b>-<b>box</b> environment that maximize the rewards it spits out. *Credit: Sutton &amp; Barto. In the feedback loop above, the subscripts denote the time steps t and t+1, each of which refer ...", "dateLastCrawled": "2022-02-03T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Deep Reinforcement Learning for Black-Box</b> Testing of Android Apps", "url": "https://www.researchgate.net/publication/348320643_Deep_Reinforcement_Learning_for_Black-Box_Testing_of_Android_Apps", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348320643_Deep_Reinforcement_Learning_for...", "snippet": "To apply RL, we hav e to map the problem of Android <b>black</b>-<b>box</b> testing to the standard mathematical formalization of RL: an MDP, de ned by the 5-tuple, \ud835\udc46, \ud835\udc34, \ud835\udc45, \ud835\udc43 , \ud835\udf0c 0", "dateLastCrawled": "2022-01-02T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Q function</b> financial definition of <b>Q function</b>", "url": "https://financial-dictionary.thefreedictionary.com/Q+function", "isFamilyFriendly": true, "displayUrl": "https://financial-dictionary.thefreedictionary.com/<b>Q+function</b>", "snippet": "Normal Distribution The well known bell shaped curve. According to the Central Limit Theorem, the probability density function of a large number of independent, identically distributed random numbers will approach the normal distribution. In the fractal family of distributions, the normal distribution only exists when alpha equals 2, or the Hurst exponent equals 0.50. Thus, the normal distribution is a special case which in time series analysis is quite rare. See: Alpha, Central Limit ...", "dateLastCrawled": "2021-12-06T01:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Learning with Feature-based Representations We would <b>like</b> to use a Q ...", "url": "https://www.coursehero.com/file/8699604/Section-5-Solutions/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/8699604/Section-5-Solutions", "snippet": "7. In some sense, we can think about this entire process, on a meta level, as an input we control that produces an output that we would <b>like</b> to maximize. If you have a magical function (F (input)) that maps an input to an output you would <b>like</b> to maximize, what techniques (from math, CS, etc) can we use to search for the best inputs?Keep in mind that the magical function is a <b>black</b> <b>box</b>.", "dateLastCrawled": "2021-12-16T07:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Amazon.in: Buy Lenovo IdeaPad Slim 5 11th Gen Intel Core i5 15.6&quot;(39 ...", "url": "https://www.amazon.in/Lenovo-Keyboard-Fingerprint-Graphite-82FG01H9IN/dp/B09M423NVT", "isFamilyFriendly": true, "displayUrl": "https://www.amazon.in/Lenovo-Keyboard-Fingerprint-Graphite-82FG01H9IN/dp/B09M423NVT", "snippet": "Buy Lenovo IdeaPad Slim 5 11th Gen Intel Core i5 15.6&quot;(39.62cm) FHD IPS Thin &amp; Light Laptop (16GB/512GB SSD/Windows 11/MS Office 2021/Backlit Keyboard/Fingerprint Reader/Graphite Grey/1.66Kg), 82FG01H9IN online at low price in India on Amazon.in. Check out Lenovo IdeaPad Slim 5 11th Gen Intel Core i5 15.6&quot;(39.62cm) FHD IPS Thin &amp; Light Laptop (16GB/512GB SSD/Windows 11/MS Office 2021/Backlit Keyboard/Fingerprint Reader/Graphite Grey/1.66Kg), 82FG01H9IN reviews, ratings, features ...", "dateLastCrawled": "2022-02-03T01:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "L09topdown - What Is <b>q function</b> s predefined programmer-defined q q q q ...", "url": "https://www.coursehero.com/file/5869538/L09topdown/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/5869538/L09topdown", "snippet": "function predefined, programmer-defined arguments, (formal) parameters return value function call, function invocation function definition head, body function prototype (declaration) expanded form, abbreviated form local variables, global variables, scope call-by-value <b>black</b> <b>box</b> principle, information hiding, procedural abstraction What Is?", "dateLastCrawled": "2021-10-17T22:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Black</b>-<b>box</b> adversarial attacks on XSS attack detection model - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0167404821003783", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167404821003783", "snippet": "After adding the influence of entropy to <b>Q-function</b>, Soft <b>Q-function</b> is defined by Eqs. (3) and . (3) ... Due to unknown benign samples trained by XSS attack detection models under the <b>Black</b>-<b>box</b> environment, the FUZZ dataset is constructed from the following three aspects to obtain benign examples: (1) We use web crawler technology to crawl the Alexa (Cooper, 2020) top 1000 website in depth first with a maximum depth of 4 layers. The parameters in the crawled URL are segmented, obtaining ...", "dateLastCrawled": "2022-01-24T15:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "\uae08 An Introduction to Interpretable Machine Learning", "url": "https://leesael.github.io/talk/KSC2018/KSC2018Winter_Tutorial_SaelLee_1.pdf", "isFamilyFriendly": true, "displayUrl": "https://leesael.github.io/talk/KSC2018/KSC2018Winter_Tutorial_SaelLee_1.pdf", "snippet": "<b>Black</b> <b>Box</b> Models Explainable. (Interpretable Machine Learning A Guide for Making <b>Black</b> <b>Box</b> Models Explainable ) q Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. \u201cModel-Agnostic Interpretability of Machine Learning.\u201d ICML Workshop on Human Interpretability in Machine Learning, no q Miller, Tim. 2017. \u201cExplanation in ...", "dateLastCrawled": "2022-01-30T13:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Attacking Reinforcement Learning: A Survey</b> on Methods", "url": "http://www.lauragraves.ca/media/deeprl.pdf", "isFamilyFriendly": true, "displayUrl": "www.lauragraves.ca/media/deeprl.pdf", "snippet": "<b>similar</b> inputs) means that <b>black</b>-<b>box</b> attacks where the attacker is given only enough access to input examples and get the resulting prediction vectors are e ective. These are done via training a substitute model and creating adversarial examples for that model, and those examples are likely to be adversarial for the <b>black</b>-<b>box</b> model [16]. In some cases these attacks are e ective against models that return only truncated prediction vectors or even models that return only the maximum ...", "dateLastCrawled": "2021-11-14T01:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Graying the <b>black</b> <b>box</b>: Understanding DQNs", "url": "http://proceedings.mlr.press/v48/zahavy16.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v48/zahavy16.pdf", "snippet": "Graying the <b>black</b> <b>box</b>: Understanding DQNs optimal policy. Another cause for control discontinuities is that for a given problem, two states with <b>similar</b> represen-tations may in fact be far from each other in terms of the number of state transitions required to reach one from the other. This observation can also explain the lack of pooling", "dateLastCrawled": "2021-12-19T11:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Deep Reinforcement Learning for Black-Box</b> Testing of Android Apps", "url": "https://www.researchgate.net/publication/348320643_Deep_Reinforcement_Learning_for_Black-Box_Testing_of_Android_Apps", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348320643_Deep_Reinforcement_Learning_for...", "snippet": "To apply RL, we hav e to map the problem of Android <b>black</b>-<b>box</b> testing to the standard mathematical formalization of RL: an MDP, de ned by the 5-tuple, \ud835\udc46, \ud835\udc34, \ud835\udc45, \ud835\udc43 , \ud835\udf0c 0", "dateLastCrawled": "2022-01-02T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep Reinforcement Learning for <b>Black</b>-<b>Box</b> Testing of Android Apps - arXiv", "url": "https://www.readkong.com/page/deep-reinforcement-learning-for-black-box-testing-of-2875203", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/deep-reinforcement-learning-for-<b>black</b>-<b>box</b>-testing-of-2875203", "snippet": "Page topic: &quot;Deep Reinforcement Learning for <b>Black</b>-<b>Box</b> Testing of Android Apps - arXiv&quot;. Created by: Mike Henderson. Language: english.", "dateLastCrawled": "2022-01-04T13:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Amazon.in: Buy Lenovo IdeaPad Slim 5 11th Gen Intel Core i5 15.6&quot; (39 ...", "url": "https://www.amazon.in/Lenovo-Keyboard-Fingerprint-Graphite-82FG013WIN/dp/B09FHT8219", "isFamilyFriendly": true, "displayUrl": "https://www.amazon.in/Lenovo-Keyboard-Fingerprint-Graphite-82FG013WIN/dp/B09FHT8219", "snippet": "Buy Lenovo IdeaPad Slim 5 11th Gen Intel Core i5 15.6&quot; (39.62cm) FHD IPS Thin &amp; Light Laptop (16GB/512GB SSD/Windows 10/Office/300Nits/Backlit Keyboard/Fingerprint Reader/Graphite Grey/1.66Kg), 82FG013WIN online at low price in India on Amazon.in. Check out Lenovo IdeaPad Slim 5 11th Gen Intel Core i5 15.6&quot; (39.62cm) FHD IPS Thin &amp; Light Laptop (16GB/512GB SSD/Windows 10/Office/300Nits/Backlit Keyboard/Fingerprint Reader/Graphite Grey/1.66Kg), 82FG013WIN reviews, ratings, features ...", "dateLastCrawled": "2022-01-29T18:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Deep Q-Learning with Keras and Gym</b> \u00b7 Keon&#39;s Blog", "url": "https://keon.github.io/deep-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://keon.github.io/deep-q-learning", "snippet": "<b>Deep Q-Learning with Keras and Gym</b>. Feb 6, 2017. This blog post will demonstrate how deep reinforcement learning (deep Q-learning) can be implemented and applied to play a CartPole game using Keras and Gym, in less than 100 lines of code! I\u2019ll explain everything without requiring any prerequisite knowledge about reinforcement learning.", "dateLastCrawled": "2022-02-01T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Amazon.in: Buy Lenovo IdeaPad Slim 5 11th Gen Intel Core i5 15.6&quot;(39 ...", "url": "https://www.amazon.in/Lenovo-Keyboard-Fingerprint-Graphite-82FG01H9IN/dp/B09M423NVT", "isFamilyFriendly": true, "displayUrl": "https://www.amazon.in/Lenovo-Keyboard-Fingerprint-Graphite-82FG01H9IN/dp/B09M423NVT", "snippet": "Buy Lenovo IdeaPad Slim 5 11th Gen Intel Core i5 15.6&quot;(39.62cm) FHD IPS Thin &amp; Light Laptop (16GB/512GB SSD/Windows 11/MS Office 2021/Backlit Keyboard/Fingerprint Reader/Graphite Grey/1.66Kg), 82FG01H9IN online at low price in India on Amazon.in. Check out Lenovo IdeaPad Slim 5 11th Gen Intel Core i5 15.6&quot;(39.62cm) FHD IPS Thin &amp; Light Laptop (16GB/512GB SSD/Windows 11/MS Office 2021/Backlit Keyboard/Fingerprint Reader/Graphite Grey/1.66Kg), 82FG01H9IN reviews, ratings, features ...", "dateLastCrawled": "2022-02-03T01:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Q&amp;A: What is the <b>Golgi apparatus</b>, and why are we asking? | BMC Biology ...", "url": "https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-9-63", "isFamilyFriendly": true, "displayUrl": "https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-9-63", "snippet": "From the perspective of the cell, the Golgi <b>can</b> <b>be thought</b> of as a <b>black</b> <b>box</b> with material entering from ER or endosomes, and then leaving with various consequences. (a) Sorting. Newly made secreted and membrane proteins arrive at the cis-Golgi from the ER in COPII-coated vesicles and are sorted from the trans-Golgi to the other organelles of the cell. COPI vesicles retrieve escaped ER residents, and are widely, but not universally, <b>thought</b> to also recycle Golgi enzymes from later to earlier ...", "dateLastCrawled": "2022-01-29T05:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Solving a Rubik\u2019s Cube with <b>Reinforcement Learning</b> (Part 1) | by ...", "url": "https://medium.com/analytics-vidhya/solving-a-rubiks-cube-with-reinforcement-learning-part-1-4f0405dd07f2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/solving-a-<b>rubiks-cube</b>-with-<b>reinforcement-learning</b>...", "snippet": "<b>Q-Function</b>. The <b>Q-function</b> in it\u2019s simplest form <b>can</b> <b>be thought</b> of as a giant look-up table with dimensions |S| x |A|. However, in the Rubik\u2019s Cube example where this would result in a table ...", "dateLastCrawled": "2022-02-02T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) Runtime Safety Assurance Using Reinforcement Learning", "url": "https://www.researchgate.net/publication/344802613_Runtime_Safety_Assurance_Using_Reinforcement_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/344802613_Runtime_Safety_Assurance_Using...", "snippet": "<b>thought</b> <b>of as black</b> boxes, which means that for the purposes of the RTSA system, the dynamics of the system are com- pletely speci\ufb01ed by the interaction of the controller with the", "dateLastCrawled": "2022-01-28T07:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Review for NeurIPS paper: What Did You Think Would Happen? Explaining ...", "url": "https://proceedings.neurips.cc/paper/2020/file/d5ab8dc7ef67ca92e41d730982c5c602-Review.html", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2020/file/d5ab8dc7ef67ca92e41d730982c5c602-Review...", "snippet": "If authors mean to say that their method <b>can</b> <b>be thought</b> of as an *instance* of this class, I&#39;d suggest they clarify this. ... They propose a deocomposition of the <b>Q-function</b> over state and action space, giving detailed reasons of sub-rewards over future states. This is an addition to standard RL frameworks. Strengths: This paper attempts to address the issue of <b>black</b>-<b>box</b> models, but enabling explainable RL. While current explanations show what in the environment drives agents to take action ...", "dateLastCrawled": "2021-11-06T13:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Beginner&#39;s <b>Guide to Deep Reinforcement Learning</b> - Rasin Tsukuba Blog", "url": "https://rasin-tsukuba.github.io/2020/08/28/Beginner's-Guide-to-Deep-Reinforcement-Learning/", "isFamilyFriendly": true, "displayUrl": "https://rasin-tsukuba.github.io/2020/08/28/Beginner&#39;s-<b>Guide-to-Deep-Reinforcement-Learning</b>", "snippet": "It is a <b>black</b> <b>box</b> where we only see the inputs and outputs. Reinforcement learning represents an agent\u2019s attempt to approximate the environment\u2019s function, such that we <b>can</b> send actions into the <b>black</b>-<b>box</b> environment that maximize the rewards it spits out. In the feedback loop above, the subscripts denote the time steps \\(t\\) and \\(t+1\\), each of which refer to different states: the state at moment \\(t\\), and the state at moment \\(t+1\\). Reinforcement learning judges actions by the ...", "dateLastCrawled": "2021-12-22T15:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Beginner&#39;s Guide to Deep <b>Reinforcement Learning</b> | Pathmind", "url": "https://wiki.pathmind.com/deep-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://wiki.pathmind.com/deep-<b>reinforcement-learning</b>", "snippet": "It is a <b>black</b> <b>box</b> where we only see the inputs and outputs. It\u2019s like most people\u2019s relationship with technology: we know what it does, but we don\u2019t know how it works. <b>Reinforcement learning</b> represents an agent\u2019s attempt to approximate the environment\u2019s function, such that we <b>can</b> send actions into the <b>black</b>-<b>box</b> environment that maximize the rewards it spits out.", "dateLastCrawled": "2022-02-03T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>GitHub</b> - <b>mickvanhulst/q_learning</b>", "url": "https://github.com/mickvanhulst/q_learning", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>mickvanhulst/q_learning</b>", "snippet": "Simple Reinforcement learning example, based on the <b>Q-function</b>. Rules: The agent (yellow <b>box</b>) has to reach one of the goals to end the game (green or red cell). Rewards: Each step gives a negative reward of -0.04. The red cell gives a negative reward of -5. The green one gives a positive reward of +5. The <b>black</b> walls give a negative reward of -1.", "dateLastCrawled": "2022-01-28T16:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Probing a quantum field in a photon</b> <b>box</b>", "url": "https://www.researchgate.net/publication/231156874_Probing_a_quantum_field_in_a_photon_box", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/231156874_<b>Probing_a_quantum_field_in_a_photon</b>_<b>box</b>", "snippet": "\ufb01gure 5 the <b>Q function</b> for a 0.5 photon thermal \ufb01eld, displaced by the real amplitude \u03b2 = \u221a 51, in units of 1 /\u03c0 . This displacement, D 0 , is performed by S at the beginning of the ...", "dateLastCrawled": "2022-01-18T03:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "A Thinking Person&#39;s Guide to Programmable Logic", "url": "https://www.physics.umd.edu/hep/drew/programmable/", "isFamilyFriendly": true, "displayUrl": "https://www.physics.umd.edu/hep/drew/programmable", "snippet": "His book &quot;The Laws of <b>Thought</b>&quot; (1854) lays out the algebra of <b>thought</b>, or reasoning. Before getting into the details of Boolean algebra, we <b>can</b> first consider a more general visual description of sets and set theory, and how elements and sets are related. To begin, consider the following depiction of the &quot;Universe&quot;:", "dateLastCrawled": "2021-12-29T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Brand new out of the <b>box</b> :( Tell me it\u2019s easy fix or something ...", "url": "https://www.reddit.com/r/LenovoLegion/comments/r6ffng/brand_new_out_of_the_box_tell_me_its_easy_fix_or/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/LenovoLegion/comments/r6ffng/brand_new_out_of_the_<b>box</b>_tell_me...", "snippet": "If the FN+<b>Q function</b> works after this, and you <b>can</b> see it changing your power plans, then that is great. However.. this was not the case for me. Alas, steps 4 &amp; 5 will fix this. Download .NET Desktop Runtime 5 which is mandatory to allow Step 5..NET 5.0 Runtime (pick the Desktop version)", "dateLastCrawled": "2022-01-30T01:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Black</b>-<b>box</b> adversarial attacks on XSS attack detection model - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0167404821003783", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167404821003783", "snippet": "After adding the influence of entropy to <b>Q-function</b>, Soft <b>Q-function</b> is defined by Eqs. (3) and . (3) ... <b>Black</b>-<b>box</b> XSS detection environment. <b>Compared</b> with the White-<b>box</b> environment, we <b>can</b> only know the confidence coefficient of sample in the <b>Black</b>-<b>box</b> environment. Therefore, the XSS detection environment will only output state and reward for the input. State represents the structure information of the current payload and the reward is defined in Eq. (20) as follows: (20) R e w a r d = (r ...", "dateLastCrawled": "2022-01-24T15:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Provably Ef\ufb01cient <b>Black</b>-<b>Box</b> Action Poisoning Attacks Against ...", "url": "https://faculty.engineering.ucdavis.edu/lai/wp-content/uploads/sites/38/2021/10/RL_action_attack_final.pdf", "isFamilyFriendly": true, "displayUrl": "https://faculty.engineering.ucdavis.edu/lai/wp-content/uploads/sites/38/2021/10/RL...", "snippet": "<b>Compared</b> with existing attack models, the attacker\u2019s ability in the proposed action poisoning attack model is more restricted, and hence the attack model is more practical. We study the action poisoning attack in both white-<b>box</b> and <b>black</b>-<b>box</b> settings. We introduce an adaptive attack scheme called LCB-H, which works for most RL agents in the <b>black</b>-<b>box</b> setting. We prove that the LCB-H attack <b>can</b> force any ef\ufb01cient RL agent, whose dynamic regret scales sublinearly with the total number of ...", "dateLastCrawled": "2021-10-03T02:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Provably Ef\ufb01cient <b>Black</b>-<b>Box</b> Action Poisoning Attacks Against ...", "url": "https://proceedings.neurips.cc/paper/2021/file/678004486c119599ed7d199f47da043a-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2021/file/678004486c119599ed7d199f47da043a-Paper.pdf", "snippet": "<b>Compared</b> with existing attack models, the attacker\u2019s ability in the proposed action poisoning attack model is more restricted, which brings some design challenges. We study the action poisoning attack in both white-<b>box</b> and <b>black</b>-<b>box</b> settings. We introduce an adaptive attack scheme called LCB-H, which works for most RL agents in the <b>black</b>-<b>box</b> setting. We prove that the LCB-H attack <b>can</b> force any ef\ufb01cient RL agent, whose dynamic regret scales sublinearly with the total number of steps ...", "dateLastCrawled": "2022-01-29T19:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Deep Reinforcement Learning for <b>Black</b>-<b>Box</b> Testing of Android Apps", "url": "https://www.researchgate.net/publication/358256427_Deep_Reinforcement_Learning_for_Black-Box_Testing_of_Android_Apps", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/358256427_Deep_Reinforcement_Learning_for...", "snippet": "T o apply RL, we have to map the problem of Android <b>black</b>-<b>box</b> testing to the standard mathematical formalization of RL: an MDP, de\ufb01ned b y the 5-tuple, S, A, R, P, \u03c1 0 . Moreover, we have to ...", "dateLastCrawled": "2022-02-03T07:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Provably Efficient <b>Black</b>-<b>Box</b> Action Poisoning Attacks Against ...", "url": "https://deepai.org/publication/provably-efficient-black-box-action-poisoning-attacks-against-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/provably-efficient-<b>black</b>-<b>box</b>-action-poisoning-attacks...", "snippet": "<b>Compared</b> with existing attack models, the attacker&#39;s ability in the proposed action poisoning attack model is more restricted, which brings some design challenges. We study the action poisoning attack in both white-<b>box</b> and <b>black</b>-<b>box</b> settings. We introduce an adaptive attack scheme called LCB-H, which works for most RL agents in the <b>black</b>-<b>box</b> setting. We prove that the LCB-H attack <b>can</b> force any efficient RL agent, whose dynamic regret scales sublinearly with the total number of steps taken ...", "dateLastCrawled": "2022-01-29T22:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Graying the <b>black</b> <b>box</b>: Understanding DQNs", "url": "http://proceedings.mlr.press/v48/zahavy16.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v48/zahavy16.pdf", "snippet": "Graying the <b>black</b> <b>box</b>: Understanding DQNs Tom Zahavy* TOMZAHAVY@CAMPUS.TECHNION.AC.IL Nir Ben Zrihem* NIRB@TX.TECHNION.AC.IL Shie Mannor SHIE@EE.TECHNION.AC.IL Electrical Engineering Department, The Technion - Israel Institute of Technology, Haifa 32000, Israel Abstract In recent years there is a growing interest in us-ing deep representations for reinforcement learn-ing. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Using our ...", "dateLastCrawled": "2021-12-19T11:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Deep Reinforcement Learning for <b>Black</b>-<b>Box</b> Testing of Android Apps - arXiv", "url": "https://www.readkong.com/page/deep-reinforcement-learning-for-black-box-testing-of-2875203", "isFamilyFriendly": true, "displayUrl": "https://www.readkong.com/page/deep-reinforcement-learning-for-<b>black</b>-<b>box</b>-testing-of-2875203", "snippet": "Page topic: &quot;Deep Reinforcement Learning for <b>Black</b>-<b>Box</b> Testing of Android Apps - arXiv&quot;. Created by: Mike Henderson. Language: english.", "dateLastCrawled": "2022-01-04T13:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reinforcement Mastering the game of Go from scratch", "url": "https://web.stanford.edu/class/cs234/CS234Win2020/past_projects/2017/2017_Painter_Johnston_Mastering_Go_Poster.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs234/CS234Win2020/past_projects/2017/2017_Painter...", "snippet": "<b>Black</b> <b>box</b>: A network trained to play on a 5x5 board is used as a <b>black</b> <b>box</b> (weights frozen), The <b>black</b> <b>box</b> is used like an oracle to query about \u2018global\u2019 information and \u2018local\u2019 information. Transfer method 1 - convolution, or \u2018local\u2019 use: The 5x5 network is applied to every possible window of the 9x9 board (as if the 5x5 network were a convolutional filter). Each application yields a 5x5 output of \u2018policy values\u2019 for actions at each board location: Which is padded with zeros ...", "dateLastCrawled": "2021-09-15T21:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Model-Augmented Actor-Critic: Backpropagating through Paths", "url": "http://www.robot-learning.ml/2019/files/papers/Improving%20Model-Based%20Reinforcement%20Learning%20via%20Model-Augmented%20Pathwise%20Derivative.pdf", "isFamilyFriendly": true, "displayUrl": "www.robot-learning.ml/2019/files/papers/Improving Model-Based Reinforcement Learning...", "snippet": "model as a <b>black</b>-<b>box</b> simulator generating samples from it. In high-dimensional environments or environments that require longer planning, substantial sampling is needed to provide meaningful signal for the policy. <b>Can</b> we further exploit our learned models? In this work, we propose to estimate the policy gradient by backpropagating its gradient through the model using the pathwise derivative estimator. Since the learned model is differentiable, one <b>can</b> link together the model, reward function ...", "dateLastCrawled": "2021-12-23T10:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Cracking the Black Box: Distilling Deep Sports Analytics</b> | DeepAI", "url": "https://deepai.org/publication/cracking-the-black-box-distilling-deep-sports-analytics", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>cracking-the-black-box-distilling-deep-sports-analytics</b>", "snippet": "We believe that converting a <b>black</b>-<b>box</b> model to a white-<b>box</b> model tree has two key advantages for sports analytics. (1) The model tree provides a comprehensive analysis of relevant interactions among domain variables. Interactions are represented in an intuitive visual tree format, so that even complex combinations of features remain comprehensible. (2) The mimic model <b>can</b> guide the user towards especially interesting and useful phenomena gleaned from the data. We illustrate this technique ...", "dateLastCrawled": "2021-12-07T06:06:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An <b>introduction to Q-Learning: Reinforcement Learning</b>", "url": "https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/", "isFamilyFriendly": true, "displayUrl": "https://blog.floydhub.com/an-<b>introduction-to-q-learning-reinforcement-learning</b>", "snippet": "In this article, we are going to step into the world of reinforcement <b>learning</b>, another beautiful branch of artificial intelligence, which lets machines learn on their own in a way different from traditional <b>machine</b> <b>learning</b>. Particularly, we will be covering the simplest reinforcement <b>learning</b> algorithm i.e. the Q-<b>Learning</b> algorithm in great detail.", "dateLastCrawled": "2022-01-31T09:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "<b>Q-function</b>: input the state-atcion pair, output the Q-value. The letter \u201cQ\u201d is used to represent the quality of taking a given action in a given state. Q-<b>learning</b>. It is used for <b>learning</b> the optimal policy by <b>learning</b> the optimal Q-values for each state-action pair in a Markov Decision Process", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Relationship between state (V) and action(Q) value function in ...", "url": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and-action-q-value-function-in-reinforcement-learning-bb9a988c0127", "isFamilyFriendly": true, "displayUrl": "https://medium.com/intro-to-artificial-intelligence/relationship-between-state-v-and...", "snippet": "Value function can be defined as the expected value of an agent in a certain state. There are two types of value functions in RL: State-value and action-value. It is important to understand the\u2026", "dateLastCrawled": "2022-02-03T03:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Q-Learning</b> - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/engineering/q-learning", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/topics/engineering/<b>q-learning</b>", "snippet": "Majed Alsadhan, in <b>Machine</b> <b>Learning</b>, Big Data, and IoT for Medical Informatics, 2021. 3.2 Reinforcement <b>learning</b> 3.2.1 Traditional. <b>Q-learning</b> (Watkins and Dayan, 1992) is a simple RL algorithm that given the current state, seeks to find the best action to take in that state. It is an off-policy algorithm because it learns from actions that are random (i.e., outside the policy). The algorithm works in three basic steps: (1) the agent starts in a state and takes an action and receives a ...", "dateLastCrawled": "2022-01-24T01:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Reinforcement Q-<b>Learning</b> from Scratch in Python with OpenAI Gym ...", "url": "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/", "isFamilyFriendly": true, "displayUrl": "https://www.learndatasci.com/tutorials/reinforcement-q-<b>learning</b>-scratch-python-openai-gym", "snippet": "Q-<b>learning</b> is one of the easiest Reinforcement <b>Learning</b> algorithms. The problem with Q-<b>learning</b> however is, once the number of states in the environment are very high, it becomes difficult to implement them with Q table as the size would become very, very large. State of the art techniques uses Deep neural networks instead of the Q-table (Deep Reinforcement <b>Learning</b>). The neural network takes in state information and actions to the input layer and learns to output the right action over the ...", "dateLastCrawled": "2022-02-03T03:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Learning rate of a Q learning agent</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/33011825/learning-rate-of-a-q-learning-agent", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/33011825", "snippet": "If the <b>learning</b> rate is constant, will <b>Q function</b> converge to the optimal on or <b>learning</b> rate should necessarily decay to guarantee convergence? <b>machine</b>-<b>learning</b> reinforcement-<b>learning</b> q-<b>learning</b>. Share. Follow asked Oct 8 &#39;15 at 9:31. uduck uduck. 119 1 1 silver badge 8 8 bronze badges. 2. 4. With a sufficiently small <b>learning</b> rate you have a convergence guarantee for a convex q <b>learning</b> problem. \u2013 Thomas Jungblut. Oct 8 &#39;15 at 15:27. I assume there is also a dependence on the nature of ...", "dateLastCrawled": "2022-01-24T06:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Why doesn&#39;t <b>Q-learning</b> converge when using function approximation ...", "url": "https://ai.stackexchange.com/questions/11679/why-doesnt-q-learning-converge-when-using-function-approximation", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/11679/why-doesnt-<b>q-learning</b>-converge-when-using...", "snippet": "$\\begingroup$ @nbro The proof doesn&#39;t say that explicitly, but it assumes an exact representation of the <b>Q-function</b> (that is, that exact values are computed and stored for every state/action pair). For infinite state spaces, it&#39;s clear that this exact representation can be infinitely large in the worst case (simple example: let Q(s,a) = sth digit of pi).", "dateLastCrawled": "2022-01-28T08:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "On using Huber loss in (Deep) Q-<b>learning</b> | \u30e4\u30ed\u30df\u30eb", "url": "https://jaromiru.wordpress.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://jaromiru.wordpress.com/2017/05/27/on-using-huber-loss-in-deep-q-<b>learning</b>", "snippet": "<b>MACHINE</b> <b>LEARNING</b> &amp; AI. Menu. Let\u2019s make a DQN. Theory; Implementation; Debugging; Full DQN; Double <b>Learning</b> and Prioritized Experience Replay; Let\u2019s make an A3C. Theory ; Implementation; About me; On using Huber loss in (Deep) Q-<b>learning</b>. Posted on May 27, 2017 May 30, 2017 by \u30e4\u30ed\u30df\u30eb. I\u2019ve been recently working on a problem where I put a plain DQN to use. The problem is very simple, deterministic, partially observable and states are quite low-dimensional. The agent however can ...", "dateLastCrawled": "2021-12-26T09:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Reinforcement Learning</b> as Heuristic Search <b>Analogy</b> - DataDrivenInvestor", "url": "https://medium.datadriveninvestor.com/reinforcement-learning-as-heuristic-search-analogy-31d92b06dadd", "isFamilyFriendly": true, "displayUrl": "https://medium.datadriveninvestor.com/<b>reinforcement-learning</b>-as-heuristic-search...", "snippet": "I will not go over all the RL Algorithms, only a subset of those that fit my <b>analogy</b> well, nor will I be giving example code. This post is a purely theoretical outlook and assumes that you can translate the pseudo-code to actual code later. This post will work best if you have some knowledge of basic RL algorithms (TD <b>Learning</b>, Dynamic Programming etc), though I will attempt to go from scratch. Those that have prior knowledge of <b>Reinforcement Learning</b> will benefit the most from this post. On ...", "dateLastCrawled": "2022-01-21T02:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Applications of <b>Reinforcement Learning</b> in Real World | by garychl ...", "url": "https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/applications-of-<b>reinforcement-learning</b>-in-real-world-1a...", "snippet": "<b>Reinforcement Learning</b> is a very general framework for <b>learning</b> sequential decision making tasks. And Deep <b>Learning</b>, on the other hand, is of course the best set of algorithms we have to learn representations. And combinations of these two different models is the best answer so far we have in terms of <b>learning</b> very good state representations of very challenging tasks that are not just for solving toy domains but actually to solve challenging real world problems.\u201d", "dateLastCrawled": "2022-02-02T20:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Using <b>Keras and Deep Q-Network to Play FlappyBird</b> | Ben Lau", "url": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "isFamilyFriendly": true, "displayUrl": "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html", "snippet": "You just follow the guidiance from the strategy book. Here, <b>Q-function is similar</b> to a strategy guide. Suppose you are in state s and you need to decide whether you take action a or b. If you have this magical Q-function, the answers become really simple \u2013 pick the action with highest Q-value! Here, represents the policy, which you will often see in the ML literature. How do we get the Q-function? That\u2019s where Q-<b>learning</b> is coming from. Let me quickly derive here: Define total future ...", "dateLastCrawled": "2022-01-30T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Learn to Make Decision <b>with Small Data for Autonomous Driving: Deep</b> ...", "url": "https://www.hindawi.com/journals/jat/2020/8495264/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/jat/2020/8495264", "snippet": "GP is a Bayesian nonparametric <b>machine</b> <b>learning</b> framework for regression, classification, and unsupervised <b>learning</b> . A GP ... In addition, the <b>learning</b> method of <b>Q function is similar</b> to that in DQN as well. In our case, we train a deep neural network by DDPG to achieve successful loop trip. It takes about 16 hours and 4000 episodes to achieve a high performance deep neural network. And tens of thousands of data will be updated in the centralized experience replay buffer during training ...", "dateLastCrawled": "2022-01-22T01:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficient Navigation of Colloidal Robots in an Unknown Environment via ...", "url": "https://onlinelibrary.wiley.com/doi/10.1002/aisy.201900106", "isFamilyFriendly": true, "displayUrl": "https://onlinelibrary.wiley.com/doi/10.1002/aisy.201900106", "snippet": "In free space navigation (Figure 2a), the navigation strategy derived from the learned optimal <b>Q* function is similar</b> to previous studies 18, 43, 44 and can be summarized approximately as \u03c0 * (s) = {v max, d n \u2208 [d c, \u221e) v max, d n \u2208 [0, d c), \u03b1 n \u2208 [\u2212 \u03b1 c, \u03b1 c] 0, otherwise (3) where d n is the projection of the target-particle vector onto the orientation vector n = (cos\u03b8, sin\u03b8), \u03b1 n is the angle between target-particle distance vector and n, and parameters d c and \u03b1 c are ...", "dateLastCrawled": "2022-01-20T08:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Adapting Soft Actor Critic for Discrete Action Spaces | by Felix ...", "url": "https://towardsdatascience.com/adapting-soft-actor-critic-for-discrete-action-spaces-a20614d4a50a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/adapting-soft-actor-critic-for-discrete-action-spaces-a...", "snippet": "This should accelerate <b>learning</b> in the later stages of training and help with avoiding local optima. Just as before we want to find \u03b8 that optimizes the expected return. To do so in the entropy regularized setting we can simply add an estimate of the entropy to our estimate of the expected return: Entropy Regularized Actor Cost Function. Figure 7: Entropy regularized critic cost functions. How we adapt the Bellman equation for our <b>Q-function is similar</b> to what we have seen in the definition ...", "dateLastCrawled": "2022-02-03T12:06:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep Reinforcement <b>Learning</b> for Agriculture: Principles and Use Cases ...", "url": "https://link.springer.com/chapter/10.1007/978-981-16-5847-1_4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-981-16-5847-1_4", "snippet": "In other words, the Q-function captures the expected total future rewards agent i can receive in state s t by taking action a t. <b>Q-function can be thought of as</b> a table look up, where rows of the table are states s and columns represent actions a.Ultimately, the <b>learning</b> agent i needs to find the best action given current state s.This is called a policy \u03c0(s).Policy captures the <b>learning</b> agent&#39;s behavior at any given time.", "dateLastCrawled": "2022-01-27T09:13:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(q-function)  is like +(black box)", "+(q-function) is similar to +(black box)", "+(q-function) can be thought of as +(black box)", "+(q-function) can be compared to +(black box)", "machine learning +(q-function AND analogy)", "machine learning +(\"q-function is like\")", "machine learning +(\"q-function is similar\")", "machine learning +(\"just as q-function\")", "machine learning +(\"q-function can be thought of as\")", "machine learning +(\"q-function can be compared to\")"]}