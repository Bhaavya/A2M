{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching algorithm that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise <b>machine</b> translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching algorithm that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Choosing the <b>Right Metric for Evaluating Machine Learning</b> Models \u2013 Part ...", "url": "https://www.kdnuggets.com/2018/04/right-metric-evaluating-machine-learning-models-1.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2018/04/right-metric-evaluating-<b>machine</b>-<b>learning</b>-models-1.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) It is mostly used to measure the quality of <b>machine</b> translation with respect to the human translation. It uses a modified form of precision metric. Steps to compute <b>BLEU</b> score: 1. Convert the sentence into unigrams, bigrams, trigrams, and 4-grams 2. Compute precision for n-grams of size 1 to 4 3. Take the ...", "dateLastCrawled": "2022-01-25T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>BLEU</b>: a Method for <b>Automatic Evaluation of Machine Translation</b>", "url": "https://www.researchgate.net/publication/2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2588204_", "snippet": "We describe <b>two</b> metrics for <b>automatic evaluation of machine trans-lation</b> quality. These metrics, <b>BLEU</b> and NEE, are compared to hu-man judgment of quality of translation of Arabic, Chinese, French ...", "dateLastCrawled": "2022-02-01T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language <b>Evaluation</b> | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) #language. A score between 0.0 and 1.0, inclusive, indicating the quality of a translation between <b>two</b> human <b>languages</b> (for example, between English and Russian). A <b>BLEU</b> score of 1.0 indicates a perfect translation; a <b>BLEU</b> score of 0.0 indicates a terrible translation. C. causal language model. #language. Synonym for unidirectional language model. See bidirectional language model to contrast <b>different</b> directional approaches in language modeling. crash ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> Translation and the Google Translate Algorithm ...", "url": "https://www.datasciencecentral.com/machine-learning-translation-and-the-google-translate-algorithm/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>data</b>sciencecentral.com/<b>machine</b>-<b>learning</b>-translation-and-the-google...", "snippet": "There are a lot of approaches that partly solve this problem, but the most popular and effective metric is <b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>). Imagine, we <b>have</b> <b>two</b> candidates from <b>machine</b> translators: Candidate 1: Statsbot makes it easy for companies to closely monitor <b>data</b> from various analytical platforms via natural language.", "dateLastCrawled": "2022-01-29T00:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Presenting artificial intelligence, deep <b>learning</b>, and <b>machine</b> <b>learning</b> ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8519529/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8519529", "snippet": "Text <b>data</b>: <b>Bilingual</b> <b>evaluation</b> <b>understudy</b>: Compares generated text with (<b>BLEU</b>) reference texts: Recall-oriented <b>Understudy</b> for: Compares generated text with : Gisting <b>Evaluation</b> (ROUGE) reference texts: Multiple measurements: Frequency weighted average: Summarizes many <b>different</b> outcomes: TP = true positive, FP = false positive, TN = true negative, FN = false negative. The confusion table. Many of the performance measures presented here are familiar to clinicians from diagnostic testing, e ...", "dateLastCrawled": "2021-12-01T21:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) Transformer Deep <b>Learning</b> Model for Bangla-English <b>Machine</b> ...", "url": "https://www.researchgate.net/publication/352440294_Transformer_Deep_Learning_Model_for_Bangla-English_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/352440294_Transformer_Deep_<b>Learning</b>_Model_for...", "snippet": "Effect of number of heads on <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) score and training time in Transformer Model for 20000 Steps. Figures - uploaded by Md. Ahsan Habib Author content", "dateLastCrawled": "2022-01-06T14:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning Translation and the Google Translate</b> Algorithm - <b>KDnuggets</b>", "url": "https://www.kdnuggets.com/2017/09/machine-learning-translation-google-translate-algorithm.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2017/09/<b>machine</b>-<b>learning</b>-translation-google-translate...", "snippet": "There are a lot of approaches that partly solve this problem, but the most popular and effective metric is <b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>). Imagine, we <b>have</b> <b>two</b> candidates from <b>machine</b> translators: Candidate 1: Statsbot makes it easy for companies to closely monitor <b>data</b> from various analytical platforms via natural language.", "dateLastCrawled": "2022-01-31T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "An <b>Evaluation of SMT in Medical Context</b> \u2013 KantanMT \u2013 <b>Machine</b> <b>Learning</b> ...", "url": "https://kantanmtblog.com/2016/12/13/an-evaluation-of-smt-in-medical-context/", "isFamilyFriendly": true, "displayUrl": "https://kantanmtblog.com/2016/12/13/an-<b>evaluation-of-smt-in-medical-context</b>", "snippet": "The main parameter used to automatically measure the translation quality was the <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) score. However, in the KantanMT cloud-based platform, quality was also measured using F-measure, <b>BLEU</b>, TER and Word Count, which are calculated automatically as the engine was trained each time, when fed with training <b>data</b>.", "dateLastCrawled": "2022-01-31T13:11:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "What is a <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score? The <b>BLEU</b> score is a string-matching algorithm that provides basic output quality metrics for MT researchers and developers. In this post, we review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality assessment metric in use by MT researchers and developers over the last 15 years. While it is widely understood that <b>BLEU</b> has many flaws, it has not been displaced or replaced by a widely accepted ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise <b>machine</b> translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching algorithm that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Evaluation</b> of <b>Text Generation</b>: A Survey | DeepAI", "url": "https://deepai.org/publication/evaluation-of-text-generation-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>evaluation</b>-of-<b>text-generation</b>-a-survey", "snippet": "The <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>bleu</b>) is one of the first metrics used to measure the similarity between <b>two</b> sentences (Papineni et al., 2002). Originally proposed for <b>machine</b> translation, it compares a candidate translation of text to one or more reference translations. <b>bleu</b>. is a weighted geometric mean of . n-gram precision scores ...", "dateLastCrawled": "2022-02-03T15:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language <b>Evaluation</b> | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) #language. A score between 0.0 and 1.0, inclusive, indicating the quality of a translation between <b>two</b> human <b>languages</b> (for example, between English and Russian). A <b>BLEU</b> score of 1.0 indicates a perfect translation; a <b>BLEU</b> score of 0.0 indicates a terrible translation. C. causal language model. #language. Synonym for unidirectional language model. See bidirectional language model to contrast <b>different</b> directional approaches in language modeling. crash ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>BLEU</b>: a Method for <b>Automatic Evaluation of Machine Translation</b>", "url": "https://www.researchgate.net/publication/2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2588204_", "snippet": "We describe <b>two</b> metrics for <b>automatic evaluation of machine trans-lation</b> quality. These metrics, <b>BLEU</b> and NEE, are compared to hu-man judgment of quality of translation of Arabic, Chinese, French ...", "dateLastCrawled": "2022-02-01T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Choosing the <b>Right Metric for Evaluating Machine Learning</b> Models \u2013 Part ...", "url": "https://www.kdnuggets.com/2018/04/right-metric-evaluating-machine-learning-models-1.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2018/04/right-metric-evaluating-<b>machine</b>-<b>learning</b>-models-1.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) It is mostly used to measure the quality of <b>machine</b> translation with respect to the human translation. It uses a modified form of precision metric. Steps to compute <b>BLEU</b> score: 1. Convert the sentence into unigrams, bigrams, trigrams, and 4-grams 2. Compute precision for n-grams of size 1 to 4 3. Take the ...", "dateLastCrawled": "2022-01-25T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An <b>automatic evaluation metric for Ancient-Modern Chinese translation</b> ...", "url": "https://link.springer.com/article/10.1007/s00521-020-05216-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00521-020-05216-8", "snippet": "Because human <b>evaluation</b> is expensive and subjective, all the above methods choose <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) to evaluate Ancient Chinese translation. <b>BLEU</b> is fast and convenient, and widely used in <b>machine</b> translation of multiple <b>languages</b>. However, it will cause lots of problems if just based on precision. For example, common words and short translated sentences may interfere with <b>evaluation</b> scores", "dateLastCrawled": "2021-12-13T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Automatic Assessment of Open Ended</b> Questions with a <b>Bleu</b>-Inspired ...", "url": "https://www.researchgate.net/publication/221418794_Automatic_Assessment_of_Open_Ended_Questions_with_a_Bleu-Inspired_Algorithm_and_Shallow_NLP", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221418794_<b>Automatic_Assessment_of_Open_Ended</b>...", "snippet": "For the <b>evaluation</b> we use <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) and NIST (National Institute of Standards and Technology) scoring techniques. A detailed <b>evaluation</b> of these models is performed by ...", "dateLastCrawled": "2021-12-24T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Automatic Image Captioning with Deep <b>Learning</b> - <b>GitHub</b>", "url": "https://github.com/Jasminehh/automatic_image_captioning", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Jasminehh/automatic_image_captioning", "snippet": "Model <b>Evaluation</b> <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) For testing purposes I turned to the <b>Bilingual</b> <b>evaluation</b> <b>understudy</b> (BLUE) set of benchmarks. <b>BLEU</b> contained a metric for determining the similarity between a model-generated sentence and various other reference sentences. Being close to 1 means that the <b>two</b> are very <b>similar</b>, being close ...", "dateLastCrawled": "2022-01-28T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Setting up <b>a neural machine translation system for English</b> to Indian ...", "url": "https://www.sciencedirect.com/science/article/pii/B9780128194430000118", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/B9780128194430000118", "snippet": "We <b>have</b> evaluated our system using the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) score . In each configuration the <b>BLEU</b> score of the translation scores are <b>different</b>, and Table 11.7 shows the <b>BLEU</b> score for each <b>different</b> configuration. The same mechanism is used to evaluate English to other 5 Indian language NMT systems as well. The <b>BLEU</b> scores ...", "dateLastCrawled": "2021-12-20T20:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Neural <b>Machine Translation</b>: Inner Workings ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/neural-machine-translation-inner-workings-seq2seq-and-transformers-229faff5895b", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/neural-<b>machine-translation</b>-inner-workings-seq2seq-and...", "snippet": "Alternatively, people mostly use the <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score. It produces a score between [0,1], in some cases, we multiply it with 100 for convenience. It uses n-grams (1, 2, 3, and 4 mostly) to evaluate the translation with some additional internal tricks. But this <b>evaluation</b> mechanism might slightly fail in this example as well. Overall, evaluating an NMT system is a subtle problem and one should expect that an increase in the <b>BLEU</b> score does not necessarily mean that ...", "dateLastCrawled": "2022-01-29T08:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Automatic Assessment of Open Ended</b> Questions with a <b>Bleu</b>-Inspired ...", "url": "https://www.researchgate.net/publication/221418794_Automatic_Assessment_of_Open_Ended_Questions_with_a_Bleu-Inspired_Algorithm_and_Shallow_NLP", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221418794_<b>Automatic_Assessment_of_Open_Ended</b>...", "snippet": "For the <b>evaluation</b> we use <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) and NIST (National Institute of Standards and Technology) scoring techniques. A detailed <b>evaluation</b> of these models is performed by ...", "dateLastCrawled": "2021-12-24T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What are <b>the benefits/disadvantages associated with word</b> error rate ...", "url": "https://www.quora.com/What-are-the-benefits-disadvantages-associated-with-word-error-rate-WER-compared-to-the-BLEU-bilingual-evaluation-understudy-metric", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-<b>the-benefits-disadvantages-associated-with-word</b>-error...", "snippet": "Answer: WER tends to predicate a raw metric of how many updates must be performed - in terms of deletion, editing, etc. Where as of <b>BLEU</b> tends to measure the translation strength - in comparison to what would be judged as being \u201cfavored by people\u201d. However - I think both of them run into distin...", "dateLastCrawled": "2022-01-21T01:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Human <b>Evaluation</b> Of <b>Machine</b> Translation", "url": "https://groups.google.com/g/ug5zfjhnv/c/JfGeRYENjvg", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/ug5zfjhnv/c/JfGeRYENjvg", "snippet": "<b>BLEU</b> Score some quick daily use, cover all <b>machine</b> translation research uses automatic metrics like <b>BLEU</b>, most expert developers now regularly use human <b>evaluation</b> on smaller <b>sets</b> of <b>data</b> to deter that they indeed <b>have</b> medicine and meaningful <b>BLEU</b> scores. The instructions include a handy decision tree to aid review the annotation <b>process</b>. Never hesitate to reach five to us directly to request additional information. <b>BLEU</b> that focuses on cash rather than precision. There mere no consideration ...", "dateLastCrawled": "2022-01-22T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Evaluation</b> of <b>Text Generation</b>: A Survey | DeepAI", "url": "https://deepai.org/publication/evaluation-of-text-generation-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>evaluation</b>-of-<b>text-generation</b>-a-survey", "snippet": "The <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>bleu</b>) is one of the first metrics used to measure the similarity between <b>two</b> sentences (Papineni et al., 2002). Originally proposed for <b>machine</b> translation, it compares a candidate translation of text to one or more reference translations. <b>bleu</b>. is a weighted geometric mean of . n-gram precision scores ...", "dateLastCrawled": "2022-02-03T15:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Automatic Image Captioning with Deep <b>Learning</b> - <b>GitHub</b>", "url": "https://github.com/Jasminehh/automatic_image_captioning", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Jasminehh/automatic_image_captioning", "snippet": "Model <b>Evaluation</b> <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) For testing purposes I turned to the <b>Bilingual</b> <b>evaluation</b> <b>understudy</b> (BLUE) set of benchmarks. <b>BLEU</b> contained a metric for determining the similarity between a model-generated sentence and various other reference sentences. Being close to 1 means that the <b>two</b> are very similar, being close ...", "dateLastCrawled": "2022-01-28T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Talk Review: Deep Learning: Practice and Trends</b>, NIPS 2017 \u2014 Part I ...", "url": "https://medium.com/syncedreview/talk-review-deep-learning-practice-and-trends-nips-2017-part-i-fbd7eb2c81f5", "isFamilyFriendly": true, "displayUrl": "https://medium.com/syncedreview/<b>talk-review-deep-learning-practice-and-trends</b>-nips...", "snippet": "<b>Machine</b> translation is measured using <b>BLEU</b> (<b>bilingual</b> <b>evaluation</b> <b>understudy</b>), an algorithm for evaluating the quality of text which has been <b>machine</b> translated from one natural language to another ...", "dateLastCrawled": "2020-11-29T02:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Paraphrase Recognition using Neural Network Classification</b> ...", "url": "https://www.academia.edu/5435816/Paraphrase_Recognition_using_Neural_Network_Classification", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/5435816/<b>Paraphrase_Recognition_using_Neural_Network</b>...", "snippet": "Of The <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) metric was these the number of paraphrases in the training set and test set proposed by Papineni et. al as a method for automatic <b>evaluation</b> are 2753 and 1147 respectively [6]. of <b>machine</b> translation. It is based on the concept of a weighted average of similar length phrase matches (n-grams). The <b>BLEU</b> 4.2 Feature Extraction metric has been adapted for assessing similarity between The Feature Extraction module is responsible for extracting ...", "dateLastCrawled": "2021-12-28T06:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Interpreting quality assessment re-imagined: The synergy between human ...", "url": "https://journals.sagepub.com/doi/10.1177/27523810211033670", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/10.1177/27523810211033670", "snippet": "A number of metrics <b>have</b> been developed and used to automatically evaluate <b>machine</b>-translated product, including <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>, see Papineni et al., 2002), National Institute of Standards and Technology (NIST, see Doddington, 2002), Metric for <b>Evaluation</b> of Translation With Explicit Ordering (METEOR, see Banerjee &amp; Lavie, 2005), and Translation Edit Rate (TER, see Snover et al., 2006). 1", "dateLastCrawled": "2022-02-03T17:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "An investigation of <b>machine</b> translation output quality and the ...", "url": "https://www.cambridge.org/core/journals/recall/article/an-investigation-of-machine-translation-output-quality-and-the-influencing-factors-of-source-texts/0112BA1949638F2EF46180D56516BF04", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/recall/article/an-investigation-of-<b>machine</b>...", "snippet": "Advances in artificial intelligence and <b>machine</b> <b>learning</b> <b>have</b> been making the quality of MT more reliable, more ... such as <b>bilingual</b> <b>evaluation</b> <b>understudy</b> (<b>BLEU</b>) or metric for <b>evaluation</b> of translation with explicit ordering (METEOR) (Maruf &amp; Haffari, Reference Maruf, Haffari, Gurevych and Miyao 2018; Yang, Chen, Wang &amp; Xu, Reference Yang, Chen, Wang, Xu, Gurevych and Miyao 2018). Therefore, they are difficult to apply to FL education. Without understanding the quality of MT from the ...", "dateLastCrawled": "2022-01-25T21:46:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Understanding MT Quality: <b>BLEU</b> Scores | by K Vashee | Medium", "url": "https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d", "isFamilyFriendly": true, "displayUrl": "https://kvashee.medium.com/understanding-mt-quality-<b>bleu</b>-scores-9a19ed20526d", "snippet": "What is <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>)? As the use of enterprise <b>machine</b> translation expands, it becomes increasingly more important for users and practitioners to understand MT quality issues in a relevant, meaningful, and accurate way. The <b>BLEU</b> score is a string-matching algorithm that provides basic outpu t quality metrics for MT researchers and developers. In this first post, we will review and look more closely at the <b>BLEU</b> score, which is probably the most widely used MT quality ...", "dateLastCrawled": "2022-01-31T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding MT Quality: <b>BLEU</b> Scores", "url": "https://blog.modernmt.com/understanding-mt-quality-bleu-scores/", "isFamilyFriendly": true, "displayUrl": "https://blog.modernmt.com/understanding-mt-quality-<b>bleu</b>-scores", "snippet": "In most cases comparing <b>BLEU</b> scores across <b>different</b> <b>languages</b> is meaningless unless very strict protocols <b>have</b> been followed. Trying to compare <b>BLEU</b> scores across <b>different</b> corpora and <b>languages</b> is strongly discouraged. Even comparing <b>BLEU</b> scores for the same corpus but with <b>different</b> numbers of reference translations <b>can</b> be highly misleading. Because of this, it is always recommended to use human translators to verify the accuracy of the metrics after systems <b>have</b> been built. Also, best ...", "dateLastCrawled": "2022-02-03T17:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>BLEU</b>: a Method for <b>Automatic Evaluation of Machine Translation</b>", "url": "https://www.researchgate.net/publication/2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/2588204_", "snippet": "We describe <b>two</b> metrics for <b>automatic evaluation of machine trans-lation</b> quality. These metrics, <b>BLEU</b> and NEE, are <b>compared</b> to hu-man judgment of quality of translation of Arabic, Chinese, French ...", "dateLastCrawled": "2022-02-01T20:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Neural <b>Machine Translation</b>: Inner Workings ... - Towards <b>Data</b> Science", "url": "https://towardsdatascience.com/neural-machine-translation-inner-workings-seq2seq-and-transformers-229faff5895b", "isFamilyFriendly": true, "displayUrl": "https://towards<b>data</b>science.com/neural-<b>machine-translation</b>-inner-workings-seq2seq-and...", "snippet": "Alternatively, people mostly use the <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) score. It produces a score between [0,1], in some cases, we multiply it with 100 for convenience. It uses n-grams (1, 2, 3, and 4 mostly) to evaluate the translation with some additional internal tricks. But this <b>evaluation</b> mechanism might slightly fail in this example as well. Overall, evaluating an NMT system is a subtle problem and one should expect that an increase in the <b>BLEU</b> score does not necessarily mean that ...", "dateLastCrawled": "2022-01-29T08:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) #language. A score between 0.0 and 1.0, inclusive, indicating the quality of a translation between <b>two</b> human <b>languages</b> (for example, between English and Russian). A <b>BLEU</b> score of 1.0 indicates a perfect translation; a <b>BLEU</b> score of 0.0 indicates a terrible translation. boosting. A <b>machine learning</b> technique that iteratively combines a set of simple and not very accurate classifiers (referred to as &quot;weak&quot; classifiers) into a classifier with high accuracy ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Setting up <b>a neural machine translation system for English</b> to Indian ...", "url": "https://www.sciencedirect.com/science/article/pii/B9780128194430000118", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/B9780128194430000118", "snippet": "We <b>have</b> evaluated our system using the <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) score . In each configuration the <b>BLEU</b> score of the translation scores are <b>different</b>, and Table 11.7 shows the <b>BLEU</b> score for each <b>different</b> configuration. The same mechanism is used to evaluate English to other 5 Indian language NMT systems as well. The <b>BLEU</b> scores ...", "dateLastCrawled": "2021-12-20T20:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "An <b>Evaluation of SMT in Medical Context</b> \u2013 KantanMT \u2013 <b>Machine</b> <b>Learning</b> ...", "url": "https://kantanmtblog.com/2016/12/13/an-evaluation-of-smt-in-medical-context/", "isFamilyFriendly": true, "displayUrl": "https://kantanmtblog.com/2016/12/13/an-<b>evaluation-of-smt-in-medical-context</b>", "snippet": "The main parameter used to automatically measure the translation quality was the <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) score. However, in the KantanMT cloud-based platform, quality was also measured using F-measure, <b>BLEU</b>, TER and Word Count, which are calculated automatically as the engine was trained each time, when fed with training <b>data</b>.", "dateLastCrawled": "2022-01-31T13:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "SUGAMAN: describing floor plans for visually impaired by annotation ...", "url": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2018.5627", "isFamilyFriendly": true, "displayUrl": "https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-ipr.2018.5627", "snippet": "For that purpose several metrics for example <b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b> (<b>BLEU</b>) , Recall-Oriented <b>Understudy</b> for Gisting <b>Evaluation</b> ... For 510 images in ROBIN <b>data</b> <b>sets</b>, there were 2040 descriptions collected in total. The <b>data</b> set is tokenised and pre-processed for further processing. In our experiments we <b>have</b> <b>compared</b> <b>machine</b>-generated description with these descriptions and an analysis regarding closeness of <b>machine</b> translations with human-written descriptions is done. Next, we ...", "dateLastCrawled": "2022-01-24T16:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Study and correlation analysis of linguistic, perceptual, and automatic ...", "url": "https://www.deepdyve.com/lp/wiley/study-and-correlation-analysis-of-linguistic-perceptual-and-automatic-BbfUB9DyGX", "isFamilyFriendly": true, "displayUrl": "https://www.deepdyve.com/lp/wiley/study-and-correlation-analysis-of-linguistic...", "snippet": "<b>Evaluation</b> of <b>machine</b> translation output is an important task. Various human <b>evaluation</b> techniques as well as automatic metrics <b>have</b> been proposed and investigated in the last decade. However, very few <b>evaluation</b> methods take the linguistic aspect into account. In this article, we use an objective <b>evaluation</b> method for <b>machine</b> translation output that classifies all translation errors into one of the five following linguistic levels: orthographic, morphological, lexical, semantic, and syntactic.", "dateLastCrawled": "2021-06-24T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) Transformer Deep <b>Learning</b> Model for Bangla-English <b>Machine</b> ...", "url": "https://www.researchgate.net/publication/352440294_Transformer_Deep_Learning_Model_for_Bangla-English_Machine_Translation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/352440294_Transformer_Deep_<b>Learning</b>_Model_for...", "snippet": "Significant efforts <b>have</b> been made for such <b>languages</b> in terms of making the <b>data</b> publicly accessible and also organizing <b>evaluation</b> campaigns. <b>Compared</b> to that there are very fewer efforts for ...", "dateLastCrawled": "2022-01-06T14:03:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Evaluation</b> of an <b>NLP</b> model \u2014 latest benchmarks | by Ria Kulshrestha ...", "url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>evaluation</b>-of-an-<b>nlp</b>-model-latest-benchmarks-90fd8ce6fae5", "snippet": "<b>BLEU</b> Score \u2014 <b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>. As the name suggests, it was originally used to evaluate translations from one language to another. How to calculate <b>BLEU</b> score? Calculating unigram precision: Step 1: Look at each word in the output sentence and assign it a score of 1 if it shows up in any of the reference sentences and 0 if it doesn\u2019t. Step 2: Normalize that count, so that it\u2019s always between 0 and 1, by dividing the number of words that showed up in one of the reference ...", "dateLastCrawled": "2022-01-28T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Machine</b> <b>Learning</b> Glossary: Language <b>Evaluation</b> | Google Developers", "url": "https://developers.google.com/machine-learning/glossary/language", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine</b>-<b>learning</b>/glossary/language", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) #language. A score between 0.0 and 1.0, inclusive, indicating the quality of a translation between two human languages (for example, between English and Russian). A <b>BLEU</b> score of 1.0 indicates a perfect translation; a <b>BLEU</b> score of 0.0 indicates a terrible translation. C. causal language model. #language. Synonym for unidirectional language model. See bidirectional language model to contrast different directional approaches in language modeling. crash ...", "dateLastCrawled": "2022-01-29T02:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Natrual language processing basic concepts - language model - word ...", "url": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "isFamilyFriendly": true, "displayUrl": "https://shuffleai.blog/blog/nlp_concepts_part_1.html", "snippet": "<b>BLEU</b> stands for <b>bilingual</b> <b>evaluation</b> <b>understudy</b>. It&#39;s an automatic metric to evaluate how close a sequence of text generated by a language model is to a reference. At first, it&#39;s used to evaluate the quality of <b>machine</b> translation text. Now other natural language processing tasks such as task-oriented dialogue generation adopt it as well. For a reference &quot;The man returned to the store&quot;, a generated text &quot;the the man the&quot; would get a BLUE score as below. For each word in the generated text ...", "dateLastCrawled": "2021-12-24T07:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "The <b>Computational</b> Limits of Deep <b>Learning</b> | DeepAI", "url": "https://deepai.org/publication/the-computational-limits-of-deep-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/the-<b>computational</b>-limits-of-deep-<b>learning</b>", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) [papineni2002bleu] score is a metric for translation and computes the similarity between human translation and <b>machine</b> translation based on n-gram. An n-gram is a continuous sequence of n items from a given text. The score is based on precision, brevity penalty, and clipping. The modified n-gram precision ...", "dateLastCrawled": "2022-01-28T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "9.7. <b>Sequence</b> to <b>Sequence</b> <b>Learning</b> \u2014 Dive into Deep <b>Learning</b> 0.17.2 ...", "url": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "isFamilyFriendly": true, "displayUrl": "https://d2l.ai/chapter_recurrent-modern/seq2seq.html", "snippet": "<b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>), though originally proposed for evaluating <b>machine</b> translation results [Papineni et al., 2002], has been extensively used in measuring the quality of output sequences for different applications.", "dateLastCrawled": "2022-01-26T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>The Evolution of Machine Translation</b> | LLM Law Review", "url": "https://www.llmlawreview.com/2018/01/26/the-evolution-of-machine-translation/", "isFamilyFriendly": true, "displayUrl": "https://www.llmlawreview.com/2018/01/26/<b>the-evolution-of-machine-translation</b>", "snippet": "Using the <b>BLEU</b> (<b>Bilingual</b> <b>Evaluation</b> <b>Understudy</b>) method to score the outcome, they found that NMT scored consistently higher than PBSMT for accuracy. In addition, human translators whose native language was Catalan but who were also fluent in English, evaluated sections of the MT translation in three of the books. Once again, the NMT outperformed its rival. It was estimated that \u201cbetween 17% and 34% of the translations \u2026 are perceived by native speakers of the target language to be of ...", "dateLastCrawled": "2022-01-19T23:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Arti cial Intelligence Master Thesis", "url": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "isFamilyFriendly": true, "displayUrl": "https://upcommons.upc.edu/bitstream/handle/2117/105513/122533.pdf?sequence=1", "snippet": "2:87 <b>BLEU</b> (<b>BiLingual</b> <b>Evaluation</b> <b>Understudy</b>) score over the baseline; attention model, for German-English translation, and 0:34 <b>BLEU</b> score improvement for Catalan-Spanish trans-lation. Keywords <b>Machine</b> <b>Learning</b>, Deep <b>Learning</b>, Natural Language Processing, Neural <b>Machine</b> Transla-tion", "dateLastCrawled": "2021-12-27T01:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Machine</b> Translation Development for Indian Languages and its ...", "url": "https://www.academia.edu/12395564/Machine_Translation_Development_for_Indian_Languages_and_its_Approaches", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/12395564", "snippet": "Urdu to English 2013 Urdu-English General Corpus based Explained <b>Machine</b> Translation methodology of using <b>Bilingual</b> each system and <b>Evaluation</b> found their <b>Understudy</b> [32] comparison based on their respective outputs using <b>BLEU</b>. The EBMT approach produced accuracy of 84.21% whereas the accuracy of the online SMT system is 62.68%. 4. Developing English- -- English-Urdu General Interlingua The English-Hindi Urdu <b>Machine</b> based Rule- lexical database is Translation Via Hindi based approach used ...", "dateLastCrawled": "2022-01-10T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Evaluation of machine translation systems and related procedures</b>", "url": "https://www.researchgate.net/publication/326320090_Evaluation_of_machine_translation_systems_and_related_procedures", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/326320090_<b>Evaluation</b>_of_<b>machine</b>_translation...", "snippet": "<b>Evaluation of machine translation systems and related procedures</b>. June 2018 ; Journal of Engineering and Applied Sciences 13(12):3961-3972; Project: <b>Machine</b> <b>learning</b>; Authors: Musatafa Albadr ...", "dateLastCrawled": "2022-01-15T04:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Learning</b> by <b>Analogy</b>: A Classification Rule for Binary and Nominal ...", "url": "https://www.researchgate.net/publication/220812160_Learning_by_Analogy_A_Classification_Rule_for_Binary_and_Nominal_Data", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220812160_<b>Learning</b>_by_<b>Analogy</b>_A...", "snippet": "We are interested here in the use of analogical proportions for making predictions, in a <b>machine</b> <b>learning</b> context. In recent works, <b>analogy</b>-based classifiers have achieved noteworthy performances ...", "dateLastCrawled": "2022-01-05T06:35:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(bleu (bilingual evaluation understudy))  is like +(machine learning process where you have two different \"languages\" or data sets)", "+(bleu (bilingual evaluation understudy)) is similar to +(machine learning process where you have two different \"languages\" or data sets)", "+(bleu (bilingual evaluation understudy)) can be thought of as +(machine learning process where you have two different \"languages\" or data sets)", "+(bleu (bilingual evaluation understudy)) can be compared to +(machine learning process where you have two different \"languages\" or data sets)", "machine learning +(bleu (bilingual evaluation understudy) AND analogy)", "machine learning +(\"bleu (bilingual evaluation understudy) is like\")", "machine learning +(\"bleu (bilingual evaluation understudy) is similar\")", "machine learning +(\"just as bleu (bilingual evaluation understudy)\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be thought of as\")", "machine learning +(\"bleu (bilingual evaluation understudy) can be compared to\")"]}