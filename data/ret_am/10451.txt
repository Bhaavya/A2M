{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "Precision-Recall <b>Area</b> <b>Under</b> <b>Curve</b> (<b>AUC</b>) Score The Precision-Recall <b>AUC</b> is just <b>like</b> the ROC <b>AUC</b>, in that it summarizes the <b>curve</b> with a range of threshold values as a single score. The score can then be used as a point of comparison between different models on a binary classification problem where a score of 1.0 represents a model with perfect skill.", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "The <b>area</b> <b>under</b> the precision\u2010recall <b>curve as a performance metric</b> for ...", "url": "https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13140", "isFamilyFriendly": true, "displayUrl": "https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13140", "snippet": "We evaluated the <b>area</b> <b>under</b> the precision-recall <b>curve</b> (<b>AUC</b>-<b>PR</b>) as a <b>performance metric for rare binary events</b>, focusing on the assessment of species distribution models. Precision is the probability that a species is present given a predicted presence, while recall (more commonly called sensitivity) is the probability the model predicts presence in locations where the species has been observed. We simulated species at three levels of prevalence, compared <b>AUC</b>-<b>PR</b> and the <b>area</b> <b>under</b> the ...", "dateLastCrawled": "2022-02-01T10:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "How to Use ROC Curves and <b>Precision-Recall Curves for Classification</b> in ...", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "<b>Area</b> <b>Under</b> <b>Curve</b>: <b>like</b> the <b>AUC</b>, summarizes the integral or an approximation of the <b>area</b> <b>under</b> the precision-recall <b>curve</b>. In terms of model selection, F-Measure summarizes model skill for a specific probability threshold (e.g. 0.5), whereas the <b>area</b> <b>under</b> <b>curve</b> summarize the skill of a model across thresholds, <b>like</b> ROC <b>AUC</b>.", "dateLastCrawled": "2022-02-03T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. See <b>PR</b> <b>AUC</b> (<b>Area</b> <b>under</b> <b>the PR</b> <b>Curve</b>). <b>area</b> <b>under</b> the ROC <b>curve</b>. See <b>AUC</b> (<b>Area</b> <b>under</b> the ROC <b>curve</b>). artificial general intelligence . A non-human mechanism that demonstrates a broad range of problem solving, creativity, and adaptability. For example, a program demonstrating artificial general intelligence could translate text, compose symphonies, and excel at games that have not yet been invented. artificial intelligence. A non-human program or model that can solve ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding <b>AUC</b> - ROC <b>Curve</b> | by Sarang Narkhede | Towards Data Science", "url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>under</b>standing-<b>auc</b>-roc-<b>curve</b>-68b2303cc9c5", "snippet": "In <b>Machine</b> <b>Learning</b>, performance measurement is an essential task. So when it comes to a classification problem, we can count on an <b>AUC</b> - ROC <b>Curve</b>. When we need to check or visualize the performance of the multi-class classification problem, we use the <b>AUC</b> <b>Area</b> <b>Under</b> The <b>Curve</b>) ROC (Receiver Operating Characteristics) <b>curve</b>. It is one of the most important evaluation metrics for checking any classification model\u2019s performance. It is also written as AUROC (<b>Area</b> <b>Under</b> the Receiver Operating ...", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is <b>the relationship between Accuracy, precision and</b> <b>AUC</b> (<b>Area</b> ...", "url": "https://www.quora.com/What-is-the-relationship-between-Accuracy-precision-and-AUC-Area-Under-the-Curve", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-relationship-between-Accuracy-precision-and</b>-<b>AUC</b>-<b>Area</b>...", "snippet": "Answer: This is surely possible. Accuracy shows the percentage of the correct classifications with respect to the all samples. But it does not say anything about the performances for negative and positive classes. Precision measures how many of the positively classified samples were really positi...", "dateLastCrawled": "2022-01-28T07:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Identification of Tumor-Specific MRI Biomarkers Using <b>Machine</b> <b>Learning</b> (ML)", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8143297/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8143297", "snippet": "ROC and precision-recall (<b>PR</b>) analyses are usually performed side by side, and the <b>area</b> <b>under</b> the <b>curve</b> (<b>AUC</b>) is calculated to assess model performance in each case . Both ROC-<b>AUC</b> <b>area</b> <b>under</b> the <b>curve</b> of receiver operating characteristic curves and <b>PR</b>-<b>AUC</b> <b>area</b> <b>under</b> the <b>curve</b> of precision-recall curves are widely used to assess the performance of ML methods for MRI biomarkers [ 100 , 129 , 130 ].", "dateLastCrawled": "2022-02-02T15:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Precision-recall</b> curves \u2013 what are they and how are they used?", "url": "https://acutecaretesting.org/en/articles/precision-recall-curves-what-are-they-and-how-are-they-used", "isFamilyFriendly": true, "displayUrl": "https://acutecaretesting.org/en/articles/<b>precision-recall</b>-<b>curves</b>-what-are-they-and-how...", "snippet": "Summary This article demonstrates that adding a lot of patients without disease and with low test results to a study may improve the ROC <b>curve</b> significantly without any improvement in sensitivity or in positive predictive value of the parameter evaluated. The <b>precision-recall</b> curves are not impacted by the addition of patients without disease and with low test results. It is highly recommended to use <b>precision-recall</b> curves as a supplement to the routinely used ROC curves to get the full ...", "dateLastCrawled": "2022-02-02T06:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why is <b>AUC</b> (<b>Area</b> <b>under</b> <b>ROC) insensitive to class distribution changes</b> ...", "url": "https://www.quora.com/Why-is-AUC-Area-under-ROC-insensitive-to-class-distribution-changes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-<b>AUC</b>-<b>Area</b>-<b>under</b>-<b>ROC-insensitive-to-class-distribution-changes</b>", "snippet": "Answer (1 of 2): Well, the ROC <b>curve</b> is sensitive to the the class conditional likelihoods P(X|C_1) and P(X|C_2) because you are plotting P(miss) and P(FA) as a function of a decision threshold applied to the the likelihood ratio P(X|C_1)/P(X|C_2). The class prior probabilities P(C_1) and P(C_2)...", "dateLastCrawled": "2022-01-12T21:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - <b>ROC</b> vs precision-and-recall curves - Cross Validated", "url": "https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/7207", "snippet": "Remarkably, when constructing the achievable <b>PR</b> <b>curve</b> one discards exactly the same points omit- ted by the convex hull in <b>ROC</b> space. Consequently, we can efficiently compute the achievable <b>PR</b> <b>curve</b>. [...] Finally, we show that an <b>algorithm</b> that optimizes the <b>area</b> <b>under</b> the <b>ROC</b> <b>curve</b> is not guaranteed to optimize the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>.", "dateLastCrawled": "2022-01-27T09:38:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The <b>area</b> <b>under</b> the precision\u2010recall <b>curve as a performance metric</b> for ...", "url": "https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13140", "isFamilyFriendly": true, "displayUrl": "https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13140", "snippet": "We evaluated the <b>area</b> <b>under</b> the precision-recall <b>curve</b> (<b>AUC</b>-<b>PR</b>) as a <b>performance metric for rare binary events</b>, focusing on the assessment of species distribution models. Precision is the probability that a species is present given a predicted presence, while recall (more commonly called sensitivity) is the probability the model predicts presence in locations where the species has been observed. We simulated species at three levels of prevalence, compared <b>AUC</b>-<b>PR</b> and the <b>area</b> <b>under</b> the ...", "dateLastCrawled": "2022-02-01T10:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "Precision-Recall <b>Area</b> <b>Under</b> <b>Curve</b> (<b>AUC</b>) Score The Precision-Recall <b>AUC</b> is just like the ROC <b>AUC</b>, in that it summarizes the <b>curve</b> with a range of threshold values as a single score. The score can then be used as a point of comparison between different models on a binary classification problem where a score of 1.0 represents a model with perfect skill.", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Demystifying ROC and precision-recall curves | by Fabio Sigrist | Jan ...", "url": "https://towardsdatascience.com/demystifying-roc-and-precision-recall-curves-d30f3fad2cbf", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/demystifying-roc-and-precision-recall-<b>curves</b>-d30f3fad2cbf", "snippet": "Further, the <b>area</b> <b>under</b> the ROC <b>curve</b> (<b>AUC</b>, aka AUROC) summarizes this <b>curve</b> in a single number. The larger the <b>AUC</b>, the better. The <b>AUC</b> has the interpretation that, for instance, an <b>AUC</b> of 0.8 means that the classifier ranks two randomly chosen test data points correctly with a probability of 80%. The precision-recall (<b>PR</b>) <b>curve</b> and the AUPRC. The precision-recall (<b>PR</b>) <b>curve</b> plots the precision versus the recall (=true positive rate) for all possible thresholds \u03b4. The goal is to have both ...", "dateLastCrawled": "2022-02-02T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. See <b>PR</b> <b>AUC</b> (<b>Area</b> <b>under</b> <b>the PR</b> <b>Curve</b>). <b>area</b> <b>under</b> the ROC <b>curve</b>. See <b>AUC</b> (<b>Area</b> <b>under</b> the ROC <b>curve</b>). artificial general intelligence . A non-human mechanism that demonstrates a broad range of problem solving, creativity, and adaptability. For example, a program demonstrating artificial general intelligence could translate text, compose symphonies, and excel at games that have not yet been invented. artificial intelligence. A non-human program or model that can solve ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "The Precision-Recall Plot Is More Informative than the ROC Plot When ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349800/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4349800", "snippet": "A ROC <b>curve</b> provides a single performance measure called the <b>Area</b> <b>under</b> the ROC <b>curve</b> (<b>AUC</b>) score. <b>AUC</b> is 0.5 for random and 1.0 for perfect classifiers . <b>AUC</b> scores are convenient to compare the performances of multiple classifiers. ROC: The concentrated ROC (CROC) plot evaluates the early-retrieval performance of a classifier. The early retrieval (ER) <b>area</b> of a ROC plot (see the grey rectangle <b>area</b> in Fig. 2A) is useful for evaluating a fraction of the data with high-ranked instances [36 ...", "dateLastCrawled": "2022-01-16T20:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>machine</b> <b>learning</b> - <b>ROC</b> vs precision-and-recall curves - Cross Validated", "url": "https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/7207", "snippet": "Remarkably, when constructing the achievable <b>PR</b> <b>curve</b> one discards exactly the same points omit- ted by the convex hull in <b>ROC</b> space. Consequently, we can efficiently compute the achievable <b>PR</b> <b>curve</b>. [...] Finally, we show that an <b>algorithm</b> that optimizes the <b>area</b> <b>under</b> the <b>ROC</b> <b>curve</b> is not guaranteed to optimize the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>.", "dateLastCrawled": "2022-01-27T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How to Use ROC Curves and <b>Precision-Recall Curves for Classification</b> in ...", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "<b>Area</b> <b>Under</b> <b>Curve</b>: like the <b>AUC</b>, summarizes the integral or an approximation of the <b>area</b> <b>under</b> the precision-recall <b>curve</b>. In terms of model selection, F-Measure summarizes model skill for a specific probability threshold (e.g. 0.5), whereas the <b>area</b> <b>under</b> <b>curve</b> summarize the skill of a model across thresholds, like ROC <b>AUC</b>.", "dateLastCrawled": "2022-02-03T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Predict <b>Imbalanced</b> Classes in Python | Towards Data Science - Medium", "url": "https://towardsdatascience.com/how-to-effectively-predict-imbalanced-classes-in-python-e8cd3b5720c4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-effectively-predict-<b>imbalanced</b>-classes-in-python...", "snippet": "<b>Area</b> <b>Under</b> the Precision-Recall <b>curve</b> (<b>PR</b> <b>AUC</b>) A <b>PR</b> <b>curve</b> plots Recall on the x-axis against Precision on the y-axis for all the possible probability thresholds. In contrast with the ROC <b>curve</b>, a perfectly skilled model bows towards the top right axis with coordinates (1,1). Since both Precision and Recall are concerned with true positives (the ...", "dateLastCrawled": "2022-02-02T20:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>MRR</b> vs MAP vs NDCG: Rank-Aware Evaluation Metrics And When To Use Them ...", "url": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "snippet": "Interpretation of the MAP measure through the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. To compare two systems we want the largest possible <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. In the above example we compare systems A, B and ...", "dateLastCrawled": "2022-01-31T10:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Evaluate AutoML experiment results - <b>Azure</b> <b>Machine</b> <b>Learning</b> | Microsoft ...", "url": "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml", "isFamilyFriendly": true, "displayUrl": "https://docs.microsoft.com/en-us/<b>azure</b>/<b>machine</b>-<b>learning</b>/how-to-<b>under</b>stand-automated-ml", "snippet": "An <b>under</b>-confident model will assign a lower probability on average to the class it predicts and the associated calibration <b>curve</b> will look <b>similar</b> to an &quot;S&quot;. The calibration <b>curve</b> does not depict a model&#39;s ability to classify correctly, but instead its ability to correctly assign confidence to its <b>predictions</b>. A bad model can still have a <b>good</b> calibration <b>curve</b> if the model correctly assigns low confidence and high uncertainty.", "dateLastCrawled": "2022-02-02T18:17:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Backorder Prediction. Predicting Backorders using <b>Machine</b>\u2026 | by ...", "url": "https://medium.com/analytics-vidhya/backorder-prediction-d4f1c5362f18", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/backorder-prediction-d4f1c5362f18", "snippet": "So it makes a <b>good</b> metric for the problem. <b>Area</b> <b>under</b> Precision \u2014 recall <b>curve</b> : <b>AUC</b> of <b>Pr</b>-Re <b>curve</b> is very important as the business needs to select threshold based on trade off b/w precision ...", "dateLastCrawled": "2022-01-31T04:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>The Metric System</b>: How to Correctly Measure Your Model | by Shaked ...", "url": "https://towardsdatascience.com/the-metric-system-how-to-correctly-measure-your-model-17d3feaed6ab", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>the-metric-system</b>-how-to-correctly-measure-your-model...", "snippet": "So we <b>can</b> use the <b>area</b> <b>under</b> the <b>curve</b>, or <b>AUC</b>, as a metric to help us assess the model. Great! \u2014 But not really. As this blogpost by Jason Brownlee clearly explains, ROC curves are actually affected by the data imbalance too, as the metrics on both axes are mixing the two classes. So instead, he suggests to use another <b>curve</b>: the Precision-Recall <b>curve</b>, where we draw the Precision over the y-axis and the Recall over the x-axis for all thresholds. A <b>PR</b> <b>curve</b>. The dashed line marks the ...", "dateLastCrawled": "2022-01-31T10:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Precision-Recall <b>AUC</b> vs ROC <b>AUC for class imbalance problems</b> | Data ...", "url": "https://www.kaggle.com/general/7517", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kaggle</b>.com/general/7517", "snippet": "Hi all, I&#39;ve been reading the paper &quot;The Relationship Between Precision-Recall and ROC Curves&quot; recently, which argues that at problems suffering from class imbalance problem, using an evaluation metric of Precision-Recall <b>AUC</b> (<b>PR</b> <b>AUC</b>) is better than Receiver-Operating-Characteristic <b>AUC</b> (ROC <b>AUC</b>).The paper states that &quot;A large number change in the number of false positives <b>can</b> lead to a small change in the false positive rate used in ROC analysis.", "dateLastCrawled": "2022-02-02T22:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>machine</b> <b>learning</b> - Advantages of <b>AUC</b> vs standard <b>accuracy</b> - Data ...", "url": "https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/806", "snippet": "Really great question, and one that I find that most people don&#39;t really understand on an intuitive level. <b>AUC</b> is in fact often preferred over <b>accuracy</b> for binary classification for a number of different reasons. First though, let&#39;s talk about exactly what <b>AUC</b> is. Honestly, for being one of the most widely used efficacy metrics, it&#39;s surprisingly obtuse to figure out exactly how <b>AUC</b> works.. <b>AUC</b> stands for <b>Area</b> <b>Under</b> the <b>Curve</b>, which <b>curve</b> you ask?Well, that would be the ROC <b>curve</b>.ROC stands ...", "dateLastCrawled": "2022-01-27T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Detecting bad customer <b>reviews</b> with NLP | by Jonathan Oheix | Towards ...", "url": "https://towardsdatascience.com/detecting-bad-customer-reviews-with-nlp-d8b36134dc7e", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/detecting-bad-customer-<b>reviews</b>-with-nlp-d8b36134dc7e", "snippet": "A better metric in this imbalanced situation is the <b>AUC</b> <b>PR</b> (<b>Area</b> <b>Under</b> the <b>Curve</b> Precision Recall), or also called AP (Average Precision). We <b>can</b> see that the precision decreases when we increase the recall. This shows us that we have to choose a prediction thresold adapted to our needs. If our goal is to have a high recall, we should set a low prediction thresold that will allow us to detect most of the observations of the positive class, but with a low precision. On the contrary, if we ...", "dateLastCrawled": "2022-01-31T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "python - Is <b>AUC</b> a better metric than accuracy in case of imbalenced ...", "url": "https://stackoverflow.com/questions/54879340/is-auc-a-better-metric-than-accuracy-in-case-of-imbalenced-datasets-in-machine-l", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/54879340", "snippet": "You <b>can</b> also calculate metrics like <b>area</b> <b>under</b> the ROC <b>curve</b> (&quot;<b>AUC</b>&quot;) and <b>area</b> <b>under</b> the precision-recall <b>curve</b> (AUPRC.) These metrics <b>can</b> <b>be thought</b> of as &quot;averages&quot; over different decision thresholds. You calculate these using the vector of predicted probabilities, NOT a vector of binary labels. <b>Area</b> <b>under</b> the ROC <b>curve</b> is super popular but is not very useful when your data is skewed to have a lot of true negatives. <b>Area</b> <b>under</b> the precision recall <b>curve</b> is a great metric to use when your ...", "dateLastCrawled": "2022-01-15T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - <b>Advantages</b> of ROC curves - Cross Validated", "url": "https://stats.stackexchange.com/questions/28745/advantages-of-roc-curves", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/28745", "snippet": "After creating a ROC <b>curve</b>, the <b>AUC</b> (<b>area</b> <b>under</b> the <b>curve</b>) <b>can</b> be calculated. The <b>AUC</b> is accuracy of the test across many thresholds. <b>AUC</b> = 1 means the test is perfect. <b>AUC</b> = .5 means performs at chance for binary classification. If there are multiple models, <b>AUC</b> provides a single measurement to compare across different models. There are always ...", "dateLastCrawled": "2022-02-03T03:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Tour of <b>Evaluation Metrics for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/tour-of-evaluation-metrics-for-imbalanced...", "snippet": "The <b>area</b> <b>under</b> the ROC <b>curve</b> <b>can</b> be calculated and provides a single score to summarize the plot that <b>can</b> be used to compare models. A no skill classifier will have a score of 0.5, whereas a perfect classifier will have a score of 1.0. ROC <b>AUC</b> = ROC <b>Area</b> <b>Under</b> <b>Curve</b>", "dateLastCrawled": "2022-02-02T08:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>MRR</b> vs MAP vs NDCG: Rank-Aware Evaluation Metrics And When To Use Them ...", "url": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832", "snippet": "Interpretation of the MAP measure through the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. To compare two systems we want the largest possible <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. In the above example we compare systems A, B and ...", "dateLastCrawled": "2022-01-31T10:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A Gentle Introduction to Threshold-Moving for Imbalanced Classification", "url": "https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/threshold-moving-for-imbalanced-classification", "snippet": "The <b>curve</b> is useful to understand the trade-off in the true-positive rate and false-positive rate for different thresholds. The <b>area</b> <b>under</b> the ROC <b>Curve</b>, so-called ROC <b>AUC</b>, provides a single number to summarize the performance of a model in terms of its ROC <b>Curve</b> with a value between 0.5 (no-skill) and 1.0 (perfect skill).", "dateLastCrawled": "2022-01-30T10:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "The <b>area</b> <b>under</b> the precision\u2010recall <b>curve as a performance metric</b> for ...", "url": "https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13140", "isFamilyFriendly": true, "displayUrl": "https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13140", "snippet": "We evaluated the <b>area</b> <b>under</b> the precision-recall <b>curve</b> (<b>AUC</b>-<b>PR</b>) as a <b>performance metric for rare binary events</b>, focusing on the assessment of species distribution models. Precision is the probability that a species is present given a predicted presence, while recall (more commonly called sensitivity) is the probability the model predicts presence in locations where the species has been observed. We simulated species at three levels of prevalence, <b>compared</b> <b>AUC</b>-<b>PR</b> and the <b>area</b> <b>under</b> the ...", "dateLastCrawled": "2022-02-01T10:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Demystifying ROC and precision-recall curves | by Fabio Sigrist | Jan ...", "url": "https://towardsdatascience.com/demystifying-roc-and-precision-recall-curves-d30f3fad2cbf", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/demystifying-roc-and-precision-recall-<b>curves</b>-d30f3fad2cbf", "snippet": "Further, the <b>area</b> <b>under</b> the ROC <b>curve</b> (<b>AUC</b>, aka AUROC) summarizes this <b>curve</b> in a single number. The larger the <b>AUC</b>, the better. The <b>AUC</b> has the interpretation that, for instance, an <b>AUC</b> of 0.8 means that the classifier ranks two randomly chosen test data points correctly with a probability of 80%. The precision-recall (<b>PR</b>) <b>curve</b> and the AUPRC. The precision-recall (<b>PR</b>) <b>curve</b> plots the precision versus the recall (=true positive rate) for all possible thresholds \u03b4. The goal is to have both ...", "dateLastCrawled": "2022-02-02T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "Instead, the <b>area</b> <b>under</b> the <b>curve</b> <b>can</b> be calculated to give a single score for a classifier model across all threshold values. This is called the ROC <b>area</b> <b>under</b> <b>curve</b> or ROC <b>AUC</b> or sometimes ROCAUC. The score is a value between 0.0 and 1.0 for a perfect classifier.", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "What is <b>the relationship between Accuracy, precision and</b> <b>AUC</b> (<b>Area</b> ...", "url": "https://www.quora.com/What-is-the-relationship-between-Accuracy-precision-and-AUC-Area-Under-the-Curve", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-relationship-between-Accuracy-precision-and</b>-<b>AUC</b>-<b>Area</b>...", "snippet": "Answer: This is surely possible. Accuracy shows the percentage of the correct classifications with respect to the all samples. But it does not say anything about the performances for negative and positive classes. Precision measures how many of the positively classified samples were really positi...", "dateLastCrawled": "2022-01-28T07:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. See <b>PR</b> <b>AUC</b> (<b>Area</b> <b>under</b> <b>the PR</b> <b>Curve</b>). <b>area</b> <b>under</b> the ROC <b>curve</b>. See <b>AUC</b> (<b>Area</b> <b>under</b> the ROC <b>curve</b>). artificial general intelligence . A non-human mechanism that demonstrates a broad range of problem solving, creativity, and adaptability. For example, a program demonstrating artificial general intelligence could translate text, compose symphonies, and excel at games that have not yet been invented. artificial intelligence. A non-human program or model that <b>can</b> solve ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Use ROC Curves and <b>Precision-Recall Curves for Classification</b> in ...", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "<b>Area</b> <b>Under</b> <b>Curve</b>: like the <b>AUC</b>, summarizes the integral or an approximation of the <b>area</b> <b>under</b> the precision-recall <b>curve</b>. In terms of model selection, F-Measure summarizes model skill for a specific probability threshold (e.g. 0.5), whereas the <b>area</b> <b>under</b> <b>curve</b> summarize the skill of a model across thresholds, like ROC <b>AUC</b>.", "dateLastCrawled": "2022-02-03T04:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - <b>ROC</b> vs precision-and-recall curves - Cross Validated", "url": "https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/7207", "snippet": "Since domination means &quot;at least as high&quot; at every point, the higher <b>curve</b> also has &quot;at least as high&quot; an <b>Area</b> <b>under</b> the <b>Curve</b> (<b>AUC</b>) as it includes also the <b>area</b> between the curves. The reverse is not true: if curves intersect, as opposed to touch, there is no dominance, but one <b>AUC</b> <b>can</b> still be bigger than the other.", "dateLastCrawled": "2022-01-27T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - Advantages of <b>AUC</b> vs standard <b>accuracy</b> - Data ...", "url": "https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/806", "snippet": "Really great question, and one that I find that most people don&#39;t really understand on an intuitive level. <b>AUC</b> is in fact often preferred over <b>accuracy</b> for binary classification for a number of different reasons. First though, let&#39;s talk about exactly what <b>AUC</b> is. Honestly, for being one of the most widely used efficacy metrics, it&#39;s surprisingly obtuse to figure out exactly how <b>AUC</b> works.. <b>AUC</b> stands for <b>Area</b> <b>Under</b> the <b>Curve</b>, which <b>curve</b> you ask?Well, that would be the ROC <b>curve</b>.ROC stands ...", "dateLastCrawled": "2022-01-27T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Precision-recall</b> curves \u2013 what are they and how are they used?", "url": "https://acutecaretesting.org/en/articles/precision-recall-curves-what-are-they-and-how-are-they-used", "isFamilyFriendly": true, "displayUrl": "https://acutecaretesting.org/en/articles/<b>precision-recall</b>-<b>curves</b>-what-are-they-and-how...", "snippet": "Generally you <b>can</b> say that the closer a PRC is to the upper right corner, the better the test is. FIG. 4: ROC <b>curve</b> for a test with no overlap between persons with and without disease The perfect test will have a ROC <b>curve</b> that passes through the upper left corner (corresponding to 100 % sensitivity and 100 % specificity). Generally you <b>can</b> say ...", "dateLastCrawled": "2022-02-02T06:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Evaluate AutoML experiment results - <b>Azure</b> <b>Machine</b> <b>Learning</b> | Microsoft ...", "url": "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml", "isFamilyFriendly": true, "displayUrl": "https://docs.microsoft.com/en-us/<b>azure</b>/<b>machine</b>-<b>learning</b>/how-to-<b>under</b>stand-automated-ml", "snippet": "The <b>area</b> <b>under</b> the <b>curve</b> (<b>AUC</b>) <b>can</b> be interpreted as the proportion of correctly classified samples. More precisely, the <b>AUC</b> is the probability that the classifier ranks a randomly chosen positive sample higher than a randomly chosen negative sample. The shape of the <b>curve</b> gives an intuition for relationship between TPR and FPR as a function of the classification threshold or decision boundary.", "dateLastCrawled": "2022-02-02T18:17:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. See <b>PR</b> <b>AUC</b> (<b>Area</b> <b>under</b> <b>the PR</b> <b>Curve</b>). <b>area</b> <b>under</b> the ROC <b>curve</b>. See <b>AUC</b> (<b>Area</b> <b>under</b> the ROC <b>curve</b>). artificial general intelligence. A non-human mechanism that demonstrates a broad range of problem solving, creativity, and adaptability. For example, a program demonstrating artificial general intelligence could translate text, compose symphonies, and excel at games that have not yet been invented. artificial intelligence. A non-human program or model that can solve ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Understanding <b>AUC</b> - ROC <b>Curve</b> | by Sarang Narkhede | Towards Data Science", "url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>under</b>standing-<b>auc</b>-roc-<b>curve</b>-68b2303cc9c5", "snippet": "In <b>Machine</b> <b>Learning</b>, performance measurement is an essential task. So when it comes to a classification problem, we can count on an <b>AUC</b> - ROC <b>Curve</b>. When we need to check or visualize the performance of the multi-class classification problem, we use the <b>AUC</b> <b>Area</b> <b>Under</b> The <b>Curve</b>) ROC (Receiver Operating Characteristics) <b>curve</b>. It is one of the most important evaluation metrics for checking any classification model\u2019s performance. It is also written as AUROC (<b>Area</b> <b>Under</b> the Receiver Operating ...", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>What is</b> <b>AUC</b> - <b>ROC</b> in <b>Machine</b> <b>Learning</b> | Overview of <b>ROC</b>", "url": "https://www.mygreatlearning.com/blog/roc-curve/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/blog/<b>roc</b>-<b>curve</b>", "snippet": "The most widely-used measure is the <b>area</b> <b>under</b> the <b>curve</b> (<b>AUC</b>). As you can see from Figure 2, the <b>AUC</b> for a classifier with no power, essentially random guessing, is 0.5, because the <b>curve</b> follows the diagonal. The <b>AUC</b> for that mythical being, the perfect classifier, is 1.0. Most classifiers have AUCs that fall somewhere between these two values.", "dateLastCrawled": "2022-01-30T19:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Applying <b>machine</b> <b>learning</b> algorithms to predict default probability in ...", "url": "https://www.sciencedirect.com/science/article/pii/S1057521921002878", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1057521921002878", "snippet": "The results show that, first, based on the <b>AUC</b> (<b>area</b> <b>under</b> the ROC <b>curve</b>) value, accuracy rate and Brier score, the <b>machine</b> <b>learning</b> models can accurately predict the default risk of online borrowers. Second, the integrated discrimination improvement (IDI) test results show that the prediction performance of the <b>machine</b> <b>learning</b> algorithms is significantly better than that of the logistic model. Third, after constructing the investor profit function with misclassification cost, we find that ...", "dateLastCrawled": "2022-01-27T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Area</b> <b>under</b> Precision-Recall Curves for Weighted and Unweighted Data", "url": "https://www.researchgate.net/publication/260997871_Area_under_Precision-Recall_Curves_for_Weighted_and_Unweighted_Data", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/260997871", "snippet": "The precision-recall <b>Area</b> <b>Under</b> the <b>Curve</b> (<b>PR</b>-<b>AUC</b>) of our models was &gt; 84.5% for each diagnostic group as evaluated on the test-set \u2013 80/20 split. In conclusion, this study provides evidence for ...", "dateLastCrawled": "2022-01-31T06:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Learning</b> Curves in <b>Machine</b> <b>Learning</b> - ResearchGate", "url": "https://www.researchgate.net/publication/247934703_Learning_Curves_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/247934703_<b>Learning</b>_<b>Curves</b>_in_<b>Machine</b>_<b>Learning</b>", "snippet": "The <b>area</b> <b>under</b> the receiver operating characteristic (ROC) <b>curve</b> (<b>AUC</b>) was 0.62 (95% confidence interval [CI]: 0.57, 0.68) and the <b>area</b> <b>under</b> the precision\u2010recall <b>curve</b> was 0.58. <b>Learning</b> <b>curve</b> ...", "dateLastCrawled": "2021-12-15T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Protein function <b>in precision medicine: deep understanding with machine</b> ...", "url": "https://febs.onlinelibrary.wiley.com/doi/full/10.1002/1873-3468.12307", "isFamilyFriendly": true, "displayUrl": "https://febs.onlinelibrary.wiley.com/doi/full/10.1002/1873-3468.12307", "snippet": "Abbreviations. <b>AUC</b>, <b>area</b> <b>under</b> the ROC <b>curve</b>. COSMIC, Catalogue of Somatic Mutations in Cancer. HGMD, Human Gene Mutation Database. OMIA, Online Mammalian Inheritance in Animals. OMIM, Online Mammalian Inheritance in Man. ROC, receiver operating characteristic. To avoid problems with the next car you buy, you may consult the reliability statistics for every make and model that you are considering.", "dateLastCrawled": "2022-02-02T16:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Cohort-Derived <b>Machine Learning</b> Models for Individual Prediction of ...", "url": "https://academic.oup.com/jid/article/224/7/1198/5835004", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jid/article/224/7/1198/5835004", "snippet": "We used 64 static and 502 time-changing variables: Across prediction horizons and algorithms and in contrast to expert-based standard models, most <b>machine learning</b> models achieved state-of-the-art predictive performances with areas <b>under</b> the receiver operating characteristic <b>curve</b> and precision recall <b>curve</b> ranging from 0.926 to 0.996 and from 0.631 to 0.956, respectively.", "dateLastCrawled": "2021-12-15T15:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An application of FEA and <b>machine</b> <b>learning</b> for the prediction and ...", "url": "https://www.sciencedirect.com/science/article/pii/S1875510021004200", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1875510021004200", "snippet": "It will build a ROC <b>curve</b>, smooth it, if requested (if smooth = TRUE), compute the <b>area</b> <b>under</b> the <b>curve</b> <b>AUC</b> (if <b>auc</b> = TRUE), the confidence interval (CI) if requested (if ci = TRUE) and plot the <b>curve</b> if requested (if plot = TRUE). The mlbench library converts X (which is basically a list) to a data frame. Lastly, the ggplot2 library initializes a ggplot object. It can be used to declare the input data frame for a graphic and to specify the set of plot aesthetics intended to be common ...", "dateLastCrawled": "2022-01-29T00:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Why is <b>AUC</b> (<b>Area</b> <b>under</b> <b>ROC) insensitive to class distribution changes</b> ...", "url": "https://www.quora.com/Why-is-AUC-Area-under-ROC-insensitive-to-class-distribution-changes", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-is-<b>AUC</b>-<b>Area</b>-<b>under</b>-<b>ROC-insensitive-to-class-distribution-changes</b>", "snippet": "Answer (1 of 2): Well, the ROC <b>curve</b> is sensitive to the the class conditional likelihoods P(X|C_1) and P(X|C_2) because you are plotting P(miss) and P(FA) as a function of a decision threshold applied to the the likelihood ratio P(X|C_1)/P(X|C_2). The class prior probabilities P(C_1) and P(C_2)...", "dateLastCrawled": "2022-01-12T21:32:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(pr auc (area under the pr curve))  is like +(how good the machine learning algorithm is at making predictions)", "+(pr auc (area under the pr curve)) is similar to +(how good the machine learning algorithm is at making predictions)", "+(pr auc (area under the pr curve)) can be thought of as +(how good the machine learning algorithm is at making predictions)", "+(pr auc (area under the pr curve)) can be compared to +(how good the machine learning algorithm is at making predictions)", "machine learning +(pr auc (area under the pr curve) AND analogy)", "machine learning +(\"pr auc (area under the pr curve) is like\")", "machine learning +(\"pr auc (area under the pr curve) is similar\")", "machine learning +(\"just as pr auc (area under the pr curve)\")", "machine learning +(\"pr auc (area under the pr curve) can be thought of as\")", "machine learning +(\"pr auc (area under the pr curve) can be compared to\")"]}