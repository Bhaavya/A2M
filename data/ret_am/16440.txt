{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Dense-and-Sparse</b> - MIT", "url": "https://web.mit.edu/18.06/www/Spring17/Dense-and-Sparse.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>web.mit.edu</b>/18.06/www/Spring17/<b>Dense-and-Sparse</b>.pdf", "snippet": "<b>Dense-and-Sparse</b> September 7, 2017 In [1]:usingPyPlot, Interact 1 Large-scale linear algebra: <b>Dense</b> <b>matrix</b> methods The basic problem with most of the linear algebra techniques we have learned so far is that they scale badly for large matrices. Ordinary Gaussian elimination (LU factorization), Gram{Schmidt and other QR factorization algorithms, and techniques that computes all the eigenvalues and eigenvectors, all require \u02d8n3 operations and \u02d8n2 storage for n nmatrices. This all assumes that ...", "dateLastCrawled": "2022-01-21T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Working With <b>Sparse</b> Features In Machine Learning Models", "url": "https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html", "isFamilyFriendly": true, "displayUrl": "https://www.<b>kdnuggets</b>.com/2021/01/<b>sparse</b>-<b>features</b>-machine-learning-models.html", "snippet": "It is recommended that <b>sparse</b> features should be pre-processed by methods <b>like</b> <b>feature</b> hashing or removing the <b>feature</b> to reduce the negative impacts on the results. Bio: Arushi Prakash, Ph.D. , is an Applied Scientist at Amazon where she solves exciting science challenges in the field of workforce analytics.", "dateLastCrawled": "2022-02-02T20:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparse Matrices For Efficient Machine Learning</b> - Standard Deviations", "url": "https://dziganto.github.io/Sparse-Matrices-For-Efficient-Machine-Learning/", "isFamilyFriendly": true, "displayUrl": "https://dziganto.github.io/<b>Sparse-Matrices-For-Efficient-Machine-Learning</b>", "snippet": "Additionally, consider multiplying a <b>sparse</b> <b>matrix</b> by a <b>dense</b> <b>matrix</b>. Even though the <b>sparse</b> <b>matrix</b> has many zeros, and zero times anything is always zero, the standard approach requires this pointless operation nonetheless. The result is slowed processing time. It is much more efficient to operate only on elements that will return nonzero values. Therefore, any algorithm that applies some basic mathematical computation <b>like</b> multiplication can benefit from a <b>sparse</b> <b>matrix</b> implementation.", "dateLastCrawled": "2022-01-28T01:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>sparse matrix dense matrix multiplication</b> - Intel Community", "url": "https://community.intel.com/t5/Intel-oneAPI-Math-Kernel-Library/sparse-matrix-dense-matrix-multiplication/td-p/777564", "isFamilyFriendly": true, "displayUrl": "https://community.intel.com/t5/Intel-oneAPI-Math-Kernel-Library/<b>sparse</b>-<b>matrix</b>-<b>dense</b>...", "snippet": "08-29-2013 05:56 AM. It appears that there&#39;s an undocumented &quot;<b>feature</b>&quot; in the mkl_dcsrmm and mkl_scsrmm family of functions. If the <b>sparse</b> <b>matrix</b> uses zero-based indexing, MKL treates the <b>dense</b> matrices as row-major. If the <b>sparse</b> <b>matrix</b> uses one-based indexing MKL treats the desnce matrices as column-major.", "dateLastCrawled": "2020-11-24T01:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Pandas <b>sparse</b> dataFrame to <b>sparse</b> <b>matrix</b>, without generating a <b>dense</b> ...", "url": "https://stackoverflow.com/questions/31084942/pandas-sparse-dataframe-to-sparse-matrix-without-generating-a-dense-matrix-in-m", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31084942", "snippet": "<b>Sparse</b> methods are available through <b>sparse</b> accessor, so conversion one-liner now looks <b>like</b> this: <b>sparse</b>_<b>matrix</b> = scipy.<b>sparse</b>.csr_<b>matrix</b>(df.<b>sparse</b>.to_coo()) Share", "dateLastCrawled": "2022-01-20T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Sparse</b> x <b>Dense</b> -&gt; <b>Dense</b> <b>matrix</b> multiplication - PyTorch Forums", "url": "https://discuss.pytorch.org/t/sparse-x-dense-dense-matrix-multiplication/6116", "isFamilyFriendly": true, "displayUrl": "https://discuss.pytorch.org/t/<b>sparse</b>-x-<b>dense</b>-<b>dense</b>-<b>matrix</b>-multiplication/6116", "snippet": "For this I need to perform multiplication of the <b>dense</b> <b>feature</b> <b>matrix</b> X by a <b>sparse</b> adjacency <b>matrix</b> A (<b>sparse</b> x <b>dense</b> -&gt; <b>dense</b>). I don\u2019t need to compute the gradients with respect to the <b>sparse</b> <b>matrix</b> A. As mentioned in this thread, torch.mm should work in this case, however, I get the TypeError: Type torch.<b>sparse</b>.FloatTensor doesn&#39;t...", "dateLastCrawled": "2022-01-14T23:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Is a bag of words <b>feature</b> representation for text classification ...", "url": "https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html", "isFamilyFriendly": true, "displayUrl": "https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html", "snippet": "By definition, a <b>sparse</b> <b>matrix</b> is called \u201c<b>sparse</b>\u201d if most of its elements are zero. In the bag of words model, each document is represented as a word-count vector. These counts can be binary counts (does a word occur or not) or absolute counts (term frequencies, or normalized counts), and the size of this vector is equal to the number of elements in your vocabulary. Thus, if most of your <b>feature</b> vectors are <b>sparse</b>, our bag-of-words <b>feature</b> <b>matrix</b> is most likely <b>sparse</b> as well!", "dateLastCrawled": "2022-02-03T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "python - <b>Sparse</b> Tensor (<b>matrix</b>) from a <b>dense Tensor</b> Tensorflow - Stack ...", "url": "https://stackoverflow.com/questions/39838234/sparse-tensor-matrix-from-a-dense-tensor-tensorflow", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/39838234", "snippet": "I am creating a convolutional <b>sparse</b> autoencoder and I need to convert a 4D <b>matrix</b> full of values (whose shape is [samples, N, N, D]) into a <b>sparse</b> <b>matrix</b>. For each sample, I have D NxN <b>feature</b> maps. I want to convert each NxN <b>feature</b> map to a <b>sparse</b> <b>matrix</b>, with the maximum value mapped to 1 and all the others to 0.", "dateLastCrawled": "2022-01-25T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "ENH: Avoid <b>dense</b> representation of <b>sparse</b> matrices \u00b7 Issue #15409 ...", "url": "https://github.com/scipy/scipy/issues/15409", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/scipy/scipy/issues/15409", "snippet": "Is your <b>feature</b> request related to a problem? Please describe. When converting <b>sparse</b> <b>matrix</b> representation there is a <b>dense</b> representation of the underlying data, see the following: import scipy.<b>sparse</b> as ss data = [1] i = [108992455309...", "dateLastCrawled": "2022-01-16T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Unifying <b>dense</b> and <b>sparse</b> features for neural networks - Engineering at ...", "url": "https://quoraengineering.quora.com/Unifying-dense-and-sparse-features-for-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://quoraengineering.quora.com/Unifying-<b>dense</b>-and-<b>sparse</b>-<b>features</b>-for-neural-networks", "snippet": "As each <b>dense</b> <b>feature</b> value can only belong to one bin and the total number of bins for a <b>dense</b> <b>feature</b> is typically large, the set of IDs generated from all <b>dense</b> features for an instance can be represented by a <b>sparse</b> binary vector. If IDs of <b>dense</b> and <b>sparse</b> features are included in a global ID dictionary, these IDs can be used to look up embeddings from a global embedding <b>matrix</b> which allows for <b>dense</b> and <b>sparse</b> features to be processed in a unified way. The embeddings of both <b>dense</b> and ...", "dateLastCrawled": "2022-01-19T05:18:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Sparse Matrices for Machine Learning quick note</b> - Petamind", "url": "https://petamind.com/sparse-matrices-for-machine-learning-quick-note/", "isFamilyFriendly": true, "displayUrl": "https://petamind.com/<b>sparse-matrices-for-machine-learning-quick-note</b>", "snippet": "<b>Sparse</b> vs <b>Dense</b> <b>Matrix</b>. First, it is good to know that <b>sparse</b> <b>matrix</b> looks <b>similar</b> to a normal <b>matrix</b>, with rows, columns or other indexes. But a <b>sparse</b> <b>matrix</b> is comprised of mostly zero 0s) values. They are distinct from <b>dense</b> matrices with mostly non-zero values. A <b>matrix</b> is <b>sparse</b> if many of its coefficients are zero. The interest in sparsity arises because its exploitation can lead to enormous computational savings and because many large <b>matrix</b> problems that occur in practice are <b>sparse</b> ...", "dateLastCrawled": "2022-01-22T00:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Quick Guide to Operations on <b>Sparse</b> Matrices | by Riccardo Di Sipio ...", "url": "https://medium.com/codex/a-quick-guide-to-operations-on-sparse-matrices-2f8776fab265", "isFamilyFriendly": true, "displayUrl": "https://medium.com/codex/a-quick-guide-to-operations-on-<b>sparse</b>-matrices-2f8776fab265", "snippet": "A(<b>sparse</b>) x B(<b>dense</b>), A(<b>sparse</b>) x B(vector), A(<b>sparse</b>) x B(diagonal), etc. Conclusions. <b>Sparse matrix</b> operations are ubiquitous, but things mostly happen under the trunk. Especially when it comes ...", "dateLastCrawled": "2022-01-01T22:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Dense-and-Sparse</b> - MIT", "url": "https://web.mit.edu/18.06/www/Spring17/Dense-and-Sparse.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>web.mit.edu</b>/18.06/www/Spring17/<b>Dense-and-Sparse</b>.pdf", "snippet": "<b>Dense-and-Sparse</b> September 7, 2017 In [1]:usingPyPlot, Interact 1 Large-scale linear algebra: <b>Dense</b> <b>matrix</b> methods The basic problem with most of the linear algebra techniques we have learned so far is that they scale badly for large matrices. Ordinary Gaussian elimination (LU factorization), Gram{Schmidt and other QR factorization algorithms, and techniques that computes all the eigenvalues and eigenvectors, all require \u02d8n3 operations and \u02d8n2 storage for n nmatrices. This all assumes that ...", "dateLastCrawled": "2022-01-21T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Unifying <b>dense</b> and <b>sparse</b> features for neural networks - Engineering at ...", "url": "https://quoraengineering.quora.com/Unifying-dense-and-sparse-features-for-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://quoraengineering.quora.com/Unifying-<b>dense</b>-and-<b>sparse</b>-<b>features</b>-for-neural-networks", "snippet": "As each <b>dense</b> <b>feature</b> value can only belong to one bin and the total number of bins for a <b>dense</b> <b>feature</b> is typically large, the set of IDs generated from all <b>dense</b> features for an instance can be represented by a <b>sparse</b> binary vector. If IDs of <b>dense</b> and <b>sparse</b> features are included in a global ID dictionary, these IDs can be used to look up embeddings from a global embedding <b>matrix</b> which allows for <b>dense</b> and <b>sparse</b> features to be processed in a unified way. The embeddings of both <b>dense</b> and ...", "dateLastCrawled": "2022-01-19T05:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Is a bag of words <b>feature</b> representation for text classification ...", "url": "https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html", "isFamilyFriendly": true, "displayUrl": "https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html", "snippet": "By definition, a <b>sparse</b> <b>matrix</b> is called \u201c<b>sparse</b>\u201d if most of its elements are zero. In the bag of words model, each document is represented as a word-count vector. These counts can be binary counts (does a word occur or not) or absolute counts (term frequencies, or normalized counts), and the size of this vector is equal to the number of elements in your vocabulary. Thus, if most of your <b>feature</b> vectors are <b>sparse</b>, our bag-of-words <b>feature</b> <b>matrix</b> is most likely <b>sparse</b> as well!", "dateLastCrawled": "2022-02-03T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "The difference between direct <b>dense</b> method and <b>feature</b>-based <b>sparse</b> ...", "url": "https://www.researchgate.net/figure/The-difference-between-direct-dense-method-and-feature-based-sparse-method-Feature-based_fig4_224252357", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/The-difference-between-direct-<b>dense</b>-method-and...", "snippet": "The difference between direct <b>dense</b> method and <b>feature</b>-based <b>sparse</b> method. <b>Feature</b>-based method has intrinsic bias with <b>feature</b> extraction errors \u2206e f as well as correspondence model errors \u2206em.", "dateLastCrawled": "2021-11-13T00:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "scikit OneClassSvm <b>sparse</b> <b>matrix</b> return (very) different result than <b>dense</b>", "url": "https://stackoverflow.com/questions/31856501/scikit-oneclasssvm-sparse-matrix-return-very-different-result-than-dense", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31856501", "snippet": "I&#39;m using a OneClassSVM classifier with <b>dense</b> <b>matrix</b>, the results are pretty good. I&#39;d like to include some texts in my features and use a <b>sparse</b> <b>matrix</b>, however I get really differents (and wrong) result while using a <b>sparse</b> <b>matrix</b>, I don&#39;t understand why. Here is an example :", "dateLastCrawled": "2022-01-12T19:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Data Types - RDD-based API - <b>Spark</b> 3.2.1 Documentation", "url": "https://spark.apache.org/docs/latest/mllib-data-types.html", "isFamilyFriendly": true, "displayUrl": "https://<b>spark</b>.apache.org/docs/latest/mllib-data-types.html", "snippet": "MLlib supports two types of local vectors: <b>dense</b> and <b>sparse</b>. A <b>dense</b> vector is backed by a double array representing its entry values, while a <b>sparse</b> vector is backed by two parallel arrays: indices and values. For example, a vector (1.0, 0.0, 3.0) can be represented in <b>dense</b> format as [1.0, 0.0, 3.0] or in <b>sparse</b> format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector. The base class of local vectors is Vector, and we provide two implementations: DenseVector and SparseVector ...", "dateLastCrawled": "2022-01-30T16:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Convert <b>dense</b> <b>matrix</b> to <b>sparse</b> <b>matrix</b> \u00b7 Issue #274 \u00b7 OpenMined/PySyft ...", "url": "https://github.com/OpenMined/PySyft/issues/274", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/OpenMined/PySyft/issues/274", "snippet": "Convert <b>dense</b> <b>matrix</b> to <b>sparse</b> ( OpenMined#274) 7b6bf6d. * fix tests by converting output of <b>sparse</b> function to string This was necessary because assert.equal returning errors when trying to compare two numpy arrays. ivuckovic added a commit to ivuckovic/PySyft that referenced this issue on Oct 30, 2017.", "dateLastCrawled": "2021-11-11T18:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - Split a <b>sparse</b> <b>matrix</b> into chunks without converting <b>to dense</b> ...", "url": "https://stackoverflow.com/questions/55049275/split-a-sparse-matrix-into-chunks-without-converting-to-dense", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/55049275/split-a-<b>sparse</b>-<b>matrix</b>-into-chunks-without...", "snippet": "Here an example of a conversion for a small <b>sparse</b> <b>matrix</b> <b>to dense</b>: # <b>sparse</b> (0, 0) -0.5 (0, 1) 3.8570557155110414 (0, 2) -1.975755301731886 (1, 0) -3.5 (1, 1) 6.54336961554629 (1, 2) -3.311314222363026 # <b>dense</b> [[-0.5 3.85705572 -1.9757553 ] [-3.5 6.54336962 -3.31131422]] Each of those two inner arrays in the <b>dense</b> <b>matrix</b> is a <b>Feature</b>-Vector representing one object. Basically, assuming we would try to classify those by doing the chunk split technique with conversion on this example, we would ...", "dateLastCrawled": "2022-01-26T08:12:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Dense-and-Sparse</b> - MIT", "url": "https://web.mit.edu/18.06/www/Spring17/Dense-and-Sparse.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>web.mit.edu</b>/18.06/www/Spring17/<b>Dense-and-Sparse</b>.pdf", "snippet": "<b>Dense-and-Sparse</b> September 7, 2017 In [1]:usingPyPlot, Interact 1 Large-scale linear algebra: <b>Dense</b> <b>matrix</b> methods The basic problem with most of the linear algebra techniques we have learned so far is that they scale badly for large matrices. Ordinary Gaussian elimination (LU factorization), Gram{Schmidt and other QR factorization algorithms, and techniques that computes all the eigenvalues and eigenvectors, all require \u02d8n3 operations and \u02d8n2 storage for n nmatrices. This all assumes that ...", "dateLastCrawled": "2022-01-21T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Gentle Introduction to <b>Sparse</b> Matrices for Machine Learning", "url": "https://machinelearningmastery.com/sparse-matrices-for-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>sparse</b>-matrices-for-machine-learning", "snippet": "A <b>dense</b> <b>matrix</b> stored in a NumPy array <b>can</b> be converted into a <b>sparse</b> <b>matrix</b> using the CSR representation by calling the csr_<b>matrix</b>() function. In the example below, we define a 3 x 6 <b>sparse</b> <b>matrix</b> as a <b>dense</b> array, convert it to a CSR <b>sparse</b> representation, and then convert it back to a <b>dense</b> array by calling the todense() function.", "dateLastCrawled": "2022-02-02T09:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Sparse</b> <b>Matrix</b> and its representations | Set 1 (Using Arrays and Linked ...", "url": "https://www.geeksforgeeks.org/sparse-matrix-representation/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>sparse</b>-<b>matrix</b>-representation", "snippet": "A <b>matrix</b> is a two-dimensional data object made of m rows and n columns, therefore having total m x n values. If most of the elements of the <b>matrix</b> have 0 value, then it is called a <b>sparse</b> <b>matrix</b>.. Why to use <b>Sparse</b> <b>Matrix</b> instead of simple <b>matrix</b> ? Storage: There are lesser non-zero elements than zeros and thus lesser memory <b>can</b> be used to store only those elements. Computing time: Computing time <b>can</b> be saved by logically designing a data structure traversing only non-zero elements.. Example ...", "dateLastCrawled": "2022-01-31T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Understanding compressed matrices for sparse</b> data \u2013 Datastronomy", "url": "https://datastronomy.com/understanding-sparse-matrices-for-recommender-systems/", "isFamilyFriendly": true, "displayUrl": "https://datastronomy.com/understanding-<b>sparse</b>-matrices-for-recommender-systems", "snippet": "A <b>matrix</b> <b>can</b> be uncompressed but <b>sparse</b>, as well as it <b>can</b> be compressed but <b>dense</b>, though these representations are both suboptimal. Compression vs. Dimensionality Reduction. The astute reader may have noticed that dimensionality reduction algorithms such as PCA and SVD also compress their inputs. This observation is correct, but it misses an important distinction: dimensionality reduction is lossy, while compression as this article defines it is lossless. Lossy vs. lossless compression ...", "dateLastCrawled": "2022-01-21T07:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Feature Scaling with Python and Sparse</b> Data \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2020/11/23/feature-scaling-with-python-and-sparse-data/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/.../2020/11/23/<b>feature-scaling-with-python-and-sparse</b>-data", "snippet": "Fortunately, there is a way in which <b>Feature</b> Scaling <b>can</b> be applied to <b>Sparse</b> Data. We <b>can</b> do so using Scikit-learn\u2019s MaxAbsScaler. Scale each <b>feature</b> by its maximum absolute value. This estimator scales and translates each <b>feature</b> individually such that the maximal absolute value of each <b>feature</b> in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity. Scikit-learn (n.d.) As we <b>can</b> see, it uses the maximum absolute value to perform the ...", "dateLastCrawled": "2022-01-28T00:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Optimising algorithms in Go for machine learning - Part 2: <b>Sparse</b> ...", "url": "http://www.jamesbowman.me/post/optimising-machine-learning-algorithms-part2/", "isFamilyFriendly": true, "displayUrl": "www.<b>jamesbowman</b>.me/post/optimising-machine-learning-algorithms-part2", "snippet": "<b>Sparse</b> <b>matrix</b> data structures <b>can</b> effectively be divided into 3 main categories: ... CSC <b>can</b> <b>be thought</b> of as a natural transpose of CSR. 3. Specialised Formats DIA (DIAgonal) format. DIA is a specialised format for storing symmetric diagonal matrices. Symmetric diagonal matrices are square shaped and so have the same number of rows and columns, with only the elements along the diagonal (top left to bottom right) containing non-zero values. The DIA format takes advantage of this fact by only ...", "dateLastCrawled": "2021-08-09T21:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Python | Pandas DataFrame.to_sparse</b> - <b>GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/python-pandas-dataframe-to_sparse/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>python-pandas-dataframe-to_sparse</b>", "snippet": "It <b>can</b> <b>be thought</b> of as a dict-like container for Series objects. This is the primary data structure of the Pandas. Pandas DataFrame.to_<b>sparse</b>() function convert to SparseDataFrame. The function implement the <b>sparse</b> version of the DataFrame meaning that any data matching a specific value it\u2019s omitted in the representation. The <b>sparse</b> ...", "dateLastCrawled": "2022-02-03T07:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "spektral Order of edge features in &amp;quot;<b>sparse</b>&amp;quot; mode Python", "url": "https://gitanswer.com/spektral-order-of-edge-features-in-sparse-mode-python-953388551", "isFamilyFriendly": true, "displayUrl": "https://gitanswer.com/spektral-order-of-edge-<b>features</b>-in-<b>sparse</b>-mode-python-953388551", "snippet": "The wording of the documentation <b>can</b> be confusing. Edge features <b>can</b> indeed be in <b>sparse</b> format, but that doesn&#39;t mean that they are scipy.<b>sparse</b> matrices. With &quot;<b>sparse</b> format&quot; we indicate the fact that the edge attributes are not tensors of shape (n_nodes, n_nodes, n_features), but instead they are stored in a <b>matrix</b> of shape (n_edges, n_features).This is &quot;<b>sparse</b>&quot;, because it lets you keep in memory only the edge features associated with nonzero edges.", "dateLastCrawled": "2022-02-03T17:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Sparse</b> torch.topk: <b>can</b> hybrid <b>sparse</b>+<b>dense</b> tensors help? - PyTorch Forums", "url": "https://discuss.pytorch.org/t/sparse-torch-topk-can-hybrid-sparse-dense-tensors-help/71832", "isFamilyFriendly": true, "displayUrl": "https://discuss.pytorch.org/t/<b>sparse</b>-torch-topk-<b>can</b>-hybrid-<b>sparse</b>-<b>dense</b>-tensors-help/71832", "snippet": "I\u2019d like to keep keep in the tensor only K largest elements in each row (corresponding to logits/logprobs of K highest scoring classes) to optimize disk space during serialization. I\u2019d like to get a <b>sparse</b> tensor as an output. Is there an simpler way than this (e.g. directly passing indices from topk to torch.<b>sparse</b>.FloatTensor constructor) ? x = torch.rand(128, 512, 40) # 128 and 512 are batch dimension, 40 is class logits dimension def <b>sparse</b>_topk(x, K, dim = -1): return torch.zeros ...", "dateLastCrawled": "2022-01-26T07:36:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "tensorflow - <b>Encoding a scipy sparse matrix as TFRecords</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/40154878/encoding-a-scipy-sparse-matrix-as-tfrecords", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/40154878", "snippet": "I am building a tensorflow model where the input data is a big scipy <b>sparse</b> <b>matrix</b>, each row being a sample of dimension &gt;50k where only few hundreds values are non-zero. Currently, I store this <b>matrix</b> as a pickle, then load it fully into memory, batch it and converting the samples in the batch to a <b>dense</b> numpy array that I feed into the model ...", "dateLastCrawled": "2022-01-24T13:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Dense</b> <b>versus Sparse Approaches for Estimating the Fundamental Matrix</b>", "url": "https://www.researchgate.net/publication/220659629_Dense_versus_Sparse_Approaches_for_Estimating_the_Fundamental_Matrix", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220659629_<b>Dense</b>_versus_<b>Sparse</b>_Approaches_for...", "snippet": "While <b>sparse</b> <b>feature</b> based methods are often used <b>for estimating the fundamental matrix</b> by matching a small set of sophistically optimised interest points, <b>dense</b> energy based methods mark the ...", "dateLastCrawled": "2021-11-11T03:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "What is the <b>difference between</b> a Categorical Column and a <b>Dense</b> Column?", "url": "https://datascience.stackexchange.com/questions/35826/what-is-the-difference-between-a-categorical-column-and-a-dense-column", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/35826", "snippet": "<b>Sparse</b> vs <b>Dense</b>. The &#39;categorical column&#39; is a <b>sparse</b> column. <b>Sparse</b> and <b>dense</b> columns (or matrices) are in a way opposites of each other. <b>Sparse</b> columns usually contain a lot of zeros. Whereas, <b>dense</b> columns have more non-zero entries. This matters, because the way they are stored and processed <b>can</b> differ. <b>Sparse</b>. If we take your <b>sparse</b> example:", "dateLastCrawled": "2022-02-02T20:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Reconfigurable <b>Sparse</b>/<b>Dense</b> <b>Matrix</b>-Vector Multiplier", "url": "https://www.researchgate.net/profile/Georgi-Kuzmanov-2/publication/224098175_Reconfigurable_sparsedense_matrix-vector_multiplier/links/55aabb1408ae481aa7fbc9d0/Reconfigurable-sparse-dense-matrix-vector-multiplier.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/.../Reconfigurable-<b>sparse</b>-<b>dense</b>-<b>matrix</b>-vector-multiplier.pdf", "snippet": "\ufb02oating-point <b>matrix</b>-vector multiplier. Its main <b>feature</b> is the capability to process ef\ufb01ciently both <b>Dense</b> <b>Matrix</b>-Vector Mul-tiplications (DMVM) and <b>Sparse</b> <b>Matrix</b>-Vector Multiplications (SMVM ...", "dateLastCrawled": "2022-01-24T19:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "InteractionNN: A Neural Network for Learning Hidden Features in <b>Sparse</b> ...", "url": "https://www.ijcai.org/Proceedings/2019/0602.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/2019/0602.pdf", "snippet": "sic <b>dense</b> features. <b>Compared</b> to those models extracting only high-level <b>feature</b> interactions, InteractionNN <b>can</b> also extract low-level <b>feature</b> interactions which is useful to characterize the semantics of <b>sparse</b> data. Hierarchical representation model (HRM)[Heet al., 2016] and neural network-based collaborative \ufb01ltering (NCF)[Liu et al., 2018] capture the <b>feature</b> interactions via a simply aver-age of embedding vectors. DeepFM[Guoet al., 2017] com-bines FM and deep learning, which <b>can</b> ...", "dateLastCrawled": "2022-01-22T11:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Pandas <b>sparse</b> dataFrame to <b>sparse</b> <b>matrix</b>, without generating a <b>dense</b> ...", "url": "https://stackoverflow.com/questions/31084942/pandas-sparse-dataframe-to-sparse-matrix-without-generating-a-dense-matrix-in-m", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/31084942", "snippet": "Since a <b>sparse</b> <b>matrix</b> is inherently 2d, it makes sense to require multiindex for the (effectively) 1d dataseries. While the dataframe <b>can</b> represent a table or 2d array. While the dataframe <b>can</b> represent a table or 2d array.", "dateLastCrawled": "2022-01-20T10:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Working with <b>sparse</b> data sets in pandas and sklearn | by Dafni ...", "url": "https://towardsdatascience.com/working-with-sparse-data-sets-in-pandas-and-sklearn-d26c1cfbe067", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/working-with-<b>sparse</b>-data-sets-in-pandas-and-sklearn-d26...", "snippet": "A <b>matrix</b> with sparsity greater than 0.5 is a <b>sparse</b> <b>matrix</b>. Handling a <b>sparse</b> <b>matrix</b> as a <b>dense</b> one is frequently inefficient, making excessive use of memory. When working with <b>sparse</b> matrices it is recommended to use dedicated data structures for efficient storage and processing. We will refer to some of the available structures in Python in the next sections. Frequently, we start from a <b>dense</b> data set that includes categorical variables. Typically, we have to apply one-hot encoding for ...", "dateLastCrawled": "2022-02-03T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Sparse</b> <b>Matrix</b> and its representations | Set 1 (Using Arrays and Linked ...", "url": "https://www.geeksforgeeks.org/sparse-matrix-representation/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/<b>sparse</b>-<b>matrix</b>-representation", "snippet": "A <b>matrix</b> is a two-dimensional data object made of m rows and n columns, therefore having total m x n values. If most of the elements of the <b>matrix</b> have 0 value, then it is called a <b>sparse</b> <b>matrix</b>.. Why to use <b>Sparse</b> <b>Matrix</b> instead of simple <b>matrix</b> ? Storage: There are lesser non-zero elements than zeros and thus lesser memory <b>can</b> be used to store only those elements. Computing time: Computing time <b>can</b> be saved by logically designing a data structure traversing only non-zero elements.. Example ...", "dateLastCrawled": "2022-01-31T00:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "What&#39;s the <b>difference between a sparse reward</b> and a <b>dense</b> reward in ...", "url": "https://www.quora.com/Whats-the-difference-between-a-sparse-reward-and-a-dense-reward-in-reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Whats-the-<b>difference-between-a-sparse-reward</b>-and-a-<b>dense</b>-reward...", "snippet": "Answer: We have to distinguish the reward function from the instantaneous reward. The reward function is associated with the environment, which for Markov decision processes is typically a function that maps the current state, current action, and future state to a real value. This reward functio...", "dateLastCrawled": "2022-01-21T02:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Feature Scaling with Python and Sparse</b> Data \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2020/11/23/feature-scaling-with-python-and-sparse-data/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/.../2020/11/23/<b>feature-scaling-with-python-and-sparse</b>-data", "snippet": "<b>Feature Scaling with Python and Sparse</b> Data. When you are training a Supervised Machine Learning model, scaling your data before you start fitting the model <b>can</b> be a crucial step for training success. In fact, without doing so, there are cases when the model\u2019s loss function will behave very strangely. However, not every dataset is made equal.", "dateLastCrawled": "2022-01-28T00:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - How to make TF-IDF <b>matrix</b> <b>dense</b>? - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/35109424/how-to-make-tf-idf-matrix-dense", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/35109424", "snippet": "How do I convert this <b>matrix</b> to a <b>dense</b> one (so that every row has the same number of columns)? &gt;print type(X) &lt;class &#39;scipy.<b>sparse</b>.csr.csr_<b>matrix</b>&#39;&gt; python scikit-learn cluster-analysis <b>sparse</b>-<b>matrix</b> tf-idf. Share. Follow edited Jan 31 &#39;16 at 2:29. Will. 22.3k 12 12 gold badges 90 90 silver badges 101 101 bronze badges. asked Jan 31 &#39;16 at 1:44. gsamaras gsamaras. 68.5k 37 37 gold badges 160 160 silver badges 270 270 bronze badges. 2. <b>Can</b> you print type(X)? \u2013 Will. Jan 31 &#39;16 at 2:20. With ...", "dateLastCrawled": "2022-01-26T22:48:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An E\ufb03cient <b>Sparse</b> Metric <b>Learning</b> in High ... - <b>Machine</b> <b>Learning</b>", "url": "http://machinelearning.org/archive/icml2009/papers/46.pdf", "isFamilyFriendly": true, "displayUrl": "<b>machinelearning</b>.org/archive/icml2009/papers/46.pdf", "snippet": "An E\ufb03cient <b>Sparse</b> Metric <b>Learning</b> in High-Dimensional Space via!1-Penalized Log-Determinant Regularization Guo-Jun Qi qi4@illinois.edu Depart. ECE, University of Illinois at Urbana-Champaign, 405 North Mathews Avenue, Urbana, IL 61801 USA Jinhui Tang, Zheng-Jun Zha, Tat-Seng Chua {tangjh, zhazj, chuats}@comp.nus.edu.sg School of Computing, National University of Singapore, Computing 1, 13 Computing Drive, Singapore 117417 Hong-Jiang Zhang hjzhang@microsoft.com Microsoft Advanced Technology ...", "dateLastCrawled": "2021-11-19T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Lecture 4: \\(k\\)-Nearest Neighbours and SVM RBFs \u2014 CPSC 330 Applied ...", "url": "https://ubc-cs.github.io/cpsc330/lectures/04_kNNs-SVM-RBF.html", "isFamilyFriendly": true, "displayUrl": "https://ubc-cs.github.io/cpsc330/lectures/04_kNNs-SVM-RBF.html", "snippet": "<b>Analogy</b>-based models ... It does not work well on datasets with many features or where most <b>feature</b> values are 0 most of the time (<b>sparse</b> datasets). Attention. For regular \\(k\\) -NN for supervised <b>learning</b> (not with <b>sparse</b> matrices), you should scale your features. We\u2019ll be looking into it soon. Parametric vs non parametric\u00b6 You might see a lot of definitions of these terms. A simple way to think about this is: do you need to store at least \\(O(n)\\) worth of stuff to make predictions? If ...", "dateLastCrawled": "2022-01-11T11:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Handling IP addresses as features when creating <b>machine</b> <b>learning</b> model ...", "url": "https://datascience.stackexchange.com/questions/106454/handling-ip-addresses-as-features-when-creating-machine-learning-model", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/106454/handling-ip-addresses-as...", "snippet": "With info spread across 70,000 features, it can drown out all other features and/or makes it hard to learn anything about individual IPs. And obviously there are billions of potential IP addresses. Treating octets as numbers is not meaningful. There is no ordinal meaning to them; 46.* is not closer to 47.* than 250.*.", "dateLastCrawled": "2022-01-29T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Introduction to Matrices and Matrix Arithmetic for <b>Machine Learning</b>", "url": "https://machinelearningmastery.com/introduction-matrices-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/introduction-matrices-<b>machine-learning</b>", "snippet": "The geometric <b>analogy</b> used to help understand vectors and some of their operations does not hold with matrices. Further, a vector itself may be considered a matrix with one column and multiple rows. Often the dimensions of the matrix are denoted as m and n for the number of rows and the number of columns. Now that we know what a matrix is, let\u2019s look at defining one in Python. Defining a Matrix. We can represent a matrix in Python using a two-dimensional NumPy array. A NumPy array can be ...", "dateLastCrawled": "2022-02-02T11:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning :: Cosine Similarity for Vector</b> Space Models (Part III ...", "url": "https://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/", "isFamilyFriendly": true, "displayUrl": "https://blog.christianperone.com/2013/09/<b>machine</b>-<b>learning</b>-", "snippet": "The tfidf_matrix[0:1] is the Scipy operation to get the first row of the <b>sparse</b> matrix and the resulting array is the Cosine Similarity between the first document with all documents in the set. Note that the first value of the array is 1.0 because it is the Cosine Similarity between the first document with itself. Also note that due to the presence of similar words on the third document (\u201cThe sun in the sky is bright\u201d), it achieved a better score. If you want, you can also solve the ...", "dateLastCrawled": "2022-01-29T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine learning MCQs</b> | T4Tutorials.com", "url": "https://t4tutorials.com/machine-learning-mcqs/", "isFamilyFriendly": true, "displayUrl": "https://t4tutorials.com/<b>machine-learning-mcqs</b>", "snippet": "<b>Machine learning MCQs</b>. 1. The general concept and process of forming definitions from examples of concepts to be learned. E. All of these. F. None of these. 2. The computer is the best <b>learning</b> for.", "dateLastCrawled": "2022-01-30T16:47:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sparse feature)  is like +(dense matrix)", "+(sparse feature) is similar to +(dense matrix)", "+(sparse feature) can be thought of as +(dense matrix)", "+(sparse feature) can be compared to +(dense matrix)", "machine learning +(sparse feature AND analogy)", "machine learning +(\"sparse feature is like\")", "machine learning +(\"sparse feature is similar\")", "machine learning +(\"just as sparse feature\")", "machine learning +(\"sparse feature can be thought of as\")", "machine learning +(\"sparse feature can be compared to\")"]}