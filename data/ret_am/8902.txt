{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias</b> in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/<b>bias</b>-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "These biases include sample <b>bias</b>, reporting <b>bias</b>, prejudice <b>bias</b>, confirmation <b>bias</b>, <b>group</b> <b>attribution</b> <b>bias</b>, <b>algorithm</b> <b>bias</b>, measurement <b>bias</b>, recall <b>bias</b>, exclusion <b>bias</b>, and automation <b>bias</b>. <b>Machine</b> <b>learning</b> is highly susceptible to many forms of <b>bias</b> that can undermine model performance. After all, AI is assembled by humans, and humans are ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "AI <b>Bias</b> Example, Meaning And How To Avoid Them - Kotai Electronics Pvt ...", "url": "https://kotaielectronics.com/ai-bias-example-meaning-and-how-to-avoid-them/", "isFamilyFriendly": true, "displayUrl": "https://kotaielectronics.com/ai-<b>bias</b>-example-meaning-and-how-to-avoid-them", "snippet": "<b>Group</b> <b>attribution</b> AI <b>bias</b> meaning, The <b>Bias</b> that takes place when the <b>algorithm</b> puts more weight onto an individual it happens because the <b>algorithm</b> classifies the data and extrapolates a certain set of data from the rest of the data set. Here the AI <b>bias</b> example can be a tool that is used for admission and recruiting people, Here the system can put more importance on students who graduate from certain universities over others.", "dateLastCrawled": "2022-01-18T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Welcome To The <b>Machine</b> <b>Learning</b> Biases That Still Exist In 2019", "url": "https://analyticsindiamag.com/welcome-to-the-machine-learning-biases-that-still-exist-in-2019/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/welcome-to-the-<b>machine</b>-<b>learning</b>-<b>bias</b>es-that-still-exist...", "snippet": "5.<b>Group</b> <b>attribution</b> <b>Bias</b>: This <b>bias</b> assumes a particular attribute to the entire <b>group</b>. For example, if the majority of women are in the designing industry and a majority of men in the hardware, it tends to assume the respective professions for both. 4.<b>Algorithm</b> <b>Bias</b>: In <b>machine</b> <b>learning</b>, <b>bias</b> is a mathematical property of an <b>algorithm</b>. The ...", "dateLastCrawled": "2022-01-08T19:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "HUMAN <b>BIAS</b> THAT CAN RESULT INTO ML BIASES | by CoffeeBeans Consulting ...", "url": "https://coffeebeansconsulting.medium.com/human-bias-that-can-result-into-ml-biases-b1ea9f6d2767?source=post_internal_links---------6----------------------------", "isFamilyFriendly": true, "displayUrl": "https://coffeebeansconsulting.medium.com/human-<b>bias</b>-that-can-result-into-ml-<b>bias</b>es-b1...", "snippet": "<b>Group</b> <b>attribution</b> <b>Bias</b>: This <b>bias</b> assumes a particular attribute to the entire <b>group</b>. For example, if the majority of women are in the designing industry and a majority of men in the hardware, it tends to assume the respective professions for both. 6. <b>Algorithm</b> <b>Bias</b>: In <b>machine</b> <b>learning</b>, <b>bias</b> is a mathematical property of an <b>algorithm</b>. The counterpart to <b>bias</b> in this context is variance. ML algorithms with a high value of variance can easily fit into training data and welcome complexity but ...", "dateLastCrawled": "2022-01-25T05:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Biases in <b>machine</b> <b>learning</b> models and big data analytics: The ...", "url": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/biases-in-machine-learning-models-and-big-data-analytics-the-international-criminal-and-humanitarian-law-implications/86BEAC9ADD165C90B2931AB2B665FFDF", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/...", "snippet": "<b>Group</b> <b>attribution</b> <b>bias</b> is a tendency to impute what is true of a few individuals to an entire <b>group</b> to which they belong. For instance, imagine that an ML model is created to identify the most suitable candidates for a position with the OTP. In creating this model, the designers assume that the \u201cbest\u201d candidates are individuals with a doctorate degree from a Western European university and internship experience with the ICC, purely because some successful employees possess those traits ...", "dateLastCrawled": "2021-12-21T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> Fairness: Types of <b>Bias</b> | by Svs Nagesh | Medium", "url": "https://nageshsomayajula.medium.com/machine-learning-fairness-types-of-bias-82bcf3df2d47", "isFamilyFriendly": true, "displayUrl": "https://nageshsomayajula.medium.com/<b>machine</b>-<b>learning</b>-fairness-types-of-<b>bias</b>-82bcf3df2d47", "snippet": "AI a p plications are <b>like</b> a small kid, we must train with the right data otherwise they can be misguided, and correcting machines or AI applications will be big challenging, for kids also (pun intended). The AI systems themselves will construct models that will explain how it works and follow anti-<b>bias</b> rules. In the <b>machine</b>, <b>learning</b> <b>bias</b> is one of the most common problems and every <b>algorithm</b> falls trap on this various kind of <b>bias</b>, let\u2019s discuss in detail various types of <b>bias</b> and how to ...", "dateLastCrawled": "2022-01-30T12:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What Is Model <b>Bias</b> In <b>Machine</b> <b>Learning</b>? \u2013 charmestrength.com", "url": "https://charmestrength.com/what-is-model-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://charmestrength.com/what-is-model-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "What is model <b>bias</b> in <b>machine</b> <b>learning</b>? <b>Machine</b> <b>learning</b> <b>bias</b>, also sometimes called <b>algorithm</b> <b>bias</b> or AI <b>bias</b>, ... What is <b>bias</b> in Ann? <b>Bias</b> <b>is like</b> the intercept added in a linear equation. It is an additional parameter in the Neural Network which is used to adjust the output along with the weighted sum of the inputs to the neuron. Thus, <b>Bias</b> is a constant which helps the model in a way that it can fit best for the given data. What are the 3 types of <b>machine</b> <b>learning</b> <b>bias</b>? Types of <b>Bias</b> in ...", "dateLastCrawled": "2022-01-15T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Attribution</b> Part 3: Applying <b>Machine</b> <b>Learning</b> to <b>Attribution</b> - Trust ...", "url": "https://www.trustinsights.ai/blog/2019/11/attribution-part-3-applying-machine-learning-to-attribution/", "isFamilyFriendly": true, "displayUrl": "https://www.trustinsights.ai/blog/2019/11/<b>attribution</b>-part-3-applying-<b>machine</b>-<b>learning</b>...", "snippet": "The <b>machine</b> <b>learning</b> approach to <b>attribution</b> analysis starts with no <b>bias</b>. With the inexpensive computational power now available to us you can run all of the data from every transaction, and all the data from transactions that did not complete. <b>Machine</b> <b>learning</b> will examine every single transition from one page to the next and from one goal (milestones along the customer journey) to the next and determine the probability of a customer moving from one point to the next. When Chris first ...", "dateLastCrawled": "2022-02-02T04:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Bias</b> in AI: What it is, Types, Examples &amp; 6 Ways to Fix it in 2022", "url": "https://research.aimultiple.com/ai-bias/", "isFamilyFriendly": true, "displayUrl": "https://research.aimultiple.com/ai-<b>bias</b>", "snippet": "AI <b>bias</b> is an anomaly in the output of <b>machine</b> <b>learning</b> algorithms, due to the prejudiced assumptions made during the <b>algorithm</b> development process or prejudices in the training data. What are the types of AI <b>bias</b>? AI systems contain biases due to two reasons: Cognitive biases: These are unconscious errors in thinking that affects individuals\u2019 judgements and decisions. These biases arise from the brain\u2019s attempt to simplify processing information about the world. More than 180 human ...", "dateLastCrawled": "2022-02-02T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Why Mitigating AI Biases Is The</b> Need Of The Hour?", "url": "https://analyticsindiamag.com/why-mitigating-ai-biases-is-the-need-of-the-hour/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>why-mitigating-ai-biases-is-the</b>-need-of-the-hour", "snippet": "Some of the common biases for AI models would include <b>group</b> <b>attribution</b> <b>bias</b>, out-<b>group</b> homogeneity <b>bias</b>, and selection-based <b>bias</b> ... What Is The Best Way To Create Training Data For <b>Machine</b> <b>Learning</b>. Real-time monitoring of the models for a better check. Although the majority of the biases arise while training the AI models, however, many unintentional biases occur over time, and therefore it is required for developers to keep a real-time check of their AI systems. It is also necessary to ...", "dateLastCrawled": "2022-02-03T09:33:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Bias</b> in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/<b>bias</b>-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "These biases include sample <b>bias</b>, reporting <b>bias</b>, prejudice <b>bias</b>, confirmation <b>bias</b>, <b>group</b> <b>attribution</b> <b>bias</b>, <b>algorithm</b> <b>bias</b>, measurement <b>bias</b>, recall <b>bias</b>, exclusion <b>bias</b>, and automation <b>bias</b>. <b>Machine</b> <b>learning</b> is highly susceptible to many forms of <b>bias</b> that can undermine model performance. After all, AI is assembled by humans, and humans are ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Biases in <b>machine</b> <b>learning</b> models and big data analytics: The ...", "url": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/biases-in-machine-learning-models-and-big-data-analytics-the-international-criminal-and-humanitarian-law-implications/86BEAC9ADD165C90B2931AB2B665FFDF", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/international-review-of-the-red-cross/article/...", "snippet": "<b>Group</b> <b>attribution</b> <b>bias</b> is a tendency to impute what is true of a few individuals to an entire <b>group</b> to which they belong. For instance, imagine that an ML model is created to identify the most suitable candidates for a position with the OTP. In creating this model, the designers assume that the \u201cbest\u201d candidates are individuals with a doctorate degree from a Western European university and internship experience with the ICC, purely because some successful employees possess those traits ...", "dateLastCrawled": "2021-12-21T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "What are the 3 types of <b>machine</b> <b>learning</b> <b>bias</b>? - Forum Topic View", "url": "https://www.cluzters.ai/forums/topic/566/what-are-the-3-types-of-machine-learning-bias?c=1597", "isFamilyFriendly": true, "displayUrl": "https://www.cluzters.ai/forums/topic/566/what-are-the-3-types-of-<b>machine</b>-<b>learning</b>-<b>bias</b>?...", "snippet": "<b>Bias</b> in <b>machine</b> <b>learning</b> can be applied when collecting the data to build the models. It can come with testing the outputs of the models to verify their validity. <b>Bias</b> <b>machine</b> <b>learning</b> can even be applied when interpreting valid or invalid results from an approved data model. Nearly all of the common <b>machine</b> <b>learning</b> biased data types come from our own cognitive biases. Some examples include Anchoring <b>bias</b>, Availability <b>bias</b>, Confirmation <b>bias</b>, and Stability <b>bias</b>.", "dateLastCrawled": "2021-12-04T02:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bias</b>, <b>Fairness</b> and Explainability \u2014 steps towards building Responsible ...", "url": "https://medium.com/walmartglobaltech/bias-fairness-and-explainability-steps-towards-building-responsible-ai-dc735b06279", "isFamilyFriendly": true, "displayUrl": "https://medium.com/walmartglobaltech/<b>bias</b>-<b>fairness</b>-and-explainability-steps-towards...", "snippet": "<b>Group</b> <b>Attribution</b> <b>bias</b>: ... As mentioned in Wikipedia: \u201cIn <b>machine</b> <b>learning</b>, a given <b>algorithm</b> is said to be fair, or to have <b>fairness</b>, if its results are independent of given variables ...", "dateLastCrawled": "2022-01-23T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "What Is <b>Bias</b> In Ai - SeniorCare2Share", "url": "https://www.seniorcare2share.com/what-is-bias-in-ai/", "isFamilyFriendly": true, "displayUrl": "https://www.seniorcare2share.com/what-is-<b>bias</b>-in-ai", "snippet": "<b>Machine</b> <b>learning</b> <b>bias</b>, also sometimes called <b>algorithm</b> <b>bias</b> or AI <b>bias</b>, is a phenomenon that occurs when an <b>algorithm</b> produces results that are systemically prejudiced due to erroneous assumptions in the <b>machine</b> <b>learning</b> process.<b>Machine</b> <b>learning</b> <b>bias</b>, also sometimes called <b>algorithm</b> biasalgorithm biasThe term algorithmic <b>bias</b> describes systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary <b>group</b> of users over others. For example, a credit score ...", "dateLastCrawled": "2022-01-08T10:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Addressing Fairness, <b>Bias</b>, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "A well-known example of <b>machine</b> <b>learning</b> <b>bias</b>, publicized by Joy Boulamwini in 2017 ... and fairness criteria could be satisfied by ensuring that the <b>algorithm</b> provides <b>similar</b> treatment to individuals that share <b>similar</b> characteristics. Mathematically, if it were possible to describe an individual by a set of parameters in a multi-dimensional space, then all individuals within the neighborhood of the same parametric space would be treated similarly and receive <b>similar</b> predictions from a ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Underdiagnosis <b>bias</b> of artificial intelligence algorithms applied to ...", "url": "https://www.nature.com/articles/s41591-021-01595-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41591-021-01595-0", "snippet": "As artificial intelligence (AI) algorithms increasingly affect decision-making in society 1, researchers have raised concerns about algorithms creating or amplifying biases 2,3,4,5,6,7,8,9,10,11 ...", "dateLastCrawled": "2022-02-02T21:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Detecting phishing websites using <b>machine</b> <b>learning</b> technique", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8504731/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8504731", "snippet": "Based on the output without and with the functionality selection a comparative study of <b>machine</b> <b>learning</b> algorithms is carried out in the study . Experiments on a phishing dataset were carried out with 30 features including 4898 phished and 6157 benign web pages. Several ML methods were used to yield a better outcome. A method for selecting functions is subsequently employed to increase model performance. Random forests <b>algorithm</b> achieved the highest accuracy prior to and after the selection ...", "dateLastCrawled": "2022-01-28T13:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "Second, A <b>machine</b> <b>learning</b> <b>algorithm</b> is chosen, and uses historical data (e.g. a set of past input data (e.g. a set of past input data, the decisions reached) to build a model, optimising against ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>Learning</b> <b>Algorithm</b> for Estimating Surface PM2.5 in Thailand ...", "url": "https://aaqr.org/articles/aaqr-21-05-oa-0105", "isFamilyFriendly": true, "displayUrl": "https://aaqr.org/articles/aaqr-21-05-oa-0105", "snippet": "ABSTRACT We have used NASA&#39;s Modern-Era Retrospective analysis for Research and Applications, Version 2 (MERRA2) reanalysis data of aerosols and meteorology into a <b>machine</b> <b>learning</b> <b>algorithm</b> (MLA) to estimate surface PM2.5 concentration in Thailand. One year of hourly data from 51 ground monitoring stations in Thailand was spatiotemporally collocated with MERRA2 fields. The integrated data then used to train and validate a supervised MLA&#39; random forest&#39; to estimate hourly and daily PM2.5 ...", "dateLastCrawled": "2022-02-03T00:47:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "A well-known example of <b>machine</b> <b>learning</b> <b>bias</b>, publicized by Joy Boulamwini in 2017 ... to enable the proper tuning of a <b>machine</b> <b>learning</b> <b>algorithm</b>. Individual vs. <b>Group</b> Fairness. Given the dictionary definition of fairness (impartial and just treatment), we <b>can</b> consider fairness at the level of an individual or a <b>group</b>. We <b>can</b> ask whether a computer <b>algorithm</b> disproportionately helps or harms specific individuals or specific groups of people. Ideally, an <b>algorithm</b> would be customized to an ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "A <b>machine</b> <b>learning</b> <b>algorithm</b> takes data as ... not available to the <b>algorithm</b>) but <b>can</b> also introduce human <b>bias</b> into the system. Humans \u2018over the loop\u2019 monitoring the fairness of the whole ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Evolution and impact of <b>bias</b> in human and <b>machine learning</b> <b>algorithm</b> ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "snippet": "Traditionally, <b>machine learning</b> algorithms relied on reliable labels from experts to build predictions. More recently however, algorithms have been receiving data from the general population in the form of labeling, annotations, etc. The result is that algorithms are subject to <b>bias</b> that is born from ingesting unchecked information, such as biased samples and biased labels. Furthermore, people and algorithms are increasingly engaged in interactive processes wherein neither the human nor the ...", "dateLastCrawled": "2021-11-15T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A <b>machine</b> <b>learning</b> toolkit for genetic engineering <b>attribution</b> to ...", "url": "https://www.nature.com/articles/s41467-020-19612-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-020-19612-0", "snippet": "With a framework for calibration of <b>attribution</b> models that <b>can</b> in principle be applied to any deep-<b>learning</b> classification <b>algorithm</b>, we next sought to expand the toolkit of genetic engineering ...", "dateLastCrawled": "2022-01-13T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Comparing different supervised <b>machine</b> <b>learning</b> algorithms for disease ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6925840/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC6925840", "snippet": "To avoid the risk of selection <b>bias</b>, from the literature we extracted those articles that used more than one supervised <b>machine</b> <b>learning</b> <b>algorithm</b>. The same supervised <b>learning</b> <b>algorithm</b> <b>can</b> generate different results across various study settings. There is a chance that a performance comparison between two supervised <b>learning</b> algorithms <b>can</b> generate imprecise results if they were employed in different studies separately. On the other side, the results of this study could suffer a variable ...", "dateLastCrawled": "2022-01-28T14:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Subjective and Objective in <b>the Development of Artificial Intelligence</b> ...", "url": "https://towardsdatascience.com/subjective-and-objective-in-the-development-of-artificial-intelligence-c5627a522f47", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/subjective-and-objective-in-the-development-of...", "snippet": "An <b>attribution</b> <b>bias</b> <b>can</b> happen when individuals assess or attempt to discover explanations behind their own and others\u2019 behaviors. People make attributions about the causes of their own and others\u2019 behaviors; but these attributions don\u2019t necessarily precisely reflect reality. Rather than operating as objective perceivers, individuals are inclined to perceptual slips that prompt biased understandings of their social world. Confirmation <b>bias</b> is the tendency to search for, interpret ...", "dateLastCrawled": "2022-01-26T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Bias</b> does not equal <b>bias</b>. A socio-technical typology of <b>bias</b> in data ...", "url": "https://policyreview.info/articles/analysis/bias-does-not-equal-bias-socio-technical-typology-bias-data-based-algorithmic", "isFamilyFriendly": true, "displayUrl": "https://policyreview.info/articles/analysis/<b>bias</b>-does-not-equal-<b>bias</b>-socio-technical...", "snippet": "Zooming into the technical details of a <b>machine</b> <b>learning</b> system\u2019s life cycle, Suresh and Guttag (2020) described various issues that <b>can</b> introduce <b>bias</b> into a system: historical <b>bias</b>, representation <b>bias</b>, measurement <b>bias</b>, aggregation <b>bias</b>, <b>learning</b> <b>bias</b>, evaluation <b>bias</b> and deployment <b>bias</b>. Some of these types of data <b>bias</b> <b>can</b> only be identified through extensive knowledge and close examination of the development process of a particular system including the underlying data used to build ...", "dateLastCrawled": "2022-01-22T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Bias</b>, awareness, and ignorance in deep-<b>learning</b>-based face recognition ...", "url": "https://link.springer.com/article/10.1007%2Fs43681-021-00108-6", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s43681-021-00108-6", "snippet": "We investigate how well <b>machine</b> <b>learning</b> models <b>can</b> predict the sensitive features, such as ethnicity and gender, based on the face embedding. The intuition is that an FR model is \u201caware\u201d of a sensitive feature if it <b>can</b> be predicted from the embedding vectors produced by the FR model. This inference is a classification task and the performance depends on the classification model at hand. If simple models, more precisely models with a low number of parameters, <b>can</b> properly infer the ...", "dateLastCrawled": "2022-02-03T10:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The ethics of <b>algorithms</b>: key problems and solutions", "url": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00146-021-01154-8", "snippet": "Actions (1) and (2) may be performed by (semi-)autonomous <b>algorithms</b>\u2014such as <b>machine</b> <b>learning</b> (ML) <b>algorithms</b>\u2014and this complicates, (3) the <b>attribution</b> of responsibility for the effects of actions that an <b>algorithm</b> may trigger. Here, ML is of particular interest, as a field which includes deep <b>learning</b> architectures. Computer systems deploying ML <b>algorithms</b> may be described as \u201cautonomous\u201d or \u201csemi-autonomous\u201d, to the extent that their outputs are induced from data and thus, non ...", "dateLastCrawled": "2022-01-30T20:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "300+ TOP <b>Neural Networks Multiple Choice Questions and Answers</b>", "url": "https://engineeringinterviewquestions.com/neural-networks-multiple-choice-questions-and-answers/", "isFamilyFriendly": true, "displayUrl": "https://engineeringinterviewquestions.com/<b>neural-networks-multiple-choice-questions</b>...", "snippet": "34. Decision Trees <b>can</b> be used for Classification Tasks. a) True b) False. Answer: a. 35. How many types of <b>learning</b> are available in <b>machine</b> <b>learning</b>? a) 1 b) 2 c) 3 d) 4. Answer: c Explanation: The three types of <b>machine</b> <b>learning</b> are supervised, unsupervised and reinforcement. 36. Choose from the following that are Decision Tree nodes. a ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What Is Model <b>Bias</b> In <b>Machine</b> <b>Learning</b>? \u2013 charmestrength.com", "url": "https://charmestrength.com/what-is-model-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://charmestrength.com/what-is-model-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "<b>Machine</b> <b>learning</b> <b>bias</b>, also sometimes called <b>algorithm</b> <b>bias</b> or AI <b>bias</b>, ... <b>Group</b> <b>attribution</b> <b>Bias</b>. What to do if model is Overfitting? Reduce the network&#39;s capacity by removing layers or reducing the number of elements in the hidden layers. Apply regularization , which comes down to adding a cost to the loss function for large weights. Use Dropout layers, which will randomly remove certain features by setting them to zero. How <b>can</b> <b>machine</b> <b>learning</b> models reduce <b>bias</b>? Choose the correct ...", "dateLastCrawled": "2022-01-15T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "AI <b>Bias</b> Example, Meaning And How To Avoid Them - Kotai Electronics Pvt ...", "url": "https://kotaielectronics.com/ai-bias-example-meaning-and-how-to-avoid-them/", "isFamilyFriendly": true, "displayUrl": "https://kotaielectronics.com/ai-<b>bias</b>-example-meaning-and-how-to-avoid-them", "snippet": "<b>Group</b> <b>attribution</b> AI <b>bias</b> meaning, The <b>Bias</b> that takes place when the <b>algorithm</b> puts more weight onto an individual it happens because the <b>algorithm</b> classifies the data and extrapolates a certain set of data from the rest of the data set. Here the AI <b>bias</b> example <b>can</b> be a tool that is used for admission and recruiting people, Here the system <b>can</b> put more importance on students who graduate from certain universities over others.", "dateLastCrawled": "2022-01-18T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Can</b> <b>bias</b> be eliminated from algorithms?", "url": "https://www.weforum.org/agenda/2021/12/bias-eliminated-algorithms-software-code-systems/", "isFamilyFriendly": true, "displayUrl": "https://www.weforum.org/agenda/2021/12/<b>bias</b>-eliminated-<b>algorithms</b>-software-code-systems", "snippet": "<b>Algorithm</b> <b>bias</b>, also called <b>machine</b> <b>learning</b> <b>bias</b>, is a phenomenon in which algorithms <b>can</b> act in a discriminatory or prejudiced manner due to misplaced assumptions during the <b>learning</b> phase of their development. Unconscious biases regarding gender, race and social class <b>can</b> make their way into the training data fed by programmers into \u201c<b>machine</b>-<b>learning</b> algorithms\u201d, systems which constantly improve their own performance by including new data into an existing model. These biases <b>can</b> be ...", "dateLastCrawled": "2022-02-03T05:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bias</b>, <b>Fairness</b> and Explainability \u2014 steps towards building Responsible ...", "url": "https://medium.com/walmartglobaltech/bias-fairness-and-explainability-steps-towards-building-responsible-ai-dc735b06279", "isFamilyFriendly": true, "displayUrl": "https://medium.com/walmartglobaltech/<b>bias</b>-<b>fairness</b>-and-explainability-steps-towards...", "snippet": "<b>Group</b> <b>Attribution</b> <b>bias</b>: ... in Wikipedia: \u201cIn <b>machine</b> <b>learning</b>, a given <b>algorithm</b> is said to be fair, or to have <b>fairness</b>, if its results are independent of given variables, especially those ...", "dateLastCrawled": "2022-01-23T22:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "how to reduce <b>bias</b> in <b>machine</b> <b>learning</b> - Publicaffairsworld.com", "url": "https://publicaffairsworld.com/how-to-reduce-bias-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://publicaffairsworld.com/how-to-reduce-<b>bias</b>-in-<b>machine</b>-<b>learning</b>", "snippet": "If you have ever developed or worked on any type of <b>machine</b> <b>learning</b> <b>algorithm</b>, ... <b>Group</b> <b>attribution</b> <b>Bias</b>. What are the necessary steps to avoid biases in gathering and interpreting data? There are ways, however, to try to maintain objectivity and avoid <b>bias</b> with qualitative data analysis: Use multiple people to code the data. \u2026 Have participants review your results. \u2026 Verify with more data sources. \u2026 Check for alternative explanations. \u2026 Review findings with peers. What <b>can</b> a data ...", "dateLastCrawled": "2022-01-22T04:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Frontiers | Addressing Fairness, <b>Bias</b>, and Appropriate Use of ...", "url": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802/full", "isFamilyFriendly": true, "displayUrl": "https://www.frontiersin.org/articles/10.3389/frai.2020.561802", "snippet": "A well-known example of <b>machine</b> <b>learning</b> <b>bias</b>, publicized by Joy Boulamwini in 2017 ... to enable the proper tuning of a <b>machine</b> <b>learning</b> <b>algorithm</b>. Individual vs. <b>Group</b> Fairness. Given the dictionary definition of fairness (impartial and just treatment), we <b>can</b> consider fairness at the level of an individual or a <b>group</b>. We <b>can</b> ask whether a computer <b>algorithm</b> disproportionately helps or harms specific individuals or specific groups of people. Ideally, an <b>algorithm</b> would be customized to an ...", "dateLastCrawled": "2022-01-29T07:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A <b>machine</b> <b>learning</b> toolkit for genetic engineering <b>attribution</b> to ...", "url": "https://www.nature.com/articles/s41467-020-19612-0", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-020-19612-0", "snippet": "With a framework for calibration of <b>attribution</b> models that <b>can</b> in principle be applied to any deep-<b>learning</b> classification <b>algorithm</b>, we next sought to expand the toolkit of genetic engineering ...", "dateLastCrawled": "2022-01-13T13:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Evolution and impact of <b>bias</b> in human and <b>machine learning</b> <b>algorithm</b> ...", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0235502", "snippet": "Traditionally, <b>machine learning</b> algorithms relied on reliable labels from experts to build predictions. More recently however, algorithms have been receiving data from the general population in the form of labeling, annotations, etc. The result is that algorithms are subject to <b>bias</b> that is born from ingesting unchecked information, such as biased samples and biased labels. Furthermore, people and algorithms are increasingly engaged in interactive processes wherein neither the human nor the ...", "dateLastCrawled": "2021-11-15T00:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Addressing Fairness, Bias, and Appropriate</b> Use of Artificial ...", "url": "https://europepmc.org/article/PMC/PMC8107824", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8107824", "snippet": "<b>Machine</b> <b>learning</b> <b>can</b> also be used to help optimize processes or to predict ... If algorithmic <b>bias</b> leads to unfavorable treatment of one patient <b>group</b> vs. another, this <b>bias</b> <b>can</b> be judged to be unfair, from a legal or ethical point of view. While <b>bias</b> is related to fairness, it should be noted that algorithmic <b>bias</b> is independent of ethics, and is simply a mathematical and statistical consequence of an <b>algorithm</b> and its data. If an <b>algorithm</b> is discovered to have <b>bias</b>, this <b>bias</b> <b>can</b> then be ...", "dateLastCrawled": "2021-06-01T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Review into bias in algorithmic decision-making</b> - <b>GOV.UK</b>", "url": "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making", "isFamilyFriendly": true, "displayUrl": "https://<b>www.gov.uk</b>/government/publications/cdei-publishes-review-into-<b>bias</b>-in...", "snippet": "A <b>machine</b> <b>learning</b> <b>algorithm</b> takes data as ... not available to the <b>algorithm</b>) but <b>can</b> also introduce human <b>bias</b> into the system. Humans \u2018over the loop\u2019 monitoring the fairness of the whole ...", "dateLastCrawled": "2022-02-03T02:05:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Attribution</b> Part 3: Applying <b>Machine</b> <b>Learning</b> to <b>Attribution</b> - Trust ...", "url": "https://www.trustinsights.ai/blog/2019/11/attribution-part-3-applying-machine-learning-to-attribution/", "isFamilyFriendly": true, "displayUrl": "https://www.trustinsights.ai/blog/2019/11/<b>attribution</b>-part-3-applying-<b>machine</b>-<b>learning</b>...", "snippet": "The <b>machine</b> <b>learning</b> approach to <b>attribution</b> analysis starts with no <b>bias</b>. With the inexpensive computational power now available to us you can run all of the data from every transaction, and all the data from transactions that did not complete. <b>Machine</b> <b>learning</b> will examine every single transition from one page to the next and from one goal (milestones along the customer journey) to the next and determine the probability of a customer moving from one point to the next.", "dateLastCrawled": "2022-02-02T04:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "6 \u2013 Interpretability \u2013 <b>Machine</b> <b>Learning</b> Blog | ML@CMU | Carnegie Mellon ...", "url": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability/", "isFamilyFriendly": true, "displayUrl": "https://blog.ml.cmu.edu/2020/08/31/6-interpretability", "snippet": "For a person inexperienced in <b>machine</b> <b>learning</b>, it would be difficult to apply the Olah method across many images, especially if you wanted to explain why the model chose a labrador retriever instead of a beagle. In such a case, abstract dog visualizations would be uninformative. Moreover, there is a potential observer <b>bias</b>: if a human expects visualizations of dogs and cats, they might miss more abstract but important visualizations, such as the snow in the husky and wolf classification ...", "dateLastCrawled": "2022-02-03T05:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Ethical Issues in <b>Machine</b> <b>Learning</b> Algorithms (Part 2)", "url": "https://www.slideshare.net/vladimirkanchev/ethical-issues-in-machine-learning-algorithms-part-2-140369359", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/vladimirkanchev/ethical-issues-in-<b>machine</b>-<b>learning</b>...", "snippet": "<b>Machine</b> <b>bias</b> \u2013 leads to discrimination and lack of fairness, ... Determination of minority is fuzzy and applying a separate classifier for minority <b>group</b> requires testing or and on protected attribute that can be considered objectionable. Learn multiple classifiers one for each <b>group</b> in an effort to discovert statistical patterns that may be uniqeto a particular <b>group</b>. Computationally more expensive algorithms for leanring anarbitrary combination of two linear classifiers \u2013 for majority ...", "dateLastCrawled": "2022-01-08T21:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "From a <b>machine</b> <b>learning</b> perspective, the interesting point is the classification of metrics into <b>bias</b>-preserving and <b>bias</b>-transforming. The terms speak for themselves: Metrics in the first <b>group</b> reflect biases in the dataset used for training; ones in the second do not. In that way, the distinction parallels", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Explainable artificial intelligence: an analytical review - Angelov ...", "url": "https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1424", "isFamilyFriendly": true, "displayUrl": "https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1424", "snippet": "Artificial intelligence (AI) and <b>machine</b> <b>learning</b> (ML) have demonstrated their potential to revolutionize ... (Arrieta et al., 2020), layer-wise feature relevance propagation and <b>attribution</b> (Tritscher et al., 2020), local pseudo explanations by LIME (Dieber &amp; Kirrane, 2020), game-theoretic Shapley additive explanations (Chen et al., 2019 ), gradient-based localization, and Grad-CAM (Selvaraju et al., 2017) or surrogate models. In this section, some of the more widely used methods are ...", "dateLastCrawled": "2022-01-29T23:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Letter to the editor: \u201cNot all biases are bad: equitable and ...", "url": "https://insightsimaging.springeropen.com/track/pdf/10.1186/s13244-021-01022-5.pdf", "isFamilyFriendly": true, "displayUrl": "https://insightsimaging.springeropen.com/track/pdf/10.1186/s13244-021-01022-5", "snippet": "equitable and inequitable biases in <b>machine</b> <b>learning</b> and radiology\u201d ... Open Access This article is licensed under a Creative Commons <b>Attribution</b> 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are ...", "dateLastCrawled": "2021-09-16T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Assessing biases, relaxing moralism: On ground-truthing practices in ...", "url": "https://journals.sagepub.com/doi/10.1177/20539517211013569", "isFamilyFriendly": true, "displayUrl": "https://journals.sagepub.com/doi/10.1177/20539517211013569", "snippet": "<b>Machine</b> <b>learning</b> (ML) algorithms\u2014computerized methods of calculation that infer rules of computation from sets of data to make predictions and support decision-making tasks\u2014are now powering many commonly used devices such as Web search engines (Richardson et al., 2006), social media applications (Hazelwood et al., 2018), online purchasing platforms (Portugal et al., 2018), and surveillance systems (Chokshi, 2019).In reaction to the growing ubiquity of these statistical methods of ...", "dateLastCrawled": "2022-01-10T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Lecture 4: \\(k\\)-Nearest Neighbours and SVM RBFs \u2014 CPSC 330 Applied ...", "url": "https://ubc-cs.github.io/cpsc330/lectures/04_kNNs-SVM-RBF.html", "isFamilyFriendly": true, "displayUrl": "https://ubc-cs.github.io/cpsc330/lectures/04_kNNs-SVM-RBF.html", "snippet": "Overview\u00b6. Another popular similarity-based algorithm is Support Vector Machines with RBF Kernel (SVM RBFs) Superficially, SVM RBFs are more like weighted \\(k\\)-NNs.. The decision boundary is defined by a set of positive and negative examples and their weights together with their similarity measure.. A test example is labeled positive if on average it looks more like positive examples than the negative examples.", "dateLastCrawled": "2022-01-11T11:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "300+ TOP <b>Neural Networks Multiple Choice Questions and Answers</b>", "url": "https://engineeringinterviewquestions.com/neural-networks-multiple-choice-questions-and-answers/", "isFamilyFriendly": true, "displayUrl": "https://engineeringinterviewquestions.com/<b>neural-networks-multiple-choice-questions</b>...", "snippet": "How many types of <b>learning</b> are available in <b>machine</b> <b>learning</b>? a) 1 b) 2 c) 3 d) 4. Answer: c Explanation: The three types of <b>machine</b> <b>learning</b> are supervised, unsupervised and reinforcement. 36. Choose from the following that are Decision Tree nodes. a) Decision Nodes b) Weighted Nodes c) Chance Nodes d) End Nodes. Answer: a, c, d. 37. Decision Nodes are represented by, a) Disks b) Squares c) Circles d) Triangles. Answer: b. 38. Chance Nodes are represented by, a) Disks b) Squares c) Circles ...", "dateLastCrawled": "2022-02-02T15:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The <b>Ethics of Machine Learning, Computer Vision, and</b> the Bone Trade ...", "url": "https://intarch.ac.uk/journal/issue52/5/3.html", "isFamilyFriendly": true, "displayUrl": "https://intarch.ac.uk/journal/issue52/5/3.html", "snippet": "The resulting app carries the appearance and authority of &#39;<b>machine</b> <b>learning</b>&#39;, where we end up with an &#39;official&#39; result: 88% (see Dao 2018 for a curated list of &#39;AI&#39; deployed in ways that take advantage of this rhetoric). And that&#39;s a powerful story in the context of the trade in human remains, where it is as much the story about an image that sells the bones as it is anything inherent in the bones themselves. Imagine a classifier that picked out Dayak remains that was trained to recognise ...", "dateLastCrawled": "2022-01-21T09:48:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Bias in A.I. &amp; <b>Machine</b> <b>Learning</b> Examples \u2013 Tanner Abraham", "url": "https://tannerabraham.com/bias-in-machine-learning-and-ai-examples/", "isFamilyFriendly": true, "displayUrl": "https://tannerabraham.com/bias-in-<b>machine</b>-<b>learning</b>-and-ai-examples", "snippet": "<b>Machine</b> <b>learning</b> algorithms are often mistaken as objective analytics and decision-making solutions to human inefficiencies. Paradoxically, humans often make <b>machine</b> <b>learning</b> algorithms inefficient by way of biases. These biases include sample bias, reporting bias, prejudice bias, confirmation bias, group attribution bias, algorithm bias, measurement bias, recall bias, exclusion bias, and automation bias. <b>Machine</b> <b>learning</b> is highly susceptible to many forms of bias that can undermine model ...", "dateLastCrawled": "2022-01-28T07:55:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(group attribution bias)  is like +(machine learning algorithm)", "+(group attribution bias) is similar to +(machine learning algorithm)", "+(group attribution bias) can be thought of as +(machine learning algorithm)", "+(group attribution bias) can be compared to +(machine learning algorithm)", "machine learning +(group attribution bias AND analogy)", "machine learning +(\"group attribution bias is like\")", "machine learning +(\"group attribution bias is similar\")", "machine learning +(\"just as group attribution bias\")", "machine learning +(\"group attribution bias can be thought of as\")", "machine learning +(\"group attribution bias can be compared to\")"]}