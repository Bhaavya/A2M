{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "Cross-entropy <b>loss</b> increases as the <b>predicted</b> probability <b>value</b> deviate from the <b>actual</b> label. <b>Hinge</b> <b>loss</b>. <b>Hinge</b> <b>loss</b> can be used as an alternative to cross-entropy, which was initially developed to use with a support vector machine algorithm. <b>Hinge</b> <b>loss</b> works best with the classification problem because target values are in the set of {-1,1 ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss Functions in Deep Learning: An Overview</b>", "url": "https://analyticsindiamag.com/loss-functions-in-deep-learning-an-overview/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>loss-functions-in-deep-learning-an-overview</b>", "snippet": "2.<b>Hinge</b> <b>Loss</b>. This type of <b>loss</b> is used when the target variable has 1 or -1 as class labels. It penalizes the model when there is a <b>difference</b> in the sign <b>between</b> the <b>actual</b> and <b>predicted</b> class values. These are particularly used in SVM models.", "dateLastCrawled": "2022-02-02T02:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "The distinction is the <b>difference</b> <b>between</b> <b>predicted</b> <b>and actual</b> probability. This adds data about information <b>loss</b> in the model training. The farther away the <b>predicted</b> probability distribution is ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "It represents the <b>difference</b> <b>between</b> the original and <b>predicted</b> values extracted by averaging the absolute <b>difference</b> over the data set. ... So predicting a probability of .012 when the <b>actual</b> observation label is 1 would be bad and result in a high <b>loss</b> <b>value</b>. A perfect model would have a log <b>loss</b> of 0. The graph above shows the range of possible <b>loss</b> values given a true observation. As the <b>predicted</b> probability approaches 1, log <b>loss</b> slowly decreases. As the <b>predicted</b> probability decreases ...", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions in <b>Machine Learning</b> - 360DigiTMG", "url": "https://360digitmg.com/loss-functions-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://360digitmg.com/<b>loss</b>-functions-in-<b>machine-learning</b>", "snippet": "Squared <b>Hinge</b> <b>Loss</b>: This is an extension of the <b>hinge</b> <b>loss</b> and it is quite simply the square of the <b>hinge</b> <b>loss</b> function. Since this is a square of the original <b>loss</b>, it has some mathematical properties that make it easier to calculate the gradients. This is perfectly suitable for Yes or No type of questions where the deviation in probability is ...", "dateLastCrawled": "2022-01-30T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Most Used <b>Loss</b> <b>Functions To Optimize Machine Learning Algorithms</b>", "url": "https://analyticsindiamag.com/most-used-loss-functions-to-optimize-machine-learning-algorithms/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/most-used-<b>loss</b>-<b>functions-to-optimize-machine-learning</b>...", "snippet": "<b>Hinge</b> <b>Loss</b>; <b>Hinge</b> <b>loss</b> was originally developed for SVM. The simple intuition behind <b>hinge</b> <b>loss</b> is, it works on the <b>difference</b> of sign. For e.g. the target variable has values <b>like</b> -1 and 1 and the model predicts 1 whereas the <b>actual</b> class is -1, the function will impose a higher penalty at this point because it can sense the <b>difference</b> in the ...", "dateLastCrawled": "2022-02-01T05:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Which <b>loss</b> function is better: <b>hinge</b> <b>loss</b> or softmax? - Quora", "url": "https://www.quora.com/Which-loss-function-is-better-hinge-loss-or-softmax", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Which-<b>loss</b>-function-is-better-<b>hinge</b>-<b>loss</b>-or-softmax", "snippet": "Answer: This is an easy one, <b>hinge</b> <b>loss</b>, since softmax is not a <b>loss</b> function. Softmax is a means for converting a set of values to a \u201cprobability distribution\u201d. We would not traditionally consider this a <b>loss</b> function as much as we would use it in the process of computing the <b>loss</b> with another f...", "dateLastCrawled": "2022-01-12T19:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How <b>To Build Custom Loss Functions In Keras</b> For Any Use Case | cnvrg.io", "url": "https://cnvrg.io/keras-custom-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://cnvrg.io/keras-custom-<b>loss</b>-functions", "snippet": "Mean Absolute error, also known as L1 Error, is defined as the average of the absolute differences <b>between</b> the <b>actual</b> <b>value</b> and the <b>predicted</b> <b>value</b>. This is the average of the absolute <b>difference</b> <b>between</b> the <b>predicted</b> and the <b>actual</b> <b>value</b>. Mathematically, it can be shown as:", "dateLastCrawled": "2022-01-31T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Scikit Learn - Support Vector Machines</b> - <b>Tutorialspoint</b>", "url": "https://www.tutorialspoint.com/scikit_learn/scikit_learn_support_vector_machines.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/scikit_learn/<b>scikit_learn_support_vector_machines</b>.htm", "snippet": "<b>loss</b> \u2212 string, <b>hinge</b>, squared_<b>hinge</b> (default = squared_<b>hinge</b>) ... and specifies the epsilon-tube within which no penalty is associated in the training <b>loss</b> function with points <b>predicted</b> within a distance epsilon from the <b>actual</b> <b>value</b>. Rest of the parameters and attributes are similar as we used in SVC. Implementation Example. Following Python script uses sklearn.svm.SVR class \u2212. from sklearn import svm X = [[1, 1], [2, 2]] y = [1, 2] SVRReg = svm.SVR(kernel = \u2019linear\u2019, gamma ...", "dateLastCrawled": "2022-02-02T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "It is calculated as the average of the absolute <b>difference</b> <b>between</b> the <b>actual</b> and <b>predicted</b> values. ... The problem is often framed as predicting a <b>value</b> of 0 or 1 for the first or second class and is often implemented as predicting the probability of the example belonging to class <b>value</b> 1. In this section, we will investigate <b>loss</b> functions that are appropriate for binary classification predictive modeling problems. We will generate examples from the circles test problem in scikit-learn as ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss Functions in Deep Learning: An Overview</b>", "url": "https://analyticsindiamag.com/loss-functions-in-deep-learning-an-overview/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>loss-functions-in-deep-learning-an-overview</b>", "snippet": "2.<b>Hinge</b> <b>Loss</b>. This type of <b>loss</b> is used when the target variable has 1 or -1 as class labels. It penalizes the model when there is a <b>difference</b> in the sign <b>between</b> the <b>actual</b> and <b>predicted</b> class values. These are particularly used in SVM models.", "dateLastCrawled": "2022-02-02T02:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "The distinction is the <b>difference</b> <b>between</b> <b>predicted</b> <b>and actual</b> probability. This adds data about information <b>loss</b> in the model training. The farther away the <b>predicted</b> probability distribution is ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Types of <b>Keras Loss Functions Explained for Beginners</b> - MLK - Machine ...", "url": "https://machinelearningknowledge.ai/types-of-keras-loss-functions-explained-for-beginners/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningknowledge.ai/types-of-<b>keras-loss-functions-explained-for-beginners</b>", "snippet": "In the Poisson <b>loss</b> function, we calculate the Poisson <b>loss</b> <b>between</b> the <b>actual</b> <b>value</b> and <b>predicted</b> <b>value</b>. Poisson <b>Loss</b> Function is generally used with datasets that consists of Poisson distribution. An example of Poisson distribution is the count of calls received by the call center in an hour.", "dateLastCrawled": "2022-01-31T03:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Neural Networks | by Sai Chandra Nerella | Becoming ...", "url": "https://becominghuman.ai/loss-functions-in-neural-networks-ec6482a15e97", "isFamilyFriendly": true, "displayUrl": "https://becominghuman.ai/<b>loss</b>-functions-in-neural-networks-ec6482a15e97", "snippet": "There are many functions out there to find the <b>loss</b> based on the <b>predicted</b> <b>and actual</b> <b>value</b> depending on the problem. And optimizers are used to minimize the <b>loss</b> to make predictions better.Let\u2019s discuss about few <b>Loss</b> functions. Optimizing <b>Loss</b>. Cross Entropy <b>Loss</b> : This is the most common <b>loss</b> function used for classification type problems. It works in such a way that <b>loss</b> decreases as the <b>predicted</b> probability converges towards the ground truth. Here\u2019s the Mathematical Formula for ...", "dateLastCrawled": "2022-02-03T05:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "y\u1d62 - <b>Actual</b> Label; \u0177\u1d62- <b>Predicted</b> Label; The <b>actual</b> labels should be in the form of a one hutz vector in this case. One hot vector means a vector with a <b>value</b> of one in the index of the ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to use <b>categorical / multiclass hinge with TensorFlow</b> 2 and Keras ...", "url": "https://www.machinecurve.com/index.php/2019/10/17/how-to-use-categorical-multiclass-hinge-with-keras/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/.../10/17/how-to-use-categorical-multiclass-<b>hinge</b>-with-keras", "snippet": "Computing the <b>loss</b> \u2013 the <b>difference</b> <b>between</b> <b>actual</b> target and <b>predicted</b> targets \u2013 is then equal to computing the <b>hinge</b> <b>loss</b> for taking the prediction for all the computed classes, except for the target class, since <b>loss</b> is always 0 there. The <b>hinge</b> <b>loss</b> computation itself <b>is similar</b> to the traditional <b>hinge</b> <b>loss</b>. Categorical <b>hinge</b> <b>loss</b> can be optimized as well and hence used for generating decision boundaries in multiclass machine learning problems. Let\u2019s now see how we can implement ...", "dateLastCrawled": "2022-01-29T21:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Loss Function</b>(Part III): Support Vector Machine | by Shuyu Luo ...", "url": "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-iii-5dff33fa015d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/optimization-<b>loss-function</b>-under-the-hood-part-iii-5dff...", "snippet": "<b>Hinge</b> <b>Loss</b>, when the <b>actual</b> is 1 (left plot as below), if \u03b8\u1d40x \u2265 1, no cost at all, if \u03b8\u1d40x &lt; 1, the cost increases as the <b>value</b> of \u03b8\u1d40x decreases. Wait! When \u03b8\u1d40x \u2265 0, we already predict 1, which is the correct prediction. Why does the cost start to increase from 1 instead of 0? Yes, SVM gives some punishment to both incorrect predictions and those close to decision boundary ( 0 &lt; \u03b8\u1d40x &lt;1), that\u2019s how we call them support vectors. When data points are just right on the ...", "dateLastCrawled": "2022-02-02T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "How to Choose <b>Loss</b> Functions When Training Deep Learning Neural Networks", "url": "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-choose-<b>loss</b>-functions-when-training-deep...", "snippet": "It is calculated as the average of the absolute <b>difference</b> <b>between</b> the <b>actual</b> and <b>predicted</b> values. ... The problem is often framed as predicting a <b>value</b> of 0 or 1 for the first or second class and is often implemented as predicting the probability of the example belonging to class <b>value</b> 1. In this section, we will investigate <b>loss</b> functions that are appropriate for binary classification predictive modeling problems. We will generate examples from the circles test problem in scikit-learn as ...", "dateLastCrawled": "2022-02-02T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning Interview Questions</b> (2022) - InterviewBit", "url": "https://www.interviewbit.com/machine-learning-interview-questions/", "isFamilyFriendly": true, "displayUrl": "https://www.interviewbit.com/<b>machine-learning-interview-questions</b>", "snippet": "In other words, the <b>loss</b> function is to capture the <b>difference</b> <b>between</b> the <b>actual</b> and <b>predicted</b> values for a single record whereas cost functions aggregate the <b>difference</b> for the entire training dataset.", "dateLastCrawled": "2022-02-03T00:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Classification vs. Regression Algorithms in Machine Learning M", "url": "https://www.projectpro.io/article/classification-vs-regression-in-machine-learning/545", "isFamilyFriendly": true, "displayUrl": "https://www.projectpro.io/article/classification-vs-regression-in-machine-learning/545", "snippet": "This <b>loss</b> is used for binary classification problems. It tries to measure the <b>difference</b> in entropy (randomness) <b>between</b> the <b>predicted</b> class and the <b>actual</b> class. Categorical Cross-Entropy: This <b>loss</b> is used for multi-class classification problems. One hot-encoded input is required to feed into the <b>loss</b> function. This function works well with ...", "dateLastCrawled": "2022-01-27T17:55:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions | <b>Loss</b> Functions in <b>Machine Learning</b> - 360DigiTMG", "url": "https://360digitmg.com/loss-functions-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://360digitmg.com/<b>loss</b>-functions-in-<b>machine-learning</b>", "snippet": "Squared <b>Hinge</b> <b>Loss</b>: This is an extension of the <b>hinge</b> <b>loss</b> and it is quite simply the square of the <b>hinge</b> <b>loss</b> function. Since this is a square of the original <b>loss</b>, it has some mathematical properties that make it easier to calculate the gradients. This is perfectly suitable for Yes or No type of questions where the deviation in probability is ...", "dateLastCrawled": "2022-01-30T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "Graph for -log(x) This is pretty simple, the more your input increases, the more output goes lower. If you have a small input(x=0.5) so the output is going to be high(y=0.305).", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Cost Functions In Machine Learning</b> - The Click Reader", "url": "https://www.theclickreader.com/cost-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.theclickreader.com/<b>cost-functions-in-machine-learning</b>", "snippet": "7. <b>Hinge</b> <b>loss</b>. The <b>hinge</b> <b>loss</b> function is a common cost function used in Support Vector Machines (SVM) for classification. It maps the output to values <b>between</b> 1, 0, -1. The <b>Hinge</b> <b>loss</b> function is calculated as, where is <b>actual</b> <b>value</b> of the output, is the classification score <b>predicted</b> by the model.", "dateLastCrawled": "2022-02-02T06:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>loss</b>-functions-and-optimization-algorithms...", "snippet": "One of the most widely used <b>loss function</b> is mean square error, which calculates the square of <b>difference</b> <b>between</b> <b>actual</b> <b>value</b> and <b>predicted</b> <b>value</b>. Different <b>loss</b> functions are used to deal with ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Support Vector Machine</b> \u2014 Introduction to Machine Learning Algorithms ...", "url": "https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>support-vector-machine</b>-introduction-to-machine-learning...", "snippet": "The <b>loss</b> function that helps maximize the margin is <b>hinge</b> <b>loss</b>. <b>Hinge</b> <b>loss</b> function (function on left <b>can</b> be represented as a function on the right) The cost is 0 if the <b>predicted</b> <b>value</b> and the <b>actual</b> <b>value</b> are of the same sign. If they are not, we then calculate the <b>loss</b> <b>value</b>. We also add a regularization parameter the cost function. The objective of the regularization parameter is to balance the margin maximization and <b>loss</b>. After adding the regularization parameter, the cost functions ...", "dateLastCrawled": "2022-02-03T00:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Cost Function and <b>Loss</b> Function in Machine Learning - Shishir Kant Singh", "url": "http://shishirkant.com/cost-function-and-loss-function-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "shishirkant.com/cost-function-and-<b>loss</b>-function-in-machine-learning", "snippet": "On taking a closer look at the formulas, one <b>can</b> observe that if the <b>difference</b> <b>between</b> the <b>predicted</b> and the <b>actual</b> <b>value</b> is high, L2 <b>loss</b> magnifies the effect when compared to L1. Since L2 succumbs to outliers, L1 <b>loss</b> function is the more robust <b>loss</b> function. L1 <b>loss</b> is less stable than L2 <b>loss</b>.", "dateLastCrawled": "2022-01-29T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>An Intro to Linear Classification with Python</b> - PyImageSearch", "url": "https://www.pyimagesearch.com/2016/08/22/an-intro-to-linear-classification-with-python/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/2016/08/22/<b>an-intro-to-linear-classification-with-python</b>", "snippet": "While <b>hinge</b> <b>loss</b> is used in many machine learning applications (e.g., SVMs), I <b>can</b> almost guarantee with absolute certainty that you\u2019ll see cross-entropy <b>loss</b> with more frequency primarily due to the fact that Softmax classifiers output probabilities rather than margins. Probabilities are much easier for us as humans to interpret, so this fact is a particularly nice quality of cross-entropy <b>loss</b> and Softmax classifiers. For more information on <b>hinge</b> <b>loss</b> and cross-entropy <b>loss</b>, please refer to", "dateLastCrawled": "2022-02-03T01:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Loss</b> Functions in Machine Learning: An Easy Overview(2021)", "url": "https://www.jigsawacademy.com/blogs/ai-ml/loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.<b>jigsawacademy</b>.com/blogs/ai-ml/<b>loss</b>-function", "snippet": "MSE <b>loss</b> performs as outlined because of the average of absolute variations <b>between</b> the particular and also the foretold <b>value</b>. It\u2019s the second most ordinarily used Regression <b>loss</b> function. The function <b>value</b> is the Mean of these Absolute Errors (MAE). The MAE <b>Loss</b> function is additional strong to outliers compared to the MSE <b>Loss</b> function. Therefore, it ought to be used if the information is liable to several outliers. The logistic <b>loss</b> function has nice mathematical properties, which ...", "dateLastCrawled": "2022-01-26T08:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Famous <b>Loss</b> Functions and Cost Functions in Machine Learning", "url": "https://www.enjoyalgorithms.com/blog/loss-and-cost-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.enjoyalgorithms.com/blog/<b>loss</b>-and-cost-functions-in-machine-learning", "snippet": "<b>Difference</b> <b>between</b> <b>Loss</b> and Cost Function. We usually consider both terms as synonyms and think that they <b>can</b> be used interchangeably. But, the <b>Loss</b> function is associated with every training example, and the cost function is the average <b>value</b> of the <b>loss</b> function over all the training samples. In Machine learning, we usually try to optimize our cost function rather than <b>loss</b> function. Why is the choice of the perfect <b>loss</b> function important? There is one famous quote in Neural Smithing Book ...", "dateLastCrawled": "2022-01-26T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "3.3. Model <b>Evaluation</b> - Scikit-learn - W3cubDocs", "url": "https://docs.w3cub.com/scikit_learn/modules/model_evaluation.html", "isFamilyFriendly": true, "displayUrl": "https://docs.w3cub.com/scikit_learn/modules/model_<b>evaluation</b>.html", "snippet": "The <b>actual</b> outcome has to be 1 or 0 (true or false), while the <b>predicted</b> probability of the <b>actual</b> outcome <b>can</b> be a <b>value</b> <b>between</b> 0 and 1. The brier score <b>loss</b> is also <b>between</b> 0 to 1 and the lower the score (the mean square <b>difference</b> is smaller), the more accurate the prediction is. It <b>can</b> <b>be thought</b> of as a measure of the \u201ccalibration\u201d of ...", "dateLastCrawled": "2022-01-28T16:07:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss Functions in Machine Learning</b> | Working | Different Types", "url": "https://www.educba.com/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.educba.com/<b>loss-functions-in-machine-learning</b>", "snippet": "Cross-entropy <b>loss</b> increases as the <b>predicted</b> probability <b>value</b> deviate from the <b>actual</b> label. <b>Hinge</b> <b>loss</b>. <b>Hinge</b> <b>loss</b> <b>can</b> be used as an alternative to cross-entropy, which was initially developed to use with a support vector machine algorithm. <b>Hinge</b> <b>loss</b> works best with the classification problem because target values are in the set of {-1,1 ...", "dateLastCrawled": "2022-02-03T02:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "The main <b>difference</b> <b>between</b> the <b>hinge</b> <b>loss</b> and the cross entropy <b>loss</b> is that the former arises from trying to maximize the margin <b>between</b> our decision boundary and data points - thus attempting to ensure that each point is correctly and confidently classified*, while the latter comes from a maximum likelihood estimate of our model\u2019s parameters. The softmax function, whose scores are used by the cross entropy <b>loss</b>, allows us to interpret our model\u2019s scores as relative probabilities ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions | <b>Loss</b> Functions in <b>Machine Learning</b> - 360DigiTMG", "url": "https://360digitmg.com/loss-functions-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://360digitmg.com/<b>loss</b>-functions-in-<b>machine-learning</b>", "snippet": "Squared <b>Hinge</b> <b>Loss</b>: This is an extension of the <b>hinge</b> <b>loss</b> and it is quite simply the square of the <b>hinge</b> <b>loss</b> function. Since this is a square of the original <b>loss</b>, it has some mathematical properties that make it easier to calculate the gradients. This is perfectly suitable for Yes or No type of questions where the deviation in probability is ...", "dateLastCrawled": "2022-01-30T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> Functions in Neural Networks", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "Cross-Entropy <b>loss</b> is also called logarithmic <b>loss</b>, log <b>loss</b>, or logistic <b>loss</b>. Each <b>predicted</b> class probability is <b>compared</b> to the <b>actual</b> class desired output 0 or 1 and a score/<b>loss</b> is calculated that penalizes the probability based on how far it is from the <b>actual</b> expected <b>value</b>. The penalty is logarithmic in nature yielding a large score for large differences close to 1 and small score for small differences tending to 0.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Cost Function and <b>Loss</b> Function in Machine Learning - Shishir Kant Singh", "url": "http://shishirkant.com/cost-function-and-loss-function-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "shishirkant.com/cost-function-and-<b>loss</b>-function-in-machine-learning", "snippet": "On taking a closer look at the formulas, one <b>can</b> observe that if the <b>difference</b> <b>between</b> the <b>predicted</b> and the <b>actual</b> <b>value</b> is high, L2 <b>loss</b> magnifies the effect when <b>compared</b> to L1. Since L2 succumbs to outliers, L1 <b>loss</b> function is the more robust <b>loss</b> function. L1 <b>loss</b> is less stable than L2 <b>loss</b>.", "dateLastCrawled": "2022-01-29T23:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Types of Loss Function</b> - OpenGenus IQ: Computing Expertise", "url": "https://iq.opengenus.org/types-of-loss-function/", "isFamilyFriendly": true, "displayUrl": "https://iq.opengenus.org/<b>types-of-loss-function</b>", "snippet": "<b>Loss</b> function is an important part in artificial neural networks, which is used to measure the inconsistency <b>between</b> <b>predicted</b> <b>value</b> (^y) <b>and actual</b> label (y). It is a non-negative <b>value</b>, where the robustness of model increases along with the decrease of the <b>value</b> of <b>loss</b> function.", "dateLastCrawled": "2022-01-28T13:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "The distinction is the <b>difference</b> <b>between</b> <b>predicted</b> <b>and actual</b> probability. This adds data about information <b>loss</b> in the model training. The farther away the <b>predicted</b> probability distribution is ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.machinecurve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "Both normal <b>hinge</b> and squared <b>hinge</b> <b>loss</b> work only for binary classification problems in which the <b>actual</b> target <b>value</b> is either +1 or -1. Although that\u2019s perfectly fine for when you have such problems (e.g. the diabetes yes/no problem that we looked at previously), there are many other problems which cannot be solved in a binary fashion.", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Which <b>loss</b> function is better: <b>hinge</b> <b>loss</b> or softmax? - Quora", "url": "https://www.quora.com/Which-loss-function-is-better-hinge-loss-or-softmax", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Which-<b>loss</b>-function-is-better-<b>hinge</b>-<b>loss</b>-or-softmax", "snippet": "Answer: This is an easy one, <b>hinge</b> <b>loss</b>, since softmax is not a <b>loss</b> function. Softmax is a means for converting a set of values to a \u201cprobability distribution\u201d. We would not traditionally consider this a <b>loss</b> function as much as we would use it in the process of computing the <b>loss</b> with another f...", "dateLastCrawled": "2022-01-12T19:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Loss</b> Functions in Machine Learning | G. Wu", "url": "https://guangyuwu.wordpress.com/2021/02/02/loss-functions-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://guangyuwu.wordpress.com/2021/02/02/<b>loss</b>-functions-in-machine-learning", "snippet": "Each <b>predicted</b> probability is <b>compared</b> to the <b>actual</b> class output <b>value</b> (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected <b>value</b>. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large <b>difference</b> (0.9 or 1.0). Cross-entropy <b>loss</b> is minimized, where smaller values represent a better model than larger values. A model that predicts perfect probabilities has a cross entropy or log ...", "dateLastCrawled": "2022-01-12T02:27:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Main concepts behind <b>Machine</b> <b>Learning</b> | by Leven.co.in | Medium", "url": "https://in-leven.medium.com/main-concepts-behind-machine-learning-848ec516ef94", "isFamilyFriendly": true, "displayUrl": "https://in-leven.medium.com/main-concepts-behind-<b>machine</b>-<b>learning</b>-848ec516ef94", "snippet": "The two most common <b>loss</b> function are <b>hinge</b>-<b>loss</b> and cross-entropy. The first one is used in SVM (Supported Vector Machines) classifiers and it concerns in getting the correct class score greater than the other scores by a margin \u0394. Formula for <b>hinge</b>-<b>loss</b>. s\u1d62 is the correct score category. The second one is used in Softmax classifiers which interprets the scores as probabilities, always trying to get the correct class close to 1. Formula for cross-entropy. s\u1d62 the correct category score ...", "dateLastCrawled": "2022-01-14T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Main concepts behind Machine Learning</b> | by Bruno Eidi Nishimoto ...", "url": "https://medium.com/neuronio/main-concepts-behind-machine-learning-22cd81d68a11", "isFamilyFriendly": true, "displayUrl": "https://medium.com/neuronio/<b>main-concepts-behind-machine-learning</b>-22cd81d68a11", "snippet": "The two most common <b>loss</b> function are <b>hinge</b>-<b>loss</b> and cross-entropy. The first one is used in SVM (Supported Vector Machines) classifiers and it concerns in getting the correct class score greater ...", "dateLastCrawled": "2022-01-19T01:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "In contrast, in <b>machine</b> <b>learning</b> methodology, log <b>loss</b> will be minimized with respect to ... <b>Hinge</b> <b>loss</b> (SVM) Logistic/log <b>loss</b> (logistic regression) Some <b>loss</b> functions are as follows: When to stop tuning <b>machine</b> <b>learning</b> models. When to stop tuning the hyperparameters in a <b>machine</b> <b>learning</b> model is a million-dollar question. This problem can be mostly solved by keeping tabs on training and testing errors. While increasing the complexity of a model, the following stages occur: Stage 1 ...", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Metrics to Evaluate Classification and Regression Algorithms | by ...", "url": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and-regression-algorithms-1554f1e00a75", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@poojitha.penmethsa/metrics-to-evaluate-classification-and...", "snippet": "<b>Hinge</b> <b>Loss</b>(Binary Classification): An alternative to cross-entropy for binary classification problems is the <b>hinge</b> <b>loss</b> function, primarily developed for use with support vector <b>machine</b> (SVM ...", "dateLastCrawled": "2022-01-17T01:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Data Science: <b>Support Vector Machines (SVM</b>)", "url": "https://www.datasciencesmachinelearning.com/2019/01/support-vector-machines-svm.html", "isFamilyFriendly": true, "displayUrl": "https://www.datasciences<b>machinelearning</b>.com/2019/01/<b>support-vector-machines-svm</b>.html", "snippet": "In this case, squared <b>hinge</b> <b>loss</b> function (as against <b>hinge</b> <b>loss</b> function) and l2 penalty are the major changes compared to the earlier three methods. This method is useful for when sample size is larger.", "dateLastCrawled": "2022-01-28T08:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Models 1.1 Support vector <b>machine</b> 1.1.1 Principle 1.1.2 Kernel 1.1.3 Soft margin SVM 1.1.4 <b>Hinge</b> <b>loss</b> view 1.1.5 Multi-class SVM 1.1.6 Extensions 1.2 Tree-based models 1.2.1 Decision tree 1.2.2 Random forest 1.2.3 Gradient boosted decision trees 1.2.4 Tools 1.3 EM Principle 1.4 MaxEnt 1.4.1 Entropy 1.5 Model selection 1.5.1 Under-fitting / Over-fitting 1.5.2 Model ensemble, sklearn 2.", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "Optimization methods are applied to minimize the <b>loss</b> function by changing the parameter values, which is the central theme of <b>machine</b> <b>learning</b>.Zero-one <b>loss</b> is L0-1 = 1 (m &lt;= 0); in zero-one <b>loss</b>, value of <b>loss</b> is 0 for m &gt;= 0 whereas 1 for m &lt; 0. The difficult part with this <b>loss</b> is it is not differentiable, non-convex, and also NP-hard. Hence, in order to make optimization feasible and solvable, these losses are replaced by different surrogate losses for different problems.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning and Civil Liberties</b> | by Joel Nantais | Towards Data ...", "url": "https://towardsdatascience.com/machine-learning-and-civil-liberties-7bfbfab8233d", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>machine-learning-and-civil-liberties</b>-7bfbfab8233d", "snippet": "The Black Box of <b>machine</b> <b>Learning</b>. In a now famous <b>analogy</b>, <b>machine</b> <b>learning</b>, especially more sophisticated techniques such as neural nets and deep <b>learning</b> have created a black box where outputs of models cannot be reversed engineered in a way where parties can know the specifics of an individual result. This has been well documented, and continues to be vigorously debated in <b>machine</b> <b>learning</b> ethics forum. Many decisions made about an individual have the prospect of being significant and ...", "dateLastCrawled": "2022-01-18T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine learning terminology for model building and</b> validation ...", "url": "https://subscription.packtpub.com/book/big-data-and-business-intelligence/9781788295758/1/ch01lvl1sec9/machine-learning-terminology-for-model-building-and-validation", "isFamilyFriendly": true, "displayUrl": "https://subscription.packtpub.com/book/big-data-and-business-intelligence/...", "snippet": "<b>Machine learning terminology for model building and</b> validation. There seems to be an <b>analogy</b> between statistical modeling and <b>machine</b> <b>learning</b> that we will cover in subsequent chapters in depth. However, a quick view has been provided as follows: in statistical modeling, linear regression with two independent variables is trying to fit the best ...", "dateLastCrawled": "2021-12-26T09:15:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>About loss and loss functions</b> \u2013 MachineCurve", "url": "https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>machine</b>curve.com/index.php/2019/10/04/<b>about-loss-and-loss-functions</b>", "snippet": "We\u2019re then using <b>machine</b> <b>learning</b> for ... The squared <b>hinge loss is like</b> the hinge formula displayed above, but then the \\(max()\\) function output is squared. This helps achieving two things: Firstly, it makes the loss value more sensitive to outliers, just as we saw with MSE vs MAE. Large errors will add to the loss more significantly than smaller errors. Note that simiarly, this may also mean that you\u2019ll need to inspect your dataset for the presence of such outliers first. Secondly ...", "dateLastCrawled": "2022-01-25T13:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>machine</b> <b>learning</b> - <b>hinge loss</b> vs logistic loss advantages and ...", "url": "https://stats.stackexchange.com/questions/146277/hinge-loss-vs-logistic-loss-advantages-and-disadvantages-limitations", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/146277/<b>hinge-loss</b>-vs-logistic-loss...", "snippet": "<b>machine</b>-<b>learning</b> svm loss-functions computer-vision. Share. Cite. Improve this question. Follow edited Jul 23 &#39;18 at 15:41. DHW. 644 3 3 silver badges 13 13 bronze badges. asked Apr 14 &#39;15 at 11:18. user570593 user570593. 1,059 2 2 gold badges 12 12 silver badges 19 19 bronze badges $\\endgroup$ Add a comment | 3 Answers Active Oldest Votes. 31 $\\begingroup$ Logarithmic loss minimization leads to well-behaved probabilistic outputs. <b>Hinge loss</b> leads to some (not guaranteed) sparsity on the ...", "dateLastCrawled": "2022-01-26T09:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>A Course in Machine Learning</b> | AZERTY UIOP - Academia.edu", "url": "https://www.academia.edu/11902068/A_Course_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/11902068/<b>A_Course_in_Machine_Learning</b>", "snippet": "<b>A Course in Machine Learning</b>. \u00d7 Close Log In. Log in with Facebook Log in with Google. or. Email. Password. Remember me on this computer. or reset password. Enter the email address you signed up with and we&#39;ll email you a reset link. Need an account? Click here to sign up. Log In Sign ...", "dateLastCrawled": "2022-01-23T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Course in <b>Machine</b> <b>Learning</b>", "url": "http://ciml.info/dl/v0_8/ciml-v0_8-ch12.pdf", "isFamilyFriendly": true, "displayUrl": "ciml.info/dl/v0_8/ciml-v0_8-ch12.pdf", "snippet": "160 a course in <b>machine</b> <b>learning</b> fortunately, not only is the zero-norm non-convex, it\u2019s also discrete. Optimizing it is NP-hard. A reasonable middle-ground is the one-norm: jjwjj 1 = \u00e5 djw j. It is indeed convex: in fact, it is the tighest \u2018p norm that is convex. Moreover, its gradients do not go to zero as in the two-norm. <b>Just as hinge-loss</b> is the tightest convex upper bound on zero-one error, the one-norm is the tighest convex upper bound on the zero-norm. At this point, you should ...", "dateLastCrawled": "2021-09-07T20:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Course in <b>Machine</b> <b>Learning</b> | PDF | <b>Machine</b> <b>Learning</b> | Prediction", "url": "https://www.scribd.com/document/346469890/a-course-in-machine-learning-pdf", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/346469890/a-course-in-<b>machine</b>-<b>learning</b>-pdf", "snippet": "The <b>machine</b> <b>learning</b> algorithm has succeeded if its performance on the test data is high. 1.2 Some Canonical <b>Learning</b> Problems. There are a large number of typical inductive <b>learning</b> problems. The primary difference between them is in what type of thing theyre trying to predict. Here are some examples: Regression: trying to predict a real value. For instance, predict the value of a stock tomorrow given its past performance. Or predict Alices score on the <b>machine</b> <b>learning</b> final exam based on ...", "dateLastCrawled": "2021-12-06T00:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Course in <b>Machine</b> <b>Learning</b>", "url": "http://ciml.info/dl/v0_9/ciml-v0_9-ch12.pdf", "isFamilyFriendly": true, "displayUrl": "ciml.info/dl/v0_9/ciml-v0_9-ch12.pdf", "snippet": "162 a course in <b>machine</b> <b>learning</b> pect the algorithm to converge. Unfortunately, in comparisong to gradient descent, stochastic gradient is quite sensitive to the selection of a good <b>learning</b> rate. There is one more practical issues related to the use of SGD as a <b>learning</b> algorithm: do you really select a random point (or subset of random points) at each step, or do you stream through the data in order. The answer is akin to the answer of the same question for the perceptron algorithm ...", "dateLastCrawled": "2021-09-20T12:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "- <b>A Course in Machine Learning</b> - Studylib", "url": "https://studylib.net/doc/8792694/--a-course-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://studylib.net/doc/8792694/--<b>a-course-in-machine-learning</b>", "snippet": "Free essays, homework help, flashcards, research papers, book reports, term papers, history, science, politics", "dateLastCrawled": "2021-12-27T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Ciml <b>v0 - 8 All Machine Learning</b> | <b>Machine Learning</b> | Prediction", "url": "https://www.scribd.com/document/172987143/Ciml-v0-8-All-Machine-Learning", "isFamilyFriendly": true, "displayUrl": "https://www.scribd.com/document/172987143/Ciml-<b>v0-8-All-Machine-Learning</b>", "snippet": "The <b>machine learning</b> algorithm has succeeded if its performance on the test data is high. 1.2 Some Canonical <b>Learning</b> Problems. There are a large number of typical inductive <b>learning</b> problems. The primary difference between them is in what type of thing theyre trying to predict. Here are some examples: Regression: trying to predict a real value. For instance, predict the value of a stock tomorrow given its past performance. Or predict Alices score on the <b>machine learning</b> nal exam based on ...", "dateLastCrawled": "2022-01-19T05:02:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(hinge loss)  is like +(difference between predicted and actual value)", "+(hinge loss) is similar to +(difference between predicted and actual value)", "+(hinge loss) can be thought of as +(difference between predicted and actual value)", "+(hinge loss) can be compared to +(difference between predicted and actual value)", "machine learning +(hinge loss AND analogy)", "machine learning +(\"hinge loss is like\")", "machine learning +(\"hinge loss is similar\")", "machine learning +(\"just as hinge loss\")", "machine learning +(\"hinge loss can be thought of as\")", "machine learning +(\"hinge loss can be compared to\")"]}