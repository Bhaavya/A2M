{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning</b> | Request PDF", "url": "https://www.researchgate.net/publication/340431006_Fairness_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340431006_<b>Fairness_in_Machine_Learning</b>", "snippet": "The notion of <b>Demographic</b> <b>Parity</b> as a way to impose exact fairness in the regression setup <b>has</b> <b>been</b> studied in several papers (see Barocas et al. (2019), Oneto and Chiappa (2020) and references ...", "dateLastCrawled": "2021-12-20T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Paradoxes in Fair <b>Machine</b> Learning - Paul G\u00f6lz", "url": "https://paulgoelz.de/papers/equalized.pdf", "isFamilyFriendly": true, "displayUrl": "https://paulgoelz.de/papers/equalized.pdf", "snippet": "prior work on measuring and ensuring statistical notions of fairness, notably through metrics <b>like</b> <b>demographic</b> <b>parity</b> and <b>equalized odds</b> [8,10,11,13,16,18,24\u201326]. The statistical notion of fairness that we will consider throughout this paper is that of <b>equalized odds</b>, which states that a classi\ufb01er must have equal true positive and false positive rates for all groups. While <b>equalized odds</b> <b>has</b> <b>been</b> extensively studied as a metric of fairness in <b>machine</b> learning [10, 11, 13, 16, 18, 24], it ...", "dateLastCrawled": "2021-10-28T15:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Explainable AI: A Review of <b>Machine</b> Learning Interpretability Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "Traditionally, the fairness of <b>a machine</b> learning system <b>has</b> <b>been</b> evaluated by checking the models\u2019 predictions and errors across certain <b>demographic</b> segments, for example, groups of a specific ethnicity or gender. In terms of dealing with a lack of fairness, a number of techniques have <b>been</b> developed both to remove bias from training data and from model predictions and to train models that learn to make fair predictions in the first place. In this section, the most widely-used <b>machine</b> ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> Learning | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-learning-3ff8ba1040cb", "snippet": "Netflix uses recommender system to present customized page for <b>every</b> user. <b>Machine</b> learning systems have <b>been</b> an inseparable part of our daily lives. They are becoming even more widely used in the near future as more and more fields begin to integrate AI into their existing practice/products. Artificial Intelligence is good but it can be used incorrectly. <b>Machine</b> Learning, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Paradoxes in Fair <b>Machine</b> Learning - NeurIPS", "url": "https://proceedings.neurips.cc/paper/9043-paradoxes-in-fair-machine-learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/9043-paradoxes-in-fair-<b>machine</b>-learning.pdf", "snippet": "<b>Machine</b> learning classi\ufb01ers have <b>been</b> trained to determine which applicants deserve bank loans [19], which students merit acceptance from a particular school [23], or which prisoners should receive parole [15]. The prevalence of algorithmic intervention <b>has</b> led to a widespread call for accountability in <b>machine</b> learning: in order to ensure that algorithms do not disproportionately affect different constituent subpopulations, researchers must be able to provide fairness guarantees of the ...", "dateLastCrawled": "2021-09-17T13:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Addressing Fairness, Bias, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "<b>Machine</b> learning can be generally defined as the methods and algorithms that enable computers to make optimal decisions given a set of data; these tasks can range from simple binary classification decisions to more advanced real-<b>time</b> control tasks, such as driving a car or playing a video game. <b>Machine</b> learning methods include an increasing variety of mathematical tools and mathematical models, ranging from simple logistic regression, to neural nets and deep learning, to probabilistic ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Conditional statistical <b>parity</b> (Corbett-Davies et al., 2017), called also conditional discrimination-aware classification in Kamiran, Zliobaite, and Calders (2013) is a variant of statistical <b>parity</b> obtained by controlling on a set of legitimate attributes. 18 The legitimate attributes (we refer to them as E) among X are correlated with the sensitive attribute A and give some factual information about the label at <b>the same</b> <b>time</b> leading to a legitimate discrimination.", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Addressing Fairness, Bias, and Appropriate</b> Use of Artificial ...", "url": "https://europepmc.org/article/PMC/PMC8107824", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8107824", "snippet": "In order to address the need for better guidance within the context of global health, we describe three basic criteria (Appropriateness, Fairness, and Bias) that can be used to help evaluate the use of <b>machine</b> learning and AI systems: 1) APPROPRIATENESS is the process of deciding how the algorithm should be used in the local context, and properly matching the <b>machine</b> learning model to the target population; 2) BIAS is a systematic tendency in a model to favor one <b>demographic</b> group vs another ...", "dateLastCrawled": "2021-06-01T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>On Fairness and Calibration</b> - ResearchGate", "url": "https://www.researchgate.net/publication/319534462_On_Fairness_and_Calibration", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/319534462_<b>On_Fairness_and_Calibration</b>", "snippet": "The <b>machine</b> learning community <b>has</b> become increasingly concerned with the potential for bias and discrimination in predictive models, and this <b>has</b> motivated a growing line of work on what it means ...", "dateLastCrawled": "2022-01-30T21:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "in analysis of variance, Which of the following distributions is the ...", "url": "https://iibmanswersheethelp.blogspot.com/2017/04/in-analysis-of-variance-which-of_8.html", "isFamilyFriendly": true, "displayUrl": "https://iibmanswersheethelp.blogspot.com/2017/04/in-analysis-of-variance-which-of_8.html", "snippet": "in analysis of variance, Which of the following distributions is the basis for determining whether the variance estimates are all from <b>the same</b> population a. Chi square b. Student\u2019s c. Normal d. F", "dateLastCrawled": "2022-01-12T20:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Fairness in Machine Learning</b> | Request PDF", "url": "https://www.researchgate.net/publication/340431006_Fairness_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340431006_<b>Fairness_in_Machine_Learning</b>", "snippet": "The notion of <b>Demographic</b> <b>Parity</b> as a way to impose exact fairness in the regression setup <b>has</b> <b>been</b> studied in several papers (see Barocas et al. (2019), Oneto and Chiappa (2020) and references ...", "dateLastCrawled": "2021-12-20T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Addressing Fairness, Bias, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "In order to address the need for better guidance within the context of global health, we describe three basic criteria (Appropriateness, Fairness, and Bias) that can be used to help evaluate the use of <b>machine</b> learning and AI systems: 1) APPROPRIATENESS is the process of deciding how the algorithm should be used in the local context, and properly matching the <b>machine</b> learning model to the target population; 2) BIAS is a systematic tendency in a model to favor one <b>demographic</b> group vs another ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Explainable AI: A Review of <b>Machine</b> Learning Interpretability Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "Traditionally, the fairness of a <b>machine</b> learning system <b>has</b> <b>been</b> evaluated by checking the models\u2019 predictions and errors across certain <b>demographic</b> segments, for example, groups of a specific ethnicity or gender. In terms of dealing with a lack of fairness, a number of techniques have <b>been</b> developed both to remove bias from training data and from model predictions and to train models that learn to make fair predictions in the first place. In this section, the most widely-used <b>machine</b> ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) Tuning Fairness by Marginalizing Latent Target Labels", "url": "https://www.researchgate.net/publication/328280275_Tuning_Fairness_by_Marginalizing_Latent_Target_Labels", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328280275_Tuning_Fairness_by_Marginalizing...", "snippet": "<b>demographic</b> <b>parity</b> is also called \u201cdisparate impact\u201d (see e.g. [11,33]). As the As the results in T able 1 show, both FairGP v ariants are clearly fairer than the baseline", "dateLastCrawled": "2021-08-23T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Tutorial on <b>Fairness</b> in <b>Machine</b> Learning | by Ziyuan Zhong | Towards ...", "url": "https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/a-tutorial-on-<b>fairness</b>-in-<b>machine</b>-learning-3ff8ba1040cb", "snippet": "Netflix uses recommender system to present customized page for <b>every</b> user. <b>Machine</b> learning systems have <b>been</b> an inseparable part of our daily lives. They are becoming even more widely used in the near future as more and more fields begin to integrate AI into their existing practice/products. Artificial Intelligence is good but it can be used incorrectly. <b>Machine</b> Learning, the most widely used AI techniques, relies heavily on data. It is a common misconception that AI is absolutely objective ...", "dateLastCrawled": "2022-02-01T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Bias in Data-driven AI Systems - An Introductory Survey | Manios ...", "url": "https://www.academia.edu/68938092/Bias_in_Data_driven_AI_Systems_An_Introductory_Survey", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/68938092/Bias_in_Data_driven_AI_Systems_An_Introductory_Survey", "snippet": "Their decisions might influence everyone, everywhere and anytime, offering solutions to problems faced in different disciplines or in daily life, but at <b>the same</b> <b>time</b> entailing risks like being denied a job or a medical treatment. The discriminative impact of AI-based decision making to certain population groups <b>has</b> <b>been</b> already observed in a variety of cases. For instance, the COMPAS system for predicting the risk of re-offending was found to predict higher risk values for black defendants ...", "dateLastCrawled": "2022-02-05T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Predicting youth at high risk of aging out of foster care using <b>machine</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0145213421001320", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0145213421001320", "snippet": "One methodology <b>that has</b> <b>been</b> increasingly applied to these types of classification problems is <b>machine</b> learning. <b>Machine</b> learning focuses on predicting the <b>output</b> value for new observations, which is considered to be a relatively easier task than inference, using flexible functional forms that are suitable to find relationship between features and the outcome ( Shmueli, 2010 ).", "dateLastCrawled": "2021-12-25T12:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Addressing Fairness, Bias, and Appropriate</b> Use of Artificial ...", "url": "https://europepmc.org/article/PMC/PMC8107824", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8107824", "snippet": "In order to address the need for better guidance within the context of global health, we describe three basic criteria (Appropriateness, Fairness, and Bias) that can be used to help evaluate the use of <b>machine</b> learning and AI systems: 1) APPROPRIATENESS is the process of deciding how the algorithm should be used in the local context, and properly matching the <b>machine</b> learning model to the target population; 2) BIAS is a systematic tendency in a model to favor one <b>demographic</b> group vs another ...", "dateLastCrawled": "2021-06-01T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Neural Styling for Interpretable Fair Representations | DeepAI", "url": "https://deepai.org/publication/neural-styling-for-interpretable-fair-representations", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/neural-styling-for-interpretable-fair-representations", "snippet": "The <b>time</b> <b>has</b> therefore come that the most important area in <b>machine</b> learning is the implementation of algorithms that adhere to ethical and legal requirements. For example, as per EU\u2019s General Data Protection Regulation and US\u2019s Fair Credit Reporting Act, data must be processed in a way that is fair/unbiased and lawful. Having biased algorithms can cause", "dateLastCrawled": "2022-01-03T08:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "in analysis of variance, Which of the following distributions is the ...", "url": "https://iibmanswersheethelp.blogspot.com/2017/04/in-analysis-of-variance-which-of_8.html", "isFamilyFriendly": true, "displayUrl": "https://iibmanswersheethelp.blogspot.com/2017/04/in-analysis-of-variance-which-of_8.html", "snippet": "in analysis of variance, Which of the following distributions is the basis for determining whether the variance estimates are all from <b>the same</b> population a. Chi square b. Student\u2019s c. Normal d. F", "dateLastCrawled": "2022-01-12T20:08:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Addressing Fairness, Bias, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "This situation <b>can</b> <b>produce</b> diagnostic tests that perform better on one <b>demographic</b> group at the expense of others, and is thus deemed unfair. While the eventual optimal tuning of an algorithm <b>can</b> thus depend on many factors, including the local system of laws and community values, the overall goal of this paper is to introduce and define the notions of bias, fairness, and appropriate use, as it pertains to <b>machine</b> learning in global health, and also to illustrate how a given <b>machine</b> learning ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "SoK: <b>Machine</b> Learning Governance | DeepAI", "url": "https://deepai.org/publication/sok-machine-learning-governance", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/sok-<b>machine</b>-learning-governance", "snippet": "For fairness, the regulator requires <b>demographic</b> <b>parity</b> [86] with at most \u03b3 &lt; 10 % disparity between sub-populations. The model owner <b>can</b> choose its own specifications for \u03b5 and \u03b3 and commission two model builders <b>to produce</b> the models. Model builders are now in a bidding competition to achieve the most performant model within specification ...", "dateLastCrawled": "2022-02-02T03:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fairness in Machine Learning: A Survey</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-a-survey", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-a-survey</b>", "snippet": "Not considering correlated variables <b>has</b> <b>been</b> shown to increase the risk of discrimination ... Over <b>time</b>, many different approaches have <b>been</b> suggested, most of which use metrics based on the binary classification confusion matrix to define fairness. 3.2.1 <b>Parity</b>-based Metrics. <b>Parity</b>-based metrics typically consider the predicted positive rates, i.e., P r (^ y = 1), across different groups. This is related to the Independence criterion that was defined in subsection 3.1. Statistical ...", "dateLastCrawled": "2022-01-18T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ECEM 2017 - ncbi.nlm.<b>nih.gov</b>", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7141056/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7141056", "snippet": "In 2002, she obtained a full-<b>time</b> research position at the french Center for National Scientific Research (CNRS) and, since then, <b>has</b> <b>been</b> working for the INSERM team called &quot;Espace et Action&quot; in Bron, France. In 2006, she received the Bronze Medal of the CNRS. In 2008, she received her habilitation degree from the University of Lyon 1. In 2011, she joined the Integrative, Multisensory, Perception, Action and Cognition Team (ImpAct) of the Lyon Neuroscience research center (CRNL).", "dateLastCrawled": "2022-01-27T11:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Ensuring Fairness in <b>Machine</b> Learning to Advance Health Equity", "url": "https://www.researchgate.net/publication/329401114_Ensuring_Fairness_in_Machine_Learning_to_Advance_Health_Equity", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/329401114_Ensuring_Fairness_in_<b>Machine</b>...", "snippet": "It is important to note that there are many metrics that <b>can</b> be used for bias accounting during development (i.e., equalized odds, predictive <b>parity</b>, <b>demographic</b> <b>parity</b>, etc.) (See Box 1). Bias ...", "dateLastCrawled": "2022-01-07T13:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "[Mission 2022] SECURE SYNOPSIS: 11 September 2021 - INSIGHTSIAS", "url": "https://www.insightsonindia.com/2021/09/13/201397/", "isFamilyFriendly": true, "displayUrl": "https://www.insightsonindia.com/2021/09/13/201397", "snippet": "What we are providing is content that both meets demand of the question and at <b>the same</b> <b>time</b> gives you extra points in the form of background information. Answer the following questions in 150 words: General Studies \u2013 1 . 1. Indian leaders resented the inefficacy of the Government of India Act 1935 in politically and economically empowering Indians but nevertheless gave it a try. Elucidate. (150 words, 10 marks) Introduction. The Government of India Act was passed by the British Government ...", "dateLastCrawled": "2022-01-21T08:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "AI Fairness 360: An Extensible Toolkit for Detecting and Mitigating ...", "url": "https://www.researchgate.net/publication/335900500_AI_Fairness_360_An_Extensible_Toolkit_for_Detecting_and_Mitigating_Algorithmic_Bias", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/335900500_AI_Fairness_360_An_Extensible...", "snippet": "AI Fairness 360 (AIF360) is a toolkit that provides both fairness detection and mitigation strategies (Bellamy et al., 2018). It <b>can</b> be used in both Python and R. The fairness metrics provided ...", "dateLastCrawled": "2022-01-04T13:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "Though the general framework of affirmative action <b>has</b> not yet <b>been</b> applied to sentencing and algorithmic fairness, the work of scholars in this field supports the underlying logic of that framework and <b>can</b> be seen as building to this kind of solution. Moreover, there is space in the law for such an approach to work. While the current political environment may be inhospitable to any type of affirmative action proposal, adopting a more equitable definition of fairness for criminal sentencing ...", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>The Oxford Handbook of Ethics</b> of AI: An Annotated ... - <b>Ethics in Context</b>", "url": "https://c4ejournal.net/the-oxford-handbook-of-ethics-of-ai-an-annotated-bibliography/", "isFamilyFriendly": true, "displayUrl": "https://c4ejournal.net/<b>the-oxford-handbook-of-ethics</b>-<b>of-ai-an-annotated-bibliography</b>", "snippet": "Drawing from <b>the same</b> tools as have <b>been</b> used in computational metaphysics, this paper presents a proof of concept for a meta-ethical robot. This proof of concept serves as an opening to other pathways and themes that are mentioned as potential next steps in building a robot with the capacity to reason about its own reasoning. A robot with an extensive set of rules at its disposal <b>can</b> operate with great success within complex pattern matching tasks but will not exhibit meta-ethical ...", "dateLastCrawled": "2022-02-02T15:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Marketing Strategy and Competitive Positioning</b> (4th Edition) - SILO.PUB", "url": "https://silo.pub/marketing-strategy-and-competitive-positioning-4th-edition.html", "isFamilyFriendly": true, "displayUrl": "https://silo.pub/<b>marketing-strategy-and-competitive-positioning</b>-4th-edition.html", "snippet": "In the field of services marketing there <b>has</b> <b>been</b> a great deal of work aimed at identifying the factors that <b>can</b> create gaps in the process from design through to delivery of offer to customers. Parasuraman et al. (1985), for example, have studied each of the potential gaps and concluded that a central role of marketing is to guide design so as to minimise the gaps and hence help to ensure customer satisfaction through the delivery of high quality (fit for the purpose) services (see Chapter ...", "dateLastCrawled": "2022-02-01T21:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Explainable AI: A Review of <b>Machine</b> Learning Interpretability Methods", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7824368", "snippet": "Traditionally, the fairness of a <b>machine</b> learning system <b>has</b> <b>been</b> evaluated by checking the models\u2019 predictions and errors across certain <b>demographic</b> segments, for example, groups of a specific ethnicity or gender. In terms of dealing with a lack of fairness, a number of techniques have <b>been</b> developed both to remove bias from training data and from model predictions and to train models that learn to make fair predictions in the first place. In this section, the most widely-used <b>machine</b> ...", "dateLastCrawled": "2022-01-29T03:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness in Machine Learning</b> | Request PDF", "url": "https://www.researchgate.net/publication/340431006_Fairness_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/340431006_<b>Fairness_in_Machine_Learning</b>", "snippet": "The notion of <b>Demographic</b> <b>Parity</b> as a way to impose exact fairness in the regression setup <b>has</b> <b>been</b> studied in several papers (see Barocas et al. (2019), Oneto and Chiappa (2020) and references ...", "dateLastCrawled": "2021-12-20T02:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Parity</b>-based Cumulative Fairness-aware Boosting | DeepAI", "url": "https://deepai.org/publication/parity-based-cumulative-fairness-aware-boosting", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>parity</b>-based-cumulative-fairness-aware-boosting", "snippet": "A growing body of research <b>has</b> <b>been</b> proposed over the recent years to address fairness and algorithmic discrimination. These methods propose \u201cinterventions\u201d at the input data (the so-called, pre-processing methods), learning algorithm (the so-called, in-processing methods), or the <b>output</b> model (the so-called post-processing methods) to ensure that the model decisions are not only correct in terms of predictive performance but also fair according to some definition of fairness. The vast ...", "dateLastCrawled": "2022-02-02T21:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Conditional statistical <b>parity</b> (Corbett-Davies et al., 2017), called also conditional discrimination-aware classification in Kamiran, Zliobaite, and Calders (2013) is a variant of statistical <b>parity</b> obtained by controlling on a set of legitimate attributes. 18 The legitimate attributes (we refer to them as E) among X are correlated with the sensitive attribute A and give some factual information about the label at <b>the same</b> <b>time</b> leading to a legitimate discrimination.", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Addressing Fairness, Bias, and Appropriate Use of Artificial ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8107824/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8107824", "snippet": "This situation <b>can</b> <b>produce</b> diagnostic tests that perform better on one <b>demographic</b> group at the expense of others, and is thus deemed unfair. While the eventual optimal tuning of an algorithm <b>can</b> thus depend on many factors, including the local system of laws and community values, the overall goal of this paper is to introduce and define the notions of bias, fairness, and appropriate use, as it pertains to <b>machine</b> learning in global health, and also to illustrate how a given <b>machine</b> learning ...", "dateLastCrawled": "2021-12-05T04:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) Explainable AI: A Review of <b>Machine</b> Learning Interpretability Methods", "url": "https://www.researchgate.net/publication/348032815_Explainable_AI_A_Review_of_Machine_Learning_Interpretability_Methods", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/348032815_Explainable_AI_A_Review_of_<b>Machine</b>...", "snippet": "<b>produce</b> <b>the same</b> <b>output</b> prediction as the original full-text input. These small pieces, called rationales, provide the necessary explanation and justi\ufb01cation for the <b>output</b> in terms", "dateLastCrawled": "2022-01-25T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Addressing Fairness, Bias, and Appropriate</b> Use of Artificial ...", "url": "https://europepmc.org/article/PMC/PMC8107824", "isFamilyFriendly": true, "displayUrl": "https://europepmc.org/article/PMC/PMC8107824", "snippet": "This situation <b>can</b> <b>produce</b> diagnostic tests that perform better on one <b>demographic</b> group at the expense of others, and is thus deemed unfair. While the eventual optimal tuning of an algorithm <b>can</b> thus depend on many factors, including the local system of laws and community values, the overall goal of this paper is to introduce and define the notions of bias, fairness, and appropriate use, as it pertains to <b>machine</b> learning in global health, and also to illustrate how a given <b>machine</b> learning ...", "dateLastCrawled": "2021-06-01T04:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Reprogramming Fairness: Affirmative Action in Algorithmic Criminal</b> ...", "url": "http://hrlr.law.columbia.edu/hrlr-online/reprogramming-fairness-affirmative-action-in-algorithmic-criminal-sentencing/", "isFamilyFriendly": true, "displayUrl": "hrlr.law.columbia.edu/hrlr-online/<b>reprogramming-fairness-affirmative-action-in</b>...", "snippet": "Though the general framework of affirmative action <b>has</b> not yet <b>been</b> applied to sentencing and algorithmic fairness, the work of scholars in this field supports the underlying logic of that framework and <b>can</b> be seen as building to this kind of solution. Moreover, there is space in the law for such an approach to work. While the current political environment may be inhospitable to any type of affirmative action proposal, adopting a more equitable definition of fairness for criminal sentencing ...", "dateLastCrawled": "2022-02-01T07:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "by topics | <b>Probabilistic Circuits</b>", "url": "https://arranger1044.github.io/probabilistic-circuits/topics", "isFamilyFriendly": true, "displayUrl": "https://arranger1044.github.io/<b>probabilistic-circuits</b>/topics", "snippet": "Recently there <b>has</b> <b>been</b> growing interest in learning probabilistic models that admit poly-<b>time</b> inference called tractable probabilistic models from data. Although they generalize poorly as <b>compared</b> to intractable models, they often yield more accurate estimates at prediction <b>time</b>. In this paper, we seek to further explore this trade-off between generalization performance and inference accuracy by proposing a novel, partially tractable representation called cutset Bayesian networks (CBNs ...", "dateLastCrawled": "2022-01-30T06:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>ALSO BY STEVEN PINKER</b> | Amanda Castello - Academia.edu", "url": "https://www.academia.edu/40560859/ALSO_BY_STEVEN_PINKER", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/40560859/<b>ALSO_BY_STEVEN_PINKER</b>", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-02-01T22:31:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Pandas <b>Machine</b> <b>Learning</b> Example", "url": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "isFamilyFriendly": true, "displayUrl": "https://groups.google.com/g/hslogb/c/-BvVGlSI3Ek", "snippet": "Regardless of your dataset, <b>demographic</b> <b>parity</b> is a <b>machine</b> <b>learning</b> algorithms. Data Munging It helps us to missing data of wedge form with another. Python with datetime module, i should equal to bring new example <b>machine</b>. Quite possibly the state important part clean the <b>machine</b> <b>learning</b> process is understanding the data you are working with and advantage it relates to reflect task you front to solve. Viewing the corresponding number of dropping down arrow illustrates that are not only all ...", "dateLastCrawled": "2022-01-24T06:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "An example of prediction which complies with <b>Demographic</b> <b>Parity</b> and ...", "url": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-demographic-parity-and-equalizes-group-wise-risks-in-the-context", "isFamilyFriendly": true, "displayUrl": "https://vertexdoc.com/doc/an-example-of-prediction-which-complies-with-<b>demographic</b>...", "snippet": "However, <b>Demographic</b> <b>Parity</b> and EGWR only define fairness on the group level and inspecting the individual level reveals a critical flow of this prediction rule. We have constrained our predictors to those that do not produce Disparate Treatment by prohibiting them from having the sensitive variable as direct input. Nevertheless, enforcing group level fairness constraints (such as DP and EGWR) forces the prediction rule to guess the sensitive attribute corresponding to a given feature vector", "dateLastCrawled": "2022-02-05T21:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "RStudio AI Blog: Starting to think about AI Fairness", "url": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness", "snippet": "Heidari et al. have written a paper comparing the three criteria \u2013 <b>demographic</b> <b>parity</b>, equality of opportunity, and predictive <b>parity</b> \u2013 to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the <b>analogy</b> is fascinating, it too assumes that we may take what is in the data at face value. In their likening predictive <b>parity</b> to luck egalitarianism, they have to go to especially great lengths, in ...", "dateLastCrawled": "2022-01-31T09:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Classification - Fairness and <b>machine</b> <b>learning</b>", "url": "https://fairmlbook.org/classification.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/classification.html", "snippet": "Simply put, the goal of classification is to determine a plausible value for an unknown variable Y given an observed variable X.For example, we might try to predict whether a loan applicant will pay back her loan by looking at various characteristics such as credit history, income, and net worth. Classification also applies in situations where the variable Y does not refer to an event that lies in the future. For example, we can try to determine if an image contains a cat by looking at the ...", "dateLastCrawled": "2022-02-03T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Mitigating Unwanted Biases with Adversarial <b>Learning</b> - arxiv-vanity.com", "url": "https://www.arxiv-vanity.com/papers/1801.07593/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1801.07593", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concerning <b>demographic</b> groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or income ...", "dateLastCrawled": "2021-10-04T02:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Mitigating Unwanted Biases with Adversarial <b>Learning</b>", "url": "http://www.m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf", "isFamilyFriendly": true, "displayUrl": "www.m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf", "snippet": "<b>Machine</b> <b>learning</b> is a tool for building models that accurately represent input training data. When undesired biases concern-ing <b>demographic</b> groups are in the training data, well-trained models will re\ufb02ect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously <b>learning</b> a predictor and an ad-versary. The input to the network X, here text or census data, produces a prediction Y, such as an <b>analogy</b> completion or in ...", "dateLastCrawled": "2022-01-23T20:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A Human-centric Approach to Fairness in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "snippet": "This includes measures such as <b>Demographic</b> <b>Parity</b> / Statistical <b>Parity</b> (Dwork et al., 2012), Equalized Odds Metric (Hardt et al., 2016) and Calibration within Groups (Chouldechova, 2017). They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are trying to test for equivalence. In another survey of fairness definitions, Verma &amp; Rubin (2018) listed 20 definitions of fairness, 13 belonging to ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Adversarial Approaches to Debiasing Word Embeddings", "url": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "isFamilyFriendly": true, "displayUrl": "https://web.stanford.edu/class/cs224n/reports/final_reports/report101.pdf", "snippet": "<b>Machine</b> <b>learning</b> for natural language processing (NLP) leverages valuable data from human language for useful downstream applications such as <b>machine</b> translation and sentiment analysis. Recent studies, however, have shown that training data in these applications are prone to harboring stereotypes and unwanted biases commonly exhibited in human language. Since NLP systems are designed to understand novel associations within training data, they are similarly vulnerable to propagating these ...", "dateLastCrawled": "2022-01-25T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "The <b>measure and mismeasure of fairness: a critical review</b> of fair ...", "url": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness/", "isFamilyFriendly": true, "displayUrl": "https://blog.acolyer.org/2020/02/03/measure-mismeasure-fairness", "snippet": "In case you\u2019re wondering where on earth I\u2019m going with this\u2026 it\u2019s a very stretched <b>analogy</b> I\u2019ve been playing with in my mind. One premise of many models of fairness in <b>machine</b> <b>learning</b> is that you can measure (\u2018prove\u2019) fairness of a <b>machine</b> <b>learning</b> model from within the system \u2013 i.e. from properties of the model itself and perhaps the data it is trained on. Beyond the questions of whether any one model of fairness is better or worse than another, I\u2019m coming to the ...", "dateLastCrawled": "2022-01-30T11:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Fighting Money Laundering with Statistics and <b>Machine</b> <b>Learning</b>: An ...", "url": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-machine-learning-an-introduction-and-review", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/fighting-money-laundering-with-statistics-and-<b>machine</b>...", "snippet": "Statistics and <b>machine</b> <b>learning</b> have long promised efficient and robust techniques for AML. So far, though, these remain to manifest [Grint2001]. One reason is that the academic literature is small and fragmented [Leite2019, Ngai2011]. With this paper, we aim to do three things. First, we propose a unified terminology to homogenize and associate research methodologies. Second, we review selected, exemplary methods. Third, we present recent <b>machine</b> <b>learning</b> concepts that have the potential to ...", "dateLastCrawled": "2022-01-28T21:12:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(demographic parity)  is like +(a machine that has been calibrated to produce the same output every time)", "+(demographic parity) is similar to +(a machine that has been calibrated to produce the same output every time)", "+(demographic parity) can be thought of as +(a machine that has been calibrated to produce the same output every time)", "+(demographic parity) can be compared to +(a machine that has been calibrated to produce the same output every time)", "machine learning +(demographic parity AND analogy)", "machine learning +(\"demographic parity is like\")", "machine learning +(\"demographic parity is similar\")", "machine learning +(\"just as demographic parity\")", "machine learning +(\"demographic parity can be thought of as\")", "machine learning +(\"demographic parity can be compared to\")"]}