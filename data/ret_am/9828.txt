{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Adaptive Newton <b>Method for Empirical Risk Minimization to Statistical</b> ...", "url": "https://proceedings.neurips.cc/paper/6262-adaptive-newton-method-for-empirical-risk-minimization-to-statistical-accuracy.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/6262-adaptive-newton-method-for-<b>empirical</b>-<b>risk</b>...", "snippet": "2 <b>Empirical</b> <b>risk</b> <b>minimization</b> We aim to solve <b>ERM</b> problems to their statistical accuracy. To state this problem formally consider an argument w 2 Rp, a random variable Z with realizations z and a convex loss function f(w;z). We want to \ufb01nd an argument w\u21e4 that minimizes the statistical average loss L(w):=E Z[f(w,Z)], w\u21e4:= argmin w L(w ...", "dateLastCrawled": "2022-01-11T20:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Diametrical <b>Risk</b> <b>Minimization</b>: theory and computations - Machine <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-021-06036-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-021-06036-0", "snippet": "The theoretical and <b>empirical</b> performance of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) often suffers when loss functions are poorly behaved with large Lipschitz moduli and spurious sharp minimizers. We propose and analyze a counterpart to <b>ERM</b> called Diametrical <b>Risk</b> <b>Minimization</b> (DRM), which accounts for worst-case <b>empirical</b> risks within neighborhoods in parameter space. DRM has generalization bounds that are independent of Lipschitz moduli for convex as well as nonconvex problems and it can be ...", "dateLastCrawled": "2021-12-24T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "9.520: Statistical <b>Learning</b> Theory and Applications ebruaryF 8th, 2010 ...", "url": "https://www.mit.edu/~9.520/scribe-notes/class02_scrb_sara.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mit.edu/~9.520/scribe-notes/class02_scrb_sara.pdf", "snippet": "would <b>like</b> our class of <b>learning</b> algorithms to be stable. 4 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> One classical and simple class of <b>learning</b> algorithms is the <b>empirical</b> <b>risk</b> <b>minimization</b> algorithm. Given a training set Sand a function space H, <b>empirical</b> <b>risk</b> <b>minimization</b> (Vapnik introduced the term) is the class of algorithms that look at Sand select f S ...", "dateLastCrawled": "2022-02-02T17:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient Episodic Memory</b> for Continual <b>Learning</b>", "url": "https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf", "snippet": "methods often employ the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle [Vapnik, 1998], where f is found by minimizing 1 j D trj P (x i;y)2 \u2018(f(x i);y i), where \u2018: YY! [0;1) is a loss function penalizing prediction errors. In practice, <b>ERM</b> often requires multiple passes over the training set. <b>ERM</b> is a major simpli\ufb01cation from what we deem as human <b>learning</b>. In stark contrast to <b>learning</b> machines, <b>learning</b> humans observe data as an ordered sequence, seldom observe the same example twice ...", "dateLastCrawled": "2022-01-30T18:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "regression - Why is <b>importance-weighted empirical risk minimization</b> ...", "url": "https://stats.stackexchange.com/questions/481783/why-is-importance-weighted-empirical-risk-minimization-finite-sample-biased", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/481783/why-is-importance-weighted-<b>empirical</b>...", "snippet": "The importance-weighted <b>empirical</b> <b>risk</b> is an unbiased estimator for the <b>risk</b> with respect to the target distribution, for any N. But it&#39;s difficult to say anything about the minimizer of the importance-weighted <b>empirical</b> <b>risk</b> for finite sample sizes. Let&#39;s abstract this a bit: Let F be a functional (i.e. takes in a function f and returns a number).", "dateLastCrawled": "2022-01-21T14:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Applications of <b>Empirical</b> Processes in <b>Learning</b> Theory: Algorithmic ...", "url": "http://cbcl.mit.edu/publications/theses/thesis-rakhlin.pdf", "isFamilyFriendly": true, "displayUrl": "cbcl.mit.edu/publications/theses/thesis-rakhlin.pdf", "snippet": "the stability of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). This algorithm is of central importance in <b>Learning</b> Theory. By studying the suprema of the <b>empirical</b> process, we prove that <b>ERM</b> over Donsker classes of functions is stable in the L1 norm. Hence, as the number of samples grows, it becomes less and less likely that a perturbation of o(p n) samples will result in a very di\ufb01erent <b>empirical</b> minimizer. Asymptotic rates of this stability are proved under metric entropy assumptions on the ...", "dateLastCrawled": "2021-12-03T22:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Chapter 10 Supervised Learning</b> | Introduction to Data Science", "url": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "snippet": "Metric <b>Learning</b>: <b>Like</b> similarity <b>learning</b>, ... Deals with the carriage of \u201c<b>experience</b>\u201d from one <b>learning</b> problem to another. A.k.a. cummulative <b>learning</b>, knowledge transfer, and meta <b>learning</b>. 10.1 Problem Setup. We now present the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) approach to supervised <b>learning</b>, a.k.a. M-estimation in the statistical literature. Remark. We do not discuss purely algorithmic approaches such as K-nearest neighbour and kernel smoothing due to space constraints. For a ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Lecture 2 Machine <b>Learning</b> Review - CMSC 35246: Deep <b>Learning</b>", "url": "https://home.ttic.edu/~shubhendu/Pages/Files/Lecture2_flat.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~shubhendu/Pages/Files/Lecture2_flat.pdf", "snippet": "\u2022 Formal Setup for Supervised <b>Learning</b> \u2022 <b>Empirical</b> <b>Risk</b>, <b>Risk</b>, Generalization \u2022 De ne and derive a linear model for Regression \u2022 Revise Regularization \u2022 De ne and derive a linear model for Classi cation \u2022 (Time permitting) Start with Feedforward Networks Lecture 2 Machine <b>Learning</b> Review CMSC 35246. Note: Most slides in this presentation are adapted from, or taken (with permission) from slides by Professor Gregory Shakhnarovich for his TTIC 31020 course Lecture 2 Machine <b>Learning</b> ...", "dateLastCrawled": "2022-01-29T18:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is an intuitive interpretation of the loss function [math] l ...", "url": "https://www.quora.com/What-is-an-intuitive-interpretation-of-the-loss-function-l-theta-x-log-P_-theta-x-when-its-used-in-Empirical-Risk-Minimization-ERM", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-an-intuitive-interpretation-of-the-loss-function-l-theta...", "snippet": "Answer (1 of 5): TL;DR -log_2(P) represents the number of binary decisions needed to branch to P^{-1} states in a partition. First thing we&#39;re taught in probability is that P(\\textrm{event})=\\dfrac{\\textrm{states with event}}{\\textrm{size of state partition}} Then what is the reciprocal (invers...", "dateLastCrawled": "2022-01-12T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Mathematics of Machine <b>Learning</b>: An introduction", "url": "https://www.math.pku.edu.cn/puremath_en/docs/2018-10/20181010175515946936.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.math.pku.edu.cn/puremath_en/docs/2018-10/20181010175515946936.pdf", "snippet": "creating machines that can improve from <b>experience</b> and interaction. It relies upon mathematical optimization, statistics, and algorithm design. Rapid <b>empirical</b> success in this field currently outstrips mathematical un-derstanding. This elementary article sketches the basic framework of ma-chine <b>learning</b> and hints at the open mathematical problems in it. An updated version of this article and related articles can be found on the author\u2019s webpage. MSC: 68-02, 68Q99, 68T05. The dictionary ...", "dateLastCrawled": "2022-02-01T19:42:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "deep <b>learning</b> - Why is <b>empirical risk minimization</b> prone to overfitting ...", "url": "https://stats.stackexchange.com/questions/272411/why-is-empirical-risk-minimization-prone-to-overfitting", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/272411/why-is-<b>empirical-risk-minimization</b>...", "snippet": "The overfitting of the <b>empirical</b> <b>risk</b> is especially prominent in cases of a small training set. When the data don&#39;t contain enough information to learn the underlying pattern, more regularization is needed to fill in the gap. In the specific case of the Deep <b>learning</b> the case is not so clear. Especially with very large nets, it is virtually ...", "dateLastCrawled": "2022-01-28T04:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Chapter 10 Supervised Learning</b> | Introduction to Data Science", "url": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "snippet": "Reinforcement <b>learning</b>: <b>Similar</b> to active <b>learning</b>, in that the machine may query for labels. Different from active <b>learning</b>, ... Deals with the carriage of \u201c<b>experience</b>\u201d from one <b>learning</b> problem to another. A.k.a. cummulative <b>learning</b>, knowledge transfer, and meta <b>learning</b>. 10.1 Problem Setup. We now present the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) approach to supervised <b>learning</b>, a.k.a. M-estimation in the statistical literature. Remark. We do not discuss purely algorithmic approaches ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>mixup: BEYOND EMPIRICAL RISK MINIMIZATION</b>", "url": "https://openreview.net/pdf?id=r1Ddp1-Rb", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=r1Ddp1-Rb", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle (Vapnik, 1998). Second, the size of these state-of-the- art neural networks scales linearly with the number of training examples. For instance, the network of Springenberg et al. (2015) used 106 parameters to model the 5104 images in the CIFAR-10 dataset, the network of (Simonyan &amp; Zisserman, 2015) used 108 parameters to model the 106 images in the ImageNet-2012 dataset, and the network of Chelba et al. (2013) used 21010 parameters to model the 109 ...", "dateLastCrawled": "2022-02-01T17:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient Episodic Memory</b> for Continual <b>Learning</b>", "url": "https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf", "snippet": "methods often employ the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle [Vapnik, 1998], where f is found by minimizing 1 j D trj P (x i;y)2 \u2018(f(x i);y i), where \u2018: YY! [0;1) is a loss function penalizing prediction errors. In practice, <b>ERM</b> often requires multiple passes over the training set. <b>ERM</b> is a major simpli\ufb01cation from what we deem as human <b>learning</b>. In stark contrast <b>to learning</b> machines, <b>learning</b> humans observe data as an ordered sequence, seldom observe the same example twice ...", "dateLastCrawled": "2022-01-30T18:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Diametrical <b>Risk</b> <b>Minimization</b>: theory and computations - Machine <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-021-06036-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-021-06036-0", "snippet": "The theoretical and <b>empirical</b> performance of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) often suffers when loss functions are poorly behaved with large Lipschitz moduli and spurious sharp minimizers. We propose and analyze a counterpart to <b>ERM</b> called Diametrical <b>Risk</b> <b>Minimization</b> (DRM), which accounts for worst-case <b>empirical</b> risks within neighborhoods in parameter space. DRM has generalization bounds that are independent of Lipschitz moduli for convex as well as nonconvex problems and it can be ...", "dateLastCrawled": "2021-12-24T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Machine <b>Learning</b> and <b>Optimization</b> - NYU Courant", "url": "https://cims.nyu.edu/~munoz/files/ml_optimization.pdf", "isFamilyFriendly": true, "displayUrl": "https://cims.nyu.edu/~munoz/files/ml_<b>optimization</b>.pdf", "snippet": "This is known as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and in a sense is the raw <b>optimization</b> part of machine <b>learning</b>, as we will see we will require something more than that. 3 <b>Learning</b> Guarantees De nition 3. Given a set of functions G= fg: Z!Rgand a sample S= (z i)n =1 the <b>empirical</b> Rademacher complexity of Gis de ned by: &lt; S(G) = E \u02d9 1 n sup ...", "dateLastCrawled": "2022-01-29T21:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "regression - Why is <b>importance-weighted empirical risk minimization</b> ...", "url": "https://stats.stackexchange.com/questions/481783/why-is-importance-weighted-empirical-risk-minimization-finite-sample-biased", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/481783/why-is-importance-weighted-<b>empirical</b>...", "snippet": "The importance-weighted <b>empirical</b> <b>risk</b> is an unbiased estimator for the <b>risk</b> with respect to the target distribution, for any N. But it&#39;s difficult to say anything about the minimizer of the importance-weighted <b>empirical</b> <b>risk</b> for finite sample sizes. Let&#39;s abstract this a bit: Let F be a functional (i.e. takes in a function f and returns a number).", "dateLastCrawled": "2022-01-21T14:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "9.520: Statistical <b>Learning</b> Theory and Applications ebruaryF 8th, 2010 ...", "url": "https://www.mit.edu/~9.520/scribe-notes/class02_scrb_sara.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mit.edu/~9.520/scribe-notes/class02_scrb_sara.pdf", "snippet": "4 <b>Empirical</b> <b>Risk</b> <b>Minimization</b> One classical and simple class of <b>learning</b> algorithms is the <b>empirical</b> <b>risk</b> <b>minimization</b> algorithm. Given a training set Sand a function space H, <b>empirical</b> <b>risk</b> <b>minimization</b> (Vapnik introduced the term) is the class of algorithms that look at Sand select f S in the hypothesis space that minimizes the <b>empirical</b> error: f", "dateLastCrawled": "2022-02-02T17:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Mathematics of Machine <b>Learning</b>: An introduction", "url": "https://www.math.pku.edu.cn/puremath_en/docs/2018-10/20181010175515946936.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.math.pku.edu.cn/puremath_en/docs/2018-10/20181010175515946936.pdf", "snippet": "creating machines that can improve from <b>experience</b> and interaction. It relies upon mathematical optimization, statistics, and algorithm design. Rapid <b>empirical</b> success in this field currently outstrips mathematical un-derstanding. This elementary article sketches the basic framework of ma-chine <b>learning</b> and hints at the open mathematical problems in it. An updated version of this article and related articles can be found on the author\u2019s webpage. MSC: 68-02, 68Q99, 68T05. The dictionary ...", "dateLastCrawled": "2022-02-01T19:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Facial Expression Recognition of Instructor Using</b> Deep Features and ...", "url": "https://www.hindawi.com/journals/cin/2021/5570870/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/cin/2021/5570870", "snippet": "ELM, contrary to the conventional backpropagation algorithm, is based on <b>empirical</b> <b>risk</b> reduction technique and needs only one iteration for its <b>learning</b> process. This property has made this algorithm have fast <b>learning</b> speed and good generalization performance yielding optimal and unique solution. For RELM, the regularization term helps in reducing overfitting without increasing computational time making a generalized instructor expression prediction model.", "dateLastCrawled": "2022-02-02T11:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Establishing connection between <b>ERM</b> (<b>Empirical</b> <b>Risk</b> <b>Minimization</b>) and MLE", "url": "https://stats.stackexchange.com/questions/464622/establishing-connection-between-erm-empirical-risk-minimization-and-mle", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/464622/establishing-connection-between-<b>erm</b>...", "snippet": "$\\begingroup$ He shows MLE, that is a way to infer an unknown parameter with an estimator which is a function of the observed data, that is $\\hat{\\theta} \\in \\Theta^{\\mathcal{X}}$, function from $\\mathcal{X} \\rightarrow \\Theta$. In section 2.3.4 he states &quot;Most model classes will have some parameters $\\theta \\in \\Theta$ that the <b>learning</b> algorithm will adjust to fit the data.", "dateLastCrawled": "2022-01-09T10:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "deep <b>learning</b> - Why is <b>empirical risk minimization</b> prone to overfitting ...", "url": "https://stats.stackexchange.com/questions/272411/why-is-empirical-risk-minimization-prone-to-overfitting", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/272411/why-is-<b>empirical-risk-minimization</b>...", "snippet": "According to Chapter 8 of the book Deep <b>Learning</b>, &quot;..<b>empirical risk minimization</b> is prone to overfitting. models with high capacity <b>can</b> simply memorize the training se.&quot; My question why is it so? Models with high capacity <b>can</b> also memorise the training set when we have the true distribution and reduce the true cost function. deep-<b>learning</b> <b>risk</b> extreme-value. Share. Cite. Improve this question. Follow asked Apr 7 &#39;17 at 8:04. user10024395 user10024395. 1 $\\endgroup$ 1 $\\begingroup$ I <b>thought</b> ...", "dateLastCrawled": "2022-01-28T04:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "svm - difference between <b>empirical</b> <b>risk</b> <b>minimization</b> and structural ...", "url": "https://datascience.stackexchange.com/questions/66729/difference-between-empirical-risk-minimization-and-structural-risk-minimization", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/66729", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) is a principle in statistical <b>learning</b> theory that defines a family of <b>learning</b> algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true &quot;<b>risk</b>&quot;) because we don&#39;t know the true distribution of data that the algorithm will work on, but we <b>can</b> instead measure its performance on a known set of training data (the &quot;<b>empirical</b>&quot; <b>risk</b>).", "dateLastCrawled": "2022-01-24T04:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Adaptive Newton Method for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> to Statistical ...", "url": "https://slideblast.com/adaptive-newton-method-for-empirical-risk-minimization-to-statistical-_59b73a4a1723dd731a2c7a55.html", "isFamilyFriendly": true, "displayUrl": "https://slideblast.com/adaptive-newton-method-for-<b>empirical</b>-<b>risk</b>-<b>minimization</b>-to...", "snippet": "A hallmark of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) on large datasets is that evaluating descent directions requires a complete pass over the dataset. Since this is undesirable due to the large number of training samples, stochastic optimization algorithms with descent directions estimated from a subset of samples are the method of choice. First order stochastic optimization has a long history [20, 19] but the last decade has seen fundamental progress in developing alternatives with faster ...", "dateLastCrawled": "2021-10-28T06:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Adaptive Newton <b>Method for Empirical Risk Minimization</b> to ...", "url": "https://www.researchgate.net/publication/303485105_Adaptive_Newton_Method_for_Empirical_Risk_Minimization_to_Statistical_Accuracy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/303485105_Adaptive_Newton_Method_for...", "snippet": "PDF | We consider <b>empirical</b> <b>risk</b> <b>minimization</b> for large-scale datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton&#39;s method with... | Find, read and cite all the research ...", "dateLastCrawled": "2021-12-24T21:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Training genetic programming classifiers by vicinal-<b>risk</b> <b>minimization</b> ...", "url": "https://link.springer.com/article/10.1007%2Fs10710-014-9222-4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10710-014-9222-4", "snippet": "A straightforward illustration of the deficiencies of <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) ... <b>can</b> be approximated by placing a vicinity function on each training datum\u2014this process <b>can</b> <b>be thought</b> of as either resampling or, equivalently, interpolating \\(\\fancyscript{D}\\). Since the shortcomings of 0/1 loss are due to its discrete nature, smoothing the training set will have the effect of stabilizing the training process. Vapnik described two possible types of vicinity functions, hard and ...", "dateLastCrawled": "2021-12-10T13:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Chapter 10 Supervised Learning</b> | Introduction to Data Science", "url": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "snippet": "We now present the <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) approach to supervised <b>learning</b>, a.k.a. M-estimation in the statistical literature. Remark. We do not discuss purely algorithmic approaches such as K-nearest neighbour and kernel smoothing due to space constraints. For a broader review of supervised <b>learning</b>, see the Bibliographic Notes. Example 10.1 (Spam classification) Consider the problem of predicting if a mail is spam or not based on its attributes: length, number of exclamation ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) An <b>Empirical</b> Analysis of the Impact of Data Augmentation on ...", "url": "https://www.researchgate.net/publication/342027025_An_Empirical_Analysis_of_the_Impact_of_Data_Augmentation_on_Knowledge_Distillation", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/342027025_An_<b>Empirical</b>_Analysis_of_the_Impact...", "snippet": "PDF | Generalization Performance of Deep <b>Learning</b> models trained using the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> <b>can</b> be improved significantly by using Data... | Find, read and cite all the research you ...", "dateLastCrawled": "2021-10-21T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is an intuitive interpretation of the loss function [math] l ...", "url": "https://www.quora.com/What-is-an-intuitive-interpretation-of-the-loss-function-l-theta-x-log-P_-theta-x-when-its-used-in-Empirical-Risk-Minimization-ERM", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-an-intuitive-interpretation-of-the-loss-function-l-theta...", "snippet": "Answer (1 of 5): TL;DR -log_2(P) represents the number of binary decisions needed to branch to P^{-1} states in a partition. First thing we&#39;re taught in probability is that P(\\textrm{event})=\\dfrac{\\textrm{states with event}}{\\textrm{size of state partition}} Then what is the reciprocal (invers...", "dateLastCrawled": "2022-01-12T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Complementary Learning for Overcoming Catastrophic</b> Forgetting Using ...", "url": "https://www.ijcai.org/Proceedings/2019/0463.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/2019/0463.pdf", "snippet": "dard classical <b>learning</b> problem. The agent <b>can</b> solve for the optimal network weight parameters using standard <b>em-pirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>), ^(t) = arg min e^ = arg min P i L d(f (x (t) i); y (t) i), whereL d( ) is a proper loss function, e.g., cross entropy. Given large enough num-ber of labeled data pointsn t, the model trained on a sin-", "dateLastCrawled": "2022-01-23T15:26:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Diametrical <b>Risk</b> <b>Minimization</b>: theory and computations - Machine <b>Learning</b>", "url": "https://link.springer.com/article/10.1007/s10994-021-06036-0", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10994-021-06036-0", "snippet": "The theoretical and <b>empirical</b> performance of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) often suffers when loss functions are poorly behaved with large Lipschitz moduli and spurious sharp minimizers. We propose and analyze a counterpart to <b>ERM</b> called Diametrical <b>Risk</b> <b>Minimization</b> (DRM), which accounts for worst-case <b>empirical</b> risks within neighborhoods in parameter space. DRM has generalization bounds that are independent of Lipschitz moduli for convex as well as nonconvex problems and it <b>can</b> be ...", "dateLastCrawled": "2021-12-24T06:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "machine <b>learning</b> - <b>Empirical</b> <b>Risk</b> <b>Minimization</b>: <b>empirical</b> vs expected ...", "url": "https://stats.stackexchange.com/questions/265551/empirical-risk-minimization-empirical-vs-expected-and-true-vs-surrogate", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/265551/<b>empirical</b>-<b>risk</b>-<b>minimization</b>-<b>empirical</b>...", "snippet": "In Tie-Yan Liu&#39;s book, he says that in a statistical <b>learning</b> theory for <b>empirical</b> <b>risk</b> <b>minimization</b> has to observe four <b>risk</b> functions: We also need to de\ufb01ne the true loss of the <b>learning</b> problem, which serves as a reference to study the properties of different surrogate loss functions used by various <b>learning</b> algorithms.", "dateLastCrawled": "2022-02-03T02:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Diametrical <b>Risk</b> <b>Minimization</b>: Theory and Computations | DeepAI", "url": "https://deepai.org/publication/diametrical-risk-minimization-theory-and-computations", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/diametrical-<b>risk</b>-<b>minimization</b>-theory-and-computations", "snippet": "The theoretical and <b>empirical</b> performance of <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) often suffers when loss functions are poorly behaved with large Lipschitz moduli and spurious sharp minimizers. We propose and analyze a counterpart to <b>ERM</b> called Diametrical <b>Risk</b> <b>Minimization</b> (DRM), which accounts for worst-case <b>empirical</b> risks within neighborhoods in parameter space. DRM has generalization bounds that are independent of Lipschitz moduli for convex as well as nonconvex problems and it <b>can</b> be ...", "dateLastCrawled": "2021-11-24T00:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Gradient Episodic Memory</b> for Continual <b>Learning</b>", "url": "https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf", "snippet": "methods often employ the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle [Vapnik, 1998], where f is found by minimizing 1 j D trj P (x i;y)2 \u2018(f(x i);y i), where \u2018: YY! [0;1) is a loss function penalizing prediction errors. In practice, <b>ERM</b> often requires multiple passes over the training set. <b>ERM</b> is a major simpli\ufb01cation from what we deem as human <b>learning</b>. In stark contrast <b>to learning</b> machines, <b>learning</b> humans observe data as an ordered sequence, seldom observe the same example twice ...", "dateLastCrawled": "2022-01-30T18:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) Neutralized <b>Empirical</b> <b>Risk</b> <b>Minimization</b> with Generalization ...", "url": "https://www.researchgate.net/publication/283619745_Neutralized_Empirical_Risk_Minimization_with_Generalization_Neutrality_Bound", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/283619745_Neutralized_<b>Empirical</b>_<b>Risk</b>...", "snippet": "In this work, we introduce a novel <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) framework for supervised <b>learning</b>, neutralized <b>ERM</b> (NERM) that ensures that any classifiers obtained <b>can</b> be guaranteed to be ...", "dateLastCrawled": "2021-12-13T04:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "STORM: Foundations of End-to-End <b>Empirical</b> <b>Risk</b> <b>Minimization</b> on the ...", "url": "https://deepai.org/publication/storm-foundations-of-end-to-end-empirical-risk-minimization-on-the-edge", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/storm-foundations-of-end-to-end-<b>empirical</b>-<b>risk</b>...", "snippet": "<b>Empirical</b> <b>risk</b> <b>minimization</b> is perhaps the most influential idea in statistical <b>learning</b>, with applications to nearly all scientific and technical domains in the form of regression and classification models. To analyze massive streaming datasets in distributed computing environments, practitioners increasingly prefer to deploy regression models on edge rather than in the cloud. By keeping data on edge devices, we minimize the energy, communication, and data security <b>risk</b> associated with the ...", "dateLastCrawled": "2022-01-08T00:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Proper Losses for <b>Learning</b> with Example-Dependent Costs", "url": "http://proceedings.mlr.press/v94/hepburn18a/hepburn18a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v94/hepburn18a/hepburn18a.pdf", "snippet": "rior probability map ^ (x) using the training set Sfollowing the <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle (Devroye et al.,1996), R emp= 1 K XK k=1 c(k) i (k);y (5) where i(k) = I ^ (x(k)) q(x(k)) is the classi cation for the k-th example following ^ . 3. Proper losses and example-dependent costs As the expression in Eq. (1) is neither convex ...", "dateLastCrawled": "2021-11-22T04:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Multi-Observation Elicitation</b> - Proceedings of Machine <b>Learning</b> Research", "url": "http://proceedings.mlr.press/v65/casalaina-martin17a/casalaina-martin17a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v65/casalaina-martin17a/casalaina-martin17a.pdf", "snippet": "of property elicitation or in machine <b>learning</b> more broadly. As <b>compared</b> to traditional loss func-tions that take only a single data point, these multi-observation loss functions <b>can</b> in some cases drastically reduce the dimensionality of the hypothesis required. In elicitation, this corresponds to requiring many fewer reports; in <b>empirical</b> <b>risk</b> <b>minimization</b>, it corresponds to algorithms on a hypothesis space of much smaller dimension. We explore some examples of the tradeoff between ...", "dateLastCrawled": "2021-12-31T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Chapter 10 Supervised Learning</b> | Introduction to Data Science", "url": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "isFamilyFriendly": true, "displayUrl": "https://bookdown.org/ronsarafian/IntrotoDS/supervised.html", "snippet": "<b>Chapter 10. Supervised Learning</b>. Machine <b>learning</b> is very similar to statistics, but it is certainly not the same. As the name suggests, in machine <b>learning</b> we want machines to learn. This means that we want to replace hard-coded expert algorithm, with data-driven self-learned algorithm. There are many <b>learning</b> setups, that depend on what ...", "dateLastCrawled": "2022-02-03T00:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Facial Expression Recognition of Instructor Using</b> Deep Features and ...", "url": "https://www.hindawi.com/journals/cin/2021/5570870/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/cin/2021/5570870", "snippet": "ELM, contrary to the conventional backpropagation algorithm, is based on <b>empirical</b> <b>risk</b> reduction technique and needs only one iteration for its <b>learning</b> process. This property has made this algorithm have fast <b>learning</b> speed and good generalization performance yielding optimal and unique solution. For RELM, the regularization term helps in reducing overfitting without increasing computational time making a generalized instructor expression prediction model.", "dateLastCrawled": "2022-02-02T11:09:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Computational and Statistical <b>Learning</b> Theory", "url": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "isFamilyFriendly": true, "displayUrl": "https://home.ttic.edu/~nati/Teaching/TTIC31120/2015/Lecture17.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) / Sample Average Approximation (SAA): Collect sample z1UYU zm ... SGD for <b>Machine</b> <b>Learning</b> Initialize S 4 L r At iteration t: Draw T \u00e7\u00e1U \u00e71\u00de If U \u00e7 S \u00e7 \u00e1\u00f6 T \u00e7 O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00e7\u00f6 T \u00e7 else: S \u00e7 &gt; 5 Z S \u00e7 Return S % \u00cd L 5 \u00cd \u00c3 \u00cd S \u00e7 \u00e7 @ 5 Draw T 5\u00e1U 5 \u00e1\u00e5\u00e1 T \u00e0 \u00e1U \u00e0 1\u00de Initialize S 4 L r At iteration t: Pick E \u00d0 s\u00e5I at random If U \u00dc S \u00e7 \u00e1\u00f6 T \u00dc O s\u00e1 S \u00e7 &gt; 5 Z S \u00e7 E\u00df \u00e7U \u00dc\u00f6 T \u00dc else: S \u00e7 &gt; 5 Z S \u00e7 S \u00e7 &gt; 5 Z ...", "dateLastCrawled": "2022-01-26T03:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Basics of <b>Machine</b> <b>Learning</b>", "url": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_learning.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.duke.edu/spring21/compsci527/slides/s_04_deep_<b>learning</b>.pdf", "snippet": "This is called <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) COMPSCI 527 \u2014 Computer Vision Basics of <b>Machine</b> <b>Learning</b> 15/26. Loss and <b>Risk</b> <b>Machine</b> <b>Learning</b> and the Statistical <b>Risk</b> <b>ERM</b>: w^ 2argmin w2R m L T(w) In <b>machine</b> <b>learning</b>, we go much farther: We also want h to do well on previously unseen inputs To relate past and future data, assume that all data comes from the same joint probability distribution p(x;y) p is called the generative data model or just model The goal of <b>machine</b> <b>learning</b> is to ...", "dateLastCrawled": "2021-11-06T09:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Statistical <b>Learning</b> Theory and the C-Loss cost function", "url": "http://www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "isFamilyFriendly": true, "displayUrl": "www.cnel.ufl.edu/courses/EEL6814/closs.pdf", "snippet": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle Let us consider a <b>learning</b> <b>machine</b> x,d are real r.v. with joint distribution P(x,y). F(x) is a function of some parameters w, i.e. f(x,w). d d. <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) principle How can we find the possible best <b>learning</b> <b>machine</b> that generalizes for unseen data from the same distribution? Define the <b>Risk</b> functional as L(.) is called the Loss function, and minimize it w.r.t. w achieving the best possible loss. But we can not do this ...", "dateLastCrawled": "2022-01-28T20:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Lecture 1: Reinforcement <b>Learning</b>: What and Why?", "url": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "isFamilyFriendly": true, "displayUrl": "https://imada.sdu.dk/~kandemir/rl-lecture1.pdf", "snippet": "<b>machine</b> <b>learning</b> and is referred to as <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>). 3 Challenges of reinforcement <b>learning</b> Consider the cart pole balancing problem, where a cart carrying an unactuated pole \ufb02oats on a straight horizontal track. The cart is actuated by a torque applied either to the right or the left direction. Seeherefor a real cart ...", "dateLastCrawled": "2021-09-30T12:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Empirical</b> <b>Risk</b> <b>Minimization</b> and Stochastic Gradient Descent for ...", "url": "http://proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v89/veitch19a/veitch19a.pdf", "snippet": "where F(Sn) is the <b>empirical</b> distribution.2 The <b>ERM</b> dogma is to select the predictor \u03c0\u02c6\u03b8 n given by \u02c6\u03b8 n = argmin\u03b8 R\u02c6(\u03b8,Sn). That is, the objective function that de\ufb01nes <b>learning</b> is the <b>empirical</b> <b>risk</b>. <b>ERM</b> has two useful properties. (1) It provides a prin-cipled framework for de\ufb01ning new <b>machine</b> <b>learning</b> methods. In particular, when ...", "dateLastCrawled": "2021-09-18T06:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Topics in <b>Machine</b> <b>Learning</b> (TIML-09)", "url": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "isFamilyFriendly": true, "displayUrl": "https://www.cse.iitb.ac.in/~saketh/teaching/cs689.html", "snippet": "Introduction to Statistical <b>Learning</b> Theory (SLT): Definitions of loss function, <b>risk</b>, <b>empirical</b> <b>risk</b>, motivation for <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) Further Reading, Supplementary: Jan 12: Consistency of <b>ERM</b>, Sufficient condition for <b>ERM</b> as one-sided uniform convergence, Analysis for finite sets of functions and extensions to general case using Symmetrization trick, Shattering Coeff. Further Reading, Supplementary: Jan 15: Shattering coeff., growth function, VC dimension, Annealed Entropy ...", "dateLastCrawled": "2022-01-11T09:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>GitHub</b> - gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Loss function 3.1 Categories 3.2 <b>Empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) and Structural <b>risk</b> <b>minimization</b> (SRM) 3.2.1 MLE 3.2.2 MAP 4. Evaluation metrics 4.1 Accuracy, precision, recall and F1 4.2 ROC, AUC 4.3 BLEU 5. Sample projects 6. Classical questions 6.1 Why is logistic regression a generalized linear model ?", "dateLastCrawled": "2021-09-12T01:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "[2006.09461] Robust <b>Compressed Sensing using Generative Models</b> - arXiv", "url": "https://arxiv.org/abs/2006.09461", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/abs/2006.09461", "snippet": "Classical recovery approaches such as <b>empirical</b> <b>risk</b> <b>minimization</b> (<b>ERM</b>) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm ...", "dateLastCrawled": "2021-06-27T11:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Support Vector Machines: Theory and Applications</b>", "url": "https://www.researchgate.net/publication/221621494_Support_Vector_Machines_Theory_and_Applications", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221621494_Support_Vector_<b>Machine</b>s", "snippet": "The principle used is <b>Empirical</b> <b>Risk</b> <b>Minimization</b> (<b>ERM</b>) over a set of possible functions, called hypothesis space. Formally this can be written as minimizing the <b>empirical</b> . error: \u2211 = l. 1 i. x ...", "dateLastCrawled": "2022-02-02T02:47:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ToyotaTechnologicalInstituteatChicago UniversityofTexasatAustin surbhi ...", "url": "https://arxiv.org/pdf/2005.07652.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/2005.07652.pdf", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, \u02c6h \u2208 RERM U(S) ,argmin h\u2208H 1 m Xm i=1 sup z\u2208U(x) 1 [h(z) 6= y]. In this paper, we provide necessary and su\ufb03cient conditions on perturbation sets U ...", "dateLastCrawled": "2021-07-27T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Efficiently Learning Adversarially Robust Halfspaces with</b> Noise | DeepAI", "url": "https://deepai.org/publication/efficiently-learning-adversarially-robust-halfspaces-with-noise", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>efficiently-learning-adversarially-robust-halfspaces</b>...", "snippet": "<b>Just as empirical risk minimization (ERM</b>) is central for non-robust PAC <b>learning</b>, a core component of adversarially robust <b>learning</b> is minimizing the robust empirical risk on a dataset S, ^ h \u2208 R E R M U ( S ) \u225c argmin h \u2208 H 1 m m \u2211 i = 1 sup z \u2208 U ( x ) 1 [ h ( z ) \u2260 y ] .", "dateLastCrawled": "2021-12-05T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Efficiently <b>Learning</b> Adversarially Robust Halfspaces with Noise", "url": "http://proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "isFamilyFriendly": true, "displayUrl": "proceedings.mlr.press/v119/montasser20a/montasser20a.pdf", "snippet": "remains a major challenge in <b>machine</b> <b>learning</b>. A line of work has shown that predictors learned by deep neural networks are not robust to adversarial examples (Szegedy et al.,2014;Biggio et al.,2013;Goodfellow et al.,2015). This has led to a long line of research studying different aspects of robustness to adversarial examples. In this paper, we consider the problem of distribution-independent <b>learning</b> of halfspaces that are robust to ad-versarial examples at test time, also referred to as ...", "dateLastCrawled": "2021-11-21T12:03:00.0000000Z", "language": "en", "isNavigational": false}], [], []], "all_bing_queries": ["+(empirical risk minimization (erm))  is like +(learning by experience)", "+(empirical risk minimization (erm)) is similar to +(learning by experience)", "+(empirical risk minimization (erm)) can be thought of as +(learning by experience)", "+(empirical risk minimization (erm)) can be compared to +(learning by experience)", "machine learning +(empirical risk minimization (erm) AND analogy)", "machine learning +(\"empirical risk minimization (erm) is like\")", "machine learning +(\"empirical risk minimization (erm) is similar\")", "machine learning +(\"just as empirical risk minimization (erm)\")", "machine learning +(\"empirical risk minimization (erm) can be thought of as\")", "machine learning +(\"empirical risk minimization (erm) can be compared to\")"]}