{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Deep Learning is Not As Impressive As you Think, It&#39;s Mere ...", "url": "https://gowrishankar.info/blog/deep-learning-is-not-as-impressive-as-you-think-its-mere-interpolation/", "isFamilyFriendly": true, "displayUrl": "https://gowrishankar.info/blog/deep-learning-is-not-as-impressive-as-you-think-its...", "snippet": "You Don\u2019t Understand Neural Networks Until You Understand the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> by Andre Ye, Jul 1, 2020. Yann LeCun - Replying to Steven Pinker. Jun 29, 2021. This is another piece that basically says \u201cdeep learning is not as impressive as you think because it\u2019s mere interpolation resulting from glorified <b>curve</b> fitting ...", "dateLastCrawled": "2021-12-25T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Neural Networks are Function Approximation</b> Algorithms", "url": "https://machinelearningmastery.com/neural-networks-are-function-approximators/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>neural-networks-are-function</b>-approximators", "snippet": "\u2026 the <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feedforward network with a linear output layer and at least one hidden layer with any \u201csquashing\u201d activation function (such as the logistic sigmoid activation function) can approximate any [\u2026] function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units", "dateLastCrawled": "2022-01-30T03:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Deep learning theory</b> lecture notes - Matus Telgarsky.", "url": "https://mjt.cs.illinois.edu/dlt/", "isFamilyFriendly": true, "displayUrl": "https://mjt.cs.illinois.edu/dlt", "snippet": "But our goal is <b>approximation</b> over a distribution of <b>points</b>. We will not get any nice <b>theorem</b> that says, roughly: \u201cthe exact complexity of shallow <b>approximation</b> depends on this function of the first \\mathcal{O}(d) derivatives\u201d (see also (Yarotsky 2016) for the deep case). This is part of why I <b>like</b> discussing the univariate case, where we ...", "dateLastCrawled": "2022-02-03T07:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Neural networks <b>with a continuous squashing function in the</b> output are ...", "url": "https://www.researchgate.net/publication/12333968_Neural_networks_with_a_continuous_squashing_function_in_the_output_are_universal_approximators", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/12333968_Neural_networks_with_a_continuous...", "snippet": "<b>Universal</b> <b>approximation</b> <b>theorem</b> Mathematical demonstrations of NNs ability to approximate functions find their origins in works from the early 90&#39;s [17, 40, 51,73,98], with the fundamental ...", "dateLastCrawled": "2022-01-16T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Using Radial Basis <b>Function Networks for Function Approximation</b> and ...", "url": "https://www.hindawi.com/journals/isrn/2012/324194/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/isrn/2012/324194", "snippet": "Another popular RBF for <b>universal</b> <b>approximation</b> is the thin-plate spline function , which is selected from <b>a curve</b>-fitting perspective . The thin-plate spline is the solution when fitting a surface through a <b>set</b> of <b>points</b> and by using a roughness penalty . It diverges at infinity and is negative over the region of \ud835\udc5f \u2208 (0, 1). However, for ...", "dateLastCrawled": "2022-01-31T11:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Optimization and Best Fitting Curves", "url": "https://mathstat.slu.edu/~may/ExcelCalculus/sec-6-4-OptimizationBestFitCurves.html", "isFamilyFriendly": true, "displayUrl": "https://mathstat.slu.edu/~may/ExcelCalculus/sec-6-4-OptimizationBestFit<b>Curves</b>.html", "snippet": "We consider a data <b>set</b> of 3 <b>points</b>, \\({(1,0),(3,5),(6,5)}\\) and a line that we will use to predict the y-value given the x-value, \\(\\predicted(x)=x/2 +1\\text{.}\\) We want to determine how well the line matches that data. For each point, \\((x_0,y_0)\\text{,}\\) in the <b>set</b> we start by <b>finding</b> the corresponding point, \\((x_0,\\predicted(x_0 ))\\text{,}\\) on the line. This gives us a <b>set</b> of predicted <b>points</b>, \\({(1,1.5),(3,2.5),(6,4)}\\text{.}\\) For each point we now compute the difference between the ...", "dateLastCrawled": "2022-02-02T07:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Universal</b> \u03b5-approximators for integrals | Request PDF", "url": "https://www.researchgate.net/publication/242086140_Universal_e-approximators_for_integrals", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/242086140_<b>Universal</b>_e-approximators_for_integrals", "snippet": "A (j, k)-coreset for projective clustering is a small <b>set</b> of <b>points</b> that yields a (1 + \u03b5)-<b>approximation</b> to the sum of squared distances from the n rows of A to any <b>set</b> of k affine subspaces, each ...", "dateLastCrawled": "2021-12-17T12:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Questions on Numerical Solution Methods", "url": "http://www.personal.psu.edu/jhm/f90/questions/meth.html", "isFamilyFriendly": true, "displayUrl": "www.personal.psu.edu/jhm/f90/questions/meth.html", "snippet": "&quot;<b>Fits</b>&quot; a line or <b>curve</b> to a <b>set</b> of data <b>points</b>. By now you should know that with 2 <b>points</b> you can exactly determine the slope and intercept of a straight line passing through <b>the points</b>. What if you have 10 <b>points</b>, that should fall on a straight line, but because of experimental error, don&#39;t really line up. There is no single straight line (y = m x + b) that passes through <b>all</b> ten <b>points</b>. The best that you can hope for is a line passing near <b>all</b> 10 <b>points</b>, but probably not through any of ...", "dateLastCrawled": "2022-01-29T09:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "1 Introduction. - MIT", "url": "http://web.mit.edu/fluids-modules/www/highspeed_flows/ver2/bl_Chap2.pdf", "isFamilyFriendly": true, "displayUrl": "<b>web.mit.edu</b>/fluids-modules/www/highspeed_flows/ver2/bl_Chap2.pdf", "snippet": "tions implicit in the boundary layer <b>approximation</b> break down for the region of slow ow around the leading edge. The solution given by the boundary layer <b>approximation</b> is not valid at the leading edge. It is valid downstream of the point x= 0. We would <b>like</b> to reduce the boundary layer equation (3.27) to an equation with a single dependent variable. We consider the stream function related to the velocities uand vaccording to the equations u= @ @y; (3.31) v= @ @x: (3.32) If we substitute ...", "dateLastCrawled": "2022-01-30T04:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Q: Is the edge of a <b>circle with an infinite radius curved or straight</b> ...", "url": "https://www.askamathematician.com/2011/04/q-is-the-edge-of-a-circle-with-an-infinite-radius-curved-or-straight/", "isFamilyFriendly": true, "displayUrl": "https://www.askamathematician.com/2011/04/q-is-", "snippet": "Say you have two lines on a plane. They\u2019ll always intersect at exactly one point, unless they\u2019re parallel in which case they\u2019ll never intersect at <b>all</b>. But the Greek Geometers, back in the day, didn\u2019t <b>like</b> that; they wanted a more <b>universal</b> <b>theorem</b>. So they included the \u201cline at infinity\u201d with their plane, and created the ...", "dateLastCrawled": "2022-01-15T21:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Neural Networks are Function Approximation</b> Algorithms", "url": "https://machinelearningmastery.com/neural-networks-are-function-approximators/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>neural-networks-are-function</b>-approximators", "snippet": "\u2026 the <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feedforward network with a linear output layer and at least one hidden layer with any \u201csquashing\u201d activation function (such as the logistic sigmoid activation function) can approximate any [\u2026] function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units", "dateLastCrawled": "2022-01-30T03:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Deep Learning is Not As Impressive As you Think, It&#39;s Mere ...", "url": "https://gowrishankar.info/blog/deep-learning-is-not-as-impressive-as-you-think-its-mere-interpolation/", "isFamilyFriendly": true, "displayUrl": "https://gowrishankar.info/blog/deep-learning-is-not-as-impressive-as-you-think-its...", "snippet": "You Don\u2019t Understand Neural Networks Until You Understand the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> by Andre Ye, Jul 1, 2020. Yann LeCun - Replying to Steven Pinker. Jun 29, 2021. This is another piece that basically says \u201cdeep learning is not as impressive as you think because it\u2019s mere interpolation resulting from glorified <b>curve</b> fitting ...", "dateLastCrawled": "2021-12-25T17:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Neural networks <b>with a continuous squashing function in the</b> output are ...", "url": "https://www.researchgate.net/publication/12333968_Neural_networks_with_a_continuous_squashing_function_in_the_output_are_universal_approximators", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/12333968_Neural_networks_with_a_continuous...", "snippet": "<b>Universal</b> <b>approximation</b> <b>theorem</b> ... <b>Similar</b> results have been ... to approximate the analytical solution of the diffusion equation and find within it any unknown parameter that best <b>fits</b> a given ...", "dateLastCrawled": "2022-01-16T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Deep learning theory lecture notes", "url": "https://mjt.cs.illinois.edu/dlt/index.html", "isFamilyFriendly": true, "displayUrl": "https://mjt.cs.illinois.edu/dlt/index.html", "snippet": "2.2 <b>Universal</b> <b>approximation</b> with a single hidden layer. The proof of <b>Theorem</b> 2.1 use two layers to construct g_\\gamma such that g_\\gamma(x) \\approx \\mathbf{1}\\left[{ x\\in \\times_i [a_i,b_i] }\\right].If instead we had a way to approximate multiplication we could instead approximate x \\mapsto \\prod_i \\mathbf{1}\\left[{ x_i \\in [a_i,b_i] }\\right] = \\mathbf{1}\\left[{ x\\in \\times_i [a_i,b_i] }\\right]. Can we do this and then form a linear combination, <b>all</b> with just one hidden layer?", "dateLastCrawled": "2022-01-30T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Using Radial Basis <b>Function Networks for Function Approximation</b> and ...", "url": "https://www.hindawi.com/journals/isrn/2012/324194/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/isrn/2012/324194", "snippet": "Another popular RBF for <b>universal</b> <b>approximation</b> is the thin-plate spline function , which is selected from <b>a curve</b>-fitting perspective . The thin-plate spline is the solution when fitting a surface through a <b>set</b> of <b>points</b> and by using a roughness penalty . It diverges at infinity and is negative over the region of \ud835\udc5f \u2208 (0, 1). However, for ...", "dateLastCrawled": "2022-01-31T11:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Universal</b> \u03b5-approximators for integrals | Request PDF", "url": "https://www.researchgate.net/publication/242086140_Universal_e-approximators_for_integrals", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/242086140_<b>Universal</b>_e-approximators_for_integrals", "snippet": "A (j, k)-coreset for projective clustering is a small <b>set</b> of <b>points</b> that yields a (1 + \u03b5)-<b>approximation</b> to the sum of squared distances from the n rows of A to any <b>set</b> of k affine subspaces, each ...", "dateLastCrawled": "2021-12-17T12:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Measuring Model Complexity of Neural Networks with <b>Curve</b> Activation ...", "url": "https://deepai.org/publication/measuring-model-complexity-of-neural-networks-with-curve-activation-functions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/measuring-model-complexity-of-neural-networks-with...", "snippet": "The rationale is that two functions having the same behavior on a <b>set</b> of data <b>points</b> may still be very different, ... further strengthen this <b>theorem</b>. However, although with the <b>universal</b> <b>approximation</b> <b>theorem</b>, the layer width can be exponentially large. Lu et al. extend the <b>universal</b> <b>approximation</b> <b>theorem</b> to deep networks with bounded layer width. Recently, deep models are empirically discovered to be more effective than a shallow one. A series of studies focus on exploring the advantages ...", "dateLastCrawled": "2021-12-12T20:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Optimization and Best Fitting Curves", "url": "https://mathstat.slu.edu/~may/ExcelCalculus/sec-6-4-OptimizationBestFitCurves.html", "isFamilyFriendly": true, "displayUrl": "https://mathstat.slu.edu/~may/ExcelCalculus/sec-6-4-OptimizationBestFit<b>Curves</b>.html", "snippet": "We consider a data <b>set</b> of 3 <b>points</b>, \\({(1,0),(3,5),(6,5)}\\) and a line that we will use to predict the y-value given the x-value, \\(\\predicted(x)=x/2 +1\\text{.}\\) We want to determine how well the line matches that data. For each point, \\((x_0,y_0)\\text{,}\\) in the <b>set</b> we start by <b>finding</b> the corresponding point, \\((x_0,\\predicted(x_0 ))\\text{,}\\) on the line. This gives us a <b>set</b> of predicted <b>points</b>, \\({(1,1.5),(3,2.5),(6,4)}\\text{.}\\) For each point we now compute the difference between the ...", "dateLastCrawled": "2022-02-02T07:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "DeLISA: Deep learning based iteration scheme <b>approximation</b> for solving ...", "url": "https://www.sciencedirect.com/science/article/pii/S0021999121007798", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0021999121007798", "snippet": "Moreover, the training <b>set</b> is made of initial <b>points</b> N 0 = 512 and boundary <b>points</b> N b at each time step. In order to further test the performance under the influence of time step, the mixed time steps are employed in a single training. Here, we define the mixed time steps as adopting different time steps on the time domain divided by some time <b>points</b>.", "dateLastCrawled": "2022-01-26T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Q: Is the edge of a <b>circle with an infinite radius curved or straight</b> ...", "url": "https://www.askamathematician.com/2011/04/q-is-the-edge-of-a-circle-with-an-infinite-radius-curved-or-straight/", "isFamilyFriendly": true, "displayUrl": "https://www.askamathematician.com/2011/04/q-is-", "snippet": "Say you have two lines on a plane. They\u2019ll always intersect at exactly one point, unless they\u2019re parallel in which case they\u2019ll never intersect at <b>all</b>. But the Greek Geometers, back in the day, didn\u2019t like that; they wanted a more <b>universal</b> <b>theorem</b>. So they included the \u201cline at infinity\u201d with their plane, and created the ...", "dateLastCrawled": "2022-01-15T21:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal</b> \u03b5-approximators for integrals | Request PDF", "url": "https://www.researchgate.net/publication/242086140_Universal_e-approximators_for_integrals", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/242086140_<b>Universal</b>_e-approximators_for_integrals", "snippet": "Think of S as a &quot;<b>universal</b> \u03b5-approximator&quot; for integration in F. S <b>can</b> actually be obtained w.h.p. just by sampling a few <b>points</b> from \u03bc. This is a mainstay of computational learning theory. It ...", "dateLastCrawled": "2021-12-17T12:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Neural networks <b>with a continuous squashing function in the</b> output are ...", "url": "https://www.researchgate.net/publication/12333968_Neural_networks_with_a_continuous_squashing_function_in_the_output_are_universal_approximators", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/12333968_Neural_networks_with_a_continuous...", "snippet": "<b>Universal</b> <b>approximation</b> <b>theorem</b> Mathematical demonstrations of NNs ability to approximate functions find their origins in works from the early 90&#39;s [17, 40, 51,73,98], with the fundamental ...", "dateLastCrawled": "2022-01-16T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Zipf\u2019s word frequency law in natural language: A critical review and ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4176592/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC4176592", "snippet": "Unfortunately, essentially <b>all</b> of the work in language research has focused solely on deriving the law itself in principle; very little work has attempted to assess the underlying assumptions of the hypothesized explanation, a problem for much work on power laws in science (Stumpf &amp; Porter, 2012). 2 It should be clear why this is problematic: The law itself <b>can</b> be derived from many starting <b>points</b>. Therefore, the ability of a theory to derive the law provides very weak evidence for that ...", "dateLastCrawled": "2022-02-02T08:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Q: Is the edge of a <b>circle with an infinite radius curved or straight</b> ...", "url": "https://www.askamathematician.com/2011/04/q-is-the-edge-of-a-circle-with-an-infinite-radius-curved-or-straight/", "isFamilyFriendly": true, "displayUrl": "https://www.askamathematician.com/2011/04/q-is-", "snippet": "In fact, in mathematics the \u201ccurvature\u201d of <b>a curve</b> is usually defined as the \u201creciprocal of the radius of the osculating circle\u201d. This is fancy talk for: fit a circle into the <b>curve</b> as best you <b>can</b>, then measure the radius of that circle, and flip it over.", "dateLastCrawled": "2022-01-15T21:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Can</b> a <b>Computer Solve Lebesgue&#39;s Universal Covering Problem</b>? | The n ...", "url": "https://classes.golem.ph.utexas.edu/category/2015/02/computability_for_lebesgues_un.html", "isFamilyFriendly": true, "displayUrl": "https://classes.golem.ph.utexas.edu/category/2015/02/computability_for_lebesgues_un.html", "snippet": "Here\u2019s a problem I hope we <b>can</b> solve here. I think it will be fun. It involves computable analysis.. To state the problem precisely, recall that the diameter of a <b>set</b> of <b>points</b> A A in a metric space is . diam (A) = sup {d (x, y): x, y \u2208 A} diam(A)=\\sup\\{d(x,y) : x,y\\in A\\} . Recall that two subsets of the Euclidean plane \u211d 2 \\mathbb{R}^2 are isometric if we <b>can</b> get one from the other by translation, rotation and/or reflection.. Finally, let\u2019s define a <b>universal</b> covering to be a ...", "dateLastCrawled": "2021-12-30T00:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Bell <b>Curve</b> and Normal Distribution Definition", "url": "https://www.thoughtco.com/bell-curve-normal-distribution-defined-2312350", "isFamilyFriendly": true, "displayUrl": "https://www.<b>thought</b>co.com/bell-<b>curve</b>-normal-distribution-defined-2312350", "snippet": "The term bell <b>curve</b> is used to describe the mathematical concept called normal distribution, sometimes referred to as Gaussian distribution. &quot;Bell <b>curve</b>&quot; refers to the bell shape that is created when a line is plotted using the data <b>points</b> for an item that meets the criteria of normal distribution. In a bell <b>curve</b>, the center contains the ...", "dateLastCrawled": "2022-02-02T21:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Central Limit <b>Theorem</b> Explained - Statistics By Jim", "url": "https://statisticsbyjim.com/basics/central-limit-theorem/", "isFamilyFriendly": true, "displayUrl": "https://statisticsbyjim.com/basics/central-", "snippet": "The central limit <b>theorem</b> in statistics states that, given a sufficiently large sample size, the sampling distribution of the mean for a variable will approximate a normal distribution regardless of that variable\u2019s distribution in the population.. Unpacking the meaning from that complex definition <b>can</b> be difficult. That\u2019s the topic for this post! I\u2019ll walk you through the various aspects of the central limit <b>theorem</b> (CLT) definition, and show you why it is vital in statistics.", "dateLastCrawled": "2022-02-02T19:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Calculus Volume 2</b> | Nurmukhammad Rakhmatov - Academia.edu", "url": "https://www.academia.edu/39673016/Calculus_Volume_2", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/39673016/<b>Calculus_Volume_2</b>", "snippet": "OpenStax provides free, peer-reviewed, openly licensed textbooks for introductory college and Advanced Placement\u00ae courses and low-cost, personalized courseware that helps students learn. A nonprofit ed tech initiative based at Rice University, we\u2019re", "dateLastCrawled": "2022-01-28T13:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Section 01 - Massey University", "url": "https://www.massey.ac.nz/~mjjohnso/notes/59302/all.html", "isFamilyFriendly": true, "displayUrl": "https://www.massey.ac.nz/~mjjohnso/notes/59302/<b>all</b>.html", "snippet": "The Hough transform <b>can</b> be used to identify the parameter(s) of <b>a curve</b> which best <b>fits</b> a <b>set</b> of given edge <b>points</b>. This edge description is commonly obtained by using an edge detector such as the zero crossings of the laplacian. The edge image may be noisy, i.e. it may contain multiple edge fragments corresponding to a single whole feature.", "dateLastCrawled": "2022-02-02T16:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Artificial neural networks - Dayhoff - 2001 - Cancer - Wiley Online Library", "url": "https://acsjournals.onlinelibrary.wiley.com/doi/full/10.1002/1097-0142%2820010415%2991%3A8%2B%3C1615%3A%3AAID-CNCR1175%3E3.0.CO%3B2-L", "isFamilyFriendly": true, "displayUrl": "https://acsjournals.onlinelibrary.wiley.com/doi/full/10.1002/1097-0142(20010415)91:8...", "snippet": "<b>A curve</b> suggested by <b>the points</b> is shown, leading to the conclusion that for T = 225 \u00b0C, the best reaction time is 130 minutes, in which the yield is approximately 75 g. Figure 1. Open in figure viewer PowerPoint. First <b>set</b> of experiments showing yield versus reaction time, with the temperature held fixed at 225 \u00b0C. Reproduced with permission from Box GEP, Hunter WG, Hunter JS. Statistics for experimenters. New York: John Wiley &amp; Sons, 1978. After a classic one-variable-at-a-time strategy ...", "dateLastCrawled": "2022-02-01T23:57:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Learning the solution operator of parametric partial differential ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8480920/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC8480920", "snippet": "Motivated by the <b>universal</b> <b>approximation</b> <b>theorem</b> for operators ... An even more intriguing <b>finding</b> is that physics-informed DeepONets <b>can</b> learn the solution operator of parametric ODEs and PDEs, even in the absence of any paired input-output training data. This capability is introducing a new radical way of simulating nonlinear and nonequilibrium phenomena across different applications in science and engineering up to three orders of magnitude faster <b>compared</b> to conventional solvers. Given ...", "dateLastCrawled": "2022-01-30T23:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Using Radial Basis <b>Function Networks for Function Approximation</b> and ...", "url": "https://www.hindawi.com/journals/isrn/2012/324194/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.hindawi.com</b>/journals/isrn/2012/324194", "snippet": "Another popular RBF for <b>universal</b> <b>approximation</b> is the thin-plate spline function , which is selected from <b>a curve</b>-fitting perspective . The thin-plate spline is the solution when fitting a surface through a <b>set</b> of <b>points</b> and by using a roughness penalty . It diverges at infinity and is negative over the region of \ud835\udc5f \u2208 (0, 1). However, for ...", "dateLastCrawled": "2022-01-31T11:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Neural networks <b>with a continuous squashing function in the</b> output are ...", "url": "https://www.researchgate.net/publication/12333968_Neural_networks_with_a_continuous_squashing_function_in_the_output_are_universal_approximators", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/12333968_Neural_networks_with_a_continuous...", "snippet": "<b>Universal</b> <b>approximation</b> <b>theorem</b> Mathematical demonstrations of NNs ability to approximate functions find their origins in works from the early 90&#39;s [17, 40, 51,73,98], with the fundamental ...", "dateLastCrawled": "2022-01-16T09:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Universal</b> \u03b5-approximators for integrals | Request PDF", "url": "https://www.researchgate.net/publication/242086140_Universal_e-approximators_for_integrals", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/242086140_<b>Universal</b>_e-approximators_for_integrals", "snippet": "Think of S as a &quot;<b>universal</b> \u03b5-approximator&quot; for integration in F. S <b>can</b> actually be obtained w.h.p. just by sampling a few <b>points</b> from \u03bc. This is a mainstay of computational learning theory. It ...", "dateLastCrawled": "2021-12-17T12:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A local <b>approximation</b> based multi-objective optimization algorithm with ...", "url": "https://link.springer.com/article/10.1007%2Fs11081-012-9211-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s11081-012-9211-5", "snippet": "This definition leads to a <b>set</b> of solution called Pareto optimal <b>set</b> which represents the <b>set</b> of best compromise solutions. Therefore, in order to solve the multi-objective optimization problem the Pareto <b>set</b> (or an <b>approximation</b> of the <b>set</b>, see Zitzler et al. for quality assessment of Pareto <b>set</b> approximations) must be computed.<b>Finding</b> an accurate <b>approximation</b> of the Pareto optimal <b>set</b> is difficult due to the problem high dimensionality and due to the non linearity of the functions f, g and h.", "dateLastCrawled": "2021-12-15T23:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "2.4 <b>Velocity</b> vs. Time Graphs - Physics | OpenStax", "url": "https://openstax.org/books/physics/pages/2-4-velocity-vs-time-graphs", "isFamilyFriendly": true, "displayUrl": "https://openstax.org/books/physics/pages/2-4-<b>velocity</b>-vs-time-graphs", "snippet": "The area under a <b>velocity</b> <b>curve</b> represents the displacement. The <b>velocity</b> <b>curve</b> also tells us whether the car is speeding up. In our earlier example, we stated that the <b>velocity</b> was constant. So, the car is not speeding up. Graphically, you <b>can</b> see that the slope of these two lines is 0. This slope tells us that the car is not speeding up, or accelerating. We will do more with this information in a later chapter. For now, just remember that the area under the graph and the slope are the two ...", "dateLastCrawled": "2022-02-03T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "DeLISA: Deep learning based iteration scheme <b>approximation</b> for solving ...", "url": "https://www.sciencedirect.com/science/article/pii/S0021999121007798", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0021999121007798", "snippet": "Moreover, the training <b>set</b> is made of initial <b>points</b> N 0 = 256 and boundary <b>points</b> N b at each time step. Fig. 3 shows the comparison of prediction and exact solution at t = ( 0 , 0.33 , 0.66 , 0.99 ) , which validates that the prediction <b>fits</b> accurately exact solution.", "dateLastCrawled": "2022-01-26T15:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Deep learning theory lecture notes", "url": "https://mjt.cs.illinois.edu/dlt/index.html", "isFamilyFriendly": true, "displayUrl": "https://mjt.cs.illinois.edu/dlt/index.html", "snippet": "<b>Theorem</b> 5.1 ((Telgarsky 2015, 2016)) was the earliest proof showing that a deep network <b>can</b> not be approximated by a reasonably-sized shallow network, however prior work showed a separation for exact representation of deep sum-product networks as <b>compared</b> with shallow ones (Bengio and Delalleau 2011). A sum-product network has nodes which compute affine transformations or multiplications, and thus a multi-layer sum-product network is a polynomial, and this result, while interesting, does not ...", "dateLastCrawled": "2022-01-30T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Bell <b>Curve</b> and Normal Distribution Definition", "url": "https://www.thoughtco.com/bell-curve-normal-distribution-defined-2312350", "isFamilyFriendly": true, "displayUrl": "https://www.thoughtco.com/bell-<b>curve</b>-normal-distribution-defined-2312350", "snippet": "This is significant in that the data has less of a tendency to produce unusually extreme values, called outliers, as <b>compared</b> to other distributions. Also, the bell <b>curve</b> signifies that the data is symmetrical. This means that you <b>can</b> create reasonable expectations as to the possibility that an outcome will lie within a range to the left or right of the center, once you have measured the amount of deviation contained in the data.This is measured in terms of", "dateLastCrawled": "2022-02-02T21:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Q: Is the edge of a <b>circle with an infinite radius curved or straight</b> ...", "url": "https://www.askamathematician.com/2011/04/q-is-the-edge-of-a-circle-with-an-infinite-radius-curved-or-straight/", "isFamilyFriendly": true, "displayUrl": "https://www.askamathematician.com/2011/04/q-is-", "snippet": "Old school topologists get very excited about this stuff. Say you have two lines on a plane. They\u2019ll always intersect at exactly one point, unless they\u2019re parallel in which case they\u2019ll never intersect at <b>all</b>. But the Greek Geometers, back in the day, didn\u2019t like that; they wanted a more <b>universal</b> <b>theorem</b>.", "dateLastCrawled": "2022-01-15T21:20:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Approximation Theorem</b>. The power of Neural Networks | by ...", "url": "https://medium.com/swlh/universal-approximation-theorem-d1a1a67c1b5b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>universal-approximation-theorem</b>-d1a1a67c1b5b", "snippet": "<b>Universal Approximation Theorem</b>, in its lose form, states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate any continuous function. Whoa ...", "dateLastCrawled": "2022-01-28T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal Approximation Theorem</b>, Neural Nets &amp; Lego Blocks | by ...", "url": "https://medium.com/analytics-vidhya/universal-approximation-theorem-neural-nets-lego-blocks-1f5a7d93542a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>universal-approximation-theorem</b>-neural-nets-lego...", "snippet": "In this post, we will look at the <b>Universal Approximation Theorem</b> \u2014 one of the fundamental theorems on which the entire concept of Deep <b>Learning</b> is based upon. We will make use of lego blocks ...", "dateLastCrawled": "2022-01-28T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> Complex Functions using <b>Universal</b> Approximate <b>Theorem</b> - Ai Nxt", "url": "https://ainxt.co.in/learning-complex-functions-using-universal-approximate-theorem/", "isFamilyFriendly": true, "displayUrl": "https://ainxt.co.in/<b>learning</b>-complex-functions-using-<b>universal</b>-approximate-<b>theorem</b>", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b>. No matter how complex our output logic is, we can use collection of neurons and form Dense Neural Network to approximate our function. This is known as \u201c<b>UNIVERSAL</b> <b>APPROXIMATION</b> <b>THEOREM</b>\u201c. Lets take an example of Two Dimensional data where y = f(x) i.e. y is some function of x. Now, we need to find that ...", "dateLastCrawled": "2022-01-21T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) <b>Universal Approximation Theorems</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336361517_Universal_Approximation_Theorems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336361517_<b>Universal_Approximation_Theorems</b>", "snippet": "In the <b>machine</b> <b>learning</b> literature, <b>universal</b> <b>approximation</b> refers to a model class\u2019 ability. to generically approximate any member of a large topological space whose elements are. functions, or ...", "dateLastCrawled": "2022-01-25T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Illustrative Proof of <b>Universal Approximation Theorem</b> | HackerNoon", "url": "https://hackernoon.com/illustrative-proof-of-universal-approximation-theorem-5845c02822f6", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/illustrative-proof-of-<b>universal-approximation-theorem</b>-5845c02822f6", "snippet": "We will talk about the <b>Universal approximation theorem</b> and we will also prove the <b>theorem</b> graphically. The most commonly used sigmoid function is the logistic function, which has a characteristic of an \u201cS\u201d shaped curve. In real life, we deal with complex functions where the relationship between input and output might be complex. To solve this problem, let&#39;s take an <b>analogy</b> of building a house. The way we are going to create complex functions is that we will combine the sigmoids neurons ...", "dateLastCrawled": "2022-02-01T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "ne.neural evol - <b>Universal Approximation Theorem</b> \u2014 Neural Networks ...", "url": "https://cstheory.stackexchange.com/questions/17545/universal-approximation-theorem-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/17545", "snippet": "<b>Universal approximation theorem</b> states that &quot;the standard multilayer feed-forward network with a single hidden layer, ... There is an advanced result, key to <b>machine</b> <b>learning</b>, known as Kolmogorov&#39;s <b>theorem</b> [1]; I have never seen an intuitive sketch of why it works. This may have to do with the different cultures that approach it. The applied <b>learning</b> crowd regards Kolmogorov&#39;s <b>theorem</b> as an existence <b>theorem</b> that merely indicates that NNs may exist, so at least the structure is not overly ...", "dateLastCrawled": "2022-02-03T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "neural networks - <b>Universal Approximation Theorem and high dimension</b> ...", "url": "https://stats.stackexchange.com/questions/298622/universal-approximation-theorem-and-high-dimension-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/298622/<b>universal</b>-<b>approximation</b>-<b>theorem</b>-and...", "snippet": "Cross Validated is a question and answer site for people interested in statistics, <b>machine</b> <b>learning</b>, data analysis, data mining, and data visualization. It only takes a minute to sign up. Sign up to join this community", "dateLastCrawled": "2022-01-17T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Neural Networks and Learning Machines</b> - etsmtl.ca", "url": "https://cours.etsmtl.ca/sys843/REFS/Books/ebook_Haykin09.pdf", "isFamilyFriendly": true, "displayUrl": "https://cours.etsmtl.ca/sys843/REFS/Books/ebook_Haykin09.pdf", "snippet": "15.3 <b>Universal</b> <b>Approximation</b> <b>Theorem</b> 797 15.4 Controllability and Observability 799 15.5 Computational Power of Recurrent Networks 804 15.6 <b>Learning</b> Algorithms 806 15.7 Back Propagation Through Time 808 15.8 Real-Time Recurrent <b>Learning</b> 812 15.9 Vanishing Gradients in Recurrent Networks 818", "dateLastCrawled": "2022-01-31T06:23:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(universal approximation theorem)  is like +(finding a curve that fits all the points in a set)", "+(universal approximation theorem) is similar to +(finding a curve that fits all the points in a set)", "+(universal approximation theorem) can be thought of as +(finding a curve that fits all the points in a set)", "+(universal approximation theorem) can be compared to +(finding a curve that fits all the points in a set)", "machine learning +(universal approximation theorem AND analogy)", "machine learning +(\"universal approximation theorem is like\")", "machine learning +(\"universal approximation theorem is similar\")", "machine learning +(\"just as universal approximation theorem\")", "machine learning +(\"universal approximation theorem can be thought of as\")", "machine learning +(\"universal approximation theorem can be compared to\")"]}