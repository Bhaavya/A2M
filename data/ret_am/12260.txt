{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "ML <b>text</b>-classification : combining dense and <b>sparse</b> features - Stack ...", "url": "https://stackoverflow.com/questions/45332672/ml-text-classification-combining-dense-and-sparse-features", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/.../ml-<b>text</b>-classification-combining-dense-and-<b>sparse</b>-<b>features</b>", "snippet": "That is to say, the same <b>document</b> <b>text</b>, together with a different combination of A and B has different class in the training data. <b>Text</b> classification of C by itself is relatively easy but it is with the combination of three that I am experiencing challenge. For example, if my training data is train with the columns A, B &amp; C with C being <b>document</b>, I could work with C by itself as: vectorizer = TfidfVectorizer() vtrain = vectorizer.fit_transform(train[&quot;C&quot;]) and this would give me <b>sparse</b> ...", "dateLastCrawled": "2022-01-26T10:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Is a bag of words <b>feature</b> representation for <b>text</b> classification ...", "url": "https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html", "isFamilyFriendly": true, "displayUrl": "https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html", "snippet": "By definition, a <b>sparse</b> matrix is called \u201c<b>sparse</b>\u201d if most of its elements are zero. In the bag of words model, each <b>document</b> is represented as a word-count vector. These counts can be binary counts (does a word occur or not) or absolute counts (term frequencies, or normalized counts), and the size of this vector is equal to the number of elements in your vocabulary. Thus, if most of your <b>feature</b> vectors are <b>sparse</b>, our bag-of-words <b>feature</b> matrix is most likely <b>sparse</b> as well!", "dateLastCrawled": "2022-02-03T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Concept Decompositions for Large <b>Sparse</b> <b>Text</b> Data using Clustering", "url": "https://www.cs.utexas.edu/users/inderjit/public_papers/concept_mlj.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.utexas.edu/users/inderjit/public_papers/concept_mlj.pdf", "snippet": "this <b>feature</b> space. Observe that we may regard the vector space model of a <b>text</b> data set ... <b>document</b> vectors are very <b>sparse</b>. Understanding and exploiting the structure and statistics of such vector space models is a major contemporary scientic and technological challenge. We shall assume that the <b>document</b> vectors have been normalized to have unit L2 norm, that is, they can be thought of as points on a high-dimensional unit sphere. Such normalization mitigates the effect of differing ...", "dateLastCrawled": "2022-01-23T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Calculating <b>Text</b> Similarity With <b>Gensim</b> | by Riley Huang | Better ...", "url": "https://betterprogramming.pub/introduction-to-gensim-calculating-text-similarity-9e8b55de342d", "isFamilyFriendly": true, "displayUrl": "https://betterprogramming.pub/introduction-to-<b>gensim</b>-calculating-<b>text</b>-similarity-9e8b...", "snippet": "An example of a <b>feature</b> could be the following: How many times does the word \u201chappy\u201d appear in the <b>text</b> <b>document</b>? Three. The question is represented by its id (integer), and hence the representation of the <b>text</b> <b>document</b> becomes a series of pairs, such as (2, 4.0), (3, 6.0), (4, 5.0). This series can be thought of as a vector. If the vectors in the two documents are similar, the documents must be similar too. <b>Sparse</b> Vector. Documents in <b>Gensim</b> are represented by <b>sparse</b> vectors. <b>Gensim</b> ...", "dateLastCrawled": "2022-01-30T03:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Feature standardization in text classification makes</b> <b>sparse</b> data look ...", "url": "https://www.quora.com/Feature-standardization-in-text-classification-makes-sparse-data-look-non-sparse-Is-this-okay", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Feature-standardization-in-text-classification-makes</b>-<b>sparse</b>-data...", "snippet": "Answer (1 of 4): This is a trick that Spark ML folks have implemented in their packages for Logistic Regression and other linear models to handle sparsity (details) i.e. you do not want to end up with dense features by doing <b>feature</b> normalization on <b>sparse</b> input. So, if you have a linear model ...", "dateLastCrawled": "2022-01-22T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Feature</b> Transformation of <b>Text</b> Data \u2014 NLP | by Prassena Kannan | The ...", "url": "https://medium.com/swlh/feature-transform-of-text-data-nlp-c6ccedbeb3cc", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>feature</b>-transform-of-<b>text</b>-data-nlp-c6ccedbeb3cc", "snippet": "<b>Feature</b> Transformation is the process of converting raw data which can be of <b>Text</b>, Image, Graph, Time series etc\u2026 into numerical <b>feature</b> (Vectors). So that we can perform all algebraic operation ...", "dateLastCrawled": "2022-01-30T07:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Ultimate Guide To <b>Text</b> Similarity With Python - NewsCatcher", "url": "https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python", "isFamilyFriendly": true, "displayUrl": "https://newscatcherapi.com/blog/ultimate-guide-to-<b>text</b>-similarity-with-python", "snippet": "The first part of this problem is representation. How do we represent the <b>text</b>? We could leave the <b>text</b> as it is or convert it into <b>feature</b> vectors using a suitable <b>text</b> embedding technique. Once we have the <b>text</b> representation, we can compute the similarity score using one of the many distance/similarity measures. Let\u2019s dive deeper into the two aspects of the problem, starting with the similarity measures. Similarity Measures Jaccard Index. Jaccard index, also known as Jaccard similarity ...", "dateLastCrawled": "2022-02-03T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Is a bag of words <b>feature representation for text classification</b> ...", "url": "https://www.quora.com/Is-a-bag-of-words-feature-representation-for-text-classification-considered-as-a-sparse-matrix", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Is-a-bag-of-words-<b>feature-representation-for-text-classification</b>...", "snippet": "Answer: It depends on your vocabulary and dataset, but typically: Yes! By definition, a <b>sparse</b> matrix is called &quot;<b>sparse</b>&quot; if most of its elements are zero. Now, in the bag of words model, each <b>document</b> is represented as some sort of count vector (e.g., binary counts -- does a word occur or not,...", "dateLastCrawled": "2022-01-24T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Scikit-Learn - <b>Feature Extraction</b> from <b>Text</b> Data", "url": "https://coderzcolumn.com/tutorials/machine-learning/feature-extraction-from-text-data-using-scikit-learn-sklearn", "isFamilyFriendly": true, "displayUrl": "https://coderzcolumn.com/tutorials/machine-learning/<b>feature-extraction</b>-from-<b>text</b>-data...", "snippet": "<b>Feature Extraction</b> From <b>Text</b> Data\u00b6 All of the machine learning libraries expect input in the form of floats and that also fixed length/dimensions. But in real life, we face data in different forms <b>like</b> <b>text</b>, images, audio, video, etc. We need to find a way to represent these forms of data as floats to be able to train learning algorithms based ...", "dateLastCrawled": "2022-01-29T04:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>TF-IDF</b> Vectorizer scikit-learn. Deep understanding TfidfVectorizer by ...", "url": "https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@cmukesh8688/<b>tf-idf</b>-vectorizer-scikit-learn-dbc0244a911a", "snippet": "<b>TF-IDF</b> is an abbreviation for Term Frequency Inverse <b>Document</b> Frequency. This is very common algorithm to transform <b>text</b> into a meaningful representation of numbers which is used to fit machine ...", "dateLastCrawled": "2022-02-03T04:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Is a bag of words <b>feature</b> representation for <b>text</b> classification ...", "url": "https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html", "isFamilyFriendly": true, "displayUrl": "https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html", "snippet": "By definition, a <b>sparse</b> matrix is called \u201c<b>sparse</b>\u201d if most of its elements are zero. In the bag of words model, each <b>document</b> is represented as a word-count vector. These counts can be binary counts (does a word occur or not) or absolute counts (term frequencies, or normalized counts), and the size of this vector is equal to the number of elements in your vocabulary. Thus, if most of your <b>feature</b> vectors are <b>sparse</b>, our bag-of-words <b>feature</b> matrix is most likely <b>sparse</b> as well!", "dateLastCrawled": "2022-02-03T18:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Ultimate Guide <b>To Text</b> Similarity With Python - NewsCatcher", "url": "https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python", "isFamilyFriendly": true, "displayUrl": "https://newscatcherapi.com/blog/ultimate-guide-<b>to-text</b>-<b>similar</b>ity-with-python", "snippet": "This means that is limited to assessing the lexical similarity of <b>text</b>, i.e., how <b>similar</b> documents are on a word level. As far as cosine and Euclidean metrics are concerned, the differentiating factor between the two is that cosine similarity is not affected by the magnitude/length of the <b>feature</b> vectors. Let\u2019s say we are creating a topic tagging algorithm. If a word (e.g. senate) occurs more frequently in <b>document</b> 1 than it does in <b>document</b> 2, we could assume that <b>document</b> 1 is more ...", "dateLastCrawled": "2022-02-03T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Featurization of Text data</b>. BAG OF WORDS | by Rana singh | Analytics ...", "url": "https://medium.com/analytics-vidhya/featurization-of-text-data-bow-tf-idf-avgw2v-tfidf-weighted-w2v-7a6c62e8b097", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>featurization-of-text-data</b>-bow-tf-idf-avgw2v-tfidf...", "snippet": "It consists of all unique words in the <b>TEXT</b>. It represents word as a <b>sparse</b> matrix. For each <b>document</b>(row), find unique words where each word is a different dimension. Each cell consists of the ...", "dateLastCrawled": "2022-01-21T15:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Data Representation in NLP. What is Vectorization ? | by Shivangi ...", "url": "https://shiivangii.medium.com/data-representation-in-nlp-7bb6a771599a", "isFamilyFriendly": true, "displayUrl": "https://shiivangii.medium.com/data-representation-in-nlp-7bb6a771599a", "snippet": "Different ways to convert <b>text</b> into numbers are: <b>Sparse</b> Vector Representations and Dense Vector Representations. Note: The GitHub codes of this blog are available. To know how to process data before making its representation, go to this blog. <b>Sparse</b> Vector Representations (1) B a g of Words (BoW) Suppose I have a <b>text</b> <b>document</b>. Cut this <b>document</b> into words (i.e. perform word tokenization ) and remove any kind of punctuations. Now, imagine that you are putting every new word (that has not ...", "dateLastCrawled": "2022-02-03T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Calculating <b>Text</b> Similarity With <b>Gensim</b> | by Riley Huang | Better ...", "url": "https://betterprogramming.pub/introduction-to-gensim-calculating-text-similarity-9e8b55de342d", "isFamilyFriendly": true, "displayUrl": "https://betterprogramming.pub/introduction-to-<b>gensim</b>-calculating-<b>text</b>-<b>similar</b>ity-9e8b...", "snippet": "Use <b>Gensim</b> to Determine <b>Text</b> Similarity. Here\u2019s a simple example of code implementation that generates <b>text</b> similarity: (Here, jieba is a <b>text</b> segmentation Python module for cutting the words into segmentations for easier analysis of <b>text</b> similarity in the future.) from <b>gensim</b> import corpora, models, similarities import jieba texts = [&#39;I love reading Japanese novels. My favorite Japanese writer is Tanizaki Junichiro.&#39;, &#39;Natsume Soseki is a well-known Japanese novelist and his Kokoro is a ...", "dateLastCrawled": "2022-01-30T03:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "nlp - How to compute the <b>similarity</b> between two <b>text</b> documents? - Stack ...", "url": "https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/8897593", "snippet": "TF-IDF (and <b>similar</b> <b>text</b> transformations) are implemented in the Python packages Gensim and scikit-learn. In the latter package, computing cosine similarities is as easy as. from sklearn.<b>feature</b>_extraction.<b>text</b> import TfidfVectorizer documents = [open(f).read() for f in <b>text</b>_files] tfidf = TfidfVectorizer().fit_transform(documents) # no need to normalize, since Vectorizer will return normalized tf-idf pairwise_<b>similarity</b> = tfidf * tfidf.T or, if the documents are plain strings, &gt;&gt;&gt; corpus ...", "dateLastCrawled": "2022-01-27T04:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Text</b> <b>Feature</b> Extraction (1/3): Bag of Words Model | by Shachi Kaul ...", "url": "https://medium.com/geekculture/text-feature-extraction-1-3-bag-of-words-model-649dbeeade79", "isFamilyFriendly": true, "displayUrl": "https://medium.com/geekculture/<b>text</b>-<b>feature</b>-extraction-1-3-bag-of-words-model-649dbeeade79", "snippet": "Bag Of Words is a <b>feature</b> extraction method of converting the <b>text</b> data into numerical vectors as features Those numbers are the count of each word (token) in a <b>document</b> Produces <b>sparse</b> matrix ...", "dateLastCrawled": "2022-01-11T15:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Feature standardization in text classification makes</b> <b>sparse</b> data look ...", "url": "https://www.quora.com/Feature-standardization-in-text-classification-makes-sparse-data-look-non-sparse-Is-this-okay", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Feature-standardization-in-text-classification-makes</b>-<b>sparse</b>-data...", "snippet": "Answer (1 of 4): This is a trick that Spark ML folks have implemented in their packages for Logistic Regression and other linear models to handle sparsity (details) i.e. you do not want to end up with dense features by doing <b>feature</b> normalization on <b>sparse</b> input. So, if you have a linear model ...", "dateLastCrawled": "2022-01-22T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Feature Extraction from Text</b> - Home", "url": "https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/", "isFamilyFriendly": true, "displayUrl": "https://andhint.github.io/machine-learning/nlp/<b>Feature-Extraction-From-Text</b>", "snippet": "This posts serves as an simple introduction to <b>feature extraction from text</b> to be used for a machine learning model using Python and sci-kit learn. I\u2019m assuming the reader has some experience with sci-kit learn and creating ML models, though it\u2019s not entirely necessary. Most machine learning algorithms can\u2019t take in straight <b>text</b>, so we will create a matrix of numerical values to represent our <b>text</b>. We\u2019ll go over the differences between two common ways of doing this: CountVectorizer ...", "dateLastCrawled": "2022-02-01T07:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "4. <b>Text</b> Vectorization and Transformation Pipelines - Applied <b>Text</b> ...", "url": "https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/applied-<b>text</b>-analysis/9781491963036/ch04.html", "snippet": "Each property of the vector representation is a <b>feature</b>. For <b>text</b>, features represent attributes and properties of documents\u2014including its content as well as meta attributes, such as <b>document</b> length, author, source, and publication date. When considered together, the features of a <b>document</b> describe a multidimensional <b>feature</b> space on which machine learning methods can be applied. For this reason, we must now make a critical shift in how we think about language\u2014from a sequence of words to ...", "dateLastCrawled": "2022-02-01T20:27:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Concept Decompositions for Large <b>Sparse</b> <b>Text</b> Data using Clustering", "url": "https://www.cs.utexas.edu/users/inderjit/public_papers/concept_mlj.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.utexas.edu/users/inderjit/public_papers/concept_mlj.pdf", "snippet": "<b>document</b> vectors are very <b>sparse</b>. Understanding and exploiting the structure and statistics of such vector space models is a major contemporary scientic and technological challenge. We shall assume that the <b>document</b> vectors have been normalized to have unit L2 norm, that is, they <b>can</b> <b>be thought</b> of as points on a high-dimensional unit sphere. Such normalization mitigates the effect of differing lengths of documents (Singhal et al., 1996). It is natural to measure \ufb01similarity\ufb02 between such ...", "dateLastCrawled": "2022-01-23T05:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Sparse representations for text categorization</b>", "url": "https://www.researchgate.net/publication/221479613_Sparse_representations_for_text_categorization", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221479613_<b>Sparse</b>_representations_for_<b>text</b>...", "snippet": "<b>Sparse</b> Methods as a tool for <b>text</b> analysis is an alley that is largely unexplored rigorously. This paper presents exploration of <b>sparse</b> representation-based methods for <b>text</b> classification. Based ...", "dateLastCrawled": "2021-12-10T18:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Sparse Representations for Text Categorization</b> | Dimitri Kanevsky ...", "url": "https://www.academia.edu/2738730/Sparse_Representations_for_Text_Categorization", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2738730/<b>Sparse_Representations_for_Text_Categorization</b>", "snippet": "<b>Sparse Representations for Text Categorization</b>. Download. <b>Sparse Representations for Text Categorization</b>. Dimitri Kanevsky. Related Papers. A Comparative Study of Machine Learning Methods for Verbal Autopsy <b>Text</b> Classification. By Eric S Atwell and Samuel Danso. CSC435 book proposal. By Russell Frith. Higher-Order Smoothing: A Novel Semantic Smoothing Method for <b>Text</b> Classification. By Murat C Ganiz, Mitat Poyraz, and Zeynep Kilimci. INFORMATION RETRIEVAL . By febi k. Introduction to ...", "dateLastCrawled": "2021-10-13T23:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>SPARSE</b> <b>MACHINE LEARNING METHODS FOR UNDERSTANDING LARGE TEXT</b> CORPORA", "url": "https://people.eecs.berkeley.edu/~elghaoui/Pubs/cidu2011_final.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~elghaoui/Pubs/cidu2011_final.pdf", "snippet": "Our paper makes the claim that <b>sparse</b> learning methods <b>can</b> be very useful to the understanding large <b>text</b> databases. Of course, machine learning methods in general have already been successfully applied to <b>text</b> classi cation and clustering, as evidenced for example by [21]. We will show that sparsity is an important added property that is a crucial component in any tool aiming at provid-ing interpretable statistical analysis, allowing in particular e cient multi-<b>document</b> summarization ...", "dateLastCrawled": "2022-01-28T01:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Calculating <b>Text</b> Similarity With <b>Gensim</b> | by Riley Huang | Better ...", "url": "https://betterprogramming.pub/introduction-to-gensim-calculating-text-similarity-9e8b55de342d", "isFamilyFriendly": true, "displayUrl": "https://betterprogramming.pub/introduction-to-<b>gensim</b>-calculating-<b>text</b>-similarity-9e8b...", "snippet": "An example of a <b>feature</b> could be the following: How many times does the word \u201chappy\u201d appear in the <b>text</b> <b>document</b>? Three. The question is represented by its id (integer), and hence the representation of the <b>text</b> <b>document</b> becomes a series of pairs, such as (2, 4.0), (3, 6.0), (4, 5.0). This series <b>can</b> <b>be thought</b> of as a vector.", "dateLastCrawled": "2022-01-30T03:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Concept Decompositions for Large Sparse Text</b> Data Using ... - Springer", "url": "https://link.springer.com/content/pdf/10.1023%2FA%3A1007612920971.pdf", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/content/pdf/10.1023/A:1007612920971.pdf", "snippet": "<b>Concept Decompositions for Large Sparse Text</b> Data Using Clustering INDERJIT S. DHILLON inderjit@cs.utexas.edu Department of Computer Science, University of Texas, Austin, TX 78712, USA DHARMENDRA S. MODHA dmodha@almaden.ibm.com IBM Almaden Research Center, 650 Harry Road, San Jose, CA 95120, USA Editor: Douglas Fisher Abstract. Unlabeled <b>document</b> collections are becoming increasingly common and available; mining such data sets represents a major contemporary challenge. Using words as ...", "dateLastCrawled": "2022-02-03T03:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Ultimate Guide To <b>Text</b> Similarity With Python - NewsCatcher", "url": "https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python", "isFamilyFriendly": true, "displayUrl": "https://newscatcherapi.com/blog/ultimate-guide-to-<b>text</b>-similarity-with-python", "snippet": "The first part of this problem is representation. How do we represent the <b>text</b>? We could leave the <b>text</b> as it is or convert it into <b>feature</b> vectors using a suitable <b>text</b> embedding technique. Once we have the <b>text</b> representation, we <b>can</b> compute the similarity score using one of the many distance/similarity measures. Let\u2019s dive deeper into the two aspects of the problem, starting with the similarity measures. Similarity Measures Jaccard Index. Jaccard index, also known as Jaccard similarity ...", "dateLastCrawled": "2022-02-03T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "python - scipy/sklearn <b>sparse</b> matrix decomposition for <b>document</b> ...", "url": "https://stackoverflow.com/questions/26249367/scipy-sklearn-sparse-matrix-decomposition-for-document-classification", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/26249367", "snippet": "scipy/sklearn <b>sparse</b> <b>matrix decomposition for document classification</b>. Ask Question Asked 7 years, 3 months ago. Active 7 years, 3 months ago. Viewed 2k times 1 2. I&#39;m trying to do documentation classification on a large corpus (4 mil documents) and keep running into memory errors when using the standard scikit-learn methods. After cleaning/stemming my data, I have a very <b>sparse</b> matrix with about 1 mil words. My first <b>thought</b> was to use sklearn.decomposition.TruncatedSVD, but I <b>can</b>&#39;t perform ...", "dateLastCrawled": "2022-01-11T05:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Hands-on implementation of TF-IDF from scratch in Python", "url": "https://analyticsindiamag.com/hands-on-implementation-of-tf-idf-from-scratch-in-python/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/hands-on-implementation-of-tf-idf-from-scratch-in-python", "snippet": "A corpus is a collection of documents. Tf is Term frequency, and IDF is Inverse <b>document</b> frequency. This method is often used for information retrieval and <b>text</b> mining. Tf(Term Frequency): Term frequency <b>can</b> <b>be thought</b> of as how often does a word \u2018w\u2019 occur in a <b>document</b> \u2018d\u2019. More importance is given to words frequently occurring in a ...", "dateLastCrawled": "2022-02-02T13:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to turn <b>Text</b> into Features. A comprehensive guide into using NLP ...", "url": "https://towardsdatascience.com/how-to-turn-text-into-features-478b57632e99", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-turn-<b>text</b>-into-<b>features</b>-478b57632e99", "snippet": "The techniques used to turn <b>Text</b> into features <b>can</b> be referred to as ... Since you\u2019re smart, you already <b>thought</b>: let me make a dictionary or some similar structure (in general, a vocabulary map) and use word indexes instead of words! One supposed solution would be to create a mapping for each word\u2026 Image by author. And then encode a vector with the word indices. Image by author. You\u2019re on the right path, but lets consider some of the problems here: Is the word \u201cmy\u201d more important ", "dateLastCrawled": "2022-02-02T07:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Improving Image Clustering using <b>Sparse</b> <b>Text</b> and the Wisdom of the Crowds", "url": "https://scholar.cgu.edu/allon-percus/wp-content/uploads/sites/11/2016/02/sparse.pdf", "isFamilyFriendly": true, "displayUrl": "https://scholar.cgu.edu/allon-percus/wp-content/uploads/sites/11/2016/02/<b>sparse</b>.pdf", "snippet": "occurs in an individual <b>document</b> <b>compared</b> with how often it occurs in other documents [8]. TF-IDF has been used for <b>text</b> mining, near duplicate detection, and information retrieval. When dealing with <b>text</b> documents, the natural features to use are words (i.e. delimiting strings by white space to obtain features). We <b>can</b> represent each word by a unique integer. In order to use <b>text</b> processing techniques for image databases, we generate a collection of image words using two steps. First, we ...", "dateLastCrawled": "2021-11-01T18:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Text</b> classification with <b>sparse</b> composite <b>document</b> vectors | Request PDF", "url": "https://www.researchgate.net/publication/311769872_Text_classification_with_sparse_composite_document_vectors", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/311769872_<b>Text</b>_classification_with_<b>sparse</b>...", "snippet": "Request PDF | <b>Text</b> classification with <b>sparse</b> composite <b>document</b> vectors | In this work, we present a modified <b>feature</b> formation technique - graded-weighted Bag of Word Vectors (gwBoWV) by Vivek ...", "dateLastCrawled": "2021-11-08T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "SCDV : <b>Sparse</b> Composite <b>Document</b> Vectors using soft clustering over ...", "url": "https://aclanthology.org/D17-1069/", "isFamilyFriendly": true, "displayUrl": "https://acl<b>anthology</b>.org/D17-1069", "snippet": "We present a <b>feature</b> vector formation technique for documents - <b>Sparse</b> Composite <b>Document</b> Vector (SCDV) - which overcomes several shortcomings of the current distributional paragraph vector representations that are widely used for <b>text</b> representation. In SCDV, word embeddings are clustered to capture multiple semantic contexts in which words occur. They are then chained together to form <b>document</b> topic-vectors that <b>can</b> express complex, multi-topic documents. Through extensive experiments on ...", "dateLastCrawled": "2022-01-22T13:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Sparse</b> Poisson coding for high dimensional <b>document</b> clustering", "url": "https://www.cse.cuhk.edu.hk/lyu/_media/conference/cwu_bigdata2013_sparse.pdf?id=publications%3Aconference&cache=cache", "isFamilyFriendly": true, "displayUrl": "https://www.cse.cuhk.edu.hk/lyu/_media/conference/cwu_bigdata2013_<b>sparse</b>.pdf?id...", "snippet": "effective representation for <b>document</b> clustering <b>compared</b> with state-of-the-art regression methods. Index Terms\u2014<b>document</b> clustering, <b>sparse</b> coding, Poisson regression I. INTRODUCTION During past decade, an explosive growth of <b>text</b> contents on Internet makes Web documents become a kind of typical \u201cBig Data\u201d and brings both opportunities and challenges for knowledge discovery, <b>text</b> mining and information re-trieval. Among these techniques, <b>document</b> clustering plays very important role in ...", "dateLastCrawled": "2021-12-02T06:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Text</b> Classification with NLP: Tf-Idf vs Word2Vec vs <b>BERT</b> | by Mauro Di ...", "url": "https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>text</b>-classification-with-nlp-tf-idf-vs-word2vec-vs-<b>bert</b>...", "snippet": "As you <b>can</b> imagine, this approach causes a significant dimensionality problem: the more documents you have the larger is the vocabulary, so the <b>feature</b> matrix will be a huge <b>sparse</b> matrix. Therefore, the Bag-of-Words model is usually preceded by an important preprocessing (word cleaning, stop words removal, stemming/lemmatization) aimed to reduce the dimensionality problem.", "dateLastCrawled": "2022-02-03T01:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Feature standardization in text classification makes</b> <b>sparse</b> data look ...", "url": "https://www.quora.com/Feature-standardization-in-text-classification-makes-sparse-data-look-non-sparse-Is-this-okay", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>Feature-standardization-in-text-classification-makes</b>-<b>sparse</b>-data...", "snippet": "Answer (1 of 4): This is a trick that Spark ML folks have implemented in their packages for Logistic Regression and other linear models to handle sparsity (details) i.e. you do not want to end up with dense features by doing <b>feature</b> normalization on <b>sparse</b> input. So, if you have a linear model ...", "dateLastCrawled": "2022-01-22T02:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Feature</b> Transformation of <b>Text</b> Data \u2014 NLP | by Prassena Kannan | The ...", "url": "https://medium.com/swlh/feature-transform-of-text-data-nlp-c6ccedbeb3cc", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>feature</b>-transform-of-<b>text</b>-data-nlp-c6ccedbeb3cc", "snippet": "<b>Feature</b> Transformation is the process of converting raw data which <b>can</b> be of <b>Text</b>, Image, Graph, Time series etc\u2026 into numerical <b>feature</b> (Vectors). So that we <b>can</b> perform all algebraic operation ...", "dateLastCrawled": "2022-01-30T07:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Text</b> <b>feature</b> extraction based on deep learning: a review", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5732309/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC5732309", "snippet": "Selection from the <b>document</b> part <b>can</b> reflect the information on the content words, and the calculation of weight is called the <b>text</b> <b>feature</b> extraction . Common methods of <b>text</b> <b>feature</b> extraction include filtration, fusion, mapping, and clustering method. Traditional methods of <b>feature</b> extraction require handcrafted features. To hand-design an effective <b>feature</b> is a lengthy process, and deep learning <b>can</b> be aimed at new applications and quickly acquire new effective characteristic ...", "dateLastCrawled": "2022-01-29T21:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Ultimate Guide <b>To Text</b> Similarity With Python - NewsCatcher", "url": "https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python", "isFamilyFriendly": true, "displayUrl": "https://newscatcherapi.com/blog/ultimate-guide-<b>to-text</b>-similarity-with-python", "snippet": "The first part of this problem is representation. How do we represent the <b>text</b>? We could leave the <b>text</b> as it is or convert it into <b>feature</b> vectors using a suitable <b>text</b> embedding technique. Once we have the <b>text</b> representation, we <b>can</b> compute the similarity score using one of the many distance/similarity measures. Let\u2019s dive deeper into the two aspects of the problem, starting with the similarity measures. Similarity Measures Jaccard Index. Jaccard index, also known as Jaccard similarity ...", "dateLastCrawled": "2022-02-03T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is Extreme Multilabel <b>Text</b> Classification?", "url": "https://analyticsindiamag.com/what-is-extreme-multilabel-text-classification/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/what-is-extreme-multilabel-<b>text</b>-classification", "snippet": "The problem of assigning the most relevant subset of class labels to each <b>document</b> from an extremely large label collection, where the number of labels could reach hundreds of thousands or millions, is known as extreme multi-label <b>text</b> classification (XMTC). In this post, we will have a look at how multi-label and multiclass classification differs from one another, as well as the approaches and techniques used to deal with XMTC. Below is a list of the main points to be discussed in this article.", "dateLastCrawled": "2022-01-25T22:53:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> II - slideshare.net", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-ii", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine</b>-<b>learning</b>-by-<b>analogy</b>-ii", "snippet": "Updated <b>Machine</b> <b>Learning</b> by <b>Analogy</b> presentation that builds to more advanced methods (TensorFlow, geometry/topology-based methods...) and adds a section on ti\u2026", "dateLastCrawled": "2021-12-31T12:33:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "An E\ufb03cient <b>Sparse</b> Metric <b>Learning</b> in High ... - <b>Machine</b> <b>Learning</b>", "url": "http://machinelearning.org/archive/icml2009/papers/46.pdf", "isFamilyFriendly": true, "displayUrl": "<b>machinelearning</b>.org/archive/icml2009/papers/46.pdf", "snippet": "An E\ufb03cient <b>Sparse</b> Metric <b>Learning</b> in High-Dimensional Space via!1-Penalized Log-Determinant Regularization Guo-Jun Qi qi4@illinois.edu Depart. ECE, University of Illinois at Urbana-Champaign, 405 North Mathews Avenue, Urbana, IL 61801 USA Jinhui Tang, Zheng-Jun Zha, Tat-Seng Chua {tangjh, zhazj, chuats}@comp.nus.edu.sg School of Computing, National University of Singapore, Computing 1, 13 Computing Drive, Singapore 117417 Hong-Jiang Zhang hjzhang@microsoft.com Microsoft Advanced Technology ...", "dateLastCrawled": "2021-11-19T11:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "On <b>Machine</b> <b>Learning</b> \u2014 Data, ML &amp; Leadership", "url": "https://bugra.github.io/posts/2014/8/23/on-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://bugra.github.io/posts/2014/8/23/on-<b>machine</b>-<b>learning</b>", "snippet": "<b>Sparse</b> Colorful Filters. Recently, I wrote how we do classification at CB Insights.The post outlines some of the things that I have been thinking about how to apply <b>machine</b> <b>learning</b> for a given problem along with the process that we adopted for the classification problem at CB Insights, but also gave me a good opportunity to reflect even further about the <b>machine</b> <b>learning</b> process; shortcomings of papers, books and even traditional education system when it comes to teach the <b>machine</b> <b>learning</b>.", "dateLastCrawled": "2021-12-10T17:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Handling IP addresses as features when creating <b>machine</b> <b>learning</b> model ...", "url": "https://datascience.stackexchange.com/questions/106454/handling-ip-addresses-as-features-when-creating-machine-learning-model", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/106454/handling-ip-addresses-as...", "snippet": "With info spread across 70,000 features, it can drown out all other features and/or makes it hard to learn anything about individual IPs. And obviously there are billions of potential IP addresses. Treating octets as numbers is not meaningful. There is no ordinal meaning to them; 46.* is not closer to 47.* than 250.*.", "dateLastCrawled": "2022-01-29T05:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning :: Cosine Similarity for Vector</b> Space Models (Part III ...", "url": "https://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/", "isFamilyFriendly": true, "displayUrl": "https://blog.christianperone.com/2013/09/<b>machine</b>-<b>learning</b>-", "snippet": "The tfidf_matrix[0:1] is the Scipy operation to get the first row of the <b>sparse</b> matrix and the resulting array is the Cosine Similarity between the first document with all documents in the set. Note that the first value of the array is 1.0 because it is the Cosine Similarity between the first document with itself. Also note that due to the presence of similar words on the third document (\u201cThe sun in the sky is bright\u201d), it achieved a better score. If you want, you can also solve the ...", "dateLastCrawled": "2022-01-29T21:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Lecture 4: \\(k\\)-Nearest Neighbours and SVM RBFs \u2014 CPSC 330 Applied ...", "url": "https://ubc-cs.github.io/cpsc330/lectures/04_kNNs-SVM-RBF.html", "isFamilyFriendly": true, "displayUrl": "https://ubc-cs.github.io/cpsc330/lectures/04_kNNs-SVM-RBF.html", "snippet": "<b>Analogy</b>-based models ... It does not work well on datasets with many features or where most <b>feature</b> values are 0 most of the time (<b>sparse</b> datasets). Attention. For regular \\(k\\) -NN for supervised <b>learning</b> (not with <b>sparse</b> matrices), you should scale your features. We\u2019ll be looking into it soon. Parametric vs non parametric\u00b6 You might see a lot of definitions of these terms. A simple way to think about this is: do you need to store at least \\(O(n)\\) worth of stuff to make predictions? If ...", "dateLastCrawled": "2022-01-11T11:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine learning MCQs</b> | T4Tutorials.com", "url": "https://t4tutorials.com/machine-learning-mcqs/", "isFamilyFriendly": true, "displayUrl": "https://t4tutorials.com/<b>machine-learning-mcqs</b>", "snippet": "<b>Machine learning MCQs</b>. 1. The general concept and process of forming definitions from examples of concepts to be learned. E. All of these. F. None of these. 2. The computer is the best <b>learning</b> for.", "dateLastCrawled": "2022-01-30T16:47:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(sparse feature)  is like +(text document)", "+(sparse feature) is similar to +(text document)", "+(sparse feature) can be thought of as +(text document)", "+(sparse feature) can be compared to +(text document)", "machine learning +(sparse feature AND analogy)", "machine learning +(\"sparse feature is like\")", "machine learning +(\"sparse feature is similar\")", "machine learning +(\"just as sparse feature\")", "machine learning +(\"sparse feature can be thought of as\")", "machine learning +(\"sparse feature can be compared to\")"]}