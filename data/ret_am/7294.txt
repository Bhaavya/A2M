{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "IFT 6085 - Lecture <b>10 Expressivity and Universal Approximation Theorems</b> ...", "url": "http://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf", "isFamilyFriendly": true, "displayUrl": "mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf", "snippet": "3 <b>Universal</b> <b>Approximation</b> <b>Theorem</b> The <b>universal</b> <b>approximation</b> <b>theorem</b> states that any continuous <b>function</b> f : [0;1]n! [0;1] can be approximated arbitrarily well by a neural network with at least 1 hidden layer with a \ufb01nite number of weights, which is what we are going to illustrate in the next subsections. 3.1 Visual proof of <b>Universal</b> ...", "dateLastCrawled": "2022-01-30T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Neural Networks and the <b>Universal Approximation Theorem</b> | by Milind ...", "url": "https://towardsdatascience.com/neural-networks-and-the-universal-approximation-theorem-8a389a33d30a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/neural-networks-and-the-<b>universal-approximation-theorem</b>...", "snippet": "The <b>function</b> f(x) can be arbitrarily complex. The <b>Universal Approximation Theorem</b> tells us that Neural Networks has a kind of universality i.e. no matter what f(x) is, there is a network that can approximately approach the result and do the job! This result holds for any number of inputs and outputs. image source. If we observe the neural network above, considering the input attributes provided as height and width, our job is to predict the gender of the person. If we exclude all the ...", "dateLastCrawled": "2022-02-03T02:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Universal approximation theorem for artificial neural</b> networks ...", "url": "https://www.fleuchaus.de/en/universal-approximation-theorem-for-artificial-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://www.fleuchaus.de/en/<b>universal-approximation-theorem-for-artificial-neural</b>-networks", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> falls within this category and many physicists with a more \u201capplicative\u201d vocation would be skeptic about its importance and practical implications. I tend to disagree with this viewpoint. The <b>universal</b> <b>theorem</b> tells us that, if we have a <b>function</b>, the treasure quest for an artificial neural network approximating our <b>function</b> is not hopeless. We know that the treasure is there, we need to find it or at least something close enough thereto. POST SCRIPTA ...", "dateLastCrawled": "2022-01-23T06:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Can neural networks solve any problem? | by Brendan Fortuner | Towards ...", "url": "https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/can-neural-networks-really-learn-any-<b>function</b>-65e106617fc6", "snippet": "The <b>Universal</b> <b>Approximation</b> <b>Theorem</b> states that a neural network with 1 hidden layer can approximate any continuous <b>function</b> for inputs within a specific range. If the <b>function</b> jumps around or has large gaps, we won\u2019t be able to approximate it. Additionally, if we train a network on inputs between 10 and 20, we can\u2019t say for sure whether it will work on inputs between 40 and 50.", "dateLastCrawled": "2022-02-02T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>The Universal Approximation Theorem for Neural Networks</b>", "url": "http://mcneela.github.io/machine_learning/2017/03/21/Universal-Approximation-Theorem.html", "isFamilyFriendly": true, "displayUrl": "mcneela.github.io/machine_learning/2017/03/21/<b>Universal</b>-<b>Approximation</b>-<b>Theorem</b>.html", "snippet": "We are now ready to present the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> and its proof. <b>Theorem</b> 1 If the \u03c3 in the neural network definition is a continuous, discriminatory <b>function</b>, then the set of all neural networks is dense in C ( I n) . Proof: Let N \u2282 C ( I n) be the set of neural networks. As mentioned earlier, N is a linear subspace of C ( I n).", "dateLastCrawled": "2022-01-25T22:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Chapter 2 - <b>Universal</b> approximators", "url": "https://chinmayhegde.github.io/fodl/representation02/", "isFamilyFriendly": true, "displayUrl": "https://chinmayhegde.github.io/fodl/representation02", "snippet": "The Weierstrass <b>theorem</b> showed that that the set of all polynomials is a <b>universal</b> approximator. In fact, a generalization of this <b>theorem</b> shows that other families of functions that behave <b>like</b> polynomials are also <b>universal</b> approximators. This is called the Stone-Weierstrass <b>theorem</b>, stated as follows. <b>Theorem</b> (Stone-Weierstrass, 1948.", "dateLastCrawled": "2022-02-01T17:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "machine learning - <b>Neural nets as universal approximators</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/8160183/neural-nets-as-universal-approximators", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/8160183", "snippet": "In fact, the <b>function</b> only needs to be measurable since, by Lusin&#39;s <b>theorem</b>, any measurable <b>function</b> is continuous on nearly all of its domain. This is good enough for the <b>universal</b> <b>approximation</b> <b>theorem</b>. Note, however, that the <b>theorem</b> only says that a <b>function</b> can be represented by a neural net. It does not say whether this representation can ...", "dateLastCrawled": "2022-01-02T10:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Artificial Neural Networks. <b>Universal</b> <b>Function</b> Approximators? | by ...", "url": "https://medium.com/predict/artificial-neural-networks-universal-function-approximators-cf5198224b58", "isFamilyFriendly": true, "displayUrl": "https://medium.com/predict/artificial-neural-networks-<b>universal</b>-<b>function</b>-approximators...", "snippet": "<b>Universal</b> <b>Function</b> <b>Approximation</b> <b>Theorem</b>. Theories are Great! A great <b>theorem</b> with a large name. Wikipedia says, In the mathematical theory of artificial neural networks, the <b>universal</b> ...", "dateLastCrawled": "2022-01-21T10:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Where can I find the proof of the <b>universal approximation theorem</b>?", "url": "https://ai.stackexchange.com/questions/13317/where-can-i-find-the-proof-of-the-universal-approximation-theorem", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/13317/where-can-i-find-the-proof-of-the...", "snippet": "Part 1: <b>Universal</b> <b>Approximation</b>. Here I&#39;ve listed a few recent <b>universal</b> <b>approximation</b> results that come to mind. Remember, <b>universal</b> <b>approximation</b> asks if feed-forward networks (or some other architecture type) can approximate any (in this case continuous) <b>function</b> to arbitrary accuracy (I&#39;ll focus on the : uniformly on compacts sense). Let me mention, that there are two types of guarantees: quantitative ones and qualitative ones.The latter are akin to Hornik&#39;s results (Neural Networks ...", "dateLastCrawled": "2022-02-03T01:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - Can neural networks approximate any <b>function</b> given ...", "url": "https://stackoverflow.com/questions/25609347/can-neural-networks-approximate-any-function-given-enough-hidden-neurons", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/25609347", "snippet": "The only rigorous <b>theorem</b> that exists about the ability of neural networks to approximate different kinds of functions is the <b>Universal</b> <b>Approximation</b> <b>Theorem</b>. The UAT states that any continuous <b>function</b> on a compact domain can be approximated by a <b>neural network</b> with only one hidden layer provided the activation functions used are BOUNDED, continuous and monotonically increasing.", "dateLastCrawled": "2022-01-16T04:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "IFT 6085 - Lecture <b>10 Expressivity and Universal Approximation Theorems</b> ...", "url": "http://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf", "isFamilyFriendly": true, "displayUrl": "mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf", "snippet": "A <b>similar</b> result was independently obtained by Hornik[4] and also by Funahashi[3] using different tools. Hornik\u2019s proof relies on the Stone-Weierstrass <b>Theorem</b> which states that every continuous <b>function</b> de\ufb01ned on a closed interval [a;b] can be uniformly approximated as closely as desired <b>by a polynomial</b> <b>function</b>. 3.3 Kolmogorov-Arnold Representation <b>theorem</b> The Kolmogorov-Arnold representation <b>theorem</b> (or superposition <b>theorem</b>) [5] states that every multivariate continu-ous <b>function</b> can ...", "dateLastCrawled": "2022-01-30T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal</b> <b>approximation</b> <b>theorem</b> - WikiMili, The Best Wikipedia Reader", "url": "https://wikimili.com/en/Universal_approximation_theorem", "isFamilyFriendly": true, "displayUrl": "https://wikimili.com/en/<b>Universal</b>_<b>approximation</b>_<b>theorem</b>", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b>: Fix a continuous <b>function</b> ... The <b>function</b> is not a <b>polynomial</b> if and only if, for every continuous ... In mathematics, the universality of zeta functions is the remarkable ability of the Riemann zeta <b>function</b> and other <b>similar</b> functions to approximate arbitrary non-vanishing holomorphic functions arbitrarily well. A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN). The term MLP is used ambiguously, sometimes loosely to ...", "dateLastCrawled": "2021-10-08T17:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Fundamental Belief: <b>Universal</b> <b>Approximation</b> Theorems", "url": "https://sunju.org/teach/DL-Fall-2020/sep-21-A.pdf", "isFamilyFriendly": true, "displayUrl": "https://sunju.org/teach/DL-Fall-2020/sep-21-A.pdf", "snippet": "[A] <b>universal</b> <b>approximation</b> <b>theorem</b> (UAT) <b>Theorem</b> (UAT, [Cybenko, 1989,Hornik, 1991]) Let \u02d9: R !R be anonconstant, bounded, and continuousfunction. Let I m denote the m-dimensionalunit hypercube [0;1]m. The space ofreal-valued continuous functions on I m is denoted by C(I m). Then, given any &quot;&gt;0 and any <b>function</b> f2C(I", "dateLastCrawled": "2022-01-31T08:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Chapter 2 - <b>Universal</b> approximators", "url": "https://chinmayhegde.github.io/fodl/representation02/", "isFamilyFriendly": true, "displayUrl": "https://chinmayhegde.github.io/fodl/representation02", "snippet": "In fact, <b>polynomial</b> activations are the only ones which don\u2019t work! $\\f$ is a <b>universal</b> approximator iff $\\psi$ is non-<b>polynomial</b>; see Leshno et al. (1993) 9 for a proof. Barron\u2019s method. <b>Universal</b> <b>approximation</b> results of the form discussed above are interesting but, at the end, not very satisfactory.", "dateLastCrawled": "2022-02-01T17:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Universal</b> <b>Approximation</b> with Quadratic Deep Networks", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7076904/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7076904", "snippet": "Since a ReLU quadratic network can represent any univariate <b>polynomial</b> in a unique and global manner, and by the Weierstrass <b>theorem</b> and the Kolmogorov <b>theorem</b> that multivariate functions can be represented through summation and composition of univariate functions, we can approximate any multivariate <b>function</b> with a well-structured ReLU quadratic neural network, justifying the <b>universal</b> <b>approximation</b> power of the quadratic network. To our best knowledge, our quadratic network is the first-of ...", "dateLastCrawled": "2021-12-16T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Can neural networks solve any problem? | by Brendan Fortuner | Towards ...", "url": "https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/can-neural-networks-really-learn-any-<b>function</b>-65e106617fc6", "snippet": "The <b>Universal</b> <b>Approximation</b> <b>Theorem</b> states that a neural network with 1 hidden layer can approximate any continuous <b>function</b> for inputs within a specific range. If the <b>function</b> jumps around or has large gaps, we won\u2019t be able to approximate it. Additionally, if we train a network on inputs between 10 and 20, we can\u2019t say for sure whether it ...", "dateLastCrawled": "2022-02-02T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Relationship between &quot;Neural Networks&quot; and the &quot;<b>Universal</b> <b>Approximation</b> ...", "url": "https://stats.stackexchange.com/questions/561880/relationship-between-neural-networks-and-the-universal-approximation-theorem", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/561880/relationship-between-neural-networks...", "snippet": "Doing some research about this topic, I quickly came across something called the &quot;<b>Universal</b> <b>Approximation</b> <b>Theorem</b>&quot;. ... as it demonstrated that in theory - a 2 layered neural network with a non-<b>polynomial</b> activation <b>function</b> (and likely with an enormous number of neurons) has the ability to approximate any <b>function</b> to an arbitrary level of precision. When this <b>theorem</b> is written in mathematical language, it become much more difficult to &quot;recognize&quot; the neural network amongst the mathematical ...", "dateLastCrawled": "2022-01-26T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Two-hidden-layer feed-<b>forward networks are universal approximators</b>: A ...", "url": "https://www.sciencedirect.com/science/article/pii/S0893608020302628", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0893608020302628", "snippet": "1. Introduction. One of the first results in the development of neural networks is the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> (Cybenko, 1989, Hornik, 1991).This classical result shows that any continuous <b>function</b> on a compact set in R n can be approximated by a multi-layer feed-forward network with only one hidden layer and non-<b>polynomial</b> activation <b>function</b> (like the sigmoid <b>function</b>). It is well-known that this result has two important drawbacks for its practical use: firstly, the width of the ...", "dateLastCrawled": "2021-12-26T21:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why Neural Nets Can Approximate Any <b>Function</b> | by Thomas Hikaru Clark ...", "url": "https://towardsdatascience.com/why-neural-nets-can-approximate-any-function-a878768502f0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-neural-nets-can-approximate-any-<b>function</b>-a878768502f0", "snippet": "The central claim of the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> is that with enough hidden neurons, there exists some set of connection weights that can approximate any <b>function</b> \u2014 even if that <b>function</b> is not something that you could possible write down neatly like f(x)=x\u00b2. Even a crazy, complicated <b>function</b>, like the one that takes as input a 100x100 pixel image and outputs either \u201cdog\u201d or \u201ccat\u201d is covered by this <b>Theorem</b>.", "dateLastCrawled": "2022-01-31T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Learning nonlinear operators via DeepONet</b> based on the <b>universal</b> ...", "url": "https://www.researchgate.net/publication/350158010_Learning_nonlinear_operators_via_DeepONet_based_on_the_universal_approximation_theorem_of_operators", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/350158010_Learning_nonlinear_operators_via...", "snippet": "<b>universal approximation theorem of oper ators</b> is suggestive of the structure and ... Suppose that \u03c3 is a continuous non-<b>polynomial</b> <b>function</b>, X is a Banach . space, K 1 \u2282 X, K 2. \u2282 R. d. are ...", "dateLastCrawled": "2022-02-03T13:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Artificial Neural Networks. <b>Universal</b> <b>Function</b> Approximators? | by ...", "url": "https://medium.com/predict/artificial-neural-networks-universal-function-approximators-cf5198224b58", "isFamilyFriendly": true, "displayUrl": "https://medium.com/predict/artificial-neural-networks-<b>universal</b>-<b>function</b>-approximators...", "snippet": "<b>Universal</b> <b>Function</b> <b>Approximation</b> <b>Theorem</b>. Theories are Great! A great <b>theorem</b> with a large name. Wikipedia says, In the mathematical theory of artificial neural networks, the <b>universal</b> ...", "dateLastCrawled": "2022-01-21T10:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Universal Approximation Theorems</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336361517_Universal_Approximation_Theorems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336361517_<b>Universal_Approximation_Theorems</b>", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> established the density of specific families of neural networks in the space of continuous functions and in certain Bochner spaces, defined between any two ...", "dateLastCrawled": "2022-01-25T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b> - Part 3", "url": "https://alexadam.ca/ml/2018/04/30/universal-approximation-theorem-part-3/", "isFamilyFriendly": true, "displayUrl": "https://alexadam.ca/ml/2018/04/30/<b>universal</b>-<b>approximation</b>-<b>theorem</b>-part-3", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b> - Part 3. By Alex Adam. Neural-Networks . Introduction. It\u2019s been too long since my last post, so expect to see a chain of new posts in the coming months. Back to the UAT madness that I started looking at last year. In part 2 we looked at fitting a relatively simple <b>polynomial</b> <b>function</b> using a single hidden layer deep NN. There is something special about fitting <b>polynomial</b> functions that makes them easy to fit: lack of periodicity. I\u2019ll justify this claim ...", "dateLastCrawled": "2022-01-30T16:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Universal Approximations of Invariant Maps</b> by Neural Networks ...", "url": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "snippet": "Our work <b>can</b> be seen as an extension of results on the <b>universal</b> <b>approximation</b> property of neural networks [ 7, 10, 18, 19, 24, 29, 31, 32] to the setting of group invariant/equivariant maps and/or infinite-dimensional input spaces. Our general results in Sect. 2 are based on classical results of the theory of <b>polynomial</b> invariants [ 16, 17, 46 ].", "dateLastCrawled": "2022-01-23T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Universal Approximation Theorem for Equivariant Maps</b> by Group CNNs | DeepAI", "url": "https://deepai.org/publication/universal-approximation-theorem-for-equivariant-maps-by-group-cnns", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>universal-approximation-theorem-for-equivariant-maps</b>-by...", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b>, which is the main objective of this paper, is one of the most classical mathematical theorems of neural networks. The <b>universal</b> <b>approximation</b> <b>theorem</b> states that a feedforward fully-connected network (FNN) with a single hidden layer containing finite neurons <b>can</b> approximate a continuous <b>function</b> on a compact subset of", "dateLastCrawled": "2022-01-28T13:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Newest &#39;universal-approximation-theorems&#39; Questions</b> - Artificial ...", "url": "https://ai.stackexchange.com/questions/tagged/universal-approximation-theorems", "isFamilyFriendly": true, "displayUrl": "https://ai.stackexchange.com/questions/tagged/<b>universal-approximation-theorems</b>", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> says that MLP with a single hidden layer and enough number of neurons <b>can</b> able to approximate any bounded continuous <b>function</b>. You <b>can</b> validate it from the ... comparison multilayer-perceptrons symbolic-ai <b>universal-approximation-theorems</b>. asked Dec 14 &#39;21 at 13:26. hanugm. 2,783 2 2 gold badges 8 8 silver badges 24 24 bronze badges. 4. votes. 1answer 68 views. Why does the activation <b>function</b> for a hidden layer in a MLP have to be non-<b>polynomial</b>? Across ...", "dateLastCrawled": "2022-01-23T13:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "machine learning - <b>Universal</b> <b>Approximation</b> <b>Theorem</b> for non-sigmoidal ...", "url": "https://cstheory.stackexchange.com/questions/38508/universal-approximation-theorem-for-non-sigmoidal-activation-functions", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/38508/<b>universal</b>-<b>approximation</b>-<b>theorem</b>-for...", "snippet": "I found a proof of the <b>Approximation</b> Theory summarized in Pinkus: <b>Approximation</b> theory of the MLP model in neural networks (1999), where he shows the <b>approximation</b> result for continuous activation functions $\\sigma \\in C(\\mathbb{R})$ if $\\sigma$ is not <b>polynomial</b>. Unfortunately, he only does this for the case of one hidden layer. I am unable to find a proof that also covers multi-hidden-layer neural networks with continuous activation functions that are not sigmoidal.", "dateLastCrawled": "2022-01-26T05:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "4. Fully Connected Deep Networks - <b>TensorFlow for Deep Learning</b> [Book]", "url": "https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/ch04.html", "isFamilyFriendly": true, "displayUrl": "https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/ch04.html", "snippet": "<b>Universal</b> <b>approximation</b> properties are more common in mathematics than one might expect. For example, the Stone-Weierstrass <b>theorem</b> proves that any continuous <b>function</b> on a closed interval <b>can</b> be a suitable <b>polynomial</b> <b>function</b>. Loosening our criteria further, Taylor series and Fourier series themselves offer some <b>universal</b> <b>approximation</b> ...", "dateLastCrawled": "2022-02-02T03:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>GitHub</b> - adeshpande3/<b>Tensorflow-Programs-and-Tutorials</b>: Implementations ...", "url": "https://github.com/adeshpande3/Tensorflow-Programs-and-Tutorials", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/adeshpande3/<b>Tensorflow-Programs-and-Tutorials</b>", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b> (Work in Progress) - The <b>Universal</b> <b>Approximation</b> <b>Theorem</b> states that any feed forward neural network with a single hidden layer <b>can</b> model any <b>function</b>. In this notebook, I&#39;ll go through a practical example of illustrating why this <b>theorem</b> works, and talk about what the implications are for when you&#39;re training your own neural networks.", "dateLastCrawled": "2022-01-31T06:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A visual proof that neural nets <b>can</b> approximate any <b>function</b> | Hacker News", "url": "https://news.ycombinator.com/item?id=19708620", "isFamilyFriendly": true, "displayUrl": "https://news.ycombinator.com/item?id=19708620", "snippet": "The <b>universal</b> <b>approximation</b> <b>theorem</b> guarantees that a finite-width neural network that approximates the <b>function</b> to within some epsilon exists. But, regardless of the <b>approximation</b> method, there is no way to certify that a given <b>approximation</b> method is sufficient for an arbitrary continuous <b>function</b> given only a finite number of samples (i.e., without oracle knowledge of the underlying <b>function</b>), which is the typical situation where neural networks are applied. I <b>can</b> construct a continuous ...", "dateLastCrawled": "2021-09-02T11:49:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal</b> <b>Approximation</b> with Quadratic Deep Networks", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7076904/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC7076904", "snippet": "Since a ReLU quadratic network <b>can</b> represent any univariate <b>polynomial</b> in a unique and global manner, and by the Weierstrass <b>theorem</b> and the Kolmogorov <b>theorem</b> that multivariate functions <b>can</b> be represented through summation and composition of univariate functions, we <b>can</b> approximate any multivariate <b>function</b> with a well-structured ReLU quadratic neural network, justifying the <b>universal</b> <b>approximation</b> power of the quadratic network. To our best knowledge, our quadratic network is the first-of ...", "dateLastCrawled": "2021-12-16T12:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Can</b> neural networks solve any problem? | by Brendan Fortuner | Towards ...", "url": "https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>can</b>-neural-networks-really-learn-any-<b>function</b>-65e106617fc6", "snippet": "The <b>Universal</b> <b>Approximation</b> <b>Theorem</b> states that a neural network with 1 hidden layer <b>can</b> approximate any continuous <b>function</b> for inputs within a specific range. If the <b>function</b> jumps around or has large gaps, we won\u2019t be able to approximate it. Additionally, if we train a network on inputs between 10 and 20, we <b>can</b>\u2019t say for sure whether it will work on inputs between 40 and 50.", "dateLastCrawled": "2022-02-02T08:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why Neural Nets <b>Can</b> Approximate Any <b>Function</b> | by Thomas Hikaru Clark ...", "url": "https://towardsdatascience.com/why-neural-nets-can-approximate-any-function-a878768502f0", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-neural-nets-<b>can</b>-approximate-any-<b>function</b>-a878768502f0", "snippet": "The central claim of the <b>Universal</b> <b>Approximation</b> <b>Theorem</b> is that with enough hidden neurons, there exists some set of connection weights that <b>can</b> approximate any <b>function</b> \u2014 even if that <b>function</b> is not something that you could possible write down neatly like f(x)=x\u00b2. Even a crazy, complicated <b>function</b>, like the one that takes as input a 100x100 pixel image and outputs either \u201cdog\u201d or \u201ccat\u201d is covered by this <b>Theorem</b>. Non-Linearity. The key to neural networks\u2019 ability to ...", "dateLastCrawled": "2022-01-31T18:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Does the <b>universal</b> <b>approximation</b> <b>theorem</b> for neural networks hold for ...", "url": "https://stats.stackexchange.com/questions/325776/does-the-universal-approximation-theorem-for-neural-networks-hold-for-any-activa", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/325776/does-the-<b>universal</b>-<b>approximation</b>...", "snippet": "Does the <b>universal</b> <b>approximation</b> <b>theorem</b> for neural networks hold for any activation <b>function</b> (sigmoid, ReLU, Softmax, etc...) or is it limited to sigmoid functions? Update: As shimao points out in the comments, it doesn&#39;t hold for absolutely any <b>function</b>. So for which class of activation functions does it hold? neural-networks <b>approximation</b>. Share. Cite. Improve this question. Follow edited Jan 30 &#39;18 at 5:11. Skander H. asked Jan 30 &#39;18 at 4:44. Skander H. Skander H. 10.5k 2 2 gold badges ...", "dateLastCrawled": "2022-01-19T03:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Universal Approximations of Invariant Maps</b> by Neural Networks ...", "url": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00365-021-09546-1", "snippet": "Our work <b>can</b> be seen as an extension of results on the <b>universal</b> <b>approximation</b> property of neural networks [ 7, 10, 18, 19, 24, 29, 31, 32] to the setting of group invariant/equivariant maps and/or infinite-dimensional input spaces. Our general results in Sect. 2 are based on classical results of the theory of <b>polynomial</b> invariants [ 16, 17, 46 ].", "dateLastCrawled": "2022-01-23T15:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>ResNet with one-neuron hidden layers is a Universal Approximator</b>", "url": "https://proceedings.neurips.cc/paper/2018/file/03bfc1d4783966c69cc6aef8247e0103-Paper.pdf", "isFamilyFriendly": true, "displayUrl": "https://proceedings.neurips.cc/paper/2018/file/03bfc1d4783966c69cc6aef8247e0103-Paper.pdf", "snippet": "15, 10, 18]. This result is referred to as the <b>universal</b> <b>approximation</b> <b>theorem</b>. Analogous to the classical Stone-Weierstrass <b>theorem</b> on polynomials or the convergence <b>theorem</b> on Fourier series, this <b>theorem</b> implies that the family of neural networks are <b>universal</b> approximators: we <b>can</b> apply neural networks to approximate any continuous <b>function</b> and the accuracy improves as we add more neurons in the width. More importantly, the coef\ufb01cients in the network <b>can</b> be ef\ufb01ciently learned via ...", "dateLastCrawled": "2021-12-29T10:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "A <b>Universal Approximation Theorem</b> of Deep Neural Networks for ...", "url": "https://deepai.org/publication/a-universal-approximation-theorem-of-deep-neural-networks-for-expressing-distributions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-<b>universal-approximation-theorem</b>-of-deep-neural...", "snippet": "Our main result is the following <b>universal approximation theorem</b> for expressing probability distributions. <b>Theorem</b> 3.1 (Main <b>theorem</b>). Let \u03c0 and pz be the target and the source distributions respectively, both defined on Rd. Assume that pz is absolutely continuous with respect to the Lebesgue measure.", "dateLastCrawled": "2022-02-01T11:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Mathematics of Image and Data Analysis Math 5467 Lecture 24: <b>Universal</b> ...", "url": "https://www-users.cse.umn.edu/~jwcalder/5467S21/lec24.pdf", "isFamilyFriendly": true, "displayUrl": "https://www-users.cse.umn.edu/~jwcalder/5467S21/lec24.pdf", "snippet": "Part of the success of neural networks is due to the fact that they <b>can</b> approximate any continuous <b>function</b>, given enough parameters. In particular, we <b>can</b> approxi- mate any <b>function</b> by a 2-layer neural network. \u2022 This cannot fully explain this success, since other methods, like <b>polynomial</b> \ufb01tting, <b>can</b> achieve the same <b>universal</b> <b>approximation</b> results. \u2022 It also does not explain why deeper networks <b>can</b> perform better, or why gradient descent \ufb01nds networks that generalize. Today we\u2019ll ...", "dateLastCrawled": "2021-12-17T14:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>While the universal approximation theorem states a</b> one layer NN <b>can</b> ...", "url": "https://www.quora.com/While-the-universal-approximation-theorem-states-a-one-layer-NN-can-approximate-any-function-wouldnt-most-such-NNs-be-impractical", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>While-the-universal-approximation-theorem-states-a</b>-one-layer-NN...", "snippet": "Answer: Yes. the value of it is purely theoretical: it <b>can</b> be done, but it probably shouldn\u2019t be used in practice. The <b>theorem</b> was mostly to show that the use of a hidden layer solved the theoretical problems with NNs without hidden layers: not only could they solve the XOR problem, they <b>can</b> act...", "dateLastCrawled": "2022-01-09T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Dongwei Chen 1, Fei Hu 2,3,*, Guokui Nian 3,4,5,* and Tiantian Yang", "url": "https://res.mdpi.com/d_attachment/entropy/entropy-22-00193/article_deploy/entropy-22-00193.pdf", "isFamilyFriendly": true, "displayUrl": "https://res.mdpi.com/d_attachment/entropy/entropy-22-00193/article_deploy/entropy-22...", "snippet": "<b>polynomial</b> time. Since the 1980s and 1990s, arti\ufb01cial neural networks have been used to approach deterministic nonlinear continuous functions based on the <b>universal</b> <b>approximation</b> <b>theorem</b>, which states that a neural network with a single hidden layer <b>can</b> approximate any continuous <b>function</b> with compact", "dateLastCrawled": "2021-12-09T10:04:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Universal Approximation Theorem</b>. The power of Neural Networks | by ...", "url": "https://medium.com/swlh/universal-approximation-theorem-d1a1a67c1b5b", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/<b>universal-approximation-theorem</b>-d1a1a67c1b5b", "snippet": "<b>Universal Approximation Theorem</b>, in its lose form, states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate any continuous function. Whoa ...", "dateLastCrawled": "2022-01-28T02:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Universal Approximation Theorem</b>, Neural Nets &amp; Lego Blocks | by ...", "url": "https://medium.com/analytics-vidhya/universal-approximation-theorem-neural-nets-lego-blocks-1f5a7d93542a", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/<b>universal-approximation-theorem</b>-neural-nets-lego...", "snippet": "In this post, we will look at the <b>Universal Approximation Theorem</b> \u2014 one of the fundamental theorems on which the entire concept of Deep <b>Learning</b> is based upon. We will make use of lego blocks ...", "dateLastCrawled": "2022-01-28T22:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Learning</b> Complex Functions using <b>Universal</b> Approximate <b>Theorem</b> - Ai Nxt", "url": "https://ainxt.co.in/learning-complex-functions-using-universal-approximate-theorem/", "isFamilyFriendly": true, "displayUrl": "https://ainxt.co.in/<b>learning</b>-complex-functions-using-<b>universal</b>-approximate-<b>theorem</b>", "snippet": "<b>Universal</b> <b>Approximation</b> <b>Theorem</b>. No matter how complex our output logic is, we can use collection of neurons and form Dense Neural Network to approximate our function. This is known as \u201c<b>UNIVERSAL</b> <b>APPROXIMATION</b> <b>THEOREM</b>\u201c. Lets take an example of Two Dimensional data where y = f(x) i.e. y is some function of x. Now, we need to find that ...", "dateLastCrawled": "2022-01-21T05:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Machine</b> <b>Learning</b> 1", "url": "https://uvaml1.github.io/2020/slides/8.2_NeuralNetworksUniversalApproximationTheorem.pdf", "isFamilyFriendly": true, "displayUrl": "https://uvaml1.github.io/2020/slides/8.2_NeuralNetworks<b>UniversalApproximationTheorem</b>.pdf", "snippet": "<b>Theorem</b>: <b>Universal</b> Approximators!f(x) ... <b>Machine</b> <b>Learning</b> 1 (a) (b) (c) (d) ! N=50 datapoints ! MLP: 2 layers, 3 hidden units with tanh activation function. 1 linear output unit. ! Hidden unit outputs: dashed curves 6 Example: Function Approximations Illustration of the ca-pability of a multilayer perceptron to approximate four different func-,(b) |,) is the Heaviside step function. In data points, shown as blue dots, have been sam-over the interval and the corresponding val-evaluated ...", "dateLastCrawled": "2022-01-22T07:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Universal Approximation Theorems</b> - ResearchGate", "url": "https://www.researchgate.net/publication/336361517_Universal_Approximation_Theorems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/336361517_<b>Universal_Approximation_Theorems</b>", "snippet": "In the <b>machine</b> <b>learning</b> literature, <b>universal</b> <b>approximation</b> refers to a model class\u2019 ability. to generically approximate any member of a large topological space whose elements are. functions, or ...", "dateLastCrawled": "2022-01-25T23:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Illustrative Proof of <b>Universal Approximation Theorem</b> | HackerNoon", "url": "https://hackernoon.com/illustrative-proof-of-universal-approximation-theorem-5845c02822f6", "isFamilyFriendly": true, "displayUrl": "https://hackernoon.com/illustrative-proof-of-<b>universal-approximation-theorem</b>-5845c02822f6", "snippet": "We will talk about the <b>Universal approximation theorem</b> and we will also prove the <b>theorem</b> graphically. The most commonly used sigmoid function is the logistic function, which has a characteristic of an \u201cS\u201d shaped curve. In real life, we deal with complex functions where the relationship between input and output might be complex. To solve this problem, let&#39;s take an <b>analogy</b> of building a house. The way we are going to create complex functions is that we will combine the sigmoids neurons ...", "dateLastCrawled": "2022-02-01T02:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "ne.neural evol - <b>Universal Approximation Theorem</b> \u2014 Neural Networks ...", "url": "https://cstheory.stackexchange.com/questions/17545/universal-approximation-theorem-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://cstheory.stackexchange.com/questions/17545", "snippet": "<b>Universal approximation theorem</b> states that &quot;the standard multilayer feed-forward network with a single hidden layer, ... There is an advanced result, key to <b>machine</b> <b>learning</b>, known as Kolmogorov&#39;s <b>theorem</b> [1]; I have never seen an intuitive sketch of why it works. This may have to do with the different cultures that approach it. The applied <b>learning</b> crowd regards Kolmogorov&#39;s <b>theorem</b> as an existence <b>theorem</b> that merely indicates that NNs may exist, so at least the structure is not overly ...", "dateLastCrawled": "2022-02-03T04:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "neural networks - <b>Universal Approximation Theorem and high dimension</b> ...", "url": "https://stats.stackexchange.com/questions/298622/universal-approximation-theorem-and-high-dimension-linear-regression", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/298622/<b>universal</b>-<b>approximation</b>-<b>theorem</b>-and...", "snippet": "Cross Validated is a question and answer site for people interested in statistics, <b>machine</b> <b>learning</b>, data analysis, data mining, and data visualization. It only takes a minute to sign up. Sign up to join this community", "dateLastCrawled": "2022-01-17T22:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Machine Learning by Analogy</b> - SlideShare", "url": "https://www.slideshare.net/ColleenFarrelly/machine-learning-by-analogy-59094152", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/ColleenFarrelly/<b>machine-learning-by-analogy</b>-59094152", "snippet": "<b>Machine Learning by Analogy</b> 1. Colleen M. Farrelly 2. Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics behind these ...", "dateLastCrawled": "2022-01-31T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [], []], "all_bing_queries": ["+(universal approximation theorem)  is like +(approximation by a polynomial function)", "+(universal approximation theorem) is similar to +(approximation by a polynomial function)", "+(universal approximation theorem) can be thought of as +(approximation by a polynomial function)", "+(universal approximation theorem) can be compared to +(approximation by a polynomial function)", "machine learning +(universal approximation theorem AND analogy)", "machine learning +(\"universal approximation theorem is like\")", "machine learning +(\"universal approximation theorem is similar\")", "machine learning +(\"just as universal approximation theorem\")", "machine learning +(\"universal approximation theorem can be thought of as\")", "machine learning +(\"universal approximation theorem can be compared to\")"]}