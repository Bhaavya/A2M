{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "scikit learn - <b>Area</b> <b>under</b> <b>Precision-Recall</b> <b>Curve</b> (AUC of <b>PR</b>-<b>curve</b>) and ...", "url": "https://stats.stackexchange.com/questions/157012/area-under-precision-recall-curve-auc-of-pr-curve-and-average-precision-ap", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/157012", "snippet": "<b>Area</b> <b>Under</b> <b>PR</b> <b>Curve</b>(AP): 0.65 AP 0.676101781304 AP 0.676101781304 AP 0.676101781304 AP 0.676101781304 scikit-learn <b>precision-recall</b> auc average-<b>precision</b> Share", "dateLastCrawled": "2022-02-02T04:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "python - <b>Generating confidence interval for precision recall</b> <b>curve</b> ...", "url": "https://stackoverflow.com/questions/57482356/generating-confidence-interval-for-precision-recall-curve", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/57482356", "snippet": "I have predictions from a trained model and I can generate a precision-recall <b>curve</b> for the data pretty easily, and thus, also the <b>area</b> <b>under</b> the precision-recall <b>curve</b> (AUPRC). However, I&#39;m trying to also generate a 95% <b>confidence</b> interval for the data, which I&#39;m having a hard time finding something for. I&#39;ve looked in sklearn for python and pROC package for R (which does have some usage for <b>PR</b>, just not AUPRC), but I&#39;m not finding anything outside of some pretty high level academic papers ...", "dateLastCrawled": "2022-01-19T10:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Measuring Performance: AUPRC</b> and Average Precision \u2013 Glass Box", "url": "https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/", "isFamilyFriendly": true, "displayUrl": "https://glassboxmedicine.com/2019/03/02/<b>measuring-performance-auprc</b>", "snippet": "The AUPRC is calculated as the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. A <b>PR</b> <b>curve</b> shows the trade-off between precision and recall across different decision thresholds. (Note that \u201crecall\u201d is another name for the true positive rate (TPR). Thus, AUPRC and AUROC both make use of the TPR. For a review of TPR, precision, and decision thresholds, see Measuring Performance: The Confusion Matrix.) The x-axis of a <b>PR</b> <b>curve</b> is the recall and the y-axis is the precision. This is in contrast to ROC curves, where ...", "dateLastCrawled": "2022-02-03T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Gaining an intuitive understanding of <b>Precision, Recall</b> and <b>Area</b> <b>Under</b> ...", "url": "https://towardsdatascience.com/gaining-an-intuitive-understanding-of-precision-and-recall-3b9df37804a7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/gaining-an-intuitive-<b>under</b>standing-of-precision-and...", "snippet": "Please note that a correct interpretation of a <b>precision-recall</b> <b>curve</b>, however, requires that you also know the ratio of positive samples w.r.t. all samples. 1.1 <b>Area</b> <b>Under</b> the <b>Precision-Recall</b> <b>Curve</b> (<b>PR</b>-AUC) Finally, we arrive at the definition of the metric <b>PR</b>-AUC. The general definition of <b>PR</b>-AUC is finding the <b>area</b> <b>under</b> the precision ...", "dateLastCrawled": "2022-01-31T16:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Confidence</b> Intervals for the <b>Area</b> <b>Under</b> an ROC <b>Curve</b>", "url": "https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/Confidence_Intervals_for_the_Area_Under_an_ROC_Curve.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ncss.com/.../PASS/<b>Confidence</b>_Intervals_for_the_<b>Area</b>_<b>Under</b>_an_ROC_<b>Curve</b>.pdf", "snippet": "<b>Confidence</b> Intervals for the <b>Area</b> <b>Under</b> an ROC <b>Curve</b> Introduction Receiver operating characteristic (ROC) curves are used to assess the accuracy of a diagnostic test. The technique is used when you have a criterion variable which will be used to make a yes or no decision based on the value of this variable. The <b>area</b> <b>under</b> the ROC <b>curve</b> (AUC) is a popular summary index of an ROC <b>curve</b>. This module computes the sample size necessary to achieve a specified width of a <b>confidence</b> interval. We use ...", "dateLastCrawled": "2022-01-28T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Entry 26: Setting thresholds - precision, recall, and</b> ROC - Data ...", "url": "https://julielinx.github.io/blog/26_thresholds_pr_roc/", "isFamilyFriendly": true, "displayUrl": "https://julielinx.github.io/blog/26_thresholds_<b>pr</b>_roc", "snippet": "The <b>area</b> <b>under</b> the <b>curve</b> (AUC) is exactly what it sounds <b>like</b>. <b>The PR</b> <b>curve</b> divides the chart into two sides. The closer the <b>curve</b> is to the upper left the more space will be <b>under</b> that <b>curve</b>. AUC calculates the <b>area</b> that lies underneath the <b>curve</b> to provide a general metric for how well the model performs. <b>PR</b> AUC is the <b>area</b> <b>under</b> the precision / recall <b>curve</b>. Receiver operating characteristic (ROC) <b>curve</b>. This plots the true positive rate (also known as recall and sensitivity) on the y ...", "dateLastCrawled": "2022-01-28T07:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Ml Roc <b>Pr</b> - Plotly", "url": "https://plotly.com/python/roc-and-pr-curves/", "isFamilyFriendly": true, "displayUrl": "https://plotly.com/python/<b>roc-and-pr-curves</b>", "snippet": "In this example, we use the average precision metric, which is an alternative scoring method to the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. In [6]: import plotly.graph_objects as go import plotly.express as px import numpy as np import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.metrics import precision_recall_<b>curve</b>, average_precision_score np. random. seed (0) # Artificially add noise to make task harder df = px. data. iris samples = df. species. sample (n = 30, random ...", "dateLastCrawled": "2022-01-31T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "This is called the ROC <b>area</b> <b>under</b> <b>curve</b> or ROC AUC or sometimes ROCAUC. The score is a value between 0.0 and 1.0 for a perfect classifier. AUCROC can be interpreted as the probability that the scores given by a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. \u2014 Page 54, Learning from Imbalanced Data Sets, 2018. This single score can be used to compare binary classifier models directly. As such, this score might be the most commonly used ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is Considered a Good AUC Score? - Statology", "url": "https://www.statology.org/what-is-a-good-auc-score/", "isFamilyFriendly": true, "displayUrl": "https://www.statology.org/what-is-a-good-auc-score", "snippet": "One way to quantify how well the logistic regression model does at classifying data is to calculate AUC, which stands for \u201c<b>area</b> <b>under</b> <b>curve</b>.\u201d The value for AUC ranges from 0 to 1. A model that has an AUC of 1 is able to perfectly classify observations into classes while a model that has an AUC of 0.5 does no better than a model that performs random guessing.", "dateLastCrawled": "2022-02-03T07:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "The <b>Area Under</b> an ROC <b>Curve</b>", "url": "http://gim.unmc.edu/dxtests/RoC3.htm", "isFamilyFriendly": true, "displayUrl": "gim.unmc.edu/dxtests/RoC3.htm", "snippet": "The <b>area under</b> the <b>curve</b> is the percentage of randomly drawn pairs for which this is true (that is, the test correctly classifies the two patients in the random pair). Computing the <b>area</b> is more difficult to explain and beyond the scope of this introductory material. Two methods are commonly used: a non-parametric method based on constructing ...", "dateLastCrawled": "2022-01-28T21:16:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Area under the Precision-Recall Curve</b>: Point Estimates and <b>Confidence</b> ...", "url": "https://link.springer.com/chapter/10.1007%2F978-3-642-40994-3_29", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-642-40994-3_29", "snippet": "The <b>area under the precision-recall curve</b> (AUCPR) is a single number summary of the information in the precision-recall (<b>PR</b>) <b>curve</b>. <b>Similar</b> to the receiver operating characteristic <b>curve</b>, <b>the PR</b> <b>curve</b> has its own unique properties that make estimating its enclosed <b>area</b> challenging. Besides a point estimate of the <b>area</b>, an interval estimate is often required to express magnitude and uncertainty. In this paper we perform a computational analysis of common AUCPR estimators and their <b>confidence</b> ...", "dateLastCrawled": "2022-01-20T14:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Measuring Performance: AUPRC</b> and Average Precision \u2013 Glass Box", "url": "https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/", "isFamilyFriendly": true, "displayUrl": "https://glassboxmedicine.com/2019/03/02/<b>measuring-performance-auprc</b>", "snippet": "The AUPRC is calculated as the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. A <b>PR</b> <b>curve</b> shows the trade-off between precision and recall across different decision thresholds. (Note that \u201crecall\u201d is another name for the true positive rate (TPR). Thus, AUPRC and AUROC both make use of the TPR. For a review of TPR, precision, and decision thresholds, see Measuring Performance: The Confusion Matrix.) The x-axis of a <b>PR</b> <b>curve</b> is the recall and the y-axis is the precision. This is in contrast to ROC curves, where ...", "dateLastCrawled": "2022-02-03T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "F1 <b>Score</b> vs ROC AUC vs Accuracy vs <b>PR</b> AUC: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-roc-auc-<b>pr</b>-auc", "snippet": "AUC means <b>area</b> <b>under</b> the <b>curve</b> so to speak about ROC AUC <b>score</b> we need to define ROC <b>curve</b> first. It is a chart that visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart. Of course, the higher TPR and the lower FPR is for each threshold the better and so classifiers that have curves that are more top-left-side are better. An extensive discussion of ROC <b>Curve</b> and ROC AUC <b>score</b> ...", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Confidence</b> Intervals for the <b>Area</b> <b>Under</b> an ROC <b>Curve</b>", "url": "https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/Confidence_Intervals_for_the_Area_Under_an_ROC_Curve.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ncss.com/.../PASS/<b>Confidence</b>_Intervals_for_the_<b>Area</b>_<b>Under</b>_an_ROC_<b>Curve</b>.pdf", "snippet": "<b>Confidence</b> Intervals for the <b>Area</b> <b>Under</b> an ROC <b>Curve</b> Introduction Receiver operating characteristic (ROC) curves are used to assess the accuracy of a diagnostic test. The technique is used when you have a criterion variable which will be used to make a yes or no decision based on the value of this variable. The <b>area</b> <b>under</b> the ROC <b>curve</b> (AUC) is a popular summary index of an ROC <b>curve</b>. This module computes the sample size necessary to achieve a specified width of a <b>confidence</b> interval. We use ...", "dateLastCrawled": "2022-01-28T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "machine learning - <b>ROC</b> vs precision-and-recall curves - Cross Validated", "url": "https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/7207", "snippet": "Finally, we show that an algorithm that optimizes the <b>area</b> <b>under</b> the <b>ROC</b> <b>curve</b> is not guaranteed to optimize the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. In other words, in principle, <b>ROC</b> and <b>PR</b> are equally suited to compare results. But for the example case of a result of 20 hits and 1980 misses they show that the differences can be rather drastic, as shown in Figures 11 and 12. Result/<b>curve</b> (I) describes a result where 10 of the 20 hits are in the top ten ranks and the remaining 10 hits are then evenly ...", "dateLastCrawled": "2022-01-27T09:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Ml Roc <b>Pr</b> - Plotly", "url": "https://plotly.com/python/roc-and-pr-curves/", "isFamilyFriendly": true, "displayUrl": "https://plotly.com/python/<b>roc-and-pr-curves</b>", "snippet": "Notice how this ROC <b>curve</b> looks <b>similar</b> to the True Positive Rate <b>curve</b> from the previous plot. This is because they are the same <b>curve</b>, except the x-axis consists of increasing values of FPR instead of threshold, which is why the line is flipped and distorted. We also display the <b>area</b> <b>under</b> the ROC <b>curve</b> (ROC AUC), which is fairly high, thus consistent with our interpretation of the previous plots. In [2]: import plotly.express as px from sklearn.linear_model import LogisticRegression from ...", "dateLastCrawled": "2022-01-31T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>PRECISION</b>-<b>RECALL</b> <b>CURVE</b> \u00b7 Issue #898 \u00b7 ultralytics/<b>yolov3</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/ultralytics/yolov3/issues/898", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/ultralytics/<b>yolov3</b>/issues/898", "snippet": "During testing we evaluate the <b>area</b> <b>under</b> the <b>curve</b> as average <b>precision</b>, AP. The <b>curve</b> should ideally go from P=1, R=0 in the top left towards P=0, R=1 at the bottom right to capture the full AP (<b>area</b> <b>under</b> the <b>curve</b>). By varying conf-thres you can select a single point on the <b>curve</b> to run your model at. Depending on your application, you may ...", "dateLastCrawled": "2022-01-26T07:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "ROC <b>Area</b> <b>Under</b> <b>Curve</b> (AUC) Score. Although the ROC <b>Curve</b> is a helpful diagnostic tool, it can be challenging to compare two or more classifiers based on their curves. Instead, the <b>area</b> <b>under</b> the <b>curve</b> can be calculated to give a single score for a classifier model across all threshold values. This is called the ROC <b>area</b> <b>under</b> <b>curve</b> or ROC AUC ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Assessing the performance of <b>prediction</b> models: a framework for some ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3575184", "snippet": "The performance of <b>prediction</b> models can be assessed using a variety of different methods and metrics. Traditional measures for binary and survival outcomes include the Brier score to indicate overall model performance, the concordance (or c) statistic for discriminative ability (or <b>area</b> <b>under</b> the receiver operating characteristic (ROC) <b>curve</b>), and goodness-of-fit statistics for calibration.. Several new measures have recently been proposed that can be seen as refinements of discrimination ...", "dateLastCrawled": "2022-02-03T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Top Data Science Interview Questions and Answers | by Dr. Dataman ...", "url": "https://medium.com/dataman-in-ai/top-data-science-interview-questions-and-answers-b1395fc2e1e4", "isFamilyFriendly": true, "displayUrl": "https://medium.com/dataman-in-ai/top-data-science-interview-questions-and-answers-b...", "snippet": "The <b>area</b> <b>under</b> the ROC <b>curve</b> (AUC) assesses overall classification performance. The 45 degree dashed line means if you select the record by chance without any model guidance. If the ROC <b>curve</b> is ...", "dateLastCrawled": "2022-01-29T12:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Measuring Performance: AUPRC</b> and Average Precision \u2013 Glass Box", "url": "https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/", "isFamilyFriendly": true, "displayUrl": "https://glassboxmedicine.com/2019/03/02/<b>measuring-performance-auprc</b>", "snippet": "The AUPRC is calculated as the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. A <b>PR</b> <b>curve</b> shows the trade-off between precision and recall across different decision thresholds. (Note that \u201crecall\u201d is another name for the true positive rate (TPR). Thus, AUPRC and AUROC both make use of the TPR. For a review of TPR, precision, and decision thresholds, see Measuring Performance: The Confusion Matrix.) The x-axis of a <b>PR</b> <b>curve</b> is the recall and the y-axis is the precision. This is in contrast to ROC curves, where ...", "dateLastCrawled": "2022-02-03T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Area</b> <b>under</b> the ROC <b>curve \u2013 assessing discrimination in logistic</b> ...", "url": "https://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://thestatsgeek.com/2014/05/05/<b>area</b>-<b>under</b>-the-roc-<b>curve</b>-assessing-discrimination...", "snippet": "Thus the <b>area</b> <b>under</b> the <b>curve</b> ranges from 1, corresponding to perfect discrimination, to 0.5, corresponding to a model with no discrimination ability. The <b>area</b> <b>under</b> the ROC <b>curve</b> is also sometimes referred to as the c-statistic (c for concordance). The <b>area</b> <b>under</b> the estimated ROC <b>curve</b> (AUC) is reported when we plot the ROC <b>curve</b> in R&#39;s Console.", "dateLastCrawled": "2022-01-29T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "data-science-interviews/theory.md at master - <b>GitHub</b>", "url": "https://github.com/alexeygrigorev/data-science-interviews/blob/master/theory.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/<b>alexeygrigorev/data-science-interviews</b>/blob/master/theory.md", "snippet": "What is the <b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>? Is it a useful metric? \u200d\u2b50\ufe0fI. The Precision-Recall AUC is just like the ROC AUC, in that it summarizes the <b>curve</b> with a range of threshold values as a single score. A high <b>area</b> <b>under</b> the <b>curve</b> represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. In which cases AU <b>PR</b> is better than AU ROC? \u200d\u2b50\ufe0f. What is different however is that AU ROC ...", "dateLastCrawled": "2022-02-01T23:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "python - <b>Generating confidence interval for precision recall</b> <b>curve</b> ...", "url": "https://stackoverflow.com/questions/57482356/generating-confidence-interval-for-precision-recall-curve", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/57482356", "snippet": "I have predictions from a trained model and I <b>can</b> generate a precision-recall <b>curve</b> for the data pretty easily, and thus, also the <b>area</b> <b>under</b> the precision-recall <b>curve</b> (AUPRC). However, I&#39;m trying to also generate a 95% <b>confidence</b> interval for the data, which I&#39;m having a hard time finding something for. I&#39;ve looked in sklearn for python and pROC package for R (which does have some usage for <b>PR</b>, just not AUPRC), but I&#39;m not finding anything outside of some pretty high level academic papers ...", "dateLastCrawled": "2022-01-19T10:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "F1 <b>Score</b> vs ROC AUC vs Accuracy vs <b>PR</b> AUC: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-roc-auc-<b>pr</b>-auc", "snippet": "In order to get one number that tells us how good our <b>curve</b> is, we <b>can</b> calculate the <b>Area</b> <b>Under</b> the ROC <b>Curve</b>, or ROC AUC <b>score</b>. The more top-left your <b>curve</b> is the higher the <b>area</b> and hence higher ROC AUC <b>score</b>. Alternatively, it <b>can</b> be shown that ROC AUC <b>score</b> is equivalent to calculating the rank correlation between predictions and targets. From an interpretation standpoint, it is more useful because it tells us that this metric shows how good at ranking predictions your model is. It ...", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "What is the value of the <b>area</b> <b>under</b> the roc <b>curve</b> (AUC) to conclude ...", "url": "https://www.researchgate.net/post/What-is-the-value-of-the-area-under-the-roc-curve-AUC-to-conclude-that-a-classifier-is-excellent", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/post/What-is-the-value-of-the-<b>area</b>-<b>under</b>-the-roc-<b>curve</b>...", "snippet": "&quot;Accuracy is measured by the <b>area</b> <b>under</b> the ROC <b>curve</b>. An <b>area</b> of 1 represents a perfect test; an <b>area</b> of .5 represents a worthless test. A rough guide for classifying the accuracy of a diagnostic ...", "dateLastCrawled": "2022-02-01T16:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "The <b>Area Under</b> an ROC <b>Curve</b>", "url": "http://gim.unmc.edu/dxtests/RoC3.htm", "isFamilyFriendly": true, "displayUrl": "gim.unmc.edu/dxtests/RoC3.htm", "snippet": "The <b>area under</b> the T4 ROC <b>curve</b> is .86. The T4 would be considered to be &quot;good&quot; at separating hypothyroid from euthyroid patients. ROC curves <b>can</b> also be constructed from clinical <b>prediction</b> rules. The graphs at right come from a study of how clinical findings predict strep throat (Wigton RS, Connor JL, Centor RM. Transportability of a decision ...", "dateLastCrawled": "2022-01-28T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 7, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>68\u201395\u201399.7 rule</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/68\u201395\u201399.7_rule", "snippet": "In statistics, the <b>68\u201395\u201399.7 rule</b>, also known as the empirical rule, is a shorthand used to remember the percentage of values that lie within an interval estimate in a normal distribution: 68%, 95%, and 99.7% of the values lie within one, two, and three standard deviations of the mean, respectively.. In mathematical notation, these facts <b>can</b> be expressed as follows, where <b>Pr</b>() is the probability function, \u03a7 is an observation from a normally distributed random variable, \u03bc is the mean ...", "dateLastCrawled": "2022-02-02T22:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "machine learning - Advantages of AUC vs standard <b>accuracy</b> - Data ...", "url": "https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/806", "snippet": "Really great question, and one that I find that most people don&#39;t really understand on an intuitive level. AUC is in fact often preferred over <b>accuracy</b> for binary classification for a number of different reasons. First though, let&#39;s talk about exactly what AUC is. Honestly, for being one of the most widely used efficacy metrics, it&#39;s surprisingly obtuse to figure out exactly how AUC works.. AUC stands for <b>Area</b> <b>Under</b> the <b>Curve</b>, which <b>curve</b> you ask?Well, that would be the ROC <b>curve</b>.ROC stands ...", "dateLastCrawled": "2022-01-27T15:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "r - <b>Custom Precision-Recall AUC measure in</b> mlr3 - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/61612931/custom-precision-recall-auc-measure-in-mlr3", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/61612931", "snippet": "Teams. Q&amp;A for work. Connect and share knowledge within a single location that is structured and easy to search. Learn more", "dateLastCrawled": "2022-01-19T03:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "F1 <b>Score</b> vs ROC AUC vs Accuracy vs <b>PR</b> AUC: Which Evaluation Metric ...", "url": "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/blog/f1-<b>score</b>-accuracy-roc-auc-<b>pr</b>-auc", "snippet": "In order to get one number that tells us how good our <b>curve</b> is, we <b>can</b> calculate the <b>Area</b> <b>Under</b> the ROC <b>Curve</b>, or ROC AUC <b>score</b>. The more top-left your <b>curve</b> is the higher the <b>area</b> and hence higher ROC AUC <b>score</b>. Alternatively, it <b>can</b> be shown that ROC AUC <b>score</b> is equivalent to calculating the rank correlation between predictions and targets. From an interpretation standpoint, it is more useful because it tells us that this metric shows how good at ranking predictions your model is. It ...", "dateLastCrawled": "2022-01-29T12:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Gaining an intuitive understanding of <b>Precision, Recall</b> and <b>Area</b> <b>Under</b> ...", "url": "https://towardsdatascience.com/gaining-an-intuitive-understanding-of-precision-and-recall-3b9df37804a7", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/gaining-an-intuitive-<b>under</b>standing-of-precision-and...", "snippet": "<b>The PR</b>-AUC hence summarizes the <b>precision-recall</b> <b>curve</b> as a single score and <b>can</b> be used to easily compare different binary neural networks models. Please note that the value of <b>the PR</b>-AUC for a perfect classifier amounts to 1.0. The value of <b>PR</b>-AUC for a random classifier is equal to the ratio of positive samples in a dataset w.r.t. all samples.", "dateLastCrawled": "2022-01-31T16:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Area</b> <b>under</b> the ROC <b>curve \u2013 assessing discrimination in logistic</b> ...", "url": "https://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/", "isFamilyFriendly": true, "displayUrl": "https://thestatsgeek.com/2014/05/05/<b>area</b>-<b>under</b>-the-roc-<b>curve</b>-assessing-discrimination...", "snippet": "Thus the <b>area</b> <b>under</b> the <b>curve</b> ranges from 1, corresponding to perfect discrimination, to 0.5, corresponding to a model with no discrimination ability. The <b>area</b> <b>under</b> the ROC <b>curve</b> is also sometimes referred to as the c-statistic (c for concordance). The <b>area</b> <b>under</b> the estimated ROC <b>curve</b> (AUC) is reported when we plot the ROC <b>curve</b> in R&#39;s Console.", "dateLastCrawled": "2022-01-29T07:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "ROC Curves and <b>Precision-Recall Curves for Imbalanced Classification</b>", "url": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/roc-<b>curves</b>-and-precision-recall-<b>curves</b>-for...", "snippet": "ROC <b>Area</b> <b>Under</b> <b>Curve</b> (AUC) Score. Although the ROC <b>Curve</b> is a helpful diagnostic tool, it <b>can</b> be challenging to compare two or more classifiers based on their curves. Instead, the <b>area</b> <b>under</b> the <b>curve</b> <b>can</b> be calculated to give a single score for a classifier model across all threshold values. This is called the ROC <b>area</b> <b>under</b> <b>curve</b> or ROC AUC ...", "dateLastCrawled": "2022-02-02T12:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Assessing the performance of <b>prediction</b> models: a framework for some ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3575184", "snippet": "The performance of <b>prediction</b> models <b>can</b> be assessed using a variety of different methods and metrics. Traditional measures for binary and survival outcomes include the Brier score to indicate overall model performance, the concordance (or c) statistic for discriminative ability (or <b>area</b> <b>under</b> the receiver operating characteristic (ROC) <b>curve</b>), and goodness-of-fit statistics for calibration.. Several new measures have recently been proposed that <b>can</b> be seen as refinements of discrimination ...", "dateLastCrawled": "2022-02-03T05:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Ml Roc <b>Pr</b> - Plotly", "url": "https://plotly.com/python/roc-and-pr-curves/", "isFamilyFriendly": true, "displayUrl": "https://plotly.com/python/<b>roc-and-pr-curves</b>", "snippet": "Before diving into the receiver operating characteristic (ROC) <b>curve</b>, we will look at two plots that will give some context to the thresholds mechanism behind the <b>ROC and PR curves</b>. In the histogram, we observe that the score spread such that most of the positive labels are binned near 1, and a lot of the negative labels are close to 0.", "dateLastCrawled": "2022-01-31T05:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How <b>to Calculate AUC (Area Under Curve</b>) in R - Statology", "url": "https://www.statology.org/auc-in-r/", "isFamilyFriendly": true, "displayUrl": "https://www.statology.org/auc-in-r", "snippet": "How <b>to Calculate AUC (Area Under Curve</b>) in R. Logistic Regression is a statistical method that we use to fit a regression model when the response variable is binary. To assess how well a logistic regression model fits a dataset, we <b>can</b> look at the following two metrics: Sensitivity: The probability that the model predicts a positive outcome for an observation when indeed the outcome is positive. This is also called the \u201ctrue positive rate.\u201d Specificity: The probability that the model ...", "dateLastCrawled": "2022-01-30T17:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Classification: <b>ROC</b> <b>Curve</b> and AUC | Machine Learning Crash Course ...", "url": "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/crash-course/classification/<b>roc</b>-and-auc", "snippet": "To compute the points in an <b>ROC</b> <b>curve</b>, we could evaluate a logistic regression model many times with different classification thresholds, but this would be inefficient. Fortunately, there&#39;s an efficient, sorting-based algorithm that <b>can</b> provide this information for us, called AUC. AUC: <b>Area</b> <b>Under</b> the <b>ROC</b> <b>Curve</b>. AUC stands for &quot;<b>Area</b> <b>under</b> the ...", "dateLastCrawled": "2022-02-02T09:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Understanding the AUC-ROC <b>Curve</b> in Machine Learning Classification", "url": "https://analyticsindiamag.com/understanding-the-auc-roc-curve-in-machine-learning-classification/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/<b>under</b>standing-the-auc-roc-<b>curve</b>-in-machine-learning...", "snippet": "The <b>area</b> <b>under</b> the <b>curve</b> is one of the good ways to estimate the accuracy of the model. An excellent model poses an AUC near to the 1 which tells that it has a good measure of separability. A poor model will have an AUC near 0 which describes that it has the worst measure of separability. In fact, it means it is reciprocating the result and predicting 0s as 1s and 1s as 0s. When an AUC is 0.5, it means the model has no class separation capacity present whatsoever.", "dateLastCrawled": "2022-02-03T00:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - Advantages of AUC vs standard <b>accuracy</b> - Data ...", "url": "https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy", "isFamilyFriendly": true, "displayUrl": "https://datascience.stackexchange.com/questions/806", "snippet": "Really great question, and one that I find that most people don&#39;t really understand on an intuitive level. AUC is in fact often preferred over <b>accuracy</b> for binary classification for a number of different reasons. First though, let&#39;s talk about exactly what AUC is. Honestly, for being one of the most widely used efficacy metrics, it&#39;s surprisingly obtuse to figure out exactly how AUC works.. AUC stands for <b>Area</b> <b>Under</b> the <b>Curve</b>, which <b>curve</b> you ask?Well, that would be the ROC <b>curve</b>.ROC stands ...", "dateLastCrawled": "2022-01-27T15:14:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Machine Learning</b> Glossary | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/glossary/", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/<b>machine-learning</b>/glossary", "snippet": "<b>area</b> <b>under</b> <b>the PR</b> <b>curve</b>. See <b>PR</b> AUC (<b>Area</b> <b>under</b> <b>the PR</b> <b>Curve</b>). <b>area</b> <b>under</b> the ROC <b>curve</b> . See AUC (<b>Area</b> <b>under</b> the ROC <b>curve</b>). artificial general intelligence. A non-human mechanism that demonstrates a broad range of problem solving, creativity, and adaptability. For example, a program demonstrating artificial general intelligence could translate text, compose symphonies, and excel at games that have not yet been invented. artificial intelligence. A non-human program or model that can solve ...", "dateLastCrawled": "2022-02-03T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Learning</b> Curves in <b>Machine</b> <b>Learning</b>", "url": "https://www.researchgate.net/publication/247934703_Learning_Curves_in_Machine_Learning", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/247934703_<b>Learning</b>_<b>Curves</b>_in_<b>Machine</b>_<b>Learning</b>", "snippet": "The <b>area</b> <b>under</b> the receiver operating characteristic (ROC) <b>curve</b> (AUC) was 0.62 (95% confidence interval [CI]: 0.57, 0.68) and the <b>area</b> <b>under</b> the precision\u2010recall <b>curve</b> was 0.58. <b>Learning</b> <b>curve</b> ...", "dateLastCrawled": "2021-12-15T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Understanding <b>AUC</b> - ROC <b>Curve</b> | by Sarang Narkhede | Towards Data Science", "url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>under</b>standing-<b>auc</b>-roc-<b>curve</b>-68b2303cc9c5", "snippet": "In <b>Machine</b> <b>Learning</b>, performance measurement is an essential task. So when it comes to a classification problem, we can count on an <b>AUC</b> - ROC <b>Curve</b>. When we need to check or visualize the performance of the multi-class classification problem, we use the <b>AUC</b> <b>Area</b> <b>Under</b> The <b>Curve</b>) ROC (Receiver Operating Characteristics) <b>curve</b>. It is one of the most important evaluation metrics for checking any classification model\u2019s performance. It is also written as AUROC (<b>Area</b> <b>Under</b> the Receiver Operating ...", "dateLastCrawled": "2022-01-30T12:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Differential and Integral Calculus - Differentiate with Respect to Anything", "url": "https://machinelearningmastery.com/differential-and-integral-calculus-differentiate-with-respect-to-anything/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/differential-and-integral-calculus-differentiate...", "snippet": "The Sweeping <b>Area</b> <b>Analogy</b>. Perhaps a simpler <b>analogy</b> to help us relate integration to differentiation, is to imagine holding one of the thinly cut slices and dragging it rightwards <b>under</b> the <b>curve</b> in infinitesimally small steps. As it moves rightwards, the thinly cut slice will sweep a larger <b>area</b> <b>under</b> the <b>curve</b>, while its height will change according to the shape of the <b>curve</b>. The question that we would like to answer is, at which rate does the <b>area</b> accumulate as the thin slice sweeps ...", "dateLastCrawled": "2022-01-28T21:25:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>What is</b> AUC - <b>ROC</b> in <b>Machine</b> <b>Learning</b> | Overview of <b>ROC</b>", "url": "https://www.mygreatlearning.com/blog/roc-curve/", "isFamilyFriendly": true, "displayUrl": "https://www.mygreat<b>learning</b>.com/blog/<b>roc</b>-<b>curve</b>", "snippet": "By <b>analogy</b>, Higher the AUC, better the model is at distinguishing between patients with the disease and no disease. The <b>ROC</b> <b>curve</b> is plotted with TPR against the FPR where TPR is on the y-axis and FPR is on the x-axis. Defining terms used in AUC and <b>ROC</b> <b>Curve</b>. Consider a two-class prediction problem, in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is ...", "dateLastCrawled": "2022-01-30T19:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Applying <b>machine</b> <b>learning</b> algorithms to predict default probability in ...", "url": "https://www.sciencedirect.com/science/article/pii/S1057521921002878", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S1057521921002878", "snippet": "The results show that, first, based on the AUC (<b>area</b> <b>under</b> the ROC <b>curve</b>) value, accuracy rate and Brier score, the <b>machine</b> <b>learning</b> models can accurately predict the default risk of online borrowers. Second, the integrated discrimination improvement (IDI) test results show that the prediction performance of the <b>machine</b> <b>learning</b> algorithms is significantly better than that of the logistic model. Third, after constructing the investor profit function with misclassification cost, we find that ...", "dateLastCrawled": "2022-01-27T19:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Cohort-Derived <b>Machine Learning</b> Models for Individual Prediction of ...", "url": "https://academic.oup.com/jid/article/224/7/1198/5835004", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jid/article/224/7/1198/5835004", "snippet": "Of 12 761 eligible individuals (median baseline eGFR, 103 mL/minute/1.73 m 2), 1192 (9%) developed a CKD after a median of 8 years.We used 64 static and 502 time-changing variables: Across prediction horizons and algorithms and in contrast to expert-based standard models, most <b>machine learning</b> models achieved state-of-the-art predictive performances with areas <b>under</b> the receiver operating characteristic <b>curve</b> and precision recall <b>curve</b> ranging from 0.926 to 0.996 and from 0.631 to 0.956 ...", "dateLastCrawled": "2021-12-15T15:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "(PDF) <b>Bishop Pattern Recognition And Machine Learning Springer</b> | Xinyue ...", "url": "https://www.academia.edu/34528598/Bishop_Pattern_Recognition_And_Machine_Learning_Springer", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/34528598/<b>Bishop_Pattern_Recognition_And_Machine_Learning_Springer</b>", "snippet": "<b>Bishop Pattern Recognition And Machine Learning Springer</b>. 758 Pages. <b>Bishop Pattern Recognition And Machine Learning Springer</b>. Xinyue Liu. Download Download PDF. Full PDF Package Download Full PDF Package. This Paper. A short summary of this paper. 36 Full PDFs related to this paper. Read Paper. Download Download PDF. Download Full PDF Package ...", "dateLastCrawled": "2022-02-02T07:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "(PDF) Driving skill analysis using <b>machine</b> <b>learning</b> The full <b>curve</b> and ...", "url": "https://www.researchgate.net/publication/261398439_Driving_skill_analysis_using_machine_learning_The_full_curve_and_curve_segmented_cases", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/261398439_Driving_skill_analysis_using...", "snippet": "In the full <b>curve</b> driving scene, principal component analysis and a support vector <b>machine</b>-based method accurately classified drivers in 95.7 % of cases when using driving data about high- and low ...", "dateLastCrawled": "2022-01-07T09:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>machine</b> <b>learning</b> - What is the <b>convex hull</b> in ROC <b>curve</b>? - Cross Validated", "url": "https://stats.stackexchange.com/questions/120361/what-is-the-convex-hull-in-roc-curve", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/120361/what-is-the-<b>convex-hull</b>-in-roc-<b>curve</b>", "snippet": "Taking the <b>convex hull</b> of the ROC <b>curve</b> points is just a way of enforcing a constraint that the estimated ROC <b>curve</b> be <b>convex</b> (concave down in this case). It is equivalent to assuming that the distributions of the marker in the cases and in the controls are unimodal. In situations where this assumption is reasonable then imposing the convexity constraint is warranted.", "dateLastCrawled": "2022-01-18T16:54:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Performance Evaluation of Machine Learning Algorithms in Apache</b> ...", "url": "https://www.researchgate.net/publication/330478085_Performance_Evaluation_of_Machine_Learning_Algorithms_in_Apache_Spark_for_Intrusion_Detection", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/330478085_Performance_Evaluation_of_<b>Machine</b>...", "snippet": "The <b>area under the PR curve is like</b> the ROC. The . difference is that instead of it being a ratio bet ween the true and . false positive rates, it is a r atio between precision and t rue ...", "dateLastCrawled": "2021-11-04T12:46:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(area under the pr curve)  is like +(confidence in a prediction)", "+(area under the pr curve) is similar to +(confidence in a prediction)", "+(area under the pr curve) can be thought of as +(confidence in a prediction)", "+(area under the pr curve) can be compared to +(confidence in a prediction)", "machine learning +(area under the pr curve AND analogy)", "machine learning +(\"area under the pr curve is like\")", "machine learning +(\"area under the pr curve is similar\")", "machine learning +(\"just as area under the pr curve\")", "machine learning +(\"area under the pr curve can be thought of as\")", "machine learning +(\"area under the pr curve can be compared to\")"]}