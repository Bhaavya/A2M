{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b>", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "The <b>counterfactual</b> definition of <b>fairness</b> considers a model fair if upon intervention and change of the race, its output does not change (Kusner et al., 2017). The intervention is usually ...", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness</b> in rankings and recommendations: an overview | SpringerLink", "url": "https://link.springer.com/article/10.1007/s00778-021-00697-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00697-y", "snippet": "<b>Fairness</b> constraints A number of group-based <b>fairness</b> models for ranking focus on the representation (i.e., number of items) of the protected group in the top-k position in the ranking. One such type of group <b>fairness</b> is achieved by constraining the number of items from the different groups that can appear in the top-k positions.Specifically, in the <b>fairness</b> constraints approach [], given a number of protected attributes, or, properties, <b>fairness</b> requirements are expressed by specifying an ...", "dateLastCrawled": "2022-02-03T01:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Chapter 11 Bias and Fairness</b> | Big <b>Data</b> and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "While the training <b>data</b> <b>used</b> for these particular commercial models is not available, many of the widely available public <b>data</b> sets for developing <b>similar</b> facial recognition algorithms <b>have</b> been heavily skewed, with as many as 80% of training images being of white individuals and 75% being of men. Improving the representativeness of these <b>data</b> sets may be helpful, but wouldn\u2019t eliminate the need for ongoing studies of disparate performance of facial recognition across populations that ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Accelerating the Convergence of Human-in-the-Loop Reinforcement ...", "url": "https://deepai.org/publication/accelerating-the-convergence-of-human-in-the-loop-reinforcement-learning-with-counterfactual-explanations", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/accelerating-the-convergence-of-human-in-the-loop...", "snippet": "The requirements for huge <b>data</b> sets <b>like</b> in supervised learning or an environmental reward signal for reinforcement learning are not needed. While the integration of humans into the learning process opens up new possibilities, they also introduce different challenges. Mainly the integration of different feedback modalities and the convergences of the <b>algorithm</b> are challenging. When teaching an agent, humans naturally want to enrich their feedback with explanations, and a too simplistic ...", "dateLastCrawled": "2022-01-08T20:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Fairness</b> On The Ground: Applying Algorithmic <b>Fairness</b> Approaches to ...", "url": "https://www.arxiv-vanity.com/papers/2103.06172/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2103.06172", "snippet": "Many technical approaches <b>have</b> been proposed for ensuring that decisions made by machine learning systems are fair, but few of these proposals <b>have</b> been stress-tested in real-world systems. This paper presents an example of one team\u2019s approach to the challenge of applying algorithmic <b>fairness</b> approaches to complex production systems within the context of a large technology company. We discuss how we disentangle normative questions of product and policy design (<b>like</b>, \u201chow should the ...", "dateLastCrawled": "2021-10-21T21:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Our attempt to answer this question consists in (1) identifying the set of <b>fairness</b>-related <b>characteristics</b> of the real-world scenario at hand, (2) analyzing the behavior of each <b>fairness</b> notion, and then (3) fitting these two elements to recommend the most suitable <b>fairness</b> notion in every specific setup. The results are summarized in a decision diagram that can be <b>used</b> by practitioners and policy makers to navigate the relatively large catalog of ML <b>fairness</b> notions.", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Algorithms as discrimination detectors | <b>PNAS</b>", "url": "https://www.pnas.org/content/117/48/30096", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/48/30096", "snippet": "But it is important to note that there are <b>actually</b> two separate \u201calgorithms\u201d at work in screening applications of the type we are considering: 1) The screening <b>algorithm</b> (or screener) simply takes the <b>characteristics</b> of an individual (<b>like</b> a job applicant) and returns a prediction of this individual\u2019s outcome. This prediction then informs a decision (such as hiring). 2) The training <b>algorithm</b> (or trainer) is what produces the screening <b>algorithm</b>. Constructing the training <b>algorithm</b> ...", "dateLastCrawled": "2022-01-28T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Discrimination</b> in the Age of Algorithms | Journal of Legal Analysis ...", "url": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "snippet": "The terminology here can be confusing since there are <b>actually</b> two algorithms: one <b>algorithm</b> (the \u2018screener\u2019) that for every potential applicant produces an evaluative score (such as an estimate of future performance); and another <b>algorithm</b> (the \u2018trainer\u2019) that uses <b>data</b> to produce the screener that best optimizes some objective function. 8 The distinction is important and often overlooked; we shall emphasize it here.", "dateLastCrawled": "2022-01-28T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Evaluating Explainable AI: Which Algorithmic Explanations</b> Help Users ...", "url": "https://deepai.org/publication/evaluating-explainable-ai-which-algorithmic-explanations-help-users-predict-model-behavior", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>evaluating-explainable-ai-which-algorithmic</b>...", "snippet": "For text <b>data</b>, we use a strategy that is <b>similar</b> to sampling from the perturbation distribution in Ribeiro et al. , which is to randomly substitute words with their neighbors in GloVe word embedding space, sampling neighbors with probability proportional to their similarity. We make a few changes: we 1) decrease probability of token change with the length of sentence, 2) cap the number of edited words at 5 in the chosen perturbation if possible, and 3) limit edited tokens to be nouns, verbs ...", "dateLastCrawled": "2021-12-22T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Social Psychology 1. Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/160266995/social-psychology-1-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/160266995/social-psychology-1-flash-cards", "snippet": "Obedience rates <b>were</b> <b>similar</b> or even higher - 85% in Munich. However, French students <b>were</b> less. p. 217. Reactance. A motive to protect or restore one&#39;s sense of freedom. Reactance arises when someone threatens our freedom of action. p. 220 . Persuasion. the action or fact of persuading someone or of being persuaded to do or believe something.. 228. Central route to persuasion. occurs when interested people focus on the arguments and respond with favorable thoughts. p. 229. Peripheral route ...", "dateLastCrawled": "2022-01-21T16:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b>", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "The <b>counterfactual</b> definition of <b>fairness</b> considers a model fair if upon intervention and change of the race, its output does not change (Kusner et al., 2017). The intervention is usually ...", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Fairness</b> in rankings and recommendations: an overview | SpringerLink", "url": "https://link.springer.com/article/10.1007/s00778-021-00697-y", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s00778-021-00697-y", "snippet": "Another form of individual <b>fairness</b> is <b>counterfactual</b> <b>fairness</b> . The intuition in this case is that an output is fair toward an entity if it is the same in both the actual world and a <b>counterfactual</b> world where the entity belonged to a different group. Causal inference is <b>used</b> to formalize this notion of <b>fairness</b>. Types of group <b>fairness</b> For simplicity, let us assume two groups, namely the protected group \\(G^+\\) and the non-protected (or, privileged) group \\(G^-\\). We will start by ...", "dateLastCrawled": "2022-02-03T01:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fairness</b> On The Ground: Applying Algorithmic <b>Fairness</b> Approaches to ...", "url": "https://www.arxiv-vanity.com/papers/2103.06172/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/2103.06172", "snippet": "Many technical approaches <b>have</b> been proposed for ensuring that decisions made by machine learning systems are fair, but few of these proposals <b>have</b> been stress-tested in real-world systems. This paper presents an example of one team\u2019s approach to the challenge of applying algorithmic <b>fairness</b> approaches to complex production systems within the context of a large technology company. We discuss how we disentangle normative questions of product and policy design (like, \u201chow should the ...", "dateLastCrawled": "2021-10-21T21:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Chapter 11 Bias and Fairness</b> | Big <b>Data</b> and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "While the training <b>data</b> <b>used</b> for these particular commercial models is not available, many of the widely available public <b>data</b> sets for developing <b>similar</b> facial recognition algorithms <b>have</b> been heavily skewed, with as many as 80% of training images being of white individuals and 75% being of men. Improving the representativeness of these <b>data</b> sets may be helpful, but wouldn\u2019t eliminate the need for ongoing studies of disparate performance of facial recognition across populations that ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) A Framework and Benchmarking Study for <b>Counterfactual</b> Generating ...", "url": "https://www.researchgate.net/publication/353208827_A_Framework_and_Benchmarking_Study_for_Counterfactual_Generating_Methods_on_Tabular_Data", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/353208827_A_Framework_and_Benchmarking_Study...", "snippet": "tual <b>data</b> selection (the <b>data</b> instances to explain) and, \ufb01nally, the <b>counterfactual</b> generation. The same models and <b>data</b> are <b>used</b> for all <b>counterfactual</b> explanation generation algorithms.", "dateLastCrawled": "2022-01-21T23:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Our attempt to answer this question consists in (1) identifying the set of <b>fairness</b>-related <b>characteristics</b> of the real-world scenario at hand, (2) analyzing the behavior of each <b>fairness</b> notion, and then (3) fitting these two elements to recommend the most suitable <b>fairness</b> notion in every specific setup. The results are summarized in a decision diagram that can be <b>used</b> by practitioners and policy makers to navigate the relatively large catalog of ML <b>fairness</b> notions.", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Counterfactual</b> state explanations for reinforcement learning agents via ...", "url": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "snippet": "Using the MDP framework, we introduce the concept of a <b>counterfactual</b> state as a <b>counterfactual</b> explanation. 1 More precisely, for an agent in state s performing action a according to its learned policy, a <b>counterfactual</b> state s \u2032 is a state that involves a minimal change to s such that the agent&#39;s policy chooses action a \u2032 instead of a.For example, a <b>counterfactual</b> state can be seen in Fig. 1 for the video game Space Invaders [Brockman et al. ].In this game, an agent exchanges fire with ...", "dateLastCrawled": "2022-01-29T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Algorithms as discrimination detectors | <b>PNAS</b>", "url": "https://www.pnas.org/content/117/48/30096", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/48/30096", "snippet": "But it is important to note that there are <b>actually</b> two separate \u201calgorithms\u201d at work in screening applications of the type we are considering: 1) The screening <b>algorithm</b> (or screener) simply takes the <b>characteristics</b> of an individual (like a job applicant) and returns a prediction of this individual\u2019s outcome. This prediction then informs a decision (such as hiring). 2) The training <b>algorithm</b> (or trainer) is what produces the screening <b>algorithm</b>. Constructing the training <b>algorithm</b> ...", "dateLastCrawled": "2022-01-28T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Discrimination</b> in the Age of Algorithms | Journal of Legal Analysis ...", "url": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "snippet": "<b>Fairness</b> versus efficiency tradeoffs in college admissions using race-aware versus race-blind algorithms. Source: Results from Kleinberg et al. (2018) using <b>data</b> from the NELS: 88 to predict college performance for those students who attended a four-year college (as measured by GPA &lt; 2.75). The graph shows the results of rank-ordering all students by predicted performance and simulating an admission rule that admits the top half; the y-axis shows the percent of the \u201cadmitted class\u201d that ...", "dateLastCrawled": "2022-01-28T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Evaluating Explainable AI: Which Algorithmic Explanations</b> Help Users ...", "url": "https://deepai.org/publication/evaluating-explainable-ai-which-algorithmic-explanations-help-users-predict-model-behavior", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>evaluating-explainable-ai-which-algorithmic</b>...", "snippet": "For text <b>data</b>, we use a strategy that <b>is similar</b> to sampling from the perturbation distribution in Ribeiro et al. , which is to randomly substitute words with their neighbors in GloVe word embedding space, sampling neighbors with probability proportional to their similarity. We make a few changes: we 1) decrease probability of token change with the length of sentence, 2) cap the number of edited words at 5 in the chosen perturbation if possible, and 3) limit edited tokens to be nouns, verbs ...", "dateLastCrawled": "2021-12-22T03:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b>", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "The <b>counterfactual</b> definition of <b>fairness</b> considers a model fair if upon intervention and change of the race, its output does not change (Kusner et al., 2017). The intervention is usually ...", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Fairness judgments and counterfactual thinking:Pricing</b> goods ...", "url": "https://www.researchgate.net/publication/276443166_Fairness_judgments_and_counterfactual_thinkingPricing_goods_versus_services", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/276443166_<b>Fairness</b>_judgments_and...", "snippet": "This has implications for <b>fairness</b> judgments more generally, as it suggests that people may be uneven in which types of <b>data</b> they attend to when making <b>fairness</b> judgments. Three experiments are ...", "dateLastCrawled": "2022-01-19T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Chapter 11 Bias and Fairness</b> | Big <b>Data</b> and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "While the training <b>data</b> <b>used</b> for these particular commercial models is not available, many of the widely available public <b>data</b> sets for developing <b>similar</b> facial recognition algorithms <b>have</b> been heavily skewed, with as many as 80% of training images being of white individuals and 75% being of men. Improving the representativeness of these <b>data</b> sets may be helpful, but wouldn\u2019t eliminate the need for ongoing studies of disparate performance of facial recognition across populations that ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Testing discrimination in practice", "url": "https://fairmlbook.org/testing.html", "isFamilyFriendly": true, "displayUrl": "https://fairmlbook.org/testing.html", "snippet": "In contrast to previous methods, online targeting offers several key advantages to advertisers: granular <b>data</b> collection about individuals, the ability to reach niche audiences (in theory, the audience size <b>can</b> be one, since ad content <b>can</b> be programmatically generated and customized with user attributes as inputs), and the ability to measure conversion (conversion is when someone who views the ad clicks on it, and then takes another action such as a purchase). To date, ad targeting has been ...", "dateLastCrawled": "2022-01-30T14:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Counterfactual</b> state explanations for reinforcement learning agents via ...", "url": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0004370221000060", "snippet": "Using the MDP framework, we introduce the concept of a <b>counterfactual</b> state as a <b>counterfactual</b> explanation. 1 More precisely, for an agent in state s performing action a according to its learned policy, a <b>counterfactual</b> state s \u2032 is a state that involves a minimal change to s such that the agent&#39;s policy chooses action a \u2032 instead of a.For example, a <b>counterfactual</b> state <b>can</b> be seen in Fig. 1 for the video game Space Invaders [Brockman et al. ].In this game, an agent exchanges fire with ...", "dateLastCrawled": "2022-01-29T14:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Using Experimental <b>Data</b> to Evaluate Methods for Observational Causal ...", "url": "https://deepai.org/publication/using-experimental-data-to-evaluate-methods-for-observational-causal-inference", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/using-experimental-<b>data</b>-to-evaluate-methods-for...", "snippet": "An RCT <b>can</b> <b>be thought</b> of as a <b>data</b> set where one potential outcome for every unit is missing at random. Since OSRCT uses the biasing covariates to select treatment, and treatment was assigned randomly, the sub-sampling process only changes the dependence between the biasing covariates and treatment. This is the same as in APO sample. The probability of a given unit-treatment pair being included in the sub-sample is proportional in APO and RCT sampling. That is, D O S R C T is equivalent to a ...", "dateLastCrawled": "2022-01-06T12:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Algorithms as discrimination detectors | <b>PNAS</b>", "url": "https://www.pnas.org/content/117/48/30096", "isFamilyFriendly": true, "displayUrl": "https://<b>www.pnas.org</b>/content/117/48/30096", "snippet": "The screening <b>algorithm</b> is just the mechanical result of running the training <b>algorithm</b> on a set of training <b>data</b>. So while the screening <b>algorithm</b> <b>can</b> produce biased decisions, the place where this algorithmic discrimination gets introduced must come from the human decisions that go into building the training <b>algorithm</b>. In ref. 6, we showed formally that algorithmic bias <b>can</b> be decomposed completely into three components: bias in the choice of input variables, bias in the choice of outcome ...", "dateLastCrawled": "2022-01-28T23:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Evaluating Explainable AI: Which Algorithmic Explanations</b> Help Users ...", "url": "https://deepai.org/publication/evaluating-explainable-ai-which-algorithmic-explanations-help-users-predict-model-behavior", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>evaluating-explainable-ai-which-algorithmic</b>...", "snippet": "For text <b>data</b>, we use a strategy that is <b>similar</b> to sampling from the perturbation distribution in Ribeiro et al. , which is to randomly substitute words with their neighbors in GloVe word embedding space, sampling neighbors with probability proportional to their similarity. We make a few changes: we 1) decrease probability of token change with the length of sentence, 2) cap the number of edited words at 5 in the chosen perturbation if possible, and 3) limit edited tokens to be nouns, verbs ...", "dateLastCrawled": "2021-12-22T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Discrimination</b> in the Age of Algorithms | Journal of Legal Analysis ...", "url": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "isFamilyFriendly": true, "displayUrl": "https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086", "snippet": "The <b>algorithm</b> <b>can</b> only select from among the candidate predictors made available to it by its human designers, but given the choice of training <b>data</b> and outcome, it is the underlying relationships between the variables in the training <b>data</b> that determines which predictors wind up in the screener, and how much weight they get; it is basically just a statistical matter of which variables are most correlated with the outcome.", "dateLastCrawled": "2022-01-28T11:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Social Psychology 1. Flashcards | <b>Quizlet</b>", "url": "https://quizlet.com/160266995/social-psychology-1-flash-cards/", "isFamilyFriendly": true, "displayUrl": "https://<b>quizlet.com</b>/160266995/social-psychology-1-flash-cards", "snippet": "Obedience rates <b>were</b> <b>similar</b> or even higher - 85% in Munich. However, French students <b>were</b> less. p. 217. Reactance. A motive to protect or restore one&#39;s sense of freedom. Reactance arises when someone threatens our freedom of action. p. 220 . Persuasion. the action or fact of persuading someone or of being persuaded to do or believe something.. 228. Central route to persuasion. occurs when interested people focus on the arguments and respond with favorable thoughts. p. 229. Peripheral route ...", "dateLastCrawled": "2022-01-21T16:51:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b>", "url": "https://www.researchgate.net/publication/324600593_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/324600593_<b>Counterfactual_Fairness</b>", "snippet": "The <b>counterfactual</b> definition of <b>fairness</b> considers a model fair if upon intervention and change of the race, its output does not change (Kusner et al., 2017). The intervention is usually ...", "dateLastCrawled": "2021-12-11T00:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Fairness judgments and counterfactual thinking:Pricing</b> goods ...", "url": "https://www.researchgate.net/publication/276443166_Fairness_judgments_and_counterfactual_thinkingPricing_goods_versus_services", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/276443166_<b>Fairness</b>_judgments_and...", "snippet": "This has implications for <b>fairness</b> judgments more generally, as it suggests that people may be uneven in which types of <b>data</b> they attend to when making <b>fairness</b> judgments. Three experiments are ...", "dateLastCrawled": "2022-01-19T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Fairness in Rankings and Recommendations: An Overview</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-rankings-and-recommendations-an-overview", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-rankings-and-recommendations-an-overview</b>", "snippet": "Another form of individual <b>fairness</b> is <b>counterfactual</b> <b>fairness</b> Kusner et al. . The intuition in this case, is that an output is fair towards an entity if it is the same in both the actual world and a <b>counterfactual</b> world where the entity belonged to a different group. Causal inference is <b>used</b> to formalize this notion of <b>fairness</b>.", "dateLastCrawled": "2021-12-21T15:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Chapter 11 Bias and Fairness</b> | Big <b>Data</b> and Social Science", "url": "https://textbook.coleridgeinitiative.org/chap-bias.html", "isFamilyFriendly": true, "displayUrl": "https://textbook.coleridgeinitiative.org/chap-bias.html", "snippet": "While the training <b>data</b> <b>used</b> for these particular commercial models is not available, many of the widely available public <b>data</b> sets for developing <b>similar</b> facial recognition algorithms <b>have</b> been heavily skewed, with as many as 80% of training images being of white individuals and 75% being of men. Improving the representativeness of these <b>data</b> sets may be helpful, but wouldn\u2019t eliminate the need for ongoing studies of disparate performance of facial recognition across populations that ...", "dateLastCrawled": "2022-01-30T01:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Towards a Fair Marketplace: <b>Counterfactual</b> Evaluation of the trade-off ...", "url": "https://www.researchgate.net/publication/328439354_Towards_a_Fair_Marketplace_Counterfactual_Evaluation_of_the_trade-off_between_Relevance_Fairness_Satisfaction_in_Recommendation_Systems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328439354_Towards_a_Fair_Marketplace...", "snippet": "\u2022 <b>Fairness</b>. As a typical <b>data</b>-driven system, recommender systems could be biased by <b>data</b> and the <b>algorithm</b>, arousing increasing concerns on the <b>fairness</b> [4,86, 108]. Specifically, according to ...", "dateLastCrawled": "2022-01-14T16:30:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine learning fairness notions: Bridging the</b> gap with real-world ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001321", "snippet": "Our attempt to answer this question consists in (1) identifying the set of <b>fairness</b>-related <b>characteristics</b> of the real-world scenario at hand, (2) analyzing the behavior of each <b>fairness</b> notion, and then (3) fitting these two elements to recommend the most suitable <b>fairness</b> notion in every specific setup. The results are summarized in a decision diagram that <b>can</b> be <b>used</b> by practitioners and policy makers to navigate the relatively large catalog of ML <b>fairness</b> notions.", "dateLastCrawled": "2022-02-03T07:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Explaining recommender systems <b>fairness</b> and accuracy through the lens ...", "url": "https://www.sciencedirect.com/science/article/pii/S0306457321001503", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0306457321001503", "snippet": "In this work, we <b>have</b> applied an explanatory framework based on regression models to better understand how <b>data</b> <b>characteristics</b> impact on the <b>fairness</b> and accuracy of recommender systems. We considered a suite of <b>data</b> <b>characteristics</b>, which <b>can</b> be classified according to: (i) the structure of the user-rating matrix, (ii) the rating frequency distribution, (iii) the item properties of the user profiles, and (iv) the distribution of rating values. We conducted extensive experiments based on ...", "dateLastCrawled": "2022-02-03T02:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Legal perspective on possible <b>fairness</b> measures \u2013 A legal discussion ...", "url": "https://www.sciencedirect.com/science/article/pii/S026736492100056X", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S026736492100056X", "snippet": "A comparison between individuals <b>can</b> only be <b>performed</b> by <b>comparing</b> the differences of their relations to the respective <b>data</b> set they are part of Guggenberger (2019). Machine learning models are trained with training <b>data</b> sets with which the model learns its prediction. The trained model is then making predictions for new <b>data</b> without being trained anymore. As such, the decision process is defined by the structure of the training set and the conditions set for the model during training ...", "dateLastCrawled": "2021-10-26T02:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "On <b>the Applicability of ML Fairness Notions</b> | DeepAI", "url": "https://deepai.org/publication/on-the-applicability-of-ml-fairness-notions", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/on-<b>the-applicability-of-ml-fairness-notions</b>", "snippet": "The survey of Verma and Rubin [] described a list of <b>fairness</b> notions <b>similar</b> to the list in this survey. To illustrate how each notion <b>can</b> be computed in real scenarios, they <b>used</b> a loan granting real use case (German credit dataset []).Rather than using a benchmark dataset, this survey uses a smaller and fictitious use case (job hiring) which allows to illustrate better the subtle differences between the <b>fairness</b> notions.", "dateLastCrawled": "2021-12-05T00:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "A survey of bias in <b>Machine Learning through the prism of Statistical</b> ...", "url": "https://deepai.org/publication/a-survey-of-bias-in-machine-learning-through-the-prism-of-statistical-parity-for-the-adult-data-set", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/a-survey-of-bias-in-<b>machine-learning-through-the-prism</b>...", "snippet": "This would indeed mean that the decision rules g reinforce the discriminations <b>compared</b> with the reference <b>data</b> on which it was trained. We will then measure hereafter the disparate impacts D I (Y, S) and D I (g, X, S) obtained on our dataset. In Table 1, we <b>have</b> quantified confidence intervals for the bias already present in the original dataset using Eq. (3.1) with the sensitive attributes Gender and Ethnic origin. They <b>were</b> computed using the method of Appendix A.2 and represent the range ...", "dateLastCrawled": "2021-12-05T13:43:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "way of assessing an existing decision making process, it is not as natural as <b>counterfactual fairness</b> in. the context of <b>machine</b> <b>learning</b>. Approximate <b>fairness</b> and model validation. The notion of ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Responsible AI practices \u2013 Google AI", "url": "https://ai.google/responsibilities/responsible-ai-practices/?category=fairness", "isFamilyFriendly": true, "displayUrl": "https://ai.google/responsibilities/responsible-ai-practices/?category=<b>fairness</b>", "snippet": "A case-study on the application of <b>fairness</b> in <b>machine</b> <b>learning</b> research to a production classification system, and new insights in how to measure and address algorithmic <b>fairness</b> issues. Research paper <b>Counterfactual</b> <b>fairness</b> in text classification through robustness Provides and compares multiple approaches for addressing <b>counterfactual</b> <b>fairness</b> issues in text models. Research paper Model Cards for Model Reporting Proposes a framework to encourage transparent model reporting. Research ...", "dateLastCrawled": "2022-02-02T14:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Fairness in Machine Learning: Lessons from Political Philosophy</b> | DeepAI", "url": "https://deepai.org/publication/fairness-in-machine-learning-lessons-from-political-philosophy", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>fairness-in-machine-learning-lessons-from-political</b>...", "snippet": "This discussion suggests that \u2018<b>fairness</b>\u2019 as used in the fair <b>machine</b> <b>learning</b> community is best understood as a placeholder term for a variety of normative egalitarian considerations. Notably, while egalitarianism is a widely held principle, exactly what it requires is the subject of much debate. I provide an overview of some of this debate and finish with implications for the incorporation of \u2018<b>fairness</b>\u2019 into algorithmic decision-making systems.", "dateLastCrawled": "2021-12-26T22:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Counterfactual</b> Explanation of <b>Machine</b> <b>Learning</b> Survival Models - IOS Press", "url": "https://content.iospress.com/articles/informatica/infor468", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/informatica/infor468", "snippet": "A method for <b>counterfactual</b> explanation of <b>machine</b> <b>learning</b> survival models is proposed. One of the difficulties of solving the <b>counterfactual</b> explanation problem is that the classes of examples are implicitly defined through outcomes of a <b>machine</b> <b>learning</b> survival model in the form of survival functions. A condition that establishes the difference between survival functions of the original example and the <b>counterfactual</b> is introduced. This condition is based on using a distance between mean ...", "dateLastCrawled": "2022-01-15T22:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Human-centric Approach to <b>Fairness</b> in AI", "url": "https://www.timlrx.com/blog/a-human-centric-approach-to-fairness-in-ai", "isFamilyFriendly": true, "displayUrl": "https://www.timlrx.com/blog/a-human-centric-approach-to-<b>fairness</b>-in-ai", "snippet": "A lot of what is discussed in the <b>machine</b> <b>learning</b> literature touches on <b>fairness</b> (or rather equivalence in certain outcomes) between groups, yet this narrowly constricts <b>fairness</b> to the notion of equality. Of course, we should think about <b>fairness</b> in the context of prejudiced groups, but we should also ask whether it is fair to an individual. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud ...", "dateLastCrawled": "2022-02-03T02:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "[PDF] A Survey on Bias and <b>Fairness</b> in <b>Machine</b> <b>Learning</b> | Semantic Scholar", "url": "https://www.semanticscholar.org/paper/A-Survey-on-Bias-and-Fairness-in-Machine-Learning-Mehrabi-Morstatter/0090023afc66cd2741568599057f4e82b566137c", "isFamilyFriendly": true, "displayUrl": "https://www.semanticscholar.org/paper/A-Survey-on-Bias-and-<b>Fairness</b>-in-<b>Machine</b>...", "snippet": "This survey investigated different real-world applications that have shown biases in various ways, and created a taxonomy for <b>fairness</b> definitions that <b>machine</b> <b>learning</b> researchers have defined to avoid the existing bias in AI systems. With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for <b>fairness</b> has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive ...", "dateLastCrawled": "2022-01-29T11:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "dimensions of <b>machine</b> Causality and the normative", "url": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "isFamilyFriendly": true, "displayUrl": "https://maxkasy.github.io/home/files/other/ML_inequality_conference/slides_loftus.pdf", "snippet": "dimensions of <b>machine</b> <b>learning</b> Joshua Loftus (LSE Statistics) High level intro Causality, what is it good for? Causal <b>fairness</b> In prediction and ranking tasks, and with intersectionality Designing interventions Optimal fair policies, causal interference Concluding thoughts 2 / 27. Tech solutionism, using ML/AI in every situation 3 / 27. Imagination Albert Einstein: Imagination is more important than knowledge. For knowledge is limited, whereas imagination [...] stimulat[es] progress, giving ...", "dateLastCrawled": "2022-01-11T12:47:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Stable <b>Learning</b> and its Causal Implication", "url": "http://pengcui.thumedialab.com/papers/Stable%20Learning-tutorial-valse2021.pdf", "isFamilyFriendly": true, "displayUrl": "pengcui.thumedialab.com/papers/Stable <b>Learning</b>-tutorial-valse2021.pdf", "snippet": "Application --- <b>counterfactual</b> visual explanations ... Goyal, Yash, et al. &quot;<b>Counterfactual</b> visual explanations.&quot; International Conference on <b>Machine</b> <b>Learning</b>. PMLR, 2019. Explainability with Causality Application --- causal recommendation 17 He et al. \u201dCollaborative Causal Filtering for Out-of-Distribution Recommendation.&quot; Under review. Caual structure among user features and item features Example . Explainability and OOD \u2022Explainability would be a side product when pursuing OOD with ...", "dateLastCrawled": "2022-01-28T03:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Mitigating Political Bias in Language Models Through Reinforced Calibration", "url": "https://www.cs.dartmouth.edu/~rbliu/aaai_copy.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.dartmouth.edu/~rbliu/aaai_copy.pdf", "snippet": "\ufb01rst proposed <b>counterfactual</b> <b>fairness</b>, which treats data samples equally in actual and <b>counterfactual</b> demographic groups. Zhao et al. mitigated gender bias by augmenting original data with gender-swapping and training a unbiased system on the union of two datasets. Other augmentation techniques have reduced gender bias in hate speech detec-tion (Park, Shin, and Fung 2018; Liu et al. 2020), knowledge graph building (Mitchell et al. 2019) and <b>machine</b> transla-tion (Stanovsky, Smith, and ...", "dateLastCrawled": "2022-01-28T20:16:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Counterfactual Fairness \u2013 arXiv Vanity", "url": "https://www.arxiv-vanity.com/papers/1703.06856/", "isFamilyFriendly": true, "displayUrl": "https://www.arxiv-vanity.com/papers/1703.06856", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however, previous decisions have been made that are unfairly biased against certain subpopulations (e.g., those of a particular race, gender, or sexual orientation). Because this past data is often biased, <b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating discriminatory practices (or ...", "dateLastCrawled": "2021-12-15T09:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>Counterfactual Fairness</b> - ResearchGate", "url": "https://www.researchgate.net/publication/315454664_Counterfactual_Fairness", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/315454664_<b>Counterfactual_Fairness</b>", "snippet": "<b>Machine</b> <b>learning</b> has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however ...", "dateLastCrawled": "2022-01-26T14:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Abstract - arXiv", "url": "https://arxiv.org/pdf/1703.06856v3.pdf", "isFamilyFriendly": true, "displayUrl": "https://arxiv.org/pdf/1703.06856v3.pdf", "snippet": "<b>machine</b> <b>learning</b> predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our de\ufb01nition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real ...", "dateLastCrawled": "2020-08-09T05:32:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(counterfactual fairness)  is like +(comparing how an algorithm would have performed if it were used on data with similar characteristics to the data it was actually used on)", "+(counterfactual fairness) is similar to +(comparing how an algorithm would have performed if it were used on data with similar characteristics to the data it was actually used on)", "+(counterfactual fairness) can be thought of as +(comparing how an algorithm would have performed if it were used on data with similar characteristics to the data it was actually used on)", "+(counterfactual fairness) can be compared to +(comparing how an algorithm would have performed if it were used on data with similar characteristics to the data it was actually used on)", "machine learning +(counterfactual fairness AND analogy)", "machine learning +(\"counterfactual fairness is like\")", "machine learning +(\"counterfactual fairness is similar\")", "machine learning +(\"just as counterfactual fairness\")", "machine learning +(\"counterfactual fairness can be thought of as\")", "machine learning +(\"counterfactual fairness can be compared to\")"]}