{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What Are <b>L1</b> and <b>L2</b> <b>Loss</b> Functions? - AfterAcademy", "url": "https://afteracademy.com/blog/what-are-l1-and-l2-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://afteracademy.com/blog/what-are-<b>l1</b>-and-<b>l2</b>-<b>loss</b>-functions", "snippet": "How to decide between <b>L1</b> and <b>L2</b> <b>Loss</b> Function? Generally, <b>L2</b> <b>Loss</b> Function is preferred in most of the cases. But when the outliers are present in the dataset, then the <b>L2</b> <b>Loss</b> Function does not perform well. The reason behind this bad performance is that if the dataset is having outliers, then because of the consideration of the squared ...", "dateLastCrawled": "2022-02-03T04:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Regularization</b> in Deep Learning \u2014 L1, <b>L2</b>, and Dropout | Towards Data ...", "url": "https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>regularization</b>-in-deep-learning-l1-<b>l2</b>-and-dropout-377e...", "snippet": "This learned noise, however, is unique to each <b>training</b> <b>set</b>. As soon as the model sees new data from the same problem domain, but that does not contain this noise, the performance of the neural network gets much worse. \u201cWhy does the neural network picks up that noise in the first place?\u201d The reason for this is that the complexity of this network is too high. A fit of a neural network with higher complexity is shown in the image on the right-hand side. Graph 1. Model with a good fit and ...", "dateLastCrawled": "2022-02-02T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Machine learning methodology: Overfitting, regularization, and all that", "url": "https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2004.pdf", "isFamilyFriendly": true, "displayUrl": "https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194 Fall 2011...", "snippet": "Learning curve = <b>loss</b> on test <b>set</b> as a function of <b>training</b> <b>set</b> size (often averaged over many trials) generalization <b>loss</b> # of examples slow learning This is a way of evaluating learningfast learning algorithms CS194-10 Fall 2011 3. Performance measurement contd. E.g., suppose data generated by quadratic + noise: \u2013 Quadratic h is the realizable case (can express true f up to noise); learns quickly, reaches noise \ufb02oor \u2013 Linear h is the non-realizable case (restricted H or missing ...", "dateLastCrawled": "2022-01-29T22:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Why is <b>my validation loss lower than my training loss</b>? - PyImageSearch", "url": "https://www.pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/.../14/why-is-<b>my-validation-loss-lower-than-my-training-loss</b>", "snippet": "Figure 4: Shifting the <b>training</b> <b>loss</b> plot 1/2 epoch to the left yields more similar plots. Clearly the time of measurement answers the question, \u201cWhy is <b>my validation loss lower than</b> <b>training</b> <b>loss</b>?\u201d. As you can observe, shifting the <b>training</b> <b>loss</b> values a half epoch to the left (bottom) makes the <b>training</b>/validation curves much more similar versus the unshifted (top) plot.Reason #3: The validation <b>set</b> may be easier than the <b>training</b> <b>set</b> (or there may be leaks)", "dateLastCrawled": "2022-02-01T16:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Experiments on Hyperparameter tuning in</b> deep learning \u2014 Rules to follow ...", "url": "https://towardsdatascience.com/experiments-on-hyperparameter-tuning-in-deep-learning-rules-to-follow-efe6a5bb60af", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>experiments-on-hyperparameter-tuning-in</b>-deep-learning...", "snippet": "Using a method somewhere in between, mini-batch gradient descent is popular. Instead of using the entire <b>training</b> <b>set</b> to average the gradients, average over gradient of a small fraction of data-points is used. The size of this batch is a hyper-parameter. To illustrate, consider the <b>loss</b> landscape as shown in the figure below", "dateLastCrawled": "2022-02-02T18:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Guide to Generalization and Regularization in Machine Learning", "url": "https://analyticsindiamag.com/a-guide-to-generalization-and-regularization-in-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://analyticsindiamag.com/a-guide-to-generalization-and-regularization-in-machine...", "snippet": "A <b>training</b> <b>set</b> is a collection of <b>training</b> examples on which the network is trained. A validation <b>set</b> is used to fine-tune hyperparameters <b>like</b> the number of hidden units and the learning rate. A test <b>set</b> designed to evaluate generalization performance. The losses on these subsets are referred to as <b>training</b>, validation, and test <b>loss</b>, in that ...", "dateLastCrawled": "2022-01-31T23:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Neural networks are trained using stochastic gradient descent and require that you choose a <b>loss</b> function when designing and configuring your model. There are many <b>loss</b> functions to choose from and it can be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when <b>training</b> a neural network. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>logistic-regression-with-a-neural-network-mindset</b> | SnailDove&#39;s blog", "url": "https://snaildove.github.io/2018/02/05/logistic-regression-with-a-neural-network-mindset_week1_and_week2/", "isFamilyFriendly": true, "displayUrl": "https://snaildove.github.io/2018/02/05/<b>logistic-regression-with-a-neural-network</b>...", "snippet": "2.1 Implement the L1 and <b>L2</b> <b>loss</b> functions. Exercise: Implement the numpy vectorized version of the L1 <b>loss</b>. You may find the function abs(x) (absolute value of x) useful. Reminder: The <b>loss</b> is used to evaluate the performance of your model. The bigger your <b>loss</b> is, the more different your predictions $(\\hat{y})$ are from the true values $(y)$. In deep learning, you use optimization algorithms <b>like</b> Gradient Descent to train your model and to minimize the cost. L1 <b>loss</b> is defined as: $$\\begin ...", "dateLastCrawled": "2022-02-02T08:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why did the L1 and <b>L2</b> <b>regularization not improve my training</b>/test ...", "url": "https://www.quora.com/Why-did-the-L1-and-L2-regularization-not-improve-my-training-test-accuracy-Instead-it-got-worst", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-did-the-L1-and-<b>L2</b>-<b>regularization-not-improve-my-training</b>...", "snippet": "Answer: Regularization will almost always hurt your <b>training</b> <b>error</b> - that\u2019s expected. Without regularization in place, the <b>training</b> algorithm will typically try to ...", "dateLastCrawled": "2022-01-17T19:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "tensorflow - what does it mean to <b>set</b> kernel_regularizer to be <b>l2</b> ...", "url": "https://stackoverflow.com/questions/51683495/what-does-it-mean-to-set-kernel-regularizer-to-be-l2-regularizer-in-tf-layers-co", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/51683495", "snippet": "I found in other questions that to do <b>L2</b> regularization in convolutional networks using tensorflow the standard way is as follow. For each conv2d layer, <b>set</b> the parameter kernel_regularizer to be <b>l2</b>_regularizer <b>like</b> this. regularizer = tf.contrib.layers.<b>l2</b>_regularizer(scale=0.1) <b>layer2</b> = tf.layers.conv2d( inputs, filters, kernel_size, kernel_regularizer=regularizer)", "dateLastCrawled": "2022-01-20T23:24:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "Graph for -log(x) This is pretty simple, the more your input increases, the more output goes lower. If you have a small input(x=0.5) so the output is going to be high(y=0.305).", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "machine learning - Validation <b>Error</b> less than <b>training</b> <b>error</b>? - Cross ...", "url": "https://stats.stackexchange.com/questions/187335/validation-error-less-than-training-error/205831", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/187335/validation-<b>error</b>-less-than-<b>training</b>...", "snippet": "A Keras model has two modes: <b>training</b> and testing. Regularization mechanisms, such as Dropout and L1/<b>L2</b> weight regularization, are turned off at testing time. They are reflected in the <b>training</b> time <b>loss</b> but not in the test time <b>loss</b>. Besides, the <b>training</b> <b>loss</b> that Keras displays is the average of the losses for each batch of <b>training</b> data, over the current epoch. Because your model is changing over time, the <b>loss</b> over the first batches of an epoch is generally higher than over the last ...", "dateLastCrawled": "2022-02-02T09:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why is <b>my validation loss lower than my training loss</b>? - PyImageSearch", "url": "https://www.pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/.../14/why-is-<b>my-validation-loss-lower-than-my-training-loss</b>", "snippet": "If you add in the regularization <b>loss</b> during validation/testing, your <b>loss</b> values and curves will look more <b>similar</b>. Reason #2: <b>Training</b> <b>loss</b> is measured during each epoch while validation <b>loss</b> is measured after each epoch. On average, the <b>training</b> <b>loss</b> is measured 1/2 an epoch earlier. If you shift your <b>training</b> <b>loss</b> curve a half epoch to the left, your losses will align a bit better. Reason #3: Your validation <b>set</b> may be easier than your <b>training</b> <b>set</b> or there is a leak in your data/bug in ...", "dateLastCrawled": "2022-02-01T16:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Neural networks are trained using stochastic gradient descent and require that you choose a <b>loss</b> function when designing and configuring your model. There are many <b>loss</b> functions to choose from and it can be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when <b>training</b> a neural network. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Regularization</b> in Deep Learning \u2014 L1, <b>L2</b>, and Dropout | Towards Data ...", "url": "https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>regularization</b>-in-deep-learning-l1-<b>l2</b>-and-dropout-377e...", "snippet": "One of the most important aspects when <b>training</b> neural networks is avoiding overfitting. ... During the <b>L2</b> <b>regularization</b> the <b>loss</b> function of the neural network as extended by a so-called <b>regularization</b> term, which is called here \u03a9. Eq. 1 <b>Regularization</b> Term. The <b>regularization</b> term \u03a9 is defined as the Euclidean Norm (or <b>L2</b> norm) of the weight matrices, which is the sum over all squared weight values of a weight matrix. The <b>regularization</b> term is weighted by the scalar alpha divided by ...", "dateLastCrawled": "2022-02-02T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "There are several different common <b>loss</b> functions to choose from: the cross-entropy <b>loss</b>, the mean-squared <b>error</b>, the huber <b>loss</b>, and the hinge <b>loss</b> - just to name a few. Given a particular model, each <b>loss</b> function has particular properties that make it interesting - for example, the (<b>L2</b>-regularized) hinge <b>loss</b> comes with the maximum-margin ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "In order to reduce test errors, how should I go about choosing between ...", "url": "https://www.quora.com/In-order-to-reduce-test-errors-how-should-I-go-about-choosing-between-L2-regularization-DropOut-and-parameter-tuning-on-dev-set-for-width-and-depth-of-the-neural-net", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/In-order-to-reduce-test-<b>errors</b>-how-should-I-go-about-choosing...", "snippet": "Answer: In Classical Supervised learning algos, Test <b>error</b> is bounded by sum of 2 terms . Test <b>Error</b> &lt;= <b>Training</b> <b>Error</b> + \u2018sort-of\u2019 Model Complexity Alongside Test ...", "dateLastCrawled": "2022-01-24T09:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "9.520: Statistical Learning Theory and Applications ebruaryF 8th, 2010 ...", "url": "https://www.mit.edu/~9.520/scribe-notes/class02_scrb_sara.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.mit.edu/~9.520/scribe-notes/class02_scrb_sara.pdf", "snippet": "2.2 <b>Loss</b> unctionsF <b>Loss</b> function V is de ned in order to measure the goodness of our prediction. V(f;z) = V(f(x);y) denotes the price we pay when we see sample xand guess that the associated yaluev is f(x) when it is actually y. orF regression, the most common <b>loss</b> function is square <b>loss</b> or <b>L2</b> <b>loss</b>: V(f(x);y) = (f(x) y)2", "dateLastCrawled": "2022-02-02T17:01:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python - <b>NaN</b> <b>loss</b> when <b>training</b> regression network - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/37232782", "snippet": "To be rigorous, compute this transformation <b>on the training</b> data, not on the entire dataset. For example, with quantile normalization, if an example is in the 60th percentile of the <b>training</b> <b>set</b>, it gets a value of 0.6. (You can also shift the quantile normalized values down by 0.5 so that the 0th percentile is -0.5 and the 100th percentile is ...", "dateLastCrawled": "2022-01-27T22:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Lecture 2: k-nearest neighbors</b> / Curse of Dimensionality", "url": "https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html", "snippet": "Formally, imagine the unit cube [ 0, 1] d. All <b>training</b> data is sampled uniformly within this cube, i.e. \u2200 i, x i \u2208 [ 0, 1] d, and we are considering the k = 10 nearest neighbors of such a test point. Let \u2113 be the edge length of the smallest hyper-cube that contains all k -nearest neighbor of a test point. Then \u2113 d \u2248 k n and \u2113 \u2248 ...", "dateLastCrawled": "2022-02-02T18:01:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "We <b>can</b> achieve this using the Huber <b>Loss</b> (Smooth L1 <b>Loss</b>), a combination of L1 (MAE) and <b>L2</b> (MSE) losses. <b>Can</b> be called Huber <b>Loss</b> or Smooth MAE Less sensitive to outliers in data than the squared ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Beyond <b>L2</b> <b>Loss</b> \u2014 How we experiment with <b>loss</b> functions at Lyft | by ...", "url": "https://eng.lyft.com/beyond-l2-loss-how-we-experiment-with-loss-functions-at-lyft-51f9303f5d2d?source=post_internal_links---------3----------------------------", "isFamilyFriendly": true, "displayUrl": "https://eng.lyft.com/beyond-<b>l2</b>-<b>loss</b>-how-we-experiment-with-<b>loss</b>-functions-at-lyft-51f...", "snippet": "<b>Loss</b> functions <b>can</b> <b>be thought</b> of as trails through a mountain range that <b>can</b> lead to different peaks. Each trail varies in difficulty. The amount of <b>training</b> data is the amount of energy one has to hike up the mountain. The optimization method is the trail guide. The result of all this is a prediction model or a trained hiker. Therefore, to take any trail we need to consider the energy and have the right guide, just as for each <b>loss</b> function we need to consider the amount of <b>training</b> data ...", "dateLastCrawled": "2022-01-14T13:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> (<b>Error</b>) Functions in <b>Machine Learning</b> - 360DigiTMG", "url": "https://360digitmg.com/loss-functions-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://360digitmg.com/<b>loss</b>-functions-in-<b>machine-learning</b>", "snippet": "Regression problems that attempt to predict a continuous value have one <b>set</b> of <b>loss</b> functions while the classification problems where the algorithm attempts to classify the <b>training</b> example into one of the target classes have another <b>set</b> of <b>loss</b>/cost function. Let us look at some of the most commonly used cost functions in <b>machine learning</b> algorithms.", "dateLastCrawled": "2022-01-30T16:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Regularization</b> in Deep Learning \u2014 L1, <b>L2</b>, and Dropout | Towards Data ...", "url": "https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>regularization</b>-in-deep-learning-l1-<b>l2</b>-and-dropout-377e...", "snippet": "<b>Regularization</b> is a <b>set</b> of techniques that <b>can</b> prevent overfitting in neural networks and thus improve the accuracy of a Deep Learning model when facing completely new data from the problem domain. In this article, we will address the most popular <b>regularization</b> techniques which are called L1, <b>L2</b>, and dropout. Table of Content. Recap: Overfitting", "dateLastCrawled": "2022-02-02T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>CNN Encoder-Decoder : Training</b> \u2013 Julian Zaidi&#39;s blog", "url": "https://julianzaidi.wordpress.com/2017/03/21/cnn-encoder-decoder-training/", "isFamilyFriendly": true, "displayUrl": "https://julianzaidi.wordpress.com/2017/03/21/<b>cnn-encoder-decoder-training</b>", "snippet": "The <b>training</b> is realized on a <b>training</b> <b>set</b> composed of 82600 examples. The <b>loss</b> used is the ... This observation <b>can</b> be explained by the <b>L2</b> mean squared <b>error</b>, which encourages the model to produce a rough outline of the predicted object by capturing the overall structure of the missing region, but tends to average together the multiple modes in predictions. The direct consequence is that it gives blurred outputs. Conclusion &amp; Potential ameliorations. This CNN encoder-decoder shows us that ...", "dateLastCrawled": "2022-01-15T18:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the scaling of L\u00b2 regularization in the context of neural ...", "url": "https://towardsdatascience.com/understanding-the-scaling-of-l%C2%B2-regularization-in-the-context-of-neural-networks-e3d25f8b50db", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-scaling-of-l\u00b2-regularization-in-the...", "snippet": "We <b>can</b> think of our <b>training</b> <b>set</b> as a sample of some unseen distribution of unknown complexity. Yet, whatever the complexity of this unseen distribution, it is always true that as our <b>training</b> <b>set</b> grows, so does the probability that it is a representative sample of this unseen source distribution. As a result, it also becomes more representative of any large enough sample of future unseen data (or a test <b>set</b>). In short, the more representative the <b>training</b> <b>set</b>, the less overfitting there is ...", "dateLastCrawled": "2022-02-02T18:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What is it and How to Avoid Overfitting a model? - <b>JournalDev</b>", "url": "https://www.journaldev.com/45052/overfitting-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.<b>journaldev</b>.com/45052/overfitting-in-machine-learning", "snippet": "Although high precision <b>on the training</b> <b>set</b> <b>can</b> always be achieved, what we really want is to build models that generalize well to a ... Regularization is a whole class of similar methods that are used to force the model to simplify itself with the least <b>loss</b> in information. The types of regularization are: L1: A type of regularization that penalizes weights in proportion to the sum of the absolute values of the weights. L1 Regularization. <b>L2</b>: A type of regularization that penalizes weights ...", "dateLastCrawled": "2022-01-30T19:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "PowerPoint Presentation", "url": "https://www.cs.cornell.edu/courses/cs4670/2018sp/lec30-recognition.pptx", "isFamilyFriendly": true, "displayUrl": "https://www.cs.cornell.edu/courses/cs4670/2018sp/lec30-recognition.pptx", "snippet": "Minimize average <b>loss</b> <b>on the training</b> <b>set</b> using SGD. Logistic Regression! Risk. Given: Distribution over (x,y) pairs. A hypothesis from hypothesis class H. <b>Loss</b> function L ...", "dateLastCrawled": "2022-01-21T14:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Why and what to <b>do when neural networks perform poorly on the training</b> <b>set</b>?", "url": "https://www.quora.com/Why-and-what-to-do-when-neural-networks-perform-poorly-on-the-training-set", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/Why-and-what-to-<b>do-when-neural-networks-perform-poorly-on</b>-the...", "snippet": "Answer (1 of 6): One method for improving network generalization is to use a network that is just large enough to provide an adequate fit. The larger network you use, the more complex the functions the network <b>can</b> create. If you use a small enough network, it will not have enough power to overfit...", "dateLastCrawled": "2022-02-02T21:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - <b>NaN</b> <b>loss</b> when <b>training</b> regression network - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/37232782", "snippet": "To be rigorous, compute this transformation <b>on the training</b> data, not on the entire dataset. For example, with quantile normalization, if an example is in the 60th percentile of the <b>training</b> <b>set</b>, it gets a value of 0.6. (You <b>can</b> also shift the quantile normalized values down by 0.5 so that the 0th percentile is -0.5 and the 100th percentile is ...", "dateLastCrawled": "2022-01-27T22:28:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "L1 and <b>L2: loss function and regularization</b> | Develop Paper", "url": "https://developpaper.com/l1-and-l2-loss-function-and-regularization/", "isFamilyFriendly": true, "displayUrl": "https://developpaper.com/l1-and-<b>l2-loss-function-and-regularization</b>", "snippet": "We often see an additional term added after the <b>loss</b> function, which is usually L1 norm, <b>L2</b> norm, which is called L1 regularization and <b>L2</b> regularization in Chinese, or L1 norm and <b>L2</b> function. L1 regularization and <b>L2</b> regularization <b>can</b> be regarded as penalty terms of <b>loss</b> function. The so-called \u201cpunishment\u201d refers to the limitation of ...", "dateLastCrawled": "2022-02-03T00:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Regularization \u2014 Understanding L1 and <b>L2</b> regularization for Deep ...", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-<b>l2</b>...", "snippet": "In L1 regularization, the penalty term used to penalize the cost function <b>can</b> <b>be compared</b> to the log-prior term that is maximized by MAP Bayesian inference when the prior is an isotropic Laplace ...", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Differences between L1 and <b>L2</b> as <b>Loss Function and Regularization</b>", "url": "http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/", "isFamilyFriendly": true, "displayUrl": "www.chioka.in/differences-between-l1-and-<b>l2</b>-as-<b>loss-function-and-regularization</b>", "snippet": "Differences between L1 and <b>L2</b> as <b>Loss Function and Regularization</b>. Posted on Dec 18, 2013 \u2022 lo [2014/11/30: Updated the L1-norm vs <b>L2</b>-norm <b>loss</b> function via a programmatic validated diagram.", "dateLastCrawled": "2022-02-02T23:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>L2</b> vs <b>L1 Regularization in Machine Learning</b> | Ridge and Lasso ...", "url": "https://www.analyticssteps.com/blogs/l2-and-l1-regularization-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.analyticssteps.com/blogs/<b>l2</b>-and-l1-regularization-machine-learning", "snippet": "Therefore, if the insignificant features are huge in number, they <b>can</b> add value to the function in <b>training</b> data, but when the new data comes up that are no connections with these features, the predictions are misinterpreted. So it becomes very important to confine the features to minimizing the plausibility of overfitting while modeling, and hence the process of regularization is preferred. What is Regularization . In regression analysis, the features are estimated using coefficients while ...", "dateLastCrawled": "2022-02-02T11:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Regularization</b> in Deep Learning \u2014 L1, <b>L2</b>, and Dropout | Towards Data ...", "url": "https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>regularization</b>-in-deep-learning-l1-<b>l2</b>-and-dropout-377e...", "snippet": "This learned noise, however, is unique to each <b>training</b> <b>set</b>. As soon as the model sees new data from the same problem domain, but that does not contain this noise, the performance of the neural network gets much worse. \u201cWhy does the neural network picks up that noise in the first place?\u201d The reason for this is that the complexity of this network is too high. A fit of a neural network with higher complexity is shown in the image on the right-hand side. Graph 1. Model with a good fit and ...", "dateLastCrawled": "2022-02-02T18:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Neural networks are trained using stochastic gradient descent and require that you choose a <b>loss</b> function when designing and configuring your model. There are many <b>loss</b> functions to choose from and it <b>can</b> be challenging to know what to choose, or even what a <b>loss</b> function is and the role it plays when <b>training</b> a neural network. In this post, you will", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "python - Compute the <b>Loss</b> <b>of L1 and L2 regularization</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/58905671/compute-the-loss-of-l1-and-l2-regularization", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/58905671", "snippet": "Overfitting describe that your model performs very well in a <b>training</b> <b>set</b>, but fail in the test <b>set</b> (i.e. <b>training</b> accuracy is much better <b>compared</b> with the test <b>set</b> accuracy), you <b>can</b> think of it, that you <b>can</b> solve a problem, that you have been solved before, but <b>can</b>&#39;t solve a similar problem, because you overthinking [Not same problem but similar],so here regularization come to solve this problem.", "dateLastCrawled": "2022-01-29T02:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Why is <b>my validation loss lower than my training loss</b>? - PyImageSearch", "url": "https://www.pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/", "isFamilyFriendly": true, "displayUrl": "https://www.pyimagesearch.com/.../14/why-is-<b>my-validation-loss-lower-than-my-training-loss</b>", "snippet": "Figure 4: Shifting the <b>training</b> <b>loss</b> plot 1/2 epoch to the left yields more similar plots. Clearly the time of measurement answers the question, \u201cWhy is <b>my validation loss lower than</b> <b>training</b> <b>loss</b>?\u201d. As you <b>can</b> observe, shifting the <b>training</b> <b>loss</b> values a half epoch to the left (bottom) makes the <b>training</b>/validation curves much more similar versus the unshifted (top) plot.Reason #3: The validation <b>set</b> may be easier than the <b>training</b> <b>set</b> (or there may be leaks)", "dateLastCrawled": "2022-02-01T16:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "noc20 cs29 assigment 3 - NPTEL", "url": "https://nptel.ac.in/content/storage2/courses/downloads_new/106106139/noc20_cs29_assigment_3.pdf", "isFamilyFriendly": true, "displayUrl": "https://nptel.ac.in/content/storage2/courses/downloads_new/106106139/noc20_cs29...", "snippet": "Ridge Regression shrinks less coefficients to O <b>compared</b> to LASSO. 9) Principal Component Regression (PCR) is an approach to find an orthogonal <b>set</b> of basis vectors which <b>can</b> then be used to reduce the dimension of the input. Which of the following matrices contains the principal component directions as its columns (follow notation from the ...", "dateLastCrawled": "2022-02-02T09:05:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Loss</b> Functions in Neural Networks - theaidream.com", "url": "https://www.theaidream.com/post/loss-functions-in-neural-networks", "isFamilyFriendly": true, "displayUrl": "https://www.theaidream.com/post/<b>loss</b>-functions-in-neural-networks", "snippet": "<b>Loss</b> functions are mainly classified into two different categories Classification <b>loss</b> and Regression <b>Loss</b>. Classification <b>loss</b> is the case where the aim is to predict the output from the different categorical values for example, if we have a dataset of handwritten images and the digit is to be predicted that lies between (0\u20139), in these kinds of scenarios classification <b>loss</b> is used.", "dateLastCrawled": "2022-02-02T10:20:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Regularization \u2014 Understanding L1 and <b>L2</b> regularization for Deep <b>Learning</b>", "url": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-l2-regularization-for-deep-learning-a7b9e4a409bf", "isFamilyFriendly": true, "displayUrl": "https://medium.com/analytics-vidhya/regularization-understanding-l1-and-<b>l2</b>...", "snippet": "Understanding what regularization is and why it is required for <b>machine</b> <b>learning</b> and diving deep to clarify the importance of L1 and <b>L2</b> regularization in Deep <b>learning</b>.", "dateLastCrawled": "2022-02-01T00:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Regularization : What? Why? and How? (Part -1) | by Siddhant Rai ...", "url": "https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "isFamilyFriendly": true, "displayUrl": "https://medium.com/m<b>learning</b>-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea", "snippet": "In general <b>machine</b> <b>learning</b> sense, it is solving an objective function to perform maximum or minimum evaluation. In reality, optimization is lot more profound in usage. Then we have two terms ...", "dateLastCrawled": "2022-01-28T00:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Why <b>Machine Learning Is A Metaphor For</b> Life \u2013 Adit Deshpande ...", "url": "https://adeshpande3.github.io/Why-Machine-Learning-is-a-Metaphor-For-Life", "isFamilyFriendly": true, "displayUrl": "https://adeshpande3.github.io/Why-<b>Machine-Learning-is-a-Metaphor-For</b>-Life", "snippet": "Well, in <b>machine</b> <b>learning</b>, we use regularizers. The first form (and the most common) of regularization that I first learned about was <b>L2</b> regularization or weight decay. This type of regularization is basically imposing a soft constraint on the cost function. We\u2019re telling the network \u201cHey, we want you to minimize the <b>loss</b> from the training ...", "dateLastCrawled": "2022-01-31T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Bias-Variance Decomposition</b> - mlxtend", "url": "http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/", "isFamilyFriendly": true, "displayUrl": "rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp", "snippet": "<b>Bias variance decomposition</b> of <b>machine</b> <b>learning</b> algorithms for various <b>loss</b> functions. from mlxtend.evaluate import bias_variance_decomp. Overview. Often, researchers use the terms bias and variance or &quot;bias-variance tradeoff&quot; to describe the performance of a model -- i.e., you may stumble upon talks, books, or articles where people say that a model has a high variance or high bias. So, what does that mean? In general, we might say that &quot;high variance&quot; is proportional to overfitting, and ...", "dateLastCrawled": "2022-01-31T12:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A <b>Gentle Introduction to Pix2Pix Generative</b> Adversarial Network", "url": "https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/a-<b>gentle-introduction-to-pix2pix-generative</b>...", "snippet": "In <b>analogy</b> to automatic language translation, we define automatic image-to-image translation as the task of translating one possible representation of a scene into another, given sufficient training data. \u2014 Image-to-Image Translation with Conditional Adversarial Networks, 2016. It is a challenging problem that typically requires the development of a specialized model and hand-crafted <b>loss</b> function for the type of translation task being performed. Classical approaches use per-pixel ...", "dateLastCrawled": "2022-02-02T13:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and-sample-codes ...", "url": "https://github.com/gaoisbest/Machine-Learning-and-Deep-Learning-basic-concepts-and-sample-codes/blob/master/README.md", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gaoisbest/<b>Machine</b>-<b>Learning</b>-and-Deep-<b>Learning</b>-basic-concepts-and...", "snippet": "Model 1: accuracy = 92%. Model 2: accuracy = 95%. Model 2 has higher accuracy than model 1, but model 2 is useless. This is called accuracy paradox, which means the model with higher accuracy may not have better generalization power. In general, when TP &lt; FP, the accuracy will always increase when we change the classifier to always output &#39;negative&#39;.Conversely, when TN &lt; FN, the same will happen when we change the classifier to always output &#39;positive&#39; [1].. Recall@k", "dateLastCrawled": "2021-09-22T11:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>machine</b> <b>learning</b> - L1-norm vs <b>l2</b>-norm as cost function when ...", "url": "https://stackoverflow.com/questions/43301036/l1-norm-vs-l2-norm-as-cost-function-when-standardizing", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/43301036", "snippet": "The basic idea/motivation is how to penalize deviations. L1-norm does not care much about outliers, while <b>L2</b>-norm penalize these heavily. This is the basic difference and you will find a lot of pros and cons, even on wikipedia. So in regards to your question if it makes sense when the expected deviations are small: sure, it behaves the same.", "dateLastCrawled": "2022-01-24T10:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What are some <b>examples in everyday life analogous to &#39;overfitting</b>&#39; in ...", "url": "https://www.quora.com/What-are-some-examples-in-everyday-life-analogous-to-overfitting-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-are-some-<b>examples-in-everyday-life-analogous-to-overfitting</b>...", "snippet": "Answer (1 of 3): Exam overfitting - When you study for an exam, only by practicing questions from previous years&#39; exams. You then discover to your horror that xx% of this year&#39;s questions are new, and you get a much lower score than on your practice ones. If you are a bit older, you can expand th...", "dateLastCrawled": "2022-01-06T06:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "On using Huber <b>loss</b> in (Deep) Q-<b>learning</b> | \u30e4\u30ed\u30df\u30eb", "url": "https://jaromiru.wordpress.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://jaromiru.wordpress.com/2017/05/27/on-using-huber-<b>loss</b>-in-deep-q-<b>learning</b>", "snippet": "<b>MACHINE</b> <b>LEARNING</b> &amp; AI. Menu. Let\u2019s make a DQN. Theory; Implementation; Debugging; Full DQN; Double <b>Learning</b> and Prioritized Experience Replay; Let\u2019s make an A3C. Theory ; Implementation; About me; On using Huber <b>loss</b> in (Deep) Q-<b>learning</b>. Posted on May 27, 2017 May 30, 2017 by \u30e4\u30ed\u30df\u30eb. I\u2019ve been recently working on a problem where I put a plain DQN to use. The problem is very simple, deterministic, partially observable and states are quite low-dimensional. The agent however can ...", "dateLastCrawled": "2021-12-26T09:10:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "[D] Looking for papers on treating regression as classification vs ...", "url": "https://www.reddit.com/r/MachineLearning/comments/7gun87/d_looking_for_papers_on_treating_regression_as/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/r/<b>MachineLearning</b>/comments/7gun87/d_looking_for_papers_on...", "snippet": "Doing the <b>L2 loss is like</b> doing maximum likelihood on a gaussian with a fixed variance - so the bad regression here is largely coming from the gaussian being mis-specified. I think the richer question would involve comparing approaches that consider the ordering vs. approaches that don t consider the ordering but where both have flexible enough distributions.", "dateLastCrawled": "2021-01-17T22:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Deep learning</b> - lectures.alex.balgavy.eu", "url": "https://lectures.alex.balgavy.eu/ml-notes/deep-learning/", "isFamilyFriendly": true, "displayUrl": "https://lectures.alex.balgavy.eu/ml-notes/<b>deep-learning</b>", "snippet": "<b>Deep learning</b> <b>Deep learning</b> systems (autodiff engines) Tensors. To scale up backpropagation, want to move from operations on scalars to tensors. Tensor: generalisation of vectors/matrices to higher dimensions. e.g. a 2-tensor has two dimensions, a 4-tensor has 4 dimensions. You can represent data as a tensor. e.g. an RGB image is a 3-tensor of the red, green, and blue values for each pixel. Functions on tensors. Functions have inputs and outputs, all of which are tensors. They implement ...", "dateLastCrawled": "2021-12-15T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A deep <b>learning</b> framework for constitutive modeling based on temporal ...", "url": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0021999121006793", "snippet": "These two features meet the requirement for sequence modeling in <b>machine</b> <b>learning</b>. Therefore, the nonlinear constitutive models may be classified as sequence modeling from the viewpoint of <b>machine</b> <b>learning</b>. Concrete material and steel material both exhibit significant ultra-long-term memory effects and many model-driven constitutive relationships were developed to simulate stress-strain curves of materials , , , , with ultra-long-term memory effect. For steel material, the traditional ...", "dateLastCrawled": "2022-01-20T12:37:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(l2 loss)  is like +(error on the training set)", "+(l2 loss) is similar to +(error on the training set)", "+(l2 loss) can be thought of as +(error on the training set)", "+(l2 loss) can be compared to +(error on the training set)", "machine learning +(l2 loss AND analogy)", "machine learning +(\"l2 loss is like\")", "machine learning +(\"l2 loss is similar\")", "machine learning +(\"just as l2 loss\")", "machine learning +(\"l2 loss can be thought of as\")", "machine learning +(\"l2 loss can be compared to\")"]}