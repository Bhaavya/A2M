{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov Decision Processes</b>", "url": "https://inst.eecs.berkeley.edu/~cs188/fa20/assets/slides/lec8.pdf", "isFamilyFriendly": true, "displayUrl": "https://inst.eecs.berkeley.edu/~cs188/fa20/assets/slides/lec8.pdf", "snippet": "<b>Markov Decision Processes</b> oAn <b>MDP</b> is defined by: oA set of states s \u00ceS oA set of actions a \u00ceA oA transition function T(s, a, s\u2019) oProbability that a from s leads to s\u2019, i.e., P(s\u2019| s, a) oAlso called the model or the dynamics oA reward function R(s, a, s\u2019) oSometimes just R(s) or R(s\u2019) oA start state oMaybe a terminal state", "dateLastCrawled": "2022-01-18T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "ARTIFICIAL INTELLIGENCE <b>Markov</b> <b>decision</b> processes", "url": "http://www.cs.uu.nl/docs/vakken/b2ki/LastYear/Docs/Slides/mdps-handouts.pdf", "isFamilyFriendly": true, "displayUrl": "www.cs.uu.nl/docs/vakken/b2ki/LastYear/Docs/Slides/<b>mdp</b>s-handouts.pdf", "snippet": "<b>MDP</b> (<b>Markov decision process</b>) Partially observable ... A <b>maze</b>\u2010<b>like</b> problem The agent lives in a grid, where walls block the agent\u2019s path Noisymovement: actions do not always go as planned If wall in chosen direction, then stay put; 80% of the time, the action North takes the agent North 10% of the time, North takes the agent West; ...", "dateLastCrawled": "2021-09-17T17:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Lecture 6: <b>Markov</b> <b>Decision</b> Processes", "url": "https://shuaili8.github.io/Teaching/CS410/L6_mdp.pdf", "isFamilyFriendly": true, "displayUrl": "https://shuaili8.github.io/Teaching/CS410/L6_<b>mdp</b>.pdf", "snippet": "\u2022A <b>maze</b>-<b>like</b> problem \u2022The agent lives in a grid \u2022Walls block the agent\u2019s path \u2022Noisy movement: actions do not always go as planned \u202280% of the time, the action North takes the agent North (if there is no wall there) \u202210% of the time, North takes the agent West; 10% East \u2022If there is a wall in the direction the agent would have been taken, the agent stays put \u2022The agent receives rewards \u2022Small \u201cliving\u201d reward each step(can be negative) \u2022Big rewards come at the end ...", "dateLastCrawled": "2022-01-02T21:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Reinforcement Learning : <b>Markov-Decision Process</b> (Part 1) | by ...", "url": "https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-reinforcement-learning-<b>markov-decision</b>...", "snippet": "Reinforcement Learning : <b>Markov-Decision Process</b> (Part 1) In a typical Reinforcement Learning (RL) problem, there is a learner and a <b>decision</b> maker called agent and the surrounding with which it interacts is called environment. The environment, in return, provides rewards and a new state based on the actions of the agent.", "dateLastCrawled": "2022-02-02T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Understanding <b>Markov Decision Process</b>: The Framework Behind ...", "url": "https://towardsdatascience.com/understanding-markov-decision-process-the-framework-behind-reinforcement-learning-4b5166f3c5b4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-<b>markov-decision-process</b>-the-framework...", "snippet": "The <b>Markov</b> Reward <b>Process</b> (MRP) is an extension of the <b>Markov</b> chain with an additional reward function. So, it consists of states, a transition probability, and a reward function. Photo by Jeremy Cai on Unsplash. This reward function gives us the reward that we get from each state. This function will tell us the reward we obtain in the state cloudy, the reward we obtain in the state of windy, and the rainy state. This reward also can be a positive or negative value. <b>Markov Decision Process</b> ...", "dateLastCrawled": "2022-01-27T01:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Reinforcement Learning Tutorial</b> - Javatpoint", "url": "https://www.javatpoint.com/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/reinforcement-learning", "snippet": "<b>Markov Decision Process</b>. <b>Markov Decision Process</b> or <b>MDP</b>, is used to formalize the reinforcement learning problems. If the environment is completely observable, then its dynamic can be modeled as a <b>Markov</b> <b>Process</b>. In <b>MDP</b>, the agent constantly interacts with the environment and performs actions; at each action, the environment responds and generates a new state. <b>MDP</b> is used to describe the environment for the RL, and almost all the RL problem can be formalized using <b>MDP</b>. <b>MDP</b> contains a tuple ...", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Markov</b> <b>Decision</b> Processes", "url": "http://ce.sharif.edu/courses/99-00/1/ce417-1/resources/root/Slides/Markov%20Decision%20Processes%20(MDPs).pdf", "isFamilyFriendly": true, "displayUrl": "ce.sharif.edu/courses/99-00/1/ce417-1/resources/root/Slides/<b>Markov</b> <b>Decision</b> <b>Process</b>es...", "snippet": "<b>Markov</b> <b>Decision</b> Processes CE417: Introduction to Artificial Intelligence Sharif University of Technology Fall 2020 Slides have been adopted from Klein and Abdeel, CS188, UC Berkeley. Example 2. Non-Deterministic Search 3. Example: Grid World A <b>maze</b>-<b>like</b> problem The agent lives in a grid Walls block the agent\u2019s path Noisy movement: actions do not always go as planned 80% of the time, the action North takes the agent North (if there is no wall there) 10% of the time, North takes the agent ...", "dateLastCrawled": "2021-12-08T04:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>maze</b>_reinforcement_learning/<b>mdp</b>.py at master \u00b7 gkhayes/<b>maze</b> ...", "url": "https://github.com/gkhayes/maze_reinforcement_learning/blob/master/mdp.py", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gkhayes/<b>maze</b>_reinforcement_learning/blob/master/<b>mdp</b>.py", "snippet": "&quot;&quot;&quot;<b>Markov Decision Process</b> (<b>MDP</b>) Toolbox: ``<b>mdp</b>`` module ===== The ``<b>mdp</b>`` module provides classes for the resolution of descrete-time <b>Markov</b>: <b>Decision</b> Processes. Available classes-----:class:`~mdptoolbox.<b>mdp</b>.<b>MDP</b>` Base <b>Markov decision process</b> class:class:`~mdptoolbox.<b>mdp</b>.FiniteHorizon` Backwards induction finite horizon <b>MDP</b>:class:`~mdptoolbox ...", "dateLastCrawled": "2021-09-12T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python 3.x - <b>MDP Policy Plot for a Maze</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/49879640/mdp-policy-plot-for-a-maze", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/49879640", "snippet": "I am trying to display my policy <b>decision</b> for a <b>Markov Decision Process</b> in the context of solving a <b>maze</b>. How would I plot something that looks <b>like</b> this? Matlab is preferable but Python is fine. Even if some body could show me how to make a plot <b>like</b> this I would be able to figure it out from there. python-3.x matlab matplotlib matlab-figure <b>markov-decision-process</b>. Share. Improve this question. Follow edited Apr 25 &#39;18 at 16:21. DeeeeRoy. asked Apr 17 &#39;18 at 13:37. DeeeeRoy DeeeeRoy. 407 5 ...", "dateLastCrawled": "2022-01-21T14:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Maze</b> Escape \u2013 Avoid Walls (<b>Reinforcement Learning</b>) \u2013 Ryan Lu \u2013 Project ...", "url": "https://rylu1.com/2020/08/19/reinforcement_learning/", "isFamilyFriendly": true, "displayUrl": "https://rylu1.com/2020/08/19/<b>reinforcement_learning</b>", "snippet": "The main mathematical framework that drives <b>reinforcement learning</b> is the <b>Markov Decision Process</b> (<b>MDP</b>) that defines a state in an environment, actions that agents can choose, reward function , and a transition probability function that will allow the agent to transition to the new state (s\u2019) if the agent takes action (a) in the current state (s). The <b>reinforcement learning</b> algorithm must either already have access to this transition function with state values (model-based learning) or ...", "dateLastCrawled": "2021-09-07T17:45:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "CS 294-5: Statistical Natural Language Processing", "url": "https://inst.eecs.berkeley.edu/~cs188/sp21/assets/slides/lec24.pptx", "isFamilyFriendly": true, "displayUrl": "https://inst.eecs.berkeley.edu/~cs188/sp21/assets/slides/lec24.pptx", "snippet": "Recap: <b>Markov Decision Process</b> (<b>MDP</b>) What is a <b>Markov Decision Process</b>? Andrey <b>Markov</b> (1856-1922) Like search: successor function only depended on current state. Can make this happen by stuffing more into the state; Very <b>similar</b> to search problems: when solving a <b>maze</b> with food pellets, we stored which food pellets were eaten . Recap: <b>Markov Decision Process</b> (<b>MDP</b>) What is a <b>Markov Decision Process</b>? State transition model is <b>markov</b>. Utility function is additive discounted rewards. An <b>MDP</b> is ...", "dateLastCrawled": "2022-01-30T21:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Lecture 6: <b>Markov</b> <b>Decision</b> Processes", "url": "https://shuaili8.github.io/Teaching/CS410/L6_mdp.pdf", "isFamilyFriendly": true, "displayUrl": "https://shuaili8.github.io/Teaching/CS410/L6_<b>mdp</b>.pdf", "snippet": "<b>Markov</b> <b>Decision</b> Processes \u2022An <b>MDP</b> is defined by: \u2022A set of states s S \u2022A set of actions a A \u2022A transition function T(s, a, s\u2019) \u2022Probability that a from s leads to s\u2019, i.e., P(s\u2019| s, a) \u2022Also called the model or the dynamics \u2022A reward function R(s, a, s\u2019) \u2022Sometimes just R(s) or R(s\u2019) \u2022A start state \u2022Maybe a terminal state \u2022MDPs are non-deterministic search problems \u2022One way to solve them is with expectimax search \u2022We\u2019ll have a new tool soon 12 [Demo ...", "dateLastCrawled": "2022-01-02T21:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Markov Decision Processes</b>", "url": "https://inst.eecs.berkeley.edu/~cs188/fa20/assets/slides/lec8.pdf", "isFamilyFriendly": true, "displayUrl": "https://inst.eecs.berkeley.edu/~cs188/fa20/assets/slides/lec8.pdf", "snippet": "<b>Markov Decision Processes</b> oAn <b>MDP</b> is defined by: oA set of states s \u00ceS oA set of actions a \u00ceA oA transition function T(s, a, s\u2019) oProbability that a from s leads to s\u2019, i.e., P(s\u2019| s, a) oAlso called the model or the dynamics oA reward function R(s, a, s\u2019) oSometimes just R(s) or R(s\u2019) oA start state oMaybe a terminal state [Demo \u2013gridworldmanual intro (L8D1)] Video of Demo GridworldManual Intro. What is <b>Markov</b> about MDPs? o\u201c<b>Markov</b>\u201d generally means that given the present ...", "dateLastCrawled": "2022-01-18T11:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>CSEP 573: Artificial Intelligence</b>", "url": "https://courses.cs.washington.edu/courses/csep573/14sp/slides/6_MDP.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.washington.edu/courses/csep573/14sp/slides/6_<b>MDP</b>.pdf", "snippet": "<b>Markov</b> <b>Decision</b> Processes (<b>MDP</b>) ! Ali Farhadi Many slides over the course adapted from Luke Zettlemoyer, Dan Klein, Pieter Abbeel, Stuart Russell or Andrew Moore 1. Outline (roughly next two weeks)!<b>Markov</b> <b>Decision</b> Processes (<b>MDP</b>) !<b>MDP</b> formalism !Value Iteration !Policy Iteration !!Reinforcement Learning (RL) !Relationship to MDPs !Several learning algorithms. Non-deterministic Search! Noisy execution of actions ! Deterministic grid world vs. non-deterministic grid world. Example: Grid World ...", "dateLastCrawled": "2021-12-28T10:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lecture 20-21: Introduction to Reinforcement Learning", "url": "https://harvard-iacs.github.io/2020-CS109B/lectures/lecture20/presentation/cs109b_RL.pdf", "isFamilyFriendly": true, "displayUrl": "https://harvard-iacs.github.io/2020-CS109B/lectures/lecture20/presentation/cs109b_RL.pdf", "snippet": "\u2022 <b>Markov Decision Process</b>: <b>Markov</b> <b>Process</b> <b>Markov</b> Reward <b>Process</b> <b>Markov Decision process</b> 3 \u2022 Learning Optimal Policies Model Based (knowing the transition matrix): Value Iteration Policy Iteration Model Free (not knowing the transition matrix): Q-Learning SARSA. CS109B, PROTOPAPAS, GLICKMAN, TANNER Outline \u2022 What is Reinforcement Learning? \u2022 RL Formalism: Reward The agent The environment Actions Observations \u2022 <b>Markov Decision Process</b>: <b>Markov</b> <b>Process</b> <b>Markov</b> Reward <b>Process</b> <b>Markov</b> ...", "dateLastCrawled": "2022-01-27T02:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Reinforcement Learning Tutorial</b> - Javatpoint", "url": "https://www.javatpoint.com/reinforcement-learning", "isFamilyFriendly": true, "displayUrl": "https://www.javatpoint.com/reinforcement-learning", "snippet": "<b>Markov Decision Process</b> or <b>MDP</b>, is used to formalize the reinforcement learning problems. If the environment is completely observable, then its dynamic can be modeled as a <b>Markov</b> <b>Process</b> . In <b>MDP</b>, the agent constantly interacts with the environment and performs actions; at each action, the environment responds and generates a new state.", "dateLastCrawled": "2022-02-02T06:14:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) <b>Reinforcement Learning and Markov Decision Processes</b>", "url": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_<b>Markov</b>...", "snippet": "First the formal framework of <b>Markov decision process</b> is defined, accompanied by the definition of value functions and policies. The main part of this text deals with introducing foundational ...", "dateLastCrawled": "2022-01-24T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Basic Principles of Reinforcement Learning [Motivating Deep Learning</b>]", "url": "http://www.charuaggarwal.net/Chap9slides.pdf", "isFamilyFriendly": true, "displayUrl": "www.charuaggarwal.net/Chap9slides.pdf", "snippet": "\u2022 The biological and AI frameworks are <b>similar</b>. \u2022 <b>MDP</b> represented as s0a0r0s1a1r1...snanrn. Examples of <b>Markov Decision Process</b> \u2022 Game of tic-tac-toe, chess, or Go: The state is the position of the board at any point, and the actions correspond to the moves made by the agent. The reward is +1, 0, or \u22121 (depending on win, draw, or loss), which is received at the end of the game. \u2022 Robot locomotion: The state corresponds to the current con\ufb01guration of robot joints and its position ...", "dateLastCrawled": "2022-01-21T17:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Q-Learning and SARSA, with Python | by Chao De-Yu | Jun, 2021 | Towards ...", "url": "https://towardsdatascience.com/q-learning-and-sasar-with-python-3775f86bd178", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/q-learning-and-sasar-with-python-3775f86bd178", "snippet": "<b>Similar</b> to the Monte Carlo Algorithm (MC), Q-Learning a nd SARSA algorithms are also model-free RL algorithms that do not use the transition probability distribution associated with <b>Markov Decision Process</b> (<b>MDP</b>). Instead, they learn the optimal policy from experience. The main difference between MC and Q-Learning or SARSA algorithm is that MC needs to sample the whole trajectory to learn the value function and find the optimal policy. However, for some problems getting a whole trajectory ...", "dateLastCrawled": "2022-01-30T02:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Reinforcement Learning</b> For Mice. An Anology Between Animals And\u2026 | by ...", "url": "https://towardsdatascience.com/reinforcement-learning-3f87a0290ba2", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/<b>reinforcement-learning</b>-3f87a0290ba2", "snippet": "Image by Author. Agent: The component that makes the <b>decision</b> of what action to take.Our agent is the mouse in this case.. Environment: Physical world in which the agent operates.The <b>maze</b> is the environment.. Actions: The agent\u2019s methods that allow it to interact and change its environment, and thus transfer between states.In this case, the mouse\u2019s motions to the right, left, forward, and backward are the actions.. State: A representation of the current world or environment of the task ...", "dateLastCrawled": "2022-01-31T10:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "An Extension of Finite-state <b>Markov Decision Process</b> and an Application ...", "url": "https://www.intechopen.com/chapters/674", "isFamilyFriendly": true, "displayUrl": "https://www.intechopen.com/chapters/674", "snippet": "Intuitively, a simple context-free <b>decision</b> <b>process</b> <b>can</b> <b>be thought</b> of as an episodic finite-state <b>MDP</b> with a stack. In fact, many reinforcement learning methods <b>can</b> be applied to the class of simple context-free <b>decision</b> processes with natural modification on their equations.On the other hand, in grammatical inference area, some non-regular subclasses of simple grammars, such as very simple grammars and right-unique simple grammars, have been found to be efficiently identifiable in the limit ...", "dateLastCrawled": "2021-11-21T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Processes I</b> | RUOCHI.AI", "url": "https://zhangruochi.com/Markov-Decision-Processes/2020/09/04/", "isFamilyFriendly": true, "displayUrl": "https://zhangruochi.com/<b>Markov-Decision-Processes</b>/2020/09/04", "snippet": "Lesson 1: Introduction to <b>Markov Decision Processes</b>. Understand <b>Markov Decision Processes</b>, or MDPs. Understand the graphical representation of a <b>Markov Decision Process</b>. Describe how the dynamics of an <b>MDP</b> are defined. Explain how many diverse processes <b>can</b> be written in terms of the <b>MDP</b> framework. Lesson 2: Goal of Reinforcement Learning.", "dateLastCrawled": "2021-09-05T04:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "RL2.<b>MDP</b>.pdf - Reinforcement Learning <b>Markov Decision Process</b> Debapriyo ...", "url": "https://www.coursehero.com/file/118517031/RL2MDPpdf/", "isFamilyFriendly": true, "displayUrl": "https://www.coursehero.com/file/118517031/RL2<b>MDP</b>pdf", "snippet": "<b>Markov decision process</b> Debapriyo Majumdar \u22c5 Continuing tasks and discounting No identifiable episodes, rather goes on \u2013 On-going <b>process</b> control task There is no terminal state, or it <b>can</b> <b>be thought</b> of as The reward also <b>can</b> be infinity Discounting: reduce weights of rewards in the future exponentially where is called the discount rate (a parameter) T = \u221e G t = R t +1 + \u03b3 R t +2 + \u03b3 2 R t +3 + \u22ef = \u221e \u2211 k =0 \u03b3 k R t + k +1 \u03b3 \u2208 [0,1] 9", "dateLastCrawled": "2022-01-27T19:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Problem Formulation and the <b>Markov Decision Process</b>", "url": "https://www.inf.ed.ac.uk/teaching/courses/rl/slides12/3_MDPs.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.inf.ed.ac.uk/teaching/courses/rl/slides12/3_<b>MDP</b>s.pdf", "snippet": "<b>Markov</b> <b>Decision</b> Processes \u2022 If a reinforcement learning task has the <b>Markov</b> Property, it is basically a <b>Markov Decision Process</b> (<b>MDP</b>). \u2022 If state and action sets are finite, it is a finite <b>MDP</b>. \u2022 To define a finite <b>MDP</b>, you need to give: \u2013state and action sets \u2013one-step \u201cdynamics\u201d defined by transition probabilities:", "dateLastCrawled": "2021-12-22T22:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Notes on Chapter 21: Reinforcement Learning \u2014 cmpt310summer2019 ...", "url": "https://www.sfu.ca/~tjd/310summer2019/chp21_reinforcement_learning.html", "isFamilyFriendly": true, "displayUrl": "https://www.sfu.ca/~tjd/310summer2019/chp21_reinforcement_learning.html", "snippet": "reinforcement learning is usually modeled as a <b>markov decision process</b> (<b>mdp</b>) you <b>can</b> think of an <b>mdp</b> as a graph, where the nodes are states of the world, and the edges are actions that go between states . if the agent is in state s and does action a, then P(s,a,s\u2019) is the probability that the agent ends of in state s\u2019 this allows for the possibility that an action might fail to do what the agent intended; Reward(s,a,s\u2019) is the immediate reward the agent receives after going from s to s ...", "dateLastCrawled": "2021-11-30T04:09:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Extending DTGOLOG with Options", "url": "https://www.ijcai.org/Proceedings/03/Papers/213.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.ijcai.org/Proceedings/03/Papers/213.pdf", "snippet": "action selection policy involves solving a <b>Markov Decision Process</b> (<b>MDP</b>) [Puterman, 1994]. In a sense, a DTGOLOG program <b>can</b> <b>be thought</b> of as a factored representation of an <b>MDP</b>. As an example, Boutilier et al. consider a mail-delivery scenario where the task of delivering mail to a particular per\u00ad son is hand-coded and fixed, while the agent chooses the or\u00ad der in which the various people are served according to some reward function. They note that this approach allows solving problems ...", "dateLastCrawled": "2021-11-01T16:07:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Extending DTGOLOG with Options", "url": "https://www.cs.toronto.edu/~fritz/publications/200506061851_ferrein03extending.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.cs.toronto.edu/~fritz/publications/200506061851_ferrein03extending.pdf", "snippet": "action selection policy involves solving a <b>Markov Decision Process</b> (<b>MDP</b>) [Puterman, 1994]. In a sense, a DTGOLOG program <b>can</b> <b>be thought</b> of as a factored representation of an <b>MDP</b>. As an example, Boutilier et al. consider a mail-delivery scenario where the task of delivering mail to a particular per-son is hand-coded and \ufb01xed, while the agent chooses the or-der in which the various people are served according to some reward function. They note that this approach allows solving problems which ...", "dateLastCrawled": "2022-01-27T09:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Deep Reinforcement Learning</b>: Guide to Deep Q-Learning", "url": "https://www.mlq.ai/deep-reinforcement-learning-q-learning/", "isFamilyFriendly": true, "displayUrl": "https://www.mlq.ai/<b>deep-reinforcement-learning</b>-q-learning", "snippet": "If you&#39;re familiar with Finance, the discount factor <b>can</b> <b>be thought</b> of like the time value of money. 3. <b>Markov</b> <b>Decision</b> Processes (MDPs) Now that we&#39;ve discussed the concept of a Q-table, let&#39;s move on to the next key concept in <b>reinforcement learning</b>: <b>Markov</b> <b>decision</b> processes, or MDPs.", "dateLastCrawled": "2022-02-02T03:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Markov</b> <b>Decision</b> Processes, Value Iteration, Policy Iteration", "url": "https://cmudeeprl.github.io/703website_f21/assets/lectures/f21/lecture_mdps%20_policyvalueIterF21.pdf", "isFamilyFriendly": true, "displayUrl": "https://cmudeeprl.github.io/703website_f21/assets/lectures/f21/lecture_<b>mdp</b>s...", "snippet": "<b>Markov</b> <b>Decision</b> Processes, Value Iteration, Policy Iteration Deep Reinforcement Learning and Control Instructors: Katerina Fragkiadaki Russ Salakhutdinov Carnegie Mellon School of Computer Science Fall 2021, CMU 10-703. 1. Learning from expert demonstrations Instructive feedback: the expert directly suggests correct actions, e.g., your advisor directly suggests to you ideas that are worth pursuing 2. Learning from rewards while interacting with the environment Evaluative feedback: the ...", "dateLastCrawled": "2021-11-09T04:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "machine learning - Why is the <b>optimal policy</b> in <b>Markov Decision Process</b> ...", "url": "https://stats.stackexchange.com/questions/132890/why-is-the-optimal-policy-in-markov-decision-process-mdp-independent-of-the-i", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132890", "snippet": "The case of solving a <b>maze</b> is one of these: the agent is (potentially randomly positioned) in a <b>maze</b> and is trying to get out of it. It gets rewarded only when it finds the exit (first image). Through experiences and propagation of the reward, it will learn what path to choose from any position (second image). So the <b>optimal policy</b> does provide the best direction (action) choice for any position (state).", "dateLastCrawled": "2022-01-25T23:19:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Markov</b> <b>Decision</b> Processes (<b>MDP</b>) Example: An Optimal Policy", "url": "http://mas.cs.umass.edu/classes/cs683/lectures-2010/Lec13_MDP2-F2010-4up.pdf", "isFamilyFriendly": true, "displayUrl": "mas.cs.umass.edu/classes/cs683/lectures-2010/Lec13_<b>MDP</b>2-F2010-4up.pdf", "snippet": "<b>Markov</b> <b>Decision</b> Processes (<b>MDP</b>) ... We <b>can</b> define an <b>MDP</b> with a state set consisting of all possible belief states thus mapping a POMDP into an <b>MDP</b> V\u2019(b i)=max a {r(b i,a)+ *(sum o P(o|b i,a)V(b i a o)} where r(b i,a) =sum s b i (s)r(s,a) The set of belief states is continuous and infinite but this problem <b>can</b> be fixed by using a set of real number basis vectors of size |S| to represent V since DP preserves the piecewise linearity and convexity of the value function. \u03b3 V. Lesser; CS683 ...", "dateLastCrawled": "2022-02-02T12:06:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Reinforcement Learning : <b>Markov-Decision Process</b> (Part 1) | by ...", "url": "https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/introduction-to-reinforcement-learning-<b>markov-decision</b>...", "snippet": "<b>Markov</b> <b>Process</b> is the memory less random <b>process</b> i.e. a sequence of a random state S[1],S[2],\u2026.S[n] with a <b>Markov</b> Property.So, it\u2019s basically a sequence of states with the <b>Markov</b> Property.It <b>can</b> be defined using a set of states(S) and transition probability matrix (P).The dynamics of the environment <b>can</b> be fully defined using the States(S) and Transition Probability matrix(P).", "dateLastCrawled": "2022-02-02T03:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Markov</b> <b>Decision</b> Problems", "url": "https://homes.cs.washington.edu/~bboots/RL-Spring2020/Lectures/MDP_notes.pdf", "isFamilyFriendly": true, "displayUrl": "https://homes.cs.washington.edu/~bboots/RL-Spring2020/Lectures/<b>MDP</b>_notes.pdf", "snippet": "A <b>Markov Decision Process</b> (<b>MDP</b>) is a mathematical framework for modeling <b>decision</b> making under uncertainty that attempts to generalize this notion of a state that is suf\ufb01cient to insulate the entire future from the past. MDPs consist of a set of states, a set of actions, a deterministic or stochastic transition model, and a reward or cost function, de\ufb01ned below. Note that MDPs do not include observations or an explicit observation model as the environment is assumed to be fully ...", "dateLastCrawled": "2022-02-03T19:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Decision</b> Making under Uncertainty: A Quasimetric Approach", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3869775/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC3869775", "snippet": "This problem that we propose to deal with in this paper <b>can</b> be viewed as sequential <b>decision</b> making, usually expressed as a Markovian <b>Decision</b> <b>Process</b> (<b>MDP</b>) \u2013 and its extension to Partially Observable cases (POMDP) , . Knowing the transition probability of switching from one state to another by performing a particular action as well as the associated instantaneous cost, the aim is to define an optimal policy, either deterministic or probabilistic, which maps the state space to the action ...", "dateLastCrawled": "2022-01-29T12:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Reinforcement Learning and Markov Decision Processes</b>", "url": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/235004620_Reinforcement_Learning_and_<b>Markov</b>...", "snippet": "The <b>Markov Decision Process</b>. Putting all elements together results in the de\ufb01nition of a <b>Markov decision process</b> , which will be the base model for the large majority of methods described in this", "dateLastCrawled": "2022-01-24T17:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Reinforcement Learning and Markov Decision Processes</b> | Marco A ...", "url": "https://www.academia.edu/23684694/Reinforcement_Learning_and_Markov_Decision_Processes", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/23684694/<b>Reinforcement_Learning_and_Markov_Decision_Processes</b>", "snippet": "The <b>Markov Decision Process</b>. Putting all elements together results in the definition of a <b>Markov decision process</b>, which will be the base model for the large majority of methods described in this book. Definition 3.1 A <b>Markov decision process</b> is a tuple hS, A, T, Ri in which S is a finite set of states, A a finite set of actions, T a transition function defined as T : S \u00d7 A \u00d7 S \u2192 [0, 1] and R a reward function defined as R : S \u00d7 A \u00d7 S \u2192 R. The transition function T and the reward ...", "dateLastCrawled": "2022-01-14T01:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "(PDF) Reinforcement-learning agents with different temperature ...", "url": "https://www.researchgate.net/publication/220550765_Reinforcement-learning_agents_with_different_temperature_parameters_explain_the_variety_of_human_action-selection_behavior_in_a_Markov_decision_process_task", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/220550765_Reinforcement-learning_agents_with...", "snippet": "The most competitive advantage of reinforcement learning is that it does not need knowledge about the <b>Markov decision process</b> (<b>MDP</b>) and <b>can</b> target the large MDPs when exact methods become fairly ...", "dateLastCrawled": "2022-01-06T14:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>maze</b>_reinforcement_learning/<b>mdp</b>.py at master \u00b7 gkhayes/<b>maze</b> ...", "url": "https://github.com/gkhayes/maze_reinforcement_learning/blob/master/mdp.py", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/gkhayes/<b>maze</b>_reinforcement_learning/blob/master/<b>mdp</b>.py", "snippet": "&quot;&quot;&quot;<b>Markov Decision Process</b> (<b>MDP</b>) Toolbox: ``<b>mdp</b>`` module ===== The ``<b>mdp</b>`` module provides classes for the resolution of descrete-time <b>Markov</b>: <b>Decision</b> Processes. Available classes-----:class:`~mdptoolbox.<b>mdp</b>.<b>MDP</b>` Base <b>Markov decision process</b> class:class:`~mdptoolbox.<b>mdp</b>.FiniteHorizon` Backwards induction finite horizon <b>MDP</b>:class:`~mdptoolbox ...", "dateLastCrawled": "2021-09-12T15:18:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "An Extension of Finite-state <b>Markov Decision Process</b> and an Application ...", "url": "https://www.intechopen.com/chapters/674", "isFamilyFriendly": true, "displayUrl": "https://www.intechopen.com/chapters/674", "snippet": "Intuitively, a simple context-free <b>decision</b> <b>process</b> <b>can</b> be thought of as an episodic finite-state <b>MDP</b> with a stack. In fact, many reinforcement learning methods <b>can</b> be applied to the class of simple context-free <b>decision</b> processes with natural modification on their equations.On the other hand, in grammatical inference area, some non-regular subclasses of simple grammars, such as very simple grammars and right-unique simple grammars, have been found to be efficiently identifiable in the limit ...", "dateLastCrawled": "2021-11-21T12:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Maze</b> Escape \u2013 Avoid Walls (<b>Reinforcement Learning</b>) \u2013 Ryan Lu \u2013 Project ...", "url": "https://rylu1.com/2020/08/19/reinforcement_learning/", "isFamilyFriendly": true, "displayUrl": "https://rylu1.com/2020/08/19/<b>reinforcement_learning</b>", "snippet": "The main mathematical framework that drives <b>reinforcement learning</b> is the <b>Markov Decision Process</b> (<b>MDP</b>) that defines a state in an environment, actions that agents <b>can</b> choose, reward function , and a transition probability function that will allow the agent to transition to the new state (s\u2019) if the agent takes action (a) in the current state (s). The <b>reinforcement learning</b> algorithm must either already have access to this transition function with state values (model-based learning) or ...", "dateLastCrawled": "2021-09-07T17:45:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does <b>Markov Decision Process</b> matter in Reinforcement <b>Learning</b>? | by ...", "url": "https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-does-malkov-<b>decision</b>-<b>process</b>-matter-in...", "snippet": "It is named by <b>analogy</b> to \u201cone-armed bandit\u201d(= a slot <b>machine</b>) although the framework has k levers instead of one. ... we introduce <b>Markov Decision Process</b>(<b>MDP</b>) to solve such a problem. An <b>MDP</b> consists of two elements; the agent and the environment. The agent is a learner or <b>decision</b>-maker. In the above example, the agent is the rabbit. The environment is everything surrounding the agent. In the example, the environment includes everything in the field where the rabbit is with food and ...", "dateLastCrawled": "2022-01-31T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov Decision Process</b>: How Does Value Iteration Work? | Baeldung on ...", "url": "https://www.baeldung.com/cs/mdp-value-iteration", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>mdp</b>-value-iteration", "snippet": "From this point, we can make an <b>analogy</b> with the <b>Markov</b> model since the solution for this problem is a sequence of actions. A <b>Markov Decision Process</b> is used to model the agent, considering that the agent itself generates a series of actions. In the real world, we can have observable, hidden, or partially observed states, depending on the ...", "dateLastCrawled": "2022-01-30T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Introduction to <b>Machine</b> <b>Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-machine-learning-3/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/30/introduction-to-<b>machine</b>-<b>learning</b>-3", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-28T18:37:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Markov decision process</b>: value iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-value-iteration-2d161d50a6ff", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-value-iteration-2d161d50a6ff", "snippet": "<b>Markov decision process</b>, <b>MDP</b>, value iteration, policy iteration, policy evaluation, policy improvement, sweep, iterative policy evaluation, policy, optimal policy ...", "dateLastCrawled": "2022-01-08T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "18.1. <b>Markov Decision Process</b> (<b>MDP</b>) \u2014 Dive into Deep <b>Learning</b> 0.17.0 ...", "url": "http://preview.d2l.ai.s3-website-us-west-2.amazonaws.com/d2l-en/master/chapter_reinforcement_learning/mdp.html", "isFamilyFriendly": true, "displayUrl": "preview.d2l.ai.s3-website-us-west-2.amazonaws.com/...reinforcement_<b>learning</b>/<b>mdp</b>.html", "snippet": "In this section, we will discuss how to formulate reinforcement <b>learning</b> problems using <b>Markov</b> <b>decision</b> processes (MDPs) and describe in detail various components of MDPs. Definition of an <b>MDP</b> \u00b6 A <b>Markov decision process</b> (<b>MDP</b>) is a model for how the state of a system evolves as different actions are applied to it.", "dateLastCrawled": "2022-01-27T05:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Markov decision process</b>: policy iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-policy-iteration-42d35ee87c82?source=post_internal_links---------0-------------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-policy-iteration-42d35ee87c82?source=...", "snippet": "<b>Markov decision process</b>: policy iteration with code implementation . Nan. Dec 19, 2021 \u00b7 16 min read. In today\u2019s story we focus on policy iteration of <b>MDP</b>. We are still using the grid world ...", "dateLastCrawled": "2022-01-22T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>", "url": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "isFamilyFriendly": true, "displayUrl": "https://hal.archives-ouvertes.fr/tel-02422144v2/document", "snippet": "Meta-<b>Learning</b> as a <b>Markov Decision Process</b>. <b>Machine</b> <b>Learning</b> [cs.LG]. Uni-versit\u00e9 Paris Saclay (COmUE), 2019. English. \uffffNNT: 2019SACLS588\uffff. \ufffftel-02422144v2\uffff I would like to dedicate this thesis to my loving parents doctorat CLS588 Meta-<b>Learning</b> as a <b>Markov Decision Process</b> Th\u00e8se de doctorat de l\u2019Universit\u00e9 Paris-Saclay pr\u00e9par\u00e9e \u00e0 l\u2019Universit\u00e9 Paris-Sud Ecole doctorale n 580 Sciences et Technologies de l\u2019Information et de la Communication (STIC) Sp\u00e9cialit\u00e9 de doctorat ...", "dateLastCrawled": "2022-01-13T07:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Why is the <b>optimal policy</b> in <b>Markov Decision Process</b> ...", "url": "https://stats.stackexchange.com/questions/132890/why-is-the-optimal-policy-in-markov-decision-process-mdp-independent-of-the-i", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132890", "snippet": "The intuition behind the argument saying that the <b>optimal policy</b> is independent of initial state is the following: The <b>optimal policy</b> is defined by a function that selects an action for every possible state and actions in different states are independent.. Formally speaking, for an unknown initial distribution, the value function to maximize would be the following (not conditioned on initial state)", "dateLastCrawled": "2022-01-25T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Real-life <b>examples</b> of <b>Markov</b> <b>Decision</b> Processes - Cross Validated", "url": "https://stats.stackexchange.com/questions/145122/real-life-examples-of-markov-decision-processes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/145122", "snippet": "A Markovian <b>Decision</b> <b>Process</b> indeed has to do with going from one state to another and is mainly used for planning and <b>decision</b> making. The theory. Just repeating the theory quickly, an <b>MDP</b> is: $$\\text{<b>MDP</b>} = \\langle S,A,T,R,\\gamma \\rangle$$", "dateLastCrawled": "2022-01-24T10:32:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Overview: Representation Techniques", "url": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66fffb5d0bd4c20697922f5ffbf9a602b66bec3f74ac83fb77c/DecisionMaking.pdf", "isFamilyFriendly": true, "displayUrl": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66...", "snippet": "<b>Markov Decision Process MDP is like</b> a Markov process, except every round we make a decision Transition probabilities depend on actions taken P(St+1 = S&#39; | St = s, At = a) = P(S, a, S&#39;) Rewards for every state, action pair u(St = s, At = a) Discount factor \u03b4 Example. A <b>machine</b> can be in one of three states: good, deteriorating, broken Can take ...", "dateLastCrawled": "2022-01-21T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(markov decision process (mdp))  is like +(maze)", "+(markov decision process (mdp)) is similar to +(maze)", "+(markov decision process (mdp)) can be thought of as +(maze)", "+(markov decision process (mdp)) can be compared to +(maze)", "machine learning +(markov decision process (mdp) AND analogy)", "machine learning +(\"markov decision process (mdp) is like\")", "machine learning +(\"markov decision process (mdp) is similar\")", "machine learning +(\"just as markov decision process (mdp)\")", "machine learning +(\"markov decision process (mdp) can be thought of as\")", "machine learning +(\"markov decision process (mdp) can be compared to\")"]}