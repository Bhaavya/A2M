{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Train custom embeddings based on co-occurrence data with KFP pipeline ...", "url": "https://cloud.google.com/blog/products/ai-machine-learning/train-custom-embeddings-based-on-co-occurrence-data-with-kfp-pipeline", "isFamilyFriendly": true, "displayUrl": "https://cloud.google.com/blog/products/ai-machine-learning/train-custom-<b>embeddings</b>...", "snippet": "Embeddings usually capture the semantics of an item by placing <b>similar</b> <b>items</b> close <b>together</b> in the <b>embedding</b> <b>space</b>. Take the following two pieces of text, for example: \u201cThe squad is ready to win the football match,\u201d and, \u201cThe team is prepared to achieve victory in the soccer game.\u201d They share almost none of the same words, but they should be close to one another in the <b>embedding</b> <b>space</b> because their meaning is very <b>similar</b>. Embeddings can be generated for <b>items</b> such as words ...", "dateLastCrawled": "2021-10-12T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "UNPACKING SUBJECTIVE CREATIVITY RATINGS: USING EMBEDDINGS TO EXPLAIN ...", "url": "https://decode.mit.edu/assets/papers/2018_ahmed_unpacking.pdf", "isFamilyFriendly": true, "displayUrl": "https://decode.mit.edu/assets/papers/2018_ahmed_unpacking.pdf", "snippet": "and <b>grouping</b> <b>similar</b> <b>items</b> <b>together</b>. Design <b>space</b> exploration techniques [19] have been developed to visualize a design <b>space</b> and generate feasible designs. Motivated by the fact that humans essentially think in two or three dimensions, many methods to vi-sualize high dimensional data by mapping it to lower dimension manifolds have been studied extensively [20, 21]. Researchers have also investigated information extraction about the broader nature of a design <b>space</b> from such embeddings [22 ...", "dateLastCrawled": "2021-08-29T19:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Divide and Conquer the Embedding Space for Metric Learning</b> | DeepAI", "url": "https://deepai.org/publication/divide-and-conquer-the-embedding-space-for-metric-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>divide-and-conquer-the-embedding-space-for-metric-learning</b>", "snippet": "Learning the <b>embedding</b> <b>space</b>, where semantically <b>similar</b> objects are located close <b>together</b> and dissimilar objects far apart, is a cornerstone of many computer vision applications. Existing approaches usually learn a single metric in the <b>embedding</b> <b>space</b> for all available data points, which may have a very complex non-uniform distribution with different notions of similarity between objects, e.g. appearance, shape, color or semantic meaning.Approaches for learning a single distance metric ...", "dateLastCrawled": "2022-01-18T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Word Embedding Tutorial | Word2vec</b> Model Gensim Example", "url": "https://www.guru99.com/word-embedding-word2vec.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/word-<b>embedding</b>-word2vec.html", "snippet": "Word <b>Embedding</b> is also called as distributed semantic model or distributed represented or semantic vector <b>space</b> or vector <b>space</b> model. As you read these names, you come across the word semantic which means categorizing <b>similar</b> words <b>together</b>. For example fruits <b>like</b> apple, mango, banana should be placed close whereas books will be far away from these words. In a broader sense, word <b>embedding</b> will create the vector of fruits which will be placed far away from vector representation of books.", "dateLastCrawled": "2022-02-02T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "4.1 Clustering: <b>Grouping</b> samples based on their <b>similarity</b> ...", "url": "https://compgenomr.github.io/book/clustering-grouping-samples-based-on-their-similarity.html", "isFamilyFriendly": true, "displayUrl": "https://compgenomr.github.io/book/clustering-<b>grouping</b>-samples-based-on-their...", "snippet": "4.1 Clustering: <b>Grouping</b> samples based on their <b>similarity</b>. In genomics, we would very frequently want to assess how our samples relate to each other. Are our replicates <b>similar</b> to each other? Do the samples from the same treatment group have <b>similar</b> genome-wide signals? Do the patients with <b>similar</b> diseases have <b>similar</b> gene expression profiles? Take the last question for example. We need to define a distance or <b>similarity</b> metric between patients\u2019 expression profiles and use that metric ...", "dateLastCrawled": "2022-02-02T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Designing large scale similarity models using deep learning | by ...", "url": "https://mecha-mind.medium.com/designing-large-scale-similarity-models-using-deep-learning-8a2bcbdffea5?source=post_internal_links---------5----------------------------", "isFamilyFriendly": true, "displayUrl": "https://mecha-mind.medium.com/designing-large-scale-<b>similar</b>ity-models-using-deep...", "snippet": "But assuming that we have supervised data or some high-level <b>grouping</b> abstraction which places <b>similar</b> <b>items</b> under one group. For e.g. in e-commerce data, \u2018product type\u2019 such as \u2018Laptops\u2019, \u2018T-Shirts\u2019, \u2018Mobile Phones\u2019 etc. is a high-level <b>grouping</b> abstraction because <b>items</b> under \u2018Laptops\u2019 would be more <b>similar</b> to each other as compared to an item from \u2018Laptop\u2019 and an item from \u2018T-Shirt\u2019. <b>Similar</b> <b>Items</b> based on Product Type. The level of <b>grouping</b> abstraction can go ...", "dateLastCrawled": "2022-01-08T07:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Clustering and community detection</b>", "url": "https://www.inf.ed.ac.uk/teaching/courses/stn/files1819/slides/community.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.inf.ed.ac.uk/teaching/courses/stn/files1819/slides/community.pdf", "snippet": "\u2013 Either compute intrinsic metrics <b>like</b> all pairs shortest paths [Floyd-Warshall algorithm O(n3)] \u2013 Or embed the nodes in a Euclidean <b>space</b>, and use the metric there \u2022 We will later study <b>embedding</b> methods \u2022 Apply a clustering algorithm with the metric. Clustering \u2022 A core problem of machine learning: \u2013 Which <b>items</b> are in the same group? \u2022 Identifies <b>items</b> that are <b>similar</b> relative to rest of data \u2022 Simplifies information by <b>grouping</b> <b>similar</b> <b>items</b> \u2013 Helps in all types of ...", "dateLastCrawled": "2021-12-02T05:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "machine learning - <b>Grouping Similar Images with names</b> in them - Stack ...", "url": "https://stackoverflow.com/questions/26266841/grouping-similar-images-with-names-in-them", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/26266841", "snippet": "The text color,font and position is not <b>similar</b> in every case.Do you think I could group <b>similar</b> images <b>together</b> even when this is the case based on the text? Or are there any other ways of <b>grouping</b> <b>similar</b> images <b>together</b>? \u2013 Gayatri. Oct 9 &#39;14 at 18:51 | Show 1 more comment. 1 Answer Active Oldest Votes. 2 If the text is as clear as this you might not even need machine learning: just group all the <b>items</b> with the same name in a dictionary using the name as the key. If the text is still ...", "dateLastCrawled": "2022-01-20T09:44:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Week 13 \u2014 Unsupervised (11/15\u201319) \u2014 CS 533 Fall 2021", "url": "https://cs533.ekstrandom.net/f21/week13/", "isFamilyFriendly": true, "displayUrl": "https://cs533.ekstrandom.net/f21/week13", "snippet": "It&#39;s useful for <b>grouping</b> <b>items</b> <b>together</b>, exploration and as input into other models. ... these low dimensional vectors that are in a <b>space</b> <b>like</b> they&#39;re 10 dimensional vector. And the 10 dimensions don&#39;t mean anything. They&#39;re just dimensions that are useful for explaining this. This this instance is relationship to whatever we&#39;re trying to do with it. And so they take you a long ways and a lot of machine learning. And then they&#39;re the core piece of a lot of different models. So to wrap up ...", "dateLastCrawled": "2022-01-30T10:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - <b>Merge related words in NLP</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/63705803/merge-related-words-in-nlp", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/63705803", "snippet": "Quote match for single word and filter out word collocations == avoid collocations of words <b>like</b> &quot;blink of an eye&quot; = &quot;Quick&quot;. its indeed very non trivial but 1 word to 1 word <b>grouping</b> would be great. i dont know much abt ML , but there is something called cosine similarity which identifies how close 2 words are using K-means. you can also use wordapi for quick fix which i updated in answer.", "dateLastCrawled": "2022-01-22T08:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>NLP with Python: Text Clustering</b> - Sanjaya\u2019s Blog", "url": "https://sanjayasubedi.com.np/nlp/nlp-with-python-document-clustering/", "isFamilyFriendly": true, "displayUrl": "https://sanjayasubedi.com.np/nlp/nlp-with-python-document-clustering", "snippet": "Clustering is a process of <b>grouping</b> <b>similar</b> <b>items</b> <b>together</b>. Each group, also called as a cluster, contains <b>items</b> that are <b>similar</b> to each other. Clustering algorithms are unsupervised learning algorithms i.e. we do not need to have labelled datasets. There are many clustering algorithms for clustering including KMeans, DBSCAN, Spectral clustering, hierarchical clustering etc and they have their own advantages and disadvantages. The choice of the algorithm mainly depends on whether or not you ...", "dateLastCrawled": "2022-02-02T16:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Divide and Conquer the Embedding Space for Metric Learning</b> | DeepAI", "url": "https://deepai.org/publication/divide-and-conquer-the-embedding-space-for-metric-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>divide-and-conquer-the-embedding-space-for-metric-learning</b>", "snippet": "Learning the <b>embedding</b> <b>space</b>, where semantically <b>similar</b> objects are located close <b>together</b> and dissimilar objects far apart, is a cornerstone of many computer vision applications. Existing approaches usually learn a single metric in the <b>embedding</b> <b>space</b> for all available data points, which may have a very complex non-uniform distribution with different notions of similarity between objects, e.g. appearance, shape, color or semantic meaning.Approaches for learning a single distance metric ...", "dateLastCrawled": "2022-01-18T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Train custom embeddings based on co-occurrence data with KFP pipeline ...", "url": "https://cloud.google.com/blog/products/ai-machine-learning/train-custom-embeddings-based-on-co-occurrence-data-with-kfp-pipeline", "isFamilyFriendly": true, "displayUrl": "https://cloud.google.com/blog/products/ai-machine-learning/train-custom-<b>embeddings</b>...", "snippet": "As mentioned above, an <b>embedding</b> is a way to represent discrete <b>items</b> (such as words, song titles, etc.) as vectors of floating point numbers. Embeddings usually capture the semantics of an item by placing <b>similar</b> <b>items</b> close <b>together</b> in the <b>embedding</b> <b>space</b>. Take the following two pieces of text, for example: \u201cThe squad is ready to win the ...", "dateLastCrawled": "2021-10-12T22:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Word Embedding Tutorial | Word2vec</b> Model Gensim Example", "url": "https://www.guru99.com/word-embedding-word2vec.html", "isFamilyFriendly": true, "displayUrl": "https://www.guru99.com/word-<b>embedding</b>-word2vec.html", "snippet": "Compute <b>similar</b> words: Word <b>embedding</b> is used to suggest <b>similar</b> words to the word being subjected to the prediction model. Along with that it also suggests dissimilar words, as well as most common words. Create a group of related words: It is used for semantic <b>grouping</b> which will group things of <b>similar</b> characteristic <b>together</b> and dissimilar far away. Feature for text classification: Text is mapped into arrays of vectors which is fed to the model for training as well as prediction. Text ...", "dateLastCrawled": "2022-02-02T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "4.1 Clustering: <b>Grouping</b> samples based on their <b>similarity</b> ...", "url": "https://compgenomr.github.io/book/clustering-grouping-samples-based-on-their-similarity.html", "isFamilyFriendly": true, "displayUrl": "https://compgenomr.github.io/book/clustering-<b>grouping</b>-samples-based-on-their...", "snippet": "Do the patients with <b>similar</b> diseases have <b>similar</b> gene expression profiles? Take the last question for example. We need to define a distance or <b>similarity</b> metric between patients\u2019 expression profiles and use that metric to find groups of patients that are more <b>similar</b> to each other than the rest of the patients. This, in essence, is the general idea behind clustering. We need a distance metric and a method to utilize that distance metric to find self-<b>similar</b> groups. Clustering is a ...", "dateLastCrawled": "2022-02-02T23:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Designing large scale similarity models using deep learning | by ...", "url": "https://mecha-mind.medium.com/designing-large-scale-similarity-models-using-deep-learning-8a2bcbdffea5?source=post_internal_links---------7----------------------------", "isFamilyFriendly": true, "displayUrl": "https://mecha-mind.medium.com/designing-large-scale-<b>similar</b>ity-models-using-deep...", "snippet": "But assuming that we have supervised data or some high-level <b>grouping</b> abstraction which places <b>similar</b> <b>items</b> under one group. For e.g. in e-commerce data, \u2018product type\u2019 such as \u2018Laptops\u2019, \u2018T-Shirts\u2019, \u2018Mobile Phones\u2019 etc. is a high-level <b>grouping</b> abstraction because <b>items</b> under \u2018Laptops\u2019 would be more <b>similar</b> to each other as compared to an item from \u2018Laptop\u2019 and an item from \u2018T-Shirt\u2019. <b>Similar</b> <b>Items</b> based on Product Type. The level of <b>grouping</b> abstraction can go ...", "dateLastCrawled": "2022-01-21T10:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Clustering and community detection</b>", "url": "https://www.inf.ed.ac.uk/teaching/courses/stn/files1819/slides/community.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.inf.ed.ac.uk/teaching/courses/stn/files1819/slides/community.pdf", "snippet": "\u2013 Or embed the nodes in a Euclidean <b>space</b>, and use the metric there \u2022 We will later study <b>embedding</b> methods \u2022 Apply a clustering algorithm with the metric. Clustering \u2022 A core problem of machine learning: \u2013 Which <b>items</b> are in the same group? \u2022 Identifies <b>items</b> that are <b>similar</b> relative to rest of data \u2022 Simplifies information by <b>grouping</b> <b>similar</b> <b>items</b> \u2013 Helps in all types of other problems. Clustering \u2022 Outline approach: \u2022 Given a set of <b>items</b> \u2013 Define a distance ...", "dateLastCrawled": "2021-12-02T05:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Understanding clustering algorithms</b> | 40 Algorithms Every Programmer ...", "url": "https://subscription.packtpub.com/book/programming/9781789801217/8/ch08lvl1sec36/understanding-clustering-algorithms", "isFamilyFriendly": true, "displayUrl": "https://subscription.packtpub.com/book/programming/9781789801217/8/ch08lvl1sec36/...", "snippet": "One of the simplest and most powerful techniques used in unsupervised learning is based on <b>grouping</b> <b>similar</b> patterns <b>together</b> through clustering algorithms. It is used to understand a particular aspect of the data that is related to the problem we are trying to solve. Clustering algorithms look for natural <b>grouping</b> in data <b>items</b>. As the group is not based on any target or assumptions, it is classified as an unsupervised learning technique.", "dateLastCrawled": "2021-11-26T03:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Text Similarities : Estimate the degree of <b>similarity</b> between two texts ...", "url": "https://medium.com/@adriensieg/text-similarities-da019229c894", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@adriensieg/text-<b>similar</b>ities-da019229c894", "snippet": "BERT <b>embedding</b> for the word in the middle is more <b>similar</b> to the same word on the right than the one on the left. When classification is the larger objective, there is no need to build a BoW ...", "dateLastCrawled": "2022-02-02T02:48:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "python - <b>Merge related words in NLP</b> - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/63705803/merge-related-words-in-nlp", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/63705803", "snippet": "Through the good times and the bad, Your understanding I have had.&#39; # converts the input text to lowercase and splits the words based on empty <b>space</b>. wordlist = input_text.lower().split() # remove all punctuation from the wordlist remove_punctuation = [&#39;&#39;.join(ch for ch in s if ch not in string.punctuation) for s in wordlist] # list for word frequencies wordfreq = [] # count the frequencies of a word for w in remove_punctuation: wordfreq.append(remove_punctuation.count(w)) word_frequencies ...", "dateLastCrawled": "2022-01-22T08:02:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Intro <b>to Machine Learning by Google Product Manager</b>", "url": "https://www.slideshare.net/productschool/intro-to-machine-learning-by-google-product-manager", "isFamilyFriendly": true, "displayUrl": "https://www.slideshare.net/productschool/intro-<b>to-machine-learning-by-google-product</b>...", "snippet": "In this case, embeddings <b>can</b> <b>be thought</b> of as a point in some high dimensional <b>space</b>. <b>Similar</b> drinks are close <b>together</b>, and dissimilar drinks are far apart. An <b>embedding</b> is a mathematical description of the context for an example. It\u2019s just a vector of floats, but those are calculated (trained) to be the most useful representation for some particular task. In this case, embeddings <b>can</b> <b>be thought</b> of as a point in some high dimensional <b>space</b>. <b>Similar</b> drinks are close <b>together</b>, and ...", "dateLastCrawled": "2022-01-18T13:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "How Slyce Solves Visual Search \u2014 Part 1 | by Sethu Hareesh Kolluru ...", "url": "https://medium.com/slyce-engineering/how-slyce-solves-visual-search-part-1-72ec34a093a2", "isFamilyFriendly": true, "displayUrl": "https://medium.com/slyce-engineering/how-slyce-solves-visual-search-part-1-72ec34a093a2", "snippet": "While this <b>grouping</b> implies that <b>similar</b> <b>items</b> are close <b>together</b>, it does not explicitly force this. For example, consider an image of product D as query image, which is mapped into <b>embedding</b> ...", "dateLastCrawled": "2021-06-17T00:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Generating Data-Driven Topic Hierarchies from Text using Primer Engines ...", "url": "https://primer.ai/blog/generating-data-driven-topic-hierarchies-from-text-using-deep-nlp-models/", "isFamilyFriendly": true, "displayUrl": "https://primer.ai/blog/generating-data-driven-topic-hierarchies-from-text-using-deep...", "snippet": "Instead of <b>grouping</b> the documents, we work on the extracted topic terms and learn the relations in that set. To do so, we carry out two simple steps using off-the-shelf tools: To measure semantic distance between terms, we project these into a vector <b>embedding</b> <b>space</b> using SentenceBERT, an open-source sentence <b>embedding</b> model.", "dateLastCrawled": "2022-01-29T11:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Multiple Concurrent Thoughts: The Meaning and Developmental ...", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2925295/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.ncbi.nlm.nih.gov</b>/pmc/articles/PMC2925295", "snippet": "The amount recalled may be enhanced by a process of <b>grouping</b> <b>items</b> <b>together</b> to form new chunks right on the spot; this may be why telephone numbers are reported in a grouped manner. Verbal rehearsal may help <b>grouping</b> processes or it may simply refresh the representations of <b>items</b>. Broadbent (1975) suggested that, in situations in which effects of such strategies are eliminated, the number of <b>items</b> that <b>can</b> be held in working memory is about 3, not 7. This would be the case, he suggested, if ...", "dateLastCrawled": "2021-12-30T20:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "17 types of <b>similarity</b> and dissimilarity measures used in data science ...", "url": "https://towardsdatascience.com/17-types-of-similarity-and-dissimilarity-measures-used-in-data-science-3eb914d2681", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/17-types-of-<b>similarity</b>-and-dis<b>similarity</b>-measures-used...", "snippet": "In this way, we <b>can</b> plot the data points in a 2-D <b>space</b> where the x-axis and the y-axis represent the petal length and the petal width, respectively. Training dataset. Each data point came along with its own label: Iris-Setosa or Iris-versicolor(0 and 1 in the dataset).", "dateLastCrawled": "2022-02-03T02:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "THE CONCEPT OF <b>SMART CLASSROOM</b> | Dr. V.K. Maheshwari, Ph.D", "url": "http://www.vkmaheshwari.com/WP/?p=2352", "isFamilyFriendly": true, "displayUrl": "www.vkmaheshwari.com/WP/?p=2352", "snippet": "Students <b>can</b> work <b>together</b> in groups. Usage: ... that is, make it possible to change student <b>grouping</b>, the type of resources being used, use of various types of resources at the same time, ICT and non-ICT, for different students to carry out different tasks, e.g. searching information, discussing, watching a video, etc. The classrooms is supplied with varied furniture elements to achieve flexibility of <b>space</b> arrangement. Principle of Multiplicity. This principle refers to smart classrooms ...", "dateLastCrawled": "2022-02-01T08:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "SHOPPER: A Probabalistic Consumer Choice Model", "url": "https://humboldt-wi.github.io/blog/research/information_systems_1920/group3_shopper/", "isFamilyFriendly": true, "displayUrl": "https://humboldt-wi.github.io/blog/research/information_systems_1920/group3_shopper", "snippet": "Item Popularity: $\\lambda_{c}$ <b>can</b> <b>be thought</b> of as representing a latent intercept term that captures overall item popularity. In this case, the more popular an item is, the higher the value of this variable should be. User Preferences: To get a more accurate utility estimate, SHOPPER creates a per-user latent vector $\\theta_{u}$, along with a per-item latent $\\alpha_c$. By taking the dot product of the two vectors we get per-item preferences for each shopper. A larger value indicates a ...", "dateLastCrawled": "2022-01-20T05:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Using Deep Learning for End to End Multiclass Text <b>Classification</b> | by ...", "url": "https://towardsdatascience.com/using-deep-learning-for-end-to-end-multiclass-text-classification-39b46aecac81", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/using-deep-learning-for-end-to-end-multiclass-text...", "snippet": "If we look for <b>similar</b> words to \u201cgood\u201d, we will find awesome, great, etc. It is this property of word2vec that makes it invaluable for text <b>classification</b>. With this, our deep learning network understands that \u201cgood\u201d and \u201cgreat\u201d are words with <b>similar</b> meanings. In simple terms, word2vec creates fixed-length vectors for words, giving us a d dimensional vector for every word (and common bigrams) in a dictionary. These word vectors are usually pre-trained, and provided by others ...", "dateLastCrawled": "2022-02-01T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Predicting purchasing intent: Automatic Feature Learning</b> using ...", "url": "https://sigir-ecom.github.io/ecom2018/ecom18Papers/paper17.pdf", "isFamilyFriendly": true, "displayUrl": "https://sigir-ecom.github.io/ecom2018/ecom18Papers/paper17.pdf", "snippet": "more about a user\u2019s shopping intent <b>can</b> be used to improve merchant profit. Virtually all Ecommerce systems <b>can</b> <b>be thought</b> of as a gener-ator of clickstream data - a log of {item - userid - action} tuples which captures user interactions with the system. A chronological <b>grouping</b> of these tuples by user ID is commonly known as a session.", "dateLastCrawled": "2021-11-18T07:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "What is <b>the difference between Multidimensional Scaling and</b> Cluster ...", "url": "https://www.quora.com/What-is-the-difference-between-Multidimensional-Scaling-and-Cluster-Analysis", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-difference-between-Multidimensional-Scaling-and</b>...", "snippet": "Answer (1 of 2): They have different goals, at least usually. The goal of MDS is to take a set of similarity measures and try to see what is accounting for it. You might ask people to rate how <b>similar</b> a group of things are, pair by pair. Then you use MDS to try to figure out which attributes o...", "dateLastCrawled": "2022-01-27T04:05:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Divide and Conquer the Embedding Space for Metric Learning</b> | DeepAI", "url": "https://deepai.org/publication/divide-and-conquer-the-embedding-space-for-metric-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>divide-and-conquer-the-embedding-space-for-metric-learning</b>", "snippet": "Learning the <b>embedding</b> <b>space</b>, where semantically <b>similar</b> objects are located close <b>together</b> and dissimilar objects far apart, is a cornerstone of many computer vision applications. Existing approaches usually learn a single metric in the <b>embedding</b> <b>space</b> for all available data points, which may have a very complex non-uniform distribution with different notions of similarity between objects, e.g. appearance, shape, color or semantic meaning.Approaches for learning a single distance metric ...", "dateLastCrawled": "2022-01-18T20:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "UNPACKING SUBJECTIVE CREATIVITY RATINGS: USING EMBEDDINGS TO EXPLAIN ...", "url": "https://decode.mit.edu/assets/papers/2018_ahmed_unpacking.pdf", "isFamilyFriendly": true, "displayUrl": "https://decode.mit.edu/assets/papers/2018_ahmed_unpacking.pdf", "snippet": "and <b>grouping</b> <b>similar</b> <b>items</b> <b>together</b>. Design <b>space</b> exploration techniques [19] have been developed to visualize a design <b>space</b> and generate feasible designs. Motivated by the fact that humans essentially think in two or three dimensions, many methods to vi-sualize high dimensional data by mapping it to lower dimension manifolds have been studied extensively [20, 21]. Researchers have also investigated information extraction about the broader nature of a design <b>space</b> from such embeddings [22 ...", "dateLastCrawled": "2021-08-29T19:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "4.1 <b>Clustering: Grouping samples based on their similarity</b> ...", "url": "http://compgenomr.github.io/book/clustering-grouping-samples-based-on-their-similarity.html", "isFamilyFriendly": true, "displayUrl": "compgenomr.github.io/book/<b>clustering-grouping-samples-based-on-their-similarity</b>.html", "snippet": "The silhouette value does just that and it is a measure of how <b>similar</b> a data point is to its own cluster <b>compared</b> to other clusters (Rousseeuw 1987). The silhouette value ranges from -1 to +1, where values that are positive indicate that the data point is well matched to its own cluster, if the value is zero it is a borderline case, and if the value is minus it means that the data point might be mis-clustered because it is more <b>similar</b> to a neighboring cluster. If most data points have a ...", "dateLastCrawled": "2022-01-29T11:23:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Graph Clustering via Variational Graph <b>Embedding</b> - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S0031320321005148", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0031320321005148", "snippet": "A joint generative model is constructed for acquiring an <b>embedding</b> <b>space</b> with high information content. ... <b>can</b> group <b>similar</b> graph nodes <b>together</b> according to the similarity of nodes and obtain the underlying attribute information of graph data, which plays an important role in user classification and community recommendation of social network graph. Graph clustering aims at dividing the graph nodes into several mutually disjoint groups. However, effective utilizing the node structures and ...", "dateLastCrawled": "2022-01-29T16:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>NLP with Python: Text Clustering</b> - Sanjaya\u2019s Blog", "url": "https://sanjayasubedi.com.np/nlp/nlp-with-python-document-clustering/", "isFamilyFriendly": true, "displayUrl": "https://sanjayasubedi.com.np/nlp/nlp-with-python-document-clustering", "snippet": "Clustering is a process of <b>grouping</b> <b>similar</b> <b>items</b> <b>together</b>. Each group, also called as a cluster, contains <b>items</b> that are <b>similar</b> to each other. Clustering algorithms are unsupervised learning algorithms i.e. we do not need to have labelled datasets. There are many clustering algorithms for clustering including KMeans, DBSCAN, Spectral clustering, hierarchical clustering etc and they have their own advantages and disadvantages. The choice of the algorithm mainly depends on whether or not you ...", "dateLastCrawled": "2022-02-02T16:02:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Designing large scale similarity models using deep learning | by ...", "url": "https://mecha-mind.medium.com/designing-large-scale-similarity-models-using-deep-learning-8a2bcbdffea5?source=post_internal_links---------5----------------------------", "isFamilyFriendly": true, "displayUrl": "https://mecha-mind.medium.com/designing-large-scale-<b>similar</b>ity-models-using-deep...", "snippet": "But assuming that we have supervised data or some high-level <b>grouping</b> abstraction which places <b>similar</b> <b>items</b> under one group. For e.g. in e-commerce data, \u2018product type\u2019 such as \u2018Laptops\u2019, \u2018T-Shirts\u2019, \u2018Mobile Phones\u2019 etc. is a high-level <b>grouping</b> abstraction because <b>items</b> under \u2018Laptops\u2019 would be more <b>similar</b> to each other as <b>compared</b> to an item from \u2018Laptop\u2019 and an item from \u2018T-Shirt\u2019. <b>Similar</b> <b>Items</b> based on Product Type. The level of <b>grouping</b> abstraction <b>can</b> go ...", "dateLastCrawled": "2022-01-08T07:43:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "PRILJ: an efficient two-step method based on <b>embedding</b> and clustering ...", "url": "https://link.springer.com/article/10.1007/s10506-021-09297-1", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s10506-021-09297-1", "snippet": "Clustering generally refers to an unsupervised task consisting in <b>grouping</b> <b>similar</b> objects into clusters. More specifically, <b>similar</b> objects should fall into the same cluster, while dissimilar objects should fall into different clusters. <b>Together</b> with the design of advanced clustering algorithms (Berkhin 2002; Ester et al. 1996; Pio et al. 2012; Corizzo et al. 2019), the most critical research aspect of clustering is in the design of a proper representation of the objects/<b>items</b> at hand ...", "dateLastCrawled": "2021-12-28T13:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Unsupervised Contextual Clustering of Abstracts", "url": "https://www.sas.com/content/dam/SAS/support/en/sas-global-forum-proceedings/2020/5203-2020.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>www.sas.com</b>/content/dam/<b>SAS</b>/support/en/<b>sas</b>-global-forum-proceedings/2020/5203...", "snippet": "learning algorithms to embed NSF funding proposal abstracts text into vector <b>space</b>. Once vectorized, the abstracts were grouped <b>together</b> using K-means clustering. These techniques <b>together</b> proved to be successful at <b>grouping</b> <b>similar</b> proposals <b>together</b> and could be used to find <b>similar</b> proposals to newly submitted NSF funding proposals. To perform text analysis, <b>SAS</b>\u00ae University Edition is used which supports SASPy, <b>SAS</b>\u00ae Studio and Python JupyterLab. Gensim Doc2vec is used to generate ...", "dateLastCrawled": "2022-01-30T16:10:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Accelerating <b>TSNE</b> with GPUs: From hours to seconds | by Daniel Han-Chen ...", "url": "https://medium.com/rapids-ai/tsne-with-gpus-hours-to-seconds-9d9c17c941db", "isFamilyFriendly": true, "displayUrl": "https://medium.com/rapids-ai/<b>tsne</b>-with-gpus-hours-to-seconds-9d9c17c941db", "snippet": "This is useful for finding a natural <b>grouping</b> that will put \u201c<b>similar</b>\u201d garments close <b>together</b>. <b>TSNE</b> is able to reduce the complex <b>space</b> of fashion images to a smaller <b>space</b>, which is easier to ...", "dateLastCrawled": "2022-01-31T15:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Product</b> <b>Matching</b> in eCommerce using deep learning | by Ajinkya More ...", "url": "https://medium.com/walmartglobaltech/product-matching-in-ecommerce-4f19b6aebaca", "isFamilyFriendly": true, "displayUrl": "https://medium.com/walmartglobaltech/<b>product</b>-<b>matching</b>-in-ecommerce-4f19b6aebaca", "snippet": "<b>Grouping</b> products using universal identifiers. There are universal identifiers such as UPC, GTIN, ISBN, etc that <b>can</b> be leveraged for the purpose of identifying identical products. However, while ...", "dateLastCrawled": "2022-01-30T12:39:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-74251-5_7", "snippet": "A suitable representation is therefore essential for the success of <b>analogy</b>-based <b>learning</b> to rank. Therefore, we propose a method for analogical <b>embedding</b>, i.e., for <b>embedding</b> the data in a target <b>space</b> such that, in this <b>space</b>, the aforementioned <b>analogy</b> assumption is as valid and strongly pronounced as possible. This is accomplished by means of a neural network with a quadruple Siamese structure, which is trained on a suitably designed set of examples in the form of quadruples of objects ...", "dateLastCrawled": "2022-01-17T09:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Towards Analogy-Based Explanations in Machine Learning</b> | DeepAI", "url": "https://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://deepai.org/publication/<b>towards-analogy-based-explanations-in-machine-learning</b>", "snippet": "More specifically, we take the view that an <b>analogy</b>-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable <b>machine</b> <b>learning</b>, and that <b>analogy</b>-based explanations of the predictions produced by a <b>machine</b> <b>learning</b> algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an <b>analogy</b>-based explanation and illustrate its potential usefulness by means of some examples.", "dateLastCrawled": "2022-01-10T12:40:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Analogical <b>Embedding</b> for <b>Analogy</b>-Based <b>Learning</b> to Rank", "url": "https://homepages.uni-paderborn.de/ahmadim/IDA%202021.pdf", "isFamilyFriendly": true, "displayUrl": "https://homepages.uni-paderborn.de/ahmadim/IDA 2021.pdf", "snippet": "7 Intelligent Systems and <b>Machine</b> <b>Learning</b> <b>Embedding</b> By ignoring irrelevant or noisy features, the performance can often be improved Common feature selection techniques tailored for the case of <b>analogy</b>-based <b>learning</b> to rank. <b>Analogy</b>-based <b>learning</b> to rank (able2rank) 8 Intelligent Systems and <b>Machine</b> <b>Learning</b> Extension to feature vectors Degree of <b>analogy</b>. Analogical <b>Embedding</b> 9 Intelligent Systems and <b>Machine</b> <b>Learning</b> Positive example: preferences on both sides are coherent Negative ...", "dateLastCrawled": "2022-01-06T10:26:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Finding the Word <b>Analogy</b> from given words using Word2Vec embeddings ...", "url": "https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/finding-the-word-<b>analogy</b>-from-given-words-using-word2vec...", "snippet": "In the word <b>analogy</b> task, ... What if we can use a <b>Machine</b> <b>Learning</b> algorithm to automate this task of finding the word <b>analogy</b>. In this tutorial, we will be using Word2Vec model and a pre-trained model named \u2018GoogleNews-vectors-negative300.bin\u2018 which is trained on over 50 Billion words by Google. Each word inside the pre-trained dataset is embedded in a 300-dimensional <b>space</b> and the words which are similar in context/meaning are placed closer to each other in the <b>space</b>. Methodology to ...", "dateLastCrawled": "2022-01-26T14:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Classifying and completing word analogies by <b>machine</b> <b>learning</b> ...", "url": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/abs/pii/S0888613X21000141", "snippet": "With the emergence of word <b>embedding</b> models, a lot of progress has been made in NLP, essentially assuming that a word <b>analogy</b> like m a n: k i n g:: w o m a n: q u e e n is an instance of a parallelogram within the underlying vector <b>space</b>. In this paper, we depart from this assumption to adopt a <b>machine</b> <b>learning</b> approach, i.e., <b>learning</b> a substitute of the parallelogram model. To achieve our goal, we first review the formal modeling of analogical proportions, highlighting the properties which ...", "dateLastCrawled": "2021-11-13T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "How to Solve <b>Analogies</b> with Word2Vec | by Khuyen Tran | Towards Data ...", "url": "https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/how-to-solve-<b>analogies</b>-with-word2vec-6ebaf2354009", "snippet": "To make words understood by <b>machine</b> <b>learning</b> algorithms, word <b>embedding</b> is used to map words into vectors of real numbers. There are various word <b>embedding</b> models and word2vec is one of them. In simple words, word2vec is a group of related models that are used to produce word embeddings. These models are trained to construct the linguistic contexts of words. Word2vec takes a large corpus of text and produces a vector <b>space</b>, with each unique word in the corpus being assigned to a ...", "dateLastCrawled": "2022-01-30T16:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting ...", "url": "https://www.researchgate.net/figure/In-the-word-embedding-space-the-analogy-pairs-exhibit-interesting-algebraic_fig1_319370400", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/figure/In-the-word-<b>embedding</b>-<b>space</b>-the-<b>analogy</b>-pairs...", "snippet": "Download scientific diagram | In the word <b>embedding</b> <b>space</b>, the <b>analogy</b> pairs exhibit interesting algebraic relationships. from publication: Visual Exploration of Semantic Relationships in Neural ...", "dateLastCrawled": "2021-12-21T11:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Graph <b>Embedding</b> for Deep <b>Learning</b> | by Flawnson Tong | Towards Data Science", "url": "https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/overview-of-deep-<b>learning</b>-on-graph-<b>embeddings</b>-4305c10ad4a4", "snippet": "Using an <b>analogy</b> with word2vec, if a document is made of sentences (which is then made of words), then a graph is made of sub-graphs ... Graph <b>embedding</b> techniques take graphs and embed them in a lower dimensional continuous latent <b>space</b> before passing that representation through a <b>machine</b> <b>learning</b> model. Walk <b>embedding</b> methods perform graph traversals with the goal of preserving structure and features and aggregates these traversals which can then be passed through a recurrent neural ...", "dateLastCrawled": "2022-02-01T16:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>What Are Word Embeddings</b> for Text? - <b>Machine</b> <b>Learning</b> Mastery", "url": "https://machinelearningmastery.com/what-are-word-embeddings/", "isFamilyFriendly": true, "displayUrl": "https://<b>machinelearning</b>mastery.com/<b>what-are-word-embeddings</b>", "snippet": "This approach of <b>learning</b> an <b>embedding</b> layer requires a lot of training data and can be slow, but will learn an <b>embedding</b> both targeted to the specific text data and the NLP task. 2. Word2Vec. Word2Vec is a statistical method for efficiently <b>learning</b> a standalone word <b>embedding</b> from a text corpus. It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make the neural-network-based training of the <b>embedding</b> more efficient and since then has become the de facto standard ...", "dateLastCrawled": "2022-01-30T18:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Word Embeddings in NLP - GeeksforGeeks</b>", "url": "https://www.geeksforgeeks.org/word-embeddings-in-nlp/", "isFamilyFriendly": true, "displayUrl": "https://www.<b>geeksforgeeks</b>.org/word-<b>embeddings</b>-in-nlp", "snippet": "Word <b>Embedding</b> or Word Vector is a numeric vector input that represents a word in a lower-dimensional <b>space</b>. It allows words with similar meaning to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features. Features: Anything that relates words to one another. Eg: Age, Sports, Fitness, Employed etc. Each word vector has values corresponding to these features. Goal of Word Embeddings. To reduce dimensionality; To use a ...", "dateLastCrawled": "2022-01-30T08:38:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Zero-shot <b>learning</b> via discriminative representation extraction ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167865517303501", "snippet": "The pioneer work in ZSL can be traced to Larochelle et al. , where it verified that when test images belong to some classes that are not available at training stage, a <b>machine</b> <b>learning</b> system can still figure out what a test image is. Due to the importance of zero-shot <b>learning</b>, the number of proposed approaches has increased steadily recently.The number of new zero-shot <b>learning</b> approaches proposed every year was increasing.", "dateLastCrawled": "2021-10-30T07:08:00.0000000Z", "language": "en", "isNavigational": false}], [], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A self-supervised domain-general <b>learning</b> framework for human ventral ...", "url": "https://www.nature.com/articles/s41467-022-28091-4", "isFamilyFriendly": true, "displayUrl": "https://www.nature.com/articles/s41467-022-28091-4", "snippet": "On this view, the <b>embedding space can be thought of as</b> a high-fidelity perceptual interface, with useful visual primitives over which separate conceptual representational systems can operate.", "dateLastCrawled": "2022-01-25T10:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Spectral Af\ufb01ne-Kernel Embeddings</b> - NSF", "url": "https://par.nsf.gov/servlets/purl/10039348", "isFamilyFriendly": true, "displayUrl": "https://par.nsf.gov/servlets/purl/10039348", "snippet": "Since <b>machine</b> <b>learn-ing</b> algorithms struggle with high dimensions (an issue known as the curse of dimensionality in this context), one typically needs to map these data points from their high-dimensional space into a lower dimensional space without signi\ufb01cant distortion. Mapping data (living in RD with D\u02db1 but sampling a manifold of low in-trinsic dimensionality d \u02ddD) into a low-dimensional <b>embedding space can be thought of as</b> a preliminary feature extraction step in <b>machine</b> <b>learning</b> ...", "dateLastCrawled": "2022-01-29T21:46:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting affinity ties in a surname network", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "isFamilyFriendly": true, "displayUrl": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256603", "snippet": "<b>Machine</b> <b>learning</b>-based approaches for knowledge graph completion To cover the broadest possible range of methods and architectures in the evaluation, we identified representative methods of different model families, taking care that these methods achieve state-of-the-art performances in knowledge graph completion and have open-source implementations that favor the reproducibility of the reported results.", "dateLastCrawled": "2021-09-03T03:18:00.0000000Z", "language": "en", "isNavigational": false}], []], "all_bing_queries": ["+(embedding space)  is like +(grouping similar items together)", "+(embedding space) is similar to +(grouping similar items together)", "+(embedding space) can be thought of as +(grouping similar items together)", "+(embedding space) can be compared to +(grouping similar items together)", "machine learning +(embedding space AND analogy)", "machine learning +(\"embedding space is like\")", "machine learning +(\"embedding space is similar\")", "machine learning +(\"just as embedding space\")", "machine learning +(\"embedding space can be thought of as\")", "machine learning +(\"embedding space can be compared to\")"]}