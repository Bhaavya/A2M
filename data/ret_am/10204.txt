{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>What is Log Loss</b>? | Kaggle", "url": "https://www.kaggle.com/dansbecker/what-is-log-loss", "isFamilyFriendly": true, "displayUrl": "https://www.kaggle.com/dansbecker/<b>what-is-log-loss</b>", "snippet": "Explore and run machine learning code with Kaggle Notebooks | Using data from No attached data sources", "dateLastCrawled": "2022-02-03T00:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Cross-entropy <b>loss</b> is often simply referred to as \u201ccross-entropy,\u201d \u201clogarithmic <b>loss</b>,\u201d \u201clogistic <b>loss</b>,\u201d or \u201c<b>log</b> <b>loss</b>\u201d for short. Each predicted probability is compared to the actual class output value (0 or <b>1</b>) and a score is calculated that penalizes the probability based on the <b>distance</b> from the expected value.", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss Function</b> (Part II): <b>Logistic Regression</b> | by Shuyu Luo | Towards ...", "url": "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/optimization-<b>loss-function</b>-under-the-hood-part-ii-d20a...", "snippet": "The <b>loss function</b> of <b>logistic regression</b> is doing this exactly which is called Logistic <b>Loss</b>. See as below. If y = <b>1</b>, looking at the plot below on left, when prediction = <b>1</b>, the cost = 0, when prediction = 0, the learning algorithm is punished by a very large cost. Similarly, if y = 0, the plot on right shows, predicting 0 has no punishment but predicting <b>1</b> has a large value of cost.", "dateLastCrawled": "2022-02-02T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Log-distance Path Loss Model</b> - Ques10", "url": "https://www.ques10.com/p/47938/log-distance-path-loss-model-1/", "isFamilyFriendly": true, "displayUrl": "https://www.ques10.com/p/47938/<b>log-distance-path-loss-model</b>-<b>1</b>", "snippet": "<b>Log-distance Path Loss Model</b>. written 2.5 years ago by ankitpandey \u2666 940: need-tagging. ADD COMMENT FOLLOW SHARE EDIT. <b>1</b> Answer. 0. 59 views. written 2.5 years ago by ankitpandey \u2666 940: Both theoretical and measurement-based propagation models indicate that average received signal power decreases logarithmically with <b>distance</b>, whether in outdoor or indoor radio channels. Such models have been used extensively in the literature. The average large-scale path <b>loss</b> for an arbitrary T-R ...", "dateLastCrawled": "2022-01-26T00:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "Assuming margin to have the default value of <b>1</b>, if y=-<b>1</b>, then the <b>loss</b> will be maximum of 0 and (<b>1</b> \u2014 x). If x &gt; 0 <b>loss</b> will be x itself (higher value), if 0&lt;x&lt;<b>1</b> <b>loss</b> will be <b>1</b> \u2014 x (smaller ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Loss</b> Functions | Generative Adversarial Networks | <b>Google Developers</b>", "url": "https://developers.google.com/machine-learning/gan/loss", "isFamilyFriendly": true, "displayUrl": "https://<b>developers.google.com</b>/machine-learning/gan/<b>loss</b>", "snippet": "In the <b>loss</b> schemes we&#39;ll look at here, the generator and discriminator losses derive from a single measure of <b>distance</b> between probability distributions. In both of these schemes, however, the generator can only affect one term in the <b>distance</b> measure: the term that reflects the distribution of the fake data. So during generator training we drop the other term, which reflects the distribution of the real data.", "dateLastCrawled": "2022-01-30T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "Think of <b>loss</b> function <b>like</b> undulating mountain and gradient descent <b>is like</b> sliding down the mountain to reach the bottommost point. There is not a single <b>loss</b> function that works for all kind of data. It depends on a number of factors including the presence of outliers, choice of machine learning algorithm, time efficiency of gradient descent, ease of finding the derivatives and confidence of predictions. The purpose of this blog series is to learn about different losses and how each of ...", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "For example, if our score for a particular training example was \\(0.2\\) but the label was \\(-<b>1</b>\\), we\u2019d incur a penalty of \\(<b>1</b>.2\\), if our score was \\(-0.7\\) (meaning that this instance was predicted to have label \\(-<b>1</b>\\)) we\u2019d still incur a penalty of \\(0.3\\), but if we predicted \\(-<b>1</b>.<b>1</b>\\) then we would incur no penalty. A visualization of the hinge <b>loss</b> (in green) compared to other cost functions is given below:", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "python - Keras <b>ValueError: Dimensions must be equal</b> issue - <b>Stack Overflow</b>", "url": "https://stackoverflow.com/questions/56302243/keras-valueerror-dimensions-must-be-equal-issue", "isFamilyFriendly": true, "displayUrl": "https://<b>stackoverflow.com</b>/questions/56302243", "snippet": "You are having two <b>loss</b> functions and so you have to pass two y (ground truths) for evaluating the <b>loss</b> with respect to the predictions.. Your first prediction is the output of layer encoded_layer which has a size of (None, 8, 8, 128) as observed from the model.summary for conv2d_59 (Conv2D). But what you are passing in the fit for y is [x_train, y_train].The <b>loss</b>_<b>1</b> is expecting input of size (None, 8, 8, 128) but you are passing x_train which has a different size.. If you want the <b>loss</b>_<b>1</b> to ...", "dateLastCrawled": "2022-01-28T08:31:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "15 <b>Long Distance Relationship Problems (And</b> How To Fix Them)", "url": "https://www.modernlovelongdistance.com/long-distance-relationship-problems/", "isFamilyFriendly": true, "displayUrl": "https://www.modernlovelong<b>distance</b>.com/<b>long-distance-relationship-problems</b>", "snippet": "<b>Long distance relationship problems</b> #<b>1</b>: Feeling <b>like</b> you\u2019ve got nothing to talk about. Ever gotten stuck in a rut and struggled to find things to talk about with your long <b>distance</b> love? Have you ever felt heartsick with longing to be with your partner, but also feel <b>like</b> you just have the same-old tired conversations over and over again when you get on the phone? This is one of the most common <b>long distance relationship problems</b>. These sorts of \u201cdry periods\u201d are normal in long ...", "dateLastCrawled": "2022-02-02T23:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Cross-entropy <b>loss</b> is often simply referred to as \u201ccross-entropy,\u201d \u201clogarithmic <b>loss</b>,\u201d \u201clogistic <b>loss</b>,\u201d or \u201c<b>log</b> <b>loss</b>\u201d for short. Each predicted probability is compared to the actual class output value (0 or <b>1</b>) and a score is calculated that penalizes the probability based on the <b>distance</b> from the expected value.", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Loss functions for classification</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Loss_functions_for_classification", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Loss_functions_for_classification</b>", "snippet": "Table-I <b>Loss</b> name ()()()()Exponential ()+ \u2061 Logistic \u2061 \u2061 (+) \u2061 [\u2061 () \u2061 ()] + \u2061 Square ()()(+)Savage The sole minimizer of the expected risk, , associated with the above generated <b>loss</b> functions can be directly found from equation (<b>1</b>) and shown to be equal to the corresponding ().This holds even for the nonconvex <b>loss</b> functions, which means that gradient descent based algorithms such as gradient boosting can be used to construct the minimizer.. Proper <b>loss</b> functions, <b>loss</b> margin ...", "dateLastCrawled": "2022-02-03T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss Function</b> (Part II): <b>Logistic Regression</b> | by Shuyu Luo | Towards ...", "url": "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/optimization-<b>loss-function</b>-under-the-hood-part-ii-d20a...", "snippet": "The <b>loss function</b> of <b>logistic regression</b> is doing this exactly which is called Logistic <b>Loss</b>. See as below. If y = <b>1</b>, looking at the plot below on left, when prediction = <b>1</b>, the cost = 0, when prediction = 0, the learning algorithm is punished by a very large cost. Similarly, if y = 0, the plot on right shows, predicting 0 has no punishment but predicting <b>1</b> has a large value of cost.", "dateLastCrawled": "2022-02-02T17:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "<b>Similar</b> to above, we can take the <b>log</b> of the above expression and use properties of logs to simplify, and finally invert our entire expression to obtain the cross entropy <b>loss</b>: \\[J = -\\sum_{i=<b>1</b>}^{N} y_i\\<b>log</b> (h_\\theta(x_i)) + (<b>1</b> - y_i)\\<b>log</b>(<b>1</b> - h_\\theta(x_i))\\] The Cross-Entropy <b>Loss</b> in the case of multi-class classification . Let\u2019s supposed that we\u2019re now interested in applying the cross-entropy <b>loss</b> to multiple (&gt; 2) classes. The idea behind the <b>loss</b> function doesn\u2019t change, but now ...", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "<b>Loss</b> Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>loss</b>-functions-and-optimization-algorithms...", "snippet": "Embedding <b>loss</b> functions: It deals with problems where we have to measure whether two inputs are <b>similar</b> or dissimilar. Some examples are: <b>1</b>. L1 Hinge Error- Calculates the L1 <b>distance</b> between two ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "Measures the <b>loss</b> given an input tensor x and a labels tensor y containing values (<b>1</b> or -<b>1</b>). It is used for measuring whether two inputs are <b>similar</b> or dissimilar. It is used for measuring whether ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "5 Regression <b>Loss</b> Functions All Machine Learners Should Know | by ...", "url": "https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "isFamilyFriendly": true, "displayUrl": "https://heartbeat.comet.ml/5-regression-<b>loss</b>-functions-all-machine-learners-should...", "snippet": "Python code for Huber and <b>Log</b>-cosh <b>loss</b> functions: 5. Quantile <b>Loss</b>. In most of the real-world prediction problems, we are often interested to know about the uncertainty in our predictions. Knowing about the range of predictions as opposed to only point estimates can significantly improve decision making processes for many business problems. Quantile <b>loss</b> functions turn out to be useful when we are interested in predicting an interval instead of only point predictions. Prediction interval ...", "dateLastCrawled": "2022-02-02T07:34:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Understanding <b>Ranking Loss, Contrastive Loss</b>, Margin <b>Loss</b>, Triplet <b>Loss</b> ...", "url": "https://gombru.github.io/2019/04/03/ranking_loss/", "isFamilyFriendly": true, "displayUrl": "https://gombru.github.io/2019/04/03/ranking_<b>loss</b>", "snippet": "Positive pairs are composed by an anchor sample \\(x_a\\) and a positive sample \\(x_p\\), which <b>is similar</b> to \\(x_a\\) in the metric we aim to learn, and negative pairs composed by an anchor sample \\(x_a\\) and a negative sample \\(x_n\\), which is dissimilar to \\(x_a\\) in that metric. The objective is to learn representations with a small <b>distance</b> \\(d\\) between them for positive pairs, and greater <b>distance</b> than some margin value \\(m\\) for negative pairs. Pairwise Ranking <b>Loss</b> forces ...", "dateLastCrawled": "2022-01-30T07:49:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sound Propagation - the <b>Inverse Square Law</b>", "url": "https://www.engineeringtoolbox.com/inverse-square-law-d_890.html", "isFamilyFriendly": true, "displayUrl": "https://<b>www.engineeringtoolbox.com</b>/<b>inverse-square-law</b>-d_890.html", "snippet": "R <b>1</b> - <b>distance</b> from source to location <b>1</b> (m, ft) L p1 - <b>distance</b> from source to location 2 (m, ft) Example - Noise from a Machine. The noise from a machine in <b>distance</b> <b>1</b> m is measured to 110 dB. The noise reduction due to the <b>inverse square law</b> to a working area at <b>distance</b> 5 m can be calculated as. dL = 20 <b>log</b> ((5 m) / (<b>1</b> m)) = 14 dB . The ...", "dateLastCrawled": "2022-01-27T22:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "models.<b>word2vec</b> \u2013 <b>Word2vec</b> embeddings \u2014 <b>gensim</b>", "url": "https://radimrehurek.com/gensim/models/word2vec.html", "isFamilyFriendly": true, "displayUrl": "https://radimrehurek.com/<b>gensim</b>/models/<b>word2vec</b>.html", "snippet": "cbow_mean ({0, <b>1</b>}, optional) \u2013 If 0, use the sum of the context word vectors. If <b>1</b>, use the mean, only applies when cbow is used. alpha (float, optional) \u2013 The initial learning rate. min_alpha (float, optional) \u2013 Learning rate will linearly drop to min_alpha as training progresses. seed (int, optional) \u2013 Seed for the random number ...", "dateLastCrawled": "2022-02-02T21:44:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> Functions. <b>Loss</b> functions explanations and\u2026 | by Tomer Kordonsky ...", "url": "https://medium.com/artificialis/loss-functions-361b2ad439a0", "isFamilyFriendly": true, "displayUrl": "https://medium.com/artificialis/<b>loss</b>-functions-361b2ad439a0", "snippet": "Cross Entropy <b>Loss</b> = -(<b>1</b> \u22c5 <b>log</b>(0.<b>1</b>) + 0 + 0+ 0) = -<b>log</b>(0.<b>1</b>) = 2.303 -&gt; <b>Loss</b> is High!! We ignore the <b>loss</b> for 0 labels; The <b>loss</b> doesn\u2019t depend on the probabilities for the incorrect classes ...", "dateLastCrawled": "2022-02-03T02:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Keras</b> <b>Loss</b> Functions: Everything You Need to Know - neptune.ai", "url": "https://neptune.ai/blog/keras-loss-functions", "isFamilyFriendly": true, "displayUrl": "https://neptune.ai/b<b>log</b>/<b>keras</b>-<b>loss</b>-functions", "snippet": "<b>Keras</b> <b>Loss</b> functions 101. In <b>Keras</b>, <b>loss</b> functions are passed during the compile stage as shown below. In this example, we\u2019re defining the <b>loss function</b> by creating an instance of the <b>loss</b> class. Using the class is advantageous because you <b>can</b> pass some additional parameters.", "dateLastCrawled": "2022-02-02T23:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Loss</b> Functions and Optimization Algorithms. Demystified. | by Apoorva ...", "url": "https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c", "isFamilyFriendly": true, "displayUrl": "https://medium.com/data-science-group-iitr/<b>loss</b>-functions-and-optimization-algorithms...", "snippet": "Embedding <b>loss</b> functions: It deals with problems where we have to measure whether two inputs are similar or dissimilar. Some examples are: <b>1</b>. L1 Hinge Error- Calculates the L1 <b>distance</b> between two ...", "dateLastCrawled": "2022-01-31T23:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Custom <b>TensorFlow</b> <b>Loss</b> Functions for Advanced Machine Learning | by ...", "url": "https://towardsdatascience.com/custom-tensorflow-loss-functions-for-advanced-machine-learning-f13cdd1d188a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/custom-<b>tensorflow</b>-<b>loss</b>-functions-for-advanced-machine...", "snippet": "The DKL in a nutshell quantifies how different a distribution f is from g, in terms of information (roughly information is inversely proportional to certainty); it <b>can</b> <b>be thought</b> of as a cross entropy between distributions, and is an asymmetric <b>loss</b> that <b>can</b> take negative values*. By minimizing the DKL between f and g we basically want to to increase the information content of f relative to g.When f and g have the same amount of information, the <b>log</b> term above is 0 and the DKL <b>loss</b> is 0 as ...", "dateLastCrawled": "2022-01-30T20:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 4, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Two-ray ground-reflection model</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Two-ray_ground-reflection_model", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Two-ray_ground-reflection_model</b>", "snippet": "The standard expression of <b>Log</b> <b>distance</b> path <b>loss</b> model in [dB] is ... The 2-ray ground reflected model may <b>be thought</b> as a case of multi-slope model with break point at critical <b>distance</b> with slope 20 dB/decade before critical <b>distance</b> and slope of 40 dB/decade after the critical <b>distance</b>. Using the free-space and two-ray model above, the propagation path <b>loss</b> <b>can</b> be expressed as = {,,,} where = (/) and = / are the free-space and 2-ray path losses; is a minimum path <b>loss</b> (at smallest ...", "dateLastCrawled": "2022-01-30T13:04:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "CS231n Convolutional Neural Networks for Visual Recognition", "url": "https://cs231n.github.io/linear-classify/", "isFamilyFriendly": true, "displayUrl": "https://cs231n.github.io/linear-classify", "snippet": "For example, suppose that we have some input vector \\(x = [<b>1</b>,<b>1</b>,<b>1</b>,<b>1</b>] \\) and two weight vectors \\(w_<b>1</b> = [<b>1</b>,0,0,0]\\), \\(w_2 = [0.25,0.25,0.25,0.25] \\). Then \\(w_<b>1</b>^Tx = w_2^Tx = <b>1</b>\\) so both weight vectors lead to the same dot product, but the L2 penalty of \\(w_<b>1</b>\\) is <b>1</b>.0 while the L2 penalty of \\(w_2\\) is only 0.5. Therefore, according to the L2 penalty the weight vector \\(w_2\\) would be preferred since it achieves a lower regularization <b>loss</b>. Intuitively, this is because the weights in \\(w_2 ...", "dateLastCrawled": "2022-02-02T19:32:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "How far will Wi-Fi 6E travel in 6 GHz? - Extreme Networks", "url": "https://www.extremenetworks.com/extreme-networks-blog/how-far-will-wi-fi-6e-travel-in-6-ghz/", "isFamilyFriendly": true, "displayUrl": "https://www.extremenetworks.com/extreme-networks-b<b>log</b>/how-far-will-wi-fi-6e-travel-in...", "snippet": "There is a fancy logarithmic equation to calculate free space path <b>loss</b>; however, the 6 dB rule <b>can</b> easily estimate FSPL. The 6 dB rule states that doubling the <b>distance</b> will result in a <b>loss</b> of amplitude of 6 dB, regardless of the frequency. Therefore, at 2 meters, the path <b>loss</b> is 46 dB for 2.4 GHz, 53 dB for 5 GHz, and 55 dB for 6 GHz.", "dateLastCrawled": "2022-02-02T23:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Sound power level SWL and sound pressure level SPL <b>distance</b> compare ...", "url": "http://www.sengpielaudio.com/calculator-soundpower.htm", "isFamilyFriendly": true, "displayUrl": "www.sengpielaudio.com/calculator-soundpower.htm", "snippet": "At r = <b>1</b> meter <b>distance</b>, the sound pressure level (SPL) ... = 20 <b>log</b> (<b>1</b>/r (Q \u03c1 c P ac /(4 \u03c0))<b>1</b>/2/p 0) (2) where L p = sound pressure level in dB p 0 = 2 \u00d7 10 \u22125 - reference sound pressure in Pa Note: That for every doubling of the <b>distance</b> from the noise source, the sound pressure level L p, will be reduced by 6 decibels. For a spherical sound propagation (Q = 2) at a <b>distance</b> of r = \u221a(<b>1</b>/2\u03c0) = 0.3989 m the decibel value of the sound pressure level is equal to the sound power level ...", "dateLastCrawled": "2022-02-02T22:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Sound level <b>distance</b> damping decibel dB damping calculation calculator ...", "url": "http://www.sengpielaudio.com/calculator-distance.htm", "isFamilyFriendly": true, "displayUrl": "www.sengpielaudio.com/calculator-<b>distance</b>.htm", "snippet": "proportional to the <b>distance</b> <b>1</b>/r from the sound source. That is the <b>1</b>/r law or the inverse <b>distance</b> law. Sound pressure (amplitude) falls inversely proportional to the square of the <b>distance</b> <b>1</b>/r 2 from the sound source. Really wrong: Sound pressure level decreases by (\u2212)6 dB for", "dateLastCrawled": "2022-02-02T21:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "15 <b>Long Distance Relationship Problems (And</b> How To Fix Them)", "url": "https://www.modernlovelongdistance.com/long-distance-relationship-problems/", "isFamilyFriendly": true, "displayUrl": "https://www.modernlovelong<b>distance</b>.com/<b>long-distance-relationship-problems</b>", "snippet": "The <b>distance</b> <b>can</b> force you to talk about all sorts of things you might not have discussed if doing other things (or, um, each other) was a realistic option. When there\u2019s nothing to build your relationship on but words, you <b>can</b> get to know someone\u2019s heart and mind at a very deep level, quite quickly. On the other hand, falling in love long <b>distance</b> is a risky business.", "dateLastCrawled": "2022-02-02T23:14:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "<b>Loss</b> and <b>Loss</b> <b>Functions for Training Deep Learning Neural Networks</b>", "url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/<b>loss</b>-and-<b>loss</b>-<b>functions-for-training-deep-learning</b>...", "snippet": "Cross-entropy <b>loss</b> is often simply referred to as \u201ccross-entropy,\u201d \u201clogarithmic <b>loss</b>,\u201d \u201clogistic <b>loss</b>,\u201d or \u201c<b>log</b> <b>loss</b>\u201d for short. Each predicted probability is <b>compared</b> to the actual class output value (0 or <b>1</b>) and a score is calculated that penalizes the probability based on the <b>distance</b> from the expected value.", "dateLastCrawled": "2022-02-02T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "contractualRules": [{"_type": "ContractualRules/LicenseAttribution", "targetPropertyName": "snippet", "targetPropertyIndex": 1, "mustBeCloseToContent": true, "license": {"name": "CC-BY-SA", "url": "http://creativecommons.org/licenses/by-sa/3.0/"}, "licenseNotice": "Text under CC-BY-SA license"}], "name": "<b>Loss functions for classification</b> - <b>Wikipedia</b>", "url": "https://en.wikipedia.org/wiki/Loss_functions_for_classification", "isFamilyFriendly": true, "displayUrl": "https://<b>en.wikipedia.org</b>/wiki/<b>Loss_functions_for_classification</b>", "snippet": "Table-I <b>Loss</b> name ()()()()Exponential ()+ \u2061 Logistic \u2061 \u2061 (+) \u2061 [\u2061 () \u2061 ()] + \u2061 Square ()()(+)Savage The sole minimizer of the expected risk, , associated with the above generated <b>loss</b> functions <b>can</b> be directly found from equation (<b>1</b>) and shown to be equal to the corresponding ().This holds even for the nonconvex <b>loss</b> functions, which means that gradient descent based algorithms such as gradient boosting <b>can</b> be used to construct the minimizer.. Proper <b>loss</b> functions, <b>loss</b> margin ...", "dateLastCrawled": "2022-02-03T06:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "A Gentle Introduction to <b>Probability</b> Scoring Methods in Python", "url": "https://machinelearningmastery.com/how-to-score-probability-predictions-in-python/", "isFamilyFriendly": true, "displayUrl": "https://machinelearningmastery.com/how-to-score-<b>probability</b>-predictions-in-python", "snippet": "<b>Log</b> <b>loss</b>, also called \u201clogistic <b>loss</b>,\u201d \u201clogarithmic <b>loss</b>,\u201d or \u201ccross entropy\u201d <b>can</b> be used as a measure for evaluating predicted probabilities. Each predicted <b>probability</b> is <b>compared</b> to the actual class output value (0 or <b>1</b>) and a score is calculated that penalizes the <b>probability</b> based on the <b>distance</b> from the expected value.", "dateLastCrawled": "2022-02-02T21:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "A Brief Overview of <b>Loss</b> Functions in <b>Pytorch</b> | by Pratyaksha Jha ...", "url": "https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7", "isFamilyFriendly": true, "displayUrl": "https://medium.com/udacity-<b>pytorch</b>-challengers/a-brief-overview-of-<b>loss</b>-functions-in-p...", "snippet": "Assuming margin to have the default value of <b>1</b>, if y=-<b>1</b>, then the <b>loss</b> will be maximum of 0 and (<b>1</b> \u2014 x). If x &gt; 0 <b>loss</b> will be x itself (higher value), if 0&lt;x&lt;<b>1</b> <b>loss</b> will be <b>1</b> \u2014 x (smaller ...", "dateLastCrawled": "2022-02-02T14:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "machine learning - <b>hinge loss</b> vs logistic <b>loss</b> advantages and ...", "url": "https://stats.stackexchange.com/questions/146277/hinge-loss-vs-logistic-loss-advantages-and-disadvantages-limitations", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/146277/<b>hinge-loss</b>-vs-<b>log</b>istic-<b>loss</b>...", "snippet": "@Firebug had a good answer (+<b>1</b>). In fact, I had a similar question here. What are the impacts of choosing different <b>loss</b> functions in classification to approximate 0-<b>1</b> <b>loss</b>. I just want to add more on another big advantages of logistic <b>loss</b>: probabilistic interpretation. An example, <b>can</b> be found here", "dateLastCrawled": "2022-01-26T09:42:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Picking Loss Functions - A comparison between MSE, Cross Entropy</b>, and ...", "url": "https://rohanvarma.me/Loss-Functions/", "isFamilyFriendly": true, "displayUrl": "https://rohanvarma.me/<b>Loss</b>-Functions", "snippet": "For example, if our score for a particular training example was \\(0.2\\) but the label was \\(-<b>1</b>\\), we\u2019d incur a penalty of \\(<b>1</b>.2\\), if our score was \\(-0.7\\) (meaning that this instance was predicted to have label \\(-<b>1</b>\\)) we\u2019d still incur a penalty of \\(0.3\\), but if we predicted \\(-<b>1</b>.<b>1</b>\\) then we would incur no penalty. A visualization of the hinge <b>loss</b> (in green) <b>compared</b> to other cost functions is given below:", "dateLastCrawled": "2021-12-04T02:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "What are <b>Loss</b> Functions?. After the post on activation functions\u2026 | by ...", "url": "https://towardsdatascience.com/what-is-loss-function-1e2605aeb904", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/what-is-<b>loss-function</b>-<b>1</b>e2605aeb904", "snippet": "The <b>loss function</b> is the function that computes the <b>distance</b> between the current output of the algorithm ... The MAE function is more robust to outliers because it is based on absolute value <b>compared</b> to the square of the MSE. It\u2019s like a median, outliers <b>can</b>\u2019t really impact her behavior. You <b>can</b> implement it easily like this: def mean_square_error(y_true, y_pred): return K.mean(K.abs(y_true-y_pred), axis=-<b>1</b>) We <b>can</b> visualize the behavior of the MAE function comparing a range of values ...", "dateLastCrawled": "2022-01-30T13:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Machine Learning - Performance Metrics</b> - <b>Tutorialspoint</b>", "url": "https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_algorithms_performance_metrics.htm", "isFamilyFriendly": true, "displayUrl": "https://<b>www.tutorialspoint.com</b>/machine_learning_with_python/machine_learning...", "snippet": "As we know that accuracy is the count of predictions (predicted value = actual value) in our model whereas <b>Log</b> <b>Loss</b> is the amount of uncertainty of our prediction based on how much it varies from the actual label. With the help of <b>Log</b> <b>Loss</b> value, we <b>can</b> have more accurate view of the performance of our model. We <b>can</b> use <b>log</b>_<b>loss</b> function of sklearn.metrics to compute <b>Log</b> <b>Loss</b>.", "dateLastCrawled": "2022-02-02T08:38:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "What is <b>the advantage/disadvantage of Hinge-loss compared</b> to cross ...", "url": "https://www.quora.com/What-is-the-advantage-disadvantage-of-Hinge-loss-compared-to-cross-entropy", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/What-is-<b>the-advantage-disadvantage-of-Hinge-loss-compared</b>-to...", "snippet": "Answer (<b>1</b> of 2): Cross Entropy (or <b>Log</b> <b>Loss</b>), Hing <b>Loss</b> (SVM <b>Loss</b>), Squared <b>Loss</b> etc. are different forms of <b>Loss</b> functions. <b>Log</b> <b>Loss</b> in the classification context gives Logistic Regression, while the Hinge <b>Loss</b> is Support Vector Machines. Logistic Regression and SVMs both are linear classifiers,...", "dateLastCrawled": "2022-01-29T16:27:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "How to evaluate my <b>Classification</b> Model results | by Songhao Wu ...", "url": "https://towardsdatascience.com/top-5-metrics-for-evaluating-classification-model-83ede24c7584", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/top-5-metrics-for-evaluating-<b>classification</b>-model-83ede...", "snippet": "Source: Author \u2014 <b>Log</b> <b>Loss</b> Value curve. <b>Log</b> <b>Loss</b> value <b>can</b> vary from 0 to infinity. However, if you calculate according to the formula, the <b>log</b> <b>loss</b> function is 0.69 if you just blindly assign 50% to each case. Thus, you should definitely keep the <b>log</b> <b>loss</b> value below 0.69 and decide the threshold based on business requirements. Example", "dateLastCrawled": "2022-02-02T13:09:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> - ResearchGate", "url": "https://www.researchgate.net/publication/321341661_Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/321341661_<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "TensorFlow is an interface for expressing <b>machine</b> <b>learning</b> algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or ...", "dateLastCrawled": "2022-01-04T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "Choosing and Customizing <b>Loss</b> Functions for Image Processing | by ...", "url": "https://towardsdatascience.com/choosing-and-customizing-loss-functions-for-image-processing-a0e4bf665b0a", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/choosing-and-customizing-<b>loss</b>-functions-for-image...", "snippet": "This is what a <b>machine</b> <b>learning</b> (ML) algorithm does during training. More specifically, ... Cross-Entropy (aka <b>log</b> <b>loss</b>): calculates the differences between the predicted class probabilities and those from ground truth across a logarithmic scale. Useful for object detection. Weighted Cross-Entropy: improves on Cross-Entropy accuracy by adding weights to certain aspects (e.g., certain object classes) which are under-represented in the data (e.g., objects occurring in fewer data samples\u00b3 ...", "dateLastCrawled": "2022-01-31T10:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PPT) <b>Machine</b> <b>Learning</b> by <b>Analogy</b> | Colleen Farrelly - Academia.edu", "url": "https://www.academia.edu/30404581/Machine_Learning_by_Analogy", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/30404581/<b>Machine</b>_<b>Learning</b>_by_<b>Analogy</b>", "snippet": "<b>Machine</b> <b>Learning</b> by <b>Analogy</b> Colleen M. Farrelly Overview of Problem Many <b>machine</b> <b>learning</b> methods exist in the literature and in industry. What works well for one problem may not work well for the next problem. In addition to poor model fit, an incorrect application of methods can lead to incorrect inference. Implications for data-driven business decisions. Low future confidence in data science and its results. Lower quality software products. Understanding the intuition and mathematics ...", "dateLastCrawled": "2022-01-02T06:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Understanding the 3 most common <b>loss</b> functions for <b>Machine</b> <b>Learning</b> ...", "url": "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/understanding-the-3-most-common-<b>loss</b>-functions-for...", "snippet": "A <b>loss function</b> in <b>Machine</b> <b>Learning</b> is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The <b>loss function</b> will take two items as input: the output value of our model and the ground truth expected value. The output of the <b>loss function</b> is called the <b>loss</b> which is a measure of how well our model did at predicting the outcome. A high value for the <b>loss</b> means our model performed very poorly. A low value for the <b>loss</b> means our model performed ...", "dateLastCrawled": "2022-02-02T13:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Machined Learnings: ML and OR: An <b>analogy</b> with cost-sensitive ...", "url": "http://www.machinedlearnings.com/2010/07/ml-and-or.html", "isFamilyFriendly": true, "displayUrl": "www.<b>machine</b>d<b>learning</b>s.com/2010/07/ml-and-or.html", "snippet": "Nonetheless I&#39;ve been amusing myself by thinking about it, in particular trying to think about it from a <b>machine</b> <b>learning</b> reduction standpoint. The simplest well-understood reduction that I can think of which is analogous to supplying estimates to a linear program is the reduction of cost-sensitive multiclass classification (CSMC) to regression.", "dateLastCrawled": "2021-12-25T17:16:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "<b>Machine</b> <b>Learning</b> Concepts for Revision | by Raunak Sarada | Medium", "url": "https://raunaksarada-cse21.medium.com/machine-learning-concepts-for-revision-491384952d27", "isFamilyFriendly": true, "displayUrl": "https://raunaksarada-cse21.medium.com/<b>machine</b>-<b>learning</b>-concepts-for-revision-491384952d27", "snippet": "ML Concepts. A.I \u2014 Intelligence showed by machines which is common for humans <b>Machine</b> <b>Learning</b>- Recognize the pattern in data and automatically learn and improve through experience without explicitly being programmed Deep <b>Learning</b>- branch of <b>machine</b> <b>learning</b>.We have to deal with lots of data so in that case problems can\u2019t be solved with simple ML algorithms.", "dateLastCrawled": "2022-01-25T20:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>machine</b> <b>learning</b> - What is the relation between a <b>loss</b> function and an ...", "url": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-loss-function-and-an-energy-function", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/409247/what-is-the-relation-between-a-<b>loss</b>...", "snippet": "A <b>loss</b> function is a function that measures the distance between the expected value and the actual value of a model (an example of a <b>loss</b> function is the cross entropy).. An energy function can be defined as a function that we want to minimise or maximise and it is a function of the variables of the system. It is referred to as &quot;energy function&quot; because it is often related or compared to the concept of &quot;energy&quot; in physics. These two expression seem to refer to the same concept.", "dateLastCrawled": "2022-01-17T15:35:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Shannon <b>entropy</b> in the context of <b>machine</b> <b>learning</b> and AI | by Frank ...", "url": "https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32", "isFamilyFriendly": true, "displayUrl": "https://medium.com/swlh/shannon-<b>entropy</b>-in-the-context-of-<b>machine</b>-<b>learning</b>-and-ai-24...", "snippet": "In <b>machine</b> <b>learning</b>, it is a very useful measure for the similarity of probability distributions and serves as a <b>loss</b> function (more details below). Uses in <b>machine</b> <b>learning</b>", "dateLastCrawled": "2022-01-30T12:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>Analogy</b> between Neural network and naive bayes - Cross Validated", "url": "https://stats.stackexchange.com/questions/219687/analogy-between-neural-network-and-naive-bayes", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/219687", "snippet": "w 0 = ln. \u2061. 1 \u2212 \u03c0 \u03c0 + \u2211 i \u03bc i 1 2 \u2212 \u03bc i 0 2 2 \u03c3 i 2. This is exactly the form of logistic regression, where w i are the weights and w 0 is the bias. However, logistic regression (and therefore single-layer neural nets) don&#39;t necessarily impose these forms on the weights/bias, and are therefore more general than naive Bayes.", "dateLastCrawled": "2022-01-26T15:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>All Unit MCQ questions of ML</b> \u2013 TheCodingShef", "url": "https://thecodingshef.com/all-unit-mcq-questions-of-machine-learning/", "isFamilyFriendly": true, "displayUrl": "https://thecodingshef.com/<b>all-unit-mcq-questions-of</b>-<b>machine</b>-<b>learning</b>", "snippet": "Correct option is C. Choose the correct option regarding <b>machine</b> <b>learning</b> (ML) and artificial intelligence (AI) ML is a set of techniques that turns a dataset into a software. AI is a software that can emulate the human mind. ML is an alternate way of programming intelligent machines. All of the above. Correct option is D.", "dateLastCrawled": "2022-01-30T22:09:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Predicting the 2019 All-<b>NBA teams with machine learning</b> - <b>Dribble Analytics</b>", "url": "https://dribbleanalytics.blog/2019/03/ml-all-nba-predict/", "isFamilyFriendly": true, "displayUrl": "https://<b>dribbleanalytics</b>.blog/2019/03/ml-all-nba-predict", "snippet": "<b>Log loss is like</b> accuracy, but instead of analyzing the labeled predictions, it analyzes the prediction probabilities. This is particularly important given that we\u2019re more interested in the probabilities than we are in the actual labels. A \u201cperfect\u201d model will have a log loss of 0. The table below shows each model\u2019s log loss. Model Log loss; SVC: 0.416: RF: 0.416: KNN: 0.403: DNN: 0.43: The SVC and RF have the same log loss, while the KNN has the lowest. Next, let\u2019s look at the ...", "dateLastCrawled": "2022-01-04T02:22:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "[OC] Predicting the 2019 All-<b>NBA teams with machine learning</b> : nba", "url": "https://www.reddit.com/r/nba/comments/aw51j6/oc_predicting_the_2019_allnba_teams_with_machine/", "isFamilyFriendly": true, "displayUrl": "https://<b>www.reddit.com</b>/.../aw51j6/oc_predicting_the_2019_allnba_teams_with_<b>machine</b>", "snippet": "[OC] Predicting the 2019 All-<b>NBA teams with machine learning</b>. Original Content. This post has a lot of graphs. If you don&#39;t want to click on each one individually, they&#39;re all in an imgur album here. There is a tl;dr and summary infographic at the very end. Introduction . Last year, media members unanimously selected LeBron James to the All-NBA first team, giving him a record 12 All-NBA first team selections. However, given the Lakers recent struggles and LeBron&#39;s absence earlier in the ...", "dateLastCrawled": "2021-10-14T12:52:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Predicting the 2019 All-NBA teams with <b>machine</b> <b>learning</b>", "url": "https://dribbleanalytics.blogspot.com/2019/03/ml-all-nba-predict.html", "isFamilyFriendly": true, "displayUrl": "https://dribbleanalytics.blogspot.com/2019/03/ml-all-nba-predict.html", "snippet": "Predicting the 2019 All-NBA teams with <b>machine</b> <b>learning</b> Get link; Facebook; Twitter; Pinterest; Email; Other Apps; March 01, 2019 There is a summary at the bottom if you want to skip to the results. Introduction Last year, media members unanimously selected LeBron James to the All-NBA first team, giving him a record 12 All-NBA first team selections. However, given the Lakers recent struggles and LeBron&#39;s absence earlier in the season, LeBron might miss not only the first team but also the ...", "dateLastCrawled": "2021-12-11T07:58:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "What\u2019s considered a good Log <b>Loss</b> in <b>Machine</b> <b>Learning</b> ? | by Federico ...", "url": "https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@fzammito/whats-considered-a-good-log-<b>loss</b>-in-<b>machine</b>-<b>learning</b>-a529...", "snippet": "<b>Log Loss is similar</b> to the Accuracy, but it will favor models that distinguish more strongly the classes. Log <b>Loss</b> it useful to compare models not only on their output but on their probabilistic ...", "dateLastCrawled": "2022-01-30T14:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>What is an intuitive explanation for the log</b> loss function? - Quora", "url": "https://www.quora.com/What-is-an-intuitive-explanation-for-the-log-loss-function", "isFamilyFriendly": true, "displayUrl": "https://www.quora.com/<b>What-is-an-intuitive-explanation-for-the-log</b>-loss-function", "snippet": "Answer (1 of 8): To me an intuitive explanation is that minimizing the log loss equals minimizing the Kullback-Leibler divergence (Kullback\u2013Leibler divergence - Wikipedia) between the function you want to optimize (for example a neural network) and the true function that generates the data (from ...", "dateLastCrawled": "2022-01-30T02:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Key techniques for Evaluating <b>Machine</b> <b>Learning</b> models - Data Analytics", "url": "https://vitalflux.com/key-techniques-evaluating-machine-learning-models-performance/", "isFamilyFriendly": true, "displayUrl": "https://vitalflux.com/key-techniques-evaluating-<b>machine</b>-<b>learning</b>-models-performance", "snippet": "Log loss is used to evaluate the performance of classification <b>machine</b> <b>learning</b> models that are built using classification algorithms such as logistic regression, support vector <b>machine</b> (SVM), random forest, and gradient boosting. The idea behind the use of <b>Log loss is similar</b> to taking a base-e exponential or natural logarithm in order to compare model scores from high-value functions which may indicate poor <b>machine</b> <b>learning</b> model performance. The logarithmic loss value is defined as ...", "dateLastCrawled": "2022-01-31T06:50:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Loss In Machine Learning</b> - 02/2021 - Course f", "url": "https://www.coursef.com/loss-in-machine-learning", "isFamilyFriendly": true, "displayUrl": "https://www.coursef.com/<b>loss-in-machine-learning</b>", "snippet": "<b>Log Loss is similar</b> to the Accuracy, but it will favor models that ... Two of the most popular loss functions in <b>machine</b> <b>learning</b> are the 0-1 loss function and the quadratic loss function. The 0-1 loss function is an indicator function that returns 1 when the target and output are not equal and zero otherwise: 0-1 Loss: The quadratic loss is a commonly used symmetric loss \u2026 161 People Used View all course \u203a\u203a Visit Site \u2039 1; 2 \u203a FAQs. Do online classes have tests? Not all online ...", "dateLastCrawled": "2021-02-08T01:12:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Diagnosing malaria from some symptoms: a <b>machine</b> <b>learning</b> approach and ...", "url": "https://link.springer.com/article/10.1007/s12553-020-00488-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s12553-020-00488-5", "snippet": "<b>Machine</b> <b>learning</b> tools have become available in the diagnosis and prediction of diseases, thereby saving costs and improving the likelihood of survivorship, especially in some terminal diseases. In the case of infectious diseases, early diagnosis is highly needed in isolating the subjects to reduce the spread of the disease. Researchers continue to propose new data mining tools that help in the early diagnosis of diseases, reducing the mortality rate, and improving the quality of life of ...", "dateLastCrawled": "2021-12-03T05:54:00.0000000Z", "language": "en", "isNavigational": false}], [], [], []], "all_bing_queries": ["+(log loss)  is like +(distance to 1)", "+(log loss) is similar to +(distance to 1)", "+(log loss) can be thought of as +(distance to 1)", "+(log loss) can be compared to +(distance to 1)", "machine learning +(log loss AND analogy)", "machine learning +(\"log loss is like\")", "machine learning +(\"log loss is similar\")", "machine learning +(\"just as log loss\")", "machine learning +(\"log loss can be thought of as\")", "machine learning +(\"log loss can be compared to\")"]}