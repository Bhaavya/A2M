{"src_spec_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>A Markov Decision Process Approach to Vacant</b> Taxi Routing with E ...", "url": "https://www.researchgate.net/publication/328013734_A_Markov_Decision_Process_Approach_to_Vacant_Taxi_Routing_with_E-hailing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328013734_<b>A_Markov_Decision_Process_Approach</b>...", "snippet": "Abstract. The optimal routing of a vacant taxi is formulated as a <b>Markov Decision Process</b> (<b>MDP</b>) problem to account for long-term profit over the full working period. The state is defined by the ...", "dateLastCrawled": "2022-01-05T03:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Study <b>of Recommender Systems Using Markov Decision Process</b>", "url": "https://www.researchgate.net/publication/331681629_A_Study_of_Recommender_Systems_Using_Markov_Decision_Process", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/331681629_A_Study_of_Recommender_Systems...", "snippet": "One of the proposed approaches is to consider a recommender system as a <b>Markov decision process</b> (<b>MDP</b>) problem and try to solve it using reinforcement learning (RL). However, existing RL-based ...", "dateLastCrawled": "2021-08-11T21:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Next-POI Recommendations Matching User\u2019s Visit <b>Behaviour</b>", "url": "https://link.springer.com/chapter/10.1007/978-3-030-65785-7_4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-65785-7_4", "snippet": "We model the next-POI selection problem as a <b>Markov Decision Process</b> (<b>MDP</b>). A <b>MDP</b> is a tuple \\((S,A,T,r,\\gamma )\\). S denotes the set states, where a state s represents the visit to a specific POI and its context (e.g., the weather condition at the visit time).", "dateLastCrawled": "2022-01-16T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Popularity, novelty and relevance in point of interest recommendation ...", "url": "https://link.springer.com/article/10.1007/s40558-021-00214-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40558-021-00214-5", "snippet": "User behaviour modelling. We model the user (<b>tourist</b>) choice-making behaviour with a <b>Markov Decision Process</b> (<b>MDP</b>) (Sutton and Barto 1998).A <b>MDP</b> is a tuple \\((S, A, T, r, \\gamma )\\). S is the set of possible states; in our scenario, a state models the visit to a POI in a specific contextual situation. For instance, a <b>tourist</b> who visits Florence could be at Ponte Vecchio (POI) in an overcast, mild morning.", "dateLastCrawled": "2022-02-02T01:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Partially observable <b>Markov</b> <b>decision</b> processes for <b>spoken dialog</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0885230806000283", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0885230806000283", "snippet": "Fully observable <b>Markov</b> <b>decision</b> processes (usually just called <b>Markov</b> <b>decision</b> processes, or MDPs) take a very different approach to automated action selection. As their name implies, a <b>Markov decision process</b> is a simplification of a POMDP in which the state is fully observable. This simplification is shown graphically in Fig. 11. In an <b>MDP</b>, A \u223c u is again regarded as a random observed variable and S m \u2032 is a deterministic function of S m, A m, A \u223c u \u2032, and A c \u2032. Since at a ...", "dateLastCrawled": "2022-01-20T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Towards bringing heterogeneous agents to cooperation: an architecture ...", "url": "https://www.deepdyve.com/lp/association-for-computing-machinery/towards-bringing-heterogeneous-agents-to-cooperation-an-architecture-P4q9CwRJPR", "isFamilyFriendly": true, "displayUrl": "https://www.deepdyve.com/lp/association-for-computing-machinery/towards-bringing...", "snippet": "As the system is probabilistic at many levels, <b>Markov Decision Process</b> (<b>MDP</b>) are appropriate to solve these problems since they can handle uncertainty and allow to find optimal policies given a reward function with machine learning techniques. More complex models also support uncertainty over the agent \u2122s perceptions and actions, <b>like</b> Partially Observable <b>MDP</b> (POMDP) that use a probabilistic state representation. To use these models for services, complexity must be heavily reduced since ...", "dateLastCrawled": "2021-03-21T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Planning to Chronicle", "url": "http://robotics.cs.rutgers.edu/wafr2020/wp-content/uploads/sites/7/2020/05/WAFR_2020_FV_34.pdf", "isFamilyFriendly": true, "displayUrl": "robotics.cs.rutgers.edu/wafr2020/wp-content/uploads/sites/7/2020/05/WAFR_2020_FV_34.pdf", "snippet": "<b>process</b> via a variant of a hidden <b>Markov</b> model, and specify the event sequences of interest as a regular language, developing a vocabulary of \u2018mutators\u2019 that enable sophisticated requirements to be expressed. Un-der di erent suppositions about the information gleaned about the event model, we formulate and solve di erent planning problems. The core un-derlying idea is the construction of a product between the event model and a speci cation automaton. The paper reports and compares perfor ...", "dateLastCrawled": "2021-11-05T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Automatic Generation of Dialogues based on Grammatical ... - IOS Press", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs179876", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs179876", "snippet": "The key idea is to model dialogue management as a <b>MDP</b> or as a Partially Observable <b>Markov Decision Process</b> (POMDP), and let the system itself to learn the best action to take in each conversational situation through repeated interactions with a user (real or simulated). In the research works of Png , Mahadik and Young RL is also used; the dialogue model and policy are parameterized by using a suitable reward function that can be optimized using RL. In Du\u0161ek , it is presented a <b>new</b> natural ...", "dateLastCrawled": "2022-02-03T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Artificial Intelligence By Example.pdf [5wgl5d599xo7]", "url": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "isFamilyFriendly": true, "displayUrl": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "snippet": "Reinforcement learning often uses the <b>Markov Decision Process</b> (<b>MDP</b>). <b>MDP</b> contains a memoryless and unlabeled actionreward equation with a learning parameter. This equation, the Bellman equation (often coined as the Q function), was used to beat world-class Atari gamers. Become an Adaptive Thinker Chapter 1 The goal here is not to simply take the easy route. We&#39;re striving to break complexity into understandable parts and confront them with reality. You are going to find out right from the ...", "dateLastCrawled": "2022-01-27T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "M NAVIGATION IN ENVIRONMENTS WITH N L USING ABSTRACT 2-D MAPS", "url": "https://openreview.net/pdf?id=_lV1OrJIgiG", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=_lV1OrJIgiG", "snippet": "of its start and goal locations (\u201ctask context\u201d in Figure 1). This is akin to a <b>tourist</b> attempting to \ufb01nd a landmark <b>in a new</b> <b>city</b>: without any further help, this would be very challenging; but when equipped with a 2-D map with a \u201cyou are here\u201d symbol and an indicator of the landmark, the <b>tourist</b> can easily plan a path to reach the landmark without needing to explore or train excessively. Navigation is a fundamental capability of all embodied agents, both arti\ufb01cial and natural ...", "dateLastCrawled": "2021-12-25T02:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "A Study <b>of Recommender Systems Using Markov Decision Process</b>", "url": "https://www.researchgate.net/publication/331681629_A_Study_of_Recommender_Systems_Using_Markov_Decision_Process", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/331681629_A_Study_of_Recommender_Systems...", "snippet": "One of the proposed approaches is to consider a recommender system as a <b>Markov decision process</b> (<b>MDP</b>) problem and try to solve it using reinforcement learning (RL). However, existing RL-based ...", "dateLastCrawled": "2021-08-11T21:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) <b>A Markov Decision Process Approach to Vacant</b> Taxi Routing with E ...", "url": "https://www.researchgate.net/publication/328013734_A_Markov_Decision_Process_Approach_to_Vacant_Taxi_Routing_with_E-hailing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328013734_<b>A_Markov_Decision_Process_Approach</b>...", "snippet": "Abstract. The optimal routing of a vacant taxi is formulated as a <b>Markov Decision Process</b> (<b>MDP</b>) problem to account for long-term profit over the full working period. The state is defined by the ...", "dateLastCrawled": "2022-01-05T03:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Popularity, novelty and relevance in point of interest recommendation ...", "url": "https://link.springer.com/article/10.1007/s40558-021-00214-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40558-021-00214-5", "snippet": "User behaviour modelling. We model the user (<b>tourist</b>) choice-making behaviour with a <b>Markov Decision Process</b> (<b>MDP</b>) (Sutton and Barto 1998).A <b>MDP</b> is a tuple \\((S, A, T, r, \\gamma )\\). S is the set of possible states; in our scenario, a state models the visit to a POI in a specific contextual situation. For instance, a <b>tourist</b> who visits Florence could be at Ponte Vecchio (POI) in an overcast, mild morning.", "dateLastCrawled": "2022-02-02T01:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Next-POI Recommendations Matching User\u2019s Visit <b>Behaviour</b>", "url": "https://link.springer.com/chapter/10.1007/978-3-030-65785-7_4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-65785-7_4", "snippet": "We model the next-POI selection problem as a <b>Markov Decision Process</b> (<b>MDP</b>). A <b>MDP</b> is a tuple \\((S,A,T,r,\\gamma )\\). S denotes the set states, where a state s represents the visit to a specific POI and its context (e.g., the weather condition at the visit time).", "dateLastCrawled": "2022-01-16T20:53:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Urban water <b>resource management for sustainable environment planning</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0195925520307939", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0195925520307939", "snippet": "In AIDWRP, <b>Markov Decision Process</b> (<b>MDP</b>) discusses the dynamic water resource management issue with annual use and released locational constraints that develop sensitivity-driven methods to optimize several efficient environmental planning and management policies. Consequently, there is a specific relief from the engagement of supply and demand for water resources, and substantial improvements in local economic efficiency have been simulated with numerical outcomes.", "dateLastCrawled": "2022-01-01T19:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Clustering Users\u2019 POIs Visit Trajectories for Next-POI Recommendation", "url": "https://www.inf.unibz.it/~ricci//papers/Massimo-Enter-2019.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.inf.unibz.it/~ricci//papers/Massimo-Enter-2019.pdf", "snippet": "3.1 Sequential <b>Decision</b> Making Model The <b>tourist</b> POIs visit trajectory generation task is modelled here as a \ufb01nite <b>Markov Decision Process</b> (<b>MDP</b>). A <b>MDP</b> is a tuple (S,A,T,r,\u03b3). S is a \ufb01nite set of states and a state represents the visit to a POI in a speci\ufb01c context (weather, temperature and day). A is a \ufb01nite set of actions: the ...", "dateLastCrawled": "2022-01-02T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Towards bringing heterogeneous agents to cooperation: an architecture ...", "url": "https://www.deepdyve.com/lp/association-for-computing-machinery/towards-bringing-heterogeneous-agents-to-cooperation-an-architecture-P4q9CwRJPR", "isFamilyFriendly": true, "displayUrl": "https://www.deepdyve.com/lp/association-for-computing-machinery/towards-bringing...", "snippet": "As the system is probabilistic at many levels, <b>Markov Decision Process</b> (<b>MDP</b>) are appropriate to solve these problems since they can handle uncertainty and allow to find optimal policies given a reward function with machine learning techniques. More complex models also support uncertainty over the agent \u2122s perceptions and actions, like Partially Observable <b>MDP</b> (POMDP) that use a probabilistic state representation. To use these models for services, complexity must be heavily reduced since ...", "dateLastCrawled": "2021-03-21T06:56:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Planning to Chronicle", "url": "http://robotics.cs.rutgers.edu/wafr2020/wp-content/uploads/sites/7/2020/05/WAFR_2020_FV_34.pdf", "isFamilyFriendly": true, "displayUrl": "robotics.cs.rutgers.edu/wafr2020/wp-content/uploads/sites/7/2020/05/WAFR_2020_FV_34.pdf", "snippet": "<b>process</b> via a variant of a hidden <b>Markov</b> model, and specify the event sequences of interest as a regular language, developing a vocabulary of \u2018mutators\u2019 that enable sophisticated requirements to be expressed. Un-der di erent suppositions about the information gleaned about the event model, we formulate and solve di erent planning problems. The core un-derlying idea is the construction of a product between the event model and a speci cation automaton. The paper reports and compares perfor ...", "dateLastCrawled": "2021-11-05T17:45:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Automatic Generation of Dialogues based on Grammatical ... - IOS Press", "url": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs179876", "isFamilyFriendly": true, "displayUrl": "https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs179876", "snippet": "Among the works that use the model based on <b>Markov</b> <b>Decision</b> Processes (<b>MDP</b>), ... to Pamplona:destination_<b>city</b>: en clase turista:clase_viaje: in <b>tourist</b> class:trip_class: As mentioned in , if the semantic language is sequential with the input language, then we can perform a segmentation of the input sentence into a number of intervals which is equal to the number of semantic units in the corresponding semantic sentence. Formally, let W be the vocabulary of the task (set of words), and let V ...", "dateLastCrawled": "2022-02-03T04:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "M NAVIGATION IN ENVIRONMENTS WITH N L USING ABSTRACT 2-D MAPS", "url": "https://openreview.net/pdf?id=_lV1OrJIgiG", "isFamilyFriendly": true, "displayUrl": "https://openreview.net/pdf?id=_lV1OrJIgiG", "snippet": "of its start and goal locations (\u201ctask context\u201d in Figure 1). This is akin to a <b>tourist</b> attempting to \ufb01nd a landmark <b>in a new</b> <b>city</b>: without any further help, this would be very challenging; but when equipped with a 2-D map with a \u201cyou are here\u201d symbol and an indicator of the landmark, the <b>tourist</b> can easily plan a path to reach the landmark without needing to explore or train excessively. Navigation is a fundamental capability of all embodied agents, both arti\ufb01cial and natural ...", "dateLastCrawled": "2021-12-25T02:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Partially observable <b>Markov</b> <b>decision</b> processes for <b>spoken dialog</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0885230806000283", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0885230806000283", "snippet": "In this paper, we will argue that a partially observable <b>Markov decision process</b> (POMDP 2) provides such a framework. We will explain how a POMDP <b>can</b> be developed to encompass a complete dialog system, how a POMDP serves as a basis for optimization, and how a POMDP <b>can</b> integrate uncertainty in the form of statistical distributions with heuristics in the form of manually specified rules. To illustrate the power of the POMDP formalism, we will show how each of the three approaches above ...", "dateLastCrawled": "2022-01-20T09:55:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "(PDF) Optimizing Spoken Dialogue Management with Fitted Value Iteration", "url": "https://www.researchgate.net/publication/221488611_Optimizing_Spoken_Dialogue_Management_with_Fitted_Value_Iteration", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/221488611_Optimizing_Spoken_Dialogue...", "snippet": "cinctly into a <b>Markov Decision Process</b> (<b>MDP</b>) [3, 11] or a Par- tially Observable <b>Markov Decision Process</b> (POMDP) [1, 23]. Reinforcement learning (RL) [22] <b>can</b> therefore be applied to. learn the ...", "dateLastCrawled": "2022-02-02T00:51:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "(PDF) <b>Partially Observable Markov Decision Processes for</b> Spoken Dialog ...", "url": "https://www.academia.edu/6729816/Partially_Observable_Markov_Decision_Processes_for_Spoken_Dialog_Systems", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/6729816", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-17T00:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "(PDF) A <b>case study of applying decision theory</b> in the real world ...", "url": "https://www.researchgate.net/publication/228777252_A_case_study_of_applying_decision_theory_in_the_real_world_POMDPs_and_spoken_dialog_systems", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/228777252_A_<b>case_study_of_applying_decision</b>...", "snippet": "As such, <b>decision</b> theory, and in particular partially-observable <b>Markov</b> <b>decision</b> processes (POMDPs), present an attractive approach to building spoken dialog systems. Initial work on &quot;toy&quot; dia-log ...", "dateLastCrawled": "2021-11-09T21:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "(PDF) <b>Probabilistic planning via linear value-approximation of first</b> ...", "url": "https://www.academia.edu/2799718/Probabilistic_planning_via_linear_value_approximation_of_first_order_MDPs", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/2799718/<b>Probabilistic_planning_via_linear_value_approximation</b>...", "snippet": "Abstract We describe a probabilistic planning approach that translates a PPDDL planning problem description to a first-order <b>MDP</b> (FOMDP) and uses approximate solution techniques for FOMDPs to derive a value function and corresponding policy. Our", "dateLastCrawled": "2022-01-21T02:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "Artificial Intelligence and Machine Learning in Business Management ...", "url": "https://ebin.pub/artificial-intelligence-and-machine-learning-in-business-management-concepts-challenges-and-case-studies-1nbsped-0367645556-9780367645557.html", "isFamilyFriendly": true, "displayUrl": "https://ebin.pub/artificial-intelligence-and-machine-learning-in-business-management...", "snippet": "A simple implementation based on <b>Markov Decision Process</b> is present to understand this approach. Chapter 10, Investigating Artificial Intelligence Usage for Revolution in E-\u00adLearning during COVID-\u00ad19, examines the portrayal of AI in E-\u00adlearning during COVID-\u00ad19 and apart from discovering the role of AI during this pandemic. The study has also investigated the future of AI in E-\u00adlearning post COVID-\u00ad19. Chapter 11, Employee Churn Management Using AI, describes an AI model which <b>can</b> help ...", "dateLastCrawled": "2022-01-29T11:24:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "Artificial Intelligence By Example.pdf [5wgl5d599xo7]", "url": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "isFamilyFriendly": true, "displayUrl": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "snippet": "This <b>thought</b> <b>process</b> in machine learning is the <b>MDP</b>. This form of action-value learning is sometimes called Q. To master the outcomes of <b>MDP</b> in theory and practice, a three-dimensional method is a prerequisite. The three-dimensional approach that will make you an artificial expert, in general terms, means: Starting by describing a problem to solve with real-life cases Then, building a mathematical model Then, write source code and/or using a cloud platform solution It is a way for you to ...", "dateLastCrawled": "2022-01-27T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "View of Review of spoken dialogue systems | Loquens", "url": "https://loquens.revistas.csic.es/index.php/loquens/article/view/17/47", "isFamilyFriendly": true, "displayUrl": "https://loquens.revistas.csic.es/index.php/loquens/article/view/17/47", "snippet": "2.1. Automatic Speech Recognition. The module that implements ASR is called the speech recogniser.Its goal is to receive the user\u2019s speech and generate as output a recognition hypothesis, which is the sequence of words that most likely corresponds to what the user has said (Rabiner &amp; Huang, 1993).Unfortunately, in many cases the recognition hypothesis contains errors in the form of inserted, substituted or deleted words.", "dateLastCrawled": "2022-02-02T17:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "arxiv-cs-analysis/cluster_phrase_semicolon_50.txt at master \u00b7 tf-dbis ...", "url": "https://github.com/tf-dbis-uni-freiburg/arxiv-cs-analysis/blob/master/Noun%20Phrase%20Frequencies%20Visualization/NPFreqSolrDash/cluster_phrase_semicolon_50.txt", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/tf-dbis-uni-freiburg/arxiv-cs-analysis/blob/master/Noun Phrase...", "snippet": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.", "dateLastCrawled": "2022-01-31T10:39:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "cybergeo20/articles.csv at master \u00b7 <b>Geographie-cites/cybergeo20</b> \u00b7 <b>GitHub</b>", "url": "https://github.com/Geographie-cites/cybergeo20/blob/master/R_Country_Wordcloud/articles.csv", "isFamilyFriendly": true, "displayUrl": "https://<b>github.com</b>/Geographie-cites/cybergeo20/blob/master/R_Country_Wordcloud/...", "snippet": "26842 Environmental Impact Assessment as a Social <b>Process</b>: The Case of Nuclear Waste Storage in Sweden Environmental Impact Assessment (EIA),<b>decision</b>-making,NGOs,nuclear waste repository impact environnemental,prise de <b>decision</b>,d\u00e9chets nucl\u00e9aires,stockage d\u00e9chets nucl\u00e9aires Keskitalo E. Carina H.,Nordlund Annika,Lindgren Urban 2015-03-10 en FALSE SE SE 26829 Multifractal portrayal of the Swiss population fractal dimension,box-counting method,multifractal dimensions,R\u00e9nyi dimensions ...", "dateLastCrawled": "2021-09-22T20:22:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "(PDF) <b>A Markov Decision Process Approach to Vacant</b> Taxi Routing with E ...", "url": "https://www.researchgate.net/publication/328013734_A_Markov_Decision_Process_Approach_to_Vacant_Taxi_Routing_with_E-hailing", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/328013734_<b>A_Markov_Decision_Process_Approach</b>...", "snippet": "Abstract. The optimal routing of a vacant taxi is formulated as a <b>Markov Decision Process</b> (<b>MDP</b>) problem to account for long-term profit over the full working period. The state is defined by the ...", "dateLastCrawled": "2022-01-05T03:03:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "A Study <b>of Recommender Systems Using Markov Decision Process</b>", "url": "https://www.researchgate.net/publication/331681629_A_Study_of_Recommender_Systems_Using_Markov_Decision_Process", "isFamilyFriendly": true, "displayUrl": "https://www.researchgate.net/publication/331681629_A_Study_of_Recommender_Systems...", "snippet": "One of the proposed approaches is to consider a recommender system as a <b>Markov decision process</b> (<b>MDP</b>) problem and try to solve it using reinforcement learning (RL). However, existing RL-based ...", "dateLastCrawled": "2021-08-11T21:17:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "Clustering Users\u2019 POIs Visit Trajectories for Next-POI Recommendation", "url": "https://www.inf.unibz.it/~ricci//papers/Massimo-Enter-2019.pdf", "isFamilyFriendly": true, "displayUrl": "https://www.inf.unibz.it/~ricci//papers/Massimo-Enter-2019.pdf", "snippet": "3.1 Sequential <b>Decision</b> Making Model The <b>tourist</b> POIs visit trajectory generation task is modelled here as a \ufb01nite <b>Markov Decision Process</b> (<b>MDP</b>). A <b>MDP</b> is a tuple (S,A,T,r,\u03b3). S is a \ufb01nite set of states and a state represents the visit to a POI in a speci\ufb01c context (weather, temperature and day). A is a \ufb01nite set of actions: the ...", "dateLastCrawled": "2022-01-02T04:59:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "Popularity, novelty and relevance in point of interest recommendation ...", "url": "https://link.springer.com/article/10.1007/s40558-021-00214-5", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/article/10.1007/s40558-021-00214-5", "snippet": "User behaviour modelling. We model the user (<b>tourist</b>) choice-making behaviour with a <b>Markov Decision Process</b> (<b>MDP</b>) (Sutton and Barto 1998).A <b>MDP</b> is a tuple \\((S, A, T, r, \\gamma )\\). S is the set of possible states; in our scenario, a state models the visit to a POI in a specific contextual situation. For instance, a <b>tourist</b> who visits Florence could be at Ponte Vecchio (POI) in an overcast, mild morning.", "dateLastCrawled": "2022-02-02T01:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Adaptive and <b>large-scale service composition based</b> on deep ...", "url": "https://www.sciencedirect.com/science/article/pii/S0950705119302266", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0950705119302266", "snippet": "Wang et al. exploited <b>Markov Decision Process</b> (<b>MDP</b>) integrated with reinforcement learning to find the optimal composition result. The <b>MDP</b> model achieved multiple workflows without preliminary knowledge of service quality and environment. When facing with the changeable environment, the learning <b>process</b> <b>can</b> perceive and respond to the changes. Some modifications have been applied to RL to address the efficiency issue. In", "dateLastCrawled": "2021-11-10T20:00:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "(PDF) <b>Partially Observable Markov Decision Processes for</b> Spoken Dialog ...", "url": "https://www.academia.edu/6729816/Partially_Observable_Markov_Decision_Processes_for_Spoken_Dialog_Systems", "isFamilyFriendly": true, "displayUrl": "https://www.academia.edu/6729816", "snippet": "Academia.edu is a platform for academics to share research papers.", "dateLastCrawled": "2022-01-17T00:41:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "TRACE: Travel Reinforcement Recommendation Based on Location-Aware ...", "url": "https://dl.acm.org/doi/fullHtml/10.1145/3487047", "isFamilyFriendly": true, "displayUrl": "https://dl.acm.org/doi/fullHtml/10.1145/3487047", "snippet": "First, the proposed method models the recommendation <b>process</b> of attractions as a <b>Markov</b> <b>Decision</b>-Making <b>Process</b> (<b>MDP</b>) in order to evaluate the long-term benefits of recommendation results rather than just calculating the short-term accuracy. Second, we construct a reinforcement recommendation framework for the modeled <b>MDP</b> based on the Actor-Critic framework to dynamically train and update the parameters. In addition, to help the TRACE model track the users\u2019 real-time context, we ...", "dateLastCrawled": "2022-01-13T19:08:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "<b>Recent research advances in Reinforcement Learning in Spoken Dialogue</b> ...", "url": "https://www.cambridge.org/core/journals/knowledge-engineering-review/article/recent-research-advances-in-reinforcement-learning-in-spoken-dialogue-systems/F902B0E8DB5FE1D802546FFF9F880F48", "isFamilyFriendly": true, "displayUrl": "https://www.cambridge.org/core/journals/knowledge-engineering-review/article/recent...", "snippet": "2. a first-order <b>Markov</b> model, i.e. an <b>MDP</b>; 3. a second-order <b>Markov</b> model; 4. a cumulative total reward model. As we already know, in an <b>MDP</b>, a time slice\u2019s state variables depend on those from the previous time slice. For a second order <b>Markov</b> model, the third time slice state variables <b>can</b> also depend on those in the first time slice ...", "dateLastCrawled": "2021-12-08T03:20:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "Artificial Intelligence By Example.pdf [5wgl5d599xo7]", "url": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "isFamilyFriendly": true, "displayUrl": "https://vbook.pub/documents/artificial-intelligence-by-examplepdf-5wgl5d599xo7", "snippet": "<b>Markov</b> chain: The <b>process</b> of going from state to state with no history in a random, stochastic way is called a <b>Markov</b> chain. To sum it up, we have three tools: Pa(s,s&#39;): A policy, P, or strategy to move from one state to another Ta(s,s&#39;): A T, or stochastic (random) transition, function to carry out that action Ra(s,s&#39;): An R, or reward, for that action, which <b>can</b> be negative, null, or positive T is the transition function, which makes the agent decide to go from one point to another with a ...", "dateLastCrawled": "2022-01-27T23:13:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "Next-POI Recommendations Matching User\u2019s Visit <b>Behaviour</b>", "url": "https://link.springer.com/chapter/10.1007/978-3-030-65785-7_4", "isFamilyFriendly": true, "displayUrl": "https://link.springer.com/chapter/10.1007/978-3-030-65785-7_4", "snippet": "User <b>decision</b>-making in urban tourism is affected by a multitude of factors: weather conditions; time at disposal; background knowledge of the places to visit; previously visited places; reputation of a place; and many others. Context-aware [] and session-based Recommender Systems (RSs) have been proposed to tackle similar settings [10, 15].Our specific goal is to support visitors to identify next points of interest (POIs) to visit that match their interests and the specific context of the ...", "dateLastCrawled": "2022-01-16T20:53:00.0000000Z", "language": "en", "isNavigational": false}]], "gen_res": [[{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Why does <b>Markov Decision Process</b> matter in Reinforcement <b>Learning</b>? | by ...", "url": "https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd", "isFamilyFriendly": true, "displayUrl": "https://towardsdatascience.com/why-does-malkov-<b>decision</b>-<b>process</b>-matter-in...", "snippet": "It is named by <b>analogy</b> to \u201cone-armed bandit\u201d(= a slot <b>machine</b>) although the framework has k levers instead of one. ... we introduce <b>Markov Decision Process</b>(<b>MDP</b>) to solve such a problem. An <b>MDP</b> consists of two elements; the agent and the environment. The agent is a learner or <b>decision</b>-maker. In the above example, the agent is the rabbit. The environment is everything surrounding the agent. In the example, the environment includes everything in the field where the rabbit is with food and ...", "dateLastCrawled": "2022-01-31T01:58:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.1", "name": "<b>Markov decision process</b>: value iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-value-iteration-2d161d50a6ff", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-value-iteration-2d161d50a6ff", "snippet": "<b>Markov decision process</b>, <b>MDP</b>, value iteration, policy iteration, policy evaluation, policy improvement, sweep, iterative policy evaluation, policy, optimal policy ...", "dateLastCrawled": "2022-01-08T01:29:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.2", "name": "<b>Introduction to Machine Learning</b> \u2013 Data and Beyond", "url": "https://dataandbeyond.wordpress.com/2017/08/24/introduction-to-machine-learning-2/", "isFamilyFriendly": true, "displayUrl": "https://dataandbeyond.wordpress.com/2017/08/24/<b>introduction-to-machine-learning</b>-2", "snippet": "<b>Markov decision process</b> (<b>MDP</b>): In reinforcement <b>learning</b>, <b>MDP</b> is a mathematical framework for modeling <b>decision</b>-making of an agent in situations or environments where outcomes are partly random and partly under control. In this model, environment is modeled as a set of states and actions that can be performed by an agent to control the system\u2019s state. The objective is to control the system in such a way that the agent\u2019s total payoff is maximized.", "dateLastCrawled": "2022-01-17T05:57:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.3", "name": "<b>Markov Decision Process</b>: How Does Value Iteration Work? | Baeldung on ...", "url": "https://www.baeldung.com/cs/mdp-value-iteration", "isFamilyFriendly": true, "displayUrl": "https://www.baeldung.com/cs/<b>mdp</b>-value-iteration", "snippet": "From this point, we can make an <b>analogy</b> with the <b>Markov</b> model since the solution for this problem is a sequence of actions. A <b>Markov Decision Process</b> is used to model the agent, considering that the agent itself generates a series of actions. In the real world, we can have observable, hidden, or partially observed states, depending on the ...", "dateLastCrawled": "2022-01-30T20:21:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.4", "name": "Lecture <b>Reinforcement Learning</b> - MIT OpenCourseWare", "url": "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-notes/MIT6_S897S19_lec16note.pdf", "isFamilyFriendly": true, "displayUrl": "https://<b>ocw.mit.edu</b>/courses/electrical-engineering-and-computer-science/6-s897-<b>machine</b>...", "snippet": "4.1 Examples of <b>decision</b> processes. A <b>Markov decision process</b> (<b>MDP</b>) is a well-known type of <b>decision</b> <b>process</b>, where the states follow the <b>Markov</b> assumption that the state transitions, rewards, and actions depend only on the most recent state-action pair. See Figure 3(a) for an illustration. Algebraically, this means the states, actions and reward", "dateLastCrawled": "2022-02-03T08:11:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.5", "name": "CSE599i: Online and Adaptive <b>Machine</b> <b>Learning</b> Winter 2018 Lecture 19 ...", "url": "https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture19/lecture19.pdf", "isFamilyFriendly": true, "displayUrl": "https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture19/lecture19.pdf", "snippet": "1.1Summary of <b>Markov</b> <b>Decision</b> Processes A <b>Markov Decision Process</b> (<b>MDP</b>) is a probabilistic model for reward-incentivized, memoryless, sequential <b>decision</b>-making. An <b>MDP</b> models a scenario in which an agent (the <b>decision</b> maker) iteratively observes the", "dateLastCrawled": "2021-09-07T09:54:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.6", "name": "<b>Markov decision process</b>: policy iteration with code implementation | by ...", "url": "https://medium.com/@ngao7/markov-decision-process-policy-iteration-42d35ee87c82?source=post_internal_links---------0-------------------------------", "isFamilyFriendly": true, "displayUrl": "https://medium.com/@ngao7/<b>markov-decision-process</b>-policy-iteration-42d35ee87c82?source=...", "snippet": "<b>Markov decision process</b>: policy iteration with code implementation . Nan. Dec 19, 2021 \u00b7 16 min read. In today\u2019s story we focus on policy iteration of <b>MDP</b>. We are still using the grid world ...", "dateLastCrawled": "2022-01-22T13:15:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.7", "name": "Reinforcement <b>Learning</b> \u2014 Controversy over Reward | by OperAI ...", "url": "https://operai.medium.com/reinforcement-learning-reward-controversy-issue-e9b88167d238", "isFamilyFriendly": true, "displayUrl": "https://operai.medium.com/reinforcement-<b>learning</b>-reward-controversy-issue-e9b88167d238", "snippet": "Example of RL algorithm is the <b>Markov Decision Process</b> (<b>MDP</b>) and there is a package for applying <b>MDP</b>. Other algorithms and packages are also under development such as the \u201cReinforcementLearning\u201d package, which is intended to partially close this gap and offers the ability to perform model-free reinforcement <b>learning</b> in a highly customizable framework.", "dateLastCrawled": "2022-02-01T20:28:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.8", "name": "<b>machine</b> <b>learning</b> - Why is the <b>optimal policy</b> in <b>Markov Decision Process</b> ...", "url": "https://stats.stackexchange.com/questions/132890/why-is-the-optimal-policy-in-markov-decision-process-mdp-independent-of-the-i", "isFamilyFriendly": true, "displayUrl": "https://stats.stackexchange.com/questions/132890", "snippet": "The intuition behind the argument saying that the <b>optimal policy</b> is independent of initial state is the following: The <b>optimal policy</b> is defined by a function that selects an action for every possible state and actions in different states are independent.. Formally speaking, for an unknown initial distribution, the value function to maximize would be the following (not conditioned on initial state)", "dateLastCrawled": "2022-01-25T23:19:00.0000000Z", "language": "en", "isNavigational": false}, {"id": "https://api.bing.microsoft.com/api/v7/#WebPages.9", "name": "<b>Machine</b> <b>learning</b> and AI <b>in marketing \u2013 Connecting computing power to</b> ...", "url": "https://www.sciencedirect.com/science/article/pii/S0167811620300410", "isFamilyFriendly": true, "displayUrl": "https://www.sciencedirect.com/science/article/pii/S0167811620300410", "snippet": "These tasks are often formulated as a <b>Markov decision process</b> (<b>MDP</b>), a structure familiar to marketing researchers who investigate forward-looking behaviors using dynamic programming models. The <b>learning</b> algorithm needs to determine the actions to take to both learn the environment&#39;s characteristics and craft optimal policy of actions given the states. This type of <b>machine</b> <b>learning</b> tasks has received heightened attention due to recent methodological advancement and increasing usages in the ...", "dateLastCrawled": "2022-01-12T18:25:00.0000000Z", "language": "en", "isNavigational": false}], [{"id": "https://api.bing.microsoft.com/api/v7/#WebPages.0", "name": "Overview: Representation Techniques", "url": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66fffb5d0bd4c20697922f5ffbf9a602b66bec3f74ac83fb77c/DecisionMaking.pdf", "isFamilyFriendly": true, "displayUrl": "https://webcms3.cse.unsw.edu.au/static/uploads/course/COMP4418/17s2/9c0e19e0e02df66...", "snippet": "<b>Markov Decision Process MDP is like</b> a Markov process, except every round we make a decision Transition probabilities depend on actions taken P(St+1 = S&#39; | St = s, At = a) = P(S, a, S&#39;) Rewards for every state, action pair u(St = s, At = a) Discount factor \u03b4 Example. A <b>machine</b> can be in one of three states: good, deteriorating, broken Can take ...", "dateLastCrawled": "2022-01-21T05:20:00.0000000Z", "language": "en", "isNavigational": false}], [], [], [], []], "all_bing_queries": ["+(markov decision process (mdp))  is like +(tourist in a new city)", "+(markov decision process (mdp)) is similar to +(tourist in a new city)", "+(markov decision process (mdp)) can be thought of as +(tourist in a new city)", "+(markov decision process (mdp)) can be compared to +(tourist in a new city)", "machine learning +(markov decision process (mdp) AND analogy)", "machine learning +(\"markov decision process (mdp) is like\")", "machine learning +(\"markov decision process (mdp) is similar\")", "machine learning +(\"just as markov decision process (mdp)\")", "machine learning +(\"markov decision process (mdp) can be thought of as\")", "machine learning +(\"markov decision process (mdp) can be compared to\")"]}